
----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/kernel_approximation.py----------------------------------------
A:sklearn.kernel_approximation.X->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.kernel_approximation.random_state->check_random_state(self.random_state)
A:sklearn.kernel_approximation.self.indexHash_->check_random_state(self.random_state).randint(0, high=self.n_components, size=(self.degree, n_features))
A:sklearn.kernel_approximation.self.bitHash_->check_random_state(self.random_state).choice(a=[-1, 1], size=(self.degree, n_features))
A:sklearn.kernel_approximation.X_gamma->numpy.hstack([X_gamma, np.sqrt(self.coef0) * np.ones((X_gamma.shape[0], 1))])
A:sklearn.kernel_approximation.count_sketches->numpy.zeros((X_gamma.shape[0], self.degree, self.n_components))
A:sklearn.kernel_approximation.count_sketches_fft->fft(count_sketches, axis=2, overwrite_x=True)
A:sklearn.kernel_approximation.count_sketches_fft_prod->numpy.prod(count_sketches_fft, axis=1)
A:sklearn.kernel_approximation.data_sketch->numpy.real(ifft(count_sketches_fft_prod, overwrite_x=True))
A:sklearn.kernel_approximation.sparse->scipy.sparse.issparse(X)
A:sklearn.kernel_approximation.self.random_offset_->self.random_offset_.astype(X.dtype, copy=False)
A:sklearn.kernel_approximation.self.random_weights_->self.random_weights_.astype(X.dtype, copy=False)
A:sklearn.kernel_approximation.projection->safe_sparse_dot(X, self.random_weights_)
A:sklearn.kernel_approximation.uniform->check_random_state(self.random_state).uniform(size=(n_features, self.n_components))
A:sklearn.kernel_approximation.input_features->_check_feature_names_in(self, input_features, generate_names=True)
A:sklearn.kernel_approximation.est_name->self.__class__.__name__.lower()
A:sklearn.kernel_approximation.X_step->scipy.sparse.csr_matrix((data_step, indices, indptr), shape=X.shape, dtype=X.dtype, copy=False)
A:sklearn.kernel_approximation.X_step[non_zero]->numpy.sqrt(X_nz * sample_interval)
A:sklearn.kernel_approximation.factor_nz->numpy.sqrt(step_nz / np.cosh(np.pi * j * sample_interval))
A:sklearn.kernel_approximation.indices->self._validate_data(X, accept_sparse='csr', reset=False).indices.copy()
A:sklearn.kernel_approximation.indptr->self._validate_data(X, accept_sparse='csr', reset=False).indptr.copy()
A:sklearn.kernel_approximation.data_step->numpy.sqrt(X.data * sample_interval)
A:sklearn.kernel_approximation.rnd->check_random_state(self.random_state)
A:sklearn.kernel_approximation.n_components->min(n_samples, n_components)
A:sklearn.kernel_approximation.inds->check_random_state(self.random_state).permutation(n_samples)
A:sklearn.kernel_approximation.basis_kernel->pairwise_kernels(basis, metric=self.kernel, filter_params=True, n_jobs=self.n_jobs, **self._get_kernel_params())
A:sklearn.kernel_approximation.(U, S, V)->svd(basis_kernel)
A:sklearn.kernel_approximation.S->numpy.maximum(S, 1e-12)
A:sklearn.kernel_approximation.self.normalization_->numpy.dot(U / np.sqrt(S), V)
A:sklearn.kernel_approximation.kernel_params->self._get_kernel_params()
A:sklearn.kernel_approximation.embedded->pairwise_kernels(X, self.components_, metric=self.kernel, filter_params=True, n_jobs=self.n_jobs, **kernel_params)
A:sklearn.kernel_approximation.params[param]->getattr(self, param)
sklearn.kernel_approximation.AdditiveChi2Sampler(self,*,sample_steps=2,sample_interval=None)
sklearn.kernel_approximation.AdditiveChi2Sampler.__init__(self,*,sample_steps=2,sample_interval=None)
sklearn.kernel_approximation.AdditiveChi2Sampler._more_tags(self)
sklearn.kernel_approximation.AdditiveChi2Sampler._transform_dense(X,sample_steps,sample_interval)
sklearn.kernel_approximation.AdditiveChi2Sampler._transform_sparse(X,sample_steps,sample_interval)
sklearn.kernel_approximation.AdditiveChi2Sampler.fit(self,X,y=None)
sklearn.kernel_approximation.AdditiveChi2Sampler.get_feature_names_out(self,input_features=None)
sklearn.kernel_approximation.AdditiveChi2Sampler.sample_interval_(self)
sklearn.kernel_approximation.AdditiveChi2Sampler.transform(self,X)
sklearn.kernel_approximation.Nystroem(self,kernel='rbf',*,gamma=None,coef0=None,degree=None,kernel_params=None,n_components=100,random_state=None,n_jobs=None)
sklearn.kernel_approximation.Nystroem.__init__(self,kernel='rbf',*,gamma=None,coef0=None,degree=None,kernel_params=None,n_components=100,random_state=None,n_jobs=None)
sklearn.kernel_approximation.Nystroem._get_kernel_params(self)
sklearn.kernel_approximation.Nystroem._more_tags(self)
sklearn.kernel_approximation.Nystroem.fit(self,X,y=None)
sklearn.kernel_approximation.Nystroem.transform(self,X)
sklearn.kernel_approximation.PolynomialCountSketch(self,*,gamma=1.0,degree=2,coef0=0,n_components=100,random_state=None)
sklearn.kernel_approximation.PolynomialCountSketch.__init__(self,*,gamma=1.0,degree=2,coef0=0,n_components=100,random_state=None)
sklearn.kernel_approximation.PolynomialCountSketch.fit(self,X,y=None)
sklearn.kernel_approximation.PolynomialCountSketch.transform(self,X)
sklearn.kernel_approximation.RBFSampler(self,*,gamma=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.RBFSampler.__init__(self,*,gamma=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.RBFSampler._more_tags(self)
sklearn.kernel_approximation.RBFSampler.fit(self,X,y=None)
sklearn.kernel_approximation.RBFSampler.transform(self,X)
sklearn.kernel_approximation.SkewedChi2Sampler(self,*,skewedness=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.SkewedChi2Sampler.__init__(self,*,skewedness=1.0,n_components=100,random_state=None)
sklearn.kernel_approximation.SkewedChi2Sampler._more_tags(self)
sklearn.kernel_approximation.SkewedChi2Sampler.fit(self,X,y=None)
sklearn.kernel_approximation.SkewedChi2Sampler.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/naive_bayes.py----------------------------------------
A:sklearn.naive_bayes.X->self._validate_data(X, dtype='int', accept_sparse=False, force_all_finite=True, reset=False)
A:sklearn.naive_bayes.jll->numpy.zeros((X.shape[0], self.class_count_.shape[0]))
A:sklearn.naive_bayes.log_prob_x->logsumexp(jll, axis=1)
A:sklearn.naive_bayes.y->self._validate_data(y=y)
A:sklearn.naive_bayes.n_new->float(sample_weight.sum())
A:sklearn.naive_bayes.new_mu->numpy.mean(X, axis=0)
A:sklearn.naive_bayes.new_var->numpy.var(X, axis=0)
A:sklearn.naive_bayes.n_total->float(n_past + n_new)
A:sklearn.naive_bayes.first_call->_check_partial_fit_first_call(self, classes)
A:sklearn.naive_bayes.(X, y)->self._validate_data(X, y, dtype='int', accept_sparse=False, force_all_finite=True, reset=reset)
A:sklearn.naive_bayes.sample_weight->numpy.atleast_2d(sample_weight)
A:sklearn.naive_bayes.n_classes->len(classes)
A:sklearn.naive_bayes.self.theta_->numpy.zeros((n_classes, n_features))
A:sklearn.naive_bayes.self.var_->numpy.zeros((n_classes, n_features))
A:sklearn.naive_bayes.self.class_count_->numpy.zeros(n_classes, dtype=np.float64)
A:sklearn.naive_bayes.priors->numpy.asarray(self.priors)
A:sklearn.naive_bayes.self.class_prior_->numpy.zeros(len(self.classes_), dtype=np.float64)
A:sklearn.naive_bayes.unique_y->numpy.unique(y)
A:sklearn.naive_bayes.unique_y_in_classes->numpy.isin(unique_y, classes)
A:sklearn.naive_bayes.i->classes.searchsorted(y_i)
A:sklearn.naive_bayes.N_i->sw_i.sum()
A:sklearn.naive_bayes.(new_theta, new_sigma)->self._update_mean_variance(self.class_count_[i], self.theta_[i, :], self.var_[i, :], X_i, sw_i)
A:sklearn.naive_bayes.jointi->numpy.log(self.class_prior_[i])
A:sklearn.naive_bayes.self.class_log_prior_->numpy.full(n_classes, -np.log(n_classes))
A:sklearn.naive_bayes.log_class_count->numpy.log(self.class_count_)
A:sklearn.naive_bayes.alpha_min->numpy.min(alpha)
A:sklearn.naive_bayes.Y->Y.astype(np.float64, copy=False).astype(np.float64, copy=False)
A:sklearn.naive_bayes.alpha->self._check_alpha()
A:sklearn.naive_bayes.labelbin->LabelBinarizer()
A:sklearn.naive_bayes.self.feature_count_->numpy.zeros((n_classes, n_features), dtype=np.float64)
A:sklearn.naive_bayes.smoothed_cc->smoothed_fc.sum(axis=1)
A:sklearn.naive_bayes.self.feature_all_->self.feature_count_.sum(axis=0)
A:sklearn.naive_bayes.logged->numpy.log(comp_count / comp_count.sum(axis=1, keepdims=True))
A:sklearn.naive_bayes.summed->numpy.log(comp_count / comp_count.sum(axis=1, keepdims=True)).sum(axis=1, keepdims=True)
A:sklearn.naive_bayes.neg_prob->numpy.log(1 - np.exp(self.feature_log_prob_))
A:sklearn.naive_bayes.min_categories_->numpy.array(min_categories)
A:sklearn.naive_bayes.n_categories_->numpy.maximum(n_categories_X, min_categories_, dtype=np.int64)
A:sklearn.naive_bayes.mask->Y[:, j].astype(bool)
A:sklearn.naive_bayes.counts->numpy.bincount(X_feature[mask], weights=weights)
A:sklearn.naive_bayes.self.n_categories_->self._validate_n_categories(X, self.min_categories)
A:sklearn.naive_bayes.self.category_count_[i]->_update_cat_count_dims(self.category_count_[i], self.n_categories_[i] - 1)
A:sklearn.naive_bayes.smoothed_class_count->smoothed_cat_count.sum(axis=1)
sklearn.naive_bayes.BernoulliNB(self,*,alpha=1.0,force_alpha=True,binarize=0.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.BernoulliNB.__init__(self,*,alpha=1.0,force_alpha=True,binarize=0.0,fit_prior=True,class_prior=None)
sklearn.naive_bayes.BernoulliNB._check_X(self,X)
sklearn.naive_bayes.BernoulliNB._check_X_y(self,X,y,reset=True)
sklearn.naive_bayes.BernoulliNB._count(self,X,Y)
sklearn.naive_bayes.BernoulliNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.BernoulliNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes.CategoricalNB(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None,min_categories=None)
sklearn.naive_bayes.CategoricalNB.__init__(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None,min_categories=None)
sklearn.naive_bayes.CategoricalNB._check_X(self,X)
sklearn.naive_bayes.CategoricalNB._check_X_y(self,X,y,reset=True)
sklearn.naive_bayes.CategoricalNB._count(self,X,Y)
sklearn.naive_bayes.CategoricalNB._init_counters(self,n_classes,n_features)
sklearn.naive_bayes.CategoricalNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.CategoricalNB._more_tags(self)
sklearn.naive_bayes.CategoricalNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes.CategoricalNB._validate_n_categories(X,min_categories)
sklearn.naive_bayes.CategoricalNB.fit(self,X,y,sample_weight=None)
sklearn.naive_bayes.CategoricalNB.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.naive_bayes.ComplementNB(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None,norm=False)
sklearn.naive_bayes.ComplementNB.__init__(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None,norm=False)
sklearn.naive_bayes.ComplementNB._count(self,X,Y)
sklearn.naive_bayes.ComplementNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.ComplementNB._more_tags(self)
sklearn.naive_bayes.ComplementNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes.GaussianNB(self,*,priors=None,var_smoothing=1e-09)
sklearn.naive_bayes.GaussianNB.__init__(self,*,priors=None,var_smoothing=1e-09)
sklearn.naive_bayes.GaussianNB._check_X(self,X)
sklearn.naive_bayes.GaussianNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.GaussianNB._partial_fit(self,X,y,classes=None,_refit=False,sample_weight=None)
sklearn.naive_bayes.GaussianNB._update_mean_variance(n_past,mu,var,X,sample_weight=None)
sklearn.naive_bayes.GaussianNB.fit(self,X,y,sample_weight=None)
sklearn.naive_bayes.GaussianNB.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.naive_bayes.MultinomialNB(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None)
sklearn.naive_bayes.MultinomialNB.__init__(self,*,alpha=1.0,force_alpha=True,fit_prior=True,class_prior=None)
sklearn.naive_bayes.MultinomialNB._count(self,X,Y)
sklearn.naive_bayes.MultinomialNB._joint_log_likelihood(self,X)
sklearn.naive_bayes.MultinomialNB._more_tags(self)
sklearn.naive_bayes.MultinomialNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes._BaseDiscreteNB(self,alpha=1.0,fit_prior=True,class_prior=None,force_alpha=True)
sklearn.naive_bayes._BaseDiscreteNB.__init__(self,alpha=1.0,fit_prior=True,class_prior=None,force_alpha=True)
sklearn.naive_bayes._BaseDiscreteNB._check_X(self,X)
sklearn.naive_bayes._BaseDiscreteNB._check_X_y(self,X,y,reset=True)
sklearn.naive_bayes._BaseDiscreteNB._check_alpha(self)
sklearn.naive_bayes._BaseDiscreteNB._count(self,X,Y)
sklearn.naive_bayes._BaseDiscreteNB._init_counters(self,n_classes,n_features)
sklearn.naive_bayes._BaseDiscreteNB._more_tags(self)
sklearn.naive_bayes._BaseDiscreteNB._update_class_log_prior(self,class_prior=None)
sklearn.naive_bayes._BaseDiscreteNB._update_feature_log_prob(self,alpha)
sklearn.naive_bayes._BaseDiscreteNB.fit(self,X,y,sample_weight=None)
sklearn.naive_bayes._BaseDiscreteNB.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.naive_bayes._BaseNB(ClassifierMixin,BaseEstimator,metaclass=ABCMeta)
sklearn.naive_bayes._BaseNB._check_X(self,X)
sklearn.naive_bayes._BaseNB._joint_log_likelihood(self,X)
sklearn.naive_bayes._BaseNB.predict(self,X)
sklearn.naive_bayes._BaseNB.predict_joint_log_proba(self,X)
sklearn.naive_bayes._BaseNB.predict_log_proba(self,X)
sklearn.naive_bayes._BaseNB.predict_proba(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/discriminant_analysis.py----------------------------------------
A:sklearn.discriminant_analysis.sc->StandardScaler()
A:sklearn.discriminant_analysis.X->self._validate_data(X, reset=False)
A:sklearn.discriminant_analysis.s->shrunk_covariance(empirical_covariance(X), shrinkage)
A:sklearn.discriminant_analysis.(xp, is_array_api_compliant)->get_namespace(X)
A:sklearn.discriminant_analysis.(classes, y)->xp.unique_inverse(y)
A:sklearn.discriminant_analysis.means->xp.zeros((classes.shape[0], X.shape[1]), device=device(X), dtype=X.dtype)
A:sklearn.discriminant_analysis.means[i, :]->xp.mean(X[y == i], axis=0)
A:sklearn.discriminant_analysis.cnt->numpy.bincount(y)
A:sklearn.discriminant_analysis.classes->numpy.unique(y)
A:sklearn.discriminant_analysis.cov->numpy.zeros(shape=(X.shape[1], X.shape[1]))
A:sklearn.discriminant_analysis.self.means_->numpy.asarray(means)
A:sklearn.discriminant_analysis.self.covariance_->_class_cov(X, y, self.priors_)
A:sklearn.discriminant_analysis.St->_cov(X, shrinkage, covariance_estimator)
A:sklearn.discriminant_analysis.(evals, evecs)->scipy.linalg.eigh(Sb, Sw)
A:sklearn.discriminant_analysis.self.coef_->xp.reshape(coef_, (1, -1))
A:sklearn.discriminant_analysis.Xc->xp.concat(Xc, axis=0)
A:sklearn.discriminant_analysis.std->xp.std(Xc, axis=0)
A:sklearn.discriminant_analysis.fac->xp.asarray(1.0 / (n_samples - n_classes))
A:sklearn.discriminant_analysis.(U, S, Vt)->svd(X, full_matrices=False)
A:sklearn.discriminant_analysis.rank->numpy.sum(S > self.tol)
A:sklearn.discriminant_analysis.(_, S, Vt)->numpy.linalg.svd(Xgc, full_matrices=False)
A:sklearn.discriminant_analysis.self.explained_variance_ratio_->xp.empty((0,), dtype=S.dtype)
A:sklearn.discriminant_analysis.(xp, _)->get_namespace(X)
A:sklearn.discriminant_analysis.(X, y)->self._validate_data(X, y)
A:sklearn.discriminant_analysis.self.classes_->unique_labels(y)
A:sklearn.discriminant_analysis.(_, cnts)->xp.unique_counts(y)
A:sklearn.discriminant_analysis.self.priors_->numpy.array(self.priors)
A:sklearn.discriminant_analysis.max_components->min(n_classes - 1, X.shape[1])
A:sklearn.discriminant_analysis.coef_->xp.asarray(self.coef_[1, :] - self.coef_[0, :], dtype=X.dtype)
A:sklearn.discriminant_analysis.intercept_->xp.asarray(self.intercept_[1] - self.intercept_[0], dtype=X.dtype)
A:sklearn.discriminant_analysis.self.intercept_->xp.reshape(intercept_, (1,))
A:sklearn.discriminant_analysis.decision->self.decision_function(X)
A:sklearn.discriminant_analysis.proba->_expit(decision)
A:sklearn.discriminant_analysis.prediction->self.predict_proba(X)
A:sklearn.discriminant_analysis.info->xp.finfo(prediction.dtype)
A:sklearn.discriminant_analysis.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.discriminant_analysis.n_classes->len(self.classes_)
A:sklearn.discriminant_analysis.meang->Xg.mean(0)
A:sklearn.discriminant_analysis.X2->numpy.dot(Xm, R * S ** (-0.5))
A:sklearn.discriminant_analysis.u->numpy.asarray([np.sum(np.log(s)) for s in self.scalings_])
A:sklearn.discriminant_analysis.dec_func->self._decision_function(X)
A:sklearn.discriminant_analysis.d->self._decision_function(X)
A:sklearn.discriminant_analysis.y_pred->self.classes_.take(d.argmax(1))
A:sklearn.discriminant_analysis.values->self._decision_function(X)
A:sklearn.discriminant_analysis.likelihood->numpy.exp(values - values.max(axis=1)[:, np.newaxis])
A:sklearn.discriminant_analysis.probas_->self.predict_proba(X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis(self,solver='svd',shrinkage=None,priors=None,n_components=None,store_covariance=False,tol=0.0001,covariance_estimator=None)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.__init__(self,solver='svd',shrinkage=None,priors=None,n_components=None,store_covariance=False,tol=0.0001,covariance_estimator=None)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._more_tags(self)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_eigen(self,X,y,shrinkage,covariance_estimator)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_lstsq(self,X,y,shrinkage,covariance_estimator)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis._solve_svd(self,X,y)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.decision_function(self,X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit(self,X,y)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_log_proba(self,X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_proba(self,X)
sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(self,*,priors=None,reg_param=0.0,store_covariance=False,tol=0.0001)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.__init__(self,*,priors=None,reg_param=0.0,store_covariance=False,tol=0.0001)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis._decision_function(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.decision_function(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.fit(self,X,y)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_log_proba(self,X)
sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_proba(self,X)
sklearn.discriminant_analysis._class_cov(X,y,priors,shrinkage=None,covariance_estimator=None)
sklearn.discriminant_analysis._class_means(X,y)
sklearn.discriminant_analysis._cov(X,shrinkage=None,covariance_estimator=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/__init__.py----------------------------------------
A:sklearn.__init__.logger->logging.getLogger(__name__)
A:sklearn.__init__._random_seed->int(_random_seed)
sklearn.__init__.setup_module(module)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_min_dependencies.py----------------------------------------
A:sklearn._min_dependencies.parser->argparse.ArgumentParser(description='Get min dependencies for a package')
A:sklearn._min_dependencies.args->argparse.ArgumentParser(description='Get min dependencies for a package').parse_args()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/exceptions.py----------------------------------------
sklearn.exceptions.ConvergenceWarning(UserWarning)
sklearn.exceptions.DataConversionWarning(UserWarning)
sklearn.exceptions.DataDimensionalityWarning(UserWarning)
sklearn.exceptions.EfficiencyWarning(UserWarning)
sklearn.exceptions.FitFailedWarning(RuntimeWarning)
sklearn.exceptions.InconsistentVersionWarning(self,*,estimator_name,current_sklearn_version,original_sklearn_version)
sklearn.exceptions.InconsistentVersionWarning.__init__(self,*,estimator_name,current_sklearn_version,original_sklearn_version)
sklearn.exceptions.InconsistentVersionWarning.__str__(self)
sklearn.exceptions.NotFittedError(ValueError,AttributeError)
sklearn.exceptions.PositiveSpectrumWarning(UserWarning)
sklearn.exceptions.SkipTestWarning(UserWarning)
sklearn.exceptions.UndefinedMetricWarning(UserWarning)
sklearn.exceptions.UnsetMetadataPassedError(self,*,message,unrequested_params,routed_params)
sklearn.exceptions.UnsetMetadataPassedError.__init__(self,*,message,unrequested_params,routed_params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_distributor_init.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/kernel_ridge.py----------------------------------------
A:sklearn.kernel_ridge.(X, y)->self._validate_data(X, y, accept_sparse=('csr', 'csc'), multi_output=True, y_numeric=True)
A:sklearn.kernel_ridge.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.kernel_ridge.K->self._get_kernel(X, self.X_fit_)
A:sklearn.kernel_ridge.alpha->numpy.atleast_1d(self.alpha)
A:sklearn.kernel_ridge.y->y.reshape(-1, 1).reshape(-1, 1)
A:sklearn.kernel_ridge.self.dual_coef_->self.dual_coef_.ravel()
A:sklearn.kernel_ridge.X->self._validate_data(X, accept_sparse=('csr', 'csc'), reset=False)
sklearn.kernel_ridge.KernelRidge(self,alpha=1,*,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None)
sklearn.kernel_ridge.KernelRidge.__init__(self,alpha=1,*,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None)
sklearn.kernel_ridge.KernelRidge._get_kernel(self,X,Y=None)
sklearn.kernel_ridge.KernelRidge._more_tags(self)
sklearn.kernel_ridge.KernelRidge.fit(self,X,y,sample_weight=None)
sklearn.kernel_ridge.KernelRidge.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_config.py----------------------------------------
A:sklearn._config._threadlocal->threading.local()
A:sklearn._config._threadlocal.global_config->_global_config.copy()
A:sklearn._config.local_config->_get_threadlocal_config()
A:sklearn._config.old_config->get_config()
sklearn._config._get_threadlocal_config()
sklearn._config.config_context(*,assume_finite=None,working_memory=None,print_changed_only=None,display=None,pairwise_dist_chunk_size=None,enable_cython_pairwise_dist=None,array_api_dispatch=None,transform_output=None,enable_metadata_routing=None,skip_parameter_validation=None)
sklearn._config.get_config()
sklearn._config.set_config(assume_finite=None,working_memory=None,print_changed_only=None,display=None,pairwise_dist_chunk_size=None,enable_cython_pairwise_dist=None,array_api_dispatch=None,transform_output=None,enable_metadata_routing=None,skip_parameter_validation=None)
sklearn.config_context(*,assume_finite=None,working_memory=None,print_changed_only=None,display=None,pairwise_dist_chunk_size=None,enable_cython_pairwise_dist=None,array_api_dispatch=None,transform_output=None,enable_metadata_routing=None,skip_parameter_validation=None)
sklearn.get_config()
sklearn.set_config(assume_finite=None,working_memory=None,print_changed_only=None,display=None,pairwise_dist_chunk_size=None,enable_cython_pairwise_dist=None,array_api_dispatch=None,transform_output=None,enable_metadata_routing=None,skip_parameter_validation=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/isotonic.py----------------------------------------
A:sklearn.isotonic.(rho, _)->spearmanr(x, y)
A:sklearn.isotonic.rho_0->math.tanh(F - 1.96 * F_se)
A:sklearn.isotonic.rho_1->math.tanh(F + 1.96 * F_se)
A:sklearn.isotonic.y->check_array(y, input_name='y', dtype=X.dtype, **check_params)
A:sklearn.isotonic.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.isotonic.self.f_->scipy.interpolate.interp1d(X, y, kind='linear', bounds_error=bounds_error)
A:sklearn.isotonic.X->check_array(X, input_name='X', dtype=[np.float64, np.float32], **check_params)
A:sklearn.isotonic.self.increasing_->check_increasing(X, y)
A:sklearn.isotonic.order->numpy.lexsort((y, X))
A:sklearn.isotonic.(unique_X, unique_y, unique_sample_weight)->_make_unique(X, y, sample_weight)
A:sklearn.isotonic.keep_data->numpy.ones((len(y),), dtype=bool)
A:sklearn.isotonic.keep_data[1:-1]->numpy.logical_or(np.not_equal(y[1:-1], y[:-2]), np.not_equal(y[1:-1], y[2:]))
A:sklearn.isotonic.check_params->dict(accept_sparse=False, ensure_2d=False)
A:sklearn.isotonic.(X, y)->self._build_y(X, y, sample_weight)
A:sklearn.isotonic.T->numpy.clip(T, self.X_min_, self.X_max_)
A:sklearn.isotonic.res->res.astype(T.dtype).astype(T.dtype)
A:sklearn.isotonic.class_name->self.__class__.__name__.lower()
A:sklearn.isotonic.state->super().__getstate__()
sklearn.isotonic.IsotonicRegression(self,*,y_min=None,y_max=None,increasing=True,out_of_bounds='nan')
sklearn.isotonic.IsotonicRegression.__getstate__(self)
sklearn.isotonic.IsotonicRegression.__init__(self,*,y_min=None,y_max=None,increasing=True,out_of_bounds='nan')
sklearn.isotonic.IsotonicRegression.__setstate__(self,state)
sklearn.isotonic.IsotonicRegression._build_f(self,X,y)
sklearn.isotonic.IsotonicRegression._build_y(self,X,y,sample_weight,trim_duplicates=True)
sklearn.isotonic.IsotonicRegression._check_input_data_shape(self,X)
sklearn.isotonic.IsotonicRegression._more_tags(self)
sklearn.isotonic.IsotonicRegression._transform(self,T)
sklearn.isotonic.IsotonicRegression.fit(self,X,y,sample_weight=None)
sklearn.isotonic.IsotonicRegression.get_feature_names_out(self,input_features=None)
sklearn.isotonic.IsotonicRegression.predict(self,T)
sklearn.isotonic.IsotonicRegression.transform(self,T)
sklearn.isotonic.check_increasing(x,y)
sklearn.isotonic.isotonic_regression(y,*,sample_weight=None,y_min=None,y_max=None,increasing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/multioutput.py----------------------------------------
A:sklearn.multioutput.estimator->clone(estimator)
A:sklearn.multioutput.y->Parallel(n_jobs=self.n_jobs)((delayed(e.predict)(X) for e in self.estimators_))
A:sklearn.multioutput.routed_params->Bunch(estimator=Bunch(fit=fit_params))
A:sklearn.multioutput.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_estimator)(self.estimator, X, y[:, i], **routed_params.estimator.fit) for i in range(y.shape[1])))
A:sklearn.multioutput.fit_params_validated->_check_method_params(X, params=fit_params)
A:sklearn.multioutput.router->MetadataRouter(owner=self.__class__.__name__).add(estimator=self.base_estimator, method_mapping=MethodMapping().add(callee='fit', caller='fit'))
A:sklearn.multioutput.n_outputs_->len(self.estimators_)
A:sklearn.multioutput.y_pred->self.predict(X)
A:sklearn.multioutput.(X, Y)->self._validate_data(X, Y, multi_output=True, accept_sparse=True)
A:sklearn.multioutput.random_state->check_random_state(self.random_state)
A:sklearn.multioutput.self.order_->check_random_state(self.random_state).permutation(Y.shape[1])
A:sklearn.multioutput.X_aug->numpy.hstack((X, previous_predictions))
A:sklearn.multioutput.Y_pred_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.message->self._log_message(estimator_idx=chain_idx + 1, n_estimators=len(self.estimators_), processing_msg=f'Processing order {self.order_[chain_idx]}')
A:sklearn.multioutput.cv_result->cross_val_predict(self.base_estimator, X_aug[:, :col_idx], y=y, cv=self.cv)
A:sklearn.multioutput.X_aug[:, col_idx]->numpy.expand_dims(cv_result, 1)
A:sklearn.multioutput.X->self._validate_data(X, accept_sparse=True, reset=False)
A:sklearn.multioutput.Y_pred_chain[:, chain_idx]->clone(estimator).predict(X_aug)
A:sklearn.multioutput.inv_order->numpy.empty_like(self.order_)
A:sklearn.multioutput.inv_order[self.order_]->numpy.arange(len(self.order_))
A:sklearn.multioutput.Y_prob_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.Y_decision_chain->numpy.zeros((X.shape[0], len(self.estimators_)))
A:sklearn.multioutput.Y_decision_chain[:, chain_idx]->clone(estimator).decision_function(X_aug)
sklearn.multioutput.ClassifierChain(MetaEstimatorMixin,ClassifierMixin,_BaseChain)
sklearn.multioutput.ClassifierChain._more_tags(self)
sklearn.multioutput.ClassifierChain.decision_function(self,X)
sklearn.multioutput.ClassifierChain.fit(self,X,Y,**fit_params)
sklearn.multioutput.ClassifierChain.get_metadata_routing(self)
sklearn.multioutput.ClassifierChain.predict_log_proba(self,X)
sklearn.multioutput.ClassifierChain.predict_proba(self,X)
sklearn.multioutput.MultiOutputClassifier(self,estimator,*,n_jobs=None)
sklearn.multioutput.MultiOutputClassifier.__init__(self,estimator,*,n_jobs=None)
sklearn.multioutput.MultiOutputClassifier._check_predict_proba(self)
sklearn.multioutput.MultiOutputClassifier._more_tags(self)
sklearn.multioutput.MultiOutputClassifier.fit(self,X,Y,sample_weight=None,**fit_params)
sklearn.multioutput.MultiOutputClassifier.predict_proba(self,X)
sklearn.multioutput.MultiOutputClassifier.score(self,X,y)
sklearn.multioutput.MultiOutputRegressor(self,estimator,*,n_jobs=None)
sklearn.multioutput.MultiOutputRegressor.__init__(self,estimator,*,n_jobs=None)
sklearn.multioutput.MultiOutputRegressor.partial_fit(self,X,y,sample_weight=None,**partial_fit_params)
sklearn.multioutput.RegressorChain(MetaEstimatorMixin,RegressorMixin,_BaseChain)
sklearn.multioutput.RegressorChain._more_tags(self)
sklearn.multioutput.RegressorChain.fit(self,X,Y,**fit_params)
sklearn.multioutput.RegressorChain.get_metadata_routing(self)
sklearn.multioutput._BaseChain(self,base_estimator,*,order=None,cv=None,random_state=None,verbose=False)
sklearn.multioutput._BaseChain.__init__(self,base_estimator,*,order=None,cv=None,random_state=None,verbose=False)
sklearn.multioutput._BaseChain._log_message(self,*,estimator_idx,n_estimators,processing_msg)
sklearn.multioutput._BaseChain.fit(self,X,Y,**fit_params)
sklearn.multioutput._BaseChain.predict(self,X)
sklearn.multioutput._MultiOutputEstimator(self,estimator,*,n_jobs=None)
sklearn.multioutput._MultiOutputEstimator.__init__(self,estimator,*,n_jobs=None)
sklearn.multioutput._MultiOutputEstimator._more_tags(self)
sklearn.multioutput._MultiOutputEstimator.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.multioutput._MultiOutputEstimator.get_metadata_routing(self)
sklearn.multioutput._MultiOutputEstimator.partial_fit(self,X,y,classes=None,sample_weight=None,**partial_fit_params)
sklearn.multioutput._MultiOutputEstimator.predict(self,X)
sklearn.multioutput._available_if_base_estimator_has(attr)
sklearn.multioutput._available_if_estimator_has(attr)
sklearn.multioutput._fit_estimator(estimator,X,y,sample_weight=None,**fit_params)
sklearn.multioutput._partial_fit_estimator(estimator,X,y,classes=None,partial_fit_params=None,first_time=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/pipeline.py----------------------------------------
A:sklearn.pipeline.(names, estimators)->zip(*self.steps)
A:sklearn.pipeline.stop->len(self.steps)
A:sklearn.pipeline.routed_params->process_routing(self, 'score', sample_weight=sample_weight, **params)
A:sklearn.pipeline.fit_params_steps->Bunch(**{name: Bunch(**{method: {} for method in METHODS}) for (name, step) in self.steps if step is not None})
A:sklearn.pipeline.(step, param)->pname.split('__', 1)
A:sklearn.pipeline.self.steps->list(self.steps)
A:sklearn.pipeline.memory->check_memory(self.memory)
A:sklearn.pipeline.fit_transform_one_cached->check_memory(self.memory).cache(_fit_transform_one)
A:sklearn.pipeline.cloned_transformer->clone(transformer)
A:sklearn.pipeline.(X, fitted_transformer)->fit_transform_one_cached(cloned_transformer, X, y, None, message_clsname='Pipeline', message=self._log_message(step_idx), params=routed_params[name])
A:sklearn.pipeline.Xt->transform.transform(Xt, **routed_params[name].transform)
A:sklearn.pipeline.y_pred->self.steps[-1][1].fit_predict(Xt, y, **params_last_step.get('fit_predict', {}))
A:sklearn.pipeline.reverse_iter->reversed(list(self._iter()))
A:sklearn.pipeline.tags['pairwise']->_safe_tags(self.steps[0][1], 'pairwise')
A:sklearn.pipeline.tags['multioutput']->_safe_tags(self.steps[-1][1], 'multioutput')
A:sklearn.pipeline.feature_names_out->transform.get_feature_names_out(feature_names_out)
A:sklearn.pipeline.(_, estimators)->zip(*self.steps)
A:sklearn.pipeline.router->MetadataRouter(owner=self.__class__.__name__)
A:sklearn.pipeline.method_mapping->MethodMapping()
A:sklearn.pipeline.namecount->defaultdict(int)
A:sklearn.pipeline.res->transformer.fit(X, y, **params.get('fit', {})).transform(X, **params.get('transform', {}))
A:sklearn.pipeline.(names, transformers)->zip(*self.transformer_list)
A:sklearn.pipeline.transformer_names->set((name for (name, _) in self.transformer_list))
A:sklearn.pipeline.trans->FunctionTransformer(feature_names_out='one-to-one')
A:sklearn.pipeline.transformers->iter(transformers)
A:sklearn.pipeline.results->self._parallel_func(X, y, fit_params, _fit_transform_one)
A:sklearn.pipeline.(Xs, transformers)->zip(*results)
A:sklearn.pipeline.self.transformer_list->list(self.transformer_list)
A:sklearn.pipeline.params->Bunch(transform={})
A:sklearn.pipeline.Xs->numpy.hstack(Xs)
A:sklearn.pipeline.adapter->_get_container_adapter('transform', self)
sklearn.pipeline.FeatureUnion(self,transformer_list,*,n_jobs=None,transformer_weights=None,verbose=False)
sklearn.pipeline.FeatureUnion.__getitem__(self,name)
sklearn.pipeline.FeatureUnion.__init__(self,transformer_list,*,n_jobs=None,transformer_weights=None,verbose=False)
sklearn.pipeline.FeatureUnion.__sklearn_is_fitted__(self)
sklearn.pipeline.FeatureUnion._hstack(self,Xs)
sklearn.pipeline.FeatureUnion._iter(self)
sklearn.pipeline.FeatureUnion._log_message(self,name,idx,total)
sklearn.pipeline.FeatureUnion._parallel_func(self,X,y,fit_params,func)
sklearn.pipeline.FeatureUnion._sk_visual_block_(self)
sklearn.pipeline.FeatureUnion._update_transformer_list(self,transformers)
sklearn.pipeline.FeatureUnion._validate_transformer_weights(self)
sklearn.pipeline.FeatureUnion._validate_transformers(self)
sklearn.pipeline.FeatureUnion.feature_names_in_(self)
sklearn.pipeline.FeatureUnion.fit(self,X,y=None,**fit_params)
sklearn.pipeline.FeatureUnion.fit_transform(self,X,y=None,**fit_params)
sklearn.pipeline.FeatureUnion.get_feature_names_out(self,input_features=None)
sklearn.pipeline.FeatureUnion.get_params(self,deep=True)
sklearn.pipeline.FeatureUnion.n_features_in_(self)
sklearn.pipeline.FeatureUnion.named_transformers(self)
sklearn.pipeline.FeatureUnion.set_output(self,*,transform=None)
sklearn.pipeline.FeatureUnion.set_params(self,**kwargs)
sklearn.pipeline.FeatureUnion.transform(self,X)
sklearn.pipeline.Pipeline(self,steps,*,memory=None,verbose=False)
sklearn.pipeline.Pipeline.__getitem__(self,ind)
sklearn.pipeline.Pipeline.__init__(self,steps,*,memory=None,verbose=False)
sklearn.pipeline.Pipeline.__len__(self)
sklearn.pipeline.Pipeline.__sklearn_is_fitted__(self)
sklearn.pipeline.Pipeline._can_fit_transform(self)
sklearn.pipeline.Pipeline._can_inverse_transform(self)
sklearn.pipeline.Pipeline._can_transform(self)
sklearn.pipeline.Pipeline._check_method_params(self,method,props,**kwargs)
sklearn.pipeline.Pipeline._estimator_type(self)
sklearn.pipeline.Pipeline._final_estimator(self)
sklearn.pipeline.Pipeline._fit(self,X,y=None,routed_params=None)
sklearn.pipeline.Pipeline._iter(self,with_final=True,filter_passthrough=True)
sklearn.pipeline.Pipeline._log_message(self,step_idx)
sklearn.pipeline.Pipeline._more_tags(self)
sklearn.pipeline.Pipeline._sk_visual_block_(self)
sklearn.pipeline.Pipeline._validate_steps(self)
sklearn.pipeline.Pipeline.classes_(self)
sklearn.pipeline.Pipeline.decision_function(self,X,**params)
sklearn.pipeline.Pipeline.feature_names_in_(self)
sklearn.pipeline.Pipeline.fit(self,X,y=None,**params)
sklearn.pipeline.Pipeline.fit_predict(self,X,y=None,**params)
sklearn.pipeline.Pipeline.fit_transform(self,X,y=None,**params)
sklearn.pipeline.Pipeline.get_feature_names_out(self,input_features=None)
sklearn.pipeline.Pipeline.get_metadata_routing(self)
sklearn.pipeline.Pipeline.get_params(self,deep=True)
sklearn.pipeline.Pipeline.inverse_transform(self,Xt,**params)
sklearn.pipeline.Pipeline.n_features_in_(self)
sklearn.pipeline.Pipeline.named_steps(self)
sklearn.pipeline.Pipeline.predict(self,X,**params)
sklearn.pipeline.Pipeline.predict_log_proba(self,X,**params)
sklearn.pipeline.Pipeline.predict_proba(self,X,**params)
sklearn.pipeline.Pipeline.score(self,X,y=None,sample_weight=None,**params)
sklearn.pipeline.Pipeline.score_samples(self,X)
sklearn.pipeline.Pipeline.set_output(self,*,transform=None)
sklearn.pipeline.Pipeline.set_params(self,**kwargs)
sklearn.pipeline.Pipeline.transform(self,X,**params)
sklearn.pipeline._final_estimator_has(attr)
sklearn.pipeline._fit_one(transformer,X,y,weight,message_clsname='',message=None,params=None)
sklearn.pipeline._fit_transform_one(transformer,X,y,weight,message_clsname='',message=None,params=None)
sklearn.pipeline._name_estimators(estimators)
sklearn.pipeline._transform_one(transformer,X,y,weight,params)
sklearn.pipeline.make_pipeline(*steps,memory=None,verbose=False)
sklearn.pipeline.make_union(*transformers,n_jobs=None,verbose=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/multiclass.py----------------------------------------
A:sklearn.multiclass.unique_y->numpy.unique(y)
A:sklearn.multiclass.estimator->clone(estimator)
A:sklearn.multiclass.score->numpy.ravel(estimator.decision_function(X))
A:sklearn.multiclass.check_params->dict(force_all_finite=False, dtype=None, ensure_2d=False, accept_sparse=True)
A:sklearn.multiclass.y_->self.y_.astype(np.float64)
A:sklearn.multiclass.routed_params->process_routing(self, 'fit', **fit_params)
A:sklearn.multiclass.self.label_binarizer_->LabelBinarizer(sparse_output=True)
A:sklearn.multiclass.Y->numpy.array([self.code_book_[classes_index[y[i]]] for i in range(_num_samples(y))], dtype=int)
A:sklearn.multiclass.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_binary)(self.estimator, X, Y[:, i], fit_params=routed_params.estimator.fit) for i in range(Y.shape[1])))
A:sklearn.multiclass.n_samples->_num_samples(X)
A:sklearn.multiclass.maxima->numpy.empty(n_samples, dtype=float)
A:sklearn.multiclass.argmaxima->numpy.zeros(n_samples, dtype=int)
A:sklearn.multiclass.pred->pairwise_distances_argmin(Y, self.code_book_, metric='euclidean')
A:sklearn.multiclass.thresh->_threshold_for_binary_predict(self.estimators_[0])
A:sklearn.multiclass.indices->array.array('i')
A:sklearn.multiclass.indptr->array.array('i', [0])
A:sklearn.multiclass.data->numpy.ones(len(indices), dtype=int)
A:sklearn.multiclass.indicator->scipy.sparse.csc_matrix((data, indices, indptr), shape=(n_samples, len(self.estimators_)))
A:sklearn.multiclass.router->MetadataRouter(owner=self.__class__.__name__).add(estimator=self.estimator, method_mapping=MethodMapping().add(callee='fit', caller='fit'))
A:sklearn.multiclass.cond->numpy.logical_or(y == i, y == j)
A:sklearn.multiclass.y_binary->numpy.zeros_like(y)
A:sklearn.multiclass.fit_params_subset->_check_method_params(X, params=fit_params, indices=indcond)
A:sklearn.multiclass.partial_fit_params_subset->_check_method_params(X, params=partial_fit_params, indices=cond)
A:sklearn.multiclass.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], force_all_finite=False, reset=first_call)
A:sklearn.multiclass.self.classes_->numpy.unique(y)
A:sklearn.multiclass.estimators_indices->list(zip(*Parallel(n_jobs=self.n_jobs)((delayed(_fit_ovo_binary)(self.estimator, X, y, self.classes_[i], self.classes_[j], fit_params=routed_params.estimator.fit) for i in range(n_classes) for j in range(i + 1, n_classes)))))
A:sklearn.multiclass.first_call->_check_partial_fit_first_call(self, classes)
A:sklearn.multiclass.combinations->itertools.combinations(range(self.n_classes_), 2)
A:sklearn.multiclass.X->self._validate_data(X, accept_sparse=True, force_all_finite=False, reset=False)
A:sklearn.multiclass.y->self._validate_data(X='no_validation', y=y)
A:sklearn.multiclass.random_state->check_random_state(self.random_state)
A:sklearn.multiclass.n_estimators->int(n_classes * self.code_size)
A:sklearn.multiclass.self.code_book_->check_random_state(self.random_state).uniform(size=(n_classes, n_estimators))
sklearn.multiclass.OneVsOneClassifier(self,estimator,*,n_jobs=None)
sklearn.multiclass.OneVsOneClassifier.__init__(self,estimator,*,n_jobs=None)
sklearn.multiclass.OneVsOneClassifier._more_tags(self)
sklearn.multiclass.OneVsOneClassifier.decision_function(self,X)
sklearn.multiclass.OneVsOneClassifier.fit(self,X,y,**fit_params)
sklearn.multiclass.OneVsOneClassifier.get_metadata_routing(self)
sklearn.multiclass.OneVsOneClassifier.n_classes_(self)
sklearn.multiclass.OneVsOneClassifier.partial_fit(self,X,y,classes=None,**partial_fit_params)
sklearn.multiclass.OneVsOneClassifier.predict(self,X)
sklearn.multiclass.OneVsRestClassifier(self,estimator,*,n_jobs=None,verbose=0)
sklearn.multiclass.OneVsRestClassifier.__init__(self,estimator,*,n_jobs=None,verbose=0)
sklearn.multiclass.OneVsRestClassifier._more_tags(self)
sklearn.multiclass.OneVsRestClassifier.decision_function(self,X)
sklearn.multiclass.OneVsRestClassifier.fit(self,X,y,**fit_params)
sklearn.multiclass.OneVsRestClassifier.get_metadata_routing(self)
sklearn.multiclass.OneVsRestClassifier.multilabel_(self)
sklearn.multiclass.OneVsRestClassifier.n_classes_(self)
sklearn.multiclass.OneVsRestClassifier.partial_fit(self,X,y,classes=None,**partial_fit_params)
sklearn.multiclass.OneVsRestClassifier.predict(self,X)
sklearn.multiclass.OneVsRestClassifier.predict_proba(self,X)
sklearn.multiclass.OutputCodeClassifier(self,estimator,*,code_size=1.5,random_state=None,n_jobs=None)
sklearn.multiclass.OutputCodeClassifier.__init__(self,estimator,*,code_size=1.5,random_state=None,n_jobs=None)
sklearn.multiclass.OutputCodeClassifier.fit(self,X,y,**fit_params)
sklearn.multiclass.OutputCodeClassifier.get_metadata_routing(self)
sklearn.multiclass.OutputCodeClassifier.predict(self,X)
sklearn.multiclass._ConstantPredictor(BaseEstimator)
sklearn.multiclass._ConstantPredictor.decision_function(self,X)
sklearn.multiclass._ConstantPredictor.fit(self,X,y)
sklearn.multiclass._ConstantPredictor.predict(self,X)
sklearn.multiclass._ConstantPredictor.predict_proba(self,X)
sklearn.multiclass._estimators_has(attr)
sklearn.multiclass._fit_binary(estimator,X,y,fit_params,classes=None)
sklearn.multiclass._fit_ovo_binary(estimator,X,y,i,j,fit_params)
sklearn.multiclass._partial_fit_binary(estimator,X,y,partial_fit_params)
sklearn.multiclass._partial_fit_ovo_binary(estimator,X,y,i,j,partial_fit_params)
sklearn.multiclass._predict_binary(estimator,X)
sklearn.multiclass._threshold_for_binary_predict(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/dummy.py----------------------------------------
A:sklearn.dummy.y->numpy.ravel(y)
A:sklearn.dummy.self.sparse_output_->scipy.sparse.issparse(y)
A:sklearn.dummy.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.dummy.constant->numpy.reshape(np.atleast_1d(self.constant), (-1, 1))
A:sklearn.dummy.(self.classes_, self.n_classes_, self.class_prior_)->class_distribution(y, sample_weight)
A:sklearn.dummy.err_msg->'The constant target value must be present in the training data. You provided constant={}. Possible values are: {}.'.format(self.constant, self.classes_[k].tolist())
A:sklearn.dummy.n_samples->_num_samples(X)
A:sklearn.dummy.rs->check_random_state(self.random_state)
A:sklearn.dummy.proba->self.predict_proba(X)
A:sklearn.dummy.ind->numpy.where(classes_[k] == constant[k])
A:sklearn.dummy.out->numpy.zeros((n_samples, n_classes_[k]), dtype=np.float64)
A:sklearn.dummy.X->numpy.zeros(shape=(len(y), 1))
A:sklearn.dummy.self.constant_->numpy.reshape(self.constant_, (1, -1))
A:sklearn.dummy.y_std->numpy.ravel(y_std)
sklearn.dummy.DummyClassifier(self,*,strategy='prior',random_state=None,constant=None)
sklearn.dummy.DummyClassifier.__init__(self,*,strategy='prior',random_state=None,constant=None)
sklearn.dummy.DummyClassifier._more_tags(self)
sklearn.dummy.DummyClassifier.fit(self,X,y,sample_weight=None)
sklearn.dummy.DummyClassifier.predict(self,X)
sklearn.dummy.DummyClassifier.predict_log_proba(self,X)
sklearn.dummy.DummyClassifier.predict_proba(self,X)
sklearn.dummy.DummyClassifier.score(self,X,y,sample_weight=None)
sklearn.dummy.DummyRegressor(self,*,strategy='mean',constant=None,quantile=None)
sklearn.dummy.DummyRegressor.__init__(self,*,strategy='mean',constant=None,quantile=None)
sklearn.dummy.DummyRegressor._more_tags(self)
sklearn.dummy.DummyRegressor.fit(self,X,y,sample_weight=None)
sklearn.dummy.DummyRegressor.predict(self,X,return_std=False)
sklearn.dummy.DummyRegressor.score(self,X,y,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/calibration.py----------------------------------------
A:sklearn.calibration.estimator->self._get_estimator()
A:sklearn.calibration.(X, y)->indexable(X, y)
A:sklearn.calibration.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.calibration.(predictions, _)->_get_response_values(self.estimator, X, response_method=['decision_function', 'predict_proba'])
A:sklearn.calibration.predictions->column_or_1d(predictions)
A:sklearn.calibration.calibrated_classifier->_fit_calibrator(estimator, predictions, y_test, classes, method, sample_weight=sw_test)
A:sklearn.calibration.label_encoder_->LabelEncoder().fit(y)
A:sklearn.calibration.routed_params->Bunch()
A:sklearn.calibration.routed_params.splitter->Bunch(split={})
A:sklearn.calibration.routed_params.estimator->Bunch(fit=fit_params)
A:sklearn.calibration.cv->check_cv(self.cv, y, classifier=True)
A:sklearn.calibration.parallel->Parallel(n_jobs=self.n_jobs)
A:sklearn.calibration.self.calibrated_classifiers_->parallel((delayed(_fit_classifier_calibrator_pair)(clone(estimator), X, y, train=train, test=test, method=self.method, classes=self.classes_, sample_weight=sample_weight, fit_params=routed_params.estimator.fit) for (train, test) in cv.split(X, y, **routed_params.splitter.split)))
A:sklearn.calibration.this_estimator->clone(estimator)
A:sklearn.calibration.mean_proba->numpy.zeros((_num_samples(X), len(self.classes_)))
A:sklearn.calibration.proba->numpy.divide(proba, denominator, out=uniform_proba, where=denominator != 0)
A:sklearn.calibration.router->MetadataRouter(owner=self.__class__.__name__).add_self_request(self).add(estimator=self._get_estimator(), method_mapping=MethodMapping().add(callee='fit', caller='fit')).add(splitter=self.cv, method_mapping=MethodMapping().add(callee='split', caller='fit'))
A:sklearn.calibration.fit_params_train->_check_method_params(X, params=fit_params, indices=train)
A:sklearn.calibration.Y->label_binarize(y, classes=classes)
A:sklearn.calibration.label_encoder->LabelEncoder().fit(self.classes)
A:sklearn.calibration.pos_class_indices->LabelEncoder().fit(self.classes).transform(self.estimator.classes_)
A:sklearn.calibration.calibrator->_SigmoidCalibration()
A:sklearn.calibration.pipeline->_CalibratedClassifier(clf, calibrators, method=method, classes=classes)
A:sklearn.calibration.n_classes->len(self.classes)
A:sklearn.calibration.proba[:, class_idx]->_SigmoidCalibration().predict(this_pred)
A:sklearn.calibration.uniform_proba->numpy.full_like(proba, 1 / n_classes)
A:sklearn.calibration.y->column_or_1d(y)
A:sklearn.calibration.max_prediction->numpy.max(np.abs(F))
A:sklearn.calibration.prior0->float(np.sum(mask_negative_samples))
A:sklearn.calibration.prior1->sample_weight[~mask_negative_samples].sum()
A:sklearn.calibration.T->column_or_1d(T)
A:sklearn.calibration.bin_loss->HalfBinomialLoss()
A:sklearn.calibration.(l, g)->HalfBinomialLoss().loss_gradient(y_true=T, raw_prediction=-(AB[0] * F + AB[1]), sample_weight=sample_weight)
A:sklearn.calibration.loss->l.sum()
A:sklearn.calibration.grad->numpy.array([-g @ F, -g.sum()])
A:sklearn.calibration.AB0->numpy.array([0.0, log((prior0 + 1.0) / (prior1 + 1.0))])
A:sklearn.calibration.opt_result->minimize(loss_grad, AB0, method='L-BFGS-B', jac=True, options={'gtol': 1e-06, 'ftol': 64 * np.finfo(float).eps})
A:sklearn.calibration.X->column_or_1d(X)
A:sklearn.calibration.(self.a_, self.b_)->_sigmoid_calibration(X, y, sample_weight)
A:sklearn.calibration.y_true->column_or_1d(y_true)
A:sklearn.calibration.y_prob->column_or_1d(y_prob)
A:sklearn.calibration.pos_label->_check_pos_label_consistency(pos_label, y_true)
A:sklearn.calibration.labels->numpy.unique(y_true)
A:sklearn.calibration.quantiles->numpy.linspace(0, 1, n_bins + 1)
A:sklearn.calibration.bins->numpy.linspace(0.0, 1.0, n_bins + 1)
A:sklearn.calibration.binids->numpy.searchsorted(bins[1:-1], y_prob)
A:sklearn.calibration.bin_sums->numpy.bincount(binids, weights=y_prob, minlength=len(bins))
A:sklearn.calibration.bin_true->numpy.bincount(binids, weights=y_true, minlength=len(bins))
A:sklearn.calibration.bin_total->numpy.bincount(binids, minlength=len(bins))
A:sklearn.calibration.(self.ax_, self.figure_, name)->self._validate_plot_params(ax=ax, name=name)
A:sklearn.calibration.(y_prob, pos_label, name)->cls._validate_and_get_response_values(estimator, X, y, response_method='predict_proba', pos_label=pos_label, name=name)
A:sklearn.calibration.(pos_label_validated, name)->cls._validate_from_predictions_params(y_true, y_prob, sample_weight=None, pos_label=pos_label, name=name)
A:sklearn.calibration.(prob_true, prob_pred)->calibration_curve(y_true, y_prob, n_bins=n_bins, strategy=strategy, pos_label=pos_label)
A:sklearn.calibration.disp->cls(prob_true=prob_true, prob_pred=prob_pred, y_prob=y_prob, estimator_name=name, pos_label=pos_label_validated)
sklearn.calibration.CalibratedClassifierCV(self,estimator=None,*,method='sigmoid',cv=None,n_jobs=None,ensemble=True)
sklearn.calibration.CalibratedClassifierCV.__init__(self,estimator=None,*,method='sigmoid',cv=None,n_jobs=None,ensemble=True)
sklearn.calibration.CalibratedClassifierCV._get_estimator(self)
sklearn.calibration.CalibratedClassifierCV._more_tags(self)
sklearn.calibration.CalibratedClassifierCV.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.calibration.CalibratedClassifierCV.get_metadata_routing(self)
sklearn.calibration.CalibratedClassifierCV.predict(self,X)
sklearn.calibration.CalibratedClassifierCV.predict_proba(self,X)
sklearn.calibration.CalibrationDisplay(self,prob_true,prob_pred,y_prob,*,estimator_name=None,pos_label=None)
sklearn.calibration.CalibrationDisplay.__init__(self,prob_true,prob_pred,y_prob,*,estimator_name=None,pos_label=None)
sklearn.calibration.CalibrationDisplay.from_estimator(cls,estimator,X,y,*,n_bins=5,strategy='uniform',pos_label=None,name=None,ref_line=True,ax=None,**kwargs)
sklearn.calibration.CalibrationDisplay.from_predictions(cls,y_true,y_prob,*,n_bins=5,strategy='uniform',pos_label=None,name=None,ref_line=True,ax=None,**kwargs)
sklearn.calibration.CalibrationDisplay.plot(self,*,ax=None,name=None,ref_line=True,**kwargs)
sklearn.calibration._CalibratedClassifier(self,estimator,calibrators,*,classes,method='sigmoid')
sklearn.calibration._CalibratedClassifier.__init__(self,estimator,calibrators,*,classes,method='sigmoid')
sklearn.calibration._CalibratedClassifier.predict_proba(self,X)
sklearn.calibration._SigmoidCalibration(RegressorMixin,BaseEstimator)
sklearn.calibration._SigmoidCalibration.fit(self,X,y,sample_weight=None)
sklearn.calibration._SigmoidCalibration.predict(self,T)
sklearn.calibration._fit_calibrator(clf,predictions,y,classes,method,sample_weight=None)
sklearn.calibration._fit_classifier_calibrator_pair(estimator,X,y,train,test,method,classes,sample_weight=None,fit_params=None)
sklearn.calibration._sigmoid_calibration(predictions,y,sample_weight=None,max_abs_prediction_threshold=30)
sklearn.calibration.calibration_curve(y_true,y_prob,*,pos_label=None,n_bins=5,strategy='uniform')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/base.py----------------------------------------
A:sklearn.base.estimator_type->type(estimator)
A:sklearn.base.new_object_params->estimator.get_params(deep=False)
A:sklearn.base.new_object_params[name]->clone(param, safe=False)
A:sklearn.base.new_object->klass(**new_object_params)
A:sklearn.base.new_object._metadata_request->copy.deepcopy(estimator._metadata_request)
A:sklearn.base.params_set->klass(**new_object_params).get_params(deep=False)
A:sklearn.base.new_object._sklearn_output_config->copy.deepcopy(estimator._sklearn_output_config)
A:sklearn.base.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.base.init_signature->inspect.signature(init)
A:sklearn.base.out->_check_y(y, **check_params)
A:sklearn.base.value->getattr(self, key)
A:sklearn.base.deep_items->getattr(self, key).get_params().items()
A:sklearn.base.valid_params->self.get_params(deep=True)
A:sklearn.base.nested_params->defaultdict(dict)
A:sklearn.base.(key, delim, sub_key)->key.partition('__')
A:sklearn.base.local_valid_params->self._get_param_names()
A:sklearn.base.pp->_EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)
A:sklearn.base.repr_->_EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW).pformat(self)
A:sklearn.base.n_nonblank->len(''.join(repr_.split()))
A:sklearn.base.left_lim->re.match(regex, repr_).end()
A:sklearn.base.right_lim->re.match(regex, repr_[::-1]).end()
A:sklearn.base.state->self.__dict__.copy()
A:sklearn.base.pickle_version->self.__dict__.copy().pop('_sklearn_version', 'pre-0.18')
A:sklearn.base.more_tags->base_class._more_tags(self)
A:sklearn.base.n_features->_num_features(X)
A:sklearn.base.feature_names_in->_get_feature_names(X)
A:sklearn.base.fitted_feature_names->getattr(self, 'feature_names_in_', None)
A:sklearn.base.X_feature_names->_get_feature_names(X)
A:sklearn.base.fitted_feature_names_set->set(fitted_feature_names)
A:sklearn.base.X_feature_names_set->set(X_feature_names)
A:sklearn.base.unexpected_names->sorted(X_feature_names_set - fitted_feature_names_set)
A:sklearn.base.missing_names->sorted(fitted_feature_names_set - X_feature_names_set)
A:sklearn.base.X->check_array(X, input_name='X', **check_X_params)
A:sklearn.base.y->check_array(y, input_name='y', **check_y_params)
A:sklearn.base.(X, y)->check_X_y(X, y, **check_params)
A:sklearn.base.output['text/html']->estimator_html_repr(self)
A:sklearn.base.y_pred->self.predict(X)
A:sklearn.base.indices->self.get_indices(i)
A:sklearn.base.data->check_array(data, accept_sparse='csr')
A:sklearn.base.(row_ind, col_ind)->self.get_indices(i)
A:sklearn.base.transform_params->self.get_metadata_routing().consumes(method='predict', params=kwargs.keys())
sklearn.base.BaseEstimator(_HTMLDocumentationLinkMixin,_MetadataRequester)
sklearn.base.BaseEstimator.__getstate__(self)
sklearn.base.BaseEstimator.__repr__(self,N_CHAR_MAX=700)
sklearn.base.BaseEstimator.__setstate__(self,state)
sklearn.base.BaseEstimator.__sklearn_clone__(self)
sklearn.base.BaseEstimator._check_feature_names(self,X,*,reset)
sklearn.base.BaseEstimator._check_n_features(self,X,reset)
sklearn.base.BaseEstimator._get_param_names(cls)
sklearn.base.BaseEstimator._get_tags(self)
sklearn.base.BaseEstimator._more_tags(self)
sklearn.base.BaseEstimator._repr_html_(self)
sklearn.base.BaseEstimator._repr_html_inner(self)
sklearn.base.BaseEstimator._repr_mimebundle_(self,**kwargs)
sklearn.base.BaseEstimator._validate_data(self,X='no_validation',y='no_validation',reset=True,validate_separately=False,cast_to_ndarray=True,**check_params)
sklearn.base.BaseEstimator._validate_params(self)
sklearn.base.BaseEstimator.get_params(self,deep=True)
sklearn.base.BaseEstimator.set_params(self,**params)
sklearn.base.BiclusterMixin
sklearn.base.BiclusterMixin.biclusters_(self)
sklearn.base.BiclusterMixin.get_indices(self,i)
sklearn.base.BiclusterMixin.get_shape(self,i)
sklearn.base.BiclusterMixin.get_submatrix(self,i,data)
sklearn.base.ClassNamePrefixFeaturesOutMixin
sklearn.base.ClassNamePrefixFeaturesOutMixin.get_feature_names_out(self,input_features=None)
sklearn.base.ClassifierMixin
sklearn.base.ClassifierMixin._more_tags(self)
sklearn.base.ClassifierMixin.score(self,X,y,sample_weight=None)
sklearn.base.ClusterMixin
sklearn.base.ClusterMixin._more_tags(self)
sklearn.base.ClusterMixin.fit_predict(self,X,y=None,**kwargs)
sklearn.base.DensityMixin
sklearn.base.DensityMixin.score(self,X,y=None)
sklearn.base.MetaEstimatorMixin
sklearn.base.MultiOutputMixin
sklearn.base.MultiOutputMixin._more_tags(self)
sklearn.base.OneToOneFeatureMixin
sklearn.base.OneToOneFeatureMixin.get_feature_names_out(self,input_features=None)
sklearn.base.OutlierMixin
sklearn.base.OutlierMixin.fit_predict(self,X,y=None,**kwargs)
sklearn.base.RegressorMixin
sklearn.base.RegressorMixin._more_tags(self)
sklearn.base.RegressorMixin.score(self,X,y,sample_weight=None)
sklearn.base.TransformerMixin(_SetOutputMixin)
sklearn.base.TransformerMixin.fit_transform(self,X,y=None,**fit_params)
sklearn.base._UnstableArchMixin
sklearn.base._UnstableArchMixin._more_tags(self)
sklearn.base._clone_parametrized(estimator,*,safe=True)
sklearn.base._fit_context(*,prefer_skip_nested_validation)
sklearn.base.clone(estimator,*,safe=True)
sklearn.base.is_classifier(estimator)
sklearn.base.is_outlier_detector(estimator)
sklearn.base.is_regressor(estimator)
sklearn.clone(estimator,*,safe=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/conftest.py----------------------------------------
A:sklearn.conftest._SKIP32_MARK->pytest.mark.skipif(environ.get('SKLEARN_RUN_FLOAT32_TESTS', '0') != '1', reason='Set SKLEARN_RUN_FLOAT32_TESTS=1 to run float32 dtype tests')
A:sklearn.conftest.fetch_20newsgroups_fxt->_fetch_fixture(fetch_20newsgroups)
A:sklearn.conftest.fetch_20newsgroups_vectorized_fxt->_fetch_fixture(fetch_20newsgroups_vectorized)
A:sklearn.conftest.fetch_california_housing_fxt->_fetch_fixture(fetch_california_housing)
A:sklearn.conftest.fetch_covtype_fxt->_fetch_fixture(fetch_covtype)
A:sklearn.conftest.fetch_kddcup99_fxt->_fetch_fixture(fetch_kddcup99)
A:sklearn.conftest.fetch_olivetti_faces_fxt->_fetch_fixture(fetch_olivetti_faces)
A:sklearn.conftest.fetch_rcv1_fxt->_fetch_fixture(fetch_rcv1)
A:sklearn.conftest.raccoon_face_fxt->pytest.fixture(raccoon_face_or_skip)
A:sklearn.conftest.skip_network->pytest.mark.skip(reason='test is enabled when SKLEARN_SKIP_NETWORK_TESTS=0')
A:sklearn.conftest.dataset_features_set->set(dataset_fetchers)
A:sklearn.conftest.datasets_to_download->set()
A:sklearn.conftest.item_fixtures->set(item.fixturenames)
A:sklearn.conftest.worker_id->os.environ.get('PYTEST_XDIST_WORKER', 'gw0')
A:sklearn.conftest.marker->pytest.mark.xfail(reason='know failure. See https://github.com/scikit-learn/scikit-learn/issues/17797')
A:sklearn.conftest.skip_marker->pytest.mark.skip(reason='pillow (or PIL) not installed!')
A:sklearn.conftest.pyplot->pytest.importorskip('matplotlib.pyplot')
A:sklearn.conftest.allowed_parallelism->max(allowed_parallelism // int(xdist_worker_count), 1)
A:sklearn.conftest.xdist_worker_count->os.environ.get('PYTEST_XDIST_WORKER_COUNT')
sklearn.conftest._fetch_fixture(f)
sklearn.conftest.enable_slep006()
sklearn.conftest.global_dtype(request)
sklearn.conftest.hide_available_pandas(monkeypatch)
sklearn.conftest.print_changed_only_false()
sklearn.conftest.pyplot()
sklearn.conftest.pytest_collection_modifyitems(config,items)
sklearn.conftest.pytest_configure(config)
sklearn.conftest.raccoon_face_or_skip()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/random_projection.py----------------------------------------
A:sklearn.random_projection.eps->numpy.asarray(eps)
A:sklearn.random_projection.n_samples->numpy.asarray(n_samples)
A:sklearn.random_projection.rng->check_random_state(random_state)
A:sklearn.random_projection.components->components.toarray().toarray()
A:sklearn.random_projection.density->_check_density(density, n_features)
A:sklearn.random_projection.n_nonzero_i->check_random_state(random_state).binomial(n_features, density)
A:sklearn.random_projection.indices_i->sample_without_replacement(n_features, n_nonzero_i, random_state=rng)
A:sklearn.random_projection.indices->numpy.concatenate(indices)
A:sklearn.random_projection.X->self._validate_data(X, accept_sparse=['csr', 'csc'], reset=False, dtype=[np.float64, np.float32])
A:sklearn.random_projection.self.n_components_->johnson_lindenstrauss_min_dim(n_samples=n_samples, eps=self.eps)
A:sklearn.random_projection.self.components_->self._make_random_matrix(self.n_components_, n_features).astype(X.dtype, copy=False)
A:sklearn.random_projection.self.inverse_components_->self._compute_inverse_components()
A:sklearn.random_projection.inverse_components->self._compute_inverse_components()
A:sklearn.random_projection.random_state->check_random_state(self.random_state)
A:sklearn.random_projection.self.density_->_check_density(self.density, n_features)
sklearn.random_projection.BaseRandomProjection(self,n_components='auto',*,eps=0.1,compute_inverse_components=False,random_state=None)
sklearn.random_projection.BaseRandomProjection.__init__(self,n_components='auto',*,eps=0.1,compute_inverse_components=False,random_state=None)
sklearn.random_projection.BaseRandomProjection._compute_inverse_components(self)
sklearn.random_projection.BaseRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection.BaseRandomProjection._more_tags(self)
sklearn.random_projection.BaseRandomProjection.fit(self,X,y=None)
sklearn.random_projection.BaseRandomProjection.inverse_transform(self,X)
sklearn.random_projection.GaussianRandomProjection(self,n_components='auto',*,eps=0.1,compute_inverse_components=False,random_state=None)
sklearn.random_projection.GaussianRandomProjection.__init__(self,n_components='auto',*,eps=0.1,compute_inverse_components=False,random_state=None)
sklearn.random_projection.GaussianRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection.GaussianRandomProjection.transform(self,X)
sklearn.random_projection.SparseRandomProjection(self,n_components='auto',*,density='auto',eps=0.1,dense_output=False,compute_inverse_components=False,random_state=None)
sklearn.random_projection.SparseRandomProjection.__init__(self,n_components='auto',*,density='auto',eps=0.1,dense_output=False,compute_inverse_components=False,random_state=None)
sklearn.random_projection.SparseRandomProjection._make_random_matrix(self,n_components,n_features)
sklearn.random_projection.SparseRandomProjection.transform(self,X)
sklearn.random_projection._check_density(density,n_features)
sklearn.random_projection._check_input_size(n_components,n_features)
sklearn.random_projection._gaussian_random_matrix(n_components,n_features,random_state=None)
sklearn.random_projection._sparse_random_matrix(n_components,n_features,density='auto',random_state=None)
sklearn.random_projection.johnson_lindenstrauss_min_dim(n_samples,*,eps=0.1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_pd_utils.py----------------------------------------
A:sklearn.inspection._pd_utils.feature_names->feature_names.tolist().tolist()
sklearn.inspection._pd_utils._check_feature_names(X,feature_names=None)
sklearn.inspection._pd_utils._get_feature_index(fx,feature_names=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_partial_dependence.py----------------------------------------
A:sklearn.inspection._partial_dependence.uniques->numpy.unique(_safe_indexing(X, feature, axis=1))
A:sklearn.inspection._partial_dependence.emp_percentiles->mquantiles(_safe_indexing(X, feature, axis=1), prob=percentiles, axis=0)
A:sklearn.inspection._partial_dependence.axis->numpy.linspace(emp_percentiles[0], emp_percentiles[1], num=grid_resolution, endpoint=True)
A:sklearn.inspection._partial_dependence.averaged_predictions->averaged_predictions.reshape(-1, *[val.shape[0] for val in values]).reshape(-1, *[val.shape[0] for val in values])
A:sklearn.inspection._partial_dependence.predict_proba->getattr(est, 'predict_proba', None)
A:sklearn.inspection._partial_dependence.decision_function->getattr(est, 'decision_function', None)
A:sklearn.inspection._partial_dependence.X_eval->check_array(X, force_all_finite='allow-nan', dtype=object).copy()
A:sklearn.inspection._partial_dependence.pred->prediction_method(X_eval)
A:sklearn.inspection._partial_dependence.predictions->predictions.reshape(-1, X.shape[0], *[val.shape[0] for val in values]).reshape(-1, X.shape[0], *[val.shape[0] for val in values])
A:sklearn.inspection._partial_dependence.X->check_array(X, force_all_finite='allow-nan', dtype=object)
A:sklearn.inspection._partial_dependence.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.inspection._partial_dependence.features_indices->numpy.asarray(_get_column_indices(X, features), dtype=np.int32, order='C').ravel()
A:sklearn.inspection._partial_dependence.feature_names->_check_feature_names(X, feature_names)
A:sklearn.inspection._partial_dependence.categorical_features->numpy.array(categorical_features, copy=False)
A:sklearn.inspection._partial_dependence.(grid, values)->_grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
A:sklearn.inspection._partial_dependence.(averaged_predictions, predictions)->_partial_dependence_brute(estimator, grid, features_indices, X, response_method, sample_weight)
A:sklearn.inspection._partial_dependence.pdp_results->Bunch()
sklearn.inspection._partial_dependence._grid_from_X(X,percentiles,is_categorical,grid_resolution)
sklearn.inspection._partial_dependence._partial_dependence_brute(est,grid,features,X,response_method,sample_weight=None)
sklearn.inspection._partial_dependence._partial_dependence_recursion(est,grid,features)
sklearn.inspection._partial_dependence.partial_dependence(estimator,X,features,*,sample_weight=None,categorical_features=None,feature_names=None,response_method='auto',percentiles=(0.05,0.95),grid_resolution=100,method='auto',kind='average')
sklearn.inspection.partial_dependence(estimator,X,features,*,sample_weight=None,categorical_features=None,feature_names=None,response_method='auto',percentiles=(0.05,0.95),grid_resolution=100,method='auto',kind='average')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_permutation_importance.py----------------------------------------
A:sklearn.inspection._permutation_importance.random_state->check_random_state(random_state)
A:sklearn.inspection._permutation_importance.row_indices->_generate_indices(random_state=random_state, bootstrap=False, n_population=X.shape[0], n_samples=max_samples)
A:sklearn.inspection._permutation_importance.X_permuted->check_array(X, force_all_finite='allow-nan', dtype=None).copy()
A:sklearn.inspection._permutation_importance.y->_safe_indexing(y, row_indices, axis=0)
A:sklearn.inspection._permutation_importance.shuffling_idx->numpy.arange(X_permuted.shape[0])
A:sklearn.inspection._permutation_importance.scores->Parallel(n_jobs=n_jobs)((delayed(_calculate_permutation_scores)(estimator, X, y, sample_weight, col_idx, random_seed, n_repeats, scorer, max_samples) for col_idx in range(X.shape[1])))
A:sklearn.inspection._permutation_importance.X->check_array(X, force_all_finite='allow-nan', dtype=None)
A:sklearn.inspection._permutation_importance.random_seed->check_random_state(random_state).randint(np.iinfo(np.int32).max + 1)
A:sklearn.inspection._permutation_importance.max_samples->int(max_samples * X.shape[0])
A:sklearn.inspection._permutation_importance.scorer->_MultimetricScorer(scorers=scorers_dict)
A:sklearn.inspection._permutation_importance.scorers_dict->_check_multimetric_scoring(estimator, scoring)
A:sklearn.inspection._permutation_importance.baseline_score->_weights_scorer(scorer, estimator, X, y, sample_weight)
sklearn.inspection._permutation_importance._calculate_permutation_scores(estimator,X,y,sample_weight,col_idx,random_state,n_repeats,scorer,max_samples)
sklearn.inspection._permutation_importance._create_importances_bunch(baseline_score,permuted_score)
sklearn.inspection._permutation_importance._weights_scorer(scorer,estimator,X,y,sample_weight)
sklearn.inspection._permutation_importance.permutation_importance(estimator,X,y,*,scoring=None,n_repeats=5,n_jobs=None,random_state=None,sample_weight=None,max_samples=1.0)
sklearn.inspection.permutation_importance(estimator,X,y,*,scoring=None,n_repeats=5,n_jobs=None,random_state=None,sample_weight=None,max_samples=1.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/partial_dependence.py----------------------------------------
A:sklearn.inspection._plot.partial_dependence.target_idx->numpy.searchsorted(estimator.classes_, target)
A:sklearn.inspection._plot.partial_dependence.X->check_array(X, force_all_finite='allow-nan', dtype=object)
A:sklearn.inspection._plot.partial_dependence.feature_names->_check_feature_names(X, feature_names)
A:sklearn.inspection._plot.partial_dependence.fxs->tuple((_get_feature_index(fx, feature_names=feature_names) for fx in fxs))
A:sklearn.inspection._plot.partial_dependence.categorical_features->numpy.array(categorical_features, copy=False)
A:sklearn.inspection._plot.partial_dependence.categorical_features_targeted->set([fx for (fxs, cats) in zip(features, is_categorical) for fx in fxs if any(cats)])
A:sklearn.inspection._plot.partial_dependence.min_n_cats->min([len(_unique(_safe_indexing(X, idx, axis=1))) for idx in categorical_features_targeted])
A:sklearn.inspection._plot.partial_dependence.axes->numpy.asarray(ax, dtype=object)
A:sklearn.inspection._plot.partial_dependence.pd_results->Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(partial_dependence)(estimator, X, fxs, sample_weight=sample_weight, feature_names=feature_names, categorical_features=categorical_features, response_method=response_method, method=method, grid_resolution=grid_resolution, percentiles=percentiles, kind=kind_plot) for (kind_plot, fxs) in zip(kind_, features)))
A:sklearn.inspection._plot.partial_dependence.X_col->_safe_indexing(X, fx, axis=1)
A:sklearn.inspection._plot.partial_dependence.deciles[fx]->mquantiles(X_col, prob=np.arange(0.1, 1.0, 0.1))
A:sklearn.inspection._plot.partial_dependence.display->cls(pd_results=pd_results, features=features, feature_names=feature_names, target_idx=target_idx, deciles=deciles, kind=kind, subsample=subsample, random_state=random_state, is_categorical=is_categorical)
A:sklearn.inspection._plot.partial_dependence.rng->check_random_state(self.random_state)
A:sklearn.inspection._plot.partial_dependence.ice_lines_idx->check_random_state(self.random_state).choice(preds.shape[0], n_ice_to_plot, replace=False)
A:sklearn.inspection._plot.partial_dependence.line_idx->numpy.unravel_index(pd_line_idx, self.lines_.shape)
A:sklearn.inspection._plot.partial_dependence.bar_idx->numpy.unravel_index(pd_line_idx, self.bars_.shape)
A:sklearn.inspection._plot.partial_dependence.trans->matplotlib.transforms.blended_transform_factory(ax.transData, ax.transAxes)
A:sklearn.inspection._plot.partial_dependence.vlines_idx->numpy.unravel_index(pd_plot_idx, self.deciles_vlines_.shape)
A:sklearn.inspection._plot.partial_dependence.self.deciles_vlines_[vlines_idx]->numpy.asarray(ax, dtype=object).vlines(self.deciles[feature_idx[0]], 0, 0.05, transform=trans, color='k')
A:sklearn.inspection._plot.partial_dependence.min_val->min((val[0] for val in pdp_lim.values()))
A:sklearn.inspection._plot.partial_dependence.max_val->max((val[1] for val in pdp_lim.values()))
A:sklearn.inspection._plot.partial_dependence.default_im_kw->dict(interpolation='nearest', cmap='viridis')
A:sklearn.inspection._plot.partial_dependence.im->numpy.asarray(ax, dtype=object).imshow(data, **im_kw)
A:sklearn.inspection._plot.partial_dependence.text->numpy.empty_like(data, dtype=object)
A:sklearn.inspection._plot.partial_dependence.(row, col)->numpy.unravel_index(flat_index, data.shape)
A:sklearn.inspection._plot.partial_dependence.text_data->format(data[row, col], values_format)
A:sklearn.inspection._plot.partial_dependence.text_kwargs->dict(ha='center', va='center', color=color)
A:sklearn.inspection._plot.partial_dependence.text[row, col]->numpy.asarray(ax, dtype=object).text(col, row, text_data, **text_kwargs)
A:sklearn.inspection._plot.partial_dependence.heatmap_idx->numpy.unravel_index(pd_plot_idx, self.heatmaps_.shape)
A:sklearn.inspection._plot.partial_dependence.(XX, YY)->numpy.meshgrid(feature_values[0], feature_values[1])
A:sklearn.inspection._plot.partial_dependence.CS->numpy.asarray(ax, dtype=object).contour(XX, YY, Z, levels=Z_level, linewidths=0.5, colors='k')
A:sklearn.inspection._plot.partial_dependence.contour_idx->numpy.unravel_index(pd_plot_idx, self.contours_.shape)
A:sklearn.inspection._plot.partial_dependence.self.contours_[contour_idx]->numpy.asarray(ax, dtype=object).contourf(XX, YY, Z, levels=Z_level, vmax=Z_level[-1], vmin=Z_level[0], **contour_kw)
A:sklearn.inspection._plot.partial_dependence.hlines_idx->numpy.unravel_index(pd_plot_idx, self.deciles_hlines_.shape)
A:sklearn.inspection._plot.partial_dependence.self.deciles_hlines_[hlines_idx]->numpy.asarray(ax, dtype=object).hlines(self.deciles[feature_idx[1]], 0, 0.05, transform=trans, color='k')
A:sklearn.inspection._plot.partial_dependence.min_pd->min(min_pd, old_min_pd)
A:sklearn.inspection._plot.partial_dependence.max_pd->max(max_pd, old_max_pd)
A:sklearn.inspection._plot.partial_dependence.n_fx->len(values)
A:sklearn.inspection._plot.partial_dependence.(old_min_pd, old_max_pd)->pdp_lim.get(n_fx, (min_pd, max_pd))
A:sklearn.inspection._plot.partial_dependence.(_, ax)->matplotlib.pyplot.subplots()
A:sklearn.inspection._plot.partial_dependence.n_features->len(self.features)
A:sklearn.inspection._plot.partial_dependence.ice_plot_idx->is_average_plot.index(False)
A:sklearn.inspection._plot.partial_dependence.n_ice_lines->self._get_sample_count(len(pd_results_[ice_plot_idx].individual[0]))
A:sklearn.inspection._plot.partial_dependence.n_cols->min(n_cols, n_features)
A:sklearn.inspection._plot.partial_dependence.n_rows->int(np.ceil(n_features / float(n_cols)))
A:sklearn.inspection._plot.partial_dependence.self.axes_->numpy.empty((n_rows, n_cols), dtype=object)
A:sklearn.inspection._plot.partial_dependence.self.lines_->numpy.empty(ax.shape + (n_lines,), dtype=object)
A:sklearn.inspection._plot.partial_dependence.self.contours_->numpy.empty_like(ax, dtype=object)
A:sklearn.inspection._plot.partial_dependence.self.bars_->numpy.empty_like(ax, dtype=object)
A:sklearn.inspection._plot.partial_dependence.self.heatmaps_->numpy.empty_like(ax, dtype=object)
A:sklearn.inspection._plot.partial_dependence.axes_ravel->self.axes_.ravel()
A:sklearn.inspection._plot.partial_dependence.gs->GridSpecFromSubplotSpec(n_rows, n_cols, subplot_spec=ax.get_subplotspec())
A:sklearn.inspection._plot.partial_dependence.axes_ravel[i]->self.figure_.add_subplot(spec)
A:sklearn.inspection._plot.partial_dependence.ax->numpy.asarray(ax, dtype=object)
A:sklearn.inspection._plot.partial_dependence.Z_level->numpy.linspace(*pdp_lim[2], num=8)
A:sklearn.inspection._plot.partial_dependence.self.deciles_vlines_->numpy.empty_like(self.axes_, dtype=object)
A:sklearn.inspection._plot.partial_dependence.self.deciles_hlines_->numpy.empty_like(self.axes_, dtype=object)
sklearn.inspection.PartialDependenceDisplay(self,pd_results,*,features,feature_names,target_idx,deciles,kind='average',subsample=1000,random_state=None,is_categorical=None)
sklearn.inspection.PartialDependenceDisplay._get_sample_count(self,n_samples)
sklearn.inspection.PartialDependenceDisplay._plot_average_dependence(self,avg_preds,feature_values,ax,pd_line_idx,line_kw,categorical,bar_kw)
sklearn.inspection.PartialDependenceDisplay._plot_ice_lines(self,preds,feature_values,n_ice_to_plot,ax,pd_plot_idx,n_total_lines_by_plot,individual_line_kw)
sklearn.inspection.PartialDependenceDisplay._plot_one_way_partial_dependence(self,kind,preds,avg_preds,feature_values,feature_idx,n_ice_lines,ax,n_cols,pd_plot_idx,n_lines,ice_lines_kw,pd_line_kw,categorical,bar_kw,pdp_lim)
sklearn.inspection.PartialDependenceDisplay._plot_two_way_partial_dependence(self,avg_preds,feature_values,feature_idx,ax,pd_plot_idx,Z_level,contour_kw,categorical,heatmap_kw)
sklearn.inspection.PartialDependenceDisplay.from_estimator(cls,estimator,X,features,*,sample_weight=None,categorical_features=None,feature_names=None,target=None,response_method='auto',n_cols=3,grid_resolution=100,percentiles=(0.05,0.95),method='auto',n_jobs=None,verbose=0,line_kw=None,ice_lines_kw=None,pd_line_kw=None,contour_kw=None,ax=None,kind='average',centered=False,subsample=1000,random_state=None)
sklearn.inspection.PartialDependenceDisplay.plot(self,*,ax=None,n_cols=3,line_kw=None,ice_lines_kw=None,pd_line_kw=None,contour_kw=None,bar_kw=None,heatmap_kw=None,pdp_lim=None,centered=False)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay(self,pd_results,*,features,feature_names,target_idx,deciles,kind='average',subsample=1000,random_state=None,is_categorical=None)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay.__init__(self,pd_results,*,features,feature_names,target_idx,deciles,kind='average',subsample=1000,random_state=None,is_categorical=None)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay._get_sample_count(self,n_samples)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay._plot_average_dependence(self,avg_preds,feature_values,ax,pd_line_idx,line_kw,categorical,bar_kw)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay._plot_ice_lines(self,preds,feature_values,n_ice_to_plot,ax,pd_plot_idx,n_total_lines_by_plot,individual_line_kw)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay._plot_one_way_partial_dependence(self,kind,preds,avg_preds,feature_values,feature_idx,n_ice_lines,ax,n_cols,pd_plot_idx,n_lines,ice_lines_kw,pd_line_kw,categorical,bar_kw,pdp_lim)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay._plot_two_way_partial_dependence(self,avg_preds,feature_values,feature_idx,ax,pd_plot_idx,Z_level,contour_kw,categorical,heatmap_kw)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay.from_estimator(cls,estimator,X,features,*,sample_weight=None,categorical_features=None,feature_names=None,target=None,response_method='auto',n_cols=3,grid_resolution=100,percentiles=(0.05,0.95),method='auto',n_jobs=None,verbose=0,line_kw=None,ice_lines_kw=None,pd_line_kw=None,contour_kw=None,ax=None,kind='average',centered=False,subsample=1000,random_state=None)
sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay.plot(self,*,ax=None,n_cols=3,line_kw=None,ice_lines_kw=None,pd_line_kw=None,contour_kw=None,bar_kw=None,heatmap_kw=None,pdp_lim=None,centered=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/decision_boundary.py----------------------------------------
A:sklearn.inspection._plot.decision_boundary.has_classes->hasattr(estimator, 'classes_')
A:sklearn.inspection._plot.decision_boundary.(_, ax)->matplotlib.pyplot.subplots()
A:sklearn.inspection._plot.decision_boundary.plot_func->getattr(ax, plot_method)
A:sklearn.inspection._plot.decision_boundary.self.surface_->plot_func(self.xx0, self.xx1, self.response, **kwargs)
A:sklearn.inspection._plot.decision_boundary.available_methods->', '.join(possible_plot_methods)
A:sklearn.inspection._plot.decision_boundary.num_features->_num_features(X)
A:sklearn.inspection._plot.decision_boundary.(xx0, xx1)->numpy.meshgrid(np.linspace(x0_min, x0_max, grid_resolution), np.linspace(x1_min, x1_max, grid_resolution))
A:sklearn.inspection._plot.decision_boundary.X_grid->X.iloc[[], :].copy()
A:sklearn.inspection._plot.decision_boundary.X_grid.iloc[:, 0]->xx0.ravel()
A:sklearn.inspection._plot.decision_boundary.X_grid.iloc[:, 1]->xx1.ravel()
A:sklearn.inspection._plot.decision_boundary.prediction_method->_check_boundary_response_method(estimator, response_method, class_of_interest)
A:sklearn.inspection._plot.decision_boundary.(response, _, response_method_used)->_get_response_values(estimator, X_grid, response_method=prediction_method, pos_label=class_of_interest, return_response_method_used=True)
A:sklearn.inspection._plot.decision_boundary.encoder->LabelEncoder()
A:sklearn.inspection._plot.decision_boundary.response->LabelEncoder().transform(response)
A:sklearn.inspection._plot.decision_boundary.display->cls(xx0=xx0, xx1=xx1, response=response.reshape(xx0.shape), xlabel=xlabel, ylabel=ylabel)
sklearn.inspection.DecisionBoundaryDisplay(self,*,xx0,xx1,response,xlabel=None,ylabel=None)
sklearn.inspection.DecisionBoundaryDisplay.from_estimator(cls,estimator,X,*,grid_resolution=100,eps=1.0,plot_method='contourf',response_method='auto',class_of_interest=None,xlabel=None,ylabel=None,ax=None,**kwargs)
sklearn.inspection.DecisionBoundaryDisplay.plot(self,plot_method='contourf',ax=None,xlabel=None,ylabel=None,**kwargs)
sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay(self,*,xx0,xx1,response,xlabel=None,ylabel=None)
sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay.__init__(self,*,xx0,xx1,response,xlabel=None,ylabel=None)
sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay.from_estimator(cls,estimator,X,*,grid_resolution=100,eps=1.0,plot_method='contourf',response_method='auto',class_of_interest=None,xlabel=None,ylabel=None,ax=None,**kwargs)
sklearn.inspection._plot.decision_boundary.DecisionBoundaryDisplay.plot(self,plot_method='contourf',ax=None,xlabel=None,ylabel=None,**kwargs)
sklearn.inspection._plot.decision_boundary._check_boundary_response_method(estimator,response_method,class_of_interest)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/tests/test_plot_partial_dependence.py----------------------------------------
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.pytestmark->pytest.mark.filterwarnings("ignore:In future, it will be an error for 'np.bool_':DeprecationWarning:matplotlib.*")
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.data->load_diabetes()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.clf->GradientBoostingClassifier(n_estimators=10, random_state=1)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_diabetes, diabetes.data, [0, 1], kind='average', method='brute')
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.fig->pyplot.gcf()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.axs->pyplot.gcf().get_axes()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.line_data->line.get_data()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.pd->pytest.importorskip('pandas')
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.X->_convert_container(X, array_type, columns_name=column_name)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.feature_names->_convert_container(diabetes.feature_names, feature_names_type)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.(fig, (ax1, ax2))->pyplot.subplots(1, 2)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp1->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_diabetes, diabetes.data, ['age', 'bmi'], kind=kind, grid_resolution=grid_resolution, feature_names=feature_names, subsample=20, random_state=0)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.lr->LinearRegression()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp2->sklearn.inspection.PartialDependenceDisplay.from_estimator(lr, diabetes.data, ['age', 'bmi'], kind=kind, grid_resolution=grid_resolution, feature_names=feature_names, ax=disp1.axes_)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.(fig, axes)->pyplot.subplots(nrows, ncols)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.msg->'Expected ax to have 2 axes, got {}'.format(nrows * ncols)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.(fig, ax)->pyplot.subplots()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.clf_int->GradientBoostingClassifier(n_estimators=10, random_state=1)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.iris->load_iris()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp_target_0->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_int, iris.data, [0, 3], target=0, grid_resolution=grid_resolution)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.clf_symbol->GradientBoostingClassifier(n_estimators=10, random_state=1)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp_symbol->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_symbol, iris.data, [0, 3], target='setosa', grid_resolution=grid_resolution)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp_target_1->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_int, iris.data, [0, 3], target=1, grid_resolution=grid_resolution)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.multioutput_regression_data->make_regression(n_samples=50, n_targets=2, random_state=0)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.df->pytest.importorskip('pandas').DataFrame(diabetes.data, columns=diabetes.feature_names)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.dummy_classification_data->make_classification(random_state=0)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.estimator->LinearRegression().fit(X, y)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.(_, axes)->pyplot.subplots(1, 2)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.preprocessor->make_column_transformer((OneHotEncoder(), categorical_features))
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.model->make_pipeline(preprocessor, LinearRegression())
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.legend_text->ax.get_legend().get_texts()
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.matplotlib->pytest.importorskip('matplotlib')
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.range_pd->numpy.array([-1, 1], dtype=np.float64)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.expect_levels->numpy.linspace(*levels, num=8)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.sample_weight->numpy.ones_like(diabetes.target)
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.disp_sw->sklearn.inspection.PartialDependenceDisplay.from_estimator(clf_diabetes, diabetes.data, [0, 1], sample_weight=sample_weight, kind='average', method='brute')
A:sklearn.inspection._plot.tests.test_plot_partial_dependence.curve->SubclassOfDisplay.from_estimator(clf_diabetes, diabetes.data, [0, 2, (0, 2)])
sklearn.inspection._plot.tests.test_plot_partial_dependence.clf_diabetes(diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.diabetes()
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_grid_resolution_with_categorical(pyplot,categorical_features,array_type)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_display_kind_centered_interaction(pyplot,kind,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_display_with_constant_sample_weight(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_display_wrong_len_kind(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_kind_error(pyplot,clf_diabetes,diabetes,features,kind)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_kind_list(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_overwrite_labels(pyplot,clf_diabetes,diabetes,kind,line_kw,label)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_plot_limits_one_way(pyplot,clf_diabetes,diabetes,kind,centered)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_partial_dependence_plot_limits_two_way(pyplot,clf_diabetes,diabetes,centered)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence(grid_resolution,pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_custom_axes(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_dataframe(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_does_not_override_ylabel(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_error(pyplot,data,params,err_msg)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_feature_name_reuse(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_incorrent_num_axes(pyplot,clf_diabetes,diabetes,nrows,ncols)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_kind(pyplot,kind,centered,subsample,shape,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_legend(pyplot)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_lines_kw(pyplot,clf_diabetes,diabetes,line_kw,pd_line_kw,ice_lines_kw,expected_colors)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_multiclass(pyplot)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_multiclass_error(pyplot,params,err_msg)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_multioutput(pyplot,target)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_passing_numpy_axes(pyplot,clf_diabetes,diabetes,kind,lines)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_str_features(pyplot,clf_diabetes,diabetes,input_type,feature_names_type)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_subsampling(pyplot,clf_diabetes,diabetes,kind,expected_shape)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_with_categorical(pyplot,categorical_features,array_type)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_plot_partial_dependence_with_same_axes(pyplot,clf_diabetes,diabetes)
sklearn.inspection._plot.tests.test_plot_partial_dependence.test_subclass_named_constructors_return_type_is_subclass(pyplot,diabetes,clf_diabetes)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/_plot/tests/test_boundary_decision_display.py----------------------------------------
A:sklearn.inspection._plot.tests.test_boundary_decision_display.pytestmark->pytest.mark.filterwarnings("ignore:In future, it will be an error for 'np.bool_':DeprecationWarning:matplotlib.*")
A:sklearn.inspection._plot.tests.test_boundary_decision_display.(X, y)->load_diabetes(return_X_y=True)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.clf->LogisticRegression().fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.prediction_method->_check_boundary_response_method(estimator, response_method, class_of_interest)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.lr->LogisticRegression(random_state=0).fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.disp->sklearn.inspection.DecisionBoundaryDisplay.from_estimator(estimator, X, response_method=response_method, class_of_interest=iris.target_names[class_of_interest_idx])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.(xx0, xx1)->numpy.meshgrid(np.linspace(x0_min, x0_max, grid_resolution), np.linspace(x1_min, x1_max, grid_resolution))
A:sklearn.inspection._plot.tests.test_boundary_decision_display.response->LogisticRegression(random_state=0).fit(X, y).predict(np.c_[xx0.ravel(), xx1.ravel()])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.(fig, ax)->pyplot.subplots()
A:sklearn.inspection._plot.tests.test_boundary_decision_display.(fig2, ax2)->pyplot.subplots()
A:sklearn.inspection._plot.tests.test_boundary_decision_display.outlier_detector->IsolationForest(random_state=0).fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.tree->DecisionTreeRegressor().fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.X->numpy.asarray([[0, 1], [1, 2]])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.y->numpy.asarray([[0, 1], [4, 1]])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.pd->pytest.importorskip('pandas')
A:sklearn.inspection._plot.tests.test_boundary_decision_display.df->pytest.importorskip('pandas').DataFrame(X, columns=['col_x', 'col_y'])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.(_, ax)->pyplot.subplots()
A:sklearn.inspection._plot.tests.test_boundary_decision_display.iris->load_iris()
A:sklearn.inspection._plot.tests.test_boundary_decision_display.log_reg->LogisticRegression().fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.estimator->LogisticRegression().fit(X, y)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.disp_default->sklearn.inspection.DecisionBoundaryDisplay.from_estimator(estimator, X, response_method=response_method, class_of_interest=None)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.disp_class_1->sklearn.inspection.DecisionBoundaryDisplay.from_estimator(estimator, X, response_method=response_method, class_of_interest=estimator.classes_[1])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.disp_class_0->sklearn.inspection.DecisionBoundaryDisplay.from_estimator(estimator, X, response_method=response_method, class_of_interest=estimator.classes_[0])
A:sklearn.inspection._plot.tests.test_boundary_decision_display.grid->numpy.concatenate([disp.xx0.reshape(-1, 1), disp.xx1.reshape(-1, 1)], axis=1)
A:sklearn.inspection._plot.tests.test_boundary_decision_display.curve->SubclassOfDisplay.from_estimator(estimator=clf, X=X)
sklearn.inspection._plot.tests.test_boundary_decision_display.fitted_clf()
sklearn.inspection._plot.tests.test_boundary_decision_display.load_iris_2d_scaled()
sklearn.inspection._plot.tests.test_boundary_decision_display.test_check_boundary_response_method(estimator,response_method,class_of_interest,expected_prediction_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_check_boundary_response_method_error()
sklearn.inspection._plot.tests.test_boundary_decision_display.test_class_of_interest_binary(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_class_of_interest_multiclass(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_dataframe_labels_used(pyplot,fitted_clf)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_dataframe_support(pyplot)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_decision_boundary_display_classifier(pyplot,fitted_clf,response_method,plot_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_decision_boundary_display_outlier_detector(pyplot,response_method,plot_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_decision_boundary_display_regressor(pyplot,response_method,plot_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_display_plot_input_error(pyplot,fitted_clf)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_error_bad_response(pyplot,response_method,msg)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_input_data_dimension(pyplot)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_input_validation_errors(pyplot,kwargs,error_msg,fitted_clf)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_multi_output_multi_class_classifier_error(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_multiclass(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_multiclass_error(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_multilabel_classifier_error(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_multioutput_regressor_error(pyplot)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_regressor_unsupported_response(pyplot,response_method)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_string_target(pyplot)
sklearn.inspection._plot.tests.test_boundary_decision_display.test_subclass_named_constructors_return_type_is_subclass(pyplot)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/tests/test_partial_dependence.py----------------------------------------
A:sklearn.inspection.tests.test_partial_dependence.iris->load_iris()
A:sklearn.inspection.tests.test_partial_dependence.est->LogisticRegression()
A:sklearn.inspection.tests.test_partial_dependence.result->partial_dependence(est, X=X, features=features, method=method, kind=kind, grid_resolution=grid_resolution)
A:sklearn.inspection.tests.test_partial_dependence.X->numpy.array(['A', 'B', 'C', np.nan], dtype=object).reshape(-1, 1)
A:sklearn.inspection.tests.test_partial_dependence.(grid, axes)->_grid_from_X(X, percentiles, is_categorical, grid_resolution=grid_resolution)
A:sklearn.inspection.tests.test_partial_dependence.rng->numpy.random.RandomState(123456)
A:sklearn.inspection.tests.test_partial_dependence.pd->pytest.importorskip('pandas')
A:sklearn.inspection.tests.test_partial_dependence.nunique->numpy.array(['A', 'B', 'C', np.nan], dtype=object).reshape(-1, 1).nunique()
A:sklearn.inspection.tests.test_partial_dependence.(X, y)->make_classification(random_state=0)
A:sklearn.inspection.tests.test_partial_dependence.features->numpy.array([f], dtype=np.int32)
A:sklearn.inspection.tests.test_partial_dependence.grid->numpy.random.RandomState(123456).randn(50).reshape(-1, 1)
A:sklearn.inspection.tests.test_partial_dependence.(pdp, predictions)->_partial_dependence_brute(est, grid, features, X, response_method='auto')
A:sklearn.inspection.tests.test_partial_dependence.pdp->partial_dependence(clf, X, features=[1], kind='average')
A:sklearn.inspection.tests.test_partial_dependence.X_->numpy.array(['A', 'B', 'C', np.nan], dtype=object).reshape(-1, 1).copy()
A:sklearn.inspection.tests.test_partial_dependence.forest->RandomForestRegressor(n_estimators=1, max_features=None, bootstrap=False, max_depth=max_depth, random_state=tree_seed)
A:sklearn.inspection.tests.test_partial_dependence.equiv_random_state->check_random_state(tree_seed).randint(np.iinfo(np.int32).max)
A:sklearn.inspection.tests.test_partial_dependence.gbdt->GradientBoostingRegressor(n_estimators=1, learning_rate=1, criterion='squared_error', max_depth=max_depth, random_state=equiv_random_state)
A:sklearn.inspection.tests.test_partial_dependence.tree->DecisionTreeRegressor(max_depth=max_depth, random_state=equiv_random_state)
A:sklearn.inspection.tests.test_partial_dependence.pdp_forest->_partial_dependence_recursion(forest, grid, features)
A:sklearn.inspection.tests.test_partial_dependence.pdp_gbdt->_partial_dependence_recursion(gbdt, grid, features)
A:sklearn.inspection.tests.test_partial_dependence.pdp_tree->_partial_dependence_recursion(tree, grid, features)
A:sklearn.inspection.tests.test_partial_dependence.preds_1->partial_dependence(est, X, [target_feature], response_method='decision_function', method='recursion', kind='average')
A:sklearn.inspection.tests.test_partial_dependence.preds_2->partial_dependence(est, X, [target_feature], response_method='decision_function', method='brute', kind='average')
A:sklearn.inspection.tests.test_partial_dependence.new_X->PolynomialFeatures(degree=power).fit_transform(new_X)
A:sklearn.inspection.tests.test_partial_dependence.lr->LinearRegression().fit(new_X, new_y)
A:sklearn.inspection.tests.test_partial_dependence.r2->r2_score(new_y, lr.predict(new_X))
A:sklearn.inspection.tests.test_partial_dependence.df->pytest.importorskip('pandas').DataFrame(iris.data, columns=iris.feature_names)
A:sklearn.inspection.tests.test_partial_dependence.gbc->GradientBoostingClassifier(init=DummyClassifier(), random_state=0)
A:sklearn.inspection.tests.test_partial_dependence.mask->numpy.random.RandomState(123456).randint(2, size=N, dtype=bool)
A:sklearn.inspection.tests.test_partial_dependence.x->numpy.random.RandomState(123456).rand(N)
A:sklearn.inspection.tests.test_partial_dependence.y->numpy.array([0, 1, 0, 1])
A:sklearn.inspection.tests.test_partial_dependence.sample_weight->numpy.ones_like(y)
A:sklearn.inspection.tests.test_partial_dependence.clf->make_pipeline(OrdinalEncoder(encoded_missing_value=-1), LogisticRegression()).fit(X, y)
A:sklearn.inspection.tests.test_partial_dependence.scaler->StandardScaler()
A:sklearn.inspection.tests.test_partial_dependence.pipe->make_pipeline(preprocessor, estimator).fit(X, y)
A:sklearn.inspection.tests.test_partial_dependence.pdp_pipe->partial_dependence(pipe, df, features=features, grid_resolution=10, kind='average')
A:sklearn.inspection.tests.test_partial_dependence.pdp_clf->partial_dependence(clf, X_proc, features=features_clf, method='brute', grid_resolution=10, kind='average')
A:sklearn.inspection.tests.test_partial_dependence.X_proc->clone(preprocessor).fit_transform(df)
A:sklearn.inspection.tests.test_partial_dependence.preprocessor->make_column_transformer((StandardScaler(), [0, 2]), (RobustScaler(), [1, 3]))
A:sklearn.inspection.tests.test_partial_dependence.pdp_avg->partial_dependence(est, X=X, features=[1, 2], kind='average')
A:sklearn.inspection.tests.test_partial_dependence.pdp_ind->partial_dependence(pipe, X, [2, 3], kind='individual', grid_resolution=10)
A:sklearn.inspection.tests.test_partial_dependence.avg_ind->numpy.mean(pdp_ind['individual'], axis=1)
A:sklearn.inspection.tests.test_partial_dependence.pdp_nsw->partial_dependence(est, X=X, features=[1, 2], kind='individual')
A:sklearn.inspection.tests.test_partial_dependence.pdp_sw->partial_dependence(pipe, X, [2, 3], kind='average', sample_weight=sample_weight, grid_resolution=10)
A:sklearn.inspection.tests.test_partial_dependence.pdp_sw_none->partial_dependence(est, **params, sample_weight=sample_weight)
A:sklearn.inspection.tests.test_partial_dependence.pdp_sw_unit->partial_dependence(est, **params, sample_weight=sample_weight)
A:sklearn.inspection.tests.test_partial_dependence.pdp_sw_doubling->partial_dependence(est, **params, sample_weight=sample_weight)
sklearn.inspection.tests.test_partial_dependence.NoPredictProbaNoDecisionFunction(ClassifierMixin,BaseEstimator)
sklearn.inspection.tests.test_partial_dependence.NoPredictProbaNoDecisionFunction.fit(self,X,y)
sklearn.inspection.tests.test_partial_dependence.test_grid_from_X()
sklearn.inspection.tests.test_partial_dependence.test_grid_from_X_error(grid_resolution,percentiles,err_msg)
sklearn.inspection.tests.test_partial_dependence.test_grid_from_X_heterogeneous_type(grid_resolution)
sklearn.inspection.tests.test_partial_dependence.test_grid_from_X_with_categorical(grid_resolution)
sklearn.inspection.tests.test_partial_dependence.test_hist_gbdt_sw_not_supported()
sklearn.inspection.tests.test_partial_dependence.test_kind_average_and_average_of_individual(Estimator,data)
sklearn.inspection.tests.test_partial_dependence.test_mixed_type_categorical()
sklearn.inspection.tests.test_partial_dependence.test_multiclass_multioutput(Estimator)
sklearn.inspection.tests.test_partial_dependence.test_output_shape(Estimator,method,data,grid_resolution,features,kind)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_X_list(estimator)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_bunch_values_deprecated()
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_dataframe(estimator,preprocessor,features)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_easy_target(est,power)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_equivalence_equal_sample_weight(Estimator,data)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_error(estimator,params,err_msg)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_feature_type(features,expected_pd_shape)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_helpers(est,method,target_feature)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_kind_individual_ignores_sample_weight(Estimator,data)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_non_null_weight_idx(estimator,non_null_weight_idx)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_pipeline()
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_sample_weight_of_fitted_estimator()
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_sample_weight_size_error()
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_sample_weight_with_recursion()
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_unfitted(estimator)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_unknown_feature_indices(estimator,features)
sklearn.inspection.tests.test_partial_dependence.test_partial_dependence_unknown_feature_string(estimator)
sklearn.inspection.tests.test_partial_dependence.test_recursion_decision_function(est,target_feature)
sklearn.inspection.tests.test_partial_dependence.test_recursion_decision_tree_vs_forest_and_gbdt(seed)
sklearn.inspection.tests.test_partial_dependence.test_warning_recursion_non_constant_init()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/tests/test_permutation_importance.py----------------------------------------
A:sklearn.inspection.tests.test_permutation_importance.rng->numpy.random.RandomState(1)
A:sklearn.inspection.tests.test_permutation_importance.(X, y)->make_classification(n_samples=n_samples, n_features=n_features, random_state=0)
A:sklearn.inspection.tests.test_permutation_importance.y_with_little_noise->(y + rng.normal(scale=0.001, size=y.shape[0])).reshape(-1, 1)
A:sklearn.inspection.tests.test_permutation_importance.X->_convert_container(X, input_type)
A:sklearn.inspection.tests.test_permutation_importance.clf->LogisticRegression()
A:sklearn.inspection.tests.test_permutation_importance.result->permutation_importance(clf, X, y, n_repeats=n_repeats, random_state=rng)
A:sklearn.inspection.tests.test_permutation_importance.pd->pytest.importorskip('pandas')
A:sklearn.inspection.tests.test_permutation_importance.dataset->load_iris()
A:sklearn.inspection.tests.test_permutation_importance.classes->numpy.arange(n_classes)
A:sklearn.inspection.tests.test_permutation_importance.y->numpy.array([0, 1, 0, 1])
A:sklearn.inspection.tests.test_permutation_importance.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=0.5, random_state=rng)
A:sklearn.inspection.tests.test_permutation_importance.r->permutation_importance(clf, X, y, n_repeats=n_repeats, n_jobs=2)
A:sklearn.inspection.tests.test_permutation_importance.result2->permutation_importance(clf, X, y, n_repeats=n_repeats, random_state=rng)
A:sklearn.inspection.tests.test_permutation_importance.num_preprocess->make_pipeline(SimpleImputer(), StandardScaler())
A:sklearn.inspection.tests.test_permutation_importance.preprocess->ColumnTransformer([('num', num_preprocess, ['col1']), ('cat', OneHotEncoder(), ['col2'])])
A:sklearn.inspection.tests.test_permutation_importance.lr->LinearRegression().fit(x, y)
A:sklearn.inspection.tests.test_permutation_importance.results->permutation_importance(lr, X, y, n_repeats=50, scoring='neg_mean_squared_error')
A:sklearn.inspection.tests.test_permutation_importance.importance_sequential->permutation_importance(lr, X, y, n_repeats=5, random_state=0, n_jobs=1, max_samples=max_samples)
A:sklearn.inspection.tests.test_permutation_importance.imp_min->importance_array['importances'].min()
A:sklearn.inspection.tests.test_permutation_importance.imp_max->importance_array['importances'].max()
A:sklearn.inspection.tests.test_permutation_importance.importance_processes->permutation_importance(lr, X, y, n_repeats=5, random_state=0, n_jobs=2)
A:sklearn.inspection.tests.test_permutation_importance.importance_threading->permutation_importance(lr, X, y, n_repeats=5, random_state=0, n_jobs=2)
A:sklearn.inspection.tests.test_permutation_importance.X_df->pytest.importorskip('pandas').DataFrame(X)
A:sklearn.inspection.tests.test_permutation_importance.binner->KBinsDiscretizer(n_bins=3, encode='ordinal')
A:sklearn.inspection.tests.test_permutation_importance.cat_column->cat_column.ravel().ravel()
A:sklearn.inspection.tests.test_permutation_importance.new_col_idx->len(X_df.columns)
A:sklearn.inspection.tests.test_permutation_importance.X_df.index->numpy.arange(len(X_df)).astype(str)
A:sklearn.inspection.tests.test_permutation_importance.rf->RandomForestRegressor(n_estimators=5, max_depth=3, random_state=0)
A:sklearn.inspection.tests.test_permutation_importance.importance_array->permutation_importance(rf, X, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs, max_samples=max_samples)
A:sklearn.inspection.tests.test_permutation_importance.importance_dataframe->permutation_importance(rf, X_df, y, n_repeats=n_repeats, random_state=0, n_jobs=n_jobs, max_samples=max_samples)
A:sklearn.inspection.tests.test_permutation_importance.expected_importances->numpy.zeros((n_features, n_repeats))
A:sklearn.inspection.tests.test_permutation_importance.x->numpy.array([[1, 2], [3, 4]])
A:sklearn.inspection.tests.test_permutation_importance.pi->permutation_importance(lr, x, y, random_state=1, scoring='neg_mean_absolute_error', n_repeats=200, sample_weight=w)
A:sklearn.inspection.tests.test_permutation_importance.w->numpy.array([1, 1])
A:sklearn.inspection.tests.test_permutation_importance.(x, y)->make_regression(n_samples=500, n_features=10, random_state=0)
A:sklearn.inspection.tests.test_permutation_importance.multi_importance->permutation_importance(lr, x, y, random_state=1, scoring=multi_scorer, n_repeats=2)
A:sklearn.inspection.tests.test_permutation_importance.single_result->permutation_importance(lr, x, y, random_state=1, scoring=scorer, n_repeats=2)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_correlated_feature_regression(n_jobs,max_samples)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_correlated_feature_regression_pandas(n_jobs,max_samples)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_equivalence_array_dataframe(n_jobs,max_samples)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_equivalence_sequential_parallel(max_samples)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_large_memmaped_data(input_type)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_linear_regresssion()
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_max_samples_error()
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_mixed_types()
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_mixed_types_pandas()
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_multi_metric(list_single_scorer,multi_scorer)
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_no_weights_scoring_function()
sklearn.inspection.tests.test_permutation_importance.test_permutation_importance_sample_weight()
sklearn.inspection.tests.test_permutation_importance.test_robustness_to_high_cardinality_noisy_feature(n_jobs,max_samples,seed=42)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/inspection/tests/test_pd_utils.py----------------------------------------
A:sklearn.inspection.tests.test_pd_utils.X->numpy.random.randn(10, 3)
A:sklearn.inspection.tests.test_pd_utils.feature_names_validated->_check_feature_names(X, feature_names)
sklearn.inspection.tests.test_pd_utils.test_check_feature_names(feature_names,array_type,expected_feature_names)
sklearn.inspection.tests.test_pd_utils.test_check_feature_names_error()
sklearn.inspection.tests.test_pd_utils.test_get_feature_index(fx,idx)
sklearn.inspection.tests.test_pd_utils.test_get_feature_names_error(fx,feature_names,err_msg)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_build_utils/openmp_helpers.py----------------------------------------
A:sklearn._build_utils.openmp_helpers.code->textwrap.dedent('        #include <omp.h>\n        #include <stdio.h>\n        int main(void) {\n        #pragma omp parallel\n        printf("nthreads=%d\\n", omp_get_num_threads());\n        return 0;\n        }\n        ')
A:sklearn._build_utils.openmp_helpers.extra_preargs->extra_preargs.strip().split(' ').strip().split(' ')
A:sklearn._build_utils.openmp_helpers.extra_postargs->get_openmp_flag()
A:sklearn._build_utils.openmp_helpers.output->compile_test_program(code, extra_preargs=extra_preargs, extra_postargs=extra_postargs)
A:sklearn._build_utils.openmp_helpers.nthreads->int(output[0].strip().split('=')[1])
A:sklearn._build_utils.openmp_helpers.message->textwrap.dedent('\n\n                                ***********\n                                * WARNING *\n                                ***********\n\n                It seems that scikit-learn cannot be built with OpenMP.\n\n                - Make sure you have followed the installation instructions:\n\n                    https://scikit-learn.org/dev/developers/advanced_installation.html\n\n                - If your compiler supports OpenMP but you still see this\n                  message, please submit a bug report at:\n\n                    https://github.com/scikit-learn/scikit-learn/issues\n\n                - The build will continue with OpenMP-based parallelism\n                  disabled. Note however that some estimators will run in\n                  sequential mode instead of leveraging thread-based\n                  parallelism.\n\n                                    ***\n                ')
sklearn._build_utils.check_openmp_support()
sklearn._build_utils.openmp_helpers.check_openmp_support()
sklearn._build_utils.openmp_helpers.get_openmp_flag()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_build_utils/__init__.py----------------------------------------
A:sklearn._build_utils.__init__.message->'Please install Cython with a version >= {0} in order to build a scikit-learn from source.'.format(CYTHON_MIN_VERSION)
A:sklearn._build_utils.__init__.sklearn._OPENMP_SUPPORTED->check_openmp_support()
A:sklearn._build_utils.__init__.n_jobs->joblib.cpu_count()
A:sklearn._build_utils.__init__.outfile->template.replace('.tp', '')
A:sklearn._build_utils.__init__.tmpl->f.read()
A:sklearn._build_utils.__init__.tmpl_->Cython.Tempita.sub(tmpl)
sklearn._build_utils.__init__._check_cython_version()
sklearn._build_utils.__init__.cythonize_extensions(extension)
sklearn._build_utils.__init__.gen_from_templates(templates)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_build_utils/pre_build_helpers.py----------------------------------------
A:sklearn._build_utils.pre_build_helpers.ccompiler->new_compiler()
A:sklearn._build_utils.pre_build_helpers.start_dir->os.path.abspath('.')
A:sklearn._build_utils.pre_build_helpers.objects->glob.glob(os.path.join('objects', '*' + ccompiler.obj_extension))
A:sklearn._build_utils.pre_build_helpers.output->output.decode(sys.stdout.encoding or 'utf-8').splitlines().decode(sys.stdout.encoding or 'utf-8').splitlines()
A:sklearn._build_utils.pre_build_helpers.code->textwrap.dedent('        #include <stdio.h>\n        int main(void) {\n        return 0;\n        }\n        ')
sklearn._build_utils.basic_check_build()
sklearn._build_utils.pre_build_helpers.basic_check_build()
sklearn._build_utils.pre_build_helpers.compile_test_program(code,extra_preargs=None,extra_postargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_truncated_svd.py----------------------------------------
A:sklearn.decomposition._truncated_svd.X->check_array(X)
A:sklearn.decomposition._truncated_svd.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._truncated_svd.v0->_init_arpack_v0(min(X.shape), random_state)
A:sklearn.decomposition._truncated_svd.(U, Sigma, VT)->randomized_svd(X, self.n_components, n_iter=self.n_iter, n_oversamples=self.n_oversamples, power_iteration_normalizer=self.power_iteration_normalizer, random_state=random_state)
A:sklearn.decomposition._truncated_svd.(U, VT)->svd_flip(U[:, ::-1], VT[::-1])
A:sklearn.decomposition._truncated_svd.X_transformed->safe_sparse_dot(X, self.components_.T)
A:sklearn.decomposition._truncated_svd.self.explained_variance_exp_var->numpy.var(X_transformed, axis=0)
A:sklearn.decomposition._truncated_svd.(_, full_var)->mean_variance_axis(X, axis=0)
A:sklearn.decomposition._truncated_svd.full_var->numpy.var(X, axis=0).sum()
sklearn.decomposition.TruncatedSVD(self,n_components=2,*,algorithm='randomized',n_iter=5,n_oversamples=10,power_iteration_normalizer='auto',random_state=None,tol=0.0)
sklearn.decomposition.TruncatedSVD._more_tags(self)
sklearn.decomposition.TruncatedSVD._n_features_out(self)
sklearn.decomposition.TruncatedSVD.fit(self,X,y=None)
sklearn.decomposition.TruncatedSVD.fit_transform(self,X,y=None)
sklearn.decomposition.TruncatedSVD.inverse_transform(self,X)
sklearn.decomposition.TruncatedSVD.transform(self,X)
sklearn.decomposition._truncated_svd.TruncatedSVD(self,n_components=2,*,algorithm='randomized',n_iter=5,n_oversamples=10,power_iteration_normalizer='auto',random_state=None,tol=0.0)
sklearn.decomposition._truncated_svd.TruncatedSVD.__init__(self,n_components=2,*,algorithm='randomized',n_iter=5,n_oversamples=10,power_iteration_normalizer='auto',random_state=None,tol=0.0)
sklearn.decomposition._truncated_svd.TruncatedSVD._more_tags(self)
sklearn.decomposition._truncated_svd.TruncatedSVD._n_features_out(self)
sklearn.decomposition._truncated_svd.TruncatedSVD.fit(self,X,y=None)
sklearn.decomposition._truncated_svd.TruncatedSVD.fit_transform(self,X,y=None)
sklearn.decomposition._truncated_svd.TruncatedSVD.inverse_transform(self,X)
sklearn.decomposition._truncated_svd.TruncatedSVD.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_pca.py----------------------------------------
A:sklearn.decomposition._pca.(xp, _)->get_namespace(X)
A:sklearn.decomposition._pca.pl->xp.sum(xp.log(spectrum[:rank]))
A:sklearn.decomposition._pca.v->max(eps, xp.sum(spectrum[rank:]) / (n_features - rank))
A:sklearn.decomposition._pca.spectrum_->xp.asarray(spectrum, copy=True)
A:sklearn.decomposition._pca.ll->xp.empty_like(spectrum)
A:sklearn.decomposition._pca.ll[rank]->_assess_dimension(spectrum, rank, n_samples)
A:sklearn.decomposition._pca.(U, S, Vt)->randomized_svd(X, n_components=n_components, n_oversamples=self.n_oversamples, n_iter=self.iterated_power, power_iteration_normalizer=self.power_iteration_normalizer, flip_sign=True, random_state=random_state)
A:sklearn.decomposition._pca.(xp, is_array_api_compliant)->get_namespace(X)
A:sklearn.decomposition._pca.X->self._validate_data(X, dtype=[xp.float64, xp.float32], reset=False)
A:sklearn.decomposition._pca.n_components->_infer_dimension(explained_variance_, n_samples)
A:sklearn.decomposition._pca.self.mean_->xp.mean(X, axis=0)
A:sklearn.decomposition._pca.(U, Vt)->svd_flip(U[:, ::-1], Vt[::-1])
A:sklearn.decomposition._pca.total_var->xp.sum(explained_variance_)
A:sklearn.decomposition._pca.singular_values_->xp.asarray(S, copy=True)
A:sklearn.decomposition._pca.explained_variance_ratio_np->_convert_to_numpy(explained_variance_ratio_, xp=xp)
A:sklearn.decomposition._pca.ratio_cumsum->stable_cumsum(explained_variance_ratio_np)
A:sklearn.decomposition._pca.self.noise_variance_->xp.mean(explained_variance_[n_components:])
A:sklearn.decomposition._pca.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._pca.(self.mean_, var)->mean_variance_axis(X, axis=0)
A:sklearn.decomposition._pca.v0->_init_arpack_v0(min(X.shape), random_state)
A:sklearn.decomposition._pca.self.singular_values_->xp.asarray(S, copy=True)
A:sklearn.decomposition._pca.precision->self.get_precision()
sklearn.decomposition.PCA(self,n_components=None,*,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',n_oversamples=10,power_iteration_normalizer='auto',random_state=None)
sklearn.decomposition.PCA._fit(self,X)
sklearn.decomposition.PCA._fit_full(self,X,n_components)
sklearn.decomposition.PCA._fit_truncated(self,X,n_components,svd_solver)
sklearn.decomposition.PCA._more_tags(self)
sklearn.decomposition.PCA.fit(self,X,y=None)
sklearn.decomposition.PCA.fit_transform(self,X,y=None)
sklearn.decomposition.PCA.score(self,X,y=None)
sklearn.decomposition.PCA.score_samples(self,X)
sklearn.decomposition._pca.PCA(self,n_components=None,*,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',n_oversamples=10,power_iteration_normalizer='auto',random_state=None)
sklearn.decomposition._pca.PCA.__init__(self,n_components=None,*,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',n_oversamples=10,power_iteration_normalizer='auto',random_state=None)
sklearn.decomposition._pca.PCA._fit(self,X)
sklearn.decomposition._pca.PCA._fit_full(self,X,n_components)
sklearn.decomposition._pca.PCA._fit_truncated(self,X,n_components,svd_solver)
sklearn.decomposition._pca.PCA._more_tags(self)
sklearn.decomposition._pca.PCA.fit(self,X,y=None)
sklearn.decomposition._pca.PCA.fit_transform(self,X,y=None)
sklearn.decomposition._pca.PCA.score(self,X,y=None)
sklearn.decomposition._pca.PCA.score_samples(self,X)
sklearn.decomposition._pca._assess_dimension(spectrum,rank,n_samples)
sklearn.decomposition._pca._infer_dimension(spectrum,n_samples)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_sparse_pca.py----------------------------------------
A:sklearn.decomposition._sparse_pca.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._sparse_pca.X->check_array(X)
A:sklearn.decomposition._sparse_pca.self.mean_->check_array(X).mean(axis=0)
A:sklearn.decomposition._sparse_pca.U->ridge_regression(self.components_.T, X.T, self.ridge_alpha, solver='cholesky')
A:sklearn.decomposition._sparse_pca.(code, dictionary, E, self.n_iter_)->dict_learning(X.T, n_components, alpha=self.alpha, tol=self.tol, max_iter=self.max_iter, method=self.method, n_jobs=self.n_jobs, verbose=self.verbose, random_state=random_state, code_init=code_init, dict_init=dict_init, return_n_iter=True)
A:sklearn.decomposition._sparse_pca.(code, dictionary)->svd_flip(code, dictionary, u_based_decision=False)
A:sklearn.decomposition._sparse_pca.self.n_components_->len(self.components_)
A:sklearn.decomposition._sparse_pca.est->MiniBatchDictionaryLearning(n_components=n_components, alpha=self.alpha, max_iter=self.max_iter, dict_init=None, batch_size=self.batch_size, shuffle=self.shuffle, n_jobs=self.n_jobs, fit_algorithm=self.method, random_state=random_state, transform_algorithm=transform_algorithm, transform_alpha=self.alpha, verbose=self.verbose, callback=self.callback, tol=self.tol, max_no_improvement=self.max_no_improvement)
sklearn.decomposition.MiniBatchSparsePCA(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=None,method='lars',random_state=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition.MiniBatchSparsePCA._fit(self,X,n_components,random_state)
sklearn.decomposition.SparsePCA(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=None,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition.SparsePCA._fit(self,X,n_components,random_state)
sklearn.decomposition._sparse_pca.MiniBatchSparsePCA(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=None,method='lars',random_state=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition._sparse_pca.MiniBatchSparsePCA.__init__(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,callback=None,batch_size=3,verbose=False,shuffle=True,n_jobs=None,method='lars',random_state=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition._sparse_pca.MiniBatchSparsePCA._fit(self,X,n_components,random_state)
sklearn.decomposition._sparse_pca.SparsePCA(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=None,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition._sparse_pca.SparsePCA.__init__(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=None,U_init=None,V_init=None,verbose=False,random_state=None)
sklearn.decomposition._sparse_pca.SparsePCA._fit(self,X,n_components,random_state)
sklearn.decomposition._sparse_pca._BaseSparsePCA(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=None,verbose=False,random_state=None)
sklearn.decomposition._sparse_pca._BaseSparsePCA.__init__(self,n_components=None,*,alpha=1,ridge_alpha=0.01,max_iter=1000,tol=1e-08,method='lars',n_jobs=None,verbose=False,random_state=None)
sklearn.decomposition._sparse_pca._BaseSparsePCA._more_tags(self)
sklearn.decomposition._sparse_pca._BaseSparsePCA._n_features_out(self)
sklearn.decomposition._sparse_pca._BaseSparsePCA.fit(self,X,y=None)
sklearn.decomposition._sparse_pca._BaseSparsePCA.inverse_transform(self,X)
sklearn.decomposition._sparse_pca._BaseSparsePCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_lda.py----------------------------------------
A:sklearn.decomposition._lda.is_sparse_x->scipy.sparse.issparse(X)
A:sklearn.decomposition._lda.doc_topic_distr->self._unnormalized_transform(X)
A:sklearn.decomposition._lda.exp_doc_topic->numpy.exp(_dirichlet_expectation_2d(doc_topic_distr))
A:sklearn.decomposition._lda.exp_doc_topic_d->exp_doc_topic[idx_d, :].copy()
A:sklearn.decomposition._lda.self.random_state_->check_random_state(self.random_state)
A:sklearn.decomposition._lda.self.components_->self.random_state_.gamma(init_gamma, init_var, (self.n_components, n_features)).astype(dtype, copy=False)
A:sklearn.decomposition._lda.self.exp_dirichlet_component_->numpy.exp(_dirichlet_expectation_2d(self.components_))
A:sklearn.decomposition._lda.n_jobs->effective_n_jobs(self.n_jobs)
A:sklearn.decomposition._lda.parallel->Parallel(n_jobs=n_jobs, verbose=max(0, self.verbose - 1))
A:sklearn.decomposition._lda.results->parallel((delayed(_update_doc_distribution)(X[idx_slice, :], self.exp_dirichlet_component_, self.doc_topic_prior_, self.max_doc_update_iter, self.mean_change_tol, cal_sstats, random_state) for idx_slice in gen_even_slices(X.shape[0], n_jobs)))
A:sklearn.decomposition._lda.(doc_topics, sstats_list)->zip(*results)
A:sklearn.decomposition._lda.suff_stats->numpy.zeros(self.components_.shape, dtype=self.components_.dtype)
A:sklearn.decomposition._lda.(_, suff_stats)->self._e_step(X, cal_sstats=True, random_init=True, parallel=parallel)
A:sklearn.decomposition._lda.weight->numpy.power(self.learning_offset + self.n_batch_iter_, -self.learning_decay)
A:sklearn.decomposition._lda.X->self._check_non_neg_array(X, reset_n_features=True, whom='LatentDirichletAllocation.perplexity')
A:sklearn.decomposition._lda.(doc_topics_distr, _)->self._e_step(X, cal_sstats=False, random_init=False, parallel=parallel)
A:sklearn.decomposition._lda.bound->self._approx_bound(X, doc_topic_distr, sub_sampling)
A:sklearn.decomposition._lda.self.bound_->self._perplexity_precomp_distr(X, doc_topics_distr, sub_sampling=False)
A:sklearn.decomposition._lda.(doc_topic_distr, _)->self._e_step(X, cal_sstats=False, random_init=False)
A:sklearn.decomposition._lda.score->self._approx_bound(X, doc_topic_distr, sub_sampling=False)
A:sklearn.decomposition._lda.dirichlet_doc_topic->_dirichlet_expectation_2d(doc_topic_distr)
A:sklearn.decomposition._lda.dirichlet_component_->_dirichlet_expectation_2d(self.components_)
A:sklearn.decomposition._lda.norm_phi->logsumexp(temp, axis=0)
A:sklearn.decomposition._lda.word_cnt->self._check_non_neg_array(X, reset_n_features=True, whom='LatentDirichletAllocation.perplexity').sum()
sklearn.decomposition.LatentDirichletAllocation(self,n_components=10,*,doc_topic_prior=None,topic_word_prior=None,learning_method='batch',learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=None,verbose=0,random_state=None)
sklearn.decomposition.LatentDirichletAllocation._approx_bound(self,X,doc_topic_distr,sub_sampling)
sklearn.decomposition.LatentDirichletAllocation._check_non_neg_array(self,X,reset_n_features,whom)
sklearn.decomposition.LatentDirichletAllocation._e_step(self,X,cal_sstats,random_init,parallel=None)
sklearn.decomposition.LatentDirichletAllocation._em_step(self,X,total_samples,batch_update,parallel=None)
sklearn.decomposition.LatentDirichletAllocation._init_latent_vars(self,n_features,dtype=np.float64)
sklearn.decomposition.LatentDirichletAllocation._more_tags(self)
sklearn.decomposition.LatentDirichletAllocation._n_features_out(self)
sklearn.decomposition.LatentDirichletAllocation._perplexity_precomp_distr(self,X,doc_topic_distr=None,sub_sampling=False)
sklearn.decomposition.LatentDirichletAllocation._unnormalized_transform(self,X)
sklearn.decomposition.LatentDirichletAllocation.fit(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.partial_fit(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.perplexity(self,X,sub_sampling=False)
sklearn.decomposition.LatentDirichletAllocation.score(self,X,y=None)
sklearn.decomposition.LatentDirichletAllocation.transform(self,X)
sklearn.decomposition._lda.LatentDirichletAllocation(self,n_components=10,*,doc_topic_prior=None,topic_word_prior=None,learning_method='batch',learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=None,verbose=0,random_state=None)
sklearn.decomposition._lda.LatentDirichletAllocation.__init__(self,n_components=10,*,doc_topic_prior=None,topic_word_prior=None,learning_method='batch',learning_decay=0.7,learning_offset=10.0,max_iter=10,batch_size=128,evaluate_every=-1,total_samples=1000000.0,perp_tol=0.1,mean_change_tol=0.001,max_doc_update_iter=100,n_jobs=None,verbose=0,random_state=None)
sklearn.decomposition._lda.LatentDirichletAllocation._approx_bound(self,X,doc_topic_distr,sub_sampling)
sklearn.decomposition._lda.LatentDirichletAllocation._check_non_neg_array(self,X,reset_n_features,whom)
sklearn.decomposition._lda.LatentDirichletAllocation._e_step(self,X,cal_sstats,random_init,parallel=None)
sklearn.decomposition._lda.LatentDirichletAllocation._em_step(self,X,total_samples,batch_update,parallel=None)
sklearn.decomposition._lda.LatentDirichletAllocation._init_latent_vars(self,n_features,dtype=np.float64)
sklearn.decomposition._lda.LatentDirichletAllocation._more_tags(self)
sklearn.decomposition._lda.LatentDirichletAllocation._n_features_out(self)
sklearn.decomposition._lda.LatentDirichletAllocation._perplexity_precomp_distr(self,X,doc_topic_distr=None,sub_sampling=False)
sklearn.decomposition._lda.LatentDirichletAllocation._unnormalized_transform(self,X)
sklearn.decomposition._lda.LatentDirichletAllocation.fit(self,X,y=None)
sklearn.decomposition._lda.LatentDirichletAllocation.partial_fit(self,X,y=None)
sklearn.decomposition._lda.LatentDirichletAllocation.perplexity(self,X,sub_sampling=False)
sklearn.decomposition._lda.LatentDirichletAllocation.score(self,X,y=None)
sklearn.decomposition._lda.LatentDirichletAllocation.transform(self,X)
sklearn.decomposition._lda._update_doc_distribution(X,exp_topic_word_distr,doc_topic_prior,max_doc_update_iter,mean_change_tol,cal_sstats,random_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_fastica.py----------------------------------------
A:sklearn.decomposition._fastica.(s, u)->scipy.linalg.eigh(np.dot(W, W.T))
A:sklearn.decomposition._fastica.s->numpy.clip(s, a_min=np.finfo(W.dtype).tiny, a_max=None)
A:sklearn.decomposition._fastica.W->_sym_decorrelation(w_init)
A:sklearn.decomposition._fastica.w->w_init[j, :].copy()
A:sklearn.decomposition._fastica.(gwtx, g_wtx)->g(np.dot(W, X), fun_args)
A:sklearn.decomposition._fastica.lim->max(abs(abs(np.einsum('ij,ij->i', W1, W)) - 1))
A:sklearn.decomposition._fastica.p_->float(X.shape[1])
A:sklearn.decomposition._fastica.W1->_sym_decorrelation(np.dot(gwtx, X.T) / p_ - g_wtx[:, np.newaxis] * W)
A:sklearn.decomposition._fastica.alpha->fun_args.get('alpha', 1.0)
A:sklearn.decomposition._fastica.gx->numpy.tanh(x, x)
A:sklearn.decomposition._fastica.g_x->numpy.empty(x.shape[0], dtype=x.dtype)
A:sklearn.decomposition._fastica.g_x[i]->(alpha * (1 - gx_i ** 2)).mean()
A:sklearn.decomposition._fastica.exp->numpy.exp(-x ** 2 / 2)
A:sklearn.decomposition._fastica.est->FastICA(n_components=n_components, algorithm=algorithm, whiten=whiten, fun=fun, fun_args=fun_args, max_iter=max_iter, tol=tol, w_init=w_init, whiten_solver=whiten_solver, random_state=random_state)
A:sklearn.decomposition._fastica.S->FastICA(n_components=n_components, algorithm=algorithm, whiten=whiten, fun=fun, fun_args=fun_args, max_iter=max_iter, tol=tol, w_init=w_init, whiten_solver=whiten_solver, random_state=random_state)._fit_transform(X, compute_sources=compute_sources)
A:sklearn.decomposition._fastica.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._fastica.n_components->min(n_samples, n_features)
A:sklearn.decomposition._fastica.X_mean->XT.mean(axis=-1)
A:sklearn.decomposition._fastica.(d, u)->scipy.linalg.eigh(XT.dot(X))
A:sklearn.decomposition._fastica.X1->as_float_array(XT, copy=False)
A:sklearn.decomposition._fastica.w_init->numpy.asarray(w_init)
A:sklearn.decomposition._fastica.(W, n_iter)->_ica_def(X1, **kwargs)
A:sklearn.decomposition._fastica.S_std->numpy.std(S, axis=0, keepdims=True)
A:sklearn.decomposition._fastica.self.components_->numpy.dot(W, K)
A:sklearn.decomposition._fastica.self.mixing_->scipy.linalg.pinv(self.components_, check_finite=False)
A:sklearn.decomposition._fastica.X->numpy.dot(X, self.mixing_.T)
sklearn.decomposition.FastICA(self,n_components=None,*,algorithm='parallel',whiten='unit-variance',fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,whiten_solver='svd',random_state=None)
sklearn.decomposition.FastICA._fit_transform(self,X,compute_sources=False)
sklearn.decomposition.FastICA._more_tags(self)
sklearn.decomposition.FastICA._n_features_out(self)
sklearn.decomposition.FastICA.fit(self,X,y=None)
sklearn.decomposition.FastICA.fit_transform(self,X,y=None)
sklearn.decomposition.FastICA.inverse_transform(self,X,copy=True)
sklearn.decomposition.FastICA.transform(self,X,copy=True)
sklearn.decomposition._fastica.FastICA(self,n_components=None,*,algorithm='parallel',whiten='unit-variance',fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,whiten_solver='svd',random_state=None)
sklearn.decomposition._fastica.FastICA.__init__(self,n_components=None,*,algorithm='parallel',whiten='unit-variance',fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,whiten_solver='svd',random_state=None)
sklearn.decomposition._fastica.FastICA._fit_transform(self,X,compute_sources=False)
sklearn.decomposition._fastica.FastICA._more_tags(self)
sklearn.decomposition._fastica.FastICA._n_features_out(self)
sklearn.decomposition._fastica.FastICA.fit(self,X,y=None)
sklearn.decomposition._fastica.FastICA.fit_transform(self,X,y=None)
sklearn.decomposition._fastica.FastICA.inverse_transform(self,X,copy=True)
sklearn.decomposition._fastica.FastICA.transform(self,X,copy=True)
sklearn.decomposition._fastica._cube(x,fun_args)
sklearn.decomposition._fastica._exp(x,fun_args)
sklearn.decomposition._fastica._gs_decorrelation(w,W,j)
sklearn.decomposition._fastica._ica_def(X,tol,g,fun_args,max_iter,w_init)
sklearn.decomposition._fastica._ica_par(X,tol,g,fun_args,max_iter,w_init)
sklearn.decomposition._fastica._logcosh(x,fun_args=None)
sklearn.decomposition._fastica._sym_decorrelation(W)
sklearn.decomposition._fastica.fastica(X,n_components=None,*,algorithm='parallel',whiten='unit-variance',fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,whiten_solver='svd',random_state=None,return_X_mean=False,compute_sources=True,return_n_iter=False)
sklearn.decomposition.fastica(X,n_components=None,*,algorithm='parallel',whiten='unit-variance',fun='logcosh',fun_args=None,max_iter=200,tol=0.0001,w_init=None,whiten_solver='svd',random_state=None,return_X_mean=False,compute_sources=True,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_dict_learning.py----------------------------------------
A:sklearn.decomposition._dict_learning.err_mgt->numpy.seterr(all='ignore')
A:sklearn.decomposition._dict_learning.lasso_lars->LassoLars(alpha=alpha, fit_intercept=False, verbose=verbose, precompute=gram, fit_path=False, positive=positive, max_iter=max_iter)
A:sklearn.decomposition._dict_learning.clf->Lasso(alpha=alpha, fit_intercept=False, precompute=gram, max_iter=max_iter, warm_start=True, positive=positive)
A:sklearn.decomposition._dict_learning.init->numpy.array(init)
A:sklearn.decomposition._dict_learning.lars->Lars(fit_intercept=False, verbose=verbose, precompute=gram, n_nonzero_coefs=int(regularization), fit_path=False)
A:sklearn.decomposition._dict_learning.dictionary->self._initialize_dict(X, self._random_state)
A:sklearn.decomposition._dict_learning.X->self._validate_data(X, dtype=[np.float64, np.float32], order='C', reset=not has_components)
A:sklearn.decomposition._dict_learning.regularization->min(max(n_features / 10, 1), n_components)
A:sklearn.decomposition._dict_learning.gram->numpy.dot(dictionary, dictionary.T)
A:sklearn.decomposition._dict_learning.cov->numpy.dot(dictionary, X.T)
A:sklearn.decomposition._dict_learning.code->_sparse_encode(X, dictionary, algorithm=self._fit_algorithm, alpha=self.alpha, n_jobs=self.n_jobs, positive=self.positive_code, max_iter=self.transform_max_iter, verbose=self.verbose)
A:sklearn.decomposition._dict_learning.slices->list(gen_even_slices(n_samples, effective_n_jobs(n_jobs)))
A:sklearn.decomposition._dict_learning.code_views->Parallel(n_jobs=n_jobs, verbose=verbose)((delayed(_sparse_encode_precomputed)(X[this_slice], dictionary, gram=gram, cov=cov[:, this_slice] if cov is not None else None, algorithm=algorithm, regularization=regularization, copy_cov=copy_cov, init=init[this_slice] if init is not None else None, max_iter=max_iter, verbose=verbose, positive=positive) for this_slice in slices))
A:sklearn.decomposition._dict_learning.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._dict_learning.noise->check_random_state(self.random_state).normal(0, noise_level, size=len(newd))
A:sklearn.decomposition._dict_learning.t0->time.time()
A:sklearn.decomposition._dict_learning.(code, S, dictionary)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.decomposition._dict_learning.(code, dictionary)->svd_flip(code, dictionary)
A:sklearn.decomposition._dict_learning.r->len(dictionary)
A:sklearn.decomposition._dict_learning.est->MiniBatchDictionaryLearning(n_components=n_components, alpha=alpha, max_iter=max_iter, n_jobs=n_jobs, fit_algorithm=method, batch_size=batch_size, shuffle=shuffle, dict_init=dict_init, random_state=random_state, transform_algorithm=transform_algorithm, transform_alpha=alpha, positive_code=positive_code, positive_dict=positive_dict, transform_max_iter=method_max_iter, verbose=verbose, callback=callback, tol=tol, max_no_improvement=max_no_improvement).fit(X)
A:sklearn.decomposition._dict_learning.estimator->DictionaryLearning(n_components=n_components, alpha=alpha, max_iter=max_iter, tol=tol, fit_algorithm=method, n_jobs=n_jobs, dict_init=dict_init, callback=callback, code_init=code_init, verbose=verbose, random_state=random_state, positive_code=positive_code, positive_dict=positive_dict, transform_max_iter=method_max_iter).set_output(transform='default')
A:sklearn.decomposition._dict_learning.split_code->numpy.empty((n_samples, 2 * n_features))
A:sklearn.decomposition._dict_learning.split_code[:, :n_features]->numpy.maximum(code, 0)
A:sklearn.decomposition._dict_learning.(V, U, E, self.n_iter_)->_dict_learning(X, n_components, alpha=self.alpha, tol=self.tol, max_iter=self.max_iter, method=method, method_max_iter=self.transform_max_iter, n_jobs=self.n_jobs, code_init=self.code_init, dict_init=self.dict_init, callback=self.callback, verbose=self.verbose, random_state=random_state, return_n_iter=True, positive_dict=self.positive_dict, positive_code=self.positive_code)
A:sklearn.decomposition._dict_learning.self._batch_size->min(self.batch_size, X.shape[0])
A:sklearn.decomposition._dict_learning.(_, S, dictionary)->randomized_svd(X, self._n_components, random_state=random_state)
A:sklearn.decomposition._dict_learning.alpha->min(alpha, 1)
A:sklearn.decomposition._dict_learning.self._random_state->check_random_state(self.random_state)
A:sklearn.decomposition._dict_learning.old_dict->self._initialize_dict(X, self._random_state).copy()
A:sklearn.decomposition._dict_learning.X_train->self._validate_data(X, dtype=[np.float64, np.float32], order='C', reset=not has_components).copy()
A:sklearn.decomposition._dict_learning.self._A->numpy.zeros((self._n_components, self._n_components), dtype=X.dtype)
A:sklearn.decomposition._dict_learning.self._B->numpy.zeros((X.shape[1], self._n_components), dtype=X.dtype)
A:sklearn.decomposition._dict_learning.batches->itertools.cycle(batches)
A:sklearn.decomposition._dict_learning.n_steps_per_iter->int(np.ceil(n_samples / self._batch_size))
A:sklearn.decomposition._dict_learning.batch_cost->self._minibatch_step(X_batch, dictionary, self._random_state, i)
A:sklearn.decomposition._dict_learning.self.n_iter_->numpy.ceil(self.n_steps_ / n_steps_per_iter)
A:sklearn.decomposition._dict_learning.has_components->hasattr(self, 'components_')
sklearn.decomposition.DictionaryLearning(self,n_components=None,*,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=None,code_init=None,dict_init=None,callback=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000)
sklearn.decomposition.DictionaryLearning._more_tags(self)
sklearn.decomposition.DictionaryLearning._n_features_out(self)
sklearn.decomposition.DictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.DictionaryLearning.fit_transform(self,X,y=None)
sklearn.decomposition.MiniBatchDictionaryLearning(self,n_components=None,*,alpha=1,max_iter=1000,fit_algorithm='lars',n_jobs=None,batch_size=256,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000,callback=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition.MiniBatchDictionaryLearning._check_convergence(self,X,batch_cost,new_dict,old_dict,n_samples,step,n_steps)
sklearn.decomposition.MiniBatchDictionaryLearning._check_params(self,X)
sklearn.decomposition.MiniBatchDictionaryLearning._initialize_dict(self,X,random_state)
sklearn.decomposition.MiniBatchDictionaryLearning._minibatch_step(self,X,dictionary,random_state,step)
sklearn.decomposition.MiniBatchDictionaryLearning._more_tags(self)
sklearn.decomposition.MiniBatchDictionaryLearning._n_features_out(self)
sklearn.decomposition.MiniBatchDictionaryLearning._update_inner_stats(self,X,code,batch_size,step)
sklearn.decomposition.MiniBatchDictionaryLearning.fit(self,X,y=None)
sklearn.decomposition.MiniBatchDictionaryLearning.partial_fit(self,X,y=None)
sklearn.decomposition.SparseCoder(self,dictionary,*,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=None,positive_code=False,transform_max_iter=1000)
sklearn.decomposition.SparseCoder._more_tags(self)
sklearn.decomposition.SparseCoder._n_features_out(self)
sklearn.decomposition.SparseCoder.fit(self,X,y=None)
sklearn.decomposition.SparseCoder.n_components_(self)
sklearn.decomposition.SparseCoder.n_features_in_(self)
sklearn.decomposition.SparseCoder.transform(self,X,y=None)
sklearn.decomposition._dict_learning.DictionaryLearning(self,n_components=None,*,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=None,code_init=None,dict_init=None,callback=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000)
sklearn.decomposition._dict_learning.DictionaryLearning.__init__(self,n_components=None,*,alpha=1,max_iter=1000,tol=1e-08,fit_algorithm='lars',transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,n_jobs=None,code_init=None,dict_init=None,callback=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000)
sklearn.decomposition._dict_learning.DictionaryLearning._more_tags(self)
sklearn.decomposition._dict_learning.DictionaryLearning._n_features_out(self)
sklearn.decomposition._dict_learning.DictionaryLearning.fit(self,X,y=None)
sklearn.decomposition._dict_learning.DictionaryLearning.fit_transform(self,X,y=None)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning(self,n_components=None,*,alpha=1,max_iter=1000,fit_algorithm='lars',n_jobs=None,batch_size=256,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000,callback=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.__init__(self,n_components=None,*,alpha=1,max_iter=1000,fit_algorithm='lars',n_jobs=None,batch_size=256,shuffle=True,dict_init=None,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,verbose=False,split_sign=False,random_state=None,positive_code=False,positive_dict=False,transform_max_iter=1000,callback=None,tol=0.001,max_no_improvement=10)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._check_convergence(self,X,batch_cost,new_dict,old_dict,n_samples,step,n_steps)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._check_params(self,X)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._initialize_dict(self,X,random_state)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._minibatch_step(self,X,dictionary,random_state,step)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._more_tags(self)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._n_features_out(self)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning._update_inner_stats(self,X,code,batch_size,step)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.fit(self,X,y=None)
sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning.partial_fit(self,X,y=None)
sklearn.decomposition._dict_learning.SparseCoder(self,dictionary,*,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=None,positive_code=False,transform_max_iter=1000)
sklearn.decomposition._dict_learning.SparseCoder.__init__(self,dictionary,*,transform_algorithm='omp',transform_n_nonzero_coefs=None,transform_alpha=None,split_sign=False,n_jobs=None,positive_code=False,transform_max_iter=1000)
sklearn.decomposition._dict_learning.SparseCoder._more_tags(self)
sklearn.decomposition._dict_learning.SparseCoder._n_features_out(self)
sklearn.decomposition._dict_learning.SparseCoder.fit(self,X,y=None)
sklearn.decomposition._dict_learning.SparseCoder.n_components_(self)
sklearn.decomposition._dict_learning.SparseCoder.n_features_in_(self)
sklearn.decomposition._dict_learning.SparseCoder.transform(self,X,y=None)
sklearn.decomposition._dict_learning._BaseSparseCoding(self,transform_algorithm,transform_n_nonzero_coefs,transform_alpha,split_sign,n_jobs,positive_code,transform_max_iter)
sklearn.decomposition._dict_learning._BaseSparseCoding.__init__(self,transform_algorithm,transform_n_nonzero_coefs,transform_alpha,split_sign,n_jobs,positive_code,transform_max_iter)
sklearn.decomposition._dict_learning._BaseSparseCoding._transform(self,X,dictionary)
sklearn.decomposition._dict_learning._BaseSparseCoding.transform(self,X)
sklearn.decomposition._dict_learning._check_positive_coding(method,positive)
sklearn.decomposition._dict_learning._dict_learning(X,n_components,*,alpha,max_iter,tol,method,n_jobs,dict_init,code_init,callback,verbose,random_state,return_n_iter,positive_dict,positive_code,method_max_iter)
sklearn.decomposition._dict_learning._sparse_encode(X,dictionary,*,gram=None,cov=None,algorithm='lasso_lars',n_nonzero_coefs=None,alpha=None,copy_cov=True,init=None,max_iter=1000,n_jobs=None,verbose=0,positive=False)
sklearn.decomposition._dict_learning._sparse_encode_precomputed(X,dictionary,*,gram=None,cov=None,algorithm='lasso_lars',regularization=None,copy_cov=True,init=None,max_iter=1000,verbose=0,positive=False)
sklearn.decomposition._dict_learning._update_dict(dictionary,Y,code,A=None,B=None,verbose=False,random_state=None,positive=False)
sklearn.decomposition._dict_learning.dict_learning(X,n_components,*,alpha,max_iter=100,tol=1e-08,method='lars',n_jobs=None,dict_init=None,code_init=None,callback=None,verbose=False,random_state=None,return_n_iter=False,positive_dict=False,positive_code=False,method_max_iter=1000)
sklearn.decomposition._dict_learning.dict_learning_online(X,n_components=2,*,alpha=1,max_iter=100,return_code=True,dict_init=None,callback=None,batch_size=256,verbose=False,shuffle=True,n_jobs=None,method='lars',random_state=None,positive_dict=False,positive_code=False,method_max_iter=1000,tol=0.001,max_no_improvement=10)
sklearn.decomposition._dict_learning.sparse_encode(X,dictionary,*,gram=None,cov=None,algorithm='lasso_lars',n_nonzero_coefs=None,alpha=None,copy_cov=True,init=None,max_iter=1000,n_jobs=None,check_input=True,verbose=0,positive=False)
sklearn.decomposition.dict_learning(X,n_components,*,alpha,max_iter=100,tol=1e-08,method='lars',n_jobs=None,dict_init=None,code_init=None,callback=None,verbose=False,random_state=None,return_n_iter=False,positive_dict=False,positive_code=False,method_max_iter=1000)
sklearn.decomposition.dict_learning_online(X,n_components=2,*,alpha=1,max_iter=100,return_code=True,dict_init=None,callback=None,batch_size=256,verbose=False,shuffle=True,n_jobs=None,method='lars',random_state=None,positive_dict=False,positive_code=False,method_max_iter=1000,tol=0.001,max_no_improvement=10)
sklearn.decomposition.sparse_encode(X,dictionary,*,gram=None,cov=None,algorithm='lasso_lars',n_nonzero_coefs=None,alpha=None,copy_cov=True,init=None,max_iter=1000,n_jobs=None,check_input=True,verbose=0,positive=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_factor_analysis.py----------------------------------------
A:sklearn.decomposition._factor_analysis.X->self._validate_data(X, reset=False)
A:sklearn.decomposition._factor_analysis.self.mean_->numpy.mean(X, axis=0)
A:sklearn.decomposition._factor_analysis.nsqrt->sqrt(n_samples)
A:sklearn.decomposition._factor_analysis.var->numpy.var(X, axis=0)
A:sklearn.decomposition._factor_analysis.psi->numpy.maximum(var - np.sum(W ** 2, axis=0), SMALL)
A:sklearn.decomposition._factor_analysis.(_, s, Vt)->randomized_svd(X, n_components, random_state=random_state, n_iter=self.iterated_power)
A:sklearn.decomposition._factor_analysis.random_state->check_random_state(self.random_state)
A:sklearn.decomposition._factor_analysis.(s, Vt, unexp_var)->my_svd(X / (sqrt_psi * nsqrt))
A:sklearn.decomposition._factor_analysis.self.components_->self._rotate(W)
A:sklearn.decomposition._factor_analysis.Ih->numpy.eye(len(self.components_))
A:sklearn.decomposition._factor_analysis.cov_z->scipy.linalg.inv(Ih + np.dot(Wpsi, self.components_.T))
A:sklearn.decomposition._factor_analysis.tmp->numpy.dot(X_transformed, Wpsi.T)
A:sklearn.decomposition._factor_analysis.X_transformed->numpy.dot(tmp, cov_z)
A:sklearn.decomposition._factor_analysis.cov->numpy.dot(self.components_.T, self.components_)
A:sklearn.decomposition._factor_analysis.precision->self.get_precision()
A:sklearn.decomposition._factor_analysis.rotation_matrix->numpy.dot(u, v)
A:sklearn.decomposition._factor_analysis.comp_rot->numpy.dot(components, rotation_matrix)
A:sklearn.decomposition._factor_analysis.(u, s, v)->numpy.linalg.svd(np.dot(components.T, comp_rot ** 3 - tmp))
A:sklearn.decomposition._factor_analysis.var_new->numpy.sum(s)
sklearn.decomposition.FactorAnalysis(self,n_components=None,*,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,rotation=None,random_state=0)
sklearn.decomposition.FactorAnalysis._n_features_out(self)
sklearn.decomposition.FactorAnalysis._rotate(self,components,n_components=None,tol=1e-06)
sklearn.decomposition.FactorAnalysis.fit(self,X,y=None)
sklearn.decomposition.FactorAnalysis.get_covariance(self)
sklearn.decomposition.FactorAnalysis.get_precision(self)
sklearn.decomposition.FactorAnalysis.score(self,X,y=None)
sklearn.decomposition.FactorAnalysis.score_samples(self,X)
sklearn.decomposition.FactorAnalysis.transform(self,X)
sklearn.decomposition._factor_analysis.FactorAnalysis(self,n_components=None,*,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,rotation=None,random_state=0)
sklearn.decomposition._factor_analysis.FactorAnalysis.__init__(self,n_components=None,*,tol=0.01,copy=True,max_iter=1000,noise_variance_init=None,svd_method='randomized',iterated_power=3,rotation=None,random_state=0)
sklearn.decomposition._factor_analysis.FactorAnalysis._n_features_out(self)
sklearn.decomposition._factor_analysis.FactorAnalysis._rotate(self,components,n_components=None,tol=1e-06)
sklearn.decomposition._factor_analysis.FactorAnalysis.fit(self,X,y=None)
sklearn.decomposition._factor_analysis.FactorAnalysis.get_covariance(self)
sklearn.decomposition._factor_analysis.FactorAnalysis.get_precision(self)
sklearn.decomposition._factor_analysis.FactorAnalysis.score(self,X,y=None)
sklearn.decomposition._factor_analysis.FactorAnalysis.score_samples(self,X)
sklearn.decomposition._factor_analysis.FactorAnalysis.transform(self,X)
sklearn.decomposition._factor_analysis._ortho_rotation(components,method='varimax',tol=1e-06,max_iter=100)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_kernel_pca.py----------------------------------------
A:sklearn.decomposition._kernel_pca.K->self._get_kernel(X, self.X_transformed_fit_)
A:sklearn.decomposition._kernel_pca.n_components->min(K.shape[0], self.n_components)
A:sklearn.decomposition._kernel_pca.(self.eigenvalues_, self.eigenvectors_)->_randomized_eigsh(K, n_components=n_components, n_iter=self.iterated_power, random_state=self.random_state, selection='module')
A:sklearn.decomposition._kernel_pca.v0->_init_arpack_v0(K.shape[0], self.random_state)
A:sklearn.decomposition._kernel_pca.self.eigenvalues_->_check_psd_eigenvalues(self.eigenvalues_, enable_warnings=False)
A:sklearn.decomposition._kernel_pca.(self.eigenvectors_, _)->svd_flip(self.eigenvectors_, np.zeros_like(self.eigenvectors_).T)
A:sklearn.decomposition._kernel_pca.self.dual_coef_->scipy.linalg.solve(K, X, assume_a='pos', overwrite_a=True)
A:sklearn.decomposition._kernel_pca.X->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.decomposition._kernel_pca.self._centerer->KernelCenterer().set_output(transform='default')
A:sklearn.decomposition._kernel_pca.non_zeros->numpy.flatnonzero(self.eigenvalues_)
A:sklearn.decomposition._kernel_pca.scaled_alphas->numpy.zeros_like(self.eigenvectors_)
sklearn.decomposition.KernelPCA(self,n_components=None,*,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,iterated_power='auto',remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=None)
sklearn.decomposition.KernelPCA._fit_inverse_transform(self,X_transformed,X)
sklearn.decomposition.KernelPCA._fit_transform(self,K)
sklearn.decomposition.KernelPCA._get_kernel(self,X,Y=None)
sklearn.decomposition.KernelPCA._more_tags(self)
sklearn.decomposition.KernelPCA._n_features_out(self)
sklearn.decomposition.KernelPCA.fit(self,X,y=None)
sklearn.decomposition.KernelPCA.fit_transform(self,X,y=None,**params)
sklearn.decomposition.KernelPCA.inverse_transform(self,X)
sklearn.decomposition.KernelPCA.transform(self,X)
sklearn.decomposition._kernel_pca.KernelPCA(self,n_components=None,*,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,iterated_power='auto',remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=None)
sklearn.decomposition._kernel_pca.KernelPCA.__init__(self,n_components=None,*,kernel='linear',gamma=None,degree=3,coef0=1,kernel_params=None,alpha=1.0,fit_inverse_transform=False,eigen_solver='auto',tol=0,max_iter=None,iterated_power='auto',remove_zero_eig=False,random_state=None,copy_X=True,n_jobs=None)
sklearn.decomposition._kernel_pca.KernelPCA._fit_inverse_transform(self,X_transformed,X)
sklearn.decomposition._kernel_pca.KernelPCA._fit_transform(self,K)
sklearn.decomposition._kernel_pca.KernelPCA._get_kernel(self,X,Y=None)
sklearn.decomposition._kernel_pca.KernelPCA._more_tags(self)
sklearn.decomposition._kernel_pca.KernelPCA._n_features_out(self)
sklearn.decomposition._kernel_pca.KernelPCA.fit(self,X,y=None)
sklearn.decomposition._kernel_pca.KernelPCA.fit_transform(self,X,y=None,**params)
sklearn.decomposition._kernel_pca.KernelPCA.inverse_transform(self,X)
sklearn.decomposition._kernel_pca.KernelPCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_base.py----------------------------------------
A:sklearn.decomposition._base.(xp, _)->get_namespace(X)
A:sklearn.decomposition._base.exp_var_diff->xp.where(exp_var > self.noise_variance_, exp_var_diff, xp.asarray(0.0, device=device(exp_var)))
A:sklearn.decomposition._base.(xp, is_array_api_compliant)->get_namespace(self.components_)
A:sklearn.decomposition._base.X->_implicit_column_offset(X, self.mean_)
sklearn.decomposition._base._BasePCA(ClassNamePrefixFeaturesOutMixin,TransformerMixin,BaseEstimator,metaclass=ABCMeta)
sklearn.decomposition._base._BasePCA._n_features_out(self)
sklearn.decomposition._base._BasePCA.fit(self,X,y=None)
sklearn.decomposition._base._BasePCA.get_covariance(self)
sklearn.decomposition._base._BasePCA.get_precision(self)
sklearn.decomposition._base._BasePCA.inverse_transform(self,X)
sklearn.decomposition._base._BasePCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py----------------------------------------
A:sklearn.decomposition._nmf.A->check_array(A)
A:sklearn.decomposition._nmf.beta->_beta_loss_to_float(beta)
A:sklearn.decomposition._nmf.X->self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=[np.float64, np.float32], reset=not has_components)
A:sklearn.decomposition._nmf.W->self._solve_W(X, self.components_, self._transform_max_iter)
A:sklearn.decomposition._nmf.H->_multiplicative_update_h(X, W, H, beta_loss=beta_loss, l1_reg_H=l1_reg_H, l2_reg_H=l2_reg_H, gamma=gamma)
A:sklearn.decomposition._nmf.norm_X->numpy.dot(X.data, X.data)
A:sklearn.decomposition._nmf.norm_WH->trace_dot(np.linalg.multi_dot([W.T, W, H]), H)
A:sklearn.decomposition._nmf.cross_prod->trace_dot(X @ H.T, W)
A:sklearn.decomposition._nmf.WH->_special_sparse_dot(W, H, X).copy()
A:sklearn.decomposition._nmf.WH_data->_special_sparse_dot(W, H, X).copy().ravel()
A:sklearn.decomposition._nmf.X_data->self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=[np.float64, np.float32], reset=not has_components).ravel()
A:sklearn.decomposition._nmf.sum_WH->numpy.dot(np.sum(W, axis=0), np.sum(H, axis=1))
A:sklearn.decomposition._nmf.res->max(res, 0)
A:sklearn.decomposition._nmf.sum_WH_beta->numpy.sum(WH ** beta)
A:sklearn.decomposition._nmf.sum_X_WH->numpy.dot(X_data, WH_data ** (beta - 1))
A:sklearn.decomposition._nmf.(ii, jj)->self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=[np.float64, np.float32], reset=not has_components).nonzero()
A:sklearn.decomposition._nmf.dot_vals->numpy.empty(n_vals)
A:sklearn.decomposition._nmf.batch_size->max(n_components, n_vals // n_components)
A:sklearn.decomposition._nmf.batch->slice(start, start + batch_size)
A:sklearn.decomposition._nmf.dot_vals[batch]->numpy.multiply(W[ii[batch], :], H.T[jj[batch], :]).sum(axis=1)
A:sklearn.decomposition._nmf.avg->numpy.sqrt(X.mean() / self._n_components)
A:sklearn.decomposition._nmf.rng->check_random_state(random_state)
A:sklearn.decomposition._nmf.(U, S, V)->randomized_svd(X, n_components, random_state=random_state)
A:sklearn.decomposition._nmf.lbd->numpy.sqrt(S[j] * sigma)
A:sklearn.decomposition._nmf.W[W == 0]->abs(avg * rng.standard_normal(size=len(W[W == 0])) / 100)
A:sklearn.decomposition._nmf.H[H == 0]->abs(avg * rng.standard_normal(size=len(H[H == 0])) / 100)
A:sklearn.decomposition._nmf.HHt->numpy.dot(H, H.T)
A:sklearn.decomposition._nmf.XHt->safe_sparse_dot(X, H.T)
A:sklearn.decomposition._nmf.permutation->numpy.asarray(permutation, dtype=np.intp)
A:sklearn.decomposition._nmf.Ht->check_array(H.T, order='C')
A:sklearn.decomposition._nmf.numerator->safe_sparse_dot(W.T, WH_safe_X)
A:sklearn.decomposition._nmf.denominator->numpy.linalg.multi_dot([W.T, W, H])
A:sklearn.decomposition._nmf.WH_safe_X->_special_sparse_dot(W, H, X)
A:sklearn.decomposition._nmf.H_sum->numpy.sum(H, axis=1)
A:sklearn.decomposition._nmf.WHHt->numpy.dot(WH, H.T)
A:sklearn.decomposition._nmf.WHi->numpy.dot(W, H[:, i])
A:sklearn.decomposition._nmf.WHHt[i, :]->numpy.dot(WHi, H.T)
A:sklearn.decomposition._nmf.W_sum->numpy.sum(W, axis=0)
A:sklearn.decomposition._nmf.WtWH->numpy.dot(W.T, WH)
A:sklearn.decomposition._nmf.WtWH[:, i]->numpy.dot(W.T, WHi)
A:sklearn.decomposition._nmf.start_time->time.time()
A:sklearn.decomposition._nmf.beta_loss->_beta_loss_to_float(beta_loss)
A:sklearn.decomposition._nmf.error_at_init->_beta_divergence(X, W, H, beta_loss, square_root=True)
A:sklearn.decomposition._nmf.(W, H_sum, HHt, XHt)->_multiplicative_update_w(X, W, H, beta_loss=beta_loss, l1_reg_W=l1_reg_W, l2_reg_W=l2_reg_W, gamma=gamma, H_sum=H_sum, HHt=HHt, XHt=XHt, update_H=update_H)
A:sklearn.decomposition._nmf.error->_beta_divergence(X, W, H, beta_loss, square_root=True)
A:sklearn.decomposition._nmf.iter_time->time.time()
A:sklearn.decomposition._nmf.end_time->time.time()
A:sklearn.decomposition._nmf.est->NMF(n_components=n_components, init=init, solver=solver, beta_loss=beta_loss, tol=tol, max_iter=max_iter, random_state=random_state, alpha_W=alpha_W, alpha_H=alpha_H, l1_ratio=l1_ratio, verbose=verbose, shuffle=shuffle)
A:sklearn.decomposition._nmf.(W, H, n_iter)->_fit_coordinate_descent(X, W, H, self.tol, self.max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H=update_H, verbose=self.verbose, shuffle=self.shuffle, random_state=self.random_state)
A:sklearn.decomposition._nmf.self._beta_loss->_beta_loss_to_float(self.beta_loss)
A:sklearn.decomposition._nmf.(W, H)->self._check_w_h(X, W, H, update_H)
A:sklearn.decomposition._nmf.self.reconstruction_err_->_beta_divergence(X, W, H, self._beta_loss, square_root=True)
A:sklearn.decomposition._nmf.(l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H)->self._compute_regularization(X)
A:sklearn.decomposition._nmf.(W, H, n_iter, *_)->_fit_multiplicative_update(X, W, H, self._beta_loss, self.max_iter, self.tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, self.verbose)
A:sklearn.decomposition._nmf.(W, *_)->_multiplicative_update_w(X, W, H, self._beta_loss, l1_reg_W, l2_reg_W, self._gamma)
A:sklearn.decomposition._nmf.self._batch_size->min(self.batch_size, X.shape[0])
A:sklearn.decomposition._nmf.W_buffer->self._solve_W(X, self.components_, self._transform_max_iter).copy()
A:sklearn.decomposition._nmf.(l1_reg_W, _, l2_reg_W, _)->self._compute_regularization(X)
A:sklearn.decomposition._nmf.H[:]->_multiplicative_update_h(X, W, H, beta_loss=self._beta_loss, l1_reg_H=l1_reg_H, l2_reg_H=l2_reg_H, gamma=self._gamma, A=self._components_numerator, B=self._components_denominator, rho=self._rho)
A:sklearn.decomposition._nmf.alpha->min(alpha, 1)
A:sklearn.decomposition._nmf.(W, H, n_iter, n_steps)->self._fit_transform(X, W=W, H=H)
A:sklearn.decomposition._nmf.H_buffer->_multiplicative_update_h(X, W, H, beta_loss=beta_loss, l1_reg_H=l1_reg_H, l2_reg_H=l2_reg_H, gamma=gamma).copy()
A:sklearn.decomposition._nmf.self._components_numerator->_multiplicative_update_h(X, W, H, beta_loss=beta_loss, l1_reg_H=l1_reg_H, l2_reg_H=l2_reg_H, gamma=gamma).copy()
A:sklearn.decomposition._nmf.self._components_denominator->numpy.ones(H.shape, dtype=H.dtype)
A:sklearn.decomposition._nmf.batches->itertools.cycle(batches)
A:sklearn.decomposition._nmf.n_steps_per_iter->int(np.ceil(n_samples / self._batch_size))
A:sklearn.decomposition._nmf.batch_cost->self._minibatch_step(X[batch], W[batch], H, update_H)
A:sklearn.decomposition._nmf.n_iter->int(np.ceil(n_steps / n_steps_per_iter))
A:sklearn.decomposition._nmf.has_components->hasattr(self, 'components_')
A:sklearn.decomposition._nmf.(_, H)->self._check_w_h(X, W=W, H=H, update_H=True)
sklearn.decomposition.MiniBatchNMF(self,n_components='warn',*,init=None,batch_size=1024,beta_loss='frobenius',tol=0.0001,max_no_improvement=10,max_iter=200,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,forget_factor=0.7,fresh_restarts=False,fresh_restarts_max_iter=30,transform_max_iter=None,random_state=None,verbose=0)
sklearn.decomposition.MiniBatchNMF._check_params(self,X)
sklearn.decomposition.MiniBatchNMF._fit_transform(self,X,W=None,H=None,update_H=True)
sklearn.decomposition.MiniBatchNMF._minibatch_convergence(self,X,batch_cost,H,H_buffer,n_samples,step,n_steps)
sklearn.decomposition.MiniBatchNMF._minibatch_step(self,X,W,H,update_H)
sklearn.decomposition.MiniBatchNMF._solve_W(self,X,H,max_iter)
sklearn.decomposition.MiniBatchNMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition.MiniBatchNMF.partial_fit(self,X,y=None,W=None,H=None)
sklearn.decomposition.MiniBatchNMF.transform(self,X)
sklearn.decomposition.NMF(self,n_components='warn',*,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition.NMF._check_params(self,X)
sklearn.decomposition.NMF._fit_transform(self,X,y=None,W=None,H=None,update_H=True)
sklearn.decomposition.NMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition.NMF.transform(self,X)
sklearn.decomposition._nmf.MiniBatchNMF(self,n_components='warn',*,init=None,batch_size=1024,beta_loss='frobenius',tol=0.0001,max_no_improvement=10,max_iter=200,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,forget_factor=0.7,fresh_restarts=False,fresh_restarts_max_iter=30,transform_max_iter=None,random_state=None,verbose=0)
sklearn.decomposition._nmf.MiniBatchNMF.__init__(self,n_components='warn',*,init=None,batch_size=1024,beta_loss='frobenius',tol=0.0001,max_no_improvement=10,max_iter=200,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,forget_factor=0.7,fresh_restarts=False,fresh_restarts_max_iter=30,transform_max_iter=None,random_state=None,verbose=0)
sklearn.decomposition._nmf.MiniBatchNMF._check_params(self,X)
sklearn.decomposition._nmf.MiniBatchNMF._fit_transform(self,X,W=None,H=None,update_H=True)
sklearn.decomposition._nmf.MiniBatchNMF._minibatch_convergence(self,X,batch_cost,H,H_buffer,n_samples,step,n_steps)
sklearn.decomposition._nmf.MiniBatchNMF._minibatch_step(self,X,W,H,update_H)
sklearn.decomposition._nmf.MiniBatchNMF._solve_W(self,X,H,max_iter)
sklearn.decomposition._nmf.MiniBatchNMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition._nmf.MiniBatchNMF.partial_fit(self,X,y=None,W=None,H=None)
sklearn.decomposition._nmf.MiniBatchNMF.transform(self,X)
sklearn.decomposition._nmf.NMF(self,n_components='warn',*,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition._nmf.NMF.__init__(self,n_components='warn',*,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.decomposition._nmf.NMF._check_params(self,X)
sklearn.decomposition._nmf.NMF._fit_transform(self,X,y=None,W=None,H=None,update_H=True)
sklearn.decomposition._nmf.NMF.fit_transform(self,X,y=None,W=None,H=None)
sklearn.decomposition._nmf.NMF.transform(self,X)
sklearn.decomposition._nmf._BaseNMF(self,n_components='warn',*,init=None,beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,verbose=0)
sklearn.decomposition._nmf._BaseNMF.__init__(self,n_components='warn',*,init=None,beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,verbose=0)
sklearn.decomposition._nmf._BaseNMF._check_params(self,X)
sklearn.decomposition._nmf._BaseNMF._check_w_h(self,X,W,H,update_H)
sklearn.decomposition._nmf._BaseNMF._compute_regularization(self,X)
sklearn.decomposition._nmf._BaseNMF._more_tags(self)
sklearn.decomposition._nmf._BaseNMF._n_features_out(self)
sklearn.decomposition._nmf._BaseNMF.fit(self,X,y=None,**params)
sklearn.decomposition._nmf._BaseNMF.inverse_transform(self,Xt=None,W=None)
sklearn.decomposition._nmf._beta_divergence(X,W,H,beta,square_root=False)
sklearn.decomposition._nmf._beta_loss_to_float(beta_loss)
sklearn.decomposition._nmf._check_init(A,shape,whom)
sklearn.decomposition._nmf._fit_coordinate_descent(X,W,H,tol=0.0001,max_iter=200,l1_reg_W=0,l1_reg_H=0,l2_reg_W=0,l2_reg_H=0,update_H=True,verbose=0,shuffle=False,random_state=None)
sklearn.decomposition._nmf._fit_multiplicative_update(X,W,H,beta_loss='frobenius',max_iter=200,tol=0.0001,l1_reg_W=0,l1_reg_H=0,l2_reg_W=0,l2_reg_H=0,update_H=True,verbose=0)
sklearn.decomposition._nmf._initialize_nmf(X,n_components,init=None,eps=1e-06,random_state=None)
sklearn.decomposition._nmf._multiplicative_update_h(X,W,H,beta_loss,l1_reg_H,l2_reg_H,gamma,A=None,B=None,rho=None)
sklearn.decomposition._nmf._multiplicative_update_w(X,W,H,beta_loss,l1_reg_W,l2_reg_W,gamma,H_sum=None,HHt=None,XHt=None,update_H=True)
sklearn.decomposition._nmf._special_sparse_dot(W,H,X)
sklearn.decomposition._nmf._update_coordinate_descent(X,W,Ht,l1_reg,l2_reg,shuffle,random_state)
sklearn.decomposition._nmf.non_negative_factorization(X,W=None,H=None,n_components='warn',*,init=None,update_H=True,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,random_state=None,verbose=0,shuffle=False)
sklearn.decomposition._nmf.norm(x)
sklearn.decomposition._nmf.trace_dot(X,Y)
sklearn.decomposition.non_negative_factorization(X,W=None,H=None,n_components='warn',*,init=None,update_H=True,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,alpha_W=0.0,alpha_H='same',l1_ratio=0.0,random_state=None,verbose=0,shuffle=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py----------------------------------------
A:sklearn.decomposition._incremental_pca.X->numpy.vstack((self.singular_values_.reshape((-1, 1)) * self.components_, X, mean_correction))
A:sklearn.decomposition._incremental_pca.X_batch->X_batch.toarray().toarray()
A:sklearn.decomposition._incremental_pca.self.n_components_->min(n_samples, n_features)
A:sklearn.decomposition._incremental_pca.(col_mean, col_var, n_total_samples)->_incremental_mean_and_var(X, last_mean=self.mean_, last_variance=self.var_, last_sample_count=np.repeat(self.n_samples_seen_, X.shape[1]))
A:sklearn.decomposition._incremental_pca.col_batch_mean->numpy.mean(X, axis=0)
A:sklearn.decomposition._incremental_pca.(U, S, Vt)->scipy.linalg.svd(X, full_matrices=False, check_finite=False)
A:sklearn.decomposition._incremental_pca.(U, Vt)->svd_flip(U, Vt, u_based_decision=False)
A:sklearn.decomposition._incremental_pca.self.noise_variance_->explained_variance[self.n_components_:].mean()
sklearn.decomposition.IncrementalPCA(self,n_components=None,*,whiten=False,copy=True,batch_size=None)
sklearn.decomposition.IncrementalPCA.fit(self,X,y=None)
sklearn.decomposition.IncrementalPCA.partial_fit(self,X,y=None,check_input=True)
sklearn.decomposition.IncrementalPCA.transform(self,X)
sklearn.decomposition._incremental_pca.IncrementalPCA(self,n_components=None,*,whiten=False,copy=True,batch_size=None)
sklearn.decomposition._incremental_pca.IncrementalPCA.__init__(self,n_components=None,*,whiten=False,copy=True,batch_size=None)
sklearn.decomposition._incremental_pca.IncrementalPCA.fit(self,X,y=None)
sklearn.decomposition._incremental_pca.IncrementalPCA.partial_fit(self,X,y=None,check_input=True)
sklearn.decomposition._incremental_pca.IncrementalPCA.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_pca.iris->sklearn.datasets.load_iris()
A:sklearn.decomposition.tests.test_pca.SPARSE_MAX_COMPONENTS->min(SPARSE_M, SPARSE_N)
A:sklearn.decomposition.tests.test_pca.pca->PCA(n_components=2, svd_solver='arpack', random_state=0)
A:sklearn.decomposition.tests.test_pca.X_r->PCA(n_components=2, svd_solver='arpack', random_state=0).transform(X)
A:sklearn.decomposition.tests.test_pca.X_r2->PCA(n_components=2, svd_solver='arpack', random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_pca.cov->PCA(n_components=2, svd_solver='arpack', random_state=0).get_covariance()
A:sklearn.decomposition.tests.test_pca.precision->PCA(n_components=2, svd_solver='arpack', random_state=0).get_precision()
A:sklearn.decomposition.tests.test_pca.random_state->numpy.random.RandomState(global_random_seed)
A:sklearn.decomposition.tests.test_pca.X->X.astype(dtype_name, copy=False).astype(dtype_name, copy=False)
A:sklearn.decomposition.tests.test_pca.Xd->X.astype(dtype_name, copy=False).astype(dtype_name, copy=False).toarray()
A:sklearn.decomposition.tests.test_pca.pcad->PCA(n_components=n_components, svd_solver=svd_solver, random_state=global_random_seed)
A:sklearn.decomposition.tests.test_pca.X2->sparse_container(sp.sparse.random(SPARSE_M, SPARSE_N, random_state=random_state, density=0.01))
A:sklearn.decomposition.tests.test_pca.X2d->sparse_container(sp.sparse.random(SPARSE_M, SPARSE_N, random_state=random_state, density=0.01)).toarray()
A:sklearn.decomposition.tests.test_pca.pca_fit->PCA(n_components=10, svd_solver='arpack', random_state=global_random_seed)
A:sklearn.decomposition.tests.test_pca.pca_fit_transform->PCA(n_components=10, svd_solver='arpack', random_state=global_random_seed)
A:sklearn.decomposition.tests.test_pca.transformed_X->numpy.zeros((20, 2))
A:sklearn.decomposition.tests.test_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_pca.X_->X.astype(dtype_name, copy=False).astype(dtype_name, copy=False).copy()
A:sklearn.decomposition.tests.test_pca.X_whitened->PCA(n_components=2, svd_solver='arpack', random_state=0).fit_transform(X_.copy())
A:sklearn.decomposition.tests.test_pca.X_whitened2->PCA(n_components=2, svd_solver='arpack', random_state=0).transform(X_)
A:sklearn.decomposition.tests.test_pca.X_unwhitened->PCA(n_components=2, svd_solver='arpack', random_state=0).transform(X_)
A:sklearn.decomposition.tests.test_pca.pca_full->PCA(n_components=1, svd_solver='full').fit(X)
A:sklearn.decomposition.tests.test_pca.pca_other->PCA(n_components=30, svd_solver=svd_solver, random_state=0)
A:sklearn.decomposition.tests.test_pca.X_pca->PCA(n_components=2, svd_solver='arpack', random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_pca.X_trans->PCA(n_components=2, svd_solver='arpack', random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_pca.X_hat->numpy.dot(X_trans, pca.components_)
A:sklearn.decomposition.tests.test_pca.Yt->PCA(n_components=2, svd_solver=svd_solver).fit(X).transform(Xt)
A:sklearn.decomposition.tests.test_pca.Y->PCA(n_components=2, svd_solver='arpack', random_state=0).transform(X)
A:sklearn.decomposition.tests.test_pca.Y_inverse->PCA(n_components=2, svd_solver='arpack', random_state=0).inverse_transform(Y)
A:sklearn.decomposition.tests.test_pca.pca_fitted->PCA(n_components, svd_solver=svd_solver)
A:sklearn.decomposition.tests.test_pca.err_msg->"n_components='mle' cannot be a string with svd_solver='{}'".format(svd_solver)
A:sklearn.decomposition.tests.test_pca.ll->numpy.zeros(p)
A:sklearn.decomposition.tests.test_pca.ll1->PCA(n_components=2, svd_solver='arpack', random_state=0).score(X)
A:sklearn.decomposition.tests.test_pca.ll2->PCA(n_components=2, svd_solver='arpack', random_state=0).score(X)
A:sklearn.decomposition.tests.test_pca.ll[k]->PCA(n_components=2, svd_solver='arpack', random_state=0).score(Xt)
A:sklearn.decomposition.tests.test_pca.(X, _)->sklearn.datasets.make_classification(n_samples=20, n_features=21, random_state=42)
A:sklearn.decomposition.tests.test_pca.pca_auto->PCA(n_components=n_components, random_state=0)
A:sklearn.decomposition.tests.test_pca.pca_test->PCA(n_components=n_components, svd_solver=expected_solver, random_state=0)
A:sklearn.decomposition.tests.test_pca.X_64->numpy.random.RandomState(0).rand(1000, 4).astype(np.float64, copy=False)
A:sklearn.decomposition.tests.test_pca.X_32->numpy.random.RandomState(0).rand(1000, 4).astype(np.float64, copy=False).astype(np.float32)
A:sklearn.decomposition.tests.test_pca.pca_64->PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_i64)
A:sklearn.decomposition.tests.test_pca.pca_32->PCA(n_components=3, svd_solver=svd_solver, random_state=0).fit(X_i32)
A:sklearn.decomposition.tests.test_pca.X_i64->X_i64.astype(np.int64, copy=False).astype(np.int64, copy=False)
A:sklearn.decomposition.tests.test_pca.X_i32->X_i64.astype(np.int64, copy=False).astype(np.int64, copy=False).astype(np.int32, copy=False)
A:sklearn.decomposition.tests.test_pca.(X, y)->make_classification(random_state=42)
A:sklearn.decomposition.tests.test_pca.pca1->PCA().fit(X, y)
A:sklearn.decomposition.tests.test_pca.pca2->PCA(n_components=n_components).fit(X, y)
A:sklearn.decomposition.tests.test_pca.spectrum->numpy.array([1, 1e-30, 1e-30, 1e-30])
A:sklearn.decomposition.tests.test_pca.X[:, -1]->numpy.mean(X[:, :-1], axis=-1)
A:sklearn.decomposition.tests.test_pca.pca_skl->PCA('mle', svd_solver='full')
A:sklearn.decomposition.tests.test_pca.(_, s, _)->numpy.linalg.svd(X, full_matrices=True)
A:sklearn.decomposition.tests.test_pca.pca_randomized->PCA(n_components=1, svd_solver='randomized', n_oversamples=n_features, random_state=0).fit(X)
A:sklearn.decomposition.tests.test_pca.pca_arpack->PCA(n_components=1, svd_solver='arpack', random_state=0).fit(X)
A:sklearn.decomposition.tests.test_pca.names->PCA(n_components=2, svd_solver='arpack', random_state=0).get_feature_names_out()
A:sklearn.decomposition.tests.test_pca.true_var->numpy.var(X, ddof=1, axis=0).sum()
A:sklearn.decomposition.tests.test_pca.xp->pytest.importorskip('numpy.array_api')
A:sklearn.decomposition.tests.test_pca.iris_np->sklearn.datasets.load_iris().data.astype(dtype_name)
A:sklearn.decomposition.tests.test_pca.iris_xp->pytest.importorskip('numpy.array_api').asarray(iris.data)
A:sklearn.decomposition.tests.test_pca.precision_np->estimator.get_precision()
A:sklearn.decomposition.tests.test_pca.covariance_np->estimator.get_covariance()
A:sklearn.decomposition.tests.test_pca.estimator_xp->clone(estimator).fit(iris_xp)
A:sklearn.decomposition.tests.test_pca.precision_xp->clone(estimator).fit(iris_xp).get_precision()
A:sklearn.decomposition.tests.test_pca.covariance_xp->clone(estimator).fit(iris_xp).get_covariance()
A:sklearn.decomposition.tests.test_pca.atol->_atol_for_type(X.dtype)
A:sklearn.decomposition.tests.test_pca.est->clone(estimator)
A:sklearn.decomposition.tests.test_pca.X_xp->pytest.importorskip('numpy.array_api').asarray(X, device=device)
A:sklearn.decomposition.tests.test_pca.y_xp->pytest.importorskip('numpy.array_api').asarray(y, device=device)
A:sklearn.decomposition.tests.test_pca.est_xp->clone(est)
A:sklearn.decomposition.tests.test_pca.components_xp_np->_convert_to_numpy(components_xp, xp=xp)
A:sklearn.decomposition.tests.test_pca.explained_variance_xp_np->_convert_to_numpy(explained_variance_xp, xp=xp)
A:sklearn.decomposition.tests.test_pca.min_components->min(components_xp_np.shape[0], components_np.shape[0])
A:sklearn.decomposition.tests.test_pca.expected_msg->re.escape("Array API does not support LU factorization, falling back to QR instead. Set `power_iteration_normalizer='QR'` explicitly to silence this warning.")
sklearn.decomposition.tests.test_pca._check_fitted_pca_close(pca1,pca2,rtol)
sklearn.decomposition.tests.test_pca.check_array_api_get_precision(name,estimator,array_namespace,device,dtype_name)
sklearn.decomposition.tests.test_pca.check_pca_float_dtype_preservation(svd_solver)
sklearn.decomposition.tests.test_pca.check_pca_int_dtype_upcast_to_double(svd_solver)
sklearn.decomposition.tests.test_pca.test_array_api_error_and_warnings_on_unsupported_params()
sklearn.decomposition.tests.test_pca.test_assess_dimension_bad_rank()
sklearn.decomposition.tests.test_pca.test_assess_dimesion_rank_one()
sklearn.decomposition.tests.test_pca.test_feature_names_out()
sklearn.decomposition.tests.test_pca.test_fit_mle_too_few_samples()
sklearn.decomposition.tests.test_pca.test_infer_dim_1()
sklearn.decomposition.tests.test_pca.test_infer_dim_2()
sklearn.decomposition.tests.test_pca.test_infer_dim_3()
sklearn.decomposition.tests.test_pca.test_infer_dim_by_explained_variance(X,n_components,n_components_validated)
sklearn.decomposition.tests.test_pca.test_mle_redundant_data()
sklearn.decomposition.tests.test_pca.test_mle_simple_case()
sklearn.decomposition.tests.test_pca.test_n_components_mle(svd_solver)
sklearn.decomposition.tests.test_pca.test_n_components_mle_error(svd_solver)
sklearn.decomposition.tests.test_pca.test_n_components_none(data,solver,n_components_)
sklearn.decomposition.tests.test_pca.test_no_empty_slice_warning()
sklearn.decomposition.tests.test_pca.test_pca(svd_solver,n_components)
sklearn.decomposition.tests.test_pca.test_pca_array_api_compliance(estimator,check,array_namespace,device,dtype_name)
sklearn.decomposition.tests.test_pca.test_pca_check_projection(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_check_projection_list(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_deterministic_output(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_dim()
sklearn.decomposition.tests.test_pca.test_pca_dtype_preservation(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_explained_variance_empirical(X,svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_explained_variance_equivalence_solver(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_inverse(svd_solver,whiten)
sklearn.decomposition.tests.test_pca.test_pca_mle_array_api_compliance(estimator,check,array_namespace,device,dtype_name)
sklearn.decomposition.tests.test_pca.test_pca_n_components_mostly_explained_variance_ratio()
sklearn.decomposition.tests.test_pca.test_pca_randomized_svd_n_oversamples()
sklearn.decomposition.tests.test_pca.test_pca_sanity_noise_variance(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_score(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_score3()
sklearn.decomposition.tests.test_pca.test_pca_score_consistency_solvers(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_singular_values(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_singular_values_consistency(svd_solver)
sklearn.decomposition.tests.test_pca.test_pca_sparse(global_random_seed,svd_solver,sparse_container,n_components,density,scale)
sklearn.decomposition.tests.test_pca.test_pca_sparse_fit_transform(global_random_seed,sparse_container)
sklearn.decomposition.tests.test_pca.test_pca_svd_solver_auto(data,n_components,expected_solver)
sklearn.decomposition.tests.test_pca.test_pca_validation(svd_solver,data,n_components,err_msg)
sklearn.decomposition.tests.test_pca.test_pca_zero_noise_variance_edge_cases(svd_solver)
sklearn.decomposition.tests.test_pca.test_small_eigenvalues_mle()
sklearn.decomposition.tests.test_pca.test_sparse_pca_solver_error(global_random_seed,svd_solver,sparse_container)
sklearn.decomposition.tests.test_pca.test_variance_correctness(copy)
sklearn.decomposition.tests.test_pca.test_whitening(solver,copy)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_fastica.py----------------------------------------
A:sklearn.decomposition.tests.test_fastica.x->numpy.rollaxis(x, axis)
A:sklearn.decomposition.tests.test_fastica.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.decomposition.tests.test_fastica.(W, _, _)->numpy.linalg.svd(rng.randn(10, 10))
A:sklearn.decomposition.tests.test_fastica.w->numpy.random.RandomState(global_random_seed).randn(10)
A:sklearn.decomposition.tests.test_fastica.u->_gs_decorrelation(w, W, 5)
A:sklearn.decomposition.tests.test_fastica.tmp->numpy.dot(u, W.T)
A:sklearn.decomposition.tests.test_fastica.X->numpy.random.RandomState(global_random_seed).random_sample((n_samples, n_features))
A:sklearn.decomposition.tests.test_fastica.fica->FastICA(n_components=5, max_iter=1000, whiten='unit-variance', random_state=0).fit(X)
A:sklearn.decomposition.tests.test_fastica.(k_, mixing_, s_)->fastica(m.T, n_components=2, whiten='unit-variance', random_state=rng)
A:sklearn.decomposition.tests.test_fastica.s2->scipy.stats.t.rvs(1, size=n_samples, random_state=rng)
A:sklearn.decomposition.tests.test_fastica.s->s.astype(global_dtype).astype(global_dtype)
A:sklearn.decomposition.tests.test_fastica.mixing->numpy.array([[np.cos(phi), np.sin(phi)], [np.sin(phi), -np.cos(phi)]])
A:sklearn.decomposition.tests.test_fastica.m->numpy.dot(mixing, s)
A:sklearn.decomposition.tests.test_fastica.pca->PCA(n_components=2, whiten=True, random_state=rng)
A:sklearn.decomposition.tests.test_fastica.(_, _, sources_fun)->fastica(m.T, fun=nl, algorithm=algo, random_state=global_random_seed)
A:sklearn.decomposition.tests.test_fastica.ica->FastICA(random_state=0, whiten='unit-variance', whiten_solver='eigh')
A:sklearn.decomposition.tests.test_fastica.sources->FastICA(random_state=0, whiten='unit-variance', whiten_solver='eigh').fit_transform(m.T)
A:sklearn.decomposition.tests.test_fastica.t->numpy.linspace(0, 100, n_samples)
A:sklearn.decomposition.tests.test_fastica.s1->numpy.sin(t)
A:sklearn.decomposition.tests.test_fastica.Xt->FastICA(random_state=0, whiten='unit-variance', whiten_solver='eigh').fit_transform(X)
A:sklearn.decomposition.tests.test_fastica.ica2->FastICA(n_components=n_components, max_iter=max_iter, whiten=whiten, random_state=0)
A:sklearn.decomposition.tests.test_fastica.Xt2->FastICA(n_components=n_components, max_iter=max_iter, whiten=whiten, random_state=0).transform(X)
A:sklearn.decomposition.tests.test_fastica.X2->FastICA(random_state=0, whiten='unit-variance', whiten_solver='eigh').inverse_transform(Xt)
A:sklearn.decomposition.tests.test_fastica.w_init->numpy.random.RandomState(global_random_seed).randn(n_features + 1, n_features + 1)
A:sklearn.decomposition.tests.test_fastica.out->fastica(X, whiten=whiten, return_n_iter=return_n_iter, return_X_mean=return_X_mean)
A:sklearn.decomposition.tests.test_fastica.A->numpy.random.RandomState(global_random_seed).randn(10, 2)
sklearn.decomposition.tests.test_fastica.center_and_norm(x,axis=-1)
sklearn.decomposition.tests.test_fastica.test_fastica_attributes_dtypes(global_dtype)
sklearn.decomposition.tests.test_fastica.test_fastica_convergence_fail()
sklearn.decomposition.tests.test_fastica.test_fastica_eigh_low_rank_warning(global_random_seed)
sklearn.decomposition.tests.test_fastica.test_fastica_errors()
sklearn.decomposition.tests.test_fastica.test_fastica_nowhiten()
sklearn.decomposition.tests.test_fastica.test_fastica_output_shape(whiten,return_X_mean,return_n_iter)
sklearn.decomposition.tests.test_fastica.test_fastica_return_dtypes(global_dtype)
sklearn.decomposition.tests.test_fastica.test_fastica_simple(add_noise,global_random_seed,global_dtype)
sklearn.decomposition.tests.test_fastica.test_fastica_simple_different_solvers(add_noise,global_random_seed)
sklearn.decomposition.tests.test_fastica.test_fastica_whiten_unit_variance()
sklearn.decomposition.tests.test_fastica.test_fit_transform(global_random_seed,global_dtype)
sklearn.decomposition.tests.test_fastica.test_gs()
sklearn.decomposition.tests.test_fastica.test_inverse_transform(whiten,n_components,expected_mixing_shape,global_random_seed,global_dtype)
sklearn.decomposition.tests.test_fastica.test_non_square_fastica(add_noise)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_truncated_svd.py----------------------------------------
A:sklearn.decomposition.tests.test_truncated_svd.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_truncated_svd.X->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_truncated_svd.svd_a->TruncatedSVD(30, algorithm='arpack')
A:sklearn.decomposition.tests.test_truncated_svd.svd->TruncatedSVD(n_components=5, n_iter=7, random_state=42, algorithm=algorithm, tol=tol)
A:sklearn.decomposition.tests.test_truncated_svd.comp_a->numpy.abs(svd_a.components_)
A:sklearn.decomposition.tests.test_truncated_svd.comp->numpy.abs(svd.components_)
A:sklearn.decomposition.tests.test_truncated_svd.tsvd->TruncatedSVD(n_components=6)
A:sklearn.decomposition.tests.test_truncated_svd.Xtrans->TruncatedSVD(n_components=6).fit_transform(Xint)
A:sklearn.decomposition.tests.test_truncated_svd.Xt->TruncatedSVD(n_components=6).fit_transform(X_sparse)
A:sklearn.decomposition.tests.test_truncated_svd.Xinv->TruncatedSVD(n_components=6).inverse_transform(Xt)
A:sklearn.decomposition.tests.test_truncated_svd.Xint->X_sparse.astype(np.int64)
A:sklearn.decomposition.tests.test_truncated_svd.X_tr->TruncatedSVD(n_components=5, n_iter=7, random_state=42, algorithm=algorithm, tol=tol).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.total_variance->numpy.var(X_sparse.toarray(), axis=0).sum()
A:sklearn.decomposition.tests.test_truncated_svd.variances->numpy.var(X_tr, axis=0)
A:sklearn.decomposition.tests.test_truncated_svd.svd_10->TruncatedSVD(10, algorithm=solver, n_iter=10).fit(X)
A:sklearn.decomposition.tests.test_truncated_svd.svd_20->TruncatedSVD(20, algorithm=solver, n_iter=10).fit(X)
A:sklearn.decomposition.tests.test_truncated_svd.pca->PCA(svd_solver='arpack', **params)
A:sklearn.decomposition.tests.test_truncated_svd.X_pca->PCA(svd_solver='arpack', **params).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_hat_pca->numpy.dot(X_pca, pca.components_)
A:sklearn.decomposition.tests.test_truncated_svd.X_dense->X_sparse.toarray()
A:sklearn.decomposition.tests.test_truncated_svd.params->dict(n_components=10, random_state=42)
A:sklearn.decomposition.tests.test_truncated_svd.Xt_svd->TruncatedSVD(n_components=5, n_iter=7, random_state=42, algorithm=algorithm, tol=tol).fit_transform(X_c)
A:sklearn.decomposition.tests.test_truncated_svd.Xt_pca->PCA(svd_solver='arpack', **params).fit_transform(X_c)
A:sklearn.decomposition.tests.test_truncated_svd.X_transformed_1->TruncatedSVD(n_components=5, n_iter=7, random_state=42, algorithm=algorithm, tol=tol).fit_transform(X)
A:sklearn.decomposition.tests.test_truncated_svd.X_transformed_2->TruncatedSVD(n_components=5, n_iter=7, random_state=42, algorithm=algorithm, tol=tol).fit(X).transform(X)
sklearn.decomposition.tests.test_truncated_svd.X_sparse()
sklearn.decomposition.tests.test_truncated_svd.test_attributes(n_components,X_sparse)
sklearn.decomposition.tests.test_truncated_svd.test_explained_variance(X_sparse,kind,n_components,solver)
sklearn.decomposition.tests.test_truncated_svd.test_explained_variance_components_10_20(X_sparse,kind,solver)
sklearn.decomposition.tests.test_truncated_svd.test_fit_transform(X_sparse,algorithm,tol,kind)
sklearn.decomposition.tests.test_truncated_svd.test_integers(X_sparse)
sklearn.decomposition.tests.test_truncated_svd.test_inverse_transform(algo,X_sparse)
sklearn.decomposition.tests.test_truncated_svd.test_singular_values_consistency(solver)
sklearn.decomposition.tests.test_truncated_svd.test_singular_values_expected(solver)
sklearn.decomposition.tests.test_truncated_svd.test_solvers(X_sparse,solver,kind)
sklearn.decomposition.tests.test_truncated_svd.test_sparse_formats(fmt,X_sparse)
sklearn.decomposition.tests.test_truncated_svd.test_too_many_components(X_sparse,algorithm,n_components)
sklearn.decomposition.tests.test_truncated_svd.test_truncated_svd_eq_pca(X_sparse)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_sparse_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_sparse_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_sparse_pca.U->PCA(n_components=n_components, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_sparse_pca.V->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_sparse_pca.img->numpy.zeros(image_size)
A:sklearn.decomposition.tests.test_sparse_pca.V[k, :]->numpy.zeros(image_size).ravel()
A:sklearn.decomposition.tests.test_sparse_pca.Y->numpy.dot(U, V)
A:sklearn.decomposition.tests.test_sparse_pca.X->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_sparse_pca.spca->SPCA(n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0)
A:sklearn.decomposition.tests.test_sparse_pca.(Y, _, _)->generate_toy_data(3, 1000, (8, 8), random_state=rng)
A:sklearn.decomposition.tests.test_sparse_pca.spca_lars->SparsePCA(n_components=3, method='lars', alpha=alpha, random_state=rng)
A:sklearn.decomposition.tests.test_sparse_pca.spca_lasso->MiniBatchSparsePCA(n_components=3, method='cd', alpha=alpha, random_state=0).fit(Y)
A:sklearn.decomposition.tests.test_sparse_pca.U1->SparsePCA(n_components=3, method='lars', alpha=alpha, random_state=rng).transform(Y)
A:sklearn.decomposition.tests.test_sparse_pca.U2->SPCA(n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0).fit(Y).transform(Y)
A:sklearn.decomposition.tests.test_sparse_pca.estimator->SparsePCA(n_components=8)
A:sklearn.decomposition.tests.test_sparse_pca.U_init->numpy.random.RandomState(0).randn(5, 3)
A:sklearn.decomposition.tests.test_sparse_pca.V_init->numpy.random.RandomState(0).randn(3, 4)
A:sklearn.decomposition.tests.test_sparse_pca.model->SPCA(n_components=4).fit(X)
A:sklearn.decomposition.tests.test_sparse_pca.pca->PCA(n_components=n_components, random_state=0)
A:sklearn.decomposition.tests.test_sparse_pca.results_train->SparsePCA(n_components=3, method='lars', alpha=alpha, random_state=rng).fit_transform(Y)
A:sklearn.decomposition.tests.test_sparse_pca.results_test->SparsePCA(n_components=3, method='lars', alpha=alpha, random_state=rng).transform(Y[:10])
A:sklearn.decomposition.tests.test_sparse_pca.(Z, _, _)->generate_toy_data(3, 10, (8, 8), random_state=rng)
A:sklearn.decomposition.tests.test_sparse_pca.results_test_pca->PCA(n_components=n_components, random_state=0).transform(Z)
A:sklearn.decomposition.tests.test_sparse_pca.results_test_spca->SPCA(n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0).transform(Z)
A:sklearn.decomposition.tests.test_sparse_pca.input_array->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_sparse_pca.transformed->SPCA(n_components=4).fit(X).fit_transform(input_array)
A:sklearn.decomposition.tests.test_sparse_pca.model_32->SPCA(n_components=n_components, alpha=alpha, method=method, random_state=0)
A:sklearn.decomposition.tests.test_sparse_pca.transformed_32->SPCA(n_components=n_components, alpha=alpha, method=method, random_state=0).fit_transform(input_array.astype(np.float32))
A:sklearn.decomposition.tests.test_sparse_pca.model_64->SPCA(n_components=n_components, alpha=alpha, method=method, random_state=0)
A:sklearn.decomposition.tests.test_sparse_pca.transformed_64->SPCA(n_components=n_components, alpha=alpha, method=method, random_state=0).fit_transform(input_array.astype(np.float64))
A:sklearn.decomposition.tests.test_sparse_pca.names->SPCA(n_components=4).fit(X).get_feature_names_out()
A:sklearn.decomposition.tests.test_sparse_pca.estimator_name->SPCA.__name__.lower()
A:sklearn.decomposition.tests.test_sparse_pca.model_early_stopped->MiniBatchSparsePCA(max_iter=100, tol=1e-06, max_no_improvement=2, random_state=global_random_seed).fit(X)
A:sklearn.decomposition.tests.test_sparse_pca.model_not_early_stopped->MiniBatchSparsePCA(max_iter=100, tol=1e-06, max_no_improvement=100, random_state=global_random_seed).fit(X)
A:sklearn.decomposition.tests.test_sparse_pca.X_trans_spca->SPCA(n_components=n_components, alpha=1e-12, ridge_alpha=1e-12, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_sparse_pca.X_trans_pca->PCA(n_components=n_components, random_state=0).fit_transform(X)
sklearn.decomposition.tests.test_sparse_pca.generate_toy_data(n_components,n_samples,image_size,random_state=None)
sklearn.decomposition.tests.test_sparse_pca.test_correct_shapes()
sklearn.decomposition.tests.test_sparse_pca.test_equivalence_components_pca_spca(global_random_seed)
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform()
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform_parallel()
sklearn.decomposition.tests.test_sparse_pca.test_fit_transform_tall()
sklearn.decomposition.tests.test_sparse_pca.test_initialization()
sklearn.decomposition.tests.test_sparse_pca.test_mini_batch_correct_shapes()
sklearn.decomposition.tests.test_sparse_pca.test_mini_batch_fit_transform()
sklearn.decomposition.tests.test_sparse_pca.test_pca_vs_spca()
sklearn.decomposition.tests.test_sparse_pca.test_scaling_fit_transform()
sklearn.decomposition.tests.test_sparse_pca.test_sparse_pca_dtype_match(SPCA,method,data_type,expected_type)
sklearn.decomposition.tests.test_sparse_pca.test_sparse_pca_inverse_transform()
sklearn.decomposition.tests.test_sparse_pca.test_sparse_pca_numerical_consistency(SPCA,method)
sklearn.decomposition.tests.test_sparse_pca.test_spca_early_stopping(global_random_seed)
sklearn.decomposition.tests.test_sparse_pca.test_spca_feature_names_out(SPCA)
sklearn.decomposition.tests.test_sparse_pca.test_spca_max_iter_None_deprecation()
sklearn.decomposition.tests.test_sparse_pca.test_spca_n_components_(SPCA,n_components)
sklearn.decomposition.tests.test_sparse_pca.test_transform_inverse_transform_round_trip(SPCA)
sklearn.decomposition.tests.test_sparse_pca.test_transform_nan()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_dict_learning.py----------------------------------------
A:sklearn.decomposition.tests.test_dict_learning.rng_global->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_dict_learning.X->numpy.random.RandomState(0).randn(100, 64)
A:sklearn.decomposition.tests.test_dict_learning.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_dict_learning.X_->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_dict_learning.dictionary->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_dict_learning.code->SparseCoder(dictionary.astype(data_type), transform_algorithm=transform_algorithm).transform(X.astype(data_type))
A:sklearn.decomposition.tests.test_dict_learning.dico->MiniBatchDictionaryLearning(n_components, batch_size=4, max_iter=0, dict_init=V, random_state=0).fit(X)
A:sklearn.decomposition.tests.test_dict_learning.x->numpy.linspace(0, resolution - 1, resolution)
A:sklearn.decomposition.tests.test_dict_learning.centers->numpy.linspace(0, resolution - 1, n_components)
A:sklearn.decomposition.tests.test_dict_learning.D->numpy.random.RandomState(0).randn(2, 64)
A:sklearn.decomposition.tests.test_dict_learning.D[i]->ricker_function(resolution, center, width)
A:sklearn.decomposition.tests.test_dict_learning.model->SparseCoder(D_multi, transform_algorithm=transform_algorithm, transform_max_iter=2000)
A:sklearn.decomposition.tests.test_dict_learning.err_msg->err_msg.format(algo).format(algo)
A:sklearn.decomposition.tests.test_dict_learning.split_code->MiniBatchDictionaryLearning(n_components, batch_size=4, max_iter=0, dict_init=V, random_state=0).fit(X).transform(X)
A:sklearn.decomposition.tests.test_dict_learning.(code, dictionary)->dict_learning_online(X.astype(data_type), n_components=n_components, alpha=1, batch_size=10, random_state=rng, method=method)
A:sklearn.decomposition.tests.test_dict_learning.sys.stdout->StringIO()
A:sklearn.decomposition.tests.test_dict_learning.V->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_dict_learning.dict1->MiniBatchDictionaryLearning(n_components, max_iter=10, batch_size=1, alpha=1, shuffle=False, dict_init=V, max_no_improvement=None, tol=0.0, random_state=0).fit(X)
A:sklearn.decomposition.tests.test_dict_learning.dict2->MiniBatchDictionaryLearning(n_components, alpha=1, dict_init=V, random_state=0)
A:sklearn.decomposition.tests.test_dict_learning.Xf->check_array(X, order='F')
A:sklearn.decomposition.tests.test_dict_learning.a->sparse_encode(X, V, algorithm=algo)
A:sklearn.decomposition.tests.test_dict_learning.b->sparse_encode(Xf, V, algorithm=algo)
A:sklearn.decomposition.tests.test_dict_learning.coder->SparseCoder(dictionary.astype(data_type), transform_algorithm=transform_algorithm)
A:sklearn.decomposition.tests.test_dict_learning.cloned->clone(coder)
A:sklearn.decomposition.tests.test_dict_learning.data->numpy.random.rand(n_samples, n_features).astype(np.float32)
A:sklearn.decomposition.tests.test_dict_learning.init_dict->numpy.random.RandomState(0).rand(n_components, n_features)
A:sklearn.decomposition.tests.test_dict_learning.sc->SparseCoder(d)
A:sklearn.decomposition.tests.test_dict_learning.check_transformer_general_memmap->partial(check_transformer_general, readonly_memmap=True)
A:sklearn.decomposition.tests.test_dict_learning.d->numpy.array([[1, 2, 3], [1, 2, 3]])
A:sklearn.decomposition.tests.test_dict_learning.newd_batch->numpy.random.RandomState(0).randn(n_components, n_features).copy()
A:sklearn.decomposition.tests.test_dict_learning.A->numpy.dot(code.T, code)
A:sklearn.decomposition.tests.test_dict_learning.B->numpy.dot(X.T, code)
A:sklearn.decomposition.tests.test_dict_learning.newd_online->numpy.random.RandomState(0).randn(n_components, n_features).copy()
A:sklearn.decomposition.tests.test_dict_learning.code_32->sparse_encode(X.astype(np.float32), dictionary.astype(np.float32), algorithm=algorithm)
A:sklearn.decomposition.tests.test_dict_learning.code_64->sparse_encode(X.astype(np.float64), dictionary.astype(np.float64), algorithm=algorithm)
A:sklearn.decomposition.tests.test_dict_learning.dict_learner->DictionaryLearning(n_components=5, random_state=0, n_jobs=2, fit_algorithm='cd', max_iter=50, verbose=True)
A:sklearn.decomposition.tests.test_dict_learning.(code, dictionary, _)->dict_learning(X.astype(data_type), n_components=n_components, alpha=1, random_state=rng, method=method)
A:sklearn.decomposition.tests.test_dict_learning.(U_64, V_64, _)->dict_learning(X.astype(np.float64), n_components=n_components, alpha=alpha, random_state=0, method=method)
A:sklearn.decomposition.tests.test_dict_learning.(U_32, V_32, _)->dict_learning(X.astype(np.float32), n_components=n_components, alpha=alpha, random_state=0, method=method)
A:sklearn.decomposition.tests.test_dict_learning.(U_64, V_64)->dict_learning_online(X.astype(np.float64), n_components=n_components, max_iter=1000, alpha=alpha, batch_size=10, random_state=0, method=method, tol=0.0, max_no_improvement=None)
A:sklearn.decomposition.tests.test_dict_learning.(U_32, V_32)->dict_learning_online(X.astype(np.float32), n_components=n_components, max_iter=1000, alpha=alpha, batch_size=10, random_state=0, method=method, tol=0.0, max_no_improvement=None)
A:sklearn.decomposition.tests.test_dict_learning.feature_names_out->estimator.get_feature_names_out()
A:sklearn.decomposition.tests.test_dict_learning.estimator_name->estimator.__class__.__name__.lower()
A:sklearn.decomposition.tests.test_dict_learning.X_train->numpy.random.RandomState(0).randn(10, 10)
sklearn.decomposition.tests.test_dict_learning.test_cd_work_on_joblib_memmapped_data(monkeypatch)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_dtype_match(data_type,expected_type,method)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_lars_code_positivity()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_lars_dict_positivity(positive_dict)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_lars_positive_parameter()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_lassocd_readonly_data()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_nonzero_coefs()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_numerical_consistency(method)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_dtype_match(data_type,expected_type,method)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_estimator_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_initialization()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_lars_positive_parameter()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_numerical_consistency(method)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_overcomplete()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_partial_fit()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_positivity(positive_code,positive_dict)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_readonly_initialization()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_online_verbosity()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_overcomplete()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_positivity(transform_algorithm,positive_code,positive_dict)
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_reconstruction()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_reconstruction_parallel()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_shapes()
sklearn.decomposition.tests.test_dict_learning.test_dict_learning_split()
sklearn.decomposition.tests.test_dict_learning.test_dictionary_learning_dtype_match(data_type,expected_type,fit_algorithm,transform_algorithm)
sklearn.decomposition.tests.test_dict_learning.test_get_feature_names_out(estimator)
sklearn.decomposition.tests.test_dict_learning.test_max_iter()
sklearn.decomposition.tests.test_dict_learning.test_minibatch_dictionary_learning_dtype_match(data_type,expected_type,fit_algorithm,transform_algorithm)
sklearn.decomposition.tests.test_dict_learning.test_minibatch_dictionary_learning_lars(positive_dict)
sklearn.decomposition.tests.test_dict_learning.test_minibatch_dictionary_learning_positivity(transform_algorithm,positive_code,positive_dict)
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_common_transformer()
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_dtype_match(data_type,transform_algorithm)
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_estimator()
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_estimator_clone()
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_n_features_in()
sklearn.decomposition.tests.test_dict_learning.test_sparse_coder_parallel_mmap()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_dtype_match(data_type,algorithm)
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_error()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_error_default_sparsity()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_input()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_numerical_consistency(algorithm)
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_positivity(algo,positive)
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_shapes()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_shapes_omp()
sklearn.decomposition.tests.test_dict_learning.test_sparse_encode_unavailable_positivity(algo)
sklearn.decomposition.tests.test_dict_learning.test_update_dict()
sklearn.decomposition.tests.test_dict_learning.test_xxx()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_online_lda.py----------------------------------------
A:sklearn.decomposition.tests.test_online_lda.block->numpy.full((3, 3), n_components, dtype=int)
A:sklearn.decomposition.tests.test_online_lda.X->numpy.random.RandomState(global_random_seed).uniform(size=(20, 10)).astype(global_dtype, copy=False)
A:sklearn.decomposition.tests.test_online_lda.(n_components, X)->_build_sparse_array(csr_container)
A:sklearn.decomposition.tests.test_online_lda.lda_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0)
A:sklearn.decomposition.tests.test_online_lda.lda_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0)
A:sklearn.decomposition.tests.test_online_lda.topic_distr_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.topic_distr_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.decomposition.tests.test_online_lda.lda->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method)
A:sklearn.decomposition.tests.test_online_lda.top_idx->set(c.argsort()[-3:][::-1])
A:sklearn.decomposition.tests.test_online_lda.X_trans->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).transform(X)
A:sklearn.decomposition.tests.test_online_lda.X_fit->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).fit_transform(X)
A:sklearn.decomposition.tests.test_online_lda.n_components->numpy.random.RandomState(global_random_seed).randint(3, 6)
A:sklearn.decomposition.tests.test_online_lda.n_samples->numpy.random.RandomState(global_random_seed).randint(6, 10)
A:sklearn.decomposition.tests.test_online_lda.invalid_n_samples->numpy.random.RandomState(global_random_seed).randint(4, size=(n_samples + 1, n_components))
A:sklearn.decomposition.tests.test_online_lda.invalid_n_components->numpy.random.RandomState(global_random_seed).randint(4, size=(n_samples, n_components + 1))
A:sklearn.decomposition.tests.test_online_lda.perp_1->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).perplexity(X)
A:sklearn.decomposition.tests.test_online_lda.perp_2->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).perplexity(X.toarray())
A:sklearn.decomposition.tests.test_online_lda.perp_1_subsampling->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).perplexity(X, sub_sampling=True)
A:sklearn.decomposition.tests.test_online_lda.perp_2_subsampling->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).perplexity(X, sub_sampling=True)
A:sklearn.decomposition.tests.test_online_lda.score_1->LatentDirichletAllocation(n_components=n_components, max_iter=1, learning_method=method, total_samples=100, random_state=0).score(X)
A:sklearn.decomposition.tests.test_online_lda.score_2->LatentDirichletAllocation(n_components=n_components, max_iter=10, learning_method=method, total_samples=100, random_state=0).score(X)
A:sklearn.decomposition.tests.test_online_lda.perplexity_1->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).perplexity(X, sub_sampling=False)
A:sklearn.decomposition.tests.test_online_lda.score->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).score(X)
A:sklearn.decomposition.tests.test_online_lda.perplexity_2->numpy.exp(-1.0 * (score / np.sum(X.data)))
A:sklearn.decomposition.tests.test_online_lda.perplexity2->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).perplexity(X)
A:sklearn.decomposition.tests.test_online_lda.Z->numpy.zeros((5, 4))
A:sklearn.decomposition.tests.test_online_lda.x->x.reshape(100, 100).reshape(100, 100)
A:sklearn.decomposition.tests.test_online_lda.expectation->numpy.empty_like(x)
A:sklearn.decomposition.tests.test_online_lda.out->StringIO()
A:sklearn.decomposition.tests.test_online_lda.n_lines->StringIO().getvalue().count('\n')
A:sklearn.decomposition.tests.test_online_lda.n_perplexity->StringIO().getvalue().count('perplexity')
A:sklearn.decomposition.tests.test_online_lda.names->LatentDirichletAllocation(n_components=5, random_state=0, learning_method=learning_method).get_feature_names_out()
A:sklearn.decomposition.tests.test_online_lda.X64->numpy.random.RandomState(global_random_seed).uniform(size=(20, 10))
A:sklearn.decomposition.tests.test_online_lda.X32->numpy.random.RandomState(global_random_seed).uniform(size=(20, 10)).astype(np.float32)
A:sklearn.decomposition.tests.test_online_lda.lda_64->LatentDirichletAllocation(n_components=5, random_state=global_random_seed, learning_method=learning_method).fit(X64)
A:sklearn.decomposition.tests.test_online_lda.lda_32->LatentDirichletAllocation(n_components=5, random_state=global_random_seed, learning_method=learning_method).fit(X32)
sklearn.decomposition.tests.test_online_lda._build_sparse_array(csr_container)
sklearn.decomposition.tests.test_online_lda.check_verbosity(verbose,evaluate_every,expected_lines,expected_perplexities,csr_container)
sklearn.decomposition.tests.test_online_lda.test_dirichlet_expectation()
sklearn.decomposition.tests.test_online_lda.test_lda_default_prior_params(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_dense_input(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_dtype_match(learning_method,global_dtype)
sklearn.decomposition.tests.test_online_lda.test_lda_empty_docs(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_feature_names_out(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_fit_batch(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_fit_online(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_fit_perplexity(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_fit_transform(method)
sklearn.decomposition.tests.test_online_lda.test_lda_multi_jobs(method,csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_negative_input()
sklearn.decomposition.tests.test_online_lda.test_lda_no_component_error()
sklearn.decomposition.tests.test_online_lda.test_lda_numerical_consistency(learning_method,global_random_seed)
sklearn.decomposition.tests.test_online_lda.test_lda_partial_fit(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_partial_fit_multi_jobs(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_perplexity(method,csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_preplexity_mismatch()
sklearn.decomposition.tests.test_online_lda.test_lda_score(method,csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_score_perplexity(csr_container)
sklearn.decomposition.tests.test_online_lda.test_lda_transform()
sklearn.decomposition.tests.test_online_lda.test_perplexity_input_format(csr_container)
sklearn.decomposition.tests.test_online_lda.test_verbosity(verbose,evaluate_every,expected_lines,expected_perplexities,csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_factor_analysis.py----------------------------------------
A:sklearn.decomposition.tests.test_factor_analysis.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_factor_analysis.W->numpy.random.RandomState(0).randn(n_components, n_features)
A:sklearn.decomposition.tests.test_factor_analysis.h->numpy.random.RandomState(0).randn(n_samples, n_components)
A:sklearn.decomposition.tests.test_factor_analysis.fa->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features))
A:sklearn.decomposition.tests.test_factor_analysis.X_t->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).transform(X)
A:sklearn.decomposition.tests.test_factor_analysis.diff->numpy.all(np.diff(fa.loglike_))
A:sklearn.decomposition.tests.test_factor_analysis.scov->numpy.cov(X, rowvar=0.0, bias=1.0)
A:sklearn.decomposition.tests.test_factor_analysis.mcov->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_covariance()
A:sklearn.decomposition.tests.test_factor_analysis.cov->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_covariance()
A:sklearn.decomposition.tests.test_factor_analysis.precision->FactorAnalysis(n_components=n_components, noise_variance_init=np.ones(n_features)).get_precision()
A:sklearn.decomposition.tests.test_factor_analysis.fa_var->FactorAnalysis(n_components=n_components, rotation=method)
A:sklearn.decomposition.tests.test_factor_analysis.results[method]->FactorAnalysis(n_components=n_components, rotation=method).fit_transform(X)
A:sklearn.decomposition.tests.test_factor_analysis.projections[method]->FactorAnalysis(n_components=n_components, rotation=method).get_covariance()
A:sklearn.decomposition.tests.test_factor_analysis.factors->numpy.array([[0.89421016, -0.35854928, -0.27770122, 0.03773647], [-0.45081822, -0.89132754, 0.0932195, -0.01787973], [0.99500666, -0.02031465, 0.05426497, -0.11539407], [0.96822861, -0.06299656, 0.24411001, 0.07540887]])
A:sklearn.decomposition.tests.test_factor_analysis.r_solution->numpy.array([[0.962, 0.052], [-0.141, 0.989], [0.949, -0.3], [0.937, -0.251]])
sklearn.decomposition.tests.test_factor_analysis.test_factor_analysis()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_incremental_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_incremental_pca.iris->sklearn.datasets.load_iris()
A:sklearn.decomposition.tests.test_incremental_pca.ipca->IncrementalPCA(n_components=2).fit(iris.data)
A:sklearn.decomposition.tests.test_incremental_pca.pca->PCA(n_components=2)
A:sklearn.decomposition.tests.test_incremental_pca.X_transformed->IncrementalPCA(n_components=2).fit(iris.data).fit_transform(X_sparse)
A:sklearn.decomposition.tests.test_incremental_pca.cov->IncrementalPCA(n_components=2).fit(iris.data).get_covariance()
A:sklearn.decomposition.tests.test_incremental_pca.precision->IncrementalPCA(n_components=2).fit(iris.data).get_precision()
A:sklearn.decomposition.tests.test_incremental_pca.X_sparse->sparse_container(X)
A:sklearn.decomposition.tests.test_incremental_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_incremental_pca.Yt->IncrementalPCA(n_components=2).fit(X).transform(Xt)
A:sklearn.decomposition.tests.test_incremental_pca.X->sklearn.datasets.make_low_rank_matrix(1000, 10, tail_strength=0.0, effective_rank=2, random_state=1999)
A:sklearn.decomposition.tests.test_incremental_pca.Y->IncrementalPCA(n_components=2).fit(iris.data).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Y_inverse->IncrementalPCA(n_components=2).fit(iris.data).inverse_transform(Y)
A:sklearn.decomposition.tests.test_incremental_pca.X2->numpy.random.RandomState(0).randn(n_samples, 50)
A:sklearn.decomposition.tests.test_incremental_pca.X3->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.decomposition.tests.test_incremental_pca.batch_sizes->numpy.arange(20, 90, 3)
A:sklearn.decomposition.tests.test_incremental_pca.pipca->IncrementalPCA(n_components=2, batch_size=batch_size)
A:sklearn.decomposition.tests.test_incremental_pca.batch_itr->numpy.arange(0, n + 1, batch_size)
A:sklearn.decomposition.tests.test_incremental_pca.Y_pca->PCA(n_components=3).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Y_ipca->IncrementalPCA(n_components=3, batch_size=25).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_pca->PCA(n_components=2).fit_transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_ipca->IncrementalPCA(n_components=2).fit(iris.data).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.X_hat->numpy.dot(X_pca, pca.components_)
A:sklearn.decomposition.tests.test_incremental_pca.Xt_pca->PCA(n_components=2).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Xt_ipca->IncrementalPCA(n_components=2).fit(iris.data).transform(X)
A:sklearn.decomposition.tests.test_incremental_pca.Xinv_ipca->IncrementalPCA(n_components=2).fit(iris.data).inverse_transform(Xt_ipca)
A:sklearn.decomposition.tests.test_incremental_pca.Xinv_pca->PCA(n_components=2).inverse_transform(Xt_pca)
A:sklearn.decomposition.tests.test_incremental_pca.pca.n_samples_seen_->float(pca.n_samples_seen_)
A:sklearn.decomposition.tests.test_incremental_pca.pca2->IncrementalPCA(n_components=2)
A:sklearn.decomposition.tests.test_incremental_pca.A->numpy.random.RandomState(0).rand(500000, 2)
A:sklearn.decomposition.tests.test_incremental_pca.names->IncrementalPCA(n_components=2).fit(iris.data).get_feature_names_out()
sklearn.decomposition.tests.test_incremental_pca.test_explained_variances()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_against_pca_iris()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_against_pca_random_data()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_batch_rank()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_batch_signs()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_batch_values()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_check_projection()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_feature_names_out()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_fit_overflow_error()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_inverse()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_num_features_change()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_partial_fit()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_partial_fit_float_division()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_set_params()
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_sparse(sparse_container)
sklearn.decomposition.tests.test_incremental_pca.test_incremental_pca_validation()
sklearn.decomposition.tests.test_incremental_pca.test_n_components_none()
sklearn.decomposition.tests.test_incremental_pca.test_n_samples_equal_n_components()
sklearn.decomposition.tests.test_incremental_pca.test_singular_values()
sklearn.decomposition.tests.test_incremental_pca.test_whitening()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_kernel_pca.py----------------------------------------
A:sklearn.decomposition.tests.test_kernel_pca.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit->numpy.random.RandomState(0).random_sample((5, 4))
A:sklearn.decomposition.tests.test_kernel_pca.X_pred->numpy.random.RandomState(0).random_sample((2, 4))
A:sklearn.decomposition.tests.test_kernel_pca.kpca->KernelPCA(n_components=2).fit(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit_transformed->KernelPCA(n_components=2).fit(X).fit_transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.X_fit_transformed2->KernelPCA(n_components=2).fit(X).fit(X_fit).transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.X_pred_transformed->KernelPCA(n_components=2).fit(X).transform(X_pred)
A:sklearn.decomposition.tests.test_kernel_pca.X_pred2->KernelPCA(n_components=2).fit(X).inverse_transform(X_pred_transformed)
A:sklearn.decomposition.tests.test_kernel_pca.estimator->KernelPCA(n_components=10, fit_inverse_transform=True, kernel='precomputed')
A:sklearn.decomposition.tests.test_kernel_pca.state->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_kernel_pca.X->numpy.random.RandomState(0).random_sample((5, 4))
A:sklearn.decomposition.tests.test_kernel_pca.transformed1->KernelPCA(n_components=2).fit(X).transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_copy->numpy.random.RandomState(0).random_sample((5, 4)).copy()
A:sklearn.decomposition.tests.test_kernel_pca.transformed2->KernelPCA(n_components=2).fit(X).transform(X_copy)
A:sklearn.decomposition.tests.test_kernel_pca.transformed_X->numpy.zeros((20, 2))
A:sklearn.decomposition.tests.test_kernel_pca.Xt->KernelPCA(n_components=2).fit(X).fit_transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.k->KernelPCA(n_components=2, remove_zero_eig=False, eigen_solver='dense')
A:sklearn.decomposition.tests.test_kernel_pca.A->KernelPCA(n_components=2, remove_zero_eig=False, eigen_solver='dense').fit(X_fit).transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.B->KernelPCA(n_components=2, remove_zero_eig=False, eigen_solver='dense').fit_transform(X_fit)
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca->KernelPCA(n_components=2).fit(X).fit_transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca2->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed', random_state=0).fit(np.dot(X_fit, X_fit.T)).transform(np.dot(X_pred, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca_train->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed', random_state=0).fit_transform(np.dot(X_fit, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.X_kpca_train2->KernelPCA(4, eigen_solver=eigen_solver, kernel='precomputed', random_state=0).fit(np.dot(X_fit, X_fit.T)).transform(np.dot(X_fit, X_fit.T))
A:sklearn.decomposition.tests.test_kernel_pca.kpca_c->KernelPCA(kernel='precomputed', eigen_solver=solver, n_components=1, random_state=0)
A:sklearn.decomposition.tests.test_kernel_pca.(X, y)->make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]], random_state=0, cluster_std=0.1)
A:sklearn.decomposition.tests.test_kernel_pca.pipeline->Pipeline([('kernel_pca', kpca), ('Perceptron', Perceptron(max_iter=5))])
A:sklearn.decomposition.tests.test_kernel_pca.param_grid->dict(Perceptron__max_iter=np.arange(1, 5))
A:sklearn.decomposition.tests.test_kernel_pca.grid_search->GridSearchCV(pipeline, cv=3, param_grid=param_grid)
A:sklearn.decomposition.tests.test_kernel_pca.X_kernel->rbf_kernel(X, gamma=2.0)
A:sklearn.decomposition.tests.test_kernel_pca.train_score->Perceptron(max_iter=5).fit(X_kpca, y).score(X_kpca, y)
A:sklearn.decomposition.tests.test_kernel_pca.(X, _)->load_iris(as_frame=True, return_X_y=True)
A:sklearn.decomposition.tests.test_kernel_pca.ref_pred->KernelPCA(n_components, eigen_solver='dense', random_state=0).fit(X_fit).transform(X_pred)
A:sklearn.decomposition.tests.test_kernel_pca.a_pred->KernelPCA(n_components, eigen_solver='arpack', random_state=0).fit(X_fit).transform(X_pred)
A:sklearn.decomposition.tests.test_kernel_pca.r_pred->KernelPCA(n_components, eigen_solver='randomized', random_state=0).fit(X_fit).transform(X_pred)
A:sklearn.decomposition.tests.test_kernel_pca.(X, *_)->make_blobs(n_samples=100, n_features=4, random_state=0)
A:sklearn.decomposition.tests.test_kernel_pca.X_trans->KernelPCA(n_components=2).fit(X).fit_transform(X)
A:sklearn.decomposition.tests.test_kernel_pca.X_reconst->KernelPCA(n_components=2).fit(X).inverse_transform(X_trans)
A:sklearn.decomposition.tests.test_kernel_pca.names->KernelPCA(n_components=2).fit(X).get_feature_names_out()
A:sklearn.decomposition.tests.test_kernel_pca.kpca1->KernelPCA(gamma=None, **kwargs).fit(X)
A:sklearn.decomposition.tests.test_kernel_pca.kpca2->KernelPCA(gamma=expected_gamma, **kwargs).fit(X)
A:sklearn.decomposition.tests.test_kernel_pca.X1_recon->KernelPCA(gamma=None, **kwargs).fit(X).inverse_transform(kpca1.transform(X))
A:sklearn.decomposition.tests.test_kernel_pca.X2_recon->KernelPCA(gamma=expected_gamma, **kwargs).fit(X).inverse_transform(kpca1.transform(X))
sklearn.decomposition.tests.test_kernel_pca.test_32_64_decomposition_shape()
sklearn.decomposition.tests.test_kernel_pca.test_gridsearch_pipeline()
sklearn.decomposition.tests.test_kernel_pca.test_gridsearch_pipeline_precomputed()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_conditioning()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_consistent_transform()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_deterministic_output()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_feature_names_out()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_invalid_parameters()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_inverse_correct_gamma()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_inverse_transform_reconstruction()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_linear_kernel(solver,n_features)
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_n_components()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_pandas_output()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_precomputed()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_precomputed_non_symmetric(solver)
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_raise_not_fitted_error()
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_solvers_equivalence(n_components)
sklearn.decomposition.tests.test_kernel_pca.test_kernel_pca_sparse(csr_container)
sklearn.decomposition.tests.test_kernel_pca.test_leave_zero_eig()
sklearn.decomposition.tests.test_kernel_pca.test_nested_circles()
sklearn.decomposition.tests.test_kernel_pca.test_precomputed_kernel_not_psd(solver)
sklearn.decomposition.tests.test_kernel_pca.test_remove_zero_eig()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/decomposition/tests/test_nmf.py----------------------------------------
A:sklearn.decomposition.tests.test_nmf.A->numpy.abs(rng.randn(6, 5))
A:sklearn.decomposition.tests.test_nmf.rng->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_nmf.data->numpy.abs(rng.randn(10, 10))
A:sklearn.decomposition.tests.test_nmf.(W, H)->NMF(n_components=2, init='custom', random_state=0)._initialize_nmf(X, n_components=n_components, init='random', random_state=0)
A:sklearn.decomposition.tests.test_nmf.clf->NMF(2, tol=0.1).fit(A)
A:sklearn.decomposition.tests.test_nmf.msg->re.escape('Array passed to NMF (input H) is full of zeros')
A:sklearn.decomposition.tests.test_nmf.error->scipy.linalg.norm(np.dot(W, H) - A)
A:sklearn.decomposition.tests.test_nmf.sdev->scipy.linalg.norm(A - A.mean())
A:sklearn.decomposition.tests.test_nmf.(W0, H0)->NMF(n_components=2, init='custom', random_state=0)._initialize_nmf(X, n_components, init='random', random_state=42)
A:sklearn.decomposition.tests.test_nmf.(Wa, Ha)->NMF(n_components=2, init='custom', random_state=0)._initialize_nmf(data, 10, init='nndsvda')
A:sklearn.decomposition.tests.test_nmf.(War, Har)->NMF(n_components=2, init='custom', random_state=0)._initialize_nmf(data, 10, init='nndsvdar', random_state=0)
A:sklearn.decomposition.tests.test_nmf.model->Estimator(n_components=n_components, alpha_W=0.0, l1_ratio=l1_ratio, random_state=42, **solver)
A:sklearn.decomposition.tests.test_nmf.transf->MiniBatchNMF(n_components=n_components, beta_loss=beta_loss, batch_size=batch_size, random_state=0, max_iter=max_iter).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.pnmf->Estimator(5, init='nndsvdar', random_state=0, max_iter=600, **solver)
A:sklearn.decomposition.tests.test_nmf.X->numpy.random.RandomState(0).random_sample((6, 5))
A:sklearn.decomposition.tests.test_nmf.W_true->numpy.zeros([n_samples, n_components])
A:sklearn.decomposition.tests.test_nmf.W_array->numpy.abs(rng.randn(n_samples))
A:sklearn.decomposition.tests.test_nmf.H_true->numpy.random.RandomState(0).random_sample((2, 5))
A:sklearn.decomposition.tests.test_nmf.H_array->numpy.abs(rng.randn(n_components))
A:sklearn.decomposition.tests.test_nmf.X_calc->numpy.dot(transf, mbmodel.components_)
A:sklearn.decomposition.tests.test_nmf.mbmodel->MiniBatchNMF(n_components=n_components, beta_loss=beta_loss, batch_size=batch_size, random_state=0, max_iter=max_iter)
A:sklearn.decomposition.tests.test_nmf.m->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000)
A:sklearn.decomposition.tests.test_nmf.ft->NMF(n_components=2, init='custom', random_state=0).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.t->NMF(solver=solver, n_components=4, init='random', random_state=0, max_iter=1000).transform(A)
A:sklearn.decomposition.tests.test_nmf.random_state->numpy.random.RandomState(0)
A:sklearn.decomposition.tests.test_nmf.avg->numpy.sqrt(A.mean() / n_components)
A:sklearn.decomposition.tests.test_nmf.H_init->numpy.random.RandomState(0).random_sample((2, 5))
A:sklearn.decomposition.tests.test_nmf.W_init->numpy.random.RandomState(0).random_sample((6, 2))
A:sklearn.decomposition.tests.test_nmf.A_new->NMF(n_components=2, init='custom', random_state=0).inverse_transform(ft)
A:sklearn.decomposition.tests.test_nmf.nmf->NMF(n_components=2, init='custom', random_state=0)
A:sklearn.decomposition.tests.test_nmf.A_sparse->sparse_container(A)
A:sklearn.decomposition.tests.test_nmf.est1->Estimator(n_components=5, init='random', alpha_W=alpha_W, alpha_H=alpha_H, random_state=0, tol=0, max_iter=100, **solver)
A:sklearn.decomposition.tests.test_nmf.est2->clone(est1)
A:sklearn.decomposition.tests.test_nmf.W1->Estimator(n_components=5, init='random', alpha_W=alpha_W, alpha_H=alpha_H, random_state=0, tol=0, max_iter=100, **solver).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.W2->clone(est1).fit_transform(A_sparse)
A:sklearn.decomposition.tests.test_nmf.A_fit_tr->Estimator(n_components=n_components, alpha_W=0.0, l1_ratio=l1_ratio, random_state=42, **solver).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.A_tr->Estimator(n_components=n_components, alpha_W=0.0, l1_ratio=l1_ratio, random_state=42, **solver).transform(A)
A:sklearn.decomposition.tests.test_nmf.(W_nmf, H, _)->non_negative_factorization(A, init=init, solver=solver, max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.(W_nmf_2, H, _)->non_negative_factorization(A, H=H, update_H=False, init=init, solver=solver, max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.model_class->NMF(init=init, solver=solver, max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, random_state=1, tol=0.01)
A:sklearn.decomposition.tests.test_nmf.W_cls->NMF(init=init, solver=solver, max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, random_state=1, tol=0.01).fit_transform(A)
A:sklearn.decomposition.tests.test_nmf.W_cls_2->NMF(init=init, solver=solver, max_iter=max_iter, alpha_W=alpha_W, alpha_H=alpha_H, random_state=1, tol=0.01).transform(A)
A:sklearn.decomposition.tests.test_nmf.WH->NMF(n_components=2, init='custom', random_state=0)._special_sparse_dot(W, H, X)
A:sklearn.decomposition.tests.test_nmf.res->NMF(n_components=2, init='custom', random_state=0)._beta_divergence(X, W, H, beta=1.0)
A:sklearn.decomposition.tests.test_nmf.X_csr->csr_container(X)
A:sklearn.decomposition.tests.test_nmf.ref->NMF(n_components=2, init='custom', random_state=0)._beta_divergence(X, W, H, beta=1.0)
A:sklearn.decomposition.tests.test_nmf.loss->NMF(n_components=2, init='custom', random_state=0)._beta_divergence(X, W, H, beta)
A:sklearn.decomposition.tests.test_nmf.loss_csr->NMF(n_components=2, init='custom', random_state=0)._beta_divergence(X_csr, W, H, beta)
A:sklearn.decomposition.tests.test_nmf.W->numpy.random.RandomState(0).random_sample((6, 2))
A:sklearn.decomposition.tests.test_nmf.H->numpy.random.RandomState(0).random_sample((2, 5))
A:sklearn.decomposition.tests.test_nmf.WH_safe->NMF(n_components=2, init='custom', random_state=0)._special_sparse_dot(W, H, X_csr)
A:sklearn.decomposition.tests.test_nmf.(ii, jj)->csr_container(X).nonzero()
A:sklearn.decomposition.tests.test_nmf.WH_safe_data->numpy.asarray(WH_safe[ii, jj]).ravel()
A:sklearn.decomposition.tests.test_nmf.(W1, H1, _)->non_negative_factorization(X, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha_W=alpha, l1_ratio=l1_ratio, random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W2, H2, _)->non_negative_factorization(X_csr, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha_W=alpha, l1_ratio=l1_ratio, random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W3, H3, _)->non_negative_factorization(X_csr, W, H, n_components, init='custom', update_H=True, solver='mu', beta_loss=beta_loss, max_iter=n_iter, alpha_W=alpha, l1_ratio=l1_ratio, random_state=42)
A:sklearn.decomposition.tests.test_nmf.(W, H, _)->non_negative_factorization(X, H=H_true, n_components='auto', update_H=False)
A:sklearn.decomposition.tests.test_nmf.regul->Estimator(n_components=n_components, alpha_W=0.5, l1_ratio=l1_ratio, random_state=42, **solver)
A:sklearn.decomposition.tests.test_nmf.W_regul->Estimator(n_components=n_components, alpha_W=0.5, l1_ratio=l1_ratio, random_state=42, **solver).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.W_model->Estimator(n_components=n_components, alpha_W=0.0, l1_ratio=l1_ratio, random_state=42, **solver).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.nmf32->Estimator(random_state=0, tol=0.001, **solver)
A:sklearn.decomposition.tests.test_nmf.W32->Estimator(random_state=0, tol=0.001, **solver).fit_transform(X.astype(np.float32))
A:sklearn.decomposition.tests.test_nmf.nmf64->Estimator(random_state=0, tol=0.001, **solver)
A:sklearn.decomposition.tests.test_nmf.W64->Estimator(random_state=0, tol=0.001, **solver).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.mbnmf->MiniBatchNMF(n_components=5, beta_loss=beta_loss, random_state=0, tol=0, max_no_improvement=None, batch_size=X.shape[0], forget_factor=0.0)
A:sklearn.decomposition.tests.test_nmf.mbW->MiniBatchNMF(n_components=5, beta_loss=beta_loss, random_state=0, tol=0, max_no_improvement=None, batch_size=X.shape[0], forget_factor=0.0).fit_transform(X)
A:sklearn.decomposition.tests.test_nmf.mbnmf1->MiniBatchNMF(n_components=n_components, init='custom', random_state=0, max_iter=max_iter, batch_size=batch_size, tol=0, max_no_improvement=None, fresh_restarts=False)
A:sklearn.decomposition.tests.test_nmf.mbnmf2->MiniBatchNMF(n_components=n_components, init='custom', random_state=0)
A:sklearn.decomposition.tests.test_nmf.names->NMF(n_components=2, init='custom', random_state=0).get_feature_names_out()
A:sklearn.decomposition.tests.test_nmf.sys.stdout->StringIO()
A:sklearn.decomposition.tests.test_nmf.est->Estimator(n_components='auto', init='custom', random_state=0, tol=1e-06)
A:sklearn.decomposition.tests.test_nmf.Xt->Estimator(n_components='auto', init='custom', random_state=0, tol=1e-06).fit_transform(A)
sklearn.decomposition.tests.test_nmf._beta_divergence_dense(X,W,H,beta)
sklearn.decomposition.tests.test_nmf.test_NMF_inverse_transform_W_deprecation()
sklearn.decomposition.tests.test_nmf.test_beta_divergence(csr_container)
sklearn.decomposition.tests.test_nmf.test_convergence_warning(Estimator,solver)
sklearn.decomposition.tests.test_nmf.test_feature_names_out()
sklearn.decomposition.tests.test_nmf.test_initialize_close()
sklearn.decomposition.tests.test_nmf.test_initialize_nn_output()
sklearn.decomposition.tests.test_nmf.test_initialize_variants()
sklearn.decomposition.tests.test_nmf.test_mbnmf_inverse_transform()
sklearn.decomposition.tests.test_nmf.test_minibatch_nmf_negative_beta_loss(beta_loss)
sklearn.decomposition.tests.test_nmf.test_minibatch_nmf_partial_fit()
sklearn.decomposition.tests.test_nmf.test_minibatch_nmf_transform()
sklearn.decomposition.tests.test_nmf.test_minibatch_nmf_verbose()
sklearn.decomposition.tests.test_nmf.test_n_components_greater_n_features(Estimator)
sklearn.decomposition.tests.test_nmf.test_nmf_custom_init_dtype_error(Estimator)
sklearn.decomposition.tests.test_nmf.test_nmf_custom_init_shape_error()
sklearn.decomposition.tests.test_nmf.test_nmf_decreasing(solver)
sklearn.decomposition.tests.test_nmf.test_nmf_dtype_match(Estimator,solver,dtype_in,dtype_out)
sklearn.decomposition.tests.test_nmf.test_nmf_fit_close(Estimator,solver)
sklearn.decomposition.tests.test_nmf.test_nmf_fit_nn_output(Estimator,solver,init,alpha_W,alpha_H)
sklearn.decomposition.tests.test_nmf.test_nmf_float32_float64_consistency(Estimator,solver)
sklearn.decomposition.tests.test_nmf.test_nmf_inverse_transform(solver)
sklearn.decomposition.tests.test_nmf.test_nmf_minibatchnmf_equivalence(beta_loss)
sklearn.decomposition.tests.test_nmf.test_nmf_multiplicative_update_sparse(csr_container)
sklearn.decomposition.tests.test_nmf.test_nmf_n_components_auto(Estimator)
sklearn.decomposition.tests.test_nmf.test_nmf_n_components_auto_no_h_update()
sklearn.decomposition.tests.test_nmf.test_nmf_n_components_default_value_warning()
sklearn.decomposition.tests.test_nmf.test_nmf_negative_beta_loss(csr_container)
sklearn.decomposition.tests.test_nmf.test_nmf_non_negative_factorization_n_components_auto()
sklearn.decomposition.tests.test_nmf.test_nmf_regularization(Estimator,solver)
sklearn.decomposition.tests.test_nmf.test_nmf_sparse_input(Estimator,solver,sparse_container,alpha_W,alpha_H)
sklearn.decomposition.tests.test_nmf.test_nmf_sparse_transform(Estimator,solver,csc_container)
sklearn.decomposition.tests.test_nmf.test_nmf_transform(solver)
sklearn.decomposition.tests.test_nmf.test_nmf_transform_custom_init(Estimator,solver)
sklearn.decomposition.tests.test_nmf.test_nmf_true_reconstruction()
sklearn.decomposition.tests.test_nmf.test_nmf_underflow()
sklearn.decomposition.tests.test_nmf.test_nmf_w_h_not_used_warning()
sklearn.decomposition.tests.test_nmf.test_non_negative_factorization_checking()
sklearn.decomposition.tests.test_nmf.test_non_negative_factorization_consistency(init,solver,alpha_W,alpha_H)
sklearn.decomposition.tests.test_nmf.test_parameter_checking()
sklearn.decomposition.tests.test_nmf.test_special_sparse_dot(csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/_spectral_embedding.py----------------------------------------
A:sklearn.manifold._spectral_embedding.graph->check_array(graph, accept_sparse=True, accept_large_sparse=accept_large_sparse)
A:sklearn.manifold._spectral_embedding.connected_nodes->numpy.zeros(n_node, dtype=bool)
A:sklearn.manifold._spectral_embedding.nodes_to_explore->numpy.zeros(n_node, dtype=bool)
A:sklearn.manifold._spectral_embedding.last_num_component->numpy.zeros(n_node, dtype=bool).sum()
A:sklearn.manifold._spectral_embedding.neighbors->graph[[i], :].toarray().ravel()
A:sklearn.manifold._spectral_embedding.(n_connected_components, _)->connected_components(graph)
A:sklearn.manifold._spectral_embedding.laplacian->_set_diag(laplacian, 1, norm_laplacian)
A:sklearn.manifold._spectral_embedding.adjacency->check_symmetric(adjacency)
A:sklearn.manifold._spectral_embedding.random_state->check_random_state(self.random_state)
A:sklearn.manifold._spectral_embedding.(laplacian, dd)->csgraph_laplacian(adjacency, normed=norm_laplacian, return_diag=True)
A:sklearn.manifold._spectral_embedding.v0->_init_arpack_v0(laplacian.shape[0], random_state)
A:sklearn.manifold._spectral_embedding.(_, diffusion_map)->lobpcg(laplacian, X, tol=tol, largest=False, maxiter=2000)
A:sklearn.manifold._spectral_embedding.ml->smoothed_aggregation_solver(check_array(laplacian, accept_sparse='csr'))
A:sklearn.manifold._spectral_embedding.M->smoothed_aggregation_solver(check_array(laplacian, accept_sparse='csr')).aspreconditioner()
A:sklearn.manifold._spectral_embedding.X->self._validate_data(X, accept_sparse='csr', ensure_min_samples=2)
A:sklearn.manifold._spectral_embedding.X[:, 0]->dd.ravel()
A:sklearn.manifold._spectral_embedding.embedding->_deterministic_vector_sign_flip(embedding)
A:sklearn.manifold._spectral_embedding.estimator->NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs, metric='precomputed').fit(X)
A:sklearn.manifold._spectral_embedding.connectivity->NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs, metric='precomputed').fit(X).kneighbors_graph(X=X, mode='connectivity')
A:sklearn.manifold._spectral_embedding.self.affinity_matrix_->self.affinity(X)
A:sklearn.manifold._spectral_embedding.affinity_matrix->self._get_affinity_matrix(X)
A:sklearn.manifold._spectral_embedding.self.embedding_->spectral_embedding(affinity_matrix, n_components=self.n_components, eigen_solver=self.eigen_solver, eigen_tol=self.eigen_tol, random_state=random_state)
sklearn.manifold.SpectralEmbedding(self,n_components=2,*,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,eigen_tol='auto',n_neighbors=None,n_jobs=None)
sklearn.manifold.SpectralEmbedding._get_affinity_matrix(self,X,Y=None)
sklearn.manifold.SpectralEmbedding._more_tags(self)
sklearn.manifold.SpectralEmbedding.fit(self,X,y=None)
sklearn.manifold.SpectralEmbedding.fit_transform(self,X,y=None)
sklearn.manifold._spectral_embedding.SpectralEmbedding(self,n_components=2,*,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,eigen_tol='auto',n_neighbors=None,n_jobs=None)
sklearn.manifold._spectral_embedding.SpectralEmbedding.__init__(self,n_components=2,*,affinity='nearest_neighbors',gamma=None,random_state=None,eigen_solver=None,eigen_tol='auto',n_neighbors=None,n_jobs=None)
sklearn.manifold._spectral_embedding.SpectralEmbedding._get_affinity_matrix(self,X,Y=None)
sklearn.manifold._spectral_embedding.SpectralEmbedding._more_tags(self)
sklearn.manifold._spectral_embedding.SpectralEmbedding.fit(self,X,y=None)
sklearn.manifold._spectral_embedding.SpectralEmbedding.fit_transform(self,X,y=None)
sklearn.manifold._spectral_embedding._graph_connected_component(graph,node_id)
sklearn.manifold._spectral_embedding._graph_is_connected(graph)
sklearn.manifold._spectral_embedding._set_diag(laplacian,value,norm_laplacian)
sklearn.manifold._spectral_embedding.spectral_embedding(adjacency,*,n_components=8,eigen_solver=None,random_state=None,eigen_tol='auto',norm_laplacian=True,drop_first=True)
sklearn.manifold.spectral_embedding(adjacency,*,n_components=8,eigen_solver=None,random_state=None,eigen_tol='auto',norm_laplacian=True,drop_first=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/_locally_linear.py----------------------------------------
A:sklearn.manifold._locally_linear.X->self._validate_data(X, reset=False)
A:sklearn.manifold._locally_linear.Y->check_array(Y, dtype=FLOAT_DTYPES)
A:sklearn.manifold._locally_linear.indices->check_array(indices, dtype=int)
A:sklearn.manifold._locally_linear.B->numpy.empty((n_samples, n_neighbors), dtype=X.dtype)
A:sklearn.manifold._locally_linear.v->numpy.ones(n_neighbors, dtype=X.dtype)
A:sklearn.manifold._locally_linear.G->numpy.dot(C, C.T)
A:sklearn.manifold._locally_linear.trace->numpy.trace(G)
A:sklearn.manifold._locally_linear.w->solve(G, v, assume_a='pos')
A:sklearn.manifold._locally_linear.knn->NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs).fit(X)
A:sklearn.manifold._locally_linear.data->barycenter_weights(X, X, ind, reg=reg)
A:sklearn.manifold._locally_linear.indptr->numpy.arange(0, n_samples * n_neighbors + 1, n_neighbors)
A:sklearn.manifold._locally_linear.v0->_init_arpack_v0(M.shape[0], random_state)
A:sklearn.manifold._locally_linear.(eigen_values, eigen_vectors)->eigh(M, subset_by_index=(k_skip, k + k_skip - 1), overwrite_a=True)
A:sklearn.manifold._locally_linear.M->numpy.zeros((N, N))
A:sklearn.manifold._locally_linear.index->numpy.argsort(np.abs(eigen_values))
A:sklearn.manifold._locally_linear.nbrs->NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs)
A:sklearn.manifold._locally_linear.W->barycenter_kneighbors_graph(nbrs, n_neighbors=n_neighbors, reg=reg, n_jobs=n_jobs)
A:sklearn.manifold._locally_linear.neighbors->NearestNeighbors(n_neighbors=n_neighbors + 1, n_jobs=n_jobs).kneighbors(X, n_neighbors=n_neighbors + 1, return_distance=False)
A:sklearn.manifold._locally_linear.Yi->numpy.empty((n_neighbors, 1 + n_components + dp), dtype=np.float64)
A:sklearn.manifold._locally_linear.Ci->numpy.dot(Xi, Xi.T)
A:sklearn.manifold._locally_linear.(Q, R)->qr(Yi)
A:sklearn.manifold._locally_linear.S->solve(G, v, assume_a='pos').sum(0)
A:sklearn.manifold._locally_linear.(nbrs_x, nbrs_y)->numpy.meshgrid(neighbors[i], neighbors[i])
A:sklearn.manifold._locally_linear.V->numpy.zeros((N, n_neighbors, n_neighbors))
A:sklearn.manifold._locally_linear.nev->min(d_in, n_neighbors)
A:sklearn.manifold._locally_linear.evals->numpy.zeros([N, nev])
A:sklearn.manifold._locally_linear.(V[i], evals[i], _)->svd(X_nbrs, full_matrices=True)
A:sklearn.manifold._locally_linear.C_nbrs->numpy.dot(X_nbrs, X_nbrs.T)
A:sklearn.manifold._locally_linear.(evi, vi)->eigh(C_nbrs)
A:sklearn.manifold._locally_linear.tmp->numpy.dot(V.transpose(0, 2, 1), np.ones(n_neighbors))
A:sklearn.manifold._locally_linear.w_reg->numpy.zeros((N, n_neighbors))
A:sklearn.manifold._locally_linear.w_reg[i]->numpy.dot(V[i], tmp[i])
A:sklearn.manifold._locally_linear.eta->numpy.median(rho)
A:sklearn.manifold._locally_linear.s_range->numpy.zeros(N, dtype=int)
A:sklearn.manifold._locally_linear.evals_cumsum->stable_cumsum(evals, 1)
A:sklearn.manifold._locally_linear.s_range[i]->numpy.searchsorted(eta_range[i, ::-1], eta)
A:sklearn.manifold._locally_linear.norm_h->numpy.linalg.norm(h)
A:sklearn.manifold._locally_linear.Wi_sum1->Wi.sum(1)
A:sklearn.manifold._locally_linear.Gi->numpy.zeros((n_neighbors, n_components + 1))
A:sklearn.manifold._locally_linear.GiGiT->numpy.dot(Gi, Gi.T)
A:sklearn.manifold._locally_linear.self.nbrs_->NearestNeighbors(n_neighbors=self.n_neighbors, algorithm=self.neighbors_algorithm, n_jobs=self.n_jobs)
A:sklearn.manifold._locally_linear.random_state->check_random_state(self.random_state)
A:sklearn.manifold._locally_linear.(self.embedding_, self.reconstruction_error_)->locally_linear_embedding(X=self.nbrs_, n_neighbors=self.n_neighbors, n_components=self.n_components, eigen_solver=self.eigen_solver, tol=self.tol, max_iter=self.max_iter, method=self.method, hessian_tol=self.hessian_tol, modified_tol=self.modified_tol, random_state=random_state, reg=self.reg, n_jobs=self.n_jobs)
A:sklearn.manifold._locally_linear.ind->self.nbrs_.kneighbors(X, n_neighbors=self.n_neighbors, return_distance=False)
A:sklearn.manifold._locally_linear.weights->barycenter_weights(X, self.nbrs_._fit_X, ind, reg=self.reg)
A:sklearn.manifold._locally_linear.X_new->numpy.empty((X.shape[0], self.n_components))
A:sklearn.manifold._locally_linear.X_new[i]->numpy.dot(self.embedding_[ind[i]].T, weights[i])
sklearn.manifold.LocallyLinearEmbedding(self,*,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=None)
sklearn.manifold.LocallyLinearEmbedding._fit_transform(self,X)
sklearn.manifold.LocallyLinearEmbedding.fit(self,X,y=None)
sklearn.manifold.LocallyLinearEmbedding.fit_transform(self,X,y=None)
sklearn.manifold.LocallyLinearEmbedding.transform(self,X)
sklearn.manifold._locally_linear.LocallyLinearEmbedding(self,*,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=None)
sklearn.manifold._locally_linear.LocallyLinearEmbedding.__init__(self,*,n_neighbors=5,n_components=2,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,neighbors_algorithm='auto',random_state=None,n_jobs=None)
sklearn.manifold._locally_linear.LocallyLinearEmbedding._fit_transform(self,X)
sklearn.manifold._locally_linear.LocallyLinearEmbedding.fit(self,X,y=None)
sklearn.manifold._locally_linear.LocallyLinearEmbedding.fit_transform(self,X,y=None)
sklearn.manifold._locally_linear.LocallyLinearEmbedding.transform(self,X)
sklearn.manifold._locally_linear.barycenter_kneighbors_graph(X,n_neighbors,reg=0.001,n_jobs=None)
sklearn.manifold._locally_linear.barycenter_weights(X,Y,indices,reg=0.001)
sklearn.manifold._locally_linear.locally_linear_embedding(X,*,n_neighbors,n_components,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,random_state=None,n_jobs=None)
sklearn.manifold._locally_linear.null_space(M,k,k_skip=1,eigen_solver='arpack',tol=1e-06,max_iter=100,random_state=None)
sklearn.manifold.locally_linear_embedding(X,*,n_neighbors,n_components,reg=0.001,eigen_solver='auto',tol=1e-06,max_iter=100,method='standard',hessian_tol=0.0001,modified_tol=1e-12,random_state=None,n_jobs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py----------------------------------------
A:sklearn.manifold._t_sne.distances->pairwise_distances(X, metric=self.metric, n_jobs=self.n_jobs, **metric_params_)
A:sklearn.manifold._t_sne.conditional_P->_utils._binary_search_perplexity(distances_data, desired_perplexity, verbose)
A:sklearn.manifold._t_sne.sum_P->numpy.maximum(P.sum(), MACHINE_EPSILON)
A:sklearn.manifold._t_sne.P->_joint_probabilities_nn(distances_nn, self.perplexity, self.verbose)
A:sklearn.manifold._t_sne.t0->time()
A:sklearn.manifold._t_sne.distances_data->distances_data.astype(np.float32, copy=False).astype(np.float32, copy=False)
A:sklearn.manifold._t_sne.X_embedded->X_embedded.ravel().reshape(n_samples, self.n_components)
A:sklearn.manifold._t_sne.dist->pdist(X_embedded, 'sqeuclidean')
A:sklearn.manifold._t_sne.Q->numpy.maximum(dist / (2.0 * np.sum(dist)), MACHINE_EPSILON)
A:sklearn.manifold._t_sne.grad->grad.ravel().ravel()
A:sklearn.manifold._t_sne.PQd->squareform((P - Q) * dist)
A:sklearn.manifold._t_sne.grad[i]->numpy.dot(np.ravel(PQd[i], order='K'), X_embedded[i] - X_embedded)
A:sklearn.manifold._t_sne.params->X_embedded.ravel().reshape(n_samples, self.n_components).ravel()
A:sklearn.manifold._t_sne.val_P->_joint_probabilities_nn(distances_nn, self.perplexity, self.verbose).data.astype(np.float32, copy=False)
A:sklearn.manifold._t_sne.neighbors->_joint_probabilities_nn(distances_nn, self.perplexity, self.verbose).indices.astype(np.int64, copy=False)
A:sklearn.manifold._t_sne.indptr->_joint_probabilities_nn(distances_nn, self.perplexity, self.verbose).indptr.astype(np.int64, copy=False)
A:sklearn.manifold._t_sne.error->_barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr, grad, angle, n_components, verbose, dof=degrees_of_freedom, compute_error=compute_error, num_threads=num_threads)
A:sklearn.manifold._t_sne.p->p0.copy().ravel()
A:sklearn.manifold._t_sne.update->numpy.zeros_like(p)
A:sklearn.manifold._t_sne.gains->numpy.ones_like(p)
A:sklearn.manifold._t_sne.tic->time()
A:sklearn.manifold._t_sne.(error, grad)->objective(p, *args, **kwargs)
A:sklearn.manifold._t_sne.dec->numpy.invert(inc)
A:sklearn.manifold._t_sne.toc->time()
A:sklearn.manifold._t_sne.grad_norm->scipy.linalg.norm(grad)
A:sklearn.manifold._t_sne.n_samples->_num_samples(X)
A:sklearn.manifold._t_sne.dist_X->dist_X.copy().copy()
A:sklearn.manifold._t_sne.ind_X->numpy.argsort(dist_X, axis=1)
A:sklearn.manifold._t_sne.ind_X_embedded->NearestNeighbors(n_neighbors=n_neighbors).fit(X_embedded).kneighbors(return_distance=False)
A:sklearn.manifold._t_sne.inverted_index->numpy.zeros((n_samples, n_samples), dtype=int)
A:sklearn.manifold._t_sne.ordered_indices->numpy.arange(n_samples + 1)
A:sklearn.manifold._t_sne.t->numpy.sum(ranks[ranks > 0])
A:sklearn.manifold._t_sne.self.learning_rate_->numpy.maximum(self.learning_rate_, 50)
A:sklearn.manifold._t_sne.X->self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float32, np.float64])
A:sklearn.manifold._t_sne.random_state->check_random_state(self.random_state)
A:sklearn.manifold._t_sne.n_neighbors->min(n_samples - 1, int(3.0 * self.perplexity + 1))
A:sklearn.manifold._t_sne.knn->NearestNeighbors(algorithm='auto', n_jobs=self.n_jobs, n_neighbors=n_neighbors, metric=self.metric, metric_params=self.metric_params)
A:sklearn.manifold._t_sne.distances_nn->NearestNeighbors(algorithm='auto', n_jobs=self.n_jobs, n_neighbors=n_neighbors, metric=self.metric, metric_params=self.metric_params).kneighbors_graph(mode='distance')
A:sklearn.manifold._t_sne.pca->PCA(n_components=self.n_components, svd_solver='randomized', random_state=random_state)
A:sklearn.manifold._t_sne.degrees_of_freedom->max(self.n_components - 1, 1)
A:sklearn.manifold._t_sne.opt_args['kwargs']['num_threads']->_openmp_effective_n_threads()
A:sklearn.manifold._t_sne.(params, kl_divergence, it)->_gradient_descent(obj_func, params, **opt_args)
A:sklearn.manifold._t_sne.embedding->self._fit(X)
sklearn.manifold.TSNE(self,n_components=2,*,perplexity=30.0,early_exaggeration=12.0,learning_rate='auto',n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',metric_params=None,init='pca',verbose=0,random_state=None,method='barnes_hut',angle=0.5,n_jobs=None)
sklearn.manifold.TSNE._check_params_vs_input(self,X)
sklearn.manifold.TSNE._fit(self,X,skip_num_points=0)
sklearn.manifold.TSNE._more_tags(self)
sklearn.manifold.TSNE._n_features_out(self)
sklearn.manifold.TSNE._tsne(self,P,degrees_of_freedom,n_samples,X_embedded,neighbors=None,skip_num_points=0)
sklearn.manifold.TSNE.fit(self,X,y=None)
sklearn.manifold.TSNE.fit_transform(self,X,y=None)
sklearn.manifold._t_sne.TSNE(self,n_components=2,*,perplexity=30.0,early_exaggeration=12.0,learning_rate='auto',n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',metric_params=None,init='pca',verbose=0,random_state=None,method='barnes_hut',angle=0.5,n_jobs=None)
sklearn.manifold._t_sne.TSNE.__init__(self,n_components=2,*,perplexity=30.0,early_exaggeration=12.0,learning_rate='auto',n_iter=1000,n_iter_without_progress=300,min_grad_norm=1e-07,metric='euclidean',metric_params=None,init='pca',verbose=0,random_state=None,method='barnes_hut',angle=0.5,n_jobs=None)
sklearn.manifold._t_sne.TSNE._check_params_vs_input(self,X)
sklearn.manifold._t_sne.TSNE._fit(self,X,skip_num_points=0)
sklearn.manifold._t_sne.TSNE._more_tags(self)
sklearn.manifold._t_sne.TSNE._n_features_out(self)
sklearn.manifold._t_sne.TSNE._tsne(self,P,degrees_of_freedom,n_samples,X_embedded,neighbors=None,skip_num_points=0)
sklearn.manifold._t_sne.TSNE.fit(self,X,y=None)
sklearn.manifold._t_sne.TSNE.fit_transform(self,X,y=None)
sklearn.manifold._t_sne._gradient_descent(objective,p0,it,n_iter,n_iter_check=1,n_iter_without_progress=300,momentum=0.8,learning_rate=200.0,min_gain=0.01,min_grad_norm=1e-07,verbose=0,args=None,kwargs=None)
sklearn.manifold._t_sne._joint_probabilities(distances,desired_perplexity,verbose)
sklearn.manifold._t_sne._joint_probabilities_nn(distances,desired_perplexity,verbose)
sklearn.manifold._t_sne._kl_divergence(params,P,degrees_of_freedom,n_samples,n_components,skip_num_points=0,compute_error=True)
sklearn.manifold._t_sne._kl_divergence_bh(params,P,degrees_of_freedom,n_samples,n_components,angle=0.5,skip_num_points=0,verbose=False,compute_error=True,num_threads=1)
sklearn.manifold._t_sne.trustworthiness(X,X_embedded,*,n_neighbors=5,metric='euclidean')
sklearn.manifold.trustworthiness(X,X_embedded,*,n_neighbors=5,metric='euclidean')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/_isomap.py----------------------------------------
A:sklearn.manifold._isomap.self.nbrs_->NearestNeighbors(n_neighbors=self.n_neighbors, radius=self.radius, algorithm=self.neighbors_algorithm, metric=self.metric, p=self.p, metric_params=self.metric_params, n_jobs=self.n_jobs)
A:sklearn.manifold._isomap.self.kernel_pca_->KernelPCA(n_components=self.n_components, kernel='precomputed', eigen_solver=self.eigen_solver, tol=self.tol, max_iter=self.max_iter, n_jobs=self.n_jobs).set_output(transform='default')
A:sklearn.manifold._isomap.nbg->_fix_connected_components(X=self.nbrs_._fit_X, graph=nbg, n_connected_components=n_connected_components, component_labels=labels, mode='distance', metric=self.nbrs_.effective_metric_, **self.nbrs_.effective_metric_params_)
A:sklearn.manifold._isomap.(n_connected_components, labels)->connected_components(nbg)
A:sklearn.manifold._isomap.self.dist_matrix_->self.dist_matrix_.astype(self.nbrs_._fit_X.dtype, copy=False)
A:sklearn.manifold._isomap.self.embedding_->self.kernel_pca_.fit_transform(G)
A:sklearn.manifold._isomap.G_center->KernelCenterer().fit_transform(G)
A:sklearn.manifold._isomap.(distances, indices)->self.nbrs_.radius_neighbors(X, return_distance=True)
A:sklearn.manifold._isomap.G_X->numpy.zeros((n_queries, n_samples_fit), dtype)
A:sklearn.manifold._isomap.G_X[i]->numpy.min(self.dist_matrix_[indices[i]] + distances[i][:, None], 0)
sklearn.manifold.Isomap(self,*,n_neighbors=5,radius=None,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=None,metric='minkowski',p=2,metric_params=None)
sklearn.manifold.Isomap._fit_transform(self,X)
sklearn.manifold.Isomap._more_tags(self)
sklearn.manifold.Isomap.fit(self,X,y=None)
sklearn.manifold.Isomap.fit_transform(self,X,y=None)
sklearn.manifold.Isomap.reconstruction_error(self)
sklearn.manifold.Isomap.transform(self,X)
sklearn.manifold._isomap.Isomap(self,*,n_neighbors=5,radius=None,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=None,metric='minkowski',p=2,metric_params=None)
sklearn.manifold._isomap.Isomap.__init__(self,*,n_neighbors=5,radius=None,n_components=2,eigen_solver='auto',tol=0,max_iter=None,path_method='auto',neighbors_algorithm='auto',n_jobs=None,metric='minkowski',p=2,metric_params=None)
sklearn.manifold._isomap.Isomap._fit_transform(self,X)
sklearn.manifold._isomap.Isomap._more_tags(self)
sklearn.manifold._isomap.Isomap.fit(self,X,y=None)
sklearn.manifold._isomap.Isomap.fit_transform(self,X,y=None)
sklearn.manifold._isomap.Isomap.reconstruction_error(self)
sklearn.manifold._isomap.Isomap.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/_mds.py----------------------------------------
A:sklearn.manifold._mds.dissimilarities->check_array(dissimilarities)
A:sklearn.manifold._mds.random_state->check_random_state(random_state)
A:sklearn.manifold._mds.sim_flat->((1 - np.tri(n_samples)) * dissimilarities).ravel()
A:sklearn.manifold._mds.X->self._validate_data(X)
A:sklearn.manifold._mds.ir->IsotonicRegression()
A:sklearn.manifold._mds.dis->numpy.sqrt((X ** 2).sum(axis=1)).sum()
A:sklearn.manifold._mds.dis_flat->numpy.sqrt((X ** 2).sum(axis=1)).sum().ravel()
A:sklearn.manifold._mds.disparities_flat->IsotonicRegression().fit_transform(sim_flat_w, dis_flat_w)
A:sklearn.manifold._mds.disparities->disparities.reshape((n_samples, n_samples)).reshape((n_samples, n_samples))
A:sklearn.manifold._mds.stress->numpy.sqrt(stress / ((disparities.ravel() ** 2).sum() / 2))
A:sklearn.manifold._mds.init->numpy.asarray(init).copy()
A:sklearn.manifold._mds.(pos, stress, n_iter_)->_smacof_single(dissimilarities, metric=metric, n_components=n_components, init=init, max_iter=max_iter, verbose=verbose, eps=eps, random_state=random_state, normalized_stress=normalized_stress)
A:sklearn.manifold._mds.best_pos->pos.copy()
A:sklearn.manifold._mds.seeds->check_random_state(random_state).randint(np.iinfo(np.int32).max, size=n_init)
A:sklearn.manifold._mds.results->Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))((delayed(_smacof_single)(dissimilarities, metric=metric, n_components=n_components, init=init, max_iter=max_iter, verbose=verbose, eps=eps, random_state=seed, normalized_stress=normalized_stress) for seed in seeds))
A:sklearn.manifold._mds.(positions, stress, n_iters)->zip(*results)
A:sklearn.manifold._mds.best->numpy.argmin(stress)
A:sklearn.manifold._mds.self.dissimilarity_matrix_->euclidean_distances(X)
A:sklearn.manifold._mds.(self.embedding_, self.stress_, self.n_iter_)->smacof(self.dissimilarity_matrix_, metric=self.metric, n_components=self.n_components, init=init, n_init=self.n_init, n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose, eps=self.eps, random_state=self.random_state, return_n_iter=True, normalized_stress=self.normalized_stress)
sklearn.manifold.MDS(self,n_components=2,*,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=None,random_state=None,dissimilarity='euclidean',normalized_stress='auto')
sklearn.manifold.MDS._more_tags(self)
sklearn.manifold.MDS.fit(self,X,y=None,init=None)
sklearn.manifold.MDS.fit_transform(self,X,y=None,init=None)
sklearn.manifold._mds.MDS(self,n_components=2,*,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=None,random_state=None,dissimilarity='euclidean',normalized_stress='auto')
sklearn.manifold._mds.MDS.__init__(self,n_components=2,*,metric=True,n_init=4,max_iter=300,verbose=0,eps=0.001,n_jobs=None,random_state=None,dissimilarity='euclidean',normalized_stress='auto')
sklearn.manifold._mds.MDS._more_tags(self)
sklearn.manifold._mds.MDS.fit(self,X,y=None,init=None)
sklearn.manifold._mds.MDS.fit_transform(self,X,y=None,init=None)
sklearn.manifold._mds._smacof_single(dissimilarities,metric=True,n_components=2,init=None,max_iter=300,verbose=0,eps=0.001,random_state=None,normalized_stress=False)
sklearn.manifold._mds.smacof(dissimilarities,*,metric=True,n_components=2,init=None,n_init=8,n_jobs=None,max_iter=300,verbose=0,eps=0.001,random_state=None,return_n_iter=False,normalized_stress='auto')
sklearn.manifold.smacof(dissimilarities,*,metric=True,n_components=2,init=None,n_init=8,n_jobs=None,max_iter=300,verbose=0,eps=0.001,random_state=None,return_n_iter=False,normalized_stress='auto')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/test_locally_linear.py----------------------------------------
A:sklearn.manifold.tests.test_locally_linear.X->numpy.random.RandomState(0).randint(0, 100, size=(20, 3))
A:sklearn.manifold.tests.test_locally_linear.graph->barycenter_kneighbors_graph(X, 2)
A:sklearn.manifold.tests.test_locally_linear.expected_graph->numpy.array([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], dtype=global_dtype)
A:sklearn.manifold.tests.test_locally_linear.pred->numpy.dot(graph.toarray(), X)
A:sklearn.manifold.tests.test_locally_linear.rng->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_locally_linear.clf->sklearn.manifold.LocallyLinearEmbedding(method=method, n_neighbors=10)
A:sklearn.manifold.tests.test_locally_linear.N->barycenter_kneighbors_graph(X, clf.n_neighbors).toarray()
A:sklearn.manifold.tests.test_locally_linear.reconstruction_error->scipy.linalg.norm(np.dot(N, X) - X)
A:sklearn.manifold.tests.test_locally_linear.X_reembedded->sklearn.manifold.LocallyLinearEmbedding(method=method, n_neighbors=10).transform(X + noise)
A:sklearn.manifold.tests.test_locally_linear.(X, y)->make_blobs(random_state=0, n_features=4)
A:sklearn.manifold.tests.test_locally_linear.M->numpy.ones((200, 3))
A:sklearn.manifold.tests.test_locally_linear.rand->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_locally_linear.iso->sklearn.manifold.LocallyLinearEmbedding(n_components=n_components)
A:sklearn.manifold.tests.test_locally_linear.names->sklearn.manifold.LocallyLinearEmbedding(n_components=n_components).get_feature_names_out()
sklearn.manifold.tests.test_locally_linear.test_barycenter_kneighbors_graph(global_dtype)
sklearn.manifold.tests.test_locally_linear.test_get_feature_names_out()
sklearn.manifold.tests.test_locally_linear.test_integer_input()
sklearn.manifold.tests.test_locally_linear.test_lle_manifold(global_dtype,method,solver)
sklearn.manifold.tests.test_locally_linear.test_lle_simple_grid(global_dtype)
sklearn.manifold.tests.test_locally_linear.test_pipeline()
sklearn.manifold.tests.test_locally_linear.test_singular_matrix()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/test_t_sne.py----------------------------------------
A:sklearn.manifold.tests.test_t_sne.x->numpy.linspace(0, 1, 10)
A:sklearn.manifold.tests.test_t_sne.(xx, yy)->numpy.meshgrid(x, x)
A:sklearn.manifold.tests.test_t_sne.X_2d_grid->numpy.hstack([xx.ravel().reshape(-1, 1), yy.ravel().reshape(-1, 1)])
A:sklearn.manifold.tests.test_t_sne.sys.stdout->StringIO()
A:sklearn.manifold.tests.test_t_sne.(_, error, it)->_gradient_descent(ObjectiveSmallGradient(), np.zeros(1), 0, n_iter=11, n_iter_without_progress=100, momentum=0.0, learning_rate=0.0, min_gain=0.0, min_grad_norm=0.0, verbose=2)
A:sklearn.manifold.tests.test_t_sne.out->sys.stdout.getvalue()
A:sklearn.manifold.tests.test_t_sne.random_state->check_random_state(0)
A:sklearn.manifold.tests.test_t_sne.data->check_random_state(0).randn(n_samples, n_features).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.distances->pairwise_distances(data)
A:sklearn.manifold.tests.test_t_sne.P->squareform(P)
A:sklearn.manifold.tests.test_t_sne.mean_perplexity->numpy.mean([np.exp(-np.sum(P[i] * np.log(P[i]))) for i in range(P.shape[0])])
A:sklearn.manifold.tests.test_t_sne.P1->P1.toarray().toarray()
A:sklearn.manifold.tests.test_t_sne.nn->NearestNeighbors(n_neighbors=1).fit(Y)
A:sklearn.manifold.tests.test_t_sne.distance_graph->NearestNeighbors(n_neighbors=1).fit(Y).kneighbors_graph(n_neighbors=n_neighbors, mode='distance')
A:sklearn.manifold.tests.test_t_sne.distances_nn->distances_nn.reshape(n_samples, k).reshape(n_samples, k)
A:sklearn.manifold.tests.test_t_sne.P2->_binary_search_perplexity(distances_nn, desired_perplexity, verbose=0)
A:sklearn.manifold.tests.test_t_sne.P1_nn->numpy.array([P1[k, distance_graph.indices[indptr[k]:indptr[k + 1]]] for k in range(n_samples)])
A:sklearn.manifold.tests.test_t_sne.k->int(k)
A:sklearn.manifold.tests.test_t_sne.P2k->_binary_search_perplexity(distances_nn, desired_perplexity, verbose=0)
A:sklearn.manifold.tests.test_t_sne.X_embedded->TSNE(metric='mahalanobis', **default_params).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X->check_random_state(0).randn(20, 2)
A:sklearn.manifold.tests.test_t_sne.rng->check_random_state(0)
A:sklearn.manifold.tests.test_t_sne.trust->trustworthiness(X, X_embedded, n_neighbors=3)
A:sklearn.manifold.tests.test_t_sne.tsne->TSNE(metric='mahalanobis', **default_params)
A:sklearn.manifold.tests.test_t_sne.t->trustworthiness(D, X_embedded, n_neighbors=1, metric='precomputed')
A:sklearn.manifold.tests.test_t_sne.(X, _)->make_blobs(n_features=3, random_state=random_state)
A:sklearn.manifold.tests.test_t_sne.X_csr->csr_container(X)
A:sklearn.manifold.tests.test_t_sne.D->pairwise_distances(X)
A:sklearn.manifold.tests.test_t_sne.dist->numpy.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 0.0, 0.0]])
A:sklearn.manifold.tests.test_t_sne.bad_dist->csr_container(dist)
A:sklearn.manifold.tests.test_t_sne.D_sparse->kneighbors_graph(X, n_neighbors=100, mode='distance', include_self=True)
A:sklearn.manifold.tests.test_t_sne.Xt_dense->TSNE(metric='mahalanobis', **default_params).fit_transform(D)
A:sklearn.manifold.tests.test_t_sne.Xt_sparse->TSNE(metric='mahalanobis', **default_params).fit_transform(sparse_container(D_sparse))
A:sklearn.manifold.tests.test_t_sne.X_embedded1->TSNE(metric='mahalanobis', **default_params).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_embedded2->TSNE(metric='mahalanobis', **default_params).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.pos_input->numpy.array([[1.0, 0.0], [0.0, 1.0], [5.0, 2.0], [7.3, 2.2]])
A:sklearn.manifold.tests.test_t_sne.pos_output->pos_output.astype(np.float32).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.neighbors->squareform(P).indices.astype(np.int64)
A:sklearn.manifold.tests.test_t_sne.grad_output->numpy.array([[0.0, 0.0], [0.0, 0.0], [4.24275173e-08, -3.69569698e-08], [-2.58720939e-09, 7.52706374e-09]])
A:sklearn.manifold.tests.test_t_sne.pij_input->squareform(pij_input).astype(np.float32)
A:sklearn.manifold.tests.test_t_sne.grad_bh->numpy.zeros(pos_output.shape, dtype=np.float32)
A:sklearn.manifold.tests.test_t_sne.indptr->squareform(P).indptr.astype(np.int64)
A:sklearn.manifold.tests.test_t_sne.degrees_of_freedom->float(n_components - 1.0)
A:sklearn.manifold.tests.test_t_sne.params->check_random_state(0).randn(n_samples, n_components)
A:sklearn.manifold.tests.test_t_sne.(kl_exact, grad_exact)->_kl_divergence(params, P, degrees_of_freedom, n_samples, n_components)
A:sklearn.manifold.tests.test_t_sne.distances_csr->NearestNeighbors().fit(data).kneighbors_graph(n_neighbors=n_neighbors, mode='distance')
A:sklearn.manifold.tests.test_t_sne.P_bh->_joint_probabilities_nn(distances_csr, perplexity, verbose=0)
A:sklearn.manifold.tests.test_t_sne.(kl_bh, grad_bh)->_kl_divergence_bh(params, P_bh, degrees_of_freedom, n_samples, n_components, angle=angle, skip_num_points=0, verbose=0)
A:sklearn.manifold.tests.test_t_sne.lines_out->sys.stdout.getvalue().split('\n')
A:sklearn.manifold.tests.test_t_sne.start_grad_norm->line.find('gradient norm')
A:sklearn.manifold.tests.test_t_sne.gradient_norm_values->numpy.array(gradient_norm_values)
A:sklearn.manifold.tests.test_t_sne.n_smaller_gradient_norms->len(gradient_norm_values[gradient_norm_values <= min_grad_norm])
A:sklearn.manifold.tests.test_t_sne.(_, _, error)->line.partition('error = ')
A:sklearn.manifold.tests.test_t_sne.(error, _, _)->error.partition(',')
A:sklearn.manifold.tests.test_t_sne.seeds->range(3)
A:sklearn.manifold.tests.test_t_sne.Y->TSNE(metric='mahalanobis', **default_params).fit_transform(X_2d_grid)
A:sklearn.manifold.tests.test_t_sne.try_name->'{}_{}'.format(method, seed)
A:sklearn.manifold.tests.test_t_sne.dist_to_nn->NearestNeighbors(n_neighbors=1).fit(Y).kneighbors(return_distance=True)[0].ravel()
A:sklearn.manifold.tests.test_t_sne.X_embeddeds[method]->TSNE(metric='mahalanobis', **default_params).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.(kl_sequential, grad_sequential)->_kl_divergence_bh(params, P_bh, degrees_of_freedom, n_samples, n_components, angle=angle, skip_num_points=0, verbose=0, num_threads=1)
A:sklearn.manifold.tests.test_t_sne.(kl_multithread, grad_multithread)->_kl_divergence_bh(params, P_bh, degrees_of_freedom, n_samples, n_components, angle=angle, skip_num_points=0, verbose=0, num_threads=num_threads)
A:sklearn.manifold.tests.test_t_sne.X_transformed_tsne->TSNE(metric=metric, method=method, n_components=n_components_embedding, random_state=0, n_iter=300, init='random', learning_rate='auto').fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_transformed_tsne_precomputed->TSNE(metric='precomputed', method=method, n_components=n_components_embedding, random_state=0, n_iter=300, init='random', learning_rate='auto').fit_transform(dist_func(X))
A:sklearn.manifold.tests.test_t_sne.X_tr_ref->TSNE(n_components=2, method=method, perplexity=25.0, angle=0, n_jobs=1, random_state=0, init='random', learning_rate='auto').fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.X_tr->TSNE(n_components=2, method=method, perplexity=25.0, angle=0, n_jobs=2, random_state=0, init='random', learning_rate='auto').fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.precomputed_X->squareform(pdist(X, metric='mahalanobis'), checks=True)
A:sklearn.manifold.tests.test_t_sne.X_trans_expected->TSNE(metric='precomputed', **default_params).fit_transform(precomputed_X)
A:sklearn.manifold.tests.test_t_sne.X_trans->TSNE(metric='mahalanobis', metric_params={'V': np.cov(X.T)}, **default_params).fit_transform(X)
A:sklearn.manifold.tests.test_t_sne.est->TSNE(learning_rate='auto', init='pca', perplexity=perplexity, random_state=random_state)
A:sklearn.manifold.tests.test_t_sne.arr->numpy.arange(35 * 4).reshape(35, 4)
sklearn.manifold.tests.test_t_sne._run_answer_test(pos_input,pos_output,neighbors,grad_output,csr_container,verbose=False,perplexity=0.1,skip_num_points=0)
sklearn.manifold.tests.test_t_sne.assert_uniform_grid(Y,try_name=None)
sklearn.manifold.tests.test_t_sne.test_64bit(method,dt)
sklearn.manifold.tests.test_t_sne.test_accessible_kl_divergence()
sklearn.manifold.tests.test_t_sne.test_answer_gradient_four_points(csr_container)
sklearn.manifold.tests.test_t_sne.test_answer_gradient_two_points(csr_container)
sklearn.manifold.tests.test_t_sne.test_bad_precomputed_distances(method,D,retype,message_regex)
sklearn.manifold.tests.test_t_sne.test_barnes_hut_angle()
sklearn.manifold.tests.test_t_sne.test_bh_match_exact()
sklearn.manifold.tests.test_t_sne.test_binary_perplexity_stability()
sklearn.manifold.tests.test_t_sne.test_binary_search()
sklearn.manifold.tests.test_t_sne.test_binary_search_neighbors()
sklearn.manifold.tests.test_t_sne.test_binary_search_underflow()
sklearn.manifold.tests.test_t_sne.test_chebyshev_metric()
sklearn.manifold.tests.test_t_sne.test_early_exaggeration_used()
sklearn.manifold.tests.test_t_sne.test_exact_no_precomputed_sparse(csr_container)
sklearn.manifold.tests.test_t_sne.test_fit_transform_csr_matrix(method,csr_container)
sklearn.manifold.tests.test_t_sne.test_gradient()
sklearn.manifold.tests.test_t_sne.test_gradient_bh_multithread_match_sequential()
sklearn.manifold.tests.test_t_sne.test_gradient_descent_stops()
sklearn.manifold.tests.test_t_sne.test_high_perplexity_precomputed_sparse_distances(csr_container)
sklearn.manifold.tests.test_t_sne.test_init_ndarray()
sklearn.manifold.tests.test_t_sne.test_init_ndarray_precomputed()
sklearn.manifold.tests.test_t_sne.test_kl_divergence_not_nan(method)
sklearn.manifold.tests.test_t_sne.test_min_grad_norm()
sklearn.manifold.tests.test_t_sne.test_n_components_range()
sklearn.manifold.tests.test_t_sne.test_n_iter_used()
sklearn.manifold.tests.test_t_sne.test_n_iter_without_progress()
sklearn.manifold.tests.test_t_sne.test_non_positive_computed_distances()
sklearn.manifold.tests.test_t_sne.test_optimization_minimizes_kl_divergence()
sklearn.manifold.tests.test_t_sne.test_pca_initialization_not_compatible_with_precomputed_kernel()
sklearn.manifold.tests.test_t_sne.test_pca_initialization_not_compatible_with_sparse_input(csr_container)
sklearn.manifold.tests.test_t_sne.test_preserve_trustworthiness_approximately(method,init)
sklearn.manifold.tests.test_t_sne.test_preserve_trustworthiness_approximately_with_precomputed_distances()
sklearn.manifold.tests.test_t_sne.test_reduction_to_one_component()
sklearn.manifold.tests.test_t_sne.test_skip_num_points_gradient(csr_container)
sklearn.manifold.tests.test_t_sne.test_sparse_precomputed_distance(sparse_container)
sklearn.manifold.tests.test_t_sne.test_trustworthiness()
sklearn.manifold.tests.test_t_sne.test_trustworthiness_n_neighbors_error()
sklearn.manifold.tests.test_t_sne.test_trustworthiness_not_euclidean_metric()
sklearn.manifold.tests.test_t_sne.test_tsne_n_jobs(method)
sklearn.manifold.tests.test_t_sne.test_tsne_perplexity_validation(perplexity)
sklearn.manifold.tests.test_t_sne.test_tsne_with_different_distance_metrics(metric,dist_func,method)
sklearn.manifold.tests.test_t_sne.test_tsne_with_mahalanobis_distance()
sklearn.manifold.tests.test_t_sne.test_tsne_works_with_pandas_output()
sklearn.manifold.tests.test_t_sne.test_uniform_grid(method)
sklearn.manifold.tests.test_t_sne.test_verbose()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/test_spectral_embedding.py----------------------------------------
A:sklearn.manifold.tests.test_spectral_embedding.skip_if_no_pyamg->pytest.mark.skipif(not pyamg_available, reason='PyAMG is required for the tests in this function.')
A:sklearn.manifold.tests.test_spectral_embedding.centers->numpy.array([[0.0, 5.0, 0.0, 0.0, 0.0], [0.0, 0.0, 4.0, 0.0, 0.0], [1.0, 0.0, 0.0, 5.0, 1.0]])
A:sklearn.manifold.tests.test_spectral_embedding.(S, true_labels)->make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.manifold.tests.test_spectral_embedding.rng->numpy.random.RandomState(42)
A:sklearn.manifold.tests.test_spectral_embedding.p->numpy.random.RandomState(42).permutation(n_samples)
A:sklearn.manifold.tests.test_spectral_embedding.source->numpy.random.RandomState(42).randint(min_idx, max_idx, size=n_random_connections)
A:sklearn.manifold.tests.test_spectral_embedding.target->numpy.random.RandomState(42).randint(min_idx, max_idx, size=n_random_connections)
A:sklearn.manifold.tests.test_spectral_embedding.(row_idx, column_idx)->tuple(np.array(connections).T)
A:sklearn.manifold.tests.test_spectral_embedding.data->numpy.random.RandomState(36).randn(10, 30)
A:sklearn.manifold.tests.test_spectral_embedding.affinity->affinity.tocsr().tocsr()
A:sklearn.manifold.tests.test_spectral_embedding.component_1->_graph_connected_component(affinity, p[start])
A:sklearn.manifold.tests.test_spectral_embedding.component_2->_graph_connected_component(affinity, p[stop - 1])
A:sklearn.manifold.tests.test_spectral_embedding.random_state->numpy.random.RandomState(36)
A:sklearn.manifold.tests.test_spectral_embedding.component->_graph_connected_component(affinity, -1)
A:sklearn.manifold.tests.test_spectral_embedding.true_label->numpy.zeros(shape=2 * n_sample)
A:sklearn.manifold.tests.test_spectral_embedding.se_precomp->SpectralEmbedding(n_components=2, affinity='rbf', eigen_solver='amg')
A:sklearn.manifold.tests.test_spectral_embedding.embedded_coordinate->SpectralEmbedding(n_components=2, affinity='rbf', eigen_solver='amg').fit_transform(affinity.astype(dtype))
A:sklearn.manifold.tests.test_spectral_embedding.label_->numpy.array(embedded_coordinate.ravel() < 0, dtype=np.int64)
A:sklearn.manifold.tests.test_spectral_embedding.se_rbf->SpectralEmbedding(n_components=n_clusters, affinity='rbf', random_state=random_state)
A:sklearn.manifold.tests.test_spectral_embedding.embed_precomp->SpectralEmbedding(n_components=2, affinity='rbf', eigen_solver='amg').fit_transform(rbf_kernel(X.astype(dtype), gamma=gamma))
A:sklearn.manifold.tests.test_spectral_embedding.embed_rbf->SpectralEmbedding(n_components=n_clusters, affinity='rbf', random_state=random_state).fit_transform(X)
A:sklearn.manifold.tests.test_spectral_embedding.nn->NearestNeighbors(n_neighbors=n_neighbors + additional_neighbors).fit(S)
A:sklearn.manifold.tests.test_spectral_embedding.graph->numpy.array([[1, 1, 0, 0, 0], [1, 1, 1, 0, 0], [0, 1, 1, 1, 0], [0, 0, 1, 1, 1], [0, 0, 0, 1, 1]])
A:sklearn.manifold.tests.test_spectral_embedding.kern->rbf_kernel(S, gamma=gamma)
A:sklearn.manifold.tests.test_spectral_embedding.se_callable->SpectralEmbedding(n_components=2, affinity=lambda x: rbf_kernel(x, gamma=gamma), gamma=gamma, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.embed_callable->SpectralEmbedding(n_components=2, affinity=lambda x: rbf_kernel(x, gamma=gamma), gamma=gamma, random_state=np.random.RandomState(seed)).fit_transform(X)
A:sklearn.manifold.tests.test_spectral_embedding.se_amg->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='amg', n_neighbors=5, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.se_arpack->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='arpack', n_neighbors=5, random_state=np.random.RandomState(seed))
A:sklearn.manifold.tests.test_spectral_embedding.embed_amg->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='amg', n_neighbors=5, random_state=np.random.RandomState(seed)).fit_transform(affinity.astype(dtype))
A:sklearn.manifold.tests.test_spectral_embedding.embed_arpack->SpectralEmbedding(n_components=2, affinity='nearest_neighbors', eigen_solver='arpack', n_neighbors=5, random_state=np.random.RandomState(seed)).fit_transform(affinity.astype(dtype))
A:sklearn.manifold.tests.test_spectral_embedding.row->numpy.array([0, 0, 1, 2, 3, 3, 4], dtype=np.int32)
A:sklearn.manifold.tests.test_spectral_embedding.col->numpy.array([1, 2, 2, 3, 4, 5, 5], dtype=np.int32)
A:sklearn.manifold.tests.test_spectral_embedding.val->numpy.array([100, 100, 100, 1, 100, 100, 100], dtype=np.int64)
A:sklearn.manifold.tests.test_spectral_embedding.affinity.indptr->affinity.tocsr().tocsr().indptr.astype(np.int64)
A:sklearn.manifold.tests.test_spectral_embedding.affinity.indices->affinity.tocsr().tocsr().indices.astype(np.int64)
A:sklearn.manifold.tests.test_spectral_embedding.X->csr_container(S).astype(dtype)
A:sklearn.manifold.tests.test_spectral_embedding.embedding->spectral_embedding(sims, norm_laplacian=False, n_components=n_components, drop_first=False, random_state=seed)
A:sklearn.manifold.tests.test_spectral_embedding.new_embedding->spectral_embedding(sym_matrix, n_components=10, eigen_solver='amg', random_state=i + 1)
A:sklearn.manifold.tests.test_spectral_embedding.se_knn->SpectralEmbedding(n_components=n_clusters, affinity='nearest_neighbors', n_neighbors=5, random_state=random_state)
A:sklearn.manifold.tests.test_spectral_embedding.km->KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
A:sklearn.manifold.tests.test_spectral_embedding.sims->rbf_kernel(data)
A:sklearn.manifold.tests.test_spectral_embedding.embedding_1->spectral_embedding(sims, norm_laplacian=False, n_components=n_components, drop_first=False)
A:sklearn.manifold.tests.test_spectral_embedding.embedding_2->spectral_embedding(sims)
A:sklearn.manifold.tests.test_spectral_embedding.(laplacian, dd)->csgraph_laplacian(sims, normed=False, return_diag=True)
A:sklearn.manifold.tests.test_spectral_embedding.(_, diffusion_map)->eigh(laplacian)
A:sklearn.manifold.tests.test_spectral_embedding.se->SpectralEmbedding(n_components=2, affinity='rbf', eigen_solver=eigen_solver, random_state=0)
A:sklearn.manifold.tests.test_spectral_embedding.X_trans->SpectralEmbedding(n_components=2, affinity='rbf', eigen_solver=eigen_solver, random_state=0).fit_transform(X)
A:sklearn.manifold.tests.test_spectral_embedding.(X, _)->make_blobs(n_samples=200, random_state=0, centers=[[1, 1], [-1, -1]], cluster_std=0.01)
A:sklearn.manifold.tests.test_spectral_embedding.D->pairwise_distances(X)
A:sklearn.manifold.tests.test_spectral_embedding.S->csr_container(S)
A:sklearn.manifold.tests.test_spectral_embedding.mocked_solver->Mock(side_effect=solver_func)
sklearn.manifold.tests.test_spectral_embedding._assert_equal_with_sign_flipping(A,B,tol=0.0)
sklearn.manifold.tests.test_spectral_embedding.test_connectivity(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_error_pyamg_not_available()
sklearn.manifold.tests.test_spectral_embedding.test_pipeline_spectral_clustering(seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_precomputed_nearest_neighbors_filtering()
sklearn.manifold.tests.test_spectral_embedding.test_sparse_graph_connected_component(coo_container)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_eigen_tol_auto(monkeypatch,solver,csr_container)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_amg_solver(dtype,coo_container,seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_amg_solver_failure(dtype,seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_callable_affinity(sparse_container,seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_deterministic()
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_first_eigen_vector()
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_precomputed_affinity(sparse_container,eigen_solver,dtype,seed=36)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_preserves_dtype(eigen_solver,dtype)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_two_components(eigen_solver,dtype,seed=0)
sklearn.manifold.tests.test_spectral_embedding.test_spectral_embedding_unnormalized()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/test_mds.py----------------------------------------
A:sklearn.manifold.tests.test_mds.sim->numpy.array([[0, 5, 3, 4], [5, 0, 2, 2], [3, 2, 0, 1], [4, 2, 1, 0]])
A:sklearn.manifold.tests.test_mds.Z->numpy.array([[-0.266, -0.539], [0.016, -0.238], [-0.2, 0.524]])
A:sklearn.manifold.tests.test_mds.(X, _)->sklearn.manifold._mds.smacof(sim, init=Z, n_components=2, max_iter=1, n_init=1)
A:sklearn.manifold.tests.test_mds.X_true->numpy.array([[-1.415, -2.471], [1.633, 1.107], [0.249, -0.067], [-0.468, 1.431]])
A:sklearn.manifold.tests.test_mds.mds_clf->sklearn.manifold._mds.MDS(metric=False, n_jobs=3, dissimilarity='precomputed')
A:sklearn.manifold.tests.test_mds.(X1, stress1)->sklearn.manifold._mds.smacof(sim, metric=False, max_iter=5, random_state=0)
A:sklearn.manifold.tests.test_mds.(X2, stress2)->sklearn.manifold._mds.smacof(k * sim, metric=False, max_iter=5, random_state=0)
A:sklearn.manifold.tests.test_mds.rng->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_mds.X->numpy.random.RandomState(0).randn(4, 3)
A:sklearn.manifold.tests.test_mds.dist->euclidean_distances(X)
A:sklearn.manifold.tests.test_mds.mock->Mock(side_effect=mds._smacof_single)
A:sklearn.manifold.tests.test_mds.est->sklearn.manifold._mds.MDS(metric=metric, normalized_stress='auto', random_state=rng)
sklearn.manifold.tests.test_mds.test_MDS()
sklearn.manifold.tests.test_mds.test_normalize_metric_warning()
sklearn.manifold.tests.test_mds.test_normalized_stress_auto(metric,monkeypatch)
sklearn.manifold.tests.test_mds.test_normed_stress(k)
sklearn.manifold.tests.test_mds.test_smacof()
sklearn.manifold.tests.test_mds.test_smacof_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/manifold/tests/test_isomap.py----------------------------------------
A:sklearn.manifold.tests.test_isomap.n_per_side->int(math.sqrt(n_pts))
A:sklearn.manifold.tests.test_isomap.X->numpy.array([0, 1, 2, 5, 6, 7])[:, None].astype(global_dtype, copy=False)
A:sklearn.manifold.tests.test_isomap.rng->numpy.random.RandomState(0)
A:sklearn.manifold.tests.test_isomap.G->sklearn.neighbors.radius_neighbors_graph(X, radius, mode='distance').toarray()
A:sklearn.manifold.tests.test_isomap.clf->sklearn.pipeline.Pipeline([('isomap', manifold.Isomap(n_neighbors=n_neighbors, radius=radius)), ('clf', neighbors.KNeighborsClassifier())])
A:sklearn.manifold.tests.test_isomap.G_iso->G_iso.toarray().toarray()
A:sklearn.manifold.tests.test_isomap.centerer->sklearn.preprocessing.KernelCenterer()
A:sklearn.manifold.tests.test_isomap.K->sklearn.preprocessing.KernelCenterer().fit_transform(-0.5 * G ** 2)
A:sklearn.manifold.tests.test_isomap.K_iso->sklearn.preprocessing.KernelCenterer().fit_transform(-0.5 * G_iso ** 2)
A:sklearn.manifold.tests.test_isomap.(X, y)->make_blobs(random_state=0, n_features=4)
A:sklearn.manifold.tests.test_isomap.iso->sklearn.manifold.Isomap(n_components=n_components)
A:sklearn.manifold.tests.test_isomap.X_iso->sklearn.manifold.Isomap(n_components=n_components).fit_transform(X)
A:sklearn.manifold.tests.test_isomap.X_iso2->sklearn.manifold.Isomap(n_components=n_components).transform(X + noise)
A:sklearn.manifold.tests.test_isomap.(X, _)->sklearn.datasets.load_digits(return_X_y=True)
A:sklearn.manifold.tests.test_isomap.(X2, _)->sklearn.datasets.make_blobs(random_state=1)
A:sklearn.manifold.tests.test_isomap.X2->X2.astype(global_dtype, copy=False).astype(global_dtype, copy=False)
A:sklearn.manifold.tests.test_isomap.est_chain->sklearn.pipeline.make_pipeline(neighbors.KNeighborsTransformer(n_neighbors=n_neighbors, algorithm=algorithm, mode='distance'), manifold.Isomap(n_neighbors=n_neighbors, metric='precomputed'))
A:sklearn.manifold.tests.test_isomap.est_compact->sklearn.manifold.Isomap(n_neighbors=n_neighbors, neighbors_algorithm=algorithm)
A:sklearn.manifold.tests.test_isomap.Xt_chain->sklearn.pipeline.make_pipeline(neighbors.KNeighborsTransformer(n_neighbors=n_neighbors, algorithm=algorithm, mode='distance'), manifold.Isomap(n_neighbors=n_neighbors, metric='precomputed')).transform(X2)
A:sklearn.manifold.tests.test_isomap.Xt_compact->sklearn.manifold.Isomap(n_neighbors=n_neighbors, neighbors_algorithm=algorithm).transform(X2)
A:sklearn.manifold.tests.test_isomap.reference->sklearn.manifold.Isomap().fit_transform(X)
A:sklearn.manifold.tests.test_isomap.embedding->sklearn.manifold.Isomap(metric=metric, p=p).fit_transform(X)
A:sklearn.manifold.tests.test_isomap.model->sklearn.manifold.Isomap()
A:sklearn.manifold.tests.test_isomap.iso_dense->sklearn.manifold.Isomap(n_components=2, eigen_solver=eigen_solver, path_method=path_method, n_neighbors=8)
A:sklearn.manifold.tests.test_isomap.iso_sparse->clone(iso_dense)
A:sklearn.manifold.tests.test_isomap.X_trans_dense->sklearn.manifold.Isomap(n_components=2, eigen_solver=eigen_solver, path_method=path_method, n_neighbors=8).fit_transform(X.toarray())
A:sklearn.manifold.tests.test_isomap.X_trans_sparse->clone(iso_dense).fit_transform(X)
A:sklearn.manifold.tests.test_isomap.g->sklearn.neighbors.radius_neighbors_graph(X, radius=radius, mode='distance')
A:sklearn.manifold.tests.test_isomap.isomap->sklearn.manifold.Isomap(n_neighbors=3, radius=5.5)
A:sklearn.manifold.tests.test_isomap.result->sklearn.manifold.Isomap(n_neighbors=3, radius=5.5).fit_transform(X)
A:sklearn.manifold.tests.test_isomap.iso_32->sklearn.manifold.Isomap(n_neighbors=2)
A:sklearn.manifold.tests.test_isomap.X_32->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)
A:sklearn.manifold.tests.test_isomap.iso_64->sklearn.manifold.Isomap(n_neighbors=2)
A:sklearn.manifold.tests.test_isomap.X_64->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float64)
A:sklearn.manifold.tests.test_isomap.X_distances->pairwise_distances(X)
A:sklearn.manifold.tests.test_isomap.X_graph->sklearn.neighbors.kneighbors_graph(X, n_neighbors=2, mode='distance')
A:sklearn.manifold.tests.test_isomap.names->sklearn.manifold.Isomap(n_components=n_components).get_feature_names_out()
sklearn.manifold.tests.test_isomap.create_sample_data(dtype,n_pts=25,add_noise=False)
sklearn.manifold.tests.test_isomap.test_different_metric(global_dtype,metric,p,is_euclidean)
sklearn.manifold.tests.test_isomap.test_get_feature_names_out()
sklearn.manifold.tests.test_isomap.test_isomap_clone_bug()
sklearn.manifold.tests.test_isomap.test_isomap_dtype_equivalence()
sklearn.manifold.tests.test_isomap.test_isomap_fit_precomputed_radius_graph(global_dtype)
sklearn.manifold.tests.test_isomap.test_isomap_fitted_attributes_dtype(global_dtype)
sklearn.manifold.tests.test_isomap.test_isomap_raise_error_when_neighbor_and_radius_both_set()
sklearn.manifold.tests.test_isomap.test_isomap_reconstruction_error(global_dtype,n_neighbors,radius,eigen_solver,path_method)
sklearn.manifold.tests.test_isomap.test_isomap_simple_grid(global_dtype,n_neighbors,radius,eigen_solver,path_method)
sklearn.manifold.tests.test_isomap.test_multiple_connected_components()
sklearn.manifold.tests.test_isomap.test_multiple_connected_components_metric_precomputed(global_dtype)
sklearn.manifold.tests.test_isomap.test_pipeline(n_neighbors,radius,global_dtype)
sklearn.manifold.tests.test_isomap.test_pipeline_with_nearest_neighbors_transformer(global_dtype)
sklearn.manifold.tests.test_isomap.test_sparse_input(global_dtype,eigen_solver,path_method,global_random_seed,csr_container)
sklearn.manifold.tests.test_isomap.test_transform(global_dtype,n_neighbors,radius)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/enable_iterative_imputer.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/enable_halving_search_cv.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/tests/test_enable_successive_halving.py----------------------------------------
sklearn.experimental.tests.test_enable_successive_halving.test_imports_strategies()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/tests/test_enable_iterative_imputer.py----------------------------------------
sklearn.experimental.tests.test_enable_iterative_imputer.test_imports_strategies()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/experimental/tests/test_enable_hist_gradient_boosting.py----------------------------------------
sklearn.experimental.tests.test_enable_hist_gradient_boosting.test_import_raises_warning()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_label.py----------------------------------------
A:sklearn.preprocessing._label.y->y.toarray().toarray()
A:sklearn.preprocessing._label.self.classes_->numpy.empty(len(classes), dtype=dtype)
A:sklearn.preprocessing._label.(self.classes_, y)->_unique(y, return_inverse=True)
A:sklearn.preprocessing._label.diff->numpy.setdiff1d(y, np.arange(len(self.classes_)))
A:sklearn.preprocessing._label.self.y_type_->type_of_target(y, input_name='y')
A:sklearn.preprocessing._label.self.sparse_input_->scipy.sparse.issparse(y)
A:sklearn.preprocessing._label.y_is_multilabel->type_of_target(y).startswith('multilabel')
A:sklearn.preprocessing._label.y_inv->y_inv.toarray().toarray()
A:sklearn.preprocessing._label.y_type->type_of_target(y)
A:sklearn.preprocessing._label.n_classes->len(classes)
A:sklearn.preprocessing._label.classes->sorted(set(itertools.chain.from_iterable(y)))
A:sklearn.preprocessing._label.Y->Y[:, -1].reshape((-1, 1))
A:sklearn.preprocessing._label.sorted_class->numpy.sort(classes)
A:sklearn.preprocessing._label.y_in_classes->numpy.isin(y, classes)
A:sklearn.preprocessing._label.indices->array.array('i')
A:sklearn.preprocessing._label.indptr->array.array('i', [0])
A:sklearn.preprocessing._label.data->numpy.ones(len(indices), dtype=int)
A:sklearn.preprocessing._label.Y.data->Y[:, -1].reshape((-1, 1)).data.astype(int, copy=False)
A:sklearn.preprocessing._label.outputs->numpy.arange(n_outputs)
A:sklearn.preprocessing._label.row_nnz->numpy.diff(y.indptr)
A:sklearn.preprocessing._label.y_data_repeated_max->numpy.repeat(row_max, row_nnz)
A:sklearn.preprocessing._label.y_i_all_argmax->numpy.append(y_i_all_argmax, [len(y.data)])
A:sklearn.preprocessing._label.index_first_argmax->numpy.searchsorted(y_i_all_argmax, y.indptr[:-1])
A:sklearn.preprocessing._label.y_ind_ext->numpy.append(y.indices, [0])
A:sklearn.preprocessing._label.y.data->numpy.array(y.data > threshold, dtype=int)
A:sklearn.preprocessing._label.class_mapping->numpy.empty(len(tmp), dtype=dtype)
A:sklearn.preprocessing._label.yt->yt.tocsr().tocsr()
A:sklearn.preprocessing._label.tmp->sorted(class_mapping, key=class_mapping.get)
A:sklearn.preprocessing._label.(self.classes_, inverse)->numpy.unique(class_mapping, return_inverse=True)
A:sklearn.preprocessing._label.yt.indices->numpy.array(inverse[yt.indices], dtype=yt.indices.dtype, copy=False)
A:sklearn.preprocessing._label.class_to_index->self._build_cache()
A:sklearn.preprocessing._label.self._cached_dict->dict(zip(self.classes_, range(len(self.classes_))))
A:sklearn.preprocessing._label.unknown->set()
A:sklearn.preprocessing._label.index->set()
A:sklearn.preprocessing._label.unexpected->numpy.setdiff1d(yt, [0, 1])
sklearn.preprocessing.LabelBinarizer(self,*,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.LabelBinarizer._more_tags(self)
sklearn.preprocessing.LabelBinarizer.fit(self,y)
sklearn.preprocessing.LabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.LabelBinarizer.inverse_transform(self,Y,threshold=None)
sklearn.preprocessing.LabelBinarizer.transform(self,y)
sklearn.preprocessing.LabelEncoder(TransformerMixin,BaseEstimator,auto_wrap_output_keys=None)
sklearn.preprocessing.LabelEncoder._more_tags(self)
sklearn.preprocessing.LabelEncoder.fit(self,y)
sklearn.preprocessing.LabelEncoder.fit_transform(self,y)
sklearn.preprocessing.LabelEncoder.inverse_transform(self,y)
sklearn.preprocessing.LabelEncoder.transform(self,y)
sklearn.preprocessing.MultiLabelBinarizer(self,*,classes=None,sparse_output=False)
sklearn.preprocessing.MultiLabelBinarizer._build_cache(self)
sklearn.preprocessing.MultiLabelBinarizer._more_tags(self)
sklearn.preprocessing.MultiLabelBinarizer._transform(self,y,class_mapping)
sklearn.preprocessing.MultiLabelBinarizer.fit(self,y)
sklearn.preprocessing.MultiLabelBinarizer.fit_transform(self,y)
sklearn.preprocessing.MultiLabelBinarizer.inverse_transform(self,yt)
sklearn.preprocessing.MultiLabelBinarizer.transform(self,y)
sklearn.preprocessing._label.LabelBinarizer(self,*,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing._label.LabelBinarizer.__init__(self,*,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing._label.LabelBinarizer._more_tags(self)
sklearn.preprocessing._label.LabelBinarizer.fit(self,y)
sklearn.preprocessing._label.LabelBinarizer.fit_transform(self,y)
sklearn.preprocessing._label.LabelBinarizer.inverse_transform(self,Y,threshold=None)
sklearn.preprocessing._label.LabelBinarizer.transform(self,y)
sklearn.preprocessing._label.LabelEncoder(TransformerMixin,BaseEstimator,auto_wrap_output_keys=None)
sklearn.preprocessing._label.LabelEncoder._more_tags(self)
sklearn.preprocessing._label.LabelEncoder.fit(self,y)
sklearn.preprocessing._label.LabelEncoder.fit_transform(self,y)
sklearn.preprocessing._label.LabelEncoder.inverse_transform(self,y)
sklearn.preprocessing._label.LabelEncoder.transform(self,y)
sklearn.preprocessing._label.MultiLabelBinarizer(self,*,classes=None,sparse_output=False)
sklearn.preprocessing._label.MultiLabelBinarizer.__init__(self,*,classes=None,sparse_output=False)
sklearn.preprocessing._label.MultiLabelBinarizer._build_cache(self)
sklearn.preprocessing._label.MultiLabelBinarizer._more_tags(self)
sklearn.preprocessing._label.MultiLabelBinarizer._transform(self,y,class_mapping)
sklearn.preprocessing._label.MultiLabelBinarizer.fit(self,y)
sklearn.preprocessing._label.MultiLabelBinarizer.fit_transform(self,y)
sklearn.preprocessing._label.MultiLabelBinarizer.inverse_transform(self,yt)
sklearn.preprocessing._label.MultiLabelBinarizer.transform(self,y)
sklearn.preprocessing._label._inverse_binarize_multiclass(y,classes)
sklearn.preprocessing._label._inverse_binarize_thresholding(y,output_type,classes,threshold)
sklearn.preprocessing._label.label_binarize(y,*,classes,neg_label=0,pos_label=1,sparse_output=False)
sklearn.preprocessing.label_binarize(y,*,classes,neg_label=0,pos_label=1,sparse_output=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_data.py----------------------------------------
A:sklearn.preprocessing._data.(xp, _)->get_namespace(K)
A:sklearn.preprocessing._data.scale->xp.asarray(scale, copy=True)
A:sklearn.preprocessing._data.X->self._validate_data(X, ensure_2d=True, dtype=FLOAT_DTYPES, copy=self.copy, force_all_finite='allow-nan', reset=in_fit)
A:sklearn.preprocessing._data.(_, var)->mean_variance_axis(X, axis=0)
A:sklearn.preprocessing._data.var->numpy.var(X, axis=0, dtype=np.float64)
A:sklearn.preprocessing._data.mean_->numpy.nanmean(X, axis)
A:sklearn.preprocessing._data.scale_->_handle_zeros_in_scale(scale_, copy=False)
A:sklearn.preprocessing._data.Xr->numpy.rollaxis(X, axis)
A:sklearn.preprocessing._data.mean_1->numpy.nanmean(Xr, axis=0)
A:sklearn.preprocessing._data.mean_2->numpy.nanmean(Xr, axis=0)
A:sklearn.preprocessing._data.data_min->xp.minimum(self.data_min_, data_min)
A:sklearn.preprocessing._data.data_max->xp.maximum(self.data_max_, data_max)
A:sklearn.preprocessing._data.s->RobustScaler(with_centering=with_centering, with_scaling=with_scaling, quantile_range=quantile_range, unit_variance=unit_variance, copy=copy)
A:sklearn.preprocessing._data.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.preprocessing._data.self.n_samples_seen_->self.n_samples_seen_.astype(dtype, copy=False)
A:sklearn.preprocessing._data.(self.mean_, self.var_, self.n_samples_seen_)->_incremental_mean_and_var(X, self.mean_, self.var_, self.n_samples_seen_, sample_weight=sample_weight)
A:sklearn.preprocessing._data.self.mean_->self.mean_.astype(np.float64, copy=False)
A:sklearn.preprocessing._data.self.var_->self.var_.astype(np.float64, copy=False)
A:sklearn.preprocessing._data.weights->_check_sample_weight(sample_weight, X)
A:sklearn.preprocessing._data.constant_mask->_is_constant_feature(self.var_, self.mean_, self.n_samples_seen_)
A:sklearn.preprocessing._data.self.scale_->_handle_zeros_in_scale(self.scale_, copy=False)
A:sklearn.preprocessing._data.(mins, maxs)->min_max_axis(X, axis=0, ignore_nan=True)
A:sklearn.preprocessing._data.max_abs->xp.maximum(self.max_abs_, max_abs)
A:sklearn.preprocessing._data.self.center_->numpy.nanmedian(X, axis=0)
A:sklearn.preprocessing._data.column_data->numpy.zeros(shape=n_samples, dtype=X.dtype)
A:sklearn.preprocessing._data.quantiles->numpy.transpose(quantiles)
A:sklearn.preprocessing._data.(mins, maxes)->min_max_axis(X, 1)
A:sklearn.preprocessing._data.norms->_handle_zeros_in_scale(norms, copy=False)
A:sklearn.preprocessing._data.norms_elementwise->_handle_zeros_in_scale(norms, copy=False).repeat(np.diff(X.indptr))
A:sklearn.preprocessing._data.not_cond->numpy.logical_not(cond)
A:sklearn.preprocessing._data.K->self._validate_data(K, copy=copy, dtype=_array_api.supported_float_dtypes(xp), reset=False)
A:sklearn.preprocessing._data.col->col.take(subsample_idx, mode='clip').take(subsample_idx, mode='clip')
A:sklearn.preprocessing._data.row->numpy.concatenate((np.arange(n_samples), X.row))
A:sklearn.preprocessing._data.data->numpy.concatenate((np.full(n_samples, value), X.data))
A:sklearn.preprocessing._data.indptr->numpy.concatenate((np.array([0]), indptr))
A:sklearn.preprocessing._data.indices->numpy.concatenate((np.arange(n_samples), X.indices))
A:sklearn.preprocessing._data.subsample_idx->random_state.choice(n_samples, size=self.subsample, replace=False)
A:sklearn.preprocessing._data.self.quantiles_->numpy.maximum.accumulate(self.quantiles_)
A:sklearn.preprocessing._data.column_data[:column_subsample]->random_state.choice(column_nnz_data, size=column_subsample, replace=False)
A:sklearn.preprocessing._data.self.n_quantiles_->max(1, min(self.n_quantiles, n_samples))
A:sklearn.preprocessing._data.rng->check_random_state(self.random_state)
A:sklearn.preprocessing._data.self.references_->numpy.linspace(0, 1, self.n_quantiles_, endpoint=True)
A:sklearn.preprocessing._data.X_col->numpy.clip(X_col, clip_min, clip_max)
A:sklearn.preprocessing._data.X_col[isfinite_mask]->numpy.interp(X_col_finite, self.references_, quantiles)
A:sklearn.preprocessing._data.clip_min->scipy.stats.norm.ppf(BOUNDS_THRESHOLD - np.spacing(1))
A:sklearn.preprocessing._data.clip_max->scipy.stats.norm.ppf(1 - (BOUNDS_THRESHOLD - np.spacing(1)))
A:sklearn.preprocessing._data.column_slice->slice(X.indptr[feature_idx], X.indptr[feature_idx + 1])
A:sklearn.preprocessing._data.X.data[column_slice]->self._transform_col(X.data[column_slice], self.quantiles_[:, feature_idx], inverse)
A:sklearn.preprocessing._data.X[:, feature_idx]->self._transform_col(X[:, feature_idx], self.quantiles_[:, feature_idx], inverse)
A:sklearn.preprocessing._data.n->QuantileTransformer(n_quantiles=n_quantiles, output_distribution=output_distribution, subsample=subsample, ignore_implicit_zeros=ignore_implicit_zeros, random_state=random_state, copy=copy)
A:sklearn.preprocessing._data.mean->numpy.mean(X, axis=0, dtype=np.float64)
A:sklearn.preprocessing._data.self.lambdas_->numpy.empty(X.shape[1], dtype=X.dtype)
A:sklearn.preprocessing._data.is_constant_feature->_is_constant_feature(var[i], mean[i], n_samples)
A:sklearn.preprocessing._data.self.lambdas_[i]->optim_function(col)
A:sklearn.preprocessing._data.X[:, i]->inv_fun(X[:, i], lmbda)
A:sklearn.preprocessing._data.self._scaler->StandardScaler(copy=False).set_output(transform='default')
A:sklearn.preprocessing._data.x_inv->numpy.zeros_like(x)
A:sklearn.preprocessing._data.out->numpy.zeros_like(x)
A:sklearn.preprocessing._data.out[pos]->numpy.log1p(x[pos])
A:sklearn.preprocessing._data.mask->numpy.isnan(x)
A:sklearn.preprocessing._data.(_, lmbda)->scipy.stats.boxcox(x[~mask], lmbda=None)
A:sklearn.preprocessing._data.x_trans->self._yeo_johnson_transform(x, lmbda)
A:sklearn.preprocessing._data.x_trans_var->self._yeo_johnson_transform(x, lmbda).var()
A:sklearn.preprocessing._data.log_var->numpy.log(x_trans_var)
A:sklearn.preprocessing._data.pt->PowerTransformer(method=method, standardize=standardize, copy=copy)
sklearn.preprocessing.Binarizer(self,*,threshold=0.0,copy=True)
sklearn.preprocessing.Binarizer._more_tags(self)
sklearn.preprocessing.Binarizer.fit(self,X,y=None)
sklearn.preprocessing.Binarizer.transform(self,X,copy=None)
sklearn.preprocessing.KernelCenterer(self)
sklearn.preprocessing.KernelCenterer._more_tags(self)
sklearn.preprocessing.KernelCenterer._n_features_out(self)
sklearn.preprocessing.KernelCenterer.fit(self,K,y=None)
sklearn.preprocessing.KernelCenterer.transform(self,K,copy=True)
sklearn.preprocessing.MaxAbsScaler(self,*,copy=True)
sklearn.preprocessing.MaxAbsScaler._more_tags(self)
sklearn.preprocessing.MaxAbsScaler._reset(self)
sklearn.preprocessing.MaxAbsScaler.fit(self,X,y=None)
sklearn.preprocessing.MaxAbsScaler.inverse_transform(self,X)
sklearn.preprocessing.MaxAbsScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.MaxAbsScaler.transform(self,X)
sklearn.preprocessing.MinMaxScaler(self,feature_range=(0,1),*,copy=True,clip=False)
sklearn.preprocessing.MinMaxScaler._more_tags(self)
sklearn.preprocessing.MinMaxScaler._reset(self)
sklearn.preprocessing.MinMaxScaler.fit(self,X,y=None)
sklearn.preprocessing.MinMaxScaler.inverse_transform(self,X)
sklearn.preprocessing.MinMaxScaler.partial_fit(self,X,y=None)
sklearn.preprocessing.MinMaxScaler.transform(self,X)
sklearn.preprocessing.Normalizer(self,norm='l2',*,copy=True)
sklearn.preprocessing.Normalizer._more_tags(self)
sklearn.preprocessing.Normalizer.fit(self,X,y=None)
sklearn.preprocessing.Normalizer.transform(self,X,copy=None)
sklearn.preprocessing.PowerTransformer(self,method='yeo-johnson',*,standardize=True,copy=True)
sklearn.preprocessing.PowerTransformer._box_cox_inverse_tranform(self,x,lmbda)
sklearn.preprocessing.PowerTransformer._box_cox_optimize(self,x)
sklearn.preprocessing.PowerTransformer._check_input(self,X,in_fit,check_positive=False,check_shape=False)
sklearn.preprocessing.PowerTransformer._fit(self,X,y=None,force_transform=False)
sklearn.preprocessing.PowerTransformer._more_tags(self)
sklearn.preprocessing.PowerTransformer._yeo_johnson_inverse_transform(self,x,lmbda)
sklearn.preprocessing.PowerTransformer._yeo_johnson_optimize(self,x)
sklearn.preprocessing.PowerTransformer._yeo_johnson_transform(self,x,lmbda)
sklearn.preprocessing.PowerTransformer.fit(self,X,y=None)
sklearn.preprocessing.PowerTransformer.fit_transform(self,X,y=None)
sklearn.preprocessing.PowerTransformer.inverse_transform(self,X)
sklearn.preprocessing.PowerTransformer.transform(self,X)
sklearn.preprocessing.QuantileTransformer(self,*,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=10000,random_state=None,copy=True)
sklearn.preprocessing.QuantileTransformer._check_inputs(self,X,in_fit,accept_sparse_negative=False,copy=False)
sklearn.preprocessing.QuantileTransformer._dense_fit(self,X,random_state)
sklearn.preprocessing.QuantileTransformer._more_tags(self)
sklearn.preprocessing.QuantileTransformer._sparse_fit(self,X,random_state)
sklearn.preprocessing.QuantileTransformer._transform(self,X,inverse=False)
sklearn.preprocessing.QuantileTransformer._transform_col(self,X_col,quantiles,inverse)
sklearn.preprocessing.QuantileTransformer.fit(self,X,y=None)
sklearn.preprocessing.QuantileTransformer.inverse_transform(self,X)
sklearn.preprocessing.QuantileTransformer.transform(self,X)
sklearn.preprocessing.RobustScaler(self,*,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True,unit_variance=False)
sklearn.preprocessing.RobustScaler._more_tags(self)
sklearn.preprocessing.RobustScaler.fit(self,X,y=None)
sklearn.preprocessing.RobustScaler.inverse_transform(self,X)
sklearn.preprocessing.RobustScaler.transform(self,X)
sklearn.preprocessing.StandardScaler(self,*,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing.StandardScaler._more_tags(self)
sklearn.preprocessing.StandardScaler._reset(self)
sklearn.preprocessing.StandardScaler.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing.StandardScaler.inverse_transform(self,X,copy=None)
sklearn.preprocessing.StandardScaler.partial_fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing.StandardScaler.transform(self,X,copy=None)
sklearn.preprocessing._data.Binarizer(self,*,threshold=0.0,copy=True)
sklearn.preprocessing._data.Binarizer.__init__(self,*,threshold=0.0,copy=True)
sklearn.preprocessing._data.Binarizer._more_tags(self)
sklearn.preprocessing._data.Binarizer.fit(self,X,y=None)
sklearn.preprocessing._data.Binarizer.transform(self,X,copy=None)
sklearn.preprocessing._data.KernelCenterer(self)
sklearn.preprocessing._data.KernelCenterer.__init__(self)
sklearn.preprocessing._data.KernelCenterer._more_tags(self)
sklearn.preprocessing._data.KernelCenterer._n_features_out(self)
sklearn.preprocessing._data.KernelCenterer.fit(self,K,y=None)
sklearn.preprocessing._data.KernelCenterer.transform(self,K,copy=True)
sklearn.preprocessing._data.MaxAbsScaler(self,*,copy=True)
sklearn.preprocessing._data.MaxAbsScaler.__init__(self,*,copy=True)
sklearn.preprocessing._data.MaxAbsScaler._more_tags(self)
sklearn.preprocessing._data.MaxAbsScaler._reset(self)
sklearn.preprocessing._data.MaxAbsScaler.fit(self,X,y=None)
sklearn.preprocessing._data.MaxAbsScaler.inverse_transform(self,X)
sklearn.preprocessing._data.MaxAbsScaler.partial_fit(self,X,y=None)
sklearn.preprocessing._data.MaxAbsScaler.transform(self,X)
sklearn.preprocessing._data.MinMaxScaler(self,feature_range=(0,1),*,copy=True,clip=False)
sklearn.preprocessing._data.MinMaxScaler.__init__(self,feature_range=(0,1),*,copy=True,clip=False)
sklearn.preprocessing._data.MinMaxScaler._more_tags(self)
sklearn.preprocessing._data.MinMaxScaler._reset(self)
sklearn.preprocessing._data.MinMaxScaler.fit(self,X,y=None)
sklearn.preprocessing._data.MinMaxScaler.inverse_transform(self,X)
sklearn.preprocessing._data.MinMaxScaler.partial_fit(self,X,y=None)
sklearn.preprocessing._data.MinMaxScaler.transform(self,X)
sklearn.preprocessing._data.Normalizer(self,norm='l2',*,copy=True)
sklearn.preprocessing._data.Normalizer.__init__(self,norm='l2',*,copy=True)
sklearn.preprocessing._data.Normalizer._more_tags(self)
sklearn.preprocessing._data.Normalizer.fit(self,X,y=None)
sklearn.preprocessing._data.Normalizer.transform(self,X,copy=None)
sklearn.preprocessing._data.PowerTransformer(self,method='yeo-johnson',*,standardize=True,copy=True)
sklearn.preprocessing._data.PowerTransformer.__init__(self,method='yeo-johnson',*,standardize=True,copy=True)
sklearn.preprocessing._data.PowerTransformer._box_cox_inverse_tranform(self,x,lmbda)
sklearn.preprocessing._data.PowerTransformer._box_cox_optimize(self,x)
sklearn.preprocessing._data.PowerTransformer._check_input(self,X,in_fit,check_positive=False,check_shape=False)
sklearn.preprocessing._data.PowerTransformer._fit(self,X,y=None,force_transform=False)
sklearn.preprocessing._data.PowerTransformer._more_tags(self)
sklearn.preprocessing._data.PowerTransformer._yeo_johnson_inverse_transform(self,x,lmbda)
sklearn.preprocessing._data.PowerTransformer._yeo_johnson_optimize(self,x)
sklearn.preprocessing._data.PowerTransformer._yeo_johnson_transform(self,x,lmbda)
sklearn.preprocessing._data.PowerTransformer.fit(self,X,y=None)
sklearn.preprocessing._data.PowerTransformer.fit_transform(self,X,y=None)
sklearn.preprocessing._data.PowerTransformer.inverse_transform(self,X)
sklearn.preprocessing._data.PowerTransformer.transform(self,X)
sklearn.preprocessing._data.QuantileTransformer(self,*,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=10000,random_state=None,copy=True)
sklearn.preprocessing._data.QuantileTransformer.__init__(self,*,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=10000,random_state=None,copy=True)
sklearn.preprocessing._data.QuantileTransformer._check_inputs(self,X,in_fit,accept_sparse_negative=False,copy=False)
sklearn.preprocessing._data.QuantileTransformer._dense_fit(self,X,random_state)
sklearn.preprocessing._data.QuantileTransformer._more_tags(self)
sklearn.preprocessing._data.QuantileTransformer._sparse_fit(self,X,random_state)
sklearn.preprocessing._data.QuantileTransformer._transform(self,X,inverse=False)
sklearn.preprocessing._data.QuantileTransformer._transform_col(self,X_col,quantiles,inverse)
sklearn.preprocessing._data.QuantileTransformer.fit(self,X,y=None)
sklearn.preprocessing._data.QuantileTransformer.inverse_transform(self,X)
sklearn.preprocessing._data.QuantileTransformer.transform(self,X)
sklearn.preprocessing._data.RobustScaler(self,*,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True,unit_variance=False)
sklearn.preprocessing._data.RobustScaler.__init__(self,*,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True,unit_variance=False)
sklearn.preprocessing._data.RobustScaler._more_tags(self)
sklearn.preprocessing._data.RobustScaler.fit(self,X,y=None)
sklearn.preprocessing._data.RobustScaler.inverse_transform(self,X)
sklearn.preprocessing._data.RobustScaler.transform(self,X)
sklearn.preprocessing._data.StandardScaler(self,*,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing._data.StandardScaler.__init__(self,*,copy=True,with_mean=True,with_std=True)
sklearn.preprocessing._data.StandardScaler._more_tags(self)
sklearn.preprocessing._data.StandardScaler._reset(self)
sklearn.preprocessing._data.StandardScaler.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing._data.StandardScaler.inverse_transform(self,X,copy=None)
sklearn.preprocessing._data.StandardScaler.partial_fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing._data.StandardScaler.transform(self,X,copy=None)
sklearn.preprocessing._data._handle_zeros_in_scale(scale,copy=True,constant_mask=None)
sklearn.preprocessing._data._is_constant_feature(var,mean,n_samples)
sklearn.preprocessing._data.add_dummy_feature(X,value=1.0)
sklearn.preprocessing._data.binarize(X,*,threshold=0.0,copy=True)
sklearn.preprocessing._data.maxabs_scale(X,*,axis=0,copy=True)
sklearn.preprocessing._data.minmax_scale(X,feature_range=(0,1),*,axis=0,copy=True)
sklearn.preprocessing._data.normalize(X,norm='l2',*,axis=1,copy=True,return_norm=False)
sklearn.preprocessing._data.power_transform(X,method='yeo-johnson',*,standardize=True,copy=True)
sklearn.preprocessing._data.quantile_transform(X,*,axis=0,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=True)
sklearn.preprocessing._data.robust_scale(X,*,axis=0,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True,unit_variance=False)
sklearn.preprocessing._data.scale(X,*,axis=0,with_mean=True,with_std=True,copy=True)
sklearn.preprocessing.add_dummy_feature(X,value=1.0)
sklearn.preprocessing.binarize(X,*,threshold=0.0,copy=True)
sklearn.preprocessing.maxabs_scale(X,*,axis=0,copy=True)
sklearn.preprocessing.minmax_scale(X,feature_range=(0,1),*,axis=0,copy=True)
sklearn.preprocessing.normalize(X,norm='l2',*,axis=1,copy=True,return_norm=False)
sklearn.preprocessing.power_transform(X,method='yeo-johnson',*,standardize=True,copy=True)
sklearn.preprocessing.quantile_transform(X,*,axis=0,n_quantiles=1000,output_distribution='uniform',ignore_implicit_zeros=False,subsample=int(100000.0),random_state=None,copy=True)
sklearn.preprocessing.robust_scale(X,*,axis=0,with_centering=True,with_scaling=True,quantile_range=(25.0,75.0),copy=True,unit_variance=False)
sklearn.preprocessing.scale(X,*,axis=0,with_mean=True,with_std=True,copy=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_polynomial.py----------------------------------------
A:sklearn.preprocessing._polynomial.total_nnz->_calc_total_nnz(X.indptr, interaction_only, deg)
A:sklearn.preprocessing._polynomial.expanded_col->_calc_expanded_nnz(n_features, interaction_only, deg)
A:sklearn.preprocessing._polynomial.expanded_data->numpy.empty(shape=total_nnz, dtype=X.data.dtype)
A:sklearn.preprocessing._polynomial.expanded_indices->numpy.empty(shape=total_nnz, dtype=index_dtype)
A:sklearn.preprocessing._polynomial.expanded_indptr->numpy.empty(shape=X.indptr.shape[0], dtype=index_dtype)
A:sklearn.preprocessing._polynomial.start->max(1, min_degree)
A:sklearn.preprocessing._polynomial.iter->chain(comb(range(n_features), 0), iter)
A:sklearn.preprocessing._polynomial.combinations->self._combinations(n_features=n_features, min_degree=self._min_degree, max_degree=self._max_degree, interaction_only=self.interaction_only, include_bias=self.include_bias)
A:sklearn.preprocessing._polynomial.input_features->_check_feature_names_in(self, input_features)
A:sklearn.preprocessing._polynomial.name->' '.join(('%s^%d' % (input_features[ind], exp) if exp != 1 else input_features[ind] for (ind, exp) in zip(inds, row[inds])))
A:sklearn.preprocessing._polynomial.self.n_output_features_->self._num_combinations(n_features=n_features, min_degree=self._min_degree, max_degree=self._max_degree, interaction_only=self.interaction_only, include_bias=self.include_bias)
A:sklearn.preprocessing._polynomial.self._n_out_full->self._num_combinations(n_features=n_features, min_degree=0, max_degree=self._max_degree, interaction_only=self.interaction_only, include_bias=self.include_bias)
A:sklearn.preprocessing._polynomial.X->self._validate_data(X, reset=False, accept_sparse=False, ensure_2d=True)
A:sklearn.preprocessing._polynomial.cumulative_size->sum((mat.shape[1] for mat in to_stack))
A:sklearn.preprocessing._polynomial.expanded->_create_expansion(X=X, interaction_only=self.interaction_only, deg=deg, n_features=n_features, cumulative_size=cumulative_size)
A:sklearn.preprocessing._polynomial.XP->numpy.empty(shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order)
A:sklearn.preprocessing._polynomial.all_int32->all((mat.indices.dtype == np.int32 for mat in to_stack))
A:sklearn.preprocessing._polynomial.out_col->X[:, [col_idx]].multiply(out_col)
A:sklearn.preprocessing._polynomial.bias->scipy.sparse.csc_matrix(np.ones((X.shape[0], 1)))
A:sklearn.preprocessing._polynomial.index->list(range(current_col, current_col + n_features))
A:sklearn.preprocessing._polynomial.Xout->XP[:, n_XP - n_Xout:].copy()
A:sklearn.preprocessing._polynomial.knots->numpy.linspace(start=x_min, stop=x_max, num=n_knots, endpoint=True, dtype=np.float64)
A:sklearn.preprocessing._polynomial.x_min->numpy.amin(X[mask], axis=0)
A:sklearn.preprocessing._polynomial.x_max->numpy.amax(X[mask], axis=0)
A:sklearn.preprocessing._polynomial.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.preprocessing._polynomial.base_knots->check_array(self.knots, dtype=np.float64)
A:sklearn.preprocessing._polynomial.coef->numpy.concatenate((coef, coef[:degree, :]))
A:sklearn.preprocessing._polynomial.kwargs_extrapolate->dict()
A:sklearn.preprocessing._polynomial.XBS->scipy.sparse.csr_matrix(XBS)
A:sklearn.preprocessing._polynomial.XBS_sparse->XBS_sparse.tocsr().tocsr()
A:sklearn.preprocessing._polynomial.XBS[:, i * n_splines:(i + 1) * n_splines]->spl(x)
A:sklearn.preprocessing._polynomial.x->X[:, i].copy()
A:sklearn.preprocessing._polynomial.XBS[mask, i * n_splines:(i + 1) * n_splines]->spl(X[mask, i])
sklearn.preprocessing.PolynomialFeatures(self,degree=2,*,interaction_only=False,include_bias=True,order='C')
sklearn.preprocessing.PolynomialFeatures._combinations(n_features,min_degree,max_degree,interaction_only,include_bias)
sklearn.preprocessing.PolynomialFeatures._num_combinations(n_features,min_degree,max_degree,interaction_only,include_bias)
sklearn.preprocessing.PolynomialFeatures.fit(self,X,y=None)
sklearn.preprocessing.PolynomialFeatures.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.PolynomialFeatures.powers_(self)
sklearn.preprocessing.PolynomialFeatures.transform(self,X)
sklearn.preprocessing.SplineTransformer(self,n_knots=5,degree=3,*,knots='uniform',extrapolation='constant',include_bias=True,order='C',sparse_output=False)
sklearn.preprocessing.SplineTransformer._get_base_knot_positions(X,n_knots=10,knots='uniform',sample_weight=None)
sklearn.preprocessing.SplineTransformer._more_tags(self)
sklearn.preprocessing.SplineTransformer.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing.SplineTransformer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.SplineTransformer.transform(self,X)
sklearn.preprocessing._polynomial.PolynomialFeatures(self,degree=2,*,interaction_only=False,include_bias=True,order='C')
sklearn.preprocessing._polynomial.PolynomialFeatures.__init__(self,degree=2,*,interaction_only=False,include_bias=True,order='C')
sklearn.preprocessing._polynomial.PolynomialFeatures._combinations(n_features,min_degree,max_degree,interaction_only,include_bias)
sklearn.preprocessing._polynomial.PolynomialFeatures._num_combinations(n_features,min_degree,max_degree,interaction_only,include_bias)
sklearn.preprocessing._polynomial.PolynomialFeatures.fit(self,X,y=None)
sklearn.preprocessing._polynomial.PolynomialFeatures.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._polynomial.PolynomialFeatures.powers_(self)
sklearn.preprocessing._polynomial.PolynomialFeatures.transform(self,X)
sklearn.preprocessing._polynomial.SplineTransformer(self,n_knots=5,degree=3,*,knots='uniform',extrapolation='constant',include_bias=True,order='C',sparse_output=False)
sklearn.preprocessing._polynomial.SplineTransformer.__init__(self,n_knots=5,degree=3,*,knots='uniform',extrapolation='constant',include_bias=True,order='C',sparse_output=False)
sklearn.preprocessing._polynomial.SplineTransformer._get_base_knot_positions(X,n_knots=10,knots='uniform',sample_weight=None)
sklearn.preprocessing._polynomial.SplineTransformer._more_tags(self)
sklearn.preprocessing._polynomial.SplineTransformer.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing._polynomial.SplineTransformer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._polynomial.SplineTransformer.transform(self,X)
sklearn.preprocessing._polynomial._create_expansion(X,interaction_only,deg,n_features,cumulative_size=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_discretization.py----------------------------------------
A:sklearn.preprocessing._discretization.X->_safe_indexing(X, subsample_idx)
A:sklearn.preprocessing._discretization.rng->check_random_state(self.random_state)
A:sklearn.preprocessing._discretization.subsample_idx->check_random_state(self.random_state).choice(n_samples, size=subsample, replace=False)
A:sklearn.preprocessing._discretization.n_bins->check_array(orig_bins, dtype=int, copy=True, ensure_2d=False)
A:sklearn.preprocessing._discretization.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.preprocessing._discretization.bin_edges->numpy.zeros(n_features, dtype=object)
A:sklearn.preprocessing._discretization.bin_edges[jj]->numpy.asarray([_weighted_percentile(column, sample_weight, q) for q in quantiles], dtype=np.float64)
A:sklearn.preprocessing._discretization.quantiles->numpy.linspace(0, 100, n_bins[jj] + 1)
A:sklearn.preprocessing._discretization.uniform_edges->numpy.linspace(col_min, col_max, n_bins[jj] + 1)
A:sklearn.preprocessing._discretization.km->KMeans(n_clusters=n_bins[jj], init=init, n_init=1)
A:sklearn.preprocessing._discretization.self._encoder->OneHotEncoder(categories=[np.arange(i) for i in self.n_bins_], sparse_output=self.encode == 'onehot', dtype=output_dtype)
A:sklearn.preprocessing._discretization.indices->', '.join((str(i) for i in violating_indices))
A:sklearn.preprocessing._discretization.Xt->self._encoder.inverse_transform(Xt)
A:sklearn.preprocessing._discretization.Xt[:, jj]->numpy.searchsorted(bin_edges[jj][1:-1], Xt[:, jj], side='right')
A:sklearn.preprocessing._discretization.Xt_enc->self._encoder.transform(Xt)
A:sklearn.preprocessing._discretization.Xinv->check_array(Xt, copy=True, dtype=(np.float64, np.float32))
A:sklearn.preprocessing._discretization.input_features->_check_feature_names_in(self, input_features)
sklearn.preprocessing.KBinsDiscretizer(self,n_bins=5,*,encode='onehot',strategy='quantile',dtype=None,subsample='warn',random_state=None)
sklearn.preprocessing.KBinsDiscretizer._validate_n_bins(self,n_features)
sklearn.preprocessing.KBinsDiscretizer.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing.KBinsDiscretizer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.KBinsDiscretizer.inverse_transform(self,Xt)
sklearn.preprocessing.KBinsDiscretizer.transform(self,X)
sklearn.preprocessing._discretization.KBinsDiscretizer(self,n_bins=5,*,encode='onehot',strategy='quantile',dtype=None,subsample='warn',random_state=None)
sklearn.preprocessing._discretization.KBinsDiscretizer.__init__(self,n_bins=5,*,encode='onehot',strategy='quantile',dtype=None,subsample='warn',random_state=None)
sklearn.preprocessing._discretization.KBinsDiscretizer._validate_n_bins(self,n_features)
sklearn.preprocessing._discretization.KBinsDiscretizer.fit(self,X,y=None,sample_weight=None)
sklearn.preprocessing._discretization.KBinsDiscretizer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._discretization.KBinsDiscretizer.inverse_transform(self,Xt)
sklearn.preprocessing._discretization.KBinsDiscretizer.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_function_transformer.py----------------------------------------
A:sklearn.preprocessing._function_transformer.idx_selected->slice(None, None, max(1, X.shape[0] // 100))
A:sklearn.preprocessing._function_transformer.X_round_trip->self.inverse_transform(self.transform(X[idx_selected]))
A:sklearn.preprocessing._function_transformer.X->check_array(X, accept_sparse=self.accept_sparse)
A:sklearn.preprocessing._function_transformer.out->self._transform(X, func=self.func, kw_args=self.kw_args)
A:sklearn.preprocessing._function_transformer.input_features->_check_feature_names_in(self, input_features)
A:sklearn.preprocessing._function_transformer.names_out->self.feature_names_out(self, input_features)
sklearn.preprocessing.FunctionTransformer(self,func=None,inverse_func=None,*,validate=False,accept_sparse=False,check_inverse=True,feature_names_out=None,kw_args=None,inv_kw_args=None)
sklearn.preprocessing.FunctionTransformer.__sklearn_is_fitted__(self)
sklearn.preprocessing.FunctionTransformer._check_input(self,X,*,reset)
sklearn.preprocessing.FunctionTransformer._check_inverse_transform(self,X)
sklearn.preprocessing.FunctionTransformer._more_tags(self)
sklearn.preprocessing.FunctionTransformer._transform(self,X,func=None,kw_args=None)
sklearn.preprocessing.FunctionTransformer.fit(self,X,y=None)
sklearn.preprocessing.FunctionTransformer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.FunctionTransformer.inverse_transform(self,X)
sklearn.preprocessing.FunctionTransformer.set_output(self,*,transform=None)
sklearn.preprocessing.FunctionTransformer.transform(self,X)
sklearn.preprocessing._function_transformer.FunctionTransformer(self,func=None,inverse_func=None,*,validate=False,accept_sparse=False,check_inverse=True,feature_names_out=None,kw_args=None,inv_kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.__init__(self,func=None,inverse_func=None,*,validate=False,accept_sparse=False,check_inverse=True,feature_names_out=None,kw_args=None,inv_kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.__sklearn_is_fitted__(self)
sklearn.preprocessing._function_transformer.FunctionTransformer._check_input(self,X,*,reset)
sklearn.preprocessing._function_transformer.FunctionTransformer._check_inverse_transform(self,X)
sklearn.preprocessing._function_transformer.FunctionTransformer._more_tags(self)
sklearn.preprocessing._function_transformer.FunctionTransformer._transform(self,X,func=None,kw_args=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.fit(self,X,y=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.inverse_transform(self,X)
sklearn.preprocessing._function_transformer.FunctionTransformer.set_output(self,*,transform=None)
sklearn.preprocessing._function_transformer.FunctionTransformer.transform(self,X)
sklearn.preprocessing._function_transformer._identity(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py----------------------------------------
A:sklearn.preprocessing._encoders.X_temp->check_array(X, dtype=None, force_all_finite=force_all_finite)
A:sklearn.preprocessing._encoders.X->check_array(X, force_all_finite='allow-nan')
A:sklearn.preprocessing._encoders.Xi->Xi.copy().copy()
A:sklearn.preprocessing._encoders.(X_list, n_samples, n_features)->self._check_X(X, force_all_finite=force_all_finite)
A:sklearn.preprocessing._encoders.result->_unique(Xi, return_counts=compute_counts)
A:sklearn.preprocessing._encoders.cats->self._remove_dropped_categories(cats, i)
A:sklearn.preprocessing._encoders.sorted_cats->numpy.sort(cats)
A:sklearn.preprocessing._encoders.diff->_check_unknown(Xi, cats)
A:sklearn.preprocessing._encoders.msg->'The following categories were supposed to be dropped, but were not found in the training data.\n{}'.format('\n'.join(['Category: {}, Feature: {}'.format(c, v) for (c, v) in missing_drops]))
A:sklearn.preprocessing._encoders.X_int->numpy.zeros((n_samples, n_features), dtype=int)
A:sklearn.preprocessing._encoders.X_mask->numpy.ones((n_samples, n_features), dtype=bool)
A:sklearn.preprocessing._encoders.(diff, valid_mask)->_check_unknown(Xi, self.categories_[i], return_mask=True)
A:sklearn.preprocessing._encoders.X_int[:, i]->_encode(Xi, uniques=self.categories_[i], check_unknown=False)
A:sklearn.preprocessing._encoders.max_categories->getattr(self, 'max_categories', None)
A:sklearn.preprocessing._encoders.min_frequency->getattr(self, 'min_frequency', None)
A:sklearn.preprocessing._encoders.infrequent_mask->numpy.zeros(category_count.shape[0], dtype=bool)
A:sklearn.preprocessing._encoders.output->numpy.flatnonzero(infrequent_mask)
A:sklearn.preprocessing._encoders.n_cats->len(cats)
A:sklearn.preprocessing._encoders.mapping->numpy.empty(n_cats, dtype=np.int64)
A:sklearn.preprocessing._encoders.frequent_indices->numpy.setdiff1d(np.arange(n_cats), infreq_idx)
A:sklearn.preprocessing._encoders.mapping[frequent_indices]->numpy.arange(n_frequent_cats)
A:sklearn.preprocessing._encoders.rows_to_update->slice(None)
A:sklearn.preprocessing._encoders.X_int[rows_to_update, i]->numpy.take(mapping, X_int[rows_to_update, i])
A:sklearn.preprocessing._encoders.drop_idx_after_grouping->numpy.array(drop_indices, dtype=object)
A:sklearn.preprocessing._encoders.drop_array->numpy.asarray(self.drop, dtype=object)
A:sklearn.preprocessing._encoders.droplen->len(drop_array)
A:sklearn.preprocessing._encoders.self.drop_idx_->numpy.asarray(drop_idx_, dtype=object)
A:sklearn.preprocessing._encoders.self._n_features_outs->self._compute_n_features_outs()
A:sklearn.preprocessing._encoders.capitalize_transform_output->transform_output.capitalize()
A:sklearn.preprocessing._encoders.(X_int, X_mask)->self._transform(X, handle_unknown=self.handle_unknown, force_all_finite='allow-nan', ignore_category_indices=self._missing_indices)
A:sklearn.preprocessing._encoders.to_drop->to_drop.reshape(1, -1).reshape(1, -1)
A:sklearn.preprocessing._encoders.to_drop[i]->len(cats)
A:sklearn.preprocessing._encoders.mask->numpy.ones((n_samples, n_features), dtype=bool).ravel()
A:sklearn.preprocessing._encoders.feature_indices->numpy.cumsum([0] + self._n_features_outs)
A:sklearn.preprocessing._encoders.indptr->numpy.empty(n_samples + 1, dtype=int)
A:sklearn.preprocessing._encoders.data->numpy.ones(indptr[-1])
A:sklearn.preprocessing._encoders.out->scipy.sparse.csr_matrix((data, indices, indptr), shape=(n_samples, feature_indices[-1]), dtype=self.dtype)
A:sklearn.preprocessing._encoders.n_features->len(self.categories_)
A:sklearn.preprocessing._encoders.n_features_out->numpy.sum(self._n_features_outs)
A:sklearn.preprocessing._encoders.dt->numpy.result_type(*[cat.dtype for cat in self.categories_])
A:sklearn.preprocessing._encoders.X_tr->X_tr.astype(object, copy=False).astype(object, copy=False)
A:sklearn.preprocessing._encoders.cats_wo_dropped->self._remove_dropped_categories(transformed_features[i], i)
A:sklearn.preprocessing._encoders.labels->numpy.asarray(sub.argmax(axis=1)).flatten()
A:sklearn.preprocessing._encoders.unknown->numpy.asarray(sub.sum(axis=1) == 0).flatten()
A:sklearn.preprocessing._encoders.dropped->numpy.asarray(sub.sum(axis=1) == 0).flatten()
A:sklearn.preprocessing._encoders.all_zero_samples->numpy.flatnonzero(dropped)
A:sklearn.preprocessing._encoders.input_features->_check_feature_names_in(self, input_features)
A:sklearn.preprocessing._encoders.name_combiner->self._check_get_feature_name_combiner()
A:sklearn.preprocessing._encoders.dry_run_combiner->self.feature_name_combiner('feature', 'category')
A:sklearn.preprocessing._encoders.fit_results->self._fit(X, handle_unknown=self.handle_unknown, force_all_finite='allow-nan', return_and_ignore_missing_for_infrequent=True)
A:sklearn.preprocessing._encoders.X_trans->numpy.zeros((n_samples, n_features), dtype=int).astype(self.dtype, copy=False)
A:sklearn.preprocessing._encoders.infrequent_indices->getattr(self, '_infrequent_indices', None)
A:sklearn.preprocessing._encoders.X_i_mask->_get_mask(labels, self.encoded_missing_value)
A:sklearn.preprocessing._encoders.frequent_categories_mask->numpy.ones_like(categories, dtype=bool)
A:sklearn.preprocessing._encoders.unknown_labels->_get_mask(labels, self.unknown_value)
A:sklearn.preprocessing._encoders.labels_int->labels[rows_to_update].astype('int64', copy=False)
sklearn.preprocessing.OneHotEncoder(self,*,categories='auto',drop=None,sparse_output=True,dtype=np.float64,handle_unknown='error',min_frequency=None,max_categories=None,feature_name_combiner='concat')
sklearn.preprocessing.OneHotEncoder._check_get_feature_name_combiner(self)
sklearn.preprocessing.OneHotEncoder._compute_n_features_outs(self)
sklearn.preprocessing.OneHotEncoder._compute_transformed_categories(self,i,remove_dropped=True)
sklearn.preprocessing.OneHotEncoder._map_drop_idx_to_infrequent(self,feature_idx,drop_idx)
sklearn.preprocessing.OneHotEncoder._remove_dropped_categories(self,categories,i)
sklearn.preprocessing.OneHotEncoder._set_drop_idx(self)
sklearn.preprocessing.OneHotEncoder.fit(self,X,y=None)
sklearn.preprocessing.OneHotEncoder.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.OneHotEncoder.inverse_transform(self,X)
sklearn.preprocessing.OneHotEncoder.transform(self,X)
sklearn.preprocessing.OrdinalEncoder(self,*,categories='auto',dtype=np.float64,handle_unknown='error',unknown_value=None,encoded_missing_value=np.nan,min_frequency=None,max_categories=None)
sklearn.preprocessing.OrdinalEncoder.fit(self,X,y=None)
sklearn.preprocessing.OrdinalEncoder.inverse_transform(self,X)
sklearn.preprocessing.OrdinalEncoder.transform(self,X)
sklearn.preprocessing._encoders.OneHotEncoder(self,*,categories='auto',drop=None,sparse_output=True,dtype=np.float64,handle_unknown='error',min_frequency=None,max_categories=None,feature_name_combiner='concat')
sklearn.preprocessing._encoders.OneHotEncoder.__init__(self,*,categories='auto',drop=None,sparse_output=True,dtype=np.float64,handle_unknown='error',min_frequency=None,max_categories=None,feature_name_combiner='concat')
sklearn.preprocessing._encoders.OneHotEncoder._check_get_feature_name_combiner(self)
sklearn.preprocessing._encoders.OneHotEncoder._compute_n_features_outs(self)
sklearn.preprocessing._encoders.OneHotEncoder._compute_transformed_categories(self,i,remove_dropped=True)
sklearn.preprocessing._encoders.OneHotEncoder._map_drop_idx_to_infrequent(self,feature_idx,drop_idx)
sklearn.preprocessing._encoders.OneHotEncoder._remove_dropped_categories(self,categories,i)
sklearn.preprocessing._encoders.OneHotEncoder._set_drop_idx(self)
sklearn.preprocessing._encoders.OneHotEncoder.fit(self,X,y=None)
sklearn.preprocessing._encoders.OneHotEncoder.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._encoders.OneHotEncoder.inverse_transform(self,X)
sklearn.preprocessing._encoders.OneHotEncoder.transform(self,X)
sklearn.preprocessing._encoders.OrdinalEncoder(self,*,categories='auto',dtype=np.float64,handle_unknown='error',unknown_value=None,encoded_missing_value=np.nan,min_frequency=None,max_categories=None)
sklearn.preprocessing._encoders.OrdinalEncoder.__init__(self,*,categories='auto',dtype=np.float64,handle_unknown='error',unknown_value=None,encoded_missing_value=np.nan,min_frequency=None,max_categories=None)
sklearn.preprocessing._encoders.OrdinalEncoder.fit(self,X,y=None)
sklearn.preprocessing._encoders.OrdinalEncoder.inverse_transform(self,X)
sklearn.preprocessing._encoders.OrdinalEncoder.transform(self,X)
sklearn.preprocessing._encoders._BaseEncoder(TransformerMixin,BaseEstimator)
sklearn.preprocessing._encoders._BaseEncoder._check_X(self,X,force_all_finite=True)
sklearn.preprocessing._encoders._BaseEncoder._check_infrequent_enabled(self)
sklearn.preprocessing._encoders._BaseEncoder._fit(self,X,handle_unknown='error',force_all_finite=True,return_counts=False,return_and_ignore_missing_for_infrequent=False)
sklearn.preprocessing._encoders._BaseEncoder._fit_infrequent_category_mapping(self,n_samples,category_counts,missing_indices)
sklearn.preprocessing._encoders._BaseEncoder._identify_infrequent(self,category_count,n_samples,col_idx)
sklearn.preprocessing._encoders._BaseEncoder._map_infrequent_categories(self,X_int,X_mask,ignore_category_indices)
sklearn.preprocessing._encoders._BaseEncoder._more_tags(self)
sklearn.preprocessing._encoders._BaseEncoder._transform(self,X,handle_unknown='error',force_all_finite=True,warn_on_unknown=False,ignore_category_indices=None)
sklearn.preprocessing._encoders._BaseEncoder.infrequent_categories_(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/_target_encoder.py----------------------------------------
A:sklearn.preprocessing._target_encoder.(X_ordinal, X_known_mask, y_encoded, n_categories)->self._fit_encodings_all(X, y)
A:sklearn.preprocessing._target_encoder.cv->StratifiedKFold(self.cv, shuffle=self.shuffle, random_state=self.random_state)
A:sklearn.preprocessing._target_encoder.X_out->numpy.empty_like(X_ordinal, dtype=np.float64)
A:sklearn.preprocessing._target_encoder.y_train_mean->numpy.mean(y_train, axis=0)
A:sklearn.preprocessing._target_encoder.encodings->_fit_encoding_fast(X_ordinal, y, n_categories, self.smooth, target_mean)
A:sklearn.preprocessing._target_encoder.(X_ordinal, X_known_mask)->self._transform(X, handle_unknown='ignore', force_all_finite='allow-nan')
A:sklearn.preprocessing._target_encoder.inferred_type_of_target->type_of_target(y, input_name='y')
A:sklearn.preprocessing._target_encoder.label_encoder->LabelEncoder()
A:sklearn.preprocessing._target_encoder.y->_check_y(y, y_numeric=True, estimator=self)
A:sklearn.preprocessing._target_encoder.label_binarizer->LabelBinarizer()
A:sklearn.preprocessing._target_encoder.self.target_mean_->numpy.mean(y, axis=0)
A:sklearn.preprocessing._target_encoder.n_categories->numpy.fromiter((len(category_for_feature) for category_for_feature in self.categories_), dtype=np.int64, count=len(self.categories_))
A:sklearn.preprocessing._target_encoder.y_variance->numpy.var(y)
A:sklearn.preprocessing._target_encoder.n_classes->len(self.classes_)
A:sklearn.preprocessing._target_encoder.encoding->self._fit_encoding_binary_or_continuous(X_ordinal, y_class, n_categories, target_mean[i])
A:sklearn.preprocessing._target_encoder.feature_names->_check_feature_names_in(self, input_features)
sklearn.preprocessing.TargetEncoder(self,categories='auto',target_type='auto',smooth='auto',cv=5,shuffle=True,random_state=None)
sklearn.preprocessing.TargetEncoder._fit_encoding_binary_or_continuous(self,X_ordinal,y,n_categories,target_mean)
sklearn.preprocessing.TargetEncoder._fit_encoding_multiclass(self,X_ordinal,y,n_categories,target_mean)
sklearn.preprocessing.TargetEncoder._fit_encodings_all(self,X,y)
sklearn.preprocessing.TargetEncoder._more_tags(self)
sklearn.preprocessing.TargetEncoder._transform_X_ordinal(self,X_out,X_ordinal,X_unknown_mask,row_indices,encodings,target_mean)
sklearn.preprocessing.TargetEncoder.fit(self,X,y)
sklearn.preprocessing.TargetEncoder.fit_transform(self,X,y)
sklearn.preprocessing.TargetEncoder.get_feature_names_out(self,input_features=None)
sklearn.preprocessing.TargetEncoder.transform(self,X)
sklearn.preprocessing._target_encoder.TargetEncoder(self,categories='auto',target_type='auto',smooth='auto',cv=5,shuffle=True,random_state=None)
sklearn.preprocessing._target_encoder.TargetEncoder.__init__(self,categories='auto',target_type='auto',smooth='auto',cv=5,shuffle=True,random_state=None)
sklearn.preprocessing._target_encoder.TargetEncoder._fit_encoding_binary_or_continuous(self,X_ordinal,y,n_categories,target_mean)
sklearn.preprocessing._target_encoder.TargetEncoder._fit_encoding_multiclass(self,X_ordinal,y,n_categories,target_mean)
sklearn.preprocessing._target_encoder.TargetEncoder._fit_encodings_all(self,X,y)
sklearn.preprocessing._target_encoder.TargetEncoder._more_tags(self)
sklearn.preprocessing._target_encoder.TargetEncoder._transform_X_ordinal(self,X_out,X_ordinal,X_unknown_mask,row_indices,encodings,target_mean)
sklearn.preprocessing._target_encoder.TargetEncoder.fit(self,X,y)
sklearn.preprocessing._target_encoder.TargetEncoder.fit_transform(self,X,y)
sklearn.preprocessing._target_encoder.TargetEncoder.get_feature_names_out(self,input_features=None)
sklearn.preprocessing._target_encoder.TargetEncoder.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_encoders.py----------------------------------------
A:sklearn.preprocessing.tests.test_encoders.X->numpy.array([['A'], ['B'], ['C']], dtype=object)
A:sklearn.preprocessing.tests.test_encoders.enc_sparse->OneHotEncoder()
A:sklearn.preprocessing.tests.test_encoders.enc_dense->OneHotEncoder(sparse_output=False)
A:sklearn.preprocessing.tests.test_encoders.X_trans_sparse->csr_container(X_trans)
A:sklearn.preprocessing.tests.test_encoders.X_trans_dense->OneHotEncoder(sparse_output=False).fit_transform(X)
A:sklearn.preprocessing.tests.test_encoders.X2->numpy.array(['55555', '22']).reshape((-1, 1))
A:sklearn.preprocessing.tests.test_encoders.oh->OneHotEncoder()
A:sklearn.preprocessing.tests.test_encoders.X2_passed->numpy.array(['55555', '22']).reshape((-1, 1)).copy()
A:sklearn.preprocessing.tests.test_encoders.X_expected->numpy.array([[0, 0]])
A:sklearn.preprocessing.tests.test_encoders.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_encoders.X_df->pytest.importorskip('pandas').DataFrame({'A': ['a', 'b'], 'B': [1, 2]})
A:sklearn.preprocessing.tests.test_encoders.enc->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X)
A:sklearn.preprocessing.tests.test_encoders.feature_names->OneHotEncoder(min_frequency=4, sparse_output=False, drop=None).fit(X).get_feature_names_out()
A:sklearn.preprocessing.tests.test_encoders.feature_names2->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).get_feature_names_out(['one', 'two', 'three', 'four', 'five'])
A:sklearn.preprocessing.tests.test_encoders.Xtr1->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).fit_transform(X)
A:sklearn.preprocessing.tests.test_encoders.Xtr2->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).fit_transform(X)
A:sklearn.preprocessing.tests.test_encoders.Xtr->check_categorical_onehot(df)
A:sklearn.preprocessing.tests.test_encoders.X_tr->numpy.array([[0, 1, 1, 2], [1, 0, 1, 0]])
A:sklearn.preprocessing.tests.test_encoders.exp->numpy.array([[0.0], [np.nan]])
A:sklearn.preprocessing.tests.test_encoders.msg->re.escape("In column 0, the predefined categories have type 'bytes' which is incompatible with values of type 'str_'.")
A:sklearn.preprocessing.tests.test_encoders.X_trans->OrdinalEncoder(min_frequency=4).fit(X).transform(X_test)
A:sklearn.preprocessing.tests.test_encoders.ohe->OneHotEncoder(min_frequency=4, sparse_output=False, drop=None).fit(X)
A:sklearn.preprocessing.tests.test_encoders.res_list->res.tolist()
A:sklearn.preprocessing.tests.test_encoders.expected->numpy.array([[1], [1], [0], [1]])
A:sklearn.preprocessing.tests.test_encoders.expected_drop_idx->numpy.array([0, None])
A:sklearn.preprocessing.tests.test_encoders.result->OneHotEncoder(min_frequency=4, sparse_output=False, drop=None).fit(X).fit_transform(X)
A:sklearn.preprocessing.tests.test_encoders.X_fit->numpy.array([[1], [2], [3]])
A:sklearn.preprocessing.tests.test_encoders.X_trans_enc->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).transform(X_trans)
A:sklearn.preprocessing.tests.test_encoders.X_trans_inv->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).inverse_transform(X_trans_enc)
A:sklearn.preprocessing.tests.test_encoders.inv_exp->numpy.array([[3, None], [None, 8], [1, 7]], dtype=object)
A:sklearn.preprocessing.tests.test_encoders.trans->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).fit_transform(X).toarray()
A:sklearn.preprocessing.tests.test_encoders.X_inv_trans->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).inverse_transform(trans)
A:sklearn.preprocessing.tests.test_encoders.X_array->numpy.array(X, dtype=object)
A:sklearn.preprocessing.tests.test_encoders.ohe_base->OneHotEncoder(sparse_output=density)
A:sklearn.preprocessing.tests.test_encoders.ohe_test->OneHotEncoder(sparse_output=density, drop=drop)
A:sklearn.preprocessing.tests.test_encoders.X_inv->OneHotEncoder(min_frequency=4, sparse_output=False, drop=None).fit(X).inverse_transform(X_expected)
A:sklearn.preprocessing.tests.test_encoders.X_test->numpy.array([['snake', 'red'], ['deer', 'green'], [np.nan, 'green'], ['dog', 'green'], ['cat', 'red']], dtype=object)
A:sklearn.preprocessing.tests.test_encoders.X_inverse->OrdinalEncoder(min_frequency=4).fit(X).inverse_transform(X_trans)
A:sklearn.preprocessing.tests.test_encoders.X_test_trans->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan).fit(X_train).transform(X_test)
A:sklearn.preprocessing.tests.test_encoders.expected_inv->numpy.array([['c', 'infrequent_sklearn'], ['infrequent_sklearn', 5]], dtype=object)
A:sklearn.preprocessing.tests.test_encoders.oe->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan).fit(X_train)
A:sklearn.preprocessing.tests.test_encoders.names->OneHotEncoder(min_frequency=4, sparse_output=False, drop=None).fit(X).get_feature_names_out()
A:sklearn.preprocessing.tests.test_encoders.df->pytest.importorskip('pandas').DataFrame({'col1': pd.Series(['c', 'a', pd_missing_value, 'b', 'a'], dtype='category')})
A:sklearn.preprocessing.tests.test_encoders.expected_df_trans->numpy.array([[0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [1, 0, 0, 0]])
A:sklearn.preprocessing.tests.test_encoders.df_trans->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan).fit(X_train).transform(df)
A:sklearn.preprocessing.tests.test_encoders.X_sparse->csr_container(X)
A:sklearn.preprocessing.tests.test_encoders.encoder->Encoder(categories=[['A', 'B', 'C']])
A:sklearn.preprocessing.tests.test_encoders.feature_names_out->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=2).fit(X).get_feature_names_out()
A:sklearn.preprocessing.tests.test_encoders.X_roundtrip->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan).fit(X_train).inverse_transform(X_test_trans)
A:sklearn.preprocessing.tests.test_encoders.ohe_default->OneHotEncoder(sparse_output=False).set_output(transform='default')
A:sklearn.preprocessing.tests.test_encoders.ohe_pandas->OneHotEncoder(sparse_output=False).set_output(transform='pandas')
A:sklearn.preprocessing.tests.test_encoders.X_default->OrdinalEncoder().set_output(transform='default').fit_transform(X_df)
A:sklearn.preprocessing.tests.test_encoders.X_pandas->OrdinalEncoder().set_output(transform='pandas').fit_transform(X_df)
A:sklearn.preprocessing.tests.test_encoders.ord_default->OrdinalEncoder().set_output(transform='default')
A:sklearn.preprocessing.tests.test_encoders.ord_pandas->OrdinalEncoder().set_output(transform='pandas')
A:sklearn.preprocessing.tests.test_encoders.ordinal->OrdinalEncoder(min_frequency=4).fit(X)
A:sklearn.preprocessing.tests.test_encoders.expected_inverse->numpy.array([[3, 0], ['infrequent_sklearn', 1]], dtype=object)
A:sklearn.preprocessing.tests.test_encoders.categorical_dtype->pytest.importorskip('pandas').CategoricalDtype(['bird', 'cat', 'dog', 'snake'])
A:sklearn.preprocessing.tests.test_encoders.adjusted_encoder->OrdinalEncoder(**kwargs, handle_unknown='use_encoded_value', unknown_value=-1).fit(X_train)
A:sklearn.preprocessing.tests.test_encoders.default_encoder->OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1).fit(X_train)
sklearn.preprocessing.tests.test_encoders.check_categorical_onehot(X)
sklearn.preprocessing.tests.test_encoders.test_X_is_not_1D(X,method)
sklearn.preprocessing.tests.test_encoders.test_X_is_not_1D_pandas(method)
sklearn.preprocessing.tests.test_encoders.test_categories(density,drop)
sklearn.preprocessing.tests.test_encoders.test_drop_idx_infrequent_categories()
sklearn.preprocessing.tests.test_encoders.test_encoder_dtypes()
sklearn.preprocessing.tests.test_encoders.test_encoder_dtypes_pandas()
sklearn.preprocessing.tests.test_encoders.test_encoder_duplicate_specified_categories(Encoder)
sklearn.preprocessing.tests.test_encoders.test_encoder_nan_ending_specified_categories(Encoder)
sklearn.preprocessing.tests.test_encoders.test_encoder_not_fitted(Encoder)
sklearn.preprocessing.tests.test_encoders.test_encoders_has_categorical_tags(Encoder)
sklearn.preprocessing.tests.test_encoders.test_encoders_string_categories(input_dtype,category_dtype,array_type)
sklearn.preprocessing.tests.test_encoders.test_invalid_drop_length(drop)
sklearn.preprocessing.tests.test_encoders.test_mixed_string_bytes_categoricals()
sklearn.preprocessing.tests.test_encoders.test_ohe_drop_first_explicit_categories(handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_ohe_drop_first_handle_unknown_ignore_warns(handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_ohe_drop_if_binary_handle_unknown_ignore_warns(handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_handle_unknown_error()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_mixed()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_multiple_categories()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_multiple_categories_dtypes()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_one_level_errors(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_three_levels(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_three_levels_drop_frequent(drop)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_three_levels_drop_infrequent_errors(drop)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_three_levels_user_cats()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_two_levels(kwargs,categories)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_two_levels_drop_frequent(drop)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_two_levels_drop_infrequent_errors(drop)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_two_levels_user_cats()
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_two_levels_user_cats_one_frequent(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ohe_infrequent_user_cats_unknown_training_errors(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ohe_missing_value_support_pandas()
sklearn.preprocessing.tests.test_encoders.test_ohe_missing_value_support_pandas_categorical(pd_nan_type,handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_ohe_missing_values_get_feature_names(missing_value)
sklearn.preprocessing.tests.test_encoders.test_ohe_more_informative_error_message()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder(X)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_categories(X,cat_exp,cat_dtype)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_custom_feature_name_combiner()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_drop_equals_if_binary()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_drop_manual(missing_value)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_drop_reset(drop,reset_drop)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_dtype(input_dtype,output_dtype)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_dtype_pandas(output_dtype)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_feature_names()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_feature_names_drop(drop,expected_names)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_feature_names_unicode()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_handle_unknown(handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_handle_unknown_strings(handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_inverse(handle_unknown,sparse_,drop)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_inverse_if_binary()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_inverse_transform_raise_error_with_unknown(X,X_trans,sparse_)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_pandas()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_set_output()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_set_params()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_sparse_dense()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_specified_categories(X,X2,cats,cat_dtype,handle_unknown)
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_specified_categories_mixed_columns()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_unsorted_categories()
sklearn.preprocessing.tests.test_encoders.test_one_hot_encoder_warning()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder(X)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_all_frequent(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_all_infrequent(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_encoded_missing_value_error(with_pandas)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_features_names_out_pandas()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_fit_with_unseen_category()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_missing_and_unknown(X,expected_X_trans,X_test)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_unknown_string_dtypes(X_train,X_test)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_unknowns_nan()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_unknowns_nan_non_float_dtype()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_unknowns_numeric(dtype)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_handle_unknowns_string()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_infrequent_custom_mapping()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_infrequent_mixed()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_infrequent_multiple_categories_dtypes()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_infrequent_three_levels(kwargs)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_infrequent_three_levels_user_cats()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_inverse()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_missing_appears_frequent()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_missing_appears_infrequent()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_missing_unknown_encoding_max()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_missing_value_support_pandas_categorical(pd_nan_type,encoded_missing_value)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_passthrough_missing_values_float(encoded_missing_value)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_passthrough_missing_values_float_errors_dtype()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_python_integer()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_raise_categories_shape()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_sparse(csr_container)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_specified_categories(X,X2,cats,cat_dtype)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_specified_categories_missing_passthrough(X,X2,cats,cat_dtype)
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_unknown_missing_interaction()
sklearn.preprocessing.tests.test_encoders.test_ordinal_encoder_unknown_missing_interaction_both_nan(X_train,X_test_trans_expected,X_roundtrip_expected)
sklearn.preprocessing.tests.test_encoders.test_ordinal_set_output()
sklearn.preprocessing.tests.test_encoders.test_predefined_categories_dtype()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_label.py----------------------------------------
A:sklearn.preprocessing.tests.test_label.iris->sklearn.datasets.load_iris()
A:sklearn.preprocessing.tests.test_label.a->a.toarray().toarray()
A:sklearn.preprocessing.tests.test_label.lb->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output)
A:sklearn.preprocessing.tests.test_label.got->_inverse_binarize_multiclass(csr_container([[0, 1, 0], [-1, 0, -1], [0, 0, 0]]), np.arange(3))
A:sklearn.preprocessing.tests.test_label.to_invert->numpy.array([[1, 0], [0, 1], [0, 1], [1, 0]])
A:sklearn.preprocessing.tests.test_label.expected->numpy.array([[3, 0], [0, 3], [3, 0]])[:, 1].reshape((-1, 1))
A:sklearn.preprocessing.tests.test_label.inp->numpy.array(inp, dtype=object)
A:sklearn.preprocessing.tests.test_label.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_label.y_true->y_true.unique().unique()
A:sklearn.preprocessing.tests.test_label.y_out->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output).transform([1, 0])
A:sklearn.preprocessing.tests.test_label.one_class->numpy.array([0, 0, 0, 0])
A:sklearn.preprocessing.tests.test_label.le->LabelEncoder()
A:sklearn.preprocessing.tests.test_label.ret->LabelEncoder().fit_transform(values)
A:sklearn.preprocessing.tests.test_label.transformed->LabelEncoder().transform([])
A:sklearn.preprocessing.tests.test_label.inverse_transformed->LabelEncoder().inverse_transform([])
A:sklearn.preprocessing.tests.test_label.indicator_mat->numpy.array([[1, 1]])
A:sklearn.preprocessing.tests.test_label.inverse->inputs[0]()
A:sklearn.preprocessing.tests.test_label.mlb->MultiLabelBinarizer()
A:sklearn.preprocessing.tests.test_label.Y->numpy.array([[1, 0, 0], [0, 1, 0]])
A:sklearn.preprocessing.tests.test_label.matrix->MultiLabelBinarizer().fit(y).transform([[4, 1], [2, 0]])
A:sklearn.preprocessing.tests.test_label.indicator_mat2->numpy.array([[0, 1, 1], [1, 0, 0], [1, 1, 0]])
A:sklearn.preprocessing.tests.test_label.tuple_classes->_to_object_array([(1,), (2,), (3,)])
A:sklearn.preprocessing.tests.test_label.indicator_mat_inv->numpy.array(mlb.inverse_transform(indicator_mat), dtype=object)
A:sklearn.preprocessing.tests.test_label.out->label_binarize([0, 1, 2, 3], classes=[3, 2, 0, 1])
A:sklearn.preprocessing.tests.test_label.binarized->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output).fit_transform(y)
A:sklearn.preprocessing.tests.test_label.y_type->type_of_target(y)
A:sklearn.preprocessing.tests.test_label.inversed->_inverse_binarize_thresholding(binarized, output_type=y_type, classes=classes, threshold=(neg_label + pos_label) / 2.0)
A:sklearn.preprocessing.tests.test_label.inverse_output->LabelBinarizer(neg_label=neg_label, pos_label=pos_label, sparse_output=sparse_output).inverse_transform(binarized)
A:sklearn.preprocessing.tests.test_label.y_ind->numpy.array([[0, 1, 0], [1, 1, 1], [0, 0, 0]])
A:sklearn.preprocessing.tests.test_label.y->arr_type(y_ind)
A:sklearn.preprocessing.tests.test_label.y_trans->LabelEncoder().transform([np.nan])
A:sklearn.preprocessing.tests.test_label.y_encoded_with_kwarg->encoder.fit_transform(y=['a', 'b', 'c'])
A:sklearn.preprocessing.tests.test_label.y_encoded_positional->encoder.fit_transform(['a', 'b', 'c'])
sklearn.preprocessing.tests.test_label.check_binarized_results(y,classes,pos_label,neg_label,expected)
sklearn.preprocessing.tests.test_label.test_invalid_input_label_binarize()
sklearn.preprocessing.tests.test_label.test_inverse_binarize_multiclass(csr_container)
sklearn.preprocessing.tests.test_label.test_label_binarize_binary()
sklearn.preprocessing.tests.test_label.test_label_binarize_multiclass()
sklearn.preprocessing.tests.test_label.test_label_binarize_multilabel(arr_type)
sklearn.preprocessing.tests.test_label.test_label_binarize_with_class_order()
sklearn.preprocessing.tests.test_label.test_label_binarizer()
sklearn.preprocessing.tests.test_label.test_label_binarizer_errors()
sklearn.preprocessing.tests.test_label.test_label_binarizer_pandas_nullable(dtype,unique_first)
sklearn.preprocessing.tests.test_label.test_label_binarizer_set_label_encoding()
sklearn.preprocessing.tests.test_label.test_label_binarizer_sparse_errors(csr_container)
sklearn.preprocessing.tests.test_label.test_label_binarizer_unseen_labels()
sklearn.preprocessing.tests.test_label.test_label_encoder(values,classes,unknown)
sklearn.preprocessing.tests.test_label.test_label_encoder_empty_array(values)
sklearn.preprocessing.tests.test_label.test_label_encoder_errors()
sklearn.preprocessing.tests.test_label.test_label_encoder_negative_ints()
sklearn.preprocessing.tests.test_label.test_label_encoder_str_bad_shape(dtype)
sklearn.preprocessing.tests.test_label.test_label_encoders_do_not_have_set_output(encoder)
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_empty_sample()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_given_classes()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_inverse_validation()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_multiple_calls()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_non_integer_labels()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_non_unique()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_same_length_sequence()
sklearn.preprocessing.tests.test_label.test_multilabel_binarizer_unknown_class()
sklearn.preprocessing.tests.test_label.test_nan_label_encoder()
sklearn.preprocessing.tests.test_label.test_sparse_output_multilabel_binarizer()
sklearn.preprocessing.tests.test_label.test_sparse_output_multilabel_binarizer_errors(csr_container)
sklearn.preprocessing.tests.test_label.toarray(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_discretization.py----------------------------------------
A:sklearn.preprocessing.tests.test_discretization.est->KBinsDiscretizer(n_bins=3, encode='ordinal')
A:sklearn.preprocessing.tests.test_discretization.sample_weight->numpy.array([1, 3, 1, 2], dtype=np.float64)
A:sklearn.preprocessing.tests.test_discretization.n_bins->numpy.full((2, 4), 2.0)
A:sklearn.preprocessing.tests.test_discretization.X->numpy.random.RandomState(0).random_sample((100, 1))
A:sklearn.preprocessing.tests.test_discretization.sample_weight_copy->numpy.copy(sample_weight)
A:sklearn.preprocessing.tests.test_discretization.Xt->KBinsDiscretizer(strategy=strategy, random_state=0).transform(X)
A:sklearn.preprocessing.tests.test_discretization.X_init->numpy.array([2.0, 4.0, 6.0, 8.0, 10.0]).reshape(-1, 1)
A:sklearn.preprocessing.tests.test_discretization.Xt_expected->numpy.array([0, 0, 1, 1, 1]).reshape(-1, 1)
A:sklearn.preprocessing.tests.test_discretization.Xt_1->KBinsDiscretizer(n_bins=3, encode='ordinal').transform(X)
A:sklearn.preprocessing.tests.test_discretization.Xt_2->KBinsDiscretizer(n_bins=3, encode='ordinal').transform(X)
A:sklearn.preprocessing.tests.test_discretization.Xt_3->KBinsDiscretizer(n_bins=3, encode='ordinal').transform(X)
A:sklearn.preprocessing.tests.test_discretization.kbd->KBinsDiscretizer(strategy=strategy, random_state=0)
A:sklearn.preprocessing.tests.test_discretization.Xinv->KBinsDiscretizer(n_bins=3, encode='ordinal').inverse_transform(Xt)
A:sklearn.preprocessing.tests.test_discretization.X2t->KBinsDiscretizer(strategy=strategy, random_state=0).transform(X2)
A:sklearn.preprocessing.tests.test_discretization.X_before->numpy.random.RandomState(0).random_sample((100, 1)).copy()
A:sklearn.preprocessing.tests.test_discretization.Xt_before->KBinsDiscretizer(strategy=strategy, random_state=0).transform(X).copy()
A:sklearn.preprocessing.tests.test_discretization.bin_edges->numpy.array([0.05, 0.23, 0.41, 0.59, 0.77, 0.95])
A:sklearn.preprocessing.tests.test_discretization.X_input->numpy.array(X, dtype=input_dtype)
A:sklearn.preprocessing.tests.test_discretization.kbd_32->KBinsDiscretizer(n_bins=3, encode=encode, dtype=np.float32)
A:sklearn.preprocessing.tests.test_discretization.Xt_32->KBinsDiscretizer(n_bins=3, encode=encode, dtype=np.float32).transform(X_input)
A:sklearn.preprocessing.tests.test_discretization.kbd_64->KBinsDiscretizer(n_bins=3, encode=encode, dtype=np.float64)
A:sklearn.preprocessing.tests.test_discretization.Xt_64->KBinsDiscretizer(n_bins=3, encode=encode, dtype=np.float64).transform(X_input)
A:sklearn.preprocessing.tests.test_discretization.kbd_default->KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')
A:sklearn.preprocessing.tests.test_discretization.kbd_without_subsampling->clone(kbd_default)
A:sklearn.preprocessing.tests.test_discretization.output_names->KBinsDiscretizer(strategy=strategy, random_state=0).get_feature_names_out(input_features)
A:sklearn.preprocessing.tests.test_discretization.kbd_subsampling->KBinsDiscretizer(strategy=strategy, subsample=50000, random_state=global_random_seed)
A:sklearn.preprocessing.tests.test_discretization.kbd_no_subsampling->clone(kbd_subsampling)
sklearn.preprocessing.tests.test_discretization.test_32_equal_64(input_dtype,encode)
sklearn.preprocessing.tests.test_discretization.test_consistent_dtype(in_dtype,out_dtype,encode)
sklearn.preprocessing.tests.test_discretization.test_encode_options()
sklearn.preprocessing.tests.test_discretization.test_fit_transform(strategy,expected,sample_weight)
sklearn.preprocessing.tests.test_discretization.test_fit_transform_n_bins_array(strategy,expected,sample_weight)
sklearn.preprocessing.tests.test_discretization.test_invalid_n_bins_array()
sklearn.preprocessing.tests.test_discretization.test_inverse_transform(strategy,encode,expected_inv)
sklearn.preprocessing.tests.test_discretization.test_kbd_subsample_warning(strategy)
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscretizer_effect_sample_weight()
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscretizer_no_mutating_sample_weight(strategy)
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscretizer_subsample(strategy,global_random_seed)
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscretizer_subsample_default()
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscretizer_wrong_strategy_with_weights(strategy)
sklearn.preprocessing.tests.test_discretization.test_kbinsdiscrtizer_get_feature_names_out(encode,expected_names)
sklearn.preprocessing.tests.test_discretization.test_nonuniform_strategies(strategy,expected_2bins,expected_3bins,expected_5bins)
sklearn.preprocessing.tests.test_discretization.test_numeric_stability(i)
sklearn.preprocessing.tests.test_discretization.test_overwrite()
sklearn.preprocessing.tests.test_discretization.test_percentile_numeric_stability()
sklearn.preprocessing.tests.test_discretization.test_redundant_bins(strategy,expected_bin_edges)
sklearn.preprocessing.tests.test_discretization.test_same_min_max(strategy)
sklearn.preprocessing.tests.test_discretization.test_transform_1d_behavior()
sklearn.preprocessing.tests.test_discretization.test_transform_outside_fit_range(strategy)
sklearn.preprocessing.tests.test_discretization.test_valid_n_bins()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_polynomial.py----------------------------------------
A:sklearn.preprocessing.tests.test_polynomial.X->csr_container((data, (row, col)))
A:sklearn.preprocessing.tests.test_polynomial._->SplineTransformer(degree=3, knots=knots, extrapolation=extrapolation).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.splt->SplineTransformer(n_knots=n_knots, degree=degree, include_bias=include_bias, extrapolation=extrapolation, sparse_output=sparse_output)
A:sklearn.preprocessing.tests.test_polynomial.feature_names->PolynomialFeatures(degree=0, include_bias=True).get_feature_names_out(['\x01F40D', '☮', 'א'])
A:sklearn.preprocessing.tests.test_polynomial.X_trans->PolynomialFeatures(interaction_only=False, include_bias=False, degree=3).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.pipe->Pipeline([['spline', SplineTransformer(n_knots=4, degree=degree, include_bias=bias, extrapolation='linear')], ['ols', LinearRegression(fit_intercept=intercept)]])
A:sklearn.preprocessing.tests.test_polynomial.base_knots->sklearn.preprocessing.SplineTransformer._get_base_knot_positions(X=X, knots=knots, n_knots=n_knots, sample_weight=sample_weight)
A:sklearn.preprocessing.tests.test_polynomial.predictions->Pipeline([['spline', SplineTransformer(n_knots=4, degree=degree, include_bias=bias, extrapolation='linear')], ['ols', LinearRegression(fit_intercept=intercept)]]).predict(X_)
A:sklearn.preprocessing.tests.test_polynomial.transformer->SplineTransformer(degree=degree, extrapolation='periodic', knots=[[0.0], [1.0], [3.0], [4.0], [5.0], [8.0]])
A:sklearn.preprocessing.tests.test_polynomial.Xt->SplineTransformer(degree=degree, extrapolation='periodic', knots=[[0.0], [1.0], [3.0], [4.0], [5.0], [8.0]]).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.coef->numpy.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])
A:sklearn.preprocessing.tests.test_polynomial.spl->BSpline(np.arange(-3, 4), coef, degree, 'periodic')
A:sklearn.preprocessing.tests.test_polynomial.Xspl->spl(X[:, 0])
A:sklearn.preprocessing.tests.test_polynomial.transformer_1->SplineTransformer(degree=3, extrapolation='periodic', knots=[[0.0], [1.0], [3.0], [4.0], [5.0], [8.0]])
A:sklearn.preprocessing.tests.test_polynomial.transformer_2->SplineTransformer(degree=3, extrapolation='periodic', knots=[[1.0], [3.0], [4.0], [5.0], [8.0], [9.0]])
A:sklearn.preprocessing.tests.test_polynomial.Xt_1->SplineTransformer(degree=3, extrapolation='periodic', knots=[[0.0], [1.0], [3.0], [4.0], [5.0], [8.0]]).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.Xt_2->SplineTransformer(degree=3, extrapolation='periodic', knots=[[1.0], [3.0], [4.0], [5.0], [8.0], [9.0]]).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.diff->numpy.diff(dXt, axis=0)
A:sklearn.preprocessing.tests.test_polynomial.y->csr_container((data, (row, col))).squeeze()
A:sklearn.preprocessing.tests.test_polynomial.rng->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_polynomial.splines->SplineTransformer(n_knots=n_knots, degree=degree, include_bias=include_bias, extrapolation=extrapolation, sparse_output=sparse_output).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.kbd->KBinsDiscretizer(n_bins=n_bins, encode='onehot-dense', strategy='quantile')
A:sklearn.preprocessing.tests.test_polynomial.kbins->KBinsDiscretizer(n_bins=n_bins, encode='onehot-dense', strategy='quantile').fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.splt_dense->SplineTransformer(degree=degree, knots=knots, extrapolation=extrapolation, include_bias=include_bias, sparse_output=False)
A:sklearn.preprocessing.tests.test_polynomial.splt_sparse->SplineTransformer(degree=degree, knots=knots, extrapolation=extrapolation, include_bias=include_bias, sparse_output=True)
A:sklearn.preprocessing.tests.test_polynomial.X_trans_sparse->SplineTransformer(degree=degree, knots=knots, extrapolation=extrapolation, include_bias=include_bias, sparse_output=True).transform(X)
A:sklearn.preprocessing.tests.test_polynomial.X_trans_dense->SplineTransformer(degree=degree, knots=knots, extrapolation=extrapolation, include_bias=include_bias, sparse_output=False).transform(X)
A:sklearn.preprocessing.tests.test_polynomial.X_min->numpy.amin(X, axis=0)
A:sklearn.preprocessing.tests.test_polynomial.X_max->numpy.amax(X, axis=0)
A:sklearn.preprocessing.tests.test_polynomial.P->numpy.hstack([x1 ** 0 * x2 ** 0, x1 ** 1 * x2 ** 0, x1 ** 0 * x2 ** 1, x1 ** 2 * x2 ** 0, x1 ** 1 * x2 ** 1, x1 ** 0 * x2 ** 2, x1 ** 3 * x2 ** 0, x1 ** 2 * x2 ** 1, x1 ** 1 * x2 ** 2, x1 ** 0 * x2 ** 3])
A:sklearn.preprocessing.tests.test_polynomial.tf->PolynomialFeatures(degree=degree, include_bias=include_bias, interaction_only=interaction_only).fit(X)
A:sklearn.preprocessing.tests.test_polynomial.out->out.toarray().toarray()
A:sklearn.preprocessing.tests.test_polynomial.poly->PolynomialFeatures(degree=0, include_bias=True)
A:sklearn.preprocessing.tests.test_polynomial.X_csc->csc_container(X)
A:sklearn.preprocessing.tests.test_polynomial.est->PolynomialFeatures(deg, interaction_only=interaction_only)
A:sklearn.preprocessing.tests.test_polynomial.Xt_csc->PolynomialFeatures(deg, interaction_only=interaction_only).fit_transform(X_csc.astype(dtype))
A:sklearn.preprocessing.tests.test_polynomial.Xt_dense->PolynomialFeatures(deg, interaction_only=interaction_only).fit_transform(X)
A:sklearn.preprocessing.tests.test_polynomial.X_csr->csr_container(sparse_random(1000, dim, 0.5, random_state=0))
A:sklearn.preprocessing.tests.test_polynomial.Xt_csr->PolynomialFeatures(deg, interaction_only=interaction_only).fit_transform(X_csr)
A:sklearn.preprocessing.tests.test_polynomial.x->csr_container(([1], ([0], [n_features - 1])))
A:sklearn.preprocessing.tests.test_polynomial.combos->sklearn.preprocessing.PolynomialFeatures._combinations(n_features=n_features, min_degree=0, max_degree=max_degree, interaction_only=interaction_only, include_bias=include_bias)
A:sklearn.preprocessing.tests.test_polynomial.data->numpy.arange(1, 5, dtype=np.int64)
A:sklearn.preprocessing.tests.test_polynomial.row->numpy.array([n_samples - 2, n_samples - 2, n_samples - 1, n_samples - 1])
A:sklearn.preprocessing.tests.test_polynomial.col->numpy.array([n_features - 2, n_features - 1, n_features - 2, n_features - 1], dtype=np.int64)
A:sklearn.preprocessing.tests.test_polynomial.pf->PolynomialFeatures(interaction_only=False, include_bias=False, degree=3)
A:sklearn.preprocessing.tests.test_polynomial.num_combinations->PolynomialFeatures(interaction_only=False, include_bias=False, degree=3)._num_combinations(n_features=n_features, min_degree=0, max_degree=degree, interaction_only=pf.interaction_only, include_bias=pf.include_bias)
A:sklearn.preprocessing.tests.test_polynomial.(row_nonzero, col_nonzero)->PolynomialFeatures(interaction_only=False, include_bias=False, degree=3).fit_transform(X).nonzero()
A:sklearn.preprocessing.tests.test_polynomial.max_indptr->_calc_total_nnz(X.indptr, interaction_only, deg)
A:sklearn.preprocessing.tests.test_polynomial.output->output.toarray().toarray()
A:sklearn.preprocessing.tests.test_polynomial.n_features->int(np.iinfo(np.int64).max ** (1 / 3) + 3)
sklearn.preprocessing.tests.test_polynomial.single_feature_degree3()
sklearn.preprocessing.tests.test_polynomial.test_csr_polynomial_expansion_index_overflow(degree,n_features,interaction_only,include_bias,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_csr_polynomial_expansion_index_overflow_non_regression(interaction_only,include_bias,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_csr_polynomial_expansion_too_large_to_index(interaction_only,include_bias,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_csr_polynomial_expansion_windows_fail(csr_container)
sklearn.preprocessing.tests.test_polynomial.test_num_combinations(n_features,min_degree,max_degree,interaction_only,include_bias,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_and_spline_array_order(est)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_feature_names()
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_behaviour_on_zero_degree(sparse_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csc_X(deg,include_bias,interaction_only,dtype,csc_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csr_X(deg,include_bias,interaction_only,dtype,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csr_X_degree_4(include_bias,interaction_only,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csr_X_dim_edges(deg,dim,interaction_only,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csr_X_floats(deg,include_bias,interaction_only,dtype,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_csr_X_zero_row(zero_row_index,deg,interaction_only,csr_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_input_validation(params,err_msg)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_one_feature(single_feature_degree3,degree,include_bias,interaction_only,indices,X_container)
sklearn.preprocessing.tests.test_polynomial.test_polynomial_features_two_features(two_features_degree3,degree,include_bias,interaction_only,indices,X_container)
sklearn.preprocessing.tests.test_polynomial.test_sizeof_LARGEST_INT_t()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_extrapolation(bias,intercept,degree)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_feature_names()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_get_base_knot_positions(knots,n_knots,sample_weight,expected_knots)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_input_validation(params,err_msg)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_integer_knots(extrapolation)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_kbindiscretizer()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_linear_regression(bias,intercept)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_n_features_out(n_knots,include_bias,degree,extrapolation,sparse_output)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_periodic_linear_regression(bias,intercept)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_periodic_spline_backport()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_periodic_splines_periodicity()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_periodic_splines_smoothness(degree)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_sparse_output(degree,knots,extrapolation,include_bias,global_random_seed)
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_sparse_output_raise_error_for_old_scipy()
sklearn.preprocessing.tests.test_polynomial.test_spline_transformer_unity_decomposition(degree,n_knots,knots,extrapolation)
sklearn.preprocessing.tests.test_polynomial.test_split_transform_feature_names_extrapolation_degree(extrapolation,degree)
sklearn.preprocessing.tests.test_polynomial.two_features_degree3()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_function_transformer.py----------------------------------------
A:sklearn.preprocessing.tests.test_function_transformer.X->numpy.ones((3, 2))
A:sklearn.preprocessing.tests.test_function_transformer.transformed->FunctionTransformer(_make_func(args_store, kwargs_store)).transform(X)
A:sklearn.preprocessing.tests.test_function_transformer.F->FunctionTransformer(func=np.sqrt, inverse_func=np.around, inv_kw_args=dict(decimals=3))
A:sklearn.preprocessing.tests.test_function_transformer.F.kw_args->dict(decimals=1)
A:sklearn.preprocessing.tests.test_function_transformer.trans->FunctionTransformer(func=add_constant_feature, inverse_func=inverse_add_constant, validate=True)
A:sklearn.preprocessing.tests.test_function_transformer.Xt->FunctionTransformer(func=add_constant_feature, inverse_func=inverse_add_constant, validate=True).fit_transform(X)
A:sklearn.preprocessing.tests.test_function_transformer.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_function_transformer.X_df->pytest.importorskip('pandas').DataFrame(np.random.randn(100, 10))
A:sklearn.preprocessing.tests.test_function_transformer.transformer->FunctionTransformer(func=func, feature_names_out=feature_names_out)
A:sklearn.preprocessing.tests.test_function_transformer.X_df_trans->FunctionTransformer(func=func, feature_names_out=feature_names_out).fit_transform(X_df)
A:sklearn.preprocessing.tests.test_function_transformer.data->_convert_container(data, X_type, columns_name=['value'], dtype=dtype)
A:sklearn.preprocessing.tests.test_function_transformer.df->pytest.importorskip('pandas').DataFrame({'a': np.random.rand(100), 'b': np.random.rand(100)})
A:sklearn.preprocessing.tests.test_function_transformer.df_out->FunctionTransformer(func=func, feature_names_out=feature_names_out).fit_transform(df)
A:sklearn.preprocessing.tests.test_function_transformer.df_mixed->pytest.importorskip('pandas').DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
A:sklearn.preprocessing.tests.test_function_transformer.names->FunctionTransformer(func=func, feature_names_out=feature_names_out).get_feature_names_out()
A:sklearn.preprocessing.tests.test_function_transformer.X_one->numpy.ones((X.shape[0], 1))
A:sklearn.preprocessing.tests.test_function_transformer.X_trans->FunctionTransformer(lambda x: 2 * x).fit_transform(X)
A:sklearn.preprocessing.tests.test_function_transformer.name->feature_names_out(None, X.columns)
A:sklearn.preprocessing.tests.test_function_transformer.ft->FunctionTransformer(lambda x: 2 * x)
A:sklearn.preprocessing.tests.test_function_transformer.ft_np->FunctionTransformer(lambda x: np.asarray(x))
sklearn.preprocessing.tests.test_function_transformer._make_func(args_store,kwargs_store,func=lambdaX,*a,**k:X)
sklearn.preprocessing.tests.test_function_transformer.test_check_inverse(sparse_container)
sklearn.preprocessing.tests.test_function_transformer.test_check_inverse_func_or_inverse_not_provided()
sklearn.preprocessing.tests.test_function_transformer.test_delegate_to_func()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_feature_names_out_is_None()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_feature_names_out_uses_estimator()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_frame()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_func_output_inconsistent_feature_names_out()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_get_feature_names_out(X,feature_names_out,input_features,expected,validate)
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_get_feature_names_out_without_validation()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_raise_error_with_mixed_dtype(X_type)
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_support_all_nummerical_dataframes_check_inverse_True()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_ufunc_inconsistent_feature_names_out()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_validate_inverse()
sklearn.preprocessing.tests.test_function_transformer.test_function_transformer_with_dataframe_and_check_inverse_True()
sklearn.preprocessing.tests.test_function_transformer.test_get_feature_names_out_dataframe_with_string_data(feature_names_out,expected,in_pipeline)
sklearn.preprocessing.tests.test_function_transformer.test_inverse_transform()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg_reset()
sklearn.preprocessing.tests.test_function_transformer.test_kw_arg_update()
sklearn.preprocessing.tests.test_function_transformer.test_np_log()
sklearn.preprocessing.tests.test_function_transformer.test_set_output_func()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_common.py----------------------------------------
A:sklearn.preprocessing.tests.test_common.iris->load_iris()
A:sklearn.preprocessing.tests.test_common.rng->numpy.random.RandomState(42)
A:sklearn.preprocessing.tests.test_common.X->load_iris().data.copy()
A:sklearn.preprocessing.tests.test_common.(X_train, X_test)->train_test_split(X, random_state=1)
A:sklearn.preprocessing.tests.test_common.Xt->est.fit(X_train).transform(X_test)
A:sklearn.preprocessing.tests.test_common.Xt_class->est.transform(X_train)
A:sklearn.preprocessing.tests.test_common.kwargs->est.get_params()
A:sklearn.preprocessing.tests.test_common._->est.get_params().pop(kwarg)
A:sklearn.preprocessing.tests.test_common.Xt_func->func(X_train, **kwargs)
A:sklearn.preprocessing.tests.test_common.Xt_inv->est.inverse_transform(Xt)
A:sklearn.preprocessing.tests.test_common.Xt_col->est.transform(X_test[:, [i]])
A:sklearn.preprocessing.tests.test_common.Xt_col_nonan->est.transform(_get_valid_samples_by_column(X_test, i))
A:sklearn.preprocessing.tests.test_common.est_dense->clone(est)
A:sklearn.preprocessing.tests.test_common.est_sparse->clone(est)
A:sklearn.preprocessing.tests.test_common.Xt_dense->clone(est).fit(X_train).transform(X_test)
A:sklearn.preprocessing.tests.test_common.Xt_inv_dense->clone(est).inverse_transform(Xt_dense)
A:sklearn.preprocessing.tests.test_common.X_train_sp->sparse_container(X_train)
A:sklearn.preprocessing.tests.test_common.X_test_sp->sparse_container(X_test)
A:sklearn.preprocessing.tests.test_common.Xt_sp->clone(est).fit(X_train_sp).transform(X_test_sp)
A:sklearn.preprocessing.tests.test_common.Xt_inv_sp->clone(est).inverse_transform(Xt_sp)
A:sklearn.preprocessing.tests.test_common.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_common.X_df->pytest.importorskip('pandas').DataFrame(X, dtype='Int16', columns=['a', 'b', 'c'])
A:sklearn.preprocessing.tests.test_common.X_df['c']->X_df['c'].astype('int').astype('int')
A:sklearn.preprocessing.tests.test_common.X_trans->est.fit_transform(X)
A:sklearn.preprocessing.tests.test_common.X_df_trans->est.fit_transform(X_df)
sklearn.preprocessing.tests.test_common._get_valid_samples_by_column(X,col)
sklearn.preprocessing.tests.test_common.test_missing_value_handling(est,func,support_sparse,strictly_positive,omit_kwargs)
sklearn.preprocessing.tests.test_common.test_missing_value_pandas_na_support(est,func)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_target_encoder.py----------------------------------------
A:sklearn.preprocessing.tests.test_target_encoder.cur_encodings->_encode_target(X_, y_, n_categories, smooth)
A:sklearn.preprocessing.tests.test_target_encoder.y_mean->numpy.mean(y_integer)
A:sklearn.preprocessing.tests.test_target_encoder.y_variance->numpy.var(y_numeric)
A:sklearn.preprocessing.tests.test_target_encoder.y_subset_variance->numpy.var(y_subset)
A:sklearn.preprocessing.tests.test_target_encoder.X_test->numpy.array([[1], [0]])
A:sklearn.preprocessing.tests.test_target_encoder.data_rng->numpy.random.RandomState(global_random_seed)
A:sklearn.preprocessing.tests.test_target_encoder.y_numeric->numpy.random.RandomState(global_random_seed).uniform(low=-10, high=20, size=n_samples)
A:sklearn.preprocessing.tests.test_target_encoder.target_names->numpy.array(['cat', 'dog'], dtype=object)
A:sklearn.preprocessing.tests.test_target_encoder.shuffled_idx->numpy.random.RandomState(global_random_seed).permutation(n_samples)
A:sklearn.preprocessing.tests.test_target_encoder.cv->ShuffleSplit(n_splits=50, random_state=global_random_seed)
A:sklearn.preprocessing.tests.test_target_encoder.expected_X_fit_transform->numpy.empty_like(X_ordinal, dtype=np.float64)
A:sklearn.preprocessing.tests.test_target_encoder.target_encoder->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_fit_transform->TargetEncoder(smooth=0.0, shuffle=False, cv=2).fit_transform(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.expected_encodings->_encode_target(X_train_int_array[:, 0], y_numeric, n_categories, smooth)
A:sklearn.preprocessing.tests.test_target_encoder.expected_X_test_transform->numpy.array([[expected_encodings[0][0], expected_encodings[1][1]], [y_mean, expected_encodings[1][0]], [expected_encodings[0][1], y_mean]], dtype=np.float64)
A:sklearn.preprocessing.tests.test_target_encoder.X_test_transform->TargetEncoder(smooth=0.0, shuffle=False, cv=2).transform(X_test)
A:sklearn.preprocessing.tests.test_target_encoder.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.preprocessing.tests.test_target_encoder.feat_1_int->numpy.array(rng.randint(low=0, high=2, size=n_samples))
A:sklearn.preprocessing.tests.test_target_encoder.feat_2_int->numpy.array(rng.randint(low=0, high=3, size=n_samples))
A:sklearn.preprocessing.tests.test_target_encoder.X_train->numpy.random.RandomState(global_random_seed).randint(0, cardinality, size=n_samples).reshape(-1, 1)
A:sklearn.preprocessing.tests.test_target_encoder.X_train_int->numpy.column_stack((feat_1_int, feat_2_int))
A:sklearn.preprocessing.tests.test_target_encoder.y_train_int->numpy.array(rng.randint(low=0, high=n_classes, size=n_samples))
A:sklearn.preprocessing.tests.test_target_encoder.y_train_enc->LabelBinarizer().fit_transform(y_train)
A:sklearn.preprocessing.tests.test_target_encoder.current_encoding->_encode_target(X_ordinal[:, f_idx], y_integer, len(cats), smooth)
A:sklearn.preprocessing.tests.test_target_encoder.X_test_int->numpy.array([[0, 1], [1, 2], [4, 5]])
A:sklearn.preprocessing.tests.test_target_encoder.y->numpy.random.RandomState(global_random_seed).randn(n_samples)
A:sklearn.preprocessing.tests.test_target_encoder.enc->TargetEncoder(smooth=0.0, shuffle=False, cv=2)
A:sklearn.preprocessing.tests.test_target_encoder.X_trans->TargetEncoder(smooth=0.0, shuffle=False, cv=2).fit_transform(X, y)
A:sklearn.preprocessing.tests.test_target_encoder.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_target_encoder.X_df->pytest.importorskip('pandas').DataFrame({'A': ['a', 'b'] * 10, 'B': [1, 2] * 10})
A:sklearn.preprocessing.tests.test_target_encoder.enc_default->TargetEncoder(cv=2, smooth=3.0, random_state=0)
A:sklearn.preprocessing.tests.test_target_encoder.enc_pandas->TargetEncoder(cv=2, smooth=3.0, random_state=0)
A:sklearn.preprocessing.tests.test_target_encoder.X_default->TargetEncoder(cv=2, smooth=3.0, random_state=0).fit_transform(X_df, y)
A:sklearn.preprocessing.tests.test_target_encoder.X_pandas->TargetEncoder(cv=2, smooth=3.0, random_state=0).fit_transform(X_df, y)
A:sklearn.preprocessing.tests.test_target_encoder.X_ordinal->numpy.array([[1, 1], [0, 1], [1, 1], [2, 1], [1, 0], [0, 1], [1, 0], [0, 0]], dtype=np.int64)
A:sklearn.preprocessing.tests.test_target_encoder.y_train->numpy.random.RandomState(global_random_seed).normal(size=n_samples)
A:sklearn.preprocessing.tests.test_target_encoder.y_integer->LabelEncoder().fit_transform(y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_test_trans->TargetEncoder(smooth=0.0, shuffle=False, cv=2).transform(X_test)
A:sklearn.preprocessing.tests.test_target_encoder.y_sorted_indices->numpy.random.RandomState(global_random_seed).normal(size=n_samples).argsort()
A:sklearn.preprocessing.tests.test_target_encoder.X_encoded_train_shuffled->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).fit_transform(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_encoded_train_no_shuffled->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).fit_transform(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.regressor->RandomForestRegressor(n_estimators=10, min_samples_leaf=20, random_state=global_random_seed)
A:sklearn.preprocessing.tests.test_target_encoder.X->numpy.concatenate([X_informative, X_shuffled, X_near_unique_categories], axis=1)
A:sklearn.preprocessing.tests.test_target_encoder.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.preprocessing.tests.test_target_encoder.permutated_labels->numpy.random.RandomState(global_random_seed).permutation(n_categories)
A:sklearn.preprocessing.tests.test_target_encoder.X_train_encoded->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).fit_transform(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_test_encoded->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).transform(X_test)
A:sklearn.preprocessing.tests.test_target_encoder.X_train_permuted_encoded->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).fit_transform(X_train_permuted, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_test_permuted_encoded->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).transform(X_test_permuted)
A:sklearn.preprocessing.tests.test_target_encoder.linear_regression->Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False)
A:sklearn.preprocessing.tests.test_target_encoder.X_informative->KBinsDiscretizer(n_bins=n_categories, encode='ordinal', strategy='uniform', random_state=rng).fit_transform((y + noise).reshape(-1, 1))
A:sklearn.preprocessing.tests.test_target_encoder.X_shuffled->numpy.random.RandomState(global_random_seed).permutation(X_informative)
A:sklearn.preprocessing.tests.test_target_encoder.X_near_unique_categories->numpy.random.RandomState(global_random_seed).choice(int(0.9 * n_samples), size=n_samples, replace=True).reshape(-1, 1)
A:sklearn.preprocessing.tests.test_target_encoder.raw_model->Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False).fit(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.model_with_cv->make_pipeline(TargetEncoder(smooth=smooth, random_state=rng), linear_regression).fit(X_train, y_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_enc_no_cv_train->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).transform(X_train)
A:sklearn.preprocessing.tests.test_target_encoder.X_enc_no_cv_test->TargetEncoder(smooth=smooth, random_state=rng).fit(X_train, y_train).transform(X_test)
A:sklearn.preprocessing.tests.test_target_encoder.model_no_cv->Ridge(alpha=1e-06, solver='lsqr', fit_intercept=False).fit(X_enc_no_cv_train, y_train)
sklearn.preprocessing.tests.test_target_encoder._encode_target(X_ordinal,y_numeric,n_categories,smooth)
sklearn.preprocessing.tests.test_target_encoder.test_constant_target_and_feature(y,y_mean,smooth)
sklearn.preprocessing.tests.test_target_encoder.test_custom_categories(X,categories,smooth)
sklearn.preprocessing.tests.test_target_encoder.test_encoding(categories,unknown_value,global_random_seed,smooth,target_type)
sklearn.preprocessing.tests.test_target_encoder.test_encoding_multiclass(global_random_seed,categories,unknown_values,target_labels,smooth)
sklearn.preprocessing.tests.test_target_encoder.test_errors(y,msg)
sklearn.preprocessing.tests.test_target_encoder.test_feature_names_out_set_output(y,feature_names)
sklearn.preprocessing.tests.test_target_encoder.test_fit_transform_not_associated_with_y_if_ordinal_categorical_is_not(global_random_seed)
sklearn.preprocessing.tests.test_target_encoder.test_invariance_of_encoding_under_label_permutation(smooth,global_random_seed)
sklearn.preprocessing.tests.test_target_encoder.test_multiple_features_quick(to_pandas,smooth,target_type)
sklearn.preprocessing.tests.test_target_encoder.test_smooth_zero()
sklearn.preprocessing.tests.test_target_encoder.test_target_encoding_for_linear_regression(smooth,global_random_seed)
sklearn.preprocessing.tests.test_target_encoder.test_use_regression_target()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/preprocessing/tests/test_data.py----------------------------------------
A:sklearn.preprocessing.tests.test_data.iris->sklearn.datasets.load_iris()
A:sklearn.preprocessing.tests.test_data.rng->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_data.offsets->numpy.random.RandomState(0).uniform(-1000000000000000.0, 1000000000000000.0, size=n_features)
A:sklearn.preprocessing.tests.test_data.scales->numpy.random.RandomState(0).uniform(1000.0, 1000000.0, size=n_features)
A:sklearn.preprocessing.tests.test_data.X_1row->X_2d[0, :].reshape(1, n_features)
A:sklearn.preprocessing.tests.test_data.X_1col->X_2d[:, 0].reshape(n_samples, 1)
A:sklearn.preprocessing.tests.test_data.X_list_1row->X_2d[0, :].reshape(1, n_features).tolist()
A:sklearn.preprocessing.tests.test_data.X_list_1col->X_2d[:, 0].reshape(n_samples, 1).tolist()
A:sklearn.preprocessing.tests.test_data.a->a.toarray().toarray()
A:sklearn.preprocessing.tests.test_data.X->numpy.random.RandomState(0).random_sample((6, 4))
A:sklearn.preprocessing.tests.test_data.y->numpy.ones(X.shape[0])
A:sklearn.preprocessing.tests.test_data.scaler->StandardScaler().fit(X_2d)
A:sklearn.preprocessing.tests.test_data.Xw->_convert_container(Xw, array_constructor)
A:sklearn.preprocessing.tests.test_data.yw->numpy.ones(Xw.shape[0])
A:sklearn.preprocessing.tests.test_data.scaler_w->StandardScaler(with_mean=with_mean)
A:sklearn.preprocessing.tests.test_data.X_scaled->StandardScaler().fit(X_2d).fit(X).transform(X)
A:sklearn.preprocessing.tests.test_data.X_scaled_back->StandardScaler().fit(X_2d).inverse_transform(X_scaled)
A:sklearn.preprocessing.tests.test_data.sample_weight->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.preprocessing.tests.test_data.fit_params->dict(sample_weight=rng.uniform(size=n_samples) * 2)
A:sklearn.preprocessing.tests.test_data.X_array->numpy.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
A:sklearn.preprocessing.tests.test_data.X_scaled_2->scale(X, with_mean=scaler.with_mean)
A:sklearn.preprocessing.tests.test_data.common_mask->numpy.logical_and(scales ** 2 > bounds, representable_diff)
A:sklearn.preprocessing.tests.test_data.X_arr->numpy.array(X_list)
A:sklearn.preprocessing.tests.test_data.x->numpy.array([[1, 1, 1, 0, 1, 0], [1, 1, 1, 0, 1, 0], [0, 8, 0, 1, 0, 0], [1, 4, 1, 1, 0, 0], [0, 1, 0, 0, 1, 0], [0, 4, 0, 1, 0, 1]], dtype=np.int32)
A:sklearn.preprocessing.tests.test_data.x_scaled->scale(x)
A:sklearn.preprocessing.tests.test_data.x_small_scaled->scale(x)
A:sklearn.preprocessing.tests.test_data.x_big->numpy.full(10, 1e+100, dtype=np.float64)
A:sklearn.preprocessing.tests.test_data.x_big_scaled->scale(x_big)
A:sklearn.preprocessing.tests.test_data.x_big_centered->scale(x_big, with_std=False)
A:sklearn.preprocessing.tests.test_data.X_scaled_f64->StandardScaler().fit_transform(X.astype(np.float64))
A:sklearn.preprocessing.tests.test_data.s1->numpy.array([0, 1e-16, 1, 2, 3])
A:sklearn.preprocessing.tests.test_data.s2->_handle_zeros_in_scale(s1, copy=True)
A:sklearn.preprocessing.tests.test_data.scaler_batch->MaxAbsScaler().fit(X)
A:sklearn.preprocessing.tests.test_data.scaler_incr->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch])
A:sklearn.preprocessing.tests.test_data.batch0->slice(0, chunk_size)
A:sklearn.preprocessing.tests.test_data.null_transform->StandardScaler(with_mean=False, with_std=False, copy=True)
A:sklearn.preprocessing.tests.test_data.X_null->StandardScaler(with_mean=False, with_std=False, copy=True).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.X_orig->StandardScaler(with_mean=False, with_std=False, copy=True).inverse_transform(X_null)
A:sklearn.preprocessing.tests.test_data.chunks_copy->X_sofar.copy()
A:sklearn.preprocessing.tests.test_data.scaled_batch->StandardScaler().fit_transform(X_sofar, sample_weight=sample_weight[:i + 1])
A:sklearn.preprocessing.tests.test_data.scaled_incr->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch]).transform(X_sofar)
A:sklearn.preprocessing.tests.test_data.right_input->scaler_incr.partial_fit(X[batch]).partial_fit(X[batch]).inverse_transform(scaled_incr)
A:sklearn.preprocessing.tests.test_data.zero->numpy.zeros(X.shape[1])
A:sklearn.preprocessing.tests.test_data.X_trans->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).fit_transform(X_non_gaussian)
A:sklearn.preprocessing.tests.test_data.X_trans_inv->StandardScaler().fit(X_2d).inverse_transform(X_trans)
A:sklearn.preprocessing.tests.test_data.X_trans_new->StandardScaler().fit(X_2d).transform(X_new)
A:sklearn.preprocessing.tests.test_data.X_1d->X_2d[0, :].reshape(1, n_features).ravel()
A:sklearn.preprocessing.tests.test_data.min_->X_2d[0, :].reshape(1, n_features).ravel().min()
A:sklearn.preprocessing.tests.test_data.max_->X_2d[0, :].reshape(1, n_features).ravel().max()
A:sklearn.preprocessing.tests.test_data.X_sparse->csr_container(X_dense)
A:sklearn.preprocessing.tests.test_data.scaler_sparse->RobustScaler(with_centering=False)
A:sklearn.preprocessing.tests.test_data.X_sparse_scaled->RobustScaler(with_centering=False).transform(X_sparse, copy=True)
A:sklearn.preprocessing.tests.test_data.(X_sparse_scaled_mean, X_sparse_scaled_var)->mean_variance_axis(X_sparse_scaled, 0)
A:sklearn.preprocessing.tests.test_data.X_sparse_scaled_back->RobustScaler(with_centering=False).inverse_transform(X_sparse_scaled)
A:sklearn.preprocessing.tests.test_data.transformer->QuantileTransformer(n_quantiles=10, random_state=42)
A:sklearn.preprocessing.tests.test_data.X_dense->numpy.array([[3.0, 0, 4.0], [1.0, 0.0, 0.0], [2.0, 3.0, 0.0]])
A:sklearn.preprocessing.tests.test_data.transformer_dense->QuantileTransformer(n_quantiles=10).fit(X.toarray())
A:sklearn.preprocessing.tests.test_data.X_trans_dense->QuantileTransformer(n_quantiles=10).fit(X.toarray()).fit_transform(X_dense)
A:sklearn.preprocessing.tests.test_data.transformer_sparse->clone(transformer_dense)
A:sklearn.preprocessing.tests.test_data.X_trans_sparse->StandardScaler().fit(X_2d).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.(X_sparse_scaled_mean, X_sparse_scaled_std)->mean_variance_axis(X_sparse_scaled.astype(float), 0)
A:sklearn.preprocessing.tests.test_data.X_copy->numpy.random.RandomState(0).random_sample((6, 4)).copy()
A:sklearn.preprocessing.tests.test_data.X_sparse_copy->csr_container(X_dense).copy()
A:sklearn.preprocessing.tests.test_data.X_transformed_sparse->sparse_container(scaler.transform(X))
A:sklearn.preprocessing.tests.test_data.X_sparse.data->numpy.zeros(X_sparse.data.shape, dtype=np.float64)
A:sklearn.preprocessing.tests.test_data.scaler_dense->RobustScaler(with_centering=False)
A:sklearn.preprocessing.tests.test_data.single_row->numpy.array([[0.1, 1.0, 2.0, 0.0, -1.0]])
A:sklearn.preprocessing.tests.test_data.row_trans->StandardScaler().fit(X_2d).transform(csr_container(single_row))
A:sklearn.preprocessing.tests.test_data.row_scaled_back->StandardScaler().fit(X_2d).inverse_transform(row_trans)
A:sklearn.preprocessing.tests.test_data.q->numpy.percentile(X_trans, q=(25, 75))
A:sklearn.preprocessing.tests.test_data.X_sparse_tran->QuantileTransformer(n_quantiles=10, random_state=42).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.X_sparse_tran_inv->QuantileTransformer(n_quantiles=10, random_state=42).inverse_transform(X_sparse_tran)
A:sklearn.preprocessing.tests.test_data.X_neg->csc_container(X_neg)
A:sklearn.preprocessing.tests.test_data.X_bad_feat->numpy.transpose([[0, 25, 50, 0, 0, 0, 75, 0, 0, 100], [0, 0, 2.6, 4.1, 0, 0, 2.3, 0, 9.5, 0.1]])
A:sklearn.preprocessing.tests.test_data.X_expected->scale(X_expected)
A:sklearn.preprocessing.tests.test_data.X_data->numpy.array([-1, -1, 1, 0, 0, 0, 1, -1, 1])
A:sklearn.preprocessing.tests.test_data.X_col->numpy.array([0, 0, 1, 1, 1, 1, 1, 1, 1])
A:sklearn.preprocessing.tests.test_data.X_row->numpy.array([0, 4, 0, 1, 2, 3, 4, 5, 6])
A:sklearn.preprocessing.tests.test_data.X_test->numpy.array([[-1, 1, 0], [101, 11, 10]])
A:sklearn.preprocessing.tests.test_data.inf_norm->numpy.max(np.abs(diff))
A:sklearn.preprocessing.tests.test_data.X_trans_a0->quantile_transform(X.T, axis=0, n_quantiles=5)
A:sklearn.preprocessing.tests.test_data.X_trans_a1->quantile_transform(X, axis=1, n_quantiles=5)
A:sklearn.preprocessing.tests.test_data.X_trans_sp->QuantileTransformer(n_quantiles=3, random_state=0).fit_transform(X_sparse)
A:sklearn.preprocessing.tests.test_data.X1->numpy.array([[0, 0.1], [0, 0.5], [1, 0.1]])
A:sklearn.preprocessing.tests.test_data.X_2->numpy.array([[0.0], [BOUNDS_THRESHOLD / 10], [1.5], [2], [3], [3], [4]])
A:sklearn.preprocessing.tests.test_data.qt->QuantileTransformer(n_quantiles=n_quantiles).fit(X)
A:sklearn.preprocessing.tests.test_data.X_csr->csr_container(X[batch])
A:sklearn.preprocessing.tests.test_data.X_csr_scaled->scale(X_csr, with_mean=False, with_std=False, copy=True)
A:sklearn.preprocessing.tests.test_data.X_csc_scaled->scale(X_csr.tocsc(), with_mean=False)
A:sklearn.preprocessing.tests.test_data.(X_csr_scaled_mean, X_csr_scaled_std)->mean_variance_axis(X_csr_scaled, 0)
A:sklearn.preprocessing.tests.test_data.X_with_outliers->numpy.vstack([X, np.ones((100, 1)) * 100, np.ones((100, 1)) * -100])
A:sklearn.preprocessing.tests.test_data.robust_scaler->RobustScaler(quantile_range=quantile_range, unit_variance=True).fit(X_with_outliers)
A:sklearn.preprocessing.tests.test_data.X_trans_sparse_inv->StandardScaler().fit(X_2d).inverse_transform(X_trans_sparse)
A:sklearn.preprocessing.tests.test_data.max_abs->numpy.abs(X_1d).max()
A:sklearn.preprocessing.tests.test_data.scaler_incr_csr->scaler_incr_csr.partial_fit(X_csr).partial_fit(X_csr)
A:sklearn.preprocessing.tests.test_data.scaler_incr_csc->scaler_incr_csc.partial_fit(X_csc).partial_fit(X_csc)
A:sklearn.preprocessing.tests.test_data.X_csc->csr_container(X[batch])
A:sklearn.preprocessing.tests.test_data.row_sums->X_norm_squared.sum(axis=1)
A:sklearn.preprocessing.tests.test_data.row_maxs->abs(X_norm).max(axis=1)
A:sklearn.preprocessing.tests.test_data.X_sparse_unpruned->csr_container(X_dense)
A:sklearn.preprocessing.tests.test_data.X_sparse_pruned->csr_container(X_dense)
A:sklearn.preprocessing.tests.test_data.normalizer->Normalizer(norm='max')
A:sklearn.preprocessing.tests.test_data.X_norm1->toarray(X_norm1)
A:sklearn.preprocessing.tests.test_data.X_norm2->toarray(X_norm2)
A:sklearn.preprocessing.tests.test_data.X_norm->toarray(X_norm)
A:sklearn.preprocessing.tests.test_data.X_all_neg_sparse->csr_container(X_all_neg)
A:sklearn.preprocessing.tests.test_data.rs->numpy.random.RandomState(0)
A:sklearn.preprocessing.tests.test_data.ones->numpy.ones(10)
A:sklearn.preprocessing.tests.test_data.(_, norms)->normalize(X_sparse, norm='max', return_norm=True)
A:sklearn.preprocessing.tests.test_data.X_->numpy.array([[1, 0, 5], [2, 3, -1]])
A:sklearn.preprocessing.tests.test_data.binarizer->Binarizer(threshold=-0.5, copy=True)
A:sklearn.preprocessing.tests.test_data.X_bin->Binarizer(threshold=-0.5, copy=True).transform(X)
A:sklearn.preprocessing.tests.test_data.X_float->numpy.array([[1, 0, 5], [2, 3, -1]], dtype=np.float64)
A:sklearn.preprocessing.tests.test_data.X_fit->numpy.random.RandomState(0).random_sample((5, 4))
A:sklearn.preprocessing.tests.test_data.X_fit_centered->StandardScaler().fit(X_2d).transform(X_fit)
A:sklearn.preprocessing.tests.test_data.K_fit->numpy.dot(X_fit, X_fit.T)
A:sklearn.preprocessing.tests.test_data.centerer->KernelCenterer().fit(X_pairwise)
A:sklearn.preprocessing.tests.test_data.K_fit_centered->numpy.dot(X_fit_centered, X_fit_centered.T)
A:sklearn.preprocessing.tests.test_data.K_fit_centered2->KernelCenterer().fit(X_pairwise).fit_transform(K_fit)
A:sklearn.preprocessing.tests.test_data.X_pred->numpy.random.RandomState(0).random_sample((2, 4))
A:sklearn.preprocessing.tests.test_data.K_pred->numpy.dot(X_pred, X_fit.T)
A:sklearn.preprocessing.tests.test_data.X_pred_centered->StandardScaler().fit(X_2d).transform(X_pred)
A:sklearn.preprocessing.tests.test_data.K_pred_centered->numpy.dot(X_pred_centered, X_fit_centered.T)
A:sklearn.preprocessing.tests.test_data.K_pred_centered2->KernelCenterer().fit(X_pairwise).transform(K_pred)
A:sklearn.preprocessing.tests.test_data.phi_X->phi(X)
A:sklearn.preprocessing.tests.test_data.phi_X_test->phi(X_test)
A:sklearn.preprocessing.tests.test_data.phi_X_center->StandardScaler().fit(X_2d).fit_transform(phi_X)
A:sklearn.preprocessing.tests.test_data.phi_X_test_center->StandardScaler().fit(X_2d).transform(phi_X_test)
A:sklearn.preprocessing.tests.test_data.kernel_centerer->KernelCenterer()
A:sklearn.preprocessing.tests.test_data.y_true->numpy.ones((4,))
A:sklearn.preprocessing.tests.test_data.K->numpy.random.RandomState(0).random_sample((6, 4)).dot(X.T)
A:sklearn.preprocessing.tests.test_data.kcent->KernelCenterer()
A:sklearn.preprocessing.tests.test_data.pipeline->Pipeline([('kernel_centerer', kcent), ('svr', SVR())])
A:sklearn.preprocessing.tests.test_data.y_pred->cross_val_predict(pipeline, K, y_true, cv=2)
A:sklearn.preprocessing.tests.test_data.X_transformed->StandardScaler().fit(X_2d).transform(X_test)
A:sklearn.preprocessing.tests.test_data.X_transformed2->obj.fit_transform(X)
A:sklearn.preprocessing.tests.test_data.pt->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X)
A:sklearn.preprocessing.tests.test_data.X_trans_func->power_transform(X, method='box-cox', standardize=standardize)
A:sklearn.preprocessing.tests.test_data.(X_expected, lambda_expected)->scipy.stats.boxcox(X.flatten())
A:sklearn.preprocessing.tests.test_data.X_trans_class->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.(X_expected, lmbda)->scipy.stats.boxcox(X[:, j].flatten())
A:sklearn.preprocessing.tests.test_data.X_inv->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).inverse_transform(X)
A:sklearn.preprocessing.tests.test_data.pt.lambdas_->numpy.array([1])
A:sklearn.preprocessing.tests.test_data.X_inv_trans->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).inverse_transform(X_trans)
A:sklearn.preprocessing.tests.test_data.X_original->numpy.random.RandomState(0).random_sample((6, 4)).copy()
A:sklearn.preprocessing.tests.test_data.X_1->scipy.sparse.random(5, 1, density=0.8)
A:sklearn.preprocessing.tests.test_data.tr->Transformer().fit(df)
A:sklearn.preprocessing.tests.test_data.names_out->KernelCenterer().fit(X_pairwise).get_feature_names_out()
A:sklearn.preprocessing.tests.test_data.pd->pytest.importorskip('pandas')
A:sklearn.preprocessing.tests.test_data.df->pytest.importorskip('pandas').DataFrame(iris.data, columns=iris.feature_names)
A:sklearn.preprocessing.tests.test_data.names_out_df_default->Transformer().fit(df).get_feature_names_out()
A:sklearn.preprocessing.tests.test_data.names_out_df_valid_in->Transformer().fit(df).get_feature_names_out(iris.feature_names)
A:sklearn.preprocessing.tests.test_data.msg->re.escape('input_features is not equal to feature_names_in_')
A:sklearn.preprocessing.tests.test_data.invalid_names->list('abcd')
A:sklearn.preprocessing.tests.test_data.X_pairwise->linear_kernel(X)
A:sklearn.preprocessing.tests.test_data.Xft->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).fit_transform(X)
A:sklearn.preprocessing.tests.test_data.Xt->PowerTransformer(method='yeo-johnson', standardize=standardize).fit(X).transform(X)
sklearn.preprocessing.tests.test_data._check_dim_1axis(a)
sklearn.preprocessing.tests.test_data._check_identity_scalers_attributes(scaler_1,scaler_2)
sklearn.preprocessing.tests.test_data.assert_correct_incr(i,batch_start,batch_stop,n,chunk_size,n_samples_seen)
sklearn.preprocessing.tests.test_data.check_normalizer(norm,X_norm)
sklearn.preprocessing.tests.test_data.test_add_dummy_feature()
sklearn.preprocessing.tests.test_data.test_add_dummy_feature_sparse(sparse_container)
sklearn.preprocessing.tests.test_data.test_binarizer(constructor)
sklearn.preprocessing.tests.test_data.test_center_kernel()
sklearn.preprocessing.tests.test_data.test_cv_pipeline_precomputed()
sklearn.preprocessing.tests.test_data.test_fit_cold_start()
sklearn.preprocessing.tests.test_data.test_fit_transform()
sklearn.preprocessing.tests.test_data.test_handle_zeros_in_scale()
sklearn.preprocessing.tests.test_data.test_kernel_centerer_feature_names_out()
sklearn.preprocessing.tests.test_data.test_kernelcenterer_non_linear_kernel()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_1d()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_large_negative_value()
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_partial_fit(csr_container)
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_transform_one_row_csr(csr_container)
sklearn.preprocessing.tests.test_data.test_maxabs_scaler_zero_variance_features(sparse_container)
sklearn.preprocessing.tests.test_data.test_min_max_scaler_1d()
sklearn.preprocessing.tests.test_data.test_min_max_scaler_iris()
sklearn.preprocessing.tests.test_data.test_min_max_scaler_zero_variance_features()
sklearn.preprocessing.tests.test_data.test_minmax_scale_axis1()
sklearn.preprocessing.tests.test_data.test_minmax_scaler_clip(feature_range)
sklearn.preprocessing.tests.test_data.test_minmax_scaler_partial_fit()
sklearn.preprocessing.tests.test_data.test_normalize(csr_container)
sklearn.preprocessing.tests.test_data.test_normalizer_l1_l2_max(norm,csr_container)
sklearn.preprocessing.tests.test_data.test_normalizer_l1_l2_max_non_csr(norm,sparse_container)
sklearn.preprocessing.tests.test_data.test_normalizer_max_sign(csr_container)
sklearn.preprocessing.tests.test_data.test_one_to_one_features(Transformer)
sklearn.preprocessing.tests.test_data.test_one_to_one_features_pandas(Transformer)
sklearn.preprocessing.tests.test_data.test_optimization_power_transformer(method,lmbda)
sklearn.preprocessing.tests.test_data.test_partial_fit_sparse_input(sample_weight,sparse_container)
sklearn.preprocessing.tests.test_data.test_power_transformer_1d()
sklearn.preprocessing.tests.test_data.test_power_transformer_2d()
sklearn.preprocessing.tests.test_data.test_power_transformer_box_cox_raise_all_nans_col()
sklearn.preprocessing.tests.test_data.test_power_transformer_boxcox_strictly_positive_exception()
sklearn.preprocessing.tests.test_data.test_power_transformer_constant_feature(standardize)
sklearn.preprocessing.tests.test_data.test_power_transformer_copy_False(method,standardize)
sklearn.preprocessing.tests.test_data.test_power_transformer_copy_True(method,standardize)
sklearn.preprocessing.tests.test_data.test_power_transformer_fit_transform(method,standardize)
sklearn.preprocessing.tests.test_data.test_power_transformer_inverse(method,standardize,X)
sklearn.preprocessing.tests.test_data.test_power_transformer_lambda_one()
sklearn.preprocessing.tests.test_data.test_power_transformer_lambda_zero()
sklearn.preprocessing.tests.test_data.test_power_transformer_nans(method)
sklearn.preprocessing.tests.test_data.test_power_transformer_notfitted(method)
sklearn.preprocessing.tests.test_data.test_power_transformer_shape_exception(method)
sklearn.preprocessing.tests.test_data.test_power_transformer_significantly_non_gaussian()
sklearn.preprocessing.tests.test_data.test_power_transformer_yeojohnson_any_input(X)
sklearn.preprocessing.tests.test_data.test_quantile_transform_and_inverse()
sklearn.preprocessing.tests.test_data.test_quantile_transform_axis1()
sklearn.preprocessing.tests.test_data.test_quantile_transform_bounds(csc_container)
sklearn.preprocessing.tests.test_data.test_quantile_transform_check_error(csc_container)
sklearn.preprocessing.tests.test_data.test_quantile_transform_dense_toy()
sklearn.preprocessing.tests.test_data.test_quantile_transform_iris(csc_container)
sklearn.preprocessing.tests.test_data.test_quantile_transform_nan()
sklearn.preprocessing.tests.test_data.test_quantile_transform_sparse_ignore_zeros(csc_container)
sklearn.preprocessing.tests.test_data.test_quantile_transform_sparse_toy(csc_container)
sklearn.preprocessing.tests.test_data.test_quantile_transform_subsampling()
sklearn.preprocessing.tests.test_data.test_quantile_transformer_sorted_quantiles(array_type)
sklearn.preprocessing.tests.test_data.test_raises_value_error_if_sample_weights_greater_than_1d()
sklearn.preprocessing.tests.test_data.test_robust_scale_1d_array()
sklearn.preprocessing.tests.test_data.test_robust_scale_axis1()
sklearn.preprocessing.tests.test_data.test_robust_scaler_2d_arrays()
sklearn.preprocessing.tests.test_data.test_robust_scaler_attributes(X,with_centering,with_scaling)
sklearn.preprocessing.tests.test_data.test_robust_scaler_col_zero_sparse(csr_container)
sklearn.preprocessing.tests.test_data.test_robust_scaler_equivalence_dense_sparse(density,strictly_signed)
sklearn.preprocessing.tests.test_data.test_robust_scaler_error_sparse()
sklearn.preprocessing.tests.test_data.test_robust_scaler_invalid_range()
sklearn.preprocessing.tests.test_data.test_robust_scaler_iris()
sklearn.preprocessing.tests.test_data.test_robust_scaler_iris_quantiles()
sklearn.preprocessing.tests.test_data.test_robust_scaler_transform_one_row_csr(csr_container)
sklearn.preprocessing.tests.test_data.test_robust_scaler_unit_variance()
sklearn.preprocessing.tests.test_data.test_robust_scaler_zero_variance_features()
sklearn.preprocessing.tests.test_data.test_scale_1d()
sklearn.preprocessing.tests.test_data.test_scale_function_without_centering(csr_container)
sklearn.preprocessing.tests.test_data.test_scale_input_finiteness_validation()
sklearn.preprocessing.tests.test_data.test_scale_sparse_with_mean_raise_exception(sparse_container)
sklearn.preprocessing.tests.test_data.test_scaler_2d_arrays()
sklearn.preprocessing.tests.test_data.test_scaler_array_api_compliance(estimator,check,array_namespace,device,dtype_name)
sklearn.preprocessing.tests.test_data.test_scaler_float16_overflow()
sklearn.preprocessing.tests.test_data.test_scaler_int(sparse_container)
sklearn.preprocessing.tests.test_data.test_scaler_n_samples_seen_with_nan(with_mean,with_std,sparse_container)
sklearn.preprocessing.tests.test_data.test_scaler_return_identity(sparse_container)
sklearn.preprocessing.tests.test_data.test_scaler_without_centering(sample_weight,sparse_container)
sklearn.preprocessing.tests.test_data.test_scaler_without_copy(sparse_container)
sklearn.preprocessing.tests.test_data.test_standard_check_array_of_inverse_transform()
sklearn.preprocessing.tests.test_data.test_standard_scaler_1d()
sklearn.preprocessing.tests.test_data.test_standard_scaler_constant_features(scaler,add_sample_weight,sparse_container,dtype,constant)
sklearn.preprocessing.tests.test_data.test_standard_scaler_dtype(add_sample_weight,sparse_container)
sklearn.preprocessing.tests.test_data.test_standard_scaler_near_constant_features(n_samples,sparse_container,average,dtype)
sklearn.preprocessing.tests.test_data.test_standard_scaler_numerical_stability()
sklearn.preprocessing.tests.test_data.test_standard_scaler_partial_fit()
sklearn.preprocessing.tests.test_data.test_standard_scaler_partial_fit_numerical_stability(sparse_container)
sklearn.preprocessing.tests.test_data.test_standard_scaler_raise_error_for_1d_input()
sklearn.preprocessing.tests.test_data.test_standard_scaler_sample_weight(Xw,X,sample_weight,array_constructor)
sklearn.preprocessing.tests.test_data.test_standard_scaler_sparse_partial_fit_finite_variance(X_2)
sklearn.preprocessing.tests.test_data.test_standard_scaler_trasform_with_partial_fit(sample_weight)
sklearn.preprocessing.tests.test_data.test_yeo_johnson_darwin_example()
sklearn.preprocessing.tests.test_data.toarray(a)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/_stochastic_optimizers.py----------------------------------------
A:sklearn.neural_network._stochastic_optimizers.self.learning_rate->float(learning_rate_init)
A:sklearn.neural_network._stochastic_optimizers.updates->self._get_updates(grads)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer(self,params,learning_rate_init=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer.__init__(self,params,learning_rate_init=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-08)
sklearn.neural_network._stochastic_optimizers.AdamOptimizer._get_updates(self,grads)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer(self,learning_rate_init=0.1)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.__init__(self,learning_rate_init=0.1)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.iteration_ends(self,time_step)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.trigger_stopping(self,msg,verbose)
sklearn.neural_network._stochastic_optimizers.BaseOptimizer.update_params(self,params,grads)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer(self,params,learning_rate_init=0.1,lr_schedule='constant',momentum=0.9,nesterov=True,power_t=0.5)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.__init__(self,params,learning_rate_init=0.1,lr_schedule='constant',momentum=0.9,nesterov=True,power_t=0.5)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer._get_updates(self,grads)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.iteration_ends(self,time_step)
sklearn.neural_network._stochastic_optimizers.SGDOptimizer.trigger_stopping(self,msg,verbose)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py----------------------------------------
A:sklearn.neural_network._multilayer_perceptron.self.coefs_[i]->numpy.reshape(packed_parameters[start:end], shape)
A:sklearn.neural_network._multilayer_perceptron.activations[i + 1]->safe_sparse_dot(activations[i], self.coefs_[i])
A:sklearn.neural_network._multilayer_perceptron.X->self._validate_data(X, accept_sparse=['csr', 'csc'], reset=False)
A:sklearn.neural_network._multilayer_perceptron.activation->safe_sparse_dot(activation, self.coefs_[i])
A:sklearn.neural_network._multilayer_perceptron.coef_grads[layer]->safe_sparse_dot(activations[layer].T, deltas[layer])
A:sklearn.neural_network._multilayer_perceptron.intercept_grads[layer]->numpy.mean(deltas[layer], 0)
A:sklearn.neural_network._multilayer_perceptron.(loss, coef_grads, intercept_grads)->self._backprop(X, y, activations, deltas, coef_grads, intercept_grads)
A:sklearn.neural_network._multilayer_perceptron.grad->_pack(coef_grads, intercept_grads)
A:sklearn.neural_network._multilayer_perceptron.activations->self._forward_pass(activations)
A:sklearn.neural_network._multilayer_perceptron.loss->LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
A:sklearn.neural_network._multilayer_perceptron.s->s.ravel().ravel()
A:sklearn.neural_network._multilayer_perceptron.deltas[i - 1]->safe_sparse_dot(deltas[i], self.coefs_[i].T)
A:sklearn.neural_network._multilayer_perceptron.self.n_layers_->len(layer_units)
A:sklearn.neural_network._multilayer_perceptron.(coef_init, intercept_init)->self._init_coef(layer_units[i], layer_units[i + 1], dtype)
A:sklearn.neural_network._multilayer_perceptron.init_bound->numpy.sqrt(factor / (fan_in + fan_out))
A:sklearn.neural_network._multilayer_perceptron.coef_init->coef_init.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.neural_network._multilayer_perceptron.intercept_init->intercept_init.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.neural_network._multilayer_perceptron.hidden_layer_sizes->list(hidden_layer_sizes)
A:sklearn.neural_network._multilayer_perceptron.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], multi_output=True, y_numeric=True, dtype=(np.float64, np.float32), reset=reset)
A:sklearn.neural_network._multilayer_perceptron.y->column_or_1d(y, warn=True)
A:sklearn.neural_network._multilayer_perceptron.self._random_state->check_random_state(self.random_state)
A:sklearn.neural_network._multilayer_perceptron.weights->chain(self.coefs_, self.intercepts_)
A:sklearn.neural_network._multilayer_perceptron.packed_coef_inter->_pack(self.coefs_, self.intercepts_)
A:sklearn.neural_network._multilayer_perceptron.opt_res->scipy.optimize.minimize(self._loss_grad_lbfgs, packed_coef_inter, method='L-BFGS-B', jac=True, options={'maxfun': self.max_fun, 'maxiter': self.max_iter, 'iprint': iprint, 'gtol': self.tol}, args=(X, y, activations, deltas, coef_grads, intercept_grads))
A:sklearn.neural_network._multilayer_perceptron.self.n_iter_->_check_optimize_result('lbfgs', opt_res, self.max_iter)
A:sklearn.neural_network._multilayer_perceptron.self._optimizer->AdamOptimizer(params, self.learning_rate_init, self.beta_1, self.beta_2, self.epsilon)
A:sklearn.neural_network._multilayer_perceptron.(X, X_val, y, y_val)->train_test_split(X, y, random_state=self._random_state, test_size=self.validation_fraction, stratify=stratify)
A:sklearn.neural_network._multilayer_perceptron.y_val->self._label_binarizer.inverse_transform(y_val)
A:sklearn.neural_network._multilayer_perceptron.sample_idx->shuffle(sample_idx, random_state=self._random_state)
A:sklearn.neural_network._multilayer_perceptron.batch_size->numpy.clip(self.batch_size, 1, n_samples)
A:sklearn.neural_network._multilayer_perceptron.X_batch->_safe_indexing(X, sample_idx[batch_slice])
A:sklearn.neural_network._multilayer_perceptron.(batch_loss, coef_grads, intercept_grads)->self._backprop(X_batch, y_batch, activations, deltas, coef_grads, intercept_grads)
A:sklearn.neural_network._multilayer_perceptron.is_stopping->self._optimizer.trigger_stopping(msg, self.verbose)
A:sklearn.neural_network._multilayer_perceptron.self._label_binarizer->LabelBinarizer()
A:sklearn.neural_network._multilayer_perceptron.classes->unique_labels(y)
A:sklearn.neural_network._multilayer_perceptron.y_pred->self._predict(X, check_input=False)
A:sklearn.neural_network._multilayer_perceptron.y_prob->self.predict_proba(X)
sklearn.neural_network.MLPClassifier(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network.MLPClassifier._more_tags(self)
sklearn.neural_network.MLPClassifier._predict(self,X,check_input=True)
sklearn.neural_network.MLPClassifier._score(self,X,y)
sklearn.neural_network.MLPClassifier._validate_input(self,X,y,incremental,reset)
sklearn.neural_network.MLPClassifier.partial_fit(self,X,y,classes=None)
sklearn.neural_network.MLPClassifier.predict(self,X)
sklearn.neural_network.MLPClassifier.predict_log_proba(self,X)
sklearn.neural_network.MLPClassifier.predict_proba(self,X)
sklearn.neural_network.MLPRegressor(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network.MLPRegressor._predict(self,X,check_input=True)
sklearn.neural_network.MLPRegressor._score(self,X,y)
sklearn.neural_network.MLPRegressor._validate_input(self,X,y,incremental,reset)
sklearn.neural_network.MLPRegressor.partial_fit(self,X,y)
sklearn.neural_network.MLPRegressor.predict(self,X)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron(self,hidden_layer_sizes,activation,solver,alpha,batch_size,learning_rate,learning_rate_init,power_t,max_iter,loss,shuffle,random_state,tol,verbose,warm_start,momentum,nesterovs_momentum,early_stopping,validation_fraction,beta_1,beta_2,epsilon,n_iter_no_change,max_fun)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron.__init__(self,hidden_layer_sizes,activation,solver,alpha,batch_size,learning_rate,learning_rate_init,power_t,max_iter,loss,shuffle,random_state,tol,verbose,warm_start,momentum,nesterovs_momentum,early_stopping,validation_fraction,beta_1,beta_2,epsilon,n_iter_no_change,max_fun)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._backprop(self,X,y,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._check_solver(self)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._compute_loss_grad(self,layer,n_samples,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._fit(self,X,y,incremental=False)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._fit_lbfgs(self,X,y,activations,deltas,coef_grads,intercept_grads,layer_units)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._fit_stochastic(self,X,y,activations,deltas,coef_grads,intercept_grads,layer_units,incremental)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._forward_pass(self,activations)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._forward_pass_fast(self,X,check_input=True)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._init_coef(self,fan_in,fan_out,dtype)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._initialize(self,y,layer_units,dtype)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._loss_grad_lbfgs(self,packed_coef_inter,X,y,activations,deltas,coef_grads,intercept_grads)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._unpack(self,packed_parameters)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron._update_no_improvement_count(self,early_stopping,X_val,y_val)
sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron.fit(self,X,y)
sklearn.neural_network._multilayer_perceptron.MLPClassifier(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network._multilayer_perceptron.MLPClassifier.__init__(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network._multilayer_perceptron.MLPClassifier._more_tags(self)
sklearn.neural_network._multilayer_perceptron.MLPClassifier._predict(self,X,check_input=True)
sklearn.neural_network._multilayer_perceptron.MLPClassifier._score(self,X,y)
sklearn.neural_network._multilayer_perceptron.MLPClassifier._validate_input(self,X,y,incremental,reset)
sklearn.neural_network._multilayer_perceptron.MLPClassifier.partial_fit(self,X,y,classes=None)
sklearn.neural_network._multilayer_perceptron.MLPClassifier.predict(self,X)
sklearn.neural_network._multilayer_perceptron.MLPClassifier.predict_log_proba(self,X)
sklearn.neural_network._multilayer_perceptron.MLPClassifier.predict_proba(self,X)
sklearn.neural_network._multilayer_perceptron.MLPRegressor(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network._multilayer_perceptron.MLPRegressor.__init__(self,hidden_layer_sizes=(100,),activation='relu',*,solver='adam',alpha=0.0001,batch_size='auto',learning_rate='constant',learning_rate_init=0.001,power_t=0.5,max_iter=200,shuffle=True,random_state=None,tol=0.0001,verbose=False,warm_start=False,momentum=0.9,nesterovs_momentum=True,early_stopping=False,validation_fraction=0.1,beta_1=0.9,beta_2=0.999,epsilon=1e-08,n_iter_no_change=10,max_fun=15000)
sklearn.neural_network._multilayer_perceptron.MLPRegressor._predict(self,X,check_input=True)
sklearn.neural_network._multilayer_perceptron.MLPRegressor._score(self,X,y)
sklearn.neural_network._multilayer_perceptron.MLPRegressor._validate_input(self,X,y,incremental,reset)
sklearn.neural_network._multilayer_perceptron.MLPRegressor.partial_fit(self,X,y)
sklearn.neural_network._multilayer_perceptron.MLPRegressor.predict(self,X)
sklearn.neural_network._multilayer_perceptron._pack(coefs_,intercepts_)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/_rbm.py----------------------------------------
A:sklearn.neural_network._rbm.X->self._validate_data(X, accept_sparse='csr', dtype=(np.float64, np.float32))
A:sklearn.neural_network._rbm.p->numpy.dot(h, self.components_)
A:sklearn.neural_network._rbm.self.random_state_->check_random_state(self.random_state)
A:sklearn.neural_network._rbm.h_->self._sample_hiddens(v, self.random_state_)
A:sklearn.neural_network._rbm.v_->self._validate_data(X, accept_sparse='csr', reset=False).copy()
A:sklearn.neural_network._rbm.self.components_->numpy.asarray(rng.normal(0, 0.01, (self.n_components, X.shape[1])), order='F', dtype=X.dtype)
A:sklearn.neural_network._rbm.self.intercept_hidden_->numpy.zeros(self.n_components, dtype=X.dtype)
A:sklearn.neural_network._rbm.self.intercept_visible_->numpy.zeros(X.shape[1], dtype=X.dtype)
A:sklearn.neural_network._rbm.self.h_samples_->numpy.zeros((self.batch_size, self.n_components), dtype=X.dtype)
A:sklearn.neural_network._rbm.h_pos->self._mean_hiddens(v_pos)
A:sklearn.neural_network._rbm.v_neg->self._sample_visibles(self.h_samples_, rng)
A:sklearn.neural_network._rbm.h_neg->self._mean_hiddens(v_neg)
A:sklearn.neural_network._rbm.v->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.neural_network._rbm.rng->check_random_state(self.random_state)
A:sklearn.neural_network._rbm.fe->self._free_energy(v)
A:sklearn.neural_network._rbm.fe_->self._free_energy(v_)
A:sklearn.neural_network._rbm.n_batches->int(np.ceil(float(n_samples) / self.batch_size))
A:sklearn.neural_network._rbm.batch_slices->list(gen_even_slices(n_batches * self.batch_size, n_batches, n_samples=n_samples))
A:sklearn.neural_network._rbm.begin->time.time()
A:sklearn.neural_network._rbm.end->time.time()
sklearn.neural_network.BernoulliRBM(self,n_components=256,*,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network.BernoulliRBM._fit(self,v_pos,rng)
sklearn.neural_network.BernoulliRBM._free_energy(self,v)
sklearn.neural_network.BernoulliRBM._mean_hiddens(self,v)
sklearn.neural_network.BernoulliRBM._more_tags(self)
sklearn.neural_network.BernoulliRBM._sample_hiddens(self,v,rng)
sklearn.neural_network.BernoulliRBM._sample_visibles(self,h,rng)
sklearn.neural_network.BernoulliRBM.fit(self,X,y=None)
sklearn.neural_network.BernoulliRBM.gibbs(self,v)
sklearn.neural_network.BernoulliRBM.partial_fit(self,X,y=None)
sklearn.neural_network.BernoulliRBM.score_samples(self,X)
sklearn.neural_network.BernoulliRBM.transform(self,X)
sklearn.neural_network._rbm.BernoulliRBM(self,n_components=256,*,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network._rbm.BernoulliRBM.__init__(self,n_components=256,*,learning_rate=0.1,batch_size=10,n_iter=10,verbose=0,random_state=None)
sklearn.neural_network._rbm.BernoulliRBM._fit(self,v_pos,rng)
sklearn.neural_network._rbm.BernoulliRBM._free_energy(self,v)
sklearn.neural_network._rbm.BernoulliRBM._mean_hiddens(self,v)
sklearn.neural_network._rbm.BernoulliRBM._more_tags(self)
sklearn.neural_network._rbm.BernoulliRBM._sample_hiddens(self,v,rng)
sklearn.neural_network._rbm.BernoulliRBM._sample_visibles(self,h,rng)
sklearn.neural_network._rbm.BernoulliRBM.fit(self,X,y=None)
sklearn.neural_network._rbm.BernoulliRBM.gibbs(self,v)
sklearn.neural_network._rbm.BernoulliRBM.partial_fit(self,X,y=None)
sklearn.neural_network._rbm.BernoulliRBM.score_samples(self,X)
sklearn.neural_network._rbm.BernoulliRBM.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/_base.py----------------------------------------
A:sklearn.neural_network._base.y_prob->numpy.clip(y_prob, eps, 1 - eps)
A:sklearn.neural_network._base.y_true->numpy.append(1 - y_true, y_true, axis=1)
sklearn.neural_network._base.binary_log_loss(y_true,y_prob)
sklearn.neural_network._base.inplace_identity(X)
sklearn.neural_network._base.inplace_identity_derivative(Z,delta)
sklearn.neural_network._base.inplace_logistic(X)
sklearn.neural_network._base.inplace_logistic_derivative(Z,delta)
sklearn.neural_network._base.inplace_relu(X)
sklearn.neural_network._base.inplace_relu_derivative(Z,delta)
sklearn.neural_network._base.inplace_softmax(X)
sklearn.neural_network._base.inplace_tanh(X)
sklearn.neural_network._base.inplace_tanh_derivative(Z,delta)
sklearn.neural_network._base.log_loss(y_true,y_prob)
sklearn.neural_network._base.squared_loss(y_true,y_pred)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/tests/test_base.py----------------------------------------
A:sklearn.neural_network.tests.test_base.loss->log_loss(y_true, y_prob)
sklearn.neural_network.tests.test_base.test_binary_log_loss_1_prob_finite()
sklearn.neural_network.tests.test_base.test_log_loss_1_prob_finite(y_true,y_prob)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/tests/test_stochastic_optimizers.py----------------------------------------
A:sklearn.neural_network.tests.test_stochastic_optimizers.optimizer->AdamOptimizer(params, lr, beta_1, beta_2, epsilon)
A:sklearn.neural_network.tests.test_stochastic_optimizers.rng->numpy.random.RandomState(0)
sklearn.neural_network.tests.test_stochastic_optimizers.test_adam_optimizer()
sklearn.neural_network.tests.test_stochastic_optimizers.test_base_optimizer()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_nesterovs_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_no_momentum()
sklearn.neural_network.tests.test_stochastic_optimizers.test_sgd_optimizer_trigger_stopping()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/tests/test_rbm.py----------------------------------------
A:sklearn.neural_network.tests.test_rbm.(Xdigits, _)->load_digits(return_X_y=True)
A:sklearn.neural_network.tests.test_rbm.X->Xdigits[:100].astype(dtype_in)
A:sklearn.neural_network.tests.test_rbm.rbm->BernoulliRBM(n_components=n_components)
A:sklearn.neural_network.tests.test_rbm.n_batches->int(np.ceil(float(n_samples) / rbm.batch_size))
A:sklearn.neural_network.tests.test_rbm.batch_slices->numpy.array_split(X, n_batches)
A:sklearn.neural_network.tests.test_rbm.rbm1->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)
A:sklearn.neural_network.tests.test_rbm.Xt1->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).transform(X)
A:sklearn.neural_network.tests.test_rbm.Xt2->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)._mean_hiddens(X)
A:sklearn.neural_network.tests.test_rbm.X_sparse->sparse_container(Xdigits[:100])
A:sklearn.neural_network.tests.test_rbm.rbm2->BernoulliRBM(n_components=2, batch_size=2, n_iter=42, random_state=rng)
A:sklearn.neural_network.tests.test_rbm.rng->numpy.random.RandomState(42)
A:sklearn.neural_network.tests.test_rbm.h->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng)._mean_hiddens(X[0])
A:sklearn.neural_network.tests.test_rbm.hs->numpy.mean([rbm1._sample_hiddens(X[0], rng) for i in range(100)], 0)
A:sklearn.neural_network.tests.test_rbm.X_sampled->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).gibbs(X)
A:sklearn.neural_network.tests.test_rbm.X_sampled2->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).gibbs(X)
A:sklearn.neural_network.tests.test_rbm.d_score->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).score_samples(X)
A:sklearn.neural_network.tests.test_rbm.s_score->BernoulliRBM(n_components=10, batch_size=2, n_iter=10, random_state=rng).score_samples(lil_containers(X))
A:sklearn.neural_network.tests.test_rbm.sys.stdout->StringIO()
A:sklearn.neural_network.tests.test_rbm.s->sys.stdout.getvalue()
A:sklearn.neural_network.tests.test_rbm.Xt->BernoulliRBM(n_components=n_components).fit_transform(X)
A:sklearn.neural_network.tests.test_rbm.X_64->Xdigits[:100].astype(np.float64)
A:sklearn.neural_network.tests.test_rbm.rbm_64->BernoulliRBM(n_components=16, batch_size=5, n_iter=5, random_state=42)
A:sklearn.neural_network.tests.test_rbm.Xt_64->BernoulliRBM(n_components=16, batch_size=5, n_iter=5, random_state=42).fit_transform(X_64)
A:sklearn.neural_network.tests.test_rbm.X_32->Xdigits[:100].astype(np.float32)
A:sklearn.neural_network.tests.test_rbm.rbm_32->BernoulliRBM(n_components=16, batch_size=5, n_iter=5, random_state=42)
A:sklearn.neural_network.tests.test_rbm.Xt_32->BernoulliRBM(n_components=16, batch_size=5, n_iter=5, random_state=42).fit_transform(X_32)
A:sklearn.neural_network.tests.test_rbm.names->BernoulliRBM(n_components=n_components).get_feature_names_out()
sklearn.neural_network.tests.test_rbm.test_convergence_dtype_consistency()
sklearn.neural_network.tests.test_rbm.test_feature_names_out(method)
sklearn.neural_network.tests.test_rbm.test_fit()
sklearn.neural_network.tests.test_rbm.test_fit_gibbs(csc_container)
sklearn.neural_network.tests.test_rbm.test_gibbs_smoke()
sklearn.neural_network.tests.test_rbm.test_partial_fit()
sklearn.neural_network.tests.test_rbm.test_rbm_verbose()
sklearn.neural_network.tests.test_rbm.test_sample_hiddens()
sklearn.neural_network.tests.test_rbm.test_score_samples(lil_containers)
sklearn.neural_network.tests.test_rbm.test_small_sparse(csr_container)
sklearn.neural_network.tests.test_rbm.test_small_sparse_partial_fit(sparse_container)
sklearn.neural_network.tests.test_rbm.test_sparse_and_verbose(csc_container)
sklearn.neural_network.tests.test_rbm.test_transform()
sklearn.neural_network.tests.test_rbm.test_transformer_dtypes_casting(dtype_in,dtype_out)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neural_network/tests/test_mlp.py----------------------------------------
A:sklearn.neural_network.tests.test_mlp.(X_digits, y_digits)->load_digits(n_class=2, return_X_y=True)
A:sklearn.neural_network.tests.test_mlp.X_digits_multi->MinMaxScaler().fit_transform(X_digits[:200])
A:sklearn.neural_network.tests.test_mlp.X_digits_binary->MinMaxScaler().fit_transform(X_digits[:200])
A:sklearn.neural_network.tests.test_mlp.(X_reg, y_reg)->make_regression(n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7)
A:sklearn.neural_network.tests.test_mlp.y_reg->scale(y_reg)
A:sklearn.neural_network.tests.test_mlp.iris->load_iris()
A:sklearn.neural_network.tests.test_mlp.alpha_values->numpy.arange(2)
A:sklearn.neural_network.tests.test_mlp.mlp->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris)
A:sklearn.neural_network.tests.test_mlp.X->pytest.importorskip('pandas').DataFrame(data=rng.randn(10, 2), columns=['colname_a', 'colname_b'])
A:sklearn.neural_network.tests.test_mlp.y->pytest.importorskip('pandas').Series(data=np.full(10, 1), name='colname_y')
A:sklearn.neural_network.tests.test_mlp.mlp.coefs_[0]->numpy.array([[0.1, 0.2], [0.3, 0.1], [0.5, 0]])
A:sklearn.neural_network.tests.test_mlp.mlp.coefs_[1]->numpy.array([[0.1], [0.2]])
A:sklearn.neural_network.tests.test_mlp.mlp.intercepts_[0]->numpy.array([0.1, 0.1])
A:sklearn.neural_network.tests.test_mlp.mlp.intercepts_[1]->numpy.array([1.0])
A:sklearn.neural_network.tests.test_mlp.random_state->numpy.random.RandomState(seed=42)
A:sklearn.neural_network.tests.test_mlp.Y->LabelBinarizer().fit_transform(y)
A:sklearn.neural_network.tests.test_mlp.theta->numpy.hstack([l.ravel() for l in mlp.coefs_ + mlp.intercepts_])
A:sklearn.neural_network.tests.test_mlp.[value, grad]->loss_grad_fun(theta)
A:sklearn.neural_network.tests.test_mlp.numgrad->numpy.zeros(np.size(theta))
A:sklearn.neural_network.tests.test_mlp.n->numpy.size(theta, 0)
A:sklearn.neural_network.tests.test_mlp.E->numpy.eye(n)
A:sklearn.neural_network.tests.test_mlp.y_predict->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris).predict(X_test)
A:sklearn.neural_network.tests.test_mlp.(X, y)->make_regression(n_samples=50, n_features=5, n_targets=1, random_state=0)
A:sklearn.neural_network.tests.test_mlp.clf->MLPClassifier(tol=tol, max_iter=max_iter, solver='sgd', n_iter_no_change=n_iter_no_change)
A:sklearn.neural_network.tests.test_mlp.pred1->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris).predict(X)
A:sklearn.neural_network.tests.test_mlp.pred2->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris).predict(X_sparse)
A:sklearn.neural_network.tests.test_mlp.score->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris).score(X, y)
A:sklearn.neural_network.tests.test_mlp.rng->numpy.random.RandomState(0)
A:sklearn.neural_network.tests.test_mlp.y_proba->MLPClassifier(tol=tol, max_iter=max_iter, solver='sgd', n_iter_no_change=n_iter_no_change).predict_proba(X)
A:sklearn.neural_network.tests.test_mlp.y_log_proba->MLPClassifier(tol=tol, max_iter=max_iter, solver='sgd', n_iter_no_change=n_iter_no_change).predict_log_proba(X)
A:sklearn.neural_network.tests.test_mlp.proba_max->MLPClassifier(tol=tol, max_iter=max_iter, solver='sgd', n_iter_no_change=n_iter_no_change).predict_proba(X).argmax(axis=1)
A:sklearn.neural_network.tests.test_mlp.proba_log_max->MLPClassifier(tol=tol, max_iter=max_iter, solver='sgd', n_iter_no_change=n_iter_no_change).predict_log_proba(X).argmax(axis=1)
A:sklearn.neural_network.tests.test_mlp.(X, Y)->make_multilabel_classification(n_samples=50, random_state=0, return_indicator=True)
A:sklearn.neural_network.tests.test_mlp.mlp1->MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1, random_state=0, shuffle=True)
A:sklearn.neural_network.tests.test_mlp.mlp2->MLPRegressor(hidden_layer_sizes=1, max_iter=1, batch_size=1, random_state=0, shuffle=False)
A:sklearn.neural_network.tests.test_mlp.X_sparse->csr_container(X)
A:sklearn.neural_network.tests.test_mlp.sys.stdoutoutput->StringIO()
A:sklearn.neural_network.tests.test_mlp.mlp_estimator->MLPEstimator(tol=tol, max_iter=3000, solver='sgd', early_stopping=False)
A:sklearn.neural_network.tests.test_mlp.y_2classes->numpy.array([0] * 75 + [1] * 75)
A:sklearn.neural_network.tests.test_mlp.y_3classes->numpy.array([0] * 40 + [1] * 40 + [2] * 70)
A:sklearn.neural_network.tests.test_mlp.y_3classes_alt->numpy.array([0] * 50 + [1] * 50 + [3] * 50)
A:sklearn.neural_network.tests.test_mlp.y_4classes->numpy.array([0] * 37 + [1] * 37 + [2] * 38 + [3] * 38)
A:sklearn.neural_network.tests.test_mlp.y_5classes->numpy.array([0] * 30 + [1] * 30 + [2] * 30 + [3] * 30 + [4] * 30)
A:sklearn.neural_network.tests.test_mlp.mlp_64->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50)
A:sklearn.neural_network.tests.test_mlp.pred_64->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50).predict(X_digits[300:])
A:sklearn.neural_network.tests.test_mlp.proba_64->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50).predict_proba(X_digits[300:])
A:sklearn.neural_network.tests.test_mlp.mlp_32->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50)
A:sklearn.neural_network.tests.test_mlp.pred_32->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50).predict(X_digits[300:].astype(np.float32))
A:sklearn.neural_network.tests.test_mlp.proba_32->MLPRegressor(alpha=1e-05, hidden_layer_sizes=(5, 3), random_state=1, max_iter=50).predict_proba(X_digits[300:].astype(np.float32))
A:sklearn.neural_network.tests.test_mlp.pred->MLPEstimator(early_stopping=True, random_state=0).fit(X_iris, y_iris).predict(X[300:])
A:sklearn.neural_network.tests.test_mlp.pre_trained_estimator->MLPRegressor(hidden_layer_sizes=(42,), random_state=42, learning_rate_init=0.01, max_iter=200)
A:sklearn.neural_network.tests.test_mlp.load_estimator->joblib.load(pickled_file)
A:sklearn.neural_network.tests.test_mlp.predicted_value->joblib.load(pickled_file).predict(fine_tune_features)
A:sklearn.neural_network.tests.test_mlp.pd->pytest.importorskip('pandas')
A:sklearn.neural_network.tests.test_mlp.model->MLPEstimator(solver=solver, warm_start=True, early_stopping=False, max_iter=10, n_iter_no_change=np.inf, random_state=0)
A:sklearn.neural_network.tests.test_mlp.n_validation_scores->len(mlp.validation_scores_)
sklearn.neural_network.tests.test_mlp.test_adaptive_learning_rate()
sklearn.neural_network.tests.test_mlp.test_alpha()
sklearn.neural_network.tests.test_mlp.test_early_stopping(MLPEstimator)
sklearn.neural_network.tests.test_mlp.test_early_stopping_stratified()
sklearn.neural_network.tests.test_mlp.test_fit()
sklearn.neural_network.tests.test_mlp.test_gradient()
sklearn.neural_network.tests.test_mlp.test_lbfgs_classification(X,y)
sklearn.neural_network.tests.test_mlp.test_lbfgs_classification_maxfun(X,y)
sklearn.neural_network.tests.test_mlp.test_lbfgs_regression(X,y)
sklearn.neural_network.tests.test_mlp.test_lbfgs_regression_maxfun(X,y)
sklearn.neural_network.tests.test_mlp.test_learning_rate_warmstart()
sklearn.neural_network.tests.test_mlp.test_mlp_classifier_dtypes_casting()
sklearn.neural_network.tests.test_mlp.test_mlp_loading_from_joblib_partial_fit(tmp_path)
sklearn.neural_network.tests.test_mlp.test_mlp_param_dtypes(dtype,Estimator)
sklearn.neural_network.tests.test_mlp.test_mlp_partial_fit_after_fit(MLPEstimator)
sklearn.neural_network.tests.test_mlp.test_mlp_regressor_dtypes_casting()
sklearn.neural_network.tests.test_mlp.test_mlp_warm_start_no_convergence(MLPEstimator,solver)
sklearn.neural_network.tests.test_mlp.test_mlp_warm_start_with_early_stopping(MLPEstimator)
sklearn.neural_network.tests.test_mlp.test_multilabel_classification()
sklearn.neural_network.tests.test_mlp.test_multioutput_regression()
sklearn.neural_network.tests.test_mlp.test_n_iter_no_change()
sklearn.neural_network.tests.test_mlp.test_n_iter_no_change_inf()
sklearn.neural_network.tests.test_mlp.test_nonfinite_params()
sklearn.neural_network.tests.test_mlp.test_partial_fit_classes_error()
sklearn.neural_network.tests.test_mlp.test_partial_fit_classification()
sklearn.neural_network.tests.test_mlp.test_partial_fit_errors()
sklearn.neural_network.tests.test_mlp.test_partial_fit_regression()
sklearn.neural_network.tests.test_mlp.test_partial_fit_unseen_classes()
sklearn.neural_network.tests.test_mlp.test_predict_proba_binary()
sklearn.neural_network.tests.test_mlp.test_predict_proba_multiclass()
sklearn.neural_network.tests.test_mlp.test_predict_proba_multilabel()
sklearn.neural_network.tests.test_mlp.test_preserve_feature_names(Estimator)
sklearn.neural_network.tests.test_mlp.test_shuffle()
sklearn.neural_network.tests.test_mlp.test_sparse_matrices(csr_container)
sklearn.neural_network.tests.test_mlp.test_tolerance()
sklearn.neural_network.tests.test_mlp.test_verbose_sgd()
sklearn.neural_network.tests.test_mlp.test_warm_start()
sklearn.neural_network.tests.test_mlp.test_warm_start_full_iteration(MLPEstimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_variance_threshold.py----------------------------------------
A:sklearn.feature_selection._variance_threshold.X->self._validate_data(X, accept_sparse=('csr', 'csc'), dtype=np.float64, force_all_finite='allow-nan')
A:sklearn.feature_selection._variance_threshold.(_, self.variances_)->mean_variance_axis(X, axis=0)
A:sklearn.feature_selection._variance_threshold.(mins, maxes)->min_max_axis(X, axis=0)
A:sklearn.feature_selection._variance_threshold.self.variances_->numpy.nanmin(compare_arr, axis=0)
A:sklearn.feature_selection._variance_threshold.peak_to_peaks->numpy.ptp(X, axis=0)
A:sklearn.feature_selection._variance_threshold.compare_arr->numpy.array([self.variances_, peak_to_peaks])
sklearn.feature_selection.VarianceThreshold(self,threshold=0.0)
sklearn.feature_selection.VarianceThreshold._get_support_mask(self)
sklearn.feature_selection.VarianceThreshold._more_tags(self)
sklearn.feature_selection.VarianceThreshold.fit(self,X,y=None)
sklearn.feature_selection._variance_threshold.VarianceThreshold(self,threshold=0.0)
sklearn.feature_selection._variance_threshold.VarianceThreshold.__init__(self,threshold=0.0)
sklearn.feature_selection._variance_threshold.VarianceThreshold._get_support_mask(self)
sklearn.feature_selection._variance_threshold.VarianceThreshold._more_tags(self)
sklearn.feature_selection._variance_threshold.VarianceThreshold.fit(self,X,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_sequential.py----------------------------------------
A:sklearn.feature_selection._sequential.tags->self._get_tags()
A:sklearn.feature_selection._sequential.X->self._validate_data(X, accept_sparse='csc', ensure_min_features=2, force_all_finite=not tags.get('allow_nan', True))
A:sklearn.feature_selection._sequential.self.n_features_to_select_->self.support_.sum()
A:sklearn.feature_selection._sequential.cv->check_cv(self.cv, y, classifier=is_classifier(self.estimator))
A:sklearn.feature_selection._sequential.cloned_estimator->clone(self.estimator)
A:sklearn.feature_selection._sequential.current_mask->numpy.zeros(shape=n_features, dtype=bool)
A:sklearn.feature_selection._sequential.(new_feature_idx, new_score)->self._get_best_new_feature_score(cloned_estimator, X, y, cv, current_mask)
A:sklearn.feature_selection._sequential.candidate_feature_indices->numpy.flatnonzero(~current_mask)
A:sklearn.feature_selection._sequential.candidate_mask->numpy.zeros(shape=n_features, dtype=bool).copy()
A:sklearn.feature_selection._sequential.scores[feature_idx]->cross_val_score(estimator, X_new, y, cv=cv, scoring=self.scoring, n_jobs=self.n_jobs).mean()
A:sklearn.feature_selection._sequential.new_feature_idx->max(scores, key=lambda feature_idx: scores[feature_idx])
sklearn.feature_selection.SequentialFeatureSelector(self,estimator,*,n_features_to_select='auto',tol=None,direction='forward',scoring=None,cv=5,n_jobs=None)
sklearn.feature_selection.SequentialFeatureSelector._get_best_new_feature_score(self,estimator,X,y,cv,current_mask)
sklearn.feature_selection.SequentialFeatureSelector._get_support_mask(self)
sklearn.feature_selection.SequentialFeatureSelector._more_tags(self)
sklearn.feature_selection.SequentialFeatureSelector.fit(self,X,y=None)
sklearn.feature_selection._sequential.SequentialFeatureSelector(self,estimator,*,n_features_to_select='auto',tol=None,direction='forward',scoring=None,cv=5,n_jobs=None)
sklearn.feature_selection._sequential.SequentialFeatureSelector.__init__(self,estimator,*,n_features_to_select='auto',tol=None,direction='forward',scoring=None,cv=5,n_jobs=None)
sklearn.feature_selection._sequential.SequentialFeatureSelector._get_best_new_feature_score(self,estimator,X,y,cv,current_mask)
sklearn.feature_selection._sequential.SequentialFeatureSelector._get_support_mask(self)
sklearn.feature_selection._sequential.SequentialFeatureSelector._more_tags(self)
sklearn.feature_selection._sequential.SequentialFeatureSelector.fit(self,X,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_from_model.py----------------------------------------
A:sklearn.feature_selection._from_model.(scale, reference)->_calculate_threshold(estimator, scores, self.threshold).split('*')
A:sklearn.feature_selection._from_model.scale->float(scale.strip())
A:sklearn.feature_selection._from_model.reference->numpy.mean(importances)
A:sklearn.feature_selection._from_model.threshold->_calculate_threshold(estimator, scores, self.threshold)
A:sklearn.feature_selection._from_model.estimator->getattr(self, 'estimator_', self.estimator)
A:sklearn.feature_selection._from_model.max_features->self.max_features(X)
A:sklearn.feature_selection._from_model.scores->_get_feature_importances(estimator=self.estimator_, getter=self.importance_getter, transform_func='norm', norm_order=self.norm_order)
A:sklearn.feature_selection._from_model.mask->numpy.ones_like(scores, dtype=bool)
A:sklearn.feature_selection._from_model.n_features->_num_features(X)
A:sklearn.feature_selection._from_model.self.estimator_->clone(self.estimator)
A:sklearn.feature_selection._from_model.routed_params->process_routing(self, 'partial_fit', **partial_fit_params)
A:sklearn.feature_selection._from_model.router->MetadataRouter(owner=self.__class__.__name__).add(estimator=self.estimator, method_mapping=MethodMapping().add(callee='partial_fit', caller='partial_fit').add(callee='fit', caller='fit'))
sklearn.feature_selection.SelectFromModel(self,estimator,*,threshold=None,prefit=False,norm_order=1,max_features=None,importance_getter='auto')
sklearn.feature_selection.SelectFromModel._check_max_features(self,X)
sklearn.feature_selection.SelectFromModel._get_support_mask(self)
sklearn.feature_selection.SelectFromModel._more_tags(self)
sklearn.feature_selection.SelectFromModel.fit(self,X,y=None,**fit_params)
sklearn.feature_selection.SelectFromModel.get_metadata_routing(self)
sklearn.feature_selection.SelectFromModel.n_features_in_(self)
sklearn.feature_selection.SelectFromModel.partial_fit(self,X,y=None,**partial_fit_params)
sklearn.feature_selection.SelectFromModel.threshold_(self)
sklearn.feature_selection._from_model.SelectFromModel(self,estimator,*,threshold=None,prefit=False,norm_order=1,max_features=None,importance_getter='auto')
sklearn.feature_selection._from_model.SelectFromModel.__init__(self,estimator,*,threshold=None,prefit=False,norm_order=1,max_features=None,importance_getter='auto')
sklearn.feature_selection._from_model.SelectFromModel._check_max_features(self,X)
sklearn.feature_selection._from_model.SelectFromModel._get_support_mask(self)
sklearn.feature_selection._from_model.SelectFromModel._more_tags(self)
sklearn.feature_selection._from_model.SelectFromModel.fit(self,X,y=None,**fit_params)
sklearn.feature_selection._from_model.SelectFromModel.get_metadata_routing(self)
sklearn.feature_selection._from_model.SelectFromModel.n_features_in_(self)
sklearn.feature_selection._from_model.SelectFromModel.partial_fit(self,X,y=None,**partial_fit_params)
sklearn.feature_selection._from_model.SelectFromModel.threshold_(self)
sklearn.feature_selection._from_model._calculate_threshold(estimator,importances,threshold)
sklearn.feature_selection._from_model._estimator_has(attr)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_rfe.py----------------------------------------
A:sklearn.feature_selection._rfe.(X_train, y_train)->_safe_split(estimator, X, y, train)
A:sklearn.feature_selection._rfe.(X_test, y_test)->_safe_split(estimator, X, y, test, train)
A:sklearn.feature_selection._rfe.(X, y)->self._validate_data(X, y, accept_sparse='csr', ensure_min_features=2, force_all_finite=False, multi_output=True)
A:sklearn.feature_selection._rfe.n_features_to_select->max(n_features - argmax_idx * step, self.min_features_to_select)
A:sklearn.feature_selection._rfe.step->int(self.step)
A:sklearn.feature_selection._rfe.support_->numpy.ones(n_features, dtype=bool)
A:sklearn.feature_selection._rfe.ranking_->numpy.ones(n_features, dtype=int)
A:sklearn.feature_selection._rfe.estimator->clone(self.estimator)
A:sklearn.feature_selection._rfe.importances->_get_feature_importances(estimator, self.importance_getter, transform_func='square')
A:sklearn.feature_selection._rfe.ranks->numpy.ravel(ranks)
A:sklearn.feature_selection._rfe.threshold->min(step, np.sum(support_) - n_features_to_select)
A:sklearn.feature_selection._rfe.self.estimator_->clone(self.estimator)
A:sklearn.feature_selection._rfe.self.n_features_->numpy.ones(n_features, dtype=bool).sum()
A:sklearn.feature_selection._rfe.cv->check_cv(self.cv, y, classifier=is_classifier(self.estimator))
A:sklearn.feature_selection._rfe.scorer->check_scoring(self.estimator, scoring=self.scoring)
A:sklearn.feature_selection._rfe.rfe->RFE(estimator=self.estimator, n_features_to_select=n_features_to_select, step=self.step, importance_getter=self.importance_getter, verbose=self.verbose)
A:sklearn.feature_selection._rfe.parallel->Parallel(n_jobs=self.n_jobs)
A:sklearn.feature_selection._rfe.func->delayed(_rfe_single_fit)
A:sklearn.feature_selection._rfe.scores->numpy.array(scores)
A:sklearn.feature_selection._rfe.scores_sum->numpy.sum(scores, axis=0)
A:sklearn.feature_selection._rfe.self.cv_results_['mean_test_score']->numpy.mean(scores_rev, axis=0)
A:sklearn.feature_selection._rfe.self.cv_results_['std_test_score']->numpy.std(scores_rev, axis=0)
sklearn.feature_selection.RFE(self,estimator,*,n_features_to_select=None,step=1,verbose=0,importance_getter='auto')
sklearn.feature_selection.RFE._estimator_type(self)
sklearn.feature_selection.RFE._fit(self,X,y,step_score=None,**fit_params)
sklearn.feature_selection.RFE._get_support_mask(self)
sklearn.feature_selection.RFE._more_tags(self)
sklearn.feature_selection.RFE.classes_(self)
sklearn.feature_selection.RFE.decision_function(self,X)
sklearn.feature_selection.RFE.fit(self,X,y,**fit_params)
sklearn.feature_selection.RFE.predict(self,X)
sklearn.feature_selection.RFE.predict_log_proba(self,X)
sklearn.feature_selection.RFE.predict_proba(self,X)
sklearn.feature_selection.RFE.score(self,X,y,**fit_params)
sklearn.feature_selection.RFECV(self,estimator,*,step=1,min_features_to_select=1,cv=None,scoring=None,verbose=0,n_jobs=None,importance_getter='auto')
sklearn.feature_selection.RFECV.fit(self,X,y,groups=None)
sklearn.feature_selection._rfe.RFE(self,estimator,*,n_features_to_select=None,step=1,verbose=0,importance_getter='auto')
sklearn.feature_selection._rfe.RFE.__init__(self,estimator,*,n_features_to_select=None,step=1,verbose=0,importance_getter='auto')
sklearn.feature_selection._rfe.RFE._estimator_type(self)
sklearn.feature_selection._rfe.RFE._fit(self,X,y,step_score=None,**fit_params)
sklearn.feature_selection._rfe.RFE._get_support_mask(self)
sklearn.feature_selection._rfe.RFE._more_tags(self)
sklearn.feature_selection._rfe.RFE.classes_(self)
sklearn.feature_selection._rfe.RFE.decision_function(self,X)
sklearn.feature_selection._rfe.RFE.fit(self,X,y,**fit_params)
sklearn.feature_selection._rfe.RFE.predict(self,X)
sklearn.feature_selection._rfe.RFE.predict_log_proba(self,X)
sklearn.feature_selection._rfe.RFE.predict_proba(self,X)
sklearn.feature_selection._rfe.RFE.score(self,X,y,**fit_params)
sklearn.feature_selection._rfe.RFECV(self,estimator,*,step=1,min_features_to_select=1,cv=None,scoring=None,verbose=0,n_jobs=None,importance_getter='auto')
sklearn.feature_selection._rfe.RFECV.__init__(self,estimator,*,step=1,min_features_to_select=1,cv=None,scoring=None,verbose=0,n_jobs=None,importance_getter='auto')
sklearn.feature_selection._rfe.RFECV.fit(self,X,y,groups=None)
sklearn.feature_selection._rfe._estimator_has(attr)
sklearn.feature_selection._rfe._rfe_single_fit(rfe,estimator,X,y,train,test,scorer)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_mutual_info.py----------------------------------------
A:sklearn.feature_selection._mutual_info.x->numpy.zeros(X.shape[0])
A:sklearn.feature_selection._mutual_info.y->scale(y, with_mean=False)
A:sklearn.feature_selection._mutual_info.xy->numpy.hstack((x, y))
A:sklearn.feature_selection._mutual_info.nn->NearestNeighbors()
A:sklearn.feature_selection._mutual_info.radius->numpy.empty(n_samples)
A:sklearn.feature_selection._mutual_info.kd->KDTree(c)
A:sklearn.feature_selection._mutual_info.nx->KDTree(c).query_radius(x, radius, count_only=True, return_distance=False)
A:sklearn.feature_selection._mutual_info.ny->KDTree(c).query_radius(y, radius, count_only=True, return_distance=False)
A:sklearn.feature_selection._mutual_info.c->c.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.feature_selection._mutual_info.label_counts->numpy.empty(n_samples)
A:sklearn.feature_selection._mutual_info.k_all->numpy.empty(n_samples)
A:sklearn.feature_selection._mutual_info.count->numpy.sum(mask)
A:sklearn.feature_selection._mutual_info.k->min(n_neighbors, count - 1)
A:sklearn.feature_selection._mutual_info.radius[mask]->numpy.nextafter(r[:, -1], 0)
A:sklearn.feature_selection._mutual_info.n_samples->numpy.sum(mask)
A:sklearn.feature_selection._mutual_info.m_all->numpy.array(m_all)
A:sklearn.feature_selection._mutual_info.columns->range(X.shape[1])
A:sklearn.feature_selection._mutual_info.(X, y)->check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)
A:sklearn.feature_selection._mutual_info.discrete_features->check_array(discrete_features, ensure_2d=False)
A:sklearn.feature_selection._mutual_info.discrete_mask->numpy.zeros(n_features, dtype=bool)
A:sklearn.feature_selection._mutual_info.rng->check_random_state(random_state)
A:sklearn.feature_selection._mutual_info.X->X.astype(np.float64, copy=copy).astype(np.float64, copy=copy)
A:sklearn.feature_selection._mutual_info.X[:, continuous_mask]->scale(X[:, continuous_mask], with_mean=False, copy=False)
A:sklearn.feature_selection._mutual_info.means->numpy.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))
sklearn.feature_selection._mutual_info._compute_mi(x,y,x_discrete,y_discrete,n_neighbors=3)
sklearn.feature_selection._mutual_info._compute_mi_cc(x,y,n_neighbors)
sklearn.feature_selection._mutual_info._compute_mi_cd(c,d,n_neighbors)
sklearn.feature_selection._mutual_info._estimate_mi(X,y,discrete_features='auto',discrete_target=False,n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection._mutual_info._iterate_columns(X,columns=None)
sklearn.feature_selection._mutual_info.mutual_info_classif(X,y,*,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection._mutual_info.mutual_info_regression(X,y,*,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_classif(X,y,*,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)
sklearn.feature_selection.mutual_info_regression(X,y,*,discrete_features='auto',n_neighbors=3,copy=True,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_base.py----------------------------------------
A:sklearn.feature_selection._base.mask->self.get_support()
A:sklearn.feature_selection._base.X->check_array(X, dtype=None)
A:sklearn.feature_selection._base.it->self.inverse_transform(np.diff(X.indptr).reshape(1, -1))
A:sklearn.feature_selection._base.col_nonzeros->self.inverse_transform(np.diff(X.indptr).reshape(1, -1)).ravel()
A:sklearn.feature_selection._base.indptr->numpy.concatenate([[0], np.cumsum(col_nonzeros)])
A:sklearn.feature_selection._base.Xt->numpy.zeros((X.shape[0], support.size), dtype=X.dtype)
A:sklearn.feature_selection._base.support->self.get_support()
A:sklearn.feature_selection._base.input_features->_check_feature_names_in(self, input_features)
A:sklearn.feature_selection._base.getter->attrgetter(getter)
A:sklearn.feature_selection._base.importances->safe_sqr(importances).sum(axis=0)
sklearn.feature_selection.SelectorMixin(TransformerMixin,metaclass=ABCMeta)
sklearn.feature_selection.SelectorMixin._get_support_mask(self)
sklearn.feature_selection.SelectorMixin._transform(self,X)
sklearn.feature_selection.SelectorMixin.get_feature_names_out(self,input_features=None)
sklearn.feature_selection.SelectorMixin.get_support(self,indices=False)
sklearn.feature_selection.SelectorMixin.inverse_transform(self,X)
sklearn.feature_selection.SelectorMixin.transform(self,X)
sklearn.feature_selection._base.SelectorMixin(TransformerMixin,metaclass=ABCMeta)
sklearn.feature_selection._base.SelectorMixin._get_support_mask(self)
sklearn.feature_selection._base.SelectorMixin._transform(self,X)
sklearn.feature_selection._base.SelectorMixin.get_feature_names_out(self,input_features=None)
sklearn.feature_selection._base.SelectorMixin.get_support(self,indices=False)
sklearn.feature_selection._base.SelectorMixin.inverse_transform(self,X)
sklearn.feature_selection._base.SelectorMixin.transform(self,X)
sklearn.feature_selection._base._get_feature_importances(estimator,getter,transform_func=None,norm_order=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py----------------------------------------
A:sklearn.feature_selection._univariate_selection.scores->_clean_nans(self.scores_)
A:sklearn.feature_selection._univariate_selection.n_classes->len(args)
A:sklearn.feature_selection._univariate_selection.n_samples_per_class->numpy.array([a.shape[0] for a in args])
A:sklearn.feature_selection._univariate_selection.n_samples->numpy.sum(n_samples_per_class)
A:sklearn.feature_selection._univariate_selection.ss_alldata->sum((safe_sqr(a).sum(axis=0) for a in args))
A:sklearn.feature_selection._univariate_selection.f->numpy.asarray(f).ravel()
A:sklearn.feature_selection._univariate_selection.prob->scipy.special.fdtrc(dfbn, dfwn, f)
A:sklearn.feature_selection._univariate_selection.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], multi_output=True)
A:sklearn.feature_selection._univariate_selection.f_obs->numpy.asarray(f_obs, dtype=np.float64)
A:sklearn.feature_selection._univariate_selection.k->len(f_obs)
A:sklearn.feature_selection._univariate_selection.chisq->chisq.sum(axis=0).sum(axis=0)
A:sklearn.feature_selection._univariate_selection.X->self._validate_data(X, accept_sparse=['csr', 'csc'])
A:sklearn.feature_selection._univariate_selection.Y->numpy.append(1 - Y, Y, axis=1)
A:sklearn.feature_selection._univariate_selection.observed->observed.toarray().toarray()
A:sklearn.feature_selection._univariate_selection.feature_count->self._validate_data(X, accept_sparse=['csr', 'csc']).sum(axis=0).reshape(1, -1)
A:sklearn.feature_selection._univariate_selection.class_prob->numpy.append(1 - Y, Y, axis=1).mean(axis=0).reshape(1, -1)
A:sklearn.feature_selection._univariate_selection.expected->numpy.dot(class_prob.T, feature_count)
A:sklearn.feature_selection._univariate_selection.X_means->self._validate_data(X, accept_sparse=['csr', 'csc']).mean(axis=0)
A:sklearn.feature_selection._univariate_selection.X_norms->row_norms(X.T)
A:sklearn.feature_selection._univariate_selection.correlation_coefficient->r_regression(X, y, center=center, force_finite=force_finite)
A:sklearn.feature_selection._univariate_selection.nan_mask->numpy.isnan(correlation_coefficient)
A:sklearn.feature_selection._univariate_selection.p_values->scipy.stats.f.sf(f_statistic, 1, deg_of_freedom)
A:sklearn.feature_selection._univariate_selection.mask_inf->numpy.isinf(f_statistic)
A:sklearn.feature_selection._univariate_selection.mask_nan->numpy.isnan(f_statistic)
A:sklearn.feature_selection._univariate_selection.score_func_ret->self.score_func(X, y)
A:sklearn.feature_selection._univariate_selection.self.pvalues_->numpy.asarray(self.pvalues_)
A:sklearn.feature_selection._univariate_selection.self.scores_->numpy.asarray(self.scores_)
A:sklearn.feature_selection._univariate_selection.threshold->numpy.percentile(scores, 100 - self.percentile)
A:sklearn.feature_selection._univariate_selection.max_feats->int(len(scores) * self.percentile / 100)
A:sklearn.feature_selection._univariate_selection.mask->numpy.zeros(scores.shape, dtype=bool)
A:sklearn.feature_selection._univariate_selection.n_features->len(self.pvalues_)
A:sklearn.feature_selection._univariate_selection.sv->numpy.sort(self.pvalues_)
A:sklearn.feature_selection._univariate_selection.selector->self._make_selector()
A:sklearn.feature_selection._univariate_selection.possible_params->self._make_selector()._get_param_names()
sklearn.feature_selection.GenericUnivariateSelect(self,score_func=f_classif,*,mode='percentile',param=1e-05)
sklearn.feature_selection.GenericUnivariateSelect._check_params(self,X,y)
sklearn.feature_selection.GenericUnivariateSelect._get_support_mask(self)
sklearn.feature_selection.GenericUnivariateSelect._make_selector(self)
sklearn.feature_selection.GenericUnivariateSelect._more_tags(self)
sklearn.feature_selection.SelectFdr(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection.SelectFdr._get_support_mask(self)
sklearn.feature_selection.SelectFpr(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection.SelectFpr._get_support_mask(self)
sklearn.feature_selection.SelectFwe(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection.SelectFwe._get_support_mask(self)
sklearn.feature_selection.SelectKBest(self,score_func=f_classif,*,k=10)
sklearn.feature_selection.SelectKBest._check_params(self,X,y)
sklearn.feature_selection.SelectKBest._get_support_mask(self)
sklearn.feature_selection.SelectKBest._more_tags(self)
sklearn.feature_selection.SelectPercentile(self,score_func=f_classif,*,percentile=10)
sklearn.feature_selection.SelectPercentile._get_support_mask(self)
sklearn.feature_selection.SelectPercentile._more_tags(self)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect(self,score_func=f_classif,*,mode='percentile',param=1e-05)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect.__init__(self,score_func=f_classif,*,mode='percentile',param=1e-05)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect._check_params(self,X,y)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect._get_support_mask(self)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect._make_selector(self)
sklearn.feature_selection._univariate_selection.GenericUnivariateSelect._more_tags(self)
sklearn.feature_selection._univariate_selection.SelectFdr(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFdr.__init__(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFdr._get_support_mask(self)
sklearn.feature_selection._univariate_selection.SelectFpr(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFpr.__init__(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFpr._get_support_mask(self)
sklearn.feature_selection._univariate_selection.SelectFwe(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFwe.__init__(self,score_func=f_classif,*,alpha=0.05)
sklearn.feature_selection._univariate_selection.SelectFwe._get_support_mask(self)
sklearn.feature_selection._univariate_selection.SelectKBest(self,score_func=f_classif,*,k=10)
sklearn.feature_selection._univariate_selection.SelectKBest.__init__(self,score_func=f_classif,*,k=10)
sklearn.feature_selection._univariate_selection.SelectKBest._check_params(self,X,y)
sklearn.feature_selection._univariate_selection.SelectKBest._get_support_mask(self)
sklearn.feature_selection._univariate_selection.SelectKBest._more_tags(self)
sklearn.feature_selection._univariate_selection.SelectPercentile(self,score_func=f_classif,*,percentile=10)
sklearn.feature_selection._univariate_selection.SelectPercentile.__init__(self,score_func=f_classif,*,percentile=10)
sklearn.feature_selection._univariate_selection.SelectPercentile._get_support_mask(self)
sklearn.feature_selection._univariate_selection.SelectPercentile._more_tags(self)
sklearn.feature_selection._univariate_selection._BaseFilter(self,score_func)
sklearn.feature_selection._univariate_selection._BaseFilter.__init__(self,score_func)
sklearn.feature_selection._univariate_selection._BaseFilter._check_params(self,X,y)
sklearn.feature_selection._univariate_selection._BaseFilter._more_tags(self)
sklearn.feature_selection._univariate_selection._BaseFilter.fit(self,X,y=None)
sklearn.feature_selection._univariate_selection._chisquare(f_obs,f_exp)
sklearn.feature_selection._univariate_selection._clean_nans(scores)
sklearn.feature_selection._univariate_selection.chi2(X,y)
sklearn.feature_selection._univariate_selection.f_classif(X,y)
sklearn.feature_selection._univariate_selection.f_oneway(*args)
sklearn.feature_selection._univariate_selection.f_regression(X,y,*,center=True,force_finite=True)
sklearn.feature_selection._univariate_selection.r_regression(X,y,*,center=True,force_finite=True)
sklearn.feature_selection.chi2(X,y)
sklearn.feature_selection.f_classif(X,y)
sklearn.feature_selection.f_oneway(*args)
sklearn.feature_selection.f_regression(X,y,*,center=True,force_finite=True)
sklearn.feature_selection.r_regression(X,y,*,center=True,force_finite=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_base.py----------------------------------------
A:sklearn.feature_selection.tests.test_base.X->pytest.importorskip('pandas').DataFrame({'a': pd.Series([1.0, 2.4, 4.5], dtype=np.float32), 'b': pd.Series(['a', 'b', 'a'], dtype='category'), 'c': pd.Series(['j', 'b', 'b'], dtype='category'), 'd': pd.Series([3.0, 2.4, 1.2], dtype=np.float64)})
A:sklearn.feature_selection.tests.test_base.mask->numpy.zeros(self.n_features_in_, dtype=bool)
A:sklearn.feature_selection.tests.test_base.Xt->numpy.arange(0, 20, 2).reshape(2, 5)
A:sklearn.feature_selection.tests.test_base.Xinv->pytest.importorskip('pandas').DataFrame({'a': pd.Series([1.0, 2.4, 4.5], dtype=np.float32), 'b': pd.Series(['a', 'b', 'a'], dtype='category'), 'c': pd.Series(['j', 'b', 'b'], dtype='category'), 'd': pd.Series([3.0, 2.4, 1.2], dtype=np.float64)}).copy()
A:sklearn.feature_selection.tests.test_base.feature_names->list('ABCDEFGHIJ')
A:sklearn.feature_selection.tests.test_base.feature_names_inv->numpy.array(feature_names)
A:sklearn.feature_selection.tests.test_base.sel->StepSelector(step=step).set_output(transform='pandas')
A:sklearn.feature_selection.tests.test_base.Xt_actual->StepSelector(step=step).set_output(transform='pandas').fit(X_sp).transform(X_sp)
A:sklearn.feature_selection.tests.test_base.Xt_actual2->StepSelector(step=step).set_output(transform='pandas').fit_transform(X_sp)
A:sklearn.feature_selection.tests.test_base.names_t_actual->StepSelector(step=step).set_output(transform='pandas').transform([feature_names])
A:sklearn.feature_selection.tests.test_base.X_sp->csc_container(X)
A:sklearn.feature_selection.tests.test_base.Xinv_actual->StepSelector(step=step).set_output(transform='pandas').fit(X_sp).inverse_transform(Xt_sp)
A:sklearn.feature_selection.tests.test_base.names_inv_actual->StepSelector(step=step).set_output(transform='pandas').inverse_transform([feature_names_t])
A:sklearn.feature_selection.tests.test_base.Xt_sp->csc_container(Xt)
A:sklearn.feature_selection.tests.test_base.pd->pytest.importorskip('pandas')
A:sklearn.feature_selection.tests.test_base.output->StepSelector(step=step).set_output(transform='pandas').transform(X)
A:sklearn.feature_selection.tests.test_base.sel0->StepSelector(step=0).set_output(transform='pandas')
A:sklearn.feature_selection.tests.test_base.output0->StepSelector(step=0).set_output(transform='pandas').transform(X)
sklearn.feature_selection.tests.test_base.StepSelector(self,step=2)
sklearn.feature_selection.tests.test_base.StepSelector.__init__(self,step=2)
sklearn.feature_selection.tests.test_base.StepSelector._get_support_mask(self)
sklearn.feature_selection.tests.test_base.StepSelector.fit(self,X,y=None)
sklearn.feature_selection.tests.test_base.test_get_support()
sklearn.feature_selection.tests.test_base.test_inverse_transform_dense()
sklearn.feature_selection.tests.test_base.test_inverse_transform_sparse(csc_container)
sklearn.feature_selection.tests.test_base.test_output_dataframe()
sklearn.feature_selection.tests.test_base.test_transform_dense()
sklearn.feature_selection.tests.test_base.test_transform_sparse(csc_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_mutual_info.py----------------------------------------
A:sklearn.feature_selection.tests.test_mutual_info.x->numpy.hstack((x, 2))
A:sklearn.feature_selection.tests.test_mutual_info.y->numpy.random.RandomState(global_random_seed).randint(100, size=100)
A:sklearn.feature_selection.tests.test_mutual_info.mean->numpy.zeros(4)
A:sklearn.feature_selection.tests.test_mutual_info.cov->numpy.array([[1, 0.5, 2, 1], [0, 1, 0.1, 0.0], [0, 0.1, 1, 0.1], [0, 0.1, 0.1, 1]]).dot(T.T)
A:sklearn.feature_selection.tests.test_mutual_info.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.feature_selection.tests.test_mutual_info.Z->numpy.random.RandomState(global_random_seed).multivariate_normal(mean, cov, size=1000).astype(global_dtype, copy=False)
A:sklearn.feature_selection.tests.test_mutual_info.I_computed->_compute_mi(x, y, x_discrete=True, y_discrete=False, n_neighbors=n_neighbors)
A:sklearn.feature_selection.tests.test_mutual_info.y[mask]->numpy.random.uniform(-1, 1, size=np.sum(mask))
A:sklearn.feature_selection.tests.test_mutual_info.y[~mask]->numpy.random.uniform(0, 2, size=np.sum(~mask))
A:sklearn.feature_selection.tests.test_mutual_info.mi_1->mutual_info(X, y, discrete_features='auto', random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_2->mutual_info(X, y, discrete_features=False, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.X->numpy.random.RandomState(global_random_seed).randint(100, size=(100, 10))
A:sklearn.feature_selection.tests.test_mutual_info.mi->mutual_info_classif(X, y, discrete_features=[2], n_neighbors=3, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.T->numpy.array([[1, 0.5, 2, 1], [0, 1, 0.1, 0.0], [0, 0.1, 1, 0.1], [0, 0.1, 0.1, 1]])
A:sklearn.feature_selection.tests.test_mutual_info.mi_nn->mutual_info_classif(X, y, discrete_features=[2], n_neighbors=n_neighbors, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.X_csr->csr_container(X)
A:sklearn.feature_selection.tests.test_mutual_info.mi_3->mutual_info(X_csr, y, discrete_features='auto', random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_4->mutual_info(X_csr, y, discrete_features=True, random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_5->mutual_info(X, y, discrete_features=[True, False, True], random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.mi_6->mutual_info(X, y, discrete_features=[0, 2], random_state=0)
A:sklearn.feature_selection.tests.test_mutual_info.d->numpy.random.RandomState(global_random_seed).randint(10, size=n)
A:sklearn.feature_selection.tests.test_mutual_info.c->numpy.random.RandomState(global_random_seed).normal(0, 1, size=n)
A:sklearn.feature_selection.tests.test_mutual_info.mi_classif->mutual_info_classif(c[:, None], d, discrete_features=[False], random_state=global_random_seed)
A:sklearn.feature_selection.tests.test_mutual_info.mi_regression->mutual_info_regression(d[:, None], c, discrete_features=[True], random_state=global_random_seed)
A:sklearn.feature_selection.tests.test_mutual_info.X_float->numpy.random.RandomState(global_random_seed).randint(100, size=(100, 10)).astype(np.float64, copy=True)
A:sklearn.feature_selection.tests.test_mutual_info.expected->mutual_info_regression(X_float, y, random_state=global_random_seed)
A:sklearn.feature_selection.tests.test_mutual_info.result->mutual_info_regression(X, y, random_state=global_random_seed)
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cc(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cd(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_cd_unique_label(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_compute_mi_dd()
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_classif_discrete(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_classif_mixed(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_options(global_dtype,csr_container)
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_regression(global_dtype)
sklearn.feature_selection.tests.test_mutual_info.test_mutual_info_regression_X_int_dtype(global_random_seed)
sklearn.feature_selection.tests.test_mutual_info.test_mutual_information_symmetry_classif_regression(correlated,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_variance_threshold.py----------------------------------------
A:sklearn.feature_selection.tests.test_variance_threshold.sel->VarianceThreshold().fit(X)
A:sklearn.feature_selection.tests.test_variance_threshold.X->VarianceThreshold(threshold=0.4).fit_transform(X)
A:sklearn.feature_selection.tests.test_variance_threshold.arr->numpy.array(data, dtype=np.float64)
sklearn.feature_selection.tests.test_variance_threshold.test_variance_nan(sparse_container)
sklearn.feature_selection.tests.test_variance_threshold.test_variance_threshold(sparse_container)
sklearn.feature_selection.tests.test_variance_threshold.test_zero_variance(sparse_container)
sklearn.feature_selection.tests.test_variance_threshold.test_zero_variance_floating_point_error(sparse_container)
sklearn.feature_selection.tests.test_variance_threshold.test_zero_variance_value_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_rfe.py----------------------------------------
A:sklearn.feature_selection.tests.test_rfe.self.coef_->numpy.ones(X.shape[1], dtype=np.float64)
A:sklearn.feature_selection.tests.test_rfe.generator->check_random_state(global_random_seed)
A:sklearn.feature_selection.tests.test_rfe.iris->load_iris()
A:sklearn.feature_selection.tests.test_rfe.clf->RandomForestClassifier(n_estimators=5)
A:sklearn.feature_selection.tests.test_rfe.rfe->RFE(estimator=clf)
A:sklearn.feature_selection.tests.test_rfe.clf_svc->SVC(kernel='linear')
A:sklearn.feature_selection.tests.test_rfe.rfe_svc->RFE(estimator=clf_svc, n_features_to_select=4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.X_sparse->csr_container(X)
A:sklearn.feature_selection.tests.test_rfe.X_r->RFECV(estimator=SVC(kernel='linear')).transform(X)
A:sklearn.feature_selection.tests.test_rfe.clf_sparse->SVC(kernel='linear')
A:sklearn.feature_selection.tests.test_rfe.rfe_sparse->RFE(estimator=clf_sparse, n_features_to_select=4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.X_r_sparse->RFECV(estimator=SVC(kernel='linear'), step=0.2).transform(X_sparse)
A:sklearn.feature_selection.tests.test_rfe.self.svc_->SVC(kernel='linear').fit(X, y)
A:sklearn.feature_selection.tests.test_rfe.(X, y)->make_friedman1(n_samples=50, n_features=10, random_state=0)
A:sklearn.feature_selection.tests.test_rfe.rfe_num->RFE(estimator=clf, n_features_to_select=4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.rfe_perc->RFE(estimator=clf, n_features_to_select=0.4, step=0.1)
A:sklearn.feature_selection.tests.test_rfe.y->numpy.random.randint(2, size=(10, 2))
A:sklearn.feature_selection.tests.test_rfe.rfecv->RFECV(estimator=SVC(kernel='linear'))
A:sklearn.feature_selection.tests.test_rfe.rfecv_sparse->RFECV(estimator=SVC(kernel='linear'), step=0.2)
A:sklearn.feature_selection.tests.test_rfe.scoring->make_scorer(zero_one_loss, greater_is_better=False)
A:sklearn.feature_selection.tests.test_rfe.scorer->get_scorer('accuracy')
A:sklearn.feature_selection.tests.test_rfe.sys.stdout->StringIO()
A:sklearn.feature_selection.tests.test_rfe.score->cross_val_score(rfe, iris.data, iris.target)
A:sklearn.feature_selection.tests.test_rfe.estimator->PLSEstimator(n_components=1)
A:sklearn.feature_selection.tests.test_rfe.selector->ClsRFE(estimator, step=1).fit(X, y)
A:sklearn.feature_selection.tests.test_rfe.sel->ClsRFE(estimator, step=1).fit(X, y).fit(X, y)
A:sklearn.feature_selection.tests.test_rfe.X->numpy.random.normal(size=(10, 3))
A:sklearn.feature_selection.tests.test_rfe.groups->numpy.floor(np.linspace(0, number_groups, len(iris.target)))
A:sklearn.feature_selection.tests.test_rfe.est_groups->RFECV(estimator=RandomForestClassifier(random_state=generator), step=1, scoring='accuracy', cv=GroupKFold(n_splits=2))
A:sklearn.feature_selection.tests.test_rfe.log_estimator->TransformedTargetRegressor(regressor=estimator, func=np.log, inverse_func=np.exp)
A:sklearn.feature_selection.tests.test_rfe.model->Selector(log_estimator, importance_getter=importance_getter)
A:sklearn.feature_selection.tests.test_rfe.pipeline->make_pipeline(StandardScaler(), LogisticRegression())
A:sklearn.feature_selection.tests.test_rfe.(data, y)->load_iris(return_X_y=True)
A:sklearn.feature_selection.tests.test_rfe.sfm->RFE(pipeline, n_features_to_select=2, importance_getter='named_steps.logisticregression.coef_')
A:sklearn.feature_selection.tests.test_rfe.cv_scores->numpy.asarray([rfecv.cv_results_[key] for key in split_keys])
A:sklearn.feature_selection.tests.test_rfe.expected_mean->numpy.mean(cv_scores, axis=0)
A:sklearn.feature_selection.tests.test_rfe.expected_std->numpy.std(cv_scores, axis=0)
A:sklearn.feature_selection.tests.test_rfe.rfe_test->ClsRFE(clf)
A:sklearn.feature_selection.tests.test_rfe.pipe->make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression())
A:sklearn.feature_selection.tests.test_rfe.fs->ClsRFE(estimator=pipe, importance_getter='named_steps.logisticregression.coef_')
sklearn.feature_selection.tests.test_rfe.MockClassifier(self,foo_param=0)
sklearn.feature_selection.tests.test_rfe.MockClassifier.__init__(self,foo_param=0)
sklearn.feature_selection.tests.test_rfe.MockClassifier._more_tags(self)
sklearn.feature_selection.tests.test_rfe.MockClassifier.fit(self,X,y)
sklearn.feature_selection.tests.test_rfe.MockClassifier.get_params(self,deep=True)
sklearn.feature_selection.tests.test_rfe.MockClassifier.predict(self,T)
sklearn.feature_selection.tests.test_rfe.MockClassifier.score(self,X=None,y=None)
sklearn.feature_selection.tests.test_rfe.MockClassifier.set_params(self,**params)
sklearn.feature_selection.tests.test_rfe.test_RFE_fit_score_params()
sklearn.feature_selection.tests.test_rfe.test_multioutput(ClsRFE)
sklearn.feature_selection.tests.test_rfe.test_number_of_subsets_of_features(global_random_seed)
sklearn.feature_selection.tests.test_rfe.test_pipeline_with_nans(ClsRFE)
sklearn.feature_selection.tests.test_rfe.test_rfe(csr_container)
sklearn.feature_selection.tests.test_rfe.test_rfe_allow_nan_inf_in_x(cv)
sklearn.feature_selection.tests.test_rfe.test_rfe_cv_groups()
sklearn.feature_selection.tests.test_rfe.test_rfe_cv_n_jobs(global_random_seed)
sklearn.feature_selection.tests.test_rfe.test_rfe_estimator_tags()
sklearn.feature_selection.tests.test_rfe.test_rfe_features_importance()
sklearn.feature_selection.tests.test_rfe.test_rfe_importance_getter_validation(importance_getter,err_type,Selector)
sklearn.feature_selection.tests.test_rfe.test_rfe_min_step(global_random_seed)
sklearn.feature_selection.tests.test_rfe.test_rfe_mockclassifier()
sklearn.feature_selection.tests.test_rfe.test_rfe_percent_n_features()
sklearn.feature_selection.tests.test_rfe.test_rfe_pls(ClsRFE,PLSEstimator)
sklearn.feature_selection.tests.test_rfe.test_rfe_wrapped_estimator(importance_getter,selector,expected_n_features)
sklearn.feature_selection.tests.test_rfe.test_rfecv(csr_container)
sklearn.feature_selection.tests.test_rfe.test_rfecv_cv_results_size(global_random_seed)
sklearn.feature_selection.tests.test_rfe.test_rfecv_mockclassifier()
sklearn.feature_selection.tests.test_rfe.test_rfecv_std_and_mean(global_random_seed)
sklearn.feature_selection.tests.test_rfe.test_rfecv_verbose_output()
sklearn.feature_selection.tests.test_rfe.test_w_pipeline_2d_coef_()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_sequential.py----------------------------------------
A:sklearn.feature_selection.tests.test_sequential.(X, y)->make_classification(random_state=0)
A:sklearn.feature_selection.tests.test_sequential.sfs->SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits)
A:sklearn.feature_selection.tests.test_sequential.selected_X->SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits).transform(X)
A:sklearn.feature_selection.tests.test_sequential.rng->numpy.random.RandomState(0)
A:sklearn.feature_selection.tests.test_sequential.added_candidates->list(set(range(X.shape[1])) - set(sfs.get_support(indices=True)))
A:sklearn.feature_selection.tests.test_sequential.added_X->numpy.hstack([selected_X, X[:, rng.choice(added_candidates)][:, np.newaxis]])
A:sklearn.feature_selection.tests.test_sequential.removed_candidate->numpy.random.RandomState(0).choice(list(range(sfs.n_features_to_select_)))
A:sklearn.feature_selection.tests.test_sequential.removed_X->numpy.delete(selected_X, removed_candidate, axis=1)
A:sklearn.feature_selection.tests.test_sequential.plain_cv_score->cross_val_score(LinearRegression(), X, y, cv=2).mean()
A:sklearn.feature_selection.tests.test_sequential.sfs_cv_score->cross_val_score(LinearRegression(), selected_X, y, cv=2).mean()
A:sklearn.feature_selection.tests.test_sequential.added_cv_score->cross_val_score(LinearRegression(), added_X, y, cv=2).mean()
A:sklearn.feature_selection.tests.test_sequential.removed_cv_score->cross_val_score(LinearRegression(), removed_X, y, cv=2).mean()
A:sklearn.feature_selection.tests.test_sequential.X->csr_container(X)
A:sklearn.feature_selection.tests.test_sequential.nan_mask->numpy.random.RandomState(0).randint(0, 2, size=(n_samples, n_features), dtype=bool)
A:sklearn.feature_selection.tests.test_sequential.pipe->make_pipeline(StandardScaler(), sfs)
A:sklearn.feature_selection.tests.test_sequential.(X, clusters)->make_blobs(n_features=6)
A:sklearn.feature_selection.tests.test_sequential.lr->LinearRegression()
A:sklearn.feature_selection.tests.test_sequential.initial_score->LinearRegression().fit(X, y).score(X, y)
A:sklearn.feature_selection.tests.test_sequential.Xr->SequentialFeatureSelector(knc, n_features_to_select=5, cv=splits).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_sequential.new_score->LinearRegression().fit(Xr, y).score(Xr, y)
A:sklearn.feature_selection.tests.test_sequential.groups->numpy.zeros_like(y, dtype=int)
A:sklearn.feature_selection.tests.test_sequential.cv->LeaveOneGroupOut()
A:sklearn.feature_selection.tests.test_sequential.splits->LeaveOneGroupOut().split(X, y, groups=groups)
A:sklearn.feature_selection.tests.test_sequential.knc->KNeighborsClassifier(n_neighbors=5)
sklearn.feature_selection.tests.test_sequential.test_backward_neg_tol()
sklearn.feature_selection.tests.test_sequential.test_bad_n_features_to_select()
sklearn.feature_selection.tests.test_sequential.test_cv_generator_support()
sklearn.feature_selection.tests.test_sequential.test_forward_neg_tol_error()
sklearn.feature_selection.tests.test_sequential.test_n_features_to_select(direction,n_features_to_select)
sklearn.feature_selection.tests.test_sequential.test_n_features_to_select_auto(direction)
sklearn.feature_selection.tests.test_sequential.test_n_features_to_select_float(direction,n_features_to_select,expected)
sklearn.feature_selection.tests.test_sequential.test_n_features_to_select_stopping_criterion(direction)
sklearn.feature_selection.tests.test_sequential.test_nan_support()
sklearn.feature_selection.tests.test_sequential.test_no_y_validation_model_fit(y)
sklearn.feature_selection.tests.test_sequential.test_pipeline_support()
sklearn.feature_selection.tests.test_sequential.test_sanity(seed,direction,n_features_to_select,expected_selected_features)
sklearn.feature_selection.tests.test_sequential.test_sparse_support(csr_container)
sklearn.feature_selection.tests.test_sequential.test_unsupervised_model_fit(n_features_to_select)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_from_model.py----------------------------------------
A:sklearn.feature_selection.tests.test_from_model.iris->sklearn.datasets.load_iris()
A:sklearn.feature_selection.tests.test_from_model.rng->numpy.random.RandomState(0)
A:sklearn.feature_selection.tests.test_from_model.clf->NaNTagRandomForest(n_estimators=100, random_state=0)
A:sklearn.feature_selection.tests.test_from_model.model->make_pipeline(SelectFromModel(estimator), estimator).fit(X, y)
A:sklearn.feature_selection.tests.test_from_model.est->RandomForestClassifier(n_estimators=50, random_state=0)
A:sklearn.feature_selection.tests.test_from_model.transformer->SelectFromModel(estimator=est)
A:sklearn.feature_selection.tests.test_from_model.err_msg->re.escape(err_msg)
A:sklearn.feature_selection.tests.test_from_model.X_trans->SelectFromModel(estimator=est).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_from_model.m->Mock(side_effect=max_features)
A:sklearn.feature_selection.tests.test_from_model.self.feature_importances_->numpy.array(self.importances)
A:sklearn.feature_selection.tests.test_from_model.(X, y)->sklearn.datasets.load_iris(as_frame=as_frame, return_X_y=True)
A:sklearn.feature_selection.tests.test_from_model.transformer1->SelectFromModel(estimator=est, max_features=3, threshold=-np.inf)
A:sklearn.feature_selection.tests.test_from_model.transformer2->SelectFromModel(estimator=est, threshold=0.04)
A:sklearn.feature_selection.tests.test_from_model.X_new1->SelectFromModel(estimator=est, max_features=3, threshold=-np.inf).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_from_model.X_new2->SelectFromModel(estimator=est, threshold=0.04).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_from_model.scores1->numpy.abs(transformer1.estimator_.coef_)
A:sklearn.feature_selection.tests.test_from_model.candidate_indices1->numpy.argsort(-scores1, kind='mergesort')
A:sklearn.feature_selection.tests.test_from_model.scores2->numpy.abs(transformer2.estimator_.coef_)
A:sklearn.feature_selection.tests.test_from_model.candidate_indices2->numpy.argsort(-scores2, kind='mergesort')
A:sklearn.feature_selection.tests.test_from_model.feature_importances->numpy.array([4, 4, 4, 4, 3, 3, 3, 2, 2, 1])
A:sklearn.feature_selection.tests.test_from_model.X_new->SelectFromModel(estimator=est).transform(X)
A:sklearn.feature_selection.tests.test_from_model.transformer3->SelectFromModel(estimator=est, max_features=3, threshold=0.04)
A:sklearn.feature_selection.tests.test_from_model.X_new3->SelectFromModel(estimator=est, max_features=3, threshold=0.04).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_from_model.selected_indices->SelectFromModel(estimator=est, max_features=3, threshold=0.04).transform(np.arange(X.shape[1])[np.newaxis, :])
A:sklearn.feature_selection.tests.test_from_model.sample_weight->numpy.ones(y.shape)
A:sklearn.feature_selection.tests.test_from_model.mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.weighted_mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.reweighted_mask->SelectFromModel(estimator=est)._get_support_mask()
A:sklearn.feature_selection.tests.test_from_model.importances->numpy.linalg.norm(est.coef_, axis=0, ord=order)
A:sklearn.feature_selection.tests.test_from_model.X_transform->make_pipeline(SelectFromModel(estimator), estimator).fit(X, y).transform(data)
A:sklearn.feature_selection.tests.test_from_model.estimator->PLSEstimator(n_components=1)
A:sklearn.feature_selection.tests.test_from_model.feature_names->make_pipeline(SelectFromModel(estimator), estimator).fit(X, y).get_feature_names_out()
A:sklearn.feature_selection.tests.test_from_model.nan_data->data.copy()
A:sklearn.feature_selection.tests.test_from_model.allow_nan_est->NaNTag()
A:sklearn.feature_selection.tests.test_from_model.no_nan_est->NoNaNTag()
A:sklearn.feature_selection.tests.test_from_model.selector->SelectFromModel(estimator=SGDClassifier(), max_features=4).partial_fit(X, y, classes=[0, 1, 2])
A:sklearn.feature_selection.tests.test_from_model.all_feature_names->set(X.columns)
A:sklearn.feature_selection.tests.test_from_model.feature_names_out->set(selector.get_feature_names_out())
sklearn.feature_selection.tests.test_from_model.FixedImportanceEstimator(self,importances)
sklearn.feature_selection.tests.test_from_model.FixedImportanceEstimator.__init__(self,importances)
sklearn.feature_selection.tests.test_from_model.FixedImportanceEstimator.fit(self,X,y=None)
sklearn.feature_selection.tests.test_from_model.NaNTag(BaseEstimator)
sklearn.feature_selection.tests.test_from_model.NaNTag._more_tags(self)
sklearn.feature_selection.tests.test_from_model.NaNTagRandomForest(RandomForestClassifier)
sklearn.feature_selection.tests.test_from_model.NaNTagRandomForest._more_tags(self)
sklearn.feature_selection.tests.test_from_model.NoNaNTag(BaseEstimator)
sklearn.feature_selection.tests.test_from_model.NoNaNTag._more_tags(self)
sklearn.feature_selection.tests.test_from_model._pca_importances(pca_estimator)
sklearn.feature_selection.tests.test_from_model.test_2d_coef()
sklearn.feature_selection.tests.test_from_model.test_allow_nan_tag_comes_from_estimator()
sklearn.feature_selection.tests.test_from_model.test_calling_fit_reinitializes()
sklearn.feature_selection.tests.test_from_model.test_coef_default_threshold(estimator)
sklearn.feature_selection.tests.test_from_model.test_estimator_does_not_support_feature_names()
sklearn.feature_selection.tests.test_from_model.test_feature_importances()
sklearn.feature_selection.tests.test_from_model.test_fit_accepts_nan_inf()
sklearn.feature_selection.tests.test_from_model.test_importance_getter(estimator,importance_getter)
sklearn.feature_selection.tests.test_from_model.test_inferred_max_features_callable(max_features)
sklearn.feature_selection.tests.test_from_model.test_inferred_max_features_integer(max_features)
sklearn.feature_selection.tests.test_from_model.test_input_estimator_unchanged()
sklearn.feature_selection.tests.test_from_model.test_invalid_input()
sklearn.feature_selection.tests.test_from_model.test_max_features()
sklearn.feature_selection.tests.test_from_model.test_max_features_array_like(max_features)
sklearn.feature_selection.tests.test_from_model.test_max_features_callable_data(max_features)
sklearn.feature_selection.tests.test_from_model.test_max_features_error(max_features,err_type,err_msg)
sklearn.feature_selection.tests.test_from_model.test_max_features_tiebreak()
sklearn.feature_selection.tests.test_from_model.test_partial_fit()
sklearn.feature_selection.tests.test_from_model.test_partial_fit_validate_feature_names(as_frame)
sklearn.feature_selection.tests.test_from_model.test_partial_fit_validate_max_features(error,err_msg,max_features)
sklearn.feature_selection.tests.test_from_model.test_prefit()
sklearn.feature_selection.tests.test_from_model.test_prefit_get_feature_names_out()
sklearn.feature_selection.tests.test_from_model.test_prefit_max_features()
sklearn.feature_selection.tests.test_from_model.test_sample_weight()
sklearn.feature_selection.tests.test_from_model.test_select_from_model_pls(PLSEstimator)
sklearn.feature_selection.tests.test_from_model.test_threshold_and_max_features()
sklearn.feature_selection.tests.test_from_model.test_threshold_string()
sklearn.feature_selection.tests.test_from_model.test_threshold_without_refitting()
sklearn.feature_selection.tests.test_from_model.test_transform_accepts_nan_inf()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_chi2.py----------------------------------------
A:sklearn.feature_selection.tests.test_chi2.chi2->mkchi2(k=2).fit(Xsp, y)
A:sklearn.feature_selection.tests.test_chi2.Xsp->csr_container(X, dtype=np.float64)
A:sklearn.feature_selection.tests.test_chi2.Xtrans->Xtrans.toarray().toarray()
A:sklearn.feature_selection.tests.test_chi2.Xtrans2->mkchi2(k=2).fit_transform(Xsp, y).toarray()
A:sklearn.feature_selection.tests.test_chi2.Xcoo->coo_container(X)
A:sklearn.feature_selection.tests.test_chi2.(chi, p)->chi2([[1, 0], [0, 0]], [1, 0])
A:sklearn.feature_selection.tests.test_chi2.obs->numpy.array([[2.0, 2.0], [1.0, 1.0]])
A:sklearn.feature_selection.tests.test_chi2.exp->numpy.array([[1.5, 1.5], [1.5, 1.5]])
A:sklearn.feature_selection.tests.test_chi2.(chi_scp, p_scp)->scipy.stats.chisquare(obs, exp)
A:sklearn.feature_selection.tests.test_chi2.(chi_our, p_our)->_chisquare(obs, exp)
sklearn.feature_selection.tests.test_chi2.mkchi2(k)
sklearn.feature_selection.tests.test_chi2.test_chi2(csr_container)
sklearn.feature_selection.tests.test_chi2.test_chi2_coo(coo_container)
sklearn.feature_selection.tests.test_chi2.test_chi2_negative(csr_container)
sklearn.feature_selection.tests.test_chi2.test_chi2_unused_feature()
sklearn.feature_selection.tests.test_chi2.test_chisquare()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_selection/tests/test_feature_select.py----------------------------------------
A:sklearn.feature_selection.tests.test_feature_select.rng->numpy.random.RandomState(0)
A:sklearn.feature_selection.tests.test_feature_select.X1->ignore_warnings(sel.fit_transform)([X], y)
A:sklearn.feature_selection.tests.test_feature_select.(f, pv)->scipy.stats.f_oneway(X1, X2)
A:sklearn.feature_selection.tests.test_feature_select.(f2, pv2)->f_oneway(X1, X2)
A:sklearn.feature_selection.tests.test_feature_select.X->numpy.random.RandomState(0).randn(10, 5)
A:sklearn.feature_selection.tests.test_feature_select.y->numpy.random.RandomState(0).randint(0, 4, size=40)
A:sklearn.feature_selection.tests.test_feature_select.(fint, pint)->f_oneway(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(f, p)->f_oneway(X.astype(float), y)
A:sklearn.feature_selection.tests.test_feature_select.(X, y)->load_iris(return_X_y=True, as_frame=True)
A:sklearn.feature_selection.tests.test_feature_select.(F, pv)->f_classif(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(F_sparse, pv_sparse)->f_regression(csr_container(X), y, center=False)
A:sklearn.feature_selection.tests.test_feature_select.corr_coeffs->r_regression(X, y, center=center)
A:sklearn.feature_selection.tests.test_feature_select.sparse_X->_convert_container(X, 'sparse')
A:sklearn.feature_selection.tests.test_feature_select.sparse_corr_coeffs->r_regression(sparse_X, y, center=center)
A:sklearn.feature_selection.tests.test_feature_select.Z->numpy.hstack((X, y[:, np.newaxis]))
A:sklearn.feature_selection.tests.test_feature_select.correlation_matrix->numpy.corrcoef(Z, rowvar=False)
A:sklearn.feature_selection.tests.test_feature_select.(F1, pv1)->f_regression(X, y)
A:sklearn.feature_selection.tests.test_feature_select.(F2, pv2)->f_regression(X, y.astype(float))
A:sklearn.feature_selection.tests.test_feature_select.Y->numpy.ones(n_samples)
A:sklearn.feature_selection.tests.test_feature_select.(F1, _)->f_regression(X, Y, center=True)
A:sklearn.feature_selection.tests.test_feature_select.(F2, _)->f_regression(X, Y, center=False)
A:sklearn.feature_selection.tests.test_feature_select.corr_coef->r_regression(X, y, force_finite=force_finite)
A:sklearn.feature_selection.tests.test_feature_select.(f_statistic, p_values)->f_regression(X, y, force_finite=force_finite)
A:sklearn.feature_selection.tests.test_feature_select.univariate_filter->SelectKBest(selector, k=3).set_output(transform='pandas')
A:sklearn.feature_selection.tests.test_feature_select.X_r->SelectKBest(selector, k=3).set_output(transform='pandas').fit(X, y).transform(X)
A:sklearn.feature_selection.tests.test_feature_select.X_r2->GenericUnivariateSelect(mutual_info_regression, mode='percentile', param=20).fit(X, y).transform(X)
A:sklearn.feature_selection.tests.test_feature_select.support->SelectKBest(selector, k=3).set_output(transform='pandas').get_support()
A:sklearn.feature_selection.tests.test_feature_select.gtruth->numpy.zeros(10)
A:sklearn.feature_selection.tests.test_feature_select.X_r2inv->SelectKBest(selector, k=3).set_output(transform='pandas').inverse_transform(X_r2)
A:sklearn.feature_selection.tests.test_feature_select.support_mask->safe_mask(X_r2inv, support)
A:sklearn.feature_selection.tests.test_feature_select.X_selected->selector.transform(X)
A:sklearn.feature_selection.tests.test_feature_select.X_2->numpy.random.RandomState(0).randn(10, 5).copy()
A:sklearn.feature_selection.tests.test_feature_select.(scores, pvalues)->chi2(X, y)
A:sklearn.feature_selection.tests.test_feature_select.filter_fdr->SelectFdr(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fdr->SelectFdr(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_kbest->SelectKBest(chi2, k=1)
A:sklearn.feature_selection.tests.test_feature_select.support_kbest->SelectKBest(chi2, k=1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_percentile->SelectPercentile(chi2, percentile=50)
A:sklearn.feature_selection.tests.test_feature_select.support_percentile->SelectPercentile(chi2, percentile=50).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_fpr->SelectFpr(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fpr->SelectFpr(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.filter_fwe->SelectFwe(chi2, alpha=0.1)
A:sklearn.feature_selection.tests.test_feature_select.support_fwe->SelectFwe(chi2, alpha=0.1).get_support()
A:sklearn.feature_selection.tests.test_feature_select.num_false_positives->numpy.sum(support[n_informative:] == 1)
A:sklearn.feature_selection.tests.test_feature_select.num_true_positives->numpy.sum(support[:n_informative] == 1)
A:sklearn.feature_selection.tests.test_feature_select.false_discovery_rate->numpy.mean([single_fdr(alpha, n_informative, random_state) for random_state in range(100)])
A:sklearn.feature_selection.tests.test_feature_select.sel->SelectKBest(chi2, k=n_features).fit(X_train, y_train)
A:sklearn.feature_selection.tests.test_feature_select.X2->ignore_warnings(sel.fit_transform)([X], y)
A:sklearn.feature_selection.tests.test_feature_select.X0->numpy.array([[10000, 9999, 9998], [1, 1, 1]])
A:sklearn.feature_selection.tests.test_feature_select.Xt->SelectPercentile(chi2, percentile=67).fit_transform(X, y)
A:sklearn.feature_selection.tests.test_feature_select.X_train->numpy.array([[0, 0, 0], [1, 1, 1]])
A:sklearn.feature_selection.tests.test_feature_select.X_test->SelectKBest(chi2, k=n_features).fit(X_train, y_train).transform([[0, 1, 2]])
A:sklearn.feature_selection.tests.test_feature_select.pd->pytest.importorskip('pandas')
A:sklearn.feature_selection.tests.test_feature_select.X['petal_width_binned']->pytest.importorskip('pandas').cut(X['petal width (cm)'], bins=10)
A:sklearn.feature_selection.tests.test_feature_select.output->SelectKBest(selector, k=3).set_output(transform='pandas').fit_transform(X, y)
A:sklearn.feature_selection.tests.test_feature_select.X_trans->selector.fit_transform(X)
sklearn.feature_selection.tests.test_feature_select.assert_best_scores_kept(score_filter)
sklearn.feature_selection.tests.test_feature_select.test_boundary_case_ch2()
sklearn.feature_selection.tests.test_feature_select.test_dataframe_output_dtypes()
sklearn.feature_selection.tests.test_feature_select.test_f_classif(csr_container)
sklearn.feature_selection.tests.test_feature_select.test_f_classif_constant_feature()
sklearn.feature_selection.tests.test_feature_select.test_f_classif_multi_class()
sklearn.feature_selection.tests.test_feature_select.test_f_oneway_ints()
sklearn.feature_selection.tests.test_feature_select.test_f_oneway_vs_scipy_stats()
sklearn.feature_selection.tests.test_feature_select.test_f_regression(csr_container)
sklearn.feature_selection.tests.test_feature_select.test_f_regression_center()
sklearn.feature_selection.tests.test_feature_select.test_f_regression_corner_case(X,y,expected_f_statistic,expected_p_values,force_finite)
sklearn.feature_selection.tests.test_feature_select.test_f_regression_input_dtype()
sklearn.feature_selection.tests.test_feature_select.test_invalid_k()
sklearn.feature_selection.tests.test_feature_select.test_mutual_info_classif()
sklearn.feature_selection.tests.test_feature_select.test_mutual_info_regression()
sklearn.feature_selection.tests.test_feature_select.test_nans()
sklearn.feature_selection.tests.test_feature_select.test_no_feature_selected()
sklearn.feature_selection.tests.test_feature_select.test_r_regression(center)
sklearn.feature_selection.tests.test_feature_select.test_r_regression_force_finite(X,y,expected_corr_coef,force_finite)
sklearn.feature_selection.tests.test_feature_select.test_scorefunc_multilabel()
sklearn.feature_selection.tests.test_feature_select.test_select_fdr_regression(alpha,n_informative)
sklearn.feature_selection.tests.test_feature_select.test_select_fwe_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_heuristics_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_heuristics_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_all()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_kbest_zero(dtype_in)
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_classif()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_classif_sparse(csr_container)
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_regression()
sklearn.feature_selection.tests.test_feature_select.test_select_percentile_regression_full()
sklearn.feature_selection.tests.test_feature_select.test_selectkbest_tiebreaking()
sklearn.feature_selection.tests.test_feature_select.test_selectpercentile_tiebreaking()
sklearn.feature_selection.tests.test_feature_select.test_tied_pvalues()
sklearn.feature_selection.tests.test_feature_select.test_tied_scores()
sklearn.feature_selection.tests.test_feature_select.test_unsupervised_filter(selector)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/conftest.py----------------------------------------
sklearn.externals.conftest.pytest_ignore_collect(path,config)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_arff.py----------------------------------------
A:sklearn.externals._arff._RE_RELATION->re.compile('^([^\\{\\}%,\\s]*|\\".*\\"|\\\'.*\\\')$', re.UNICODE)
A:sklearn.externals._arff._RE_ATTRIBUTE->re.compile('^(\\".*\\"|\\\'.*\\\'|[^\\{\\}%,\\s]*)\\s+(.+)$', re.UNICODE)
A:sklearn.externals._arff._RE_QUOTE_CHARS->re.compile('["\\\'\\\\\\s%,\\000-\\031]', re.UNICODE)
A:sklearn.externals._arff._RE_ESCAPE_CHARS->re.compile('(?=["\\\'\\\\%])|[\\n\\r\\t\\000-\\031]')
A:sklearn.externals._arff._RE_SPARSE_LINE->re.compile('^\\s*\\{.*\\}\\s*$', re.UNICODE)
A:sklearn.externals._arff._RE_NONTRIVIAL_DATA->re.compile('["\'{}\\s]', re.UNICODE)
A:sklearn.externals._arff.dense->re.compile("(?x)\n        ,                # may follow ','\n        \\s*\n        ((?=,)|$|{value_re})  # empty or value\n        |\n        (\\S.*)           # error\n        ".format(value_re=value_re))
A:sklearn.externals._arff.sparse->re.compile("(?x)\n        (?:^\\s*\\{|,)   # may follow ',', or '{' at line start\n        \\s*\n        (\\d+)          # attribute key\n        \\s+\n        (%(value_re)s) # value\n        |\n        (?!}\\s*$)      # not an error if it's }$\n        (?!^\\s*{\\s*}\\s*$)  # not an error if it's ^{}$\n        \\S.*           # error\n        " % {'value_re': value_re})
A:sklearn.externals._arff.(_RE_DENSE_VALUES, _RE_SPARSE_KEY_VALUES)->_build_re_values()
A:sklearn.externals._arff.s->iter(s)
A:sklearn.externals._arff.(values, errors)->zip(*_RE_DENSE_VALUES.findall(',' + s))
A:sklearn.externals._arff.self.values->set(values)
A:sklearn.externals._arff.values->_parse_values(row)
A:sklearn.externals._arff.(row_cols, values)->zip(*sorted(values.items()))
A:sklearn.externals._arff.num_attributes->len(attributes)
A:sklearn.externals._arff.res->str(v.strip('"\''))
A:sklearn.externals._arff.(_, v)->iter(s).split(' ', 1)
A:sklearn.externals._arff.v->v.strip().strip()
A:sklearn.externals._arff.m->re.compile('^(\\".*\\"|\\\'.*\\\'|[^\\{\\}%,\\s]*)\\s+(.+)$', re.UNICODE).match(v)
A:sklearn.externals._arff.(name, type_)->re.compile('^(\\".*\\"|\\\'.*\\\'|[^\\{\\}%,\\s]*)\\s+(.+)$', re.UNICODE).match(v).groups()
A:sklearn.externals._arff.name->str(name.strip('"\''))
A:sklearn.externals._arff.type_->str(type_).upper()
A:sklearn.externals._arff.data->_get_data_object_for_encoding(obj.get('data'))
A:sklearn.externals._arff.row->row.strip().strip()
A:sklearn.externals._arff.u_row->row.strip().strip().upper()
A:sklearn.externals._arff.obj['relation']->self._decode_relation(row)
A:sklearn.externals._arff.attr->self._decode_attribute(row)
A:sklearn.externals._arff.conversor->NominalConversor(attr[1])
A:sklearn.externals._arff.obj['data']->_get_data_object_for_encoding(obj.get('data')).decode_rows(stream(), self._conversors)
A:sklearn.externals._arff.attribute_names->set()
A:sklearn.externals._arff.decoder->ArffDecoder()
A:sklearn.externals._arff.encoder->ArffEncoder()
A:sklearn.externals._arff.generator->ArffEncoder().iter_encode(obj)
A:sklearn.externals._arff.last_row->next(generator)
sklearn.externals._arff.ArffDecoder(self)
sklearn.externals._arff.ArffDecoder.__init__(self)
sklearn.externals._arff.ArffDecoder._decode(self,s,encode_nominal=False,matrix_type=DENSE)
sklearn.externals._arff.ArffDecoder._decode_attribute(self,s)
sklearn.externals._arff.ArffDecoder._decode_comment(self,s)
sklearn.externals._arff.ArffDecoder._decode_relation(self,s)
sklearn.externals._arff.ArffDecoder.decode(self,s,encode_nominal=False,return_type=DENSE)
sklearn.externals._arff.ArffEncoder
sklearn.externals._arff.ArffEncoder._encode_attribute(self,name,type_)
sklearn.externals._arff.ArffEncoder._encode_comment(self,s='')
sklearn.externals._arff.ArffEncoder._encode_relation(self,name)
sklearn.externals._arff.ArffEncoder.encode(self,obj)
sklearn.externals._arff.ArffEncoder.iter_encode(self,obj)
sklearn.externals._arff.ArffException(self)
sklearn.externals._arff.ArffException.__init__(self)
sklearn.externals._arff.ArffException.__str__(self)
sklearn.externals._arff.BadAttributeFormat(ArffException)
sklearn.externals._arff.BadAttributeName(self,value,value2)
sklearn.externals._arff.BadAttributeName.__init__(self,value,value2)
sklearn.externals._arff.BadAttributeType(ArffException)
sklearn.externals._arff.BadDataFormat(self,value)
sklearn.externals._arff.BadDataFormat.__init__(self,value)
sklearn.externals._arff.BadLayout(self,msg='')
sklearn.externals._arff.BadLayout.__init__(self,msg='')
sklearn.externals._arff.BadNominalFormatting(self,value)
sklearn.externals._arff.BadNominalFormatting.__init__(self,value)
sklearn.externals._arff.BadNominalValue(self,value)
sklearn.externals._arff.BadNominalValue.__init__(self,value)
sklearn.externals._arff.BadNumericalValue(ArffException)
sklearn.externals._arff.BadObject(self,msg='Invalidobject.')
sklearn.externals._arff.BadObject.__init__(self,msg='Invalidobject.')
sklearn.externals._arff.BadObject.__str__(self)
sklearn.externals._arff.BadRelationFormat(ArffException)
sklearn.externals._arff.BadStringValue(ArffException)
sklearn.externals._arff.COOData
sklearn.externals._arff.COOData.decode_rows(self,stream,conversors)
sklearn.externals._arff.COOData.encode_data(self,data,attributes)
sklearn.externals._arff.Data(_DataListMixin,DenseGeneratorData)
sklearn.externals._arff.DenseGeneratorData
sklearn.externals._arff.DenseGeneratorData._decode_values(values,conversors)
sklearn.externals._arff.DenseGeneratorData.decode_rows(self,stream,conversors)
sklearn.externals._arff.DenseGeneratorData.encode_data(self,data,attributes)
sklearn.externals._arff.EncodedNominalConversor(self,values)
sklearn.externals._arff.EncodedNominalConversor.__init__(self,values)
sklearn.externals._arff.LODData(_DataListMixin,LODGeneratorData)
sklearn.externals._arff.LODGeneratorData
sklearn.externals._arff.LODGeneratorData.decode_rows(self,stream,conversors)
sklearn.externals._arff.LODGeneratorData.encode_data(self,data,attributes)
sklearn.externals._arff.NominalConversor(self,values)
sklearn.externals._arff.NominalConversor.__init__(self,values)
sklearn.externals._arff._DataListMixin
sklearn.externals._arff._DataListMixin.decode_rows(self,stream,conversors)
sklearn.externals._arff._build_re_values()
sklearn.externals._arff._escape_sub_callback(match)
sklearn.externals._arff._get_data_object_for_decoding(matrix_type)
sklearn.externals._arff._get_data_object_for_encoding(matrix)
sklearn.externals._arff._parse_values(s)
sklearn.externals._arff._unescape_sub_callback(match)
sklearn.externals._arff._unquote(v)
sklearn.externals._arff.dump(obj,fp)
sklearn.externals._arff.dumps(obj)
sklearn.externals._arff.encode_string(s)
sklearn.externals._arff.load(fp,encode_nominal=False,return_type=DENSE)
sklearn.externals._arff.loads(s,encode_nominal=False,return_type=DENSE)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_scipy/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_scipy/sparse/csgraph/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_scipy/sparse/csgraph/_laplacian.py----------------------------------------
A:sklearn.externals._scipy.sparse.csgraph._laplacian.csgraph->csgraph.astype(np.float64).astype(np.float64)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.(lap, d)->create_lap(csgraph, normed=normed, axis=degree_axis, copy=copy, form=form, dtype=dtype, symmetrized=symmetrized)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.laplace->_laplace(m, d)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.laplace_sym->_laplace_sym(m, d)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.graph_sum->numpy.asarray(graph).sum(axis=axis)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.graph_diagonal->numpy.asarray(graph).diagonal()
A:sklearn.externals._scipy.sparse.csgraph._laplacian.w->numpy.where(isolated_node_mask, 1, np.sqrt(w))
A:sklearn.externals._scipy.sparse.csgraph._laplacian.md->_laplace(m, graph_sum)
A:sklearn.externals._scipy.sparse.csgraph._laplacian.m->numpy.asarray(graph)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplace(m,d)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplace_normed(m,d,nd)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplace_normed_sym(m,d,nd)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplace_sym(m,d)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplacian_dense(graph,normed,axis,copy,form,dtype,symmetrized)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplacian_dense_flo(graph,normed,axis,copy,form,dtype,symmetrized)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplacian_sparse(graph,normed,axis,copy,form,dtype,symmetrized)
sklearn.externals._scipy.sparse.csgraph._laplacian._laplacian_sparse_flo(graph,normed,axis,copy,form,dtype,symmetrized)
sklearn.externals._scipy.sparse.csgraph._laplacian._linearoperator(mv,shape,dtype)
sklearn.externals._scipy.sparse.csgraph._laplacian._setdiag_dense(m,d)
sklearn.externals._scipy.sparse.csgraph._laplacian.laplacian(csgraph,normed=False,return_diag=False,use_out_degree=False,*,copy=True,form='array',dtype=None,symmetrized=False)
sklearn.externals._scipy.sparse.csgraph.laplacian(csgraph,normed=False,return_diag=False,use_out_degree=False,*,copy=True,form='array',dtype=None,symmetrized=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_packaging/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_packaging/_structures.py----------------------------------------
A:sklearn.externals._packaging._structures.Infinity->InfinityType()
A:sklearn.externals._packaging._structures.NegativeInfinity->NegativeInfinityType()
sklearn.externals._packaging._structures.InfinityType
sklearn.externals._packaging._structures.InfinityType.__eq__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__ge__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__gt__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__hash__(self)->int
sklearn.externals._packaging._structures.InfinityType.__le__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__lt__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__ne__(self,other:object)->bool
sklearn.externals._packaging._structures.InfinityType.__neg__(self:object)->'NegativeInfinityType'
sklearn.externals._packaging._structures.InfinityType.__repr__(self)->str
sklearn.externals._packaging._structures.NegativeInfinityType
sklearn.externals._packaging._structures.NegativeInfinityType.__eq__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__ge__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__gt__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__hash__(self)->int
sklearn.externals._packaging._structures.NegativeInfinityType.__le__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__lt__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__ne__(self,other:object)->bool
sklearn.externals._packaging._structures.NegativeInfinityType.__neg__(self:object)->InfinityType
sklearn.externals._packaging._structures.NegativeInfinityType.__repr__(self)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/externals/_packaging/version.py----------------------------------------
A:sklearn.externals._packaging.version._Version->collections.namedtuple('_Version', ['epoch', 'release', 'dev', 'pre', 'post', 'local'])
A:sklearn.externals._packaging.version.self._version->_Version(epoch=int(match.group('epoch')) if match.group('epoch') else 0, release=tuple((int(i) for i in match.group('release').split('.'))), pre=_parse_letter_version(match.group('pre_l'), match.group('pre_n')), post=_parse_letter_version(match.group('post_l'), match.group('post_n1') or match.group('post_n2')), dev=_parse_letter_version(match.group('dev_l'), match.group('dev_n')), local=_parse_local_version(match.group('local')))
A:sklearn.externals._packaging.version.self._key->_cmpkey(self._version.epoch, self._version.release, self._version.pre, self._version.post, self._version.dev, self._version.local)
A:sklearn.externals._packaging.version._legacy_version_component_re->re.compile('(\\d+ | [a-z]+ | \\.| -)', re.VERBOSE)
A:sklearn.externals._packaging.version.part->_legacy_version_replacement_map.get(part, part)
A:sklearn.externals._packaging.version._regex->re.compile('^\\s*' + VERSION_PATTERN + '\\s*$', re.VERBOSE | re.IGNORECASE)
A:sklearn.externals._packaging.version.match->self._regex.search(version)
A:sklearn.externals._packaging.version.letter->letter.lower().lower()
A:sklearn.externals._packaging.version._local_version_separators->re.compile('[\\._-]')
A:sklearn.externals._packaging.version._release->tuple(reversed(list(itertools.dropwhile(lambda x: x == 0, reversed(release)))))
A:sklearn.externals._packaging.version._local->tuple(((i, '') if isinstance(i, int) else (NegativeInfinity, i) for i in local))
sklearn.externals._packaging.version.InvalidVersion(ValueError)
sklearn.externals._packaging.version.LegacyVersion(self,version:str)
sklearn.externals._packaging.version.LegacyVersion.__init__(self,version:str)
sklearn.externals._packaging.version.LegacyVersion.__repr__(self)->str
sklearn.externals._packaging.version.LegacyVersion.__str__(self)->str
sklearn.externals._packaging.version.LegacyVersion.base_version(self)->str
sklearn.externals._packaging.version.LegacyVersion.dev(self)->None
sklearn.externals._packaging.version.LegacyVersion.epoch(self)->int
sklearn.externals._packaging.version.LegacyVersion.is_devrelease(self)->bool
sklearn.externals._packaging.version.LegacyVersion.is_postrelease(self)->bool
sklearn.externals._packaging.version.LegacyVersion.is_prerelease(self)->bool
sklearn.externals._packaging.version.LegacyVersion.local(self)->None
sklearn.externals._packaging.version.LegacyVersion.post(self)->None
sklearn.externals._packaging.version.LegacyVersion.pre(self)->None
sklearn.externals._packaging.version.LegacyVersion.public(self)->str
sklearn.externals._packaging.version.LegacyVersion.release(self)->None
sklearn.externals._packaging.version.Version(self,version:str)
sklearn.externals._packaging.version.Version.__init__(self,version:str)
sklearn.externals._packaging.version.Version.__repr__(self)->str
sklearn.externals._packaging.version.Version.__str__(self)->str
sklearn.externals._packaging.version.Version.base_version(self)->str
sklearn.externals._packaging.version.Version.dev(self)->Optional[int]
sklearn.externals._packaging.version.Version.epoch(self)->int
sklearn.externals._packaging.version.Version.is_devrelease(self)->bool
sklearn.externals._packaging.version.Version.is_postrelease(self)->bool
sklearn.externals._packaging.version.Version.is_prerelease(self)->bool
sklearn.externals._packaging.version.Version.local(self)->Optional[str]
sklearn.externals._packaging.version.Version.major(self)->int
sklearn.externals._packaging.version.Version.micro(self)->int
sklearn.externals._packaging.version.Version.minor(self)->int
sklearn.externals._packaging.version.Version.post(self)->Optional[int]
sklearn.externals._packaging.version.Version.pre(self)->Optional[Tuple[str, int]]
sklearn.externals._packaging.version.Version.public(self)->str
sklearn.externals._packaging.version.Version.release(self)->Tuple[int, ...]
sklearn.externals._packaging.version._BaseVersion
sklearn.externals._packaging.version._BaseVersion.__eq__(self,other:object)->bool
sklearn.externals._packaging.version._BaseVersion.__ge__(self,other:'_BaseVersion')->bool
sklearn.externals._packaging.version._BaseVersion.__gt__(self,other:'_BaseVersion')->bool
sklearn.externals._packaging.version._BaseVersion.__hash__(self)->int
sklearn.externals._packaging.version._BaseVersion.__le__(self,other:'_BaseVersion')->bool
sklearn.externals._packaging.version._BaseVersion.__lt__(self,other:'_BaseVersion')->bool
sklearn.externals._packaging.version._BaseVersion.__ne__(self,other:object)->bool
sklearn.externals._packaging.version._cmpkey(epoch:int,release:Tuple[int,...],pre:Optional[Tuple[str,int]],post:Optional[Tuple[str,int]],dev:Optional[Tuple[str,int]],local:Optional[Tuple[SubLocalType]])->CmpKey
sklearn.externals._packaging.version._legacy_cmpkey(version:str)->LegacyCmpKey
sklearn.externals._packaging.version._parse_letter_version(letter:str,number:Union[str,bytes,SupportsInt])->Optional[Tuple[str, int]]
sklearn.externals._packaging.version._parse_local_version(local:str)->Optional[LocalType]
sklearn.externals._packaging.version._parse_version_parts(s:str)->Iterator[str]
sklearn.externals._packaging.version.parse(version:str)->Union['LegacyVersion', 'Version']


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_samples_generator.py----------------------------------------
A:sklearn.datasets._samples_generator.out->sample_without_replacement(2 ** dimensions, samples, random_state=rng).astype(dtype='>u4', copy=False)
A:sklearn.datasets._samples_generator.generator->check_random_state(random_state)
A:sklearn.datasets._samples_generator.weights->numpy.resize(weights, n_classes)
A:sklearn.datasets._samples_generator.X->check_random_state(random_state).multivariate_normal(mean, cov * np.identity(n_features), (n_samples,))
A:sklearn.datasets._samples_generator.y->numpy.hstack([np.repeat(np.arange(n_classes), step), np.repeat(n_classes - 1, n_samples - step * n_classes)])
A:sklearn.datasets._samples_generator.centroids->_generate_hypercube(n_clusters, n_informative, generator).astype(float, copy=False)
A:sklearn.datasets._samples_generator.X[:, :n_informative]->check_random_state(random_state).standard_normal(size=(n_samples, n_informative))
A:sklearn.datasets._samples_generator.X_k[...]->numpy.dot(X_k, A)
A:sklearn.datasets._samples_generator.X[:, n_informative:n_informative + n_redundant]->numpy.dot(X[:, :n_informative], B)
A:sklearn.datasets._samples_generator.indices->numpy.arange(n_features)
A:sklearn.datasets._samples_generator.X[:, -n_useless:]->check_random_state(random_state).standard_normal(size=(n_samples, n_useless))
A:sklearn.datasets._samples_generator.y[flip_mask]->check_random_state(random_state).randint(n_classes, size=flip_mask.sum())
A:sklearn.datasets._samples_generator.(X, y)->util_shuffle(X, y, random_state=generator)
A:sklearn.datasets._samples_generator.p_c->check_random_state(random_state).uniform(size=n_classes)
A:sklearn.datasets._samples_generator.cumulative_p_c->numpy.cumsum(p_c)
A:sklearn.datasets._samples_generator.p_w_c->check_random_state(random_state).uniform(size=(n_features, n_classes))
A:sklearn.datasets._samples_generator.y_size->check_random_state(random_state).poisson(n_labels)
A:sklearn.datasets._samples_generator.c->numpy.searchsorted(cumulative_p_c, generator.uniform(size=y_size - len(y)))
A:sklearn.datasets._samples_generator.n_words->check_random_state(random_state).poisson(length)
A:sklearn.datasets._samples_generator.words->numpy.searchsorted(cumulative_p_w_sample, generator.uniform(size=n_words))
A:sklearn.datasets._samples_generator.cumulative_p_w_sample->check_random_state(random_state).uniform(size=(n_features, n_classes)).take(y, axis=1).sum(axis=1).cumsum()
A:sklearn.datasets._samples_generator.X_indices->array.array('i')
A:sklearn.datasets._samples_generator.X_indptr->array.array('i', [0])
A:sklearn.datasets._samples_generator.(words, y)->sample_example()
A:sklearn.datasets._samples_generator.X_data->numpy.ones(len(X_indices), dtype=np.float64)
A:sklearn.datasets._samples_generator.lb->MultiLabelBinarizer(sparse_output=return_indicator == 'sparse')
A:sklearn.datasets._samples_generator.Y->numpy.dot(D, X)
A:sklearn.datasets._samples_generator.rs->check_random_state(random_state)
A:sklearn.datasets._samples_generator.n_informative->min(n_features, n_informative)
A:sklearn.datasets._samples_generator.ground_truth->numpy.zeros((n_features, n_targets))
A:sklearn.datasets._samples_generator.linspace_out->numpy.linspace(0, 2 * np.pi, n_samples_out, endpoint=False)
A:sklearn.datasets._samples_generator.linspace_in->numpy.linspace(0, 2 * np.pi, n_samples_in, endpoint=False)
A:sklearn.datasets._samples_generator.outer_circ_x->numpy.cos(np.linspace(0, np.pi, n_samples_out))
A:sklearn.datasets._samples_generator.outer_circ_y->numpy.sin(np.linspace(0, np.pi, n_samples_out))
A:sklearn.datasets._samples_generator.centers->check_array(centers)
A:sklearn.datasets._samples_generator.n_centers->len(n_samples)
A:sklearn.datasets._samples_generator.cluster_std->numpy.full(len(centers), cluster_std)
A:sklearn.datasets._samples_generator.cum_sum_n_samples->numpy.cumsum(n_samples_per_center)
A:sklearn.datasets._samples_generator.X[start_idx:end_idx]->check_random_state(random_state).normal(loc=centers[i], scale=std, size=(n, n_features))
A:sklearn.datasets._samples_generator.n->min(n_samples, n_features)
A:sklearn.datasets._samples_generator.(u, _)->scipy.linalg.qr(generator.standard_normal(size=(n_samples, n)), mode='economic', check_finite=False)
A:sklearn.datasets._samples_generator.(v, _)->scipy.linalg.qr(generator.standard_normal(size=(n_features, n)), mode='economic', check_finite=False)
A:sklearn.datasets._samples_generator.singular_ind->numpy.arange(n, dtype=np.float64)
A:sklearn.datasets._samples_generator.D->check_random_state(random_state).standard_normal(size=(n_features, n_components))
A:sklearn.datasets._samples_generator.idx->numpy.argsort(np.sum((X - mean[np.newaxis, :]) ** 2, axis=1))
A:sklearn.datasets._samples_generator.X[idx, i]->check_random_state(random_state).standard_normal(size=n_nonzero_coefs)
A:sklearn.datasets._samples_generator.A->check_random_state(random_state).uniform(size=(n_dim, n_dim))
A:sklearn.datasets._samples_generator.(U, _, Vt)->scipy.linalg.svd(np.dot(A.T, A), check_finite=False)
A:sklearn.datasets._samples_generator.random_state->check_random_state(random_state)
A:sklearn.datasets._samples_generator.aux->scipy.sparse.tril(aux, k=-1, format='csc')
A:sklearn.datasets._samples_generator.permutation->check_random_state(random_state).permutation(_n_dim)
A:sklearn.datasets._samples_generator.d->scipy.sparse.diags(1.0 / np.sqrt(prec.diagonal()))
A:sklearn.datasets._samples_generator.corners->numpy.delete(corners, 4, axis=0)
A:sklearn.datasets._samples_generator.corner_index->check_random_state(random_state).choice(8, n_samples)
A:sklearn.datasets._samples_generator.t->numpy.squeeze(t)
A:sklearn.datasets._samples_generator.X[:, 0]->numpy.sin(t)
A:sklearn.datasets._samples_generator.mean->numpy.array(mean)
A:sklearn.datasets._samples_generator.row_idx->check_random_state(random_state).permutation(n_rows)
A:sklearn.datasets._samples_generator.col_idx->check_random_state(random_state).permutation(n_cols)
A:sklearn.datasets._samples_generator.consts->check_random_state(random_state).uniform(minval, maxval, n_clusters)
A:sklearn.datasets._samples_generator.row_sizes->check_random_state(random_state).multinomial(n_rows, np.repeat(1.0 / n_row_clusters, n_row_clusters))
A:sklearn.datasets._samples_generator.col_sizes->check_random_state(random_state).multinomial(n_cols, np.repeat(1.0 / n_col_clusters, n_col_clusters))
A:sklearn.datasets._samples_generator.row_labels->numpy.hstack([np.repeat(val, rep) for (val, rep) in zip(range(n_row_clusters), row_sizes)])
A:sklearn.datasets._samples_generator.col_labels->numpy.hstack([np.repeat(val, rep) for (val, rep) in zip(range(n_col_clusters), col_sizes)])
A:sklearn.datasets._samples_generator.result->numpy.zeros(shape, dtype=np.float64)
A:sklearn.datasets._samples_generator.selector->numpy.outer(row_labels == i, col_labels == j)
A:sklearn.datasets._samples_generator.(result, row_idx, col_idx)->_shuffle(result, random_state)
A:sklearn.datasets._samples_generator.rows->numpy.vstack([row_labels == label for label in range(n_row_clusters) for _ in range(n_col_clusters)])
A:sklearn.datasets._samples_generator.cols->numpy.vstack([col_labels == label for _ in range(n_row_clusters) for label in range(n_col_clusters)])
sklearn.datasets._samples_generator._generate_hypercube(samples,dimensions,rng)
sklearn.datasets._samples_generator._shuffle(data,random_state=None)
sklearn.datasets._samples_generator.make_biclusters(shape,n_clusters,*,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets._samples_generator.make_blobs(n_samples=100,n_features=2,*,centers=None,cluster_std=1.0,center_box=(-10.0,10.0),shuffle=True,random_state=None,return_centers=False)
sklearn.datasets._samples_generator.make_checkerboard(shape,n_clusters,*,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets._samples_generator.make_circles(n_samples=100,*,shuffle=True,noise=None,random_state=None,factor=0.8)
sklearn.datasets._samples_generator.make_classification(n_samples=100,n_features=20,*,n_informative=2,n_redundant=2,n_repeated=0,n_classes=2,n_clusters_per_class=2,weights=None,flip_y=0.01,class_sep=1.0,hypercube=True,shift=0.0,scale=1.0,shuffle=True,random_state=None)
sklearn.datasets._samples_generator.make_friedman1(n_samples=100,n_features=10,*,noise=0.0,random_state=None)
sklearn.datasets._samples_generator.make_friedman2(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets._samples_generator.make_friedman3(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets._samples_generator.make_gaussian_quantiles(*,mean=None,cov=1.0,n_samples=100,n_features=2,n_classes=3,shuffle=True,random_state=None)
sklearn.datasets._samples_generator.make_hastie_10_2(n_samples=12000,*,random_state=None)
sklearn.datasets._samples_generator.make_low_rank_matrix(n_samples=100,n_features=100,*,effective_rank=10,tail_strength=0.5,random_state=None)
sklearn.datasets._samples_generator.make_moons(n_samples=100,*,shuffle=True,noise=None,random_state=None)
sklearn.datasets._samples_generator.make_multilabel_classification(n_samples=100,n_features=20,*,n_classes=5,n_labels=2,length=50,allow_unlabeled=True,sparse=False,return_indicator='dense',return_distributions=False,random_state=None)
sklearn.datasets._samples_generator.make_regression(n_samples=100,n_features=100,*,n_informative=10,n_targets=1,bias=0.0,effective_rank=None,tail_strength=0.5,noise=0.0,shuffle=True,coef=False,random_state=None)
sklearn.datasets._samples_generator.make_s_curve(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets._samples_generator.make_sparse_coded_signal(n_samples,*,n_components,n_features,n_nonzero_coefs,random_state=None,data_transposed='deprecated')
sklearn.datasets._samples_generator.make_sparse_spd_matrix(n_dim=None,*,alpha=0.95,norm_diag=False,smallest_coef=0.1,largest_coef=0.9,sparse_format=None,random_state=None,dim='deprecated')
sklearn.datasets._samples_generator.make_sparse_uncorrelated(n_samples=100,n_features=10,*,random_state=None)
sklearn.datasets._samples_generator.make_spd_matrix(n_dim,*,random_state=None)
sklearn.datasets._samples_generator.make_swiss_roll(n_samples=100,*,noise=0.0,random_state=None,hole=False)
sklearn.datasets.make_biclusters(shape,n_clusters,*,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.make_blobs(n_samples=100,n_features=2,*,centers=None,cluster_std=1.0,center_box=(-10.0,10.0),shuffle=True,random_state=None,return_centers=False)
sklearn.datasets.make_checkerboard(shape,n_clusters,*,noise=0.0,minval=10,maxval=100,shuffle=True,random_state=None)
sklearn.datasets.make_circles(n_samples=100,*,shuffle=True,noise=None,random_state=None,factor=0.8)
sklearn.datasets.make_classification(n_samples=100,n_features=20,*,n_informative=2,n_redundant=2,n_repeated=0,n_classes=2,n_clusters_per_class=2,weights=None,flip_y=0.01,class_sep=1.0,hypercube=True,shift=0.0,scale=1.0,shuffle=True,random_state=None)
sklearn.datasets.make_friedman1(n_samples=100,n_features=10,*,noise=0.0,random_state=None)
sklearn.datasets.make_friedman2(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets.make_friedman3(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets.make_gaussian_quantiles(*,mean=None,cov=1.0,n_samples=100,n_features=2,n_classes=3,shuffle=True,random_state=None)
sklearn.datasets.make_hastie_10_2(n_samples=12000,*,random_state=None)
sklearn.datasets.make_low_rank_matrix(n_samples=100,n_features=100,*,effective_rank=10,tail_strength=0.5,random_state=None)
sklearn.datasets.make_moons(n_samples=100,*,shuffle=True,noise=None,random_state=None)
sklearn.datasets.make_multilabel_classification(n_samples=100,n_features=20,*,n_classes=5,n_labels=2,length=50,allow_unlabeled=True,sparse=False,return_indicator='dense',return_distributions=False,random_state=None)
sklearn.datasets.make_regression(n_samples=100,n_features=100,*,n_informative=10,n_targets=1,bias=0.0,effective_rank=None,tail_strength=0.5,noise=0.0,shuffle=True,coef=False,random_state=None)
sklearn.datasets.make_s_curve(n_samples=100,*,noise=0.0,random_state=None)
sklearn.datasets.make_sparse_coded_signal(n_samples,*,n_components,n_features,n_nonzero_coefs,random_state=None,data_transposed='deprecated')
sklearn.datasets.make_sparse_spd_matrix(n_dim=None,*,alpha=0.95,norm_diag=False,smallest_coef=0.1,largest_coef=0.9,sparse_format=None,random_state=None,dim='deprecated')
sklearn.datasets.make_sparse_uncorrelated(n_samples=100,n_features=10,*,random_state=None)
sklearn.datasets.make_spd_matrix(n_dim,*,random_state=None)
sklearn.datasets.make_swiss_roll(n_samples=100,*,noise=0.0,random_state=None,hole=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/__init__.py----------------------------------------
A:sklearn.datasets.__init__.msg->textwrap.dedent('\n            `load_boston` has been removed from scikit-learn since version 1.2.\n\n            The Boston housing prices dataset has an ethical problem: as\n            investigated in [1], the authors of this dataset engineered a\n            non-invertible variable "B" assuming that racial self-segregation had a\n            positive impact on house prices [2]. Furthermore the goal of the\n            research that led to the creation of this dataset was to study the\n            impact of air quality but it did not give adequate demonstration of the\n            validity of this assumption.\n\n            The scikit-learn maintainers therefore strongly discourage the use of\n            this dataset unless the purpose of the code is to study and educate\n            about ethical issues in data science and machine learning.\n\n            In this special case, you can fetch the dataset from the original\n            source::\n\n                import pandas as pd\n                import numpy as np\n\n                data_url = "http://lib.stat.cmu.edu/datasets/boston"\n                raw_df = pd.read_csv(data_url, sep="\\s+", skiprows=22, header=None)\n                data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n                target = raw_df.values[1::2, 2]\n\n            Alternative datasets include the California housing dataset and the\n            Ames housing dataset. You can load the datasets as follows::\n\n                from sklearn.datasets import fetch_california_housing\n                housing = fetch_california_housing()\n\n            for the California housing dataset and::\n\n                from sklearn.datasets import fetch_openml\n                housing = fetch_openml(name="house_prices", as_frame=True)\n\n            for the Ames housing dataset.\n\n            [1] M Carlisle.\n            "Racist data destruction?"\n            <https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n            [2] Harrison Jr, David, and Daniel L. Rubinfeld.\n            "Hedonic housing prices and the demand for clean air."\n            Journal of environmental economics and management 5.1 (1978): 81-102.\n            <https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n            ')
sklearn.datasets.__init__.__getattr__(name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_arff_parser.py----------------------------------------
A:sklearn.datasets._arff_parser.y->y.reshape((-1,)).reshape((-1,))
A:sklearn.datasets._arff_parser.stream->_io_to_generator(gzip_file)
A:sklearn.datasets._arff_parser.arff_container->externals._arff.load(stream, return_type=return_type, encode_nominal=encode_nominal)
A:sklearn.datasets._arff_parser.pd->check_pandas_support('fetch_openml with as_frame=True')
A:sklearn.datasets._arff_parser.columns_info->OrderedDict(arff_container['attributes'])
A:sklearn.datasets._arff_parser.columns_names->list(columns_info.keys())
A:sklearn.datasets._arff_parser.first_row->next(arff_container['data'])
A:sklearn.datasets._arff_parser.first_df->check_pandas_support('fetch_openml with as_frame=True').DataFrame([first_row], columns=columns_names, copy=False)
A:sklearn.datasets._arff_parser.row_bytes->check_pandas_support('fetch_openml with as_frame=True').DataFrame([first_row], columns=columns_names, copy=False).memory_usage(deep=True).sum()
A:sklearn.datasets._arff_parser.chunksize->get_chunk_n_rows(row_bytes)
A:sklearn.datasets._arff_parser.dfs[0]->dfs[0].astype(dfs[1].dtypes).astype(dfs[1].dtypes)
A:sklearn.datasets._arff_parser.frame->check_pandas_support('fetch_openml with as_frame=True').read_csv(gzip_file, **read_csv_kwargs)
A:sklearn.datasets._arff_parser.(X, y)->_post_process_frame(frame, feature_names_to_select, target_names_to_select)
A:sklearn.datasets._arff_parser.data->data.reshape(*shape).reshape(*shape)
A:sklearn.datasets._arff_parser.arff_data_X->_split_sparse_columns(arff_data, feature_indices_to_select)
A:sklearn.datasets._arff_parser.X->X.tocsr().tocsr()
A:sklearn.datasets._arff_parser.single_quote_pattern->re.compile("^'(?P<contents>.*)'$")
A:sklearn.datasets._arff_parser.match->re.search(single_quote_pattern, input_string)
A:sklearn.datasets._arff_parser.frame[col]->frame[col].cat.rename_categories(strip_single_quotes).cat.rename_categories(strip_single_quotes)
sklearn.datasets._arff_parser._liac_arff_parser(gzip_file,output_arrays_type,openml_columns_info,feature_names_to_select,target_names_to_select,shape=None)
sklearn.datasets._arff_parser._pandas_arff_parser(gzip_file,output_arrays_type,openml_columns_info,feature_names_to_select,target_names_to_select,read_csv_kwargs=None)
sklearn.datasets._arff_parser._post_process_frame(frame,feature_names,target_names)
sklearn.datasets._arff_parser._sparse_data_to_array(arff_data:ArffSparseDataType,include_columns:List)->np.ndarray
sklearn.datasets._arff_parser._split_sparse_columns(arff_data:ArffSparseDataType,include_columns:List)->ArffSparseDataType
sklearn.datasets._arff_parser.load_arff_from_gzip_file(gzip_file,parser,output_type,openml_columns_info,feature_names_to_select,target_names_to_select,shape=None,read_csv_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_species_distributions.py----------------------------------------
A:sklearn.datasets._species_distributions.SAMPLES->RemoteFileMetadata(filename='samples.zip', url='https://ndownloader.figshare.com/files/5976075', checksum='abb07ad284ac50d9e6d20f1c4211e0fd3c098f7f85955e89d321ee8efe37ac28')
A:sklearn.datasets._species_distributions.COVERAGES->RemoteFileMetadata(filename='coverages.zip', url='https://ndownloader.figshare.com/files/5976078', checksum='4d862674d72e79d6cee77e63b98651ec7926043ba7d39dcb31329cf3f6073807')
A:sklearn.datasets._species_distributions.logger->logging.getLogger(__name__)
A:sklearn.datasets._species_distributions.header->dict([make_tuple(line) for line in header])
A:sklearn.datasets._species_distributions.M->numpy.loadtxt(F, dtype=dtype)
A:sklearn.datasets._species_distributions.nodata->int(header[b'NODATA_value'])
A:sklearn.datasets._species_distributions.names->F.readline().decode('ascii').strip().split(',')
A:sklearn.datasets._species_distributions.rec->numpy.loadtxt(F, skiprows=0, delimiter=',', dtype='a22,f4,f4')
A:sklearn.datasets._species_distributions.xgrid->numpy.arange(xmin, xmax, batch.grid_size)
A:sklearn.datasets._species_distributions.ygrid->numpy.arange(ymin, ymax, batch.grid_size)
A:sklearn.datasets._species_distributions.data_home->get_data_home(data_home)
A:sklearn.datasets._species_distributions.extra_params->dict(x_left_lower_corner=-94.8, Nx=1212, y_left_lower_corner=-56.05, Ny=1592, grid_size=0.05)
A:sklearn.datasets._species_distributions.archive_path->_pkl_filepath(data_home, DATA_ARCHIVE_NAME)
A:sklearn.datasets._species_distributions.samples_path->_fetch_remote(SAMPLES, dirname=data_home)
A:sklearn.datasets._species_distributions.fhandle->BytesIO(X[f])
A:sklearn.datasets._species_distributions.train->_load_csv(fhandle)
A:sklearn.datasets._species_distributions.test->_load_csv(fhandle)
A:sklearn.datasets._species_distributions.coverages_path->_fetch_remote(COVERAGES, dirname=data_home)
A:sklearn.datasets._species_distributions.coverages->numpy.asarray(coverages, dtype=dtype)
A:sklearn.datasets._species_distributions.bunch->joblib.load(archive_path)
sklearn.datasets._species_distributions._load_coverage(F,header_length=6,dtype=np.int16)
sklearn.datasets._species_distributions._load_csv(F)
sklearn.datasets._species_distributions.construct_grids(batch)
sklearn.datasets._species_distributions.fetch_species_distributions(*,data_home=None,download_if_missing=True)
sklearn.datasets.fetch_species_distributions(*,data_home=None,download_if_missing=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_rcv1.py----------------------------------------
A:sklearn.datasets._rcv1.TOPICS_METADATA->RemoteFileMetadata(url='https://ndownloader.figshare.com/files/5976048', checksum='2a98e5e5d8b770bded93afc8930d88299474317fe14181aee1466cc754d0d1c1', filename='rcv1v2.topics.qrels.gz')
A:sklearn.datasets._rcv1.logger->logging.getLogger(__name__)
A:sklearn.datasets._rcv1.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._rcv1.rcv1_dir->join(data_home, 'RCV1')
A:sklearn.datasets._rcv1.samples_path->_pkl_filepath(rcv1_dir, 'samples.pkl')
A:sklearn.datasets._rcv1.sample_id_path->_pkl_filepath(rcv1_dir, 'sample_id.pkl')
A:sklearn.datasets._rcv1.sample_topics_path->_pkl_filepath(rcv1_dir, 'sample_topics.pkl')
A:sklearn.datasets._rcv1.topics_path->_pkl_filepath(rcv1_dir, 'topics_names.pkl')
A:sklearn.datasets._rcv1.file_path->_fetch_remote(each, dirname=rcv1_dir)
A:sklearn.datasets._rcv1.Xy->load_svmlight_files(files, n_features=N_FEATURES)
A:sklearn.datasets._rcv1.X->joblib.load(samples_path)
A:sklearn.datasets._rcv1.sample_id->joblib.load(sample_id_path)
A:sklearn.datasets._rcv1.topics_archive_path->_fetch_remote(TOPICS_METADATA, dirname=rcv1_dir)
A:sklearn.datasets._rcv1.y->joblib.load(sample_topics_path)
A:sklearn.datasets._rcv1.sample_id_bis->numpy.zeros(N_SAMPLES, dtype=np.int32)
A:sklearn.datasets._rcv1.line_components->line.decode('ascii').split(' ')
A:sklearn.datasets._rcv1.doc->int(doc)
A:sklearn.datasets._rcv1.permutation->_find_permutation(sample_id_bis, sample_id)
A:sklearn.datasets._rcv1.categories->joblib.load(topics_path)
A:sklearn.datasets._rcv1.order->numpy.argsort(categories)
A:sklearn.datasets._rcv1.(X, y, sample_id)->shuffle_(X, y, sample_id, random_state=random_state)
A:sklearn.datasets._rcv1.fdescr->load_descr('rcv1.rst')
A:sklearn.datasets._rcv1.s->numpy.zeros(n, dtype=np.int32)
A:sklearn.datasets._rcv1.i->numpy.arange(n, dtype=np.int32)
A:sklearn.datasets._rcv1.t->numpy.argsort(a)
A:sklearn.datasets._rcv1.u->numpy.argsort(b)
A:sklearn.datasets._rcv1.u_->_inverse_permutation(u)
sklearn.datasets._rcv1._find_permutation(a,b)
sklearn.datasets._rcv1._inverse_permutation(p)
sklearn.datasets._rcv1.fetch_rcv1(*,data_home=None,subset='all',download_if_missing=True,random_state=None,shuffle=False,return_X_y=False)
sklearn.datasets.fetch_rcv1(*,data_home=None,subset='all',download_if_missing=True,random_state=None,shuffle=False,return_X_y=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_olivetti_faces.py----------------------------------------
A:sklearn.datasets._olivetti_faces.FACES->RemoteFileMetadata(filename='olivettifaces.mat', url='https://ndownloader.figshare.com/files/5976027', checksum='b612fb967f2dc77c9c62d3e1266e0c73d5fca46a4b8906c18e454d41af987794')
A:sklearn.datasets._olivetti_faces.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._olivetti_faces.filepath->_pkl_filepath(data_home, 'olivetti.pkz')
A:sklearn.datasets._olivetti_faces.mat_path->_fetch_remote(FACES, dirname=data_home)
A:sklearn.datasets._olivetti_faces.mfile->loadmat(file_name=mat_path)
A:sklearn.datasets._olivetti_faces.faces->faces.reshape((400, 64, 64)).transpose(0, 2, 1).reshape((400, 64, 64)).transpose(0, 2, 1)
A:sklearn.datasets._olivetti_faces.target->numpy.array([i // 10 for i in range(400)])
A:sklearn.datasets._olivetti_faces.random_state->check_random_state(random_state)
A:sklearn.datasets._olivetti_faces.order->check_random_state(random_state).permutation(len(faces))
A:sklearn.datasets._olivetti_faces.faces_vectorized->faces.reshape((400, 64, 64)).transpose(0, 2, 1).reshape((400, 64, 64)).transpose(0, 2, 1).reshape(len(faces), -1)
A:sklearn.datasets._olivetti_faces.fdescr->load_descr('olivetti_faces.rst')
sklearn.datasets._olivetti_faces.fetch_olivetti_faces(*,data_home=None,shuffle=False,random_state=0,download_if_missing=True,return_X_y=False)
sklearn.datasets.fetch_olivetti_faces(*,data_home=None,shuffle=False,random_state=0,download_if_missing=True,return_X_y=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_openml.py----------------------------------------
A:sklearn.datasets._openml.local_path->_get_local_path(openml_path, data_home)
A:sklearn.datasets._openml.req->Request(_OPENML_PREFIX + openml_path)
A:sklearn.datasets._openml.fsrc->_retry_on_network_error(n_retries, delay, req.full_url)(urlopen)(req)
A:sklearn.datasets._openml.(dir_name, file_name)->os.path.split(local_path)
A:sklearn.datasets._openml.error_msg->'Dataset {} with version {} not found.'.format(name, version)
A:sklearn.datasets._openml.json_data->_get_json_content_from_openml_api(url, error_message, data_home=data_home, n_retries=n_retries, delay=delay)
A:sklearn.datasets._openml.url->_DATA_FILE.format(data_description['file_id'])
A:sklearn.datasets._openml.error_message->'Dataset with data_id {} not found.'.format(data_id)
A:sklearn.datasets._openml.gzip_file->_open_openml_url(url, data_home, n_retries=n_retries, delay=delay)
A:sklearn.datasets._openml.md5->hashlib.md5()
A:sklearn.datasets._openml.actual_md5_checksum->hashlib.md5().hexdigest()
A:sklearn.datasets._openml.(X, y, frame, categories)->_retry_with_clean_cache(url, data_home, no_retry_exception)(_load_arff_response)(url, data_home, parser=parser, output_type=output_type, openml_columns_info=features_dict, feature_names_to_select=data_columns, target_names_to_select=target_columns, shape=shape, md5_checksum=md5_checksum, n_retries=n_retries, delay=delay, read_csv_kwargs=read_csv_kwargs)
A:sklearn.datasets._openml.n_missing_values->int(column_info['number_of_missing_values'])
A:sklearn.datasets._openml.found_types->set()
A:sklearn.datasets._openml.data_home->join(str(data_home), 'openml')
A:sklearn.datasets._openml.name->name.lower().lower()
A:sklearn.datasets._openml.data_info->_get_data_info_by_name(name, version, data_home, n_retries=n_retries, delay=delay)
A:sklearn.datasets._openml.data_description->_get_data_description_by_id(data_id, data_home)
A:sklearn.datasets._openml.features_list->_get_data_features(data_id, data_home)
A:sklearn.datasets._openml.data_columns->_valid_data_column_names(features_list, target_columns)
A:sklearn.datasets._openml.data_qualities->_get_data_qualities(data_id, data_home)
A:sklearn.datasets._openml.bunch->_download_data_to_bunch(url, return_sparse, data_home, as_frame=bool(as_frame), openml_columns_info=features_list, shape=shape, target_columns=target_columns, data_columns=data_columns, md5_checksum=data_description['md5_checksum'], n_retries=n_retries, delay=delay, parser=parser_, read_csv_kwargs=read_csv_kwargs)
A:sklearn.datasets._openml.description->'{}\n\nDownloaded from openml.org.'.format(data_description.pop('description'))
sklearn.datasets._openml.OpenMLError(ValueError)
sklearn.datasets._openml._download_data_to_bunch(url:str,sparse:bool,data_home:Optional[str],*,as_frame:bool,openml_columns_info:List[dict],data_columns:List[str],target_columns:List[str],shape:Optional[Tuple[int,int]],md5_checksum:str,n_retries:int=3,delay:float=1.0,parser:str,read_csv_kwargs:Optional[Dict]=None)
sklearn.datasets._openml._get_data_description_by_id(data_id:int,data_home:Optional[str],n_retries:int=3,delay:float=1.0)->Dict[str, Any]
sklearn.datasets._openml._get_data_features(data_id:int,data_home:Optional[str],n_retries:int=3,delay:float=1.0)->OpenmlFeaturesType
sklearn.datasets._openml._get_data_info_by_name(name:str,version:Union[int,str],data_home:Optional[str],n_retries:int=3,delay:float=1.0)
sklearn.datasets._openml._get_data_qualities(data_id:int,data_home:Optional[str],n_retries:int=3,delay:float=1.0)->OpenmlQualitiesType
sklearn.datasets._openml._get_json_content_from_openml_api(url:str,error_message:Optional[str],data_home:Optional[str],n_retries:int=3,delay:float=1.0)->Dict
sklearn.datasets._openml._get_local_path(openml_path:str,data_home:str)->str
sklearn.datasets._openml._get_num_samples(data_qualities:OpenmlQualitiesType)->int
sklearn.datasets._openml._load_arff_response(url:str,data_home:Optional[str],parser:str,output_type:str,openml_columns_info:dict,feature_names_to_select:List[str],target_names_to_select:List[str],shape:Optional[Tuple[int,int]],md5_checksum:str,n_retries:int=3,delay:float=1.0,read_csv_kwargs:Optional[Dict]=None)
sklearn.datasets._openml._open_openml_url(openml_path:str,data_home:Optional[str],n_retries:int=3,delay:float=1.0)
sklearn.datasets._openml._retry_on_network_error(n_retries:int=3,delay:float=1.0,url:str='')->Callable
sklearn.datasets._openml._retry_with_clean_cache(openml_path:str,data_home:Optional[str],no_retry_exception:Optional[Exception]=None)->Callable
sklearn.datasets._openml._valid_data_column_names(features_list,target_columns)
sklearn.datasets._openml._verify_target_data_type(features_dict,target_columns)
sklearn.datasets._openml.fetch_openml(name:Optional[str]=None,*,version:Union[str,int]='active',data_id:Optional[int]=None,data_home:Optional[Union[str,os.PathLike]]=None,target_column:Optional[Union[str,List]]='default-target',cache:bool=True,return_X_y:bool=False,as_frame:Union[str,bool]='auto',n_retries:int=3,delay:float=1.0,parser:str='auto',read_csv_kwargs:Optional[Dict]=None)
sklearn.datasets.fetch_openml(name:Optional[str]=None,*,version:Union[str,int]='active',data_id:Optional[int]=None,data_home:Optional[Union[str,os.PathLike]]=None,target_column:Optional[Union[str,List]]='default-target',cache:bool=True,return_X_y:bool=False,as_frame:Union[str,bool]='auto',n_retries:int=3,delay:float=1.0,parser:str='auto',read_csv_kwargs:Optional[Dict]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_covtype.py----------------------------------------
A:sklearn.datasets._covtype.ARCHIVE->RemoteFileMetadata(filename='covtype.data.gz', url='https://ndownloader.figshare.com/files/5976039', checksum='614360d0257557dd1792834a85a1cdebfadc3c4f30b011d56afee7ffb5b15771')
A:sklearn.datasets._covtype.logger->logging.getLogger(__name__)
A:sklearn.datasets._covtype.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._covtype.covtype_dir->join(data_home, 'covertype')
A:sklearn.datasets._covtype.samples_path->_pkl_filepath(covtype_dir, 'samples')
A:sklearn.datasets._covtype.targets_path->_pkl_filepath(covtype_dir, 'targets')
A:sklearn.datasets._covtype.archive_path->_fetch_remote(ARCHIVE, dirname=temp_dir)
A:sklearn.datasets._covtype.Xy->numpy.genfromtxt(GzipFile(filename=archive_path), delimiter=',')
A:sklearn.datasets._covtype.y->joblib.load(targets_path)
A:sklearn.datasets._covtype.samples_tmp_path->_pkl_filepath(temp_dir, 'samples')
A:sklearn.datasets._covtype.targets_tmp_path->_pkl_filepath(temp_dir, 'targets')
A:sklearn.datasets._covtype.X->joblib.load(samples_path)
A:sklearn.datasets._covtype.ind->numpy.arange(X.shape[0])
A:sklearn.datasets._covtype.rng->check_random_state(random_state)
A:sklearn.datasets._covtype.fdescr->load_descr('covtype.rst')
A:sklearn.datasets._covtype.(frame, X, y)->_convert_data_dataframe(caller_name='fetch_covtype', data=X, target=y, feature_names=FEATURE_NAMES, target_names=TARGET_NAMES)
sklearn.datasets._covtype.fetch_covtype(*,data_home=None,download_if_missing=True,random_state=None,shuffle=False,return_X_y=False,as_frame=False)
sklearn.datasets.fetch_covtype(*,data_home=None,download_if_missing=True,random_state=None,shuffle=False,return_X_y=False,as_frame=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_twenty_newsgroups.py----------------------------------------
A:sklearn.datasets._twenty_newsgroups.logger->logging.getLogger(__name__)
A:sklearn.datasets._twenty_newsgroups.ARCHIVE->RemoteFileMetadata(filename='20news-bydate.tar.gz', url='https://ndownloader.figshare.com/files/5975967', checksum='8f1b2514ca22a5ade8fbb9cfa5727df95fa587f4c87b786e15c759fa66d95610')
A:sklearn.datasets._twenty_newsgroups.train_path->os.path.join(target_dir, TRAIN_FOLDER)
A:sklearn.datasets._twenty_newsgroups.test_path->os.path.join(target_dir, TEST_FOLDER)
A:sklearn.datasets._twenty_newsgroups.archive_path->_fetch_remote(ARCHIVE, dirname=target_dir)
A:sklearn.datasets._twenty_newsgroups.cache->_download_20newsgroups(target_dir=twenty_home, cache_path=cache_path)
A:sklearn.datasets._twenty_newsgroups.compressed_content->f.read()
A:sklearn.datasets._twenty_newsgroups.(_before, _blankline, after)->text.partition('\n\n')
A:sklearn.datasets._twenty_newsgroups._QUOTE_RE->re.compile('(writes in|writes:|wrote:|says:|said:|^In article|^Quoted from|^\\||^>)')
A:sklearn.datasets._twenty_newsgroups.lines->text.strip().split('\n')
A:sklearn.datasets._twenty_newsgroups.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._twenty_newsgroups.cache_path->_pkl_filepath(data_home, CACHE_NAME)
A:sklearn.datasets._twenty_newsgroups.twenty_home->os.path.join(data_home, '20news_home')
A:sklearn.datasets._twenty_newsgroups.uncompressed_content->codecs.decode(compressed_content, 'zlib_codec')
A:sklearn.datasets._twenty_newsgroups.data_lst->numpy.array(data.data, dtype=object)
A:sklearn.datasets._twenty_newsgroups.target->numpy.concatenate((data_train.target, data_test.target))
A:sklearn.datasets._twenty_newsgroups.filenames->list()
A:sklearn.datasets._twenty_newsgroups.data.target->numpy.searchsorted(labels, data.target)
A:sklearn.datasets._twenty_newsgroups.data.filenames->numpy.array(filenames)
A:sklearn.datasets._twenty_newsgroups.fdescr->load_descr('twenty_newsgroups.rst')
A:sklearn.datasets._twenty_newsgroups.(labels, categories)->zip(*labels)
A:sklearn.datasets._twenty_newsgroups.mask->numpy.isin(data.target, labels)
A:sklearn.datasets._twenty_newsgroups.data.target_names->list(categories)
A:sklearn.datasets._twenty_newsgroups.data.data->numpy.array(data.data, dtype=object).tolist()
A:sklearn.datasets._twenty_newsgroups.random_state->check_random_state(random_state)
A:sklearn.datasets._twenty_newsgroups.indices->numpy.arange(data.target.shape[0])
A:sklearn.datasets._twenty_newsgroups.target_file->_pkl_filepath(data_home, filebase + '.pkl')
A:sklearn.datasets._twenty_newsgroups.data_train->fetch_20newsgroups(data_home=data_home, subset='train', categories=None, shuffle=True, random_state=12, remove=remove, download_if_missing=download_if_missing)
A:sklearn.datasets._twenty_newsgroups.data_test->fetch_20newsgroups(data_home=data_home, subset='test', categories=None, shuffle=True, random_state=12, remove=remove, download_if_missing=download_if_missing)
A:sklearn.datasets._twenty_newsgroups.(X_train, X_test, feature_names)->joblib.load(target_file)
A:sklearn.datasets._twenty_newsgroups.vectorizer->CountVectorizer(dtype=np.int16)
A:sklearn.datasets._twenty_newsgroups.X_train->X_train.astype(np.float64).astype(np.float64)
A:sklearn.datasets._twenty_newsgroups.X_test->X_test.astype(np.float64).astype(np.float64)
A:sklearn.datasets._twenty_newsgroups.feature_names->CountVectorizer(dtype=np.int16).get_feature_names_out()
A:sklearn.datasets._twenty_newsgroups.data->scipy.sparse.vstack((X_train, X_test)).tocsr()
A:sklearn.datasets._twenty_newsgroups.(frame, data, target)->_convert_data_dataframe('fetch_20newsgroups_vectorized', data, target, feature_names, target_names=target_name, sparse_data=True)
sklearn.datasets._twenty_newsgroups._download_20newsgroups(target_dir,cache_path)
sklearn.datasets._twenty_newsgroups.fetch_20newsgroups(*,data_home=None,subset='train',categories=None,shuffle=True,random_state=42,remove=(),download_if_missing=True,return_X_y=False)
sklearn.datasets._twenty_newsgroups.fetch_20newsgroups_vectorized(*,subset='train',remove=(),data_home=None,download_if_missing=True,return_X_y=False,normalize=True,as_frame=False)
sklearn.datasets._twenty_newsgroups.strip_newsgroup_footer(text)
sklearn.datasets._twenty_newsgroups.strip_newsgroup_header(text)
sklearn.datasets._twenty_newsgroups.strip_newsgroup_quoting(text)
sklearn.datasets.fetch_20newsgroups(*,data_home=None,subset='train',categories=None,shuffle=True,random_state=42,remove=(),download_if_missing=True,return_X_y=False)
sklearn.datasets.fetch_20newsgroups_vectorized(*,subset='train',remove=(),data_home=None,download_if_missing=True,return_X_y=False,normalize=True,as_frame=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_lfw.py----------------------------------------
A:sklearn.datasets._lfw.logger->logging.getLogger(__name__)
A:sklearn.datasets._lfw.ARCHIVE->RemoteFileMetadata(filename='lfw.tgz', url='https://ndownloader.figshare.com/files/5976018', checksum='055f7d9c632d7370e6fb4afc7468d40f970c34a80d4c6f50ffec63f5a8d536c0')
A:sklearn.datasets._lfw.FUNNELED_ARCHIVE->RemoteFileMetadata(filename='lfw-funneled.tgz', url='https://ndownloader.figshare.com/files/5976015', checksum='b47c8422c8cded889dc5a13418c4bc2abbda121092b3533a83306f90d900100a')
A:sklearn.datasets._lfw.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._lfw.lfw_home->join(data_home, 'lfw_home')
A:sklearn.datasets._lfw.target_filepath->join(lfw_home, target.filename)
A:sklearn.datasets._lfw.data_folder_path->join(lfw_home, 'lfw')
A:sklearn.datasets._lfw.archive_path->join(lfw_home, archive.filename)
A:sklearn.datasets._lfw.slice_->tuple((s or ds for (s, ds) in zip(slice_, default_slice)))
A:sklearn.datasets._lfw.resize->float(resize)
A:sklearn.datasets._lfw.h->int(resize * h)
A:sklearn.datasets._lfw.w->int(resize * w)
A:sklearn.datasets._lfw.n_faces->list(pairs.shape).pop(0)
A:sklearn.datasets._lfw.faces->_load_imgs(file_paths, slice_, color, resize)
A:sklearn.datasets._lfw.pil_img->pil_img.resize((w, h)).resize((w, h))
A:sklearn.datasets._lfw.face->face.mean(axis=2).mean(axis=2)
A:sklearn.datasets._lfw.folder_path->join(data_folder_path, person_name)
A:sklearn.datasets._lfw.n_pictures->len(paths)
A:sklearn.datasets._lfw.person_name->person_name.replace('_', ' ').replace('_', ' ')
A:sklearn.datasets._lfw.target_names->numpy.unique(person_names)
A:sklearn.datasets._lfw.target->numpy.zeros(n_pairs, dtype=int)
A:sklearn.datasets._lfw.indices->numpy.arange(n_faces)
A:sklearn.datasets._lfw.(lfw_home, data_folder_path)->_check_fetch_lfw(data_home=data_home, funneled=funneled, download_if_missing=download_if_missing)
A:sklearn.datasets._lfw.m->Memory(location=lfw_home, compress=6, verbose=0)
A:sklearn.datasets._lfw.load_func->Memory(location=lfw_home, compress=6, verbose=0).cache(_fetch_lfw_pairs)
A:sklearn.datasets._lfw.(faces, target, target_names)->load_func(data_folder_path, resize=resize, min_faces_per_person=min_faces_per_person, color=color, slice_=slice_)
A:sklearn.datasets._lfw.X->_load_imgs(file_paths, slice_, color, resize).reshape(len(faces), -1)
A:sklearn.datasets._lfw.fdescr->load_descr('lfw.rst')
A:sklearn.datasets._lfw.n_pairs->len(pair_specs)
A:sklearn.datasets._lfw.file_paths->list()
A:sklearn.datasets._lfw.person_folder->join(data_folder_path, str(name, 'UTF-8'))
A:sklearn.datasets._lfw.filenames->list(sorted(listdir(person_folder)))
A:sklearn.datasets._lfw.file_path->join(person_folder, filenames[idx])
A:sklearn.datasets._lfw.pairs->_load_imgs(file_paths, slice_, color, resize)
A:sklearn.datasets._lfw.shape->list(pairs.shape)
A:sklearn.datasets._lfw.index_file_path->join(lfw_home, label_filenames[subset])
A:sklearn.datasets._lfw.(pairs, target, target_names)->load_func(index_file_path, data_folder_path, resize=resize, color=color, slice_=slice_)
sklearn.datasets._lfw._check_fetch_lfw(data_home=None,funneled=True,download_if_missing=True)
sklearn.datasets._lfw._fetch_lfw_pairs(index_file_path,data_folder_path,slice_=None,color=False,resize=None)
sklearn.datasets._lfw._fetch_lfw_people(data_folder_path,slice_=None,color=False,resize=None,min_faces_per_person=0)
sklearn.datasets._lfw._load_imgs(file_paths,slice_,color,resize)
sklearn.datasets._lfw.fetch_lfw_pairs(*,subset='train',data_home=None,funneled=True,resize=0.5,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets._lfw.fetch_lfw_people(*,data_home=None,funneled=True,resize=0.5,min_faces_per_person=0,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True,return_X_y=False)
sklearn.datasets.fetch_lfw_pairs(*,subset='train',data_home=None,funneled=True,resize=0.5,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True)
sklearn.datasets.fetch_lfw_people(*,data_home=None,funneled=True,resize=0.5,min_faces_per_person=0,color=False,slice_=(slice(70,195),slice(78,172)),download_if_missing=True,return_X_y=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_base.py----------------------------------------
A:sklearn.datasets._base.RemoteFileMetadata->namedtuple('RemoteFileMetadata', ['filename', 'url', 'checksum'])
A:sklearn.datasets._base.data_home->get_data_home(data_home)
A:sklearn.datasets._base.pd->check_pandas_support('{} with as_frame=True'.format(caller_name))
A:sklearn.datasets._base.data_df->check_pandas_support('{} with as_frame=True'.format(caller_name)).DataFrame.sparse.from_spmatrix(data, columns=feature_names)
A:sklearn.datasets._base.target_df->check_pandas_support('{} with as_frame=True'.format(caller_name)).DataFrame(target, columns=target_names)
A:sklearn.datasets._base.combined_df->check_pandas_support('{} with as_frame=True'.format(caller_name)).concat([data_df, target_df], axis=1)
A:sklearn.datasets._base.allowed_extensions->frozenset(allowed_extensions)
A:sklearn.datasets._base.folder_path->join(container_path, folder)
A:sklearn.datasets._base.files->sorted(listdir(folder_path))
A:sklearn.datasets._base.filenames->numpy.array(filenames)
A:sklearn.datasets._base.target->load_gzip_compressed_csv_data(target_filename)
A:sklearn.datasets._base.random_state->check_random_state(random_state)
A:sklearn.datasets._base.indices->numpy.arange(filenames.shape[0])
A:sklearn.datasets._base.data_file->csv.reader(csv_file)
A:sklearn.datasets._base.temp->next(data_file)
A:sklearn.datasets._base.n_samples->int(temp[0])
A:sklearn.datasets._base.n_features->int(temp[1])
A:sklearn.datasets._base.target_names->numpy.array(temp[2:])
A:sklearn.datasets._base.data->scale(data, copy=False)
A:sklearn.datasets._base.data[i]->numpy.asarray(ir[:-1], dtype=np.float64)
A:sklearn.datasets._base.target[i]->numpy.asarray(ir[-1], dtype=int)
A:sklearn.datasets._base.descr->load_descr('README.txt', descr_module=IMAGES_MODULE)
A:sklearn.datasets._base.compressed_file->gzip.open(compressed_file, mode='rt', encoding=encoding)
A:sklearn.datasets._base.(data, target, target_names, fdescr)->load_csv_data(data_file_name=data_file_name, descr_file_name='breast_cancer.rst')
A:sklearn.datasets._base.(frame, data, target)->_convert_data_dataframe('load_diabetes', data, target, feature_names, target_columns)
A:sklearn.datasets._base.feature_names->numpy.array(['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension'])
A:sklearn.datasets._base.(data, fdescr)->load_gzip_compressed_csv_data(data_file_name='digits.csv.gz', descr_file_name='digits.rst', delimiter=',')
A:sklearn.datasets._base.images->load_sample_images()
A:sklearn.datasets._base.(frame, flat_data, target)->_convert_data_dataframe('load_digits', flat_data, target, feature_names, target_columns)
A:sklearn.datasets._base.fdescr->load_descr('linnerud.rst')
A:sklearn.datasets._base.data_module_path->importlib.resources.files(DATA_MODULE)
A:sklearn.datasets._base.header_exercise->f.readline().split()
A:sklearn.datasets._base.data_exercise->numpy.loadtxt(f, skiprows=1)
A:sklearn.datasets._base.header_physiological->f.readline().split()
A:sklearn.datasets._base.data_physiological->numpy.loadtxt(f, skiprows=1)
A:sklearn.datasets._base.(frame, data_exercise, data_physiological)->_convert_data_dataframe('load_linnerud', data_exercise, data_physiological, header_exercise, header_physiological)
A:sklearn.datasets._base.jpg_paths->sorted((resource for resource in resources.files(IMAGES_MODULE).iterdir() if resource.is_file() and resource.match('*.jpg')))
A:sklearn.datasets._base.pil_image->PIL.Image.open(image_file)
A:sklearn.datasets._base.image->numpy.asarray(pil_image)
A:sklearn.datasets._base.py3_suffix->kwargs.get('py3_suffix', '_py3')
A:sklearn.datasets._base.(basename, ext)->splitext(args[-1])
A:sklearn.datasets._base.sha256hash->hashlib.sha256()
A:sklearn.datasets._base.buffer->f.read(chunk_size)
A:sklearn.datasets._base.checksum->_sha256(file_path)
sklearn.datasets._base._convert_data_dataframe(caller_name,data,target,feature_names,target_names,sparse_data=False)
sklearn.datasets._base._fetch_remote(remote,dirname=None)
sklearn.datasets._base._pkl_filepath(*args,**kwargs)
sklearn.datasets._base._sha256(path)
sklearn.datasets._base.clear_data_home(data_home=None)
sklearn.datasets._base.get_data_home(data_home=None)->str
sklearn.datasets._base.load_breast_cancer(*,return_X_y=False,as_frame=False)
sklearn.datasets._base.load_csv_data(data_file_name,*,data_module=DATA_MODULE,descr_file_name=None,descr_module=DESCR_MODULE,encoding='utf-8')
sklearn.datasets._base.load_descr(descr_file_name,*,descr_module=DESCR_MODULE,encoding='utf-8')
sklearn.datasets._base.load_diabetes(*,return_X_y=False,as_frame=False,scaled=True)
sklearn.datasets._base.load_digits(*,n_class=10,return_X_y=False,as_frame=False)
sklearn.datasets._base.load_files(container_path,*,description=None,categories=None,load_content=True,shuffle=True,encoding=None,decode_error='strict',random_state=0,allowed_extensions=None)
sklearn.datasets._base.load_gzip_compressed_csv_data(data_file_name,*,data_module=DATA_MODULE,descr_file_name=None,descr_module=DESCR_MODULE,encoding='utf-8',**kwargs)
sklearn.datasets._base.load_iris(*,return_X_y=False,as_frame=False)
sklearn.datasets._base.load_linnerud(*,return_X_y=False,as_frame=False)
sklearn.datasets._base.load_sample_image(image_name)
sklearn.datasets._base.load_sample_images()
sklearn.datasets._base.load_wine(*,return_X_y=False,as_frame=False)
sklearn.datasets.clear_data_home(data_home=None)
sklearn.datasets.get_data_home(data_home=None)->str
sklearn.datasets.load_breast_cancer(*,return_X_y=False,as_frame=False)
sklearn.datasets.load_diabetes(*,return_X_y=False,as_frame=False,scaled=True)
sklearn.datasets.load_digits(*,n_class=10,return_X_y=False,as_frame=False)
sklearn.datasets.load_files(container_path,*,description=None,categories=None,load_content=True,shuffle=True,encoding=None,decode_error='strict',random_state=0,allowed_extensions=None)
sklearn.datasets.load_iris(*,return_X_y=False,as_frame=False)
sklearn.datasets.load_linnerud(*,return_X_y=False,as_frame=False)
sklearn.datasets.load_sample_image(image_name)
sklearn.datasets.load_sample_images()
sklearn.datasets.load_wine(*,return_X_y=False,as_frame=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_kddcup99.py----------------------------------------
A:sklearn.datasets._kddcup99.ARCHIVE->RemoteFileMetadata(filename='kddcup99_data', url='https://ndownloader.figshare.com/files/5976045', checksum='3b6c942aa0356c0ca35b7b595a26c89d343652c9db428893e7494f837b274292')
A:sklearn.datasets._kddcup99.ARCHIVE_10_PERCENT->RemoteFileMetadata(filename='kddcup99_10_data', url='https://ndownloader.figshare.com/files/5976042', checksum='8045aca0d84e70e622d1148d7df782496f6333bf6eb979a1b0837c42a9fd9561')
A:sklearn.datasets._kddcup99.logger->logging.getLogger(__name__)
A:sklearn.datasets._kddcup99.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._kddcup99.kddcup99->_fetch_brute_kddcup99(data_home=data_home, percent10=percent10, download_if_missing=download_if_missing)
A:sklearn.datasets._kddcup99.t->numpy.logical_not(s)
A:sklearn.datasets._kddcup99.random_state->check_random_state(random_state)
A:sklearn.datasets._kddcup99.r->check_random_state(random_state).randint(0, n_samples_abnormal, 3377)
A:sklearn.datasets._kddcup99.data[:, 0]->numpy.log((data[:, 0] + 0.1).astype(float, copy=False))
A:sklearn.datasets._kddcup99.data[:, 4]->numpy.log((data[:, 4] + 0.1).astype(float, copy=False))
A:sklearn.datasets._kddcup99.data[:, 5]->numpy.log((data[:, 5] + 0.1).astype(float, copy=False))
A:sklearn.datasets._kddcup99.(data, target)->shuffle_method(data, target, random_state=random_state)
A:sklearn.datasets._kddcup99.fdescr->load_descr('kddcup99.rst')
A:sklearn.datasets._kddcup99.(frame, data, target)->_convert_data_dataframe('fetch_kddcup99', data, target, feature_names, target_names)
A:sklearn.datasets._kddcup99.kddcup_dir->join(data_home, 'kddcup99' + dir_suffix)
A:sklearn.datasets._kddcup99.samples_path->join(kddcup_dir, 'samples')
A:sklearn.datasets._kddcup99.targets_path->join(kddcup_dir, 'targets')
A:sklearn.datasets._kddcup99.available->exists(samples_path)
A:sklearn.datasets._kddcup99.X->joblib.load(samples_path)
A:sklearn.datasets._kddcup99.y->joblib.load(targets_path)
A:sklearn.datasets._kddcup99.DT->numpy.dtype(dt)
A:sklearn.datasets._kddcup99.archive_path->join(kddcup_dir, archive.filename)
A:sklearn.datasets._kddcup99.file_->GzipFile(filename=archive_path, mode='r')
A:sklearn.datasets._kddcup99.line->line.decode().decode()
A:sklearn.datasets._kddcup99.Xy->numpy.asarray(Xy, dtype=object)
A:sklearn.datasets._kddcup99.Xy[:, j]->Xy[:, j].astype(DT[j]).astype(DT[j])
sklearn.datasets._kddcup99._fetch_brute_kddcup99(data_home=None,download_if_missing=True,percent10=True)
sklearn.datasets._kddcup99._mkdirp(d)
sklearn.datasets._kddcup99.fetch_kddcup99(*,subset=None,data_home=None,shuffle=False,random_state=None,percent10=True,download_if_missing=True,return_X_y=False,as_frame=False)
sklearn.datasets.fetch_kddcup99(*,subset=None,data_home=None,shuffle=False,random_state=None,percent10=True,download_if_missing=True,return_X_y=False,as_frame=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_california_housing.py----------------------------------------
A:sklearn.datasets._california_housing.ARCHIVE->RemoteFileMetadata(filename='cal_housing.tgz', url='https://ndownloader.figshare.com/files/5976036', checksum='aaa5c9a6afe2225cc2aed2723682ae403280c4a3695a2ddda4ffb5d8215ea681')
A:sklearn.datasets._california_housing.logger->logging.getLogger(__name__)
A:sklearn.datasets._california_housing.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets._california_housing.filepath->_pkl_filepath(data_home, 'cal_housing.pkz')
A:sklearn.datasets._california_housing.archive_path->_fetch_remote(ARCHIVE, dirname=data_home)
A:sklearn.datasets._california_housing.cal_housing->joblib.load(filepath)
A:sklearn.datasets._california_housing.descr->load_descr('california_housing.rst')
A:sklearn.datasets._california_housing.(frame, X, y)->_convert_data_dataframe('fetch_california_housing', data, target, feature_names, target_names)
sklearn.datasets._california_housing.fetch_california_housing(*,data_home=None,download_if_missing=True,return_X_y=False,as_frame=False)
sklearn.datasets.fetch_california_housing(*,data_home=None,download_if_missing=True,return_X_y=False,as_frame=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/_svmlight_format_io.py----------------------------------------
A:sklearn.datasets._svmlight_format_io.f->os.fspath(f)
A:sklearn.datasets._svmlight_format_io.(_, ext)->os.path.splitext(f)
A:sklearn.datasets._svmlight_format_io.(actual_dtype, data, ind, indptr, labels, query)->_load_svmlight_file(f, dtype, multilabel, zero_based, query_id, offset, length)
A:sklearn.datasets._svmlight_format_io.labels->numpy.frombuffer(labels, np.float64)
A:sklearn.datasets._svmlight_format_io.data->numpy.asarray(data, dtype=dtype)
A:sklearn.datasets._svmlight_format_io.indices->numpy.frombuffer(ind, np.longlong)
A:sklearn.datasets._svmlight_format_io.indptr->numpy.frombuffer(indptr, dtype=np.longlong)
A:sklearn.datasets._svmlight_format_io.query->numpy.frombuffer(query, np.int64)
A:sklearn.datasets._svmlight_format_io.X->check_array(X, accept_sparse='csr').sorted_indices()
A:sklearn.datasets._svmlight_format_io.X_is_sp->scipy.sparse.issparse(X)
A:sklearn.datasets._svmlight_format_io.y_is_sp->scipy.sparse.issparse(y)
A:sklearn.datasets._svmlight_format_io.comment->comment.encode('utf-8').encode('utf-8')
A:sklearn.datasets._svmlight_format_io.yval->check_array(y, accept_sparse='csr', ensure_2d=False)
A:sklearn.datasets._svmlight_format_io.Xval->check_array(X, accept_sparse='csr')
A:sklearn.datasets._svmlight_format_io.y->check_array(y, accept_sparse='csr', ensure_2d=False).sorted_indices()
A:sklearn.datasets._svmlight_format_io.query_id->numpy.asarray(query_id)
sklearn.datasets._svmlight_format_io._dump_svmlight(X,y,f,multilabel,one_based,comment,query_id)
sklearn.datasets._svmlight_format_io._gen_open(f)
sklearn.datasets._svmlight_format_io._open_and_load(f,dtype,multilabel,zero_based,query_id,offset=0,length=-1)
sklearn.datasets._svmlight_format_io.dump_svmlight_file(X,y,f,*,zero_based=True,comment=None,query_id=None,multilabel=False)
sklearn.datasets._svmlight_format_io.load_svmlight_file(f,*,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets._svmlight_format_io.load_svmlight_files(files,*,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets.dump_svmlight_file(X,y,f,*,zero_based=True,comment=None,query_id=None,multilabel=False)
sklearn.datasets.load_svmlight_file(f,*,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)
sklearn.datasets.load_svmlight_files(files,*,n_features=None,dtype=np.float64,multilabel=False,zero_based='auto',query_id=False,offset=0,length=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_base.py----------------------------------------
A:sklearn.datasets.tests.test_base.tmp_file->str(tmpdir_factory.mktemp('scikit_learn_load_files_test'))
A:sklearn.datasets.tests.test_base.test_category_dir1->tempfile.mkdtemp(dir=load_files_root)
A:sklearn.datasets.tests.test_base.sample_file->tempfile.NamedTemporaryFile(dir=test_category_dir1, delete=False)
A:sklearn.datasets.tests.test_base.test_category_dir2->tempfile.mkdtemp(dir=load_files_root)
A:sklearn.datasets.tests.test_base.data_home->get_data_home(data_home=data_home)
A:sklearn.datasets.tests.test_base.res->load_sample_images()
A:sklearn.datasets.tests.test_base.category->os.path.abspath(test_category_dir_1).split(os.sep).pop()
A:sklearn.datasets.tests.test_base.(actual_data, actual_target, actual_target_names)->load_csv_data(filename)
A:sklearn.datasets.tests.test_base.res_without_descr->load_csv_data(data_file_name=data_file_name)
A:sklearn.datasets.tests.test_base.res_with_descr->load_csv_data(data_file_name=data_file_name, descr_file_name=descr_file_name)
A:sklearn.datasets.tests.test_base.actual_data->load_gzip_compressed_csv_data(filename, **kwargs)
A:sklearn.datasets.tests.test_base.expected_data->load_gzip_compressed_csv_data(data_file_name=data_file_name)
A:sklearn.datasets.tests.test_base.(actual_data, descr)->load_gzip_compressed_csv_data(data_file_name=data_file_name, descr_file_name=descr_file_name)
A:sklearn.datasets.tests.test_base.china->load_sample_image('china.jpg')
A:sklearn.datasets.tests.test_base.diabetes_raw->load_diabetes(scaled=False)
A:sklearn.datasets.tests.test_base.diabetes_default->load_diabetes()
A:sklearn.datasets.tests.test_base.bunch->Bunch(key='original')
A:sklearn.datasets.tests.test_base.default_result->loader_func()
A:sklearn.datasets.tests.test_base.bunch_from_pkl->loads(dumps(bunch))
A:sklearn.datasets.tests.test_base.data->load_iris()
sklearn.datasets.tests.test_base._DummyPath(self,path)
sklearn.datasets.tests.test_base._DummyPath.__fspath__(self)
sklearn.datasets.tests.test_base._DummyPath.__init__(self,path)
sklearn.datasets.tests.test_base._remove_dir(path)
sklearn.datasets.tests.test_base.data_home(tmpdir_factory)
sklearn.datasets.tests.test_base.load_files_root(tmpdir_factory)
sklearn.datasets.tests.test_base.test_bunch_dir()
sklearn.datasets.tests.test_base.test_bunch_pickle_generated_with_0_16_and_read_with_0_17()
sklearn.datasets.tests.test_base.test_category_dir_1(load_files_root)
sklearn.datasets.tests.test_base.test_category_dir_2(load_files_root)
sklearn.datasets.tests.test_base.test_data_home(path_container,data_home)
sklearn.datasets.tests.test_base.test_default_empty_load_files(load_files_root)
sklearn.datasets.tests.test_base.test_default_load_files(test_category_dir_1,test_category_dir_2,load_files_root)
sklearn.datasets.tests.test_base.test_load_boston_error()
sklearn.datasets.tests.test_base.test_load_csv_data(filename,expected_n_samples,expected_n_features,expected_target_names)
sklearn.datasets.tests.test_base.test_load_csv_data_with_descr()
sklearn.datasets.tests.test_base.test_load_diabetes_raw()
sklearn.datasets.tests.test_base.test_load_files_allowed_extensions(tmp_path,allowed_extensions)
sklearn.datasets.tests.test_base.test_load_files_w_categories_desc_and_encoding(test_category_dir_1,test_category_dir_2,load_files_root)
sklearn.datasets.tests.test_base.test_load_files_wo_load_content(test_category_dir_1,test_category_dir_2,load_files_root)
sklearn.datasets.tests.test_base.test_load_gzip_compressed_csv_data(filename,kwargs,expected_shape)
sklearn.datasets.tests.test_base.test_load_gzip_compressed_csv_data_with_descr()
sklearn.datasets.tests.test_base.test_load_sample_image()
sklearn.datasets.tests.test_base.test_load_sample_images()
sklearn.datasets.tests.test_base.test_loader(loader_func,data_shape,target_shape,n_target,has_descr,filenames)
sklearn.datasets.tests.test_base.test_loads_dumps_bunch()
sklearn.datasets.tests.test_base.test_toy_dataset_frame_dtype(loader_func,data_dtype,target_dtype)
sklearn.datasets.tests.testload_files_root(tmpdir_factory)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_rcv1.py----------------------------------------
A:sklearn.datasets.tests.test_rcv1.data1->fetch_rcv1_fxt(shuffle=False)
A:sklearn.datasets.tests.test_rcv1.j->cat_list.index(cat)
A:sklearn.datasets.tests.test_rcv1.data2->fetch_rcv1_fxt(shuffle=True, subset='train', random_state=global_random_seed)
A:sklearn.datasets.tests.test_rcv1.fetch_func->partial(fetch_rcv1_fxt, shuffle=False, subset='train')
A:sklearn.datasets.tests.test_rcv1.idx1->s1.tolist().index(sample_id)
A:sklearn.datasets.tests.test_rcv1.idx2->s2.tolist().index(sample_id)
A:sklearn.datasets.tests.test_rcv1.feature_values_1->X1[idx1, :].toarray()
A:sklearn.datasets.tests.test_rcv1.feature_values_2->X2[idx2, :].toarray()
A:sklearn.datasets.tests.test_rcv1.target_values_1->Y1[idx1, :].toarray()
A:sklearn.datasets.tests.test_rcv1.target_values_2->Y2[idx2, :].toarray()
sklearn.datasets.tests.test_rcv1.test_fetch_rcv1(fetch_rcv1_fxt,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_samples_generator.py----------------------------------------
A:sklearn.datasets.tests.test_samples_generator.(X, y)->make_circles(n_samples=(2, 8))
A:sklearn.datasets.tests.test_samples_generator.make->partial(make_classification, class_sep=class_sep, n_redundant=0, n_repeated=0, flip_y=0, shift=0, scale=1, shuffle=False)
A:sklearn.datasets.tests.test_samples_generator.n_classes->len(weights)
A:sklearn.datasets.tests.test_samples_generator.signs->signs.view(dtype='|S{0}'.format(signs.strides[0])).ravel().view(dtype='|S{0}'.format(signs.strides[0])).ravel()
A:sklearn.datasets.tests.test_samples_generator.(unique_signs, cluster_index)->numpy.unique(signs, return_inverse=True)
A:sklearn.datasets.tests.test_samples_generator.clusters_by_class->defaultdict(set)
A:sklearn.datasets.tests.test_samples_generator.centroid->X[cluster_index == cluster].mean(axis=0)
A:sklearn.datasets.tests.test_samples_generator.(X1, y1)->make_classification(weights=[0.1, 0.9], random_state=0, **kwargs)
A:sklearn.datasets.tests.test_samples_generator.(X2, y2)->make_classification(weights=np.array([0.1, 0.9]), random_state=0, **kwargs)
A:sklearn.datasets.tests.test_samples_generator.(X, Y)->make_multilabel_classification(n_samples=25, n_features=20, n_classes=3, random_state=0, return_indicator='sparse', allow_unlabeled=allow_unlabeled)
A:sklearn.datasets.tests.test_samples_generator.(X2, Y2, p_c, p_w_c)->make_multilabel_classification(n_samples=25, n_features=20, n_classes=3, random_state=0, allow_unlabeled=allow_unlabeled, return_distributions=True)
A:sklearn.datasets.tests.test_samples_generator.(X, y, c)->make_regression(n_samples=100, n_features=10, n_informative=3, n_targets=3, coef=True, noise=1.0, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.cluster_stds->numpy.array([0.05, 0.2, 0.4])
A:sklearn.datasets.tests.test_samples_generator.cluster_centers->numpy.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])
A:sklearn.datasets.tests.test_samples_generator.centers->numpy.array([[0.0, 0.0], [1.0, 1.0], [0.0, 1.0]])
A:sklearn.datasets.tests.test_samples_generator.(X, y, centers)->make_blobs(n_samples=n_samples, n_features=n_features, return_centers=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.wrong_centers_msg->re.escape(f'Length of `n_samples` not consistent with number of centers. Got n_samples = {n_samples} and centers = {centers[:-1]}')
A:sklearn.datasets.tests.test_samples_generator.wrong_std_msg->re.escape(f'Length of `clusters_std` not consistent with number of centers. Got centers = {centers} and cluster_std = {cluster_stds[:-1]}')
A:sklearn.datasets.tests.test_samples_generator.wrong_type_msg->'Parameter `centers` must be array-like. Got {!r} instead'.format(3)
A:sklearn.datasets.tests.test_samples_generator.X->make_sparse_spd_matrix()
A:sklearn.datasets.tests.test_samples_generator.(u, s, v)->svd(X)
A:sklearn.datasets.tests.test_samples_generator.(Y, D, X)->make_sparse_coded_signal(n_samples=5, n_components=8, n_features=10, n_nonzero_coefs=3, random_state=0, data_transposed=True)
A:sklearn.datasets.tests.test_samples_generator.(eigenvalues, _)->eig(Xarr)
A:sklearn.datasets.tests.test_samples_generator.Xarr->make_sparse_spd_matrix().toarray()
A:sklearn.datasets.tests.test_samples_generator.(X, t)->make_s_curve(n_samples=5, noise=0.0, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X, rows, cols)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X2, _, _)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.(X1, _, _)->make_checkerboard(shape=(100, 100), n_clusters=2, shuffle=True, random_state=0)
A:sklearn.datasets.tests.test_samples_generator.dist_sqr->((x - center) ** 2).sum()
sklearn.datasets.tests.test_samples_generator.test_make_biclusters()
sklearn.datasets.tests.test_samples_generator.test_make_blobs()
sklearn.datasets.tests.test_samples_generator.test_make_blobs_error()
sklearn.datasets.tests.test_samples_generator.test_make_blobs_n_samples_centers_none(n_samples)
sklearn.datasets.tests.test_samples_generator.test_make_blobs_n_samples_list()
sklearn.datasets.tests.test_samples_generator.test_make_blobs_n_samples_list_with_centers()
sklearn.datasets.tests.test_samples_generator.test_make_blobs_return_centers()
sklearn.datasets.tests.test_samples_generator.test_make_checkerboard()
sklearn.datasets.tests.test_samples_generator.test_make_circles()
sklearn.datasets.tests.test_samples_generator.test_make_circles_unbalanced()
sklearn.datasets.tests.test_samples_generator.test_make_classification()
sklearn.datasets.tests.test_samples_generator.test_make_classification_informative_features()
sklearn.datasets.tests.test_samples_generator.test_make_classification_weights_array_or_list_ok(kwargs)
sklearn.datasets.tests.test_samples_generator.test_make_classification_weights_type(weights,err_type,err_msg)
sklearn.datasets.tests.test_samples_generator.test_make_friedman1()
sklearn.datasets.tests.test_samples_generator.test_make_friedman2()
sklearn.datasets.tests.test_samples_generator.test_make_friedman3()
sklearn.datasets.tests.test_samples_generator.test_make_hastie_10_2()
sklearn.datasets.tests.test_samples_generator.test_make_low_rank_matrix()
sklearn.datasets.tests.test_samples_generator.test_make_moons()
sklearn.datasets.tests.test_samples_generator.test_make_moons_unbalanced()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_indicator()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_indicator_sparse()
sklearn.datasets.tests.test_samples_generator.test_make_multilabel_classification_return_sequences()
sklearn.datasets.tests.test_samples_generator.test_make_regression()
sklearn.datasets.tests.test_samples_generator.test_make_regression_multitarget()
sklearn.datasets.tests.test_samples_generator.test_make_s_curve()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_code_signal_deprecation_warning()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_coded_signal()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_coded_signal_transposed()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_spd_matrix(norm_diag,sparse_format,global_random_seed)
sklearn.datasets.tests.test_samples_generator.test_make_sparse_spd_matrix_deprecation_warning()
sklearn.datasets.tests.test_samples_generator.test_make_sparse_uncorrelated()
sklearn.datasets.tests.test_samples_generator.test_make_spd_matrix()
sklearn.datasets.tests.test_samples_generator.test_make_swiss_roll(hole)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_arff_parser.py----------------------------------------
A:sklearn.datasets.tests.test_arff_parser.pd->pytest.importorskip('pandas')
A:sklearn.datasets.tests.test_arff_parser.X_original->pytest.importorskip('pandas').DataFrame({'col_int_as_integer': [1, 2, 3], 'col_int_as_numeric': [1, 2, 3], 'col_float_as_real': [1.0, 2.0, 3.0], 'col_float_as_numeric': [1.0, 2.0, 3.0], 'col_categorical': ['a', 'b', 'c'], 'col_string': ['a', 'b', 'c']})
A:sklearn.datasets.tests.test_arff_parser.(X, y)->_post_process_frame(X_original, feature_names, target_names)
A:sklearn.datasets.tests.test_arff_parser.arff_file->BytesIO(textwrap.dedent("\n            @relation 'toy'\n            @attribute 'cat_without_quote' {A, B, C}\n            @attribute 'str_without_quote' string\n            @attribute 'str_internal_quote' string\n            @attribute 'class' numeric\n            @data\n            A,some text,'internal' quote,0\n            ").encode('utf-8'))
A:sklearn.datasets.tests.test_arff_parser.(_, _, frame, _)->parser_func(arff_file, output_arrays_type='pandas', openml_columns_info=columns_info, feature_names_to_select=feature_names, target_names_to_select=target_names)
sklearn.datasets.tests.test_arff_parser.test_load_arff_from_gzip_file_error_parser()
sklearn.datasets.tests.test_arff_parser.test_pandas_arff_parser_strip_double_quotes(parser_func)
sklearn.datasets.tests.test_arff_parser.test_pandas_arff_parser_strip_no_quotes(parser_func)
sklearn.datasets.tests.test_arff_parser.test_pandas_arff_parser_strip_single_quotes(parser_func)
sklearn.datasets.tests.test_arff_parser.test_post_process_frame(feature_names,target_names)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_20news.py----------------------------------------
A:sklearn.datasets.tests.test_20news.data->fetch_20newsgroups_fxt(subset='all')
A:sklearn.datasets.tests.test_20news.data2cats->fetch_20newsgroups_fxt(subset='all', categories=data.target_names[-1:-3:-1], shuffle=False)
A:sklearn.datasets.tests.test_20news.label->fetch_20newsgroups_fxt(subset='all').target_names.index(category)
A:sklearn.datasets.tests.test_20news.(X, y)->fetch_20newsgroups_fxt(subset='all', shuffle=False, return_X_y=True)
A:sklearn.datasets.tests.test_20news.bunch->fetch_20newsgroups_vectorized_fxt(as_frame=True)
A:sklearn.datasets.tests.test_20news.fetch_func->partial(fetch_20newsgroups_vectorized_fxt, subset='test')
A:sklearn.datasets.tests.test_20news.X->fetch_20newsgroups_vectorized_fxt(normalize=False)
A:sklearn.datasets.tests.test_20news.X_->fetch_20newsgroups_vectorized_fxt(normalize=True)
A:sklearn.datasets.tests.test_20news.pd->pytest.importorskip('pandas')
sklearn.datasets.tests.test_20news.test_20news(fetch_20newsgroups_fxt)
sklearn.datasets.tests.test_20news.test_20news_as_frame(fetch_20newsgroups_vectorized_fxt)
sklearn.datasets.tests.test_20news.test_20news_length_consistency(fetch_20newsgroups_fxt)
sklearn.datasets.tests.test_20news.test_20news_normalization(fetch_20newsgroups_vectorized_fxt)
sklearn.datasets.tests.test_20news.test_20news_vectorized(fetch_20newsgroups_vectorized_fxt)
sklearn.datasets.tests.test_20news.test_as_frame_no_pandas(fetch_20newsgroups_vectorized_fxt,hide_available_pandas)
sklearn.datasets.tests.test_20news.test_outdated_pickle(fetch_20newsgroups_vectorized_fxt)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_common.py----------------------------------------
A:sklearn.datasets.tests.test_common.X_y_tuple->dataset_func(return_X_y=True)
A:sklearn.datasets.tests.test_common.pd->pytest.importorskip('pandas')
A:sklearn.datasets.tests.test_common.frame_bunch->dataset_func(as_frame=True)
A:sklearn.datasets.tests.test_common.(frame_X, frame_y)->dataset_func(as_frame=True, return_X_y=True)
A:sklearn.datasets.tests.test_common.markers_fetch->FETCH_PYTEST_MARKERS.get(param, {})
A:sklearn.datasets.tests.test_common.is_dataset_type->any([name.startswith(t) for t in dataset_type])
A:sklearn.datasets.tests.test_common.bunch->dataset_func()
sklearn.datasets.tests.test_common._generate_func_supporting_param(param,dataset_type=('load','fetch'))
sklearn.datasets.tests.test_common._skip_network_tests()
sklearn.datasets.tests.test_common.check_as_frame(bunch,dataset_func,expected_data_dtype=None,expected_target_dtype=None)
sklearn.datasets.tests.test_common.check_pandas_dependency_message(fetch_func)
sklearn.datasets.tests.test_common.check_return_X_y(bunch,dataset_func)
sklearn.datasets.tests.test_common.is_pillow_installed()
sklearn.datasets.tests.test_common.test_common_check_as_frame(name,dataset_func)
sklearn.datasets.tests.test_common.test_common_check_pandas_dependency(name,dataset_func)
sklearn.datasets.tests.test_common.test_common_check_return_X_y(name,dataset_func)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_olivetti_faces.py----------------------------------------
A:sklearn.datasets.tests.test_olivetti_faces.data->fetch_olivetti_faces_fxt(shuffle=True, random_state=0)
sklearn.datasets.tests.test_olivetti_faces.test_olivetti_faces(fetch_olivetti_faces_fxt)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_lfw.py----------------------------------------
A:sklearn.datasets.tests.test_lfw.Image->pytest.importorskip('PIL.Image')
A:sklearn.datasets.tests.test_lfw.SCIKIT_LEARN_DATA->tempfile.mkdtemp(prefix='scikit_learn_lfw_test_')
A:sklearn.datasets.tests.test_lfw.LFW_HOME->os.path.join(SCIKIT_LEARN_DATA, 'lfw_home')
A:sklearn.datasets.tests.test_lfw.SCIKIT_LEARN_EMPTY_DATA->tempfile.mkdtemp(prefix='scikit_learn_empty_test_')
A:sklearn.datasets.tests.test_lfw.random_state->random.Random(42)
A:sklearn.datasets.tests.test_lfw.np_rng->numpy.random.RandomState(42)
A:sklearn.datasets.tests.test_lfw.folder_name->os.path.join(LFW_HOME, 'lfw_funneled', name)
A:sklearn.datasets.tests.test_lfw.n_faces->numpy.random.RandomState(42).randint(1, 5)
A:sklearn.datasets.tests.test_lfw.file_path->os.path.join(folder_name, name + '_%04d.jpg' % i)
A:sklearn.datasets.tests.test_lfw.uniface->numpy.random.RandomState(42).randint(0, 255, size=(250, 250, 3))
A:sklearn.datasets.tests.test_lfw.img->pytest.importorskip('PIL.Image').fromarray(uniface.astype(np.uint8))
A:sklearn.datasets.tests.test_lfw.name->random.Random(42).choice(more_than_two)
A:sklearn.datasets.tests.test_lfw.(first, second)->random.Random(42).sample(range(counts[name]), 2)
A:sklearn.datasets.tests.test_lfw.(first_name, second_name)->random.Random(42).sample(FAKE_NAMES, 2)
A:sklearn.datasets.tests.test_lfw.first_index->numpy.random.RandomState(42).choice(np.arange(counts[first_name]))
A:sklearn.datasets.tests.test_lfw.second_index->numpy.random.RandomState(42).choice(np.arange(counts[second_name]))
A:sklearn.datasets.tests.test_lfw.lfw_people->fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False)
A:sklearn.datasets.tests.test_lfw.fetch_func->partial(fetch_lfw_people, data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False)
A:sklearn.datasets.tests.test_lfw.lfw_pairs_train->fetch_lfw_pairs(data_home=SCIKIT_LEARN_DATA, resize=None, slice_=None, color=True, download_if_missing=False)
A:sklearn.datasets.tests.test_lfw.lfw->fetch_lfw_people(data_home=SCIKIT_LEARN_DATA, min_faces_per_person=3, download_if_missing=False, resize=None, slice_=slice_)
sklearn.datasets.tests.test_lfw.setup_module()
sklearn.datasets.tests.test_lfw.teardown_module()
sklearn.datasets.tests.test_lfw.test_fetch_lfw_people_internal_cropping()
sklearn.datasets.tests.test_lfw.test_load_empty_lfw_pairs()
sklearn.datasets.tests.test_lfw.test_load_empty_lfw_people()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_pairs()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people()
sklearn.datasets.tests.test_lfw.test_load_fake_lfw_people_too_restrictive()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_svmlight_format.py----------------------------------------
A:sklearn.datasets.tests.test_svmlight_format.data_path->_svmlight_local_test_file_path(datafile)
A:sklearn.datasets.tests.test_svmlight_format.(X, y)->create_memmap_backed_data([X, y])
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1)->load_svmlight_file(str(data_path))
A:sklearn.datasets.tests.test_svmlight_format.fd->os.open(data_path, os.O_RDONLY)
A:sklearn.datasets.tests.test_svmlight_format.(X2, y2)->load_svmlight_file(f, zero_based=False)
A:sklearn.datasets.tests.test_svmlight_format.(X_train, y_train, X_test, y_test)->load_svmlight_files([str(data_path)] * 2, dtype=np.float32)
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, X2, y2, X3, y3)->load_svmlight_files([str(data_path)] * 3, dtype=np.float64)
A:sklearn.datasets.tests.test_svmlight_format.(Xgz, ygz)->load_svmlight_file(tmp.name)
A:sklearn.datasets.tests.test_svmlight_format.(Xbz, ybz)->load_svmlight_file(tmp.name)
A:sklearn.datasets.tests.test_svmlight_format.f->BytesIO()
A:sklearn.datasets.tests.test_svmlight_format.f1->BytesIO(data1)
A:sklearn.datasets.tests.test_svmlight_format.f2->BytesIO(data2)
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, X2, y2)->load_svmlight_files([f1, f2], zero_based='auto')
A:sklearn.datasets.tests.test_svmlight_format.res1->load_svmlight_files([BytesIO(data)], query_id=True)
A:sklearn.datasets.tests.test_svmlight_format.res2->load_svmlight_file(BytesIO(data), query_id=True)
A:sklearn.datasets.tests.test_svmlight_format.data->numpy.array([0, 1, 1, 1, 1, 0])
A:sklearn.datasets.tests.test_svmlight_format.(X, y, qid)->load_svmlight_file(f, query_id=True, zero_based=True)
A:sklearn.datasets.tests.test_svmlight_format.invalid_path->_svmlight_local_test_file_path(invalidfile)
A:sklearn.datasets.tests.test_svmlight_format.(X_sparse, y_dense)->_load_svmlight_local_test_file(datafile)
A:sklearn.datasets.tests.test_svmlight_format.X_dense->X_sparse.toarray()
A:sklearn.datasets.tests.test_svmlight_format.y_sparse->csr_container(y_dense)
A:sklearn.datasets.tests.test_svmlight_format.X_input->rng.randn(5, 2).astype(dtype)
A:sklearn.datasets.tests.test_svmlight_format.comment->str(comment, 'utf-8')
A:sklearn.datasets.tests.test_svmlight_format.X2_dense->X2.toarray()
A:sklearn.datasets.tests.test_svmlight_format.X_input_dense->rng.randn(5, 2).astype(dtype).toarray()
A:sklearn.datasets.tests.test_svmlight_format.X->numpy.random.RandomState(42).randn(5, 2)
A:sklearn.datasets.tests.test_svmlight_format.unicode_comment->utf8_comment.decode('utf-8')
A:sklearn.datasets.tests.test_svmlight_format.(X1, y1, query_id1)->load_svmlight_file(f, query_id=True, zero_based=True)
A:sklearn.datasets.tests.test_svmlight_format.true_X->csr_container(np.zeros(shape=(3, 4)))
A:sklearn.datasets.tests.test_svmlight_format.true_y->numpy.array([0, 1, 0])
A:sklearn.datasets.tests.test_svmlight_format.rng->numpy.random.RandomState(42)
A:sklearn.datasets.tests.test_svmlight_format.y->numpy.random.RandomState(42).randn(5)
A:sklearn.datasets.tests.test_svmlight_format.size->len(f.getvalue())
A:sklearn.datasets.tests.test_svmlight_format.(X_0, y_0)->load_svmlight_file(f, n_features=n_features, offset=mark_0, length=length_0)
A:sklearn.datasets.tests.test_svmlight_format.(X_1, y_1)->load_svmlight_file(f, n_features=n_features, offset=mark_1, length=length_1)
A:sklearn.datasets.tests.test_svmlight_format.(X_2, y_2)->load_svmlight_file(f, n_features=n_features, offset=mark_2)
A:sklearn.datasets.tests.test_svmlight_format.y_concat->numpy.concatenate([y_0, y_1])
A:sklearn.datasets.tests.test_svmlight_format.X_concat->scipy.sparse.vstack([X_0, X_1])
A:sklearn.datasets.tests.test_svmlight_format.(X_0, y_0, q_0)->load_svmlight_file(f, n_features=n_features, query_id=True, offset=0, length=mark)
A:sklearn.datasets.tests.test_svmlight_format.(X_1, y_1, q_1)->load_svmlight_file(f, n_features=n_features, query_id=True, offset=mark, length=-1)
A:sklearn.datasets.tests.test_svmlight_format.q_concat->numpy.concatenate([q_0, q_1])
A:sklearn.datasets.tests.test_svmlight_format.save_path->str(tmp_path / 'svm_read_only')
A:sklearn.datasets.tests.test_svmlight_format.indptr->numpy.array([0, 2, 3, 6])
A:sklearn.datasets.tests.test_svmlight_format.indices->numpy.array([0, 2, 2, 0, 1, 2])
A:sklearn.datasets.tests.test_svmlight_format.(_, y_load)->load_svmlight_file(save_path, multilabel=True)
sklearn.datasets.tests.test_svmlight_format._load_svmlight_local_test_file(filename,**kwargs)
sklearn.datasets.tests.test_svmlight_format._svmlight_local_test_file_path(filename)
sklearn.datasets.tests.test_svmlight_format.test_dump(csr_container)
sklearn.datasets.tests.test_svmlight_format.test_dump_comment()
sklearn.datasets.tests.test_svmlight_format.test_dump_concise()
sklearn.datasets.tests.test_svmlight_format.test_dump_invalid()
sklearn.datasets.tests.test_svmlight_format.test_dump_multilabel(csr_container)
sklearn.datasets.tests.test_svmlight_format.test_dump_query_id()
sklearn.datasets.tests.test_svmlight_format.test_dump_read_only(tmp_path)
sklearn.datasets.tests.test_svmlight_format.test_invalid_filename()
sklearn.datasets.tests.test_svmlight_format.test_load_compressed()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_file()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_file2()
sklearn.datasets.tests.test_svmlight_format.test_load_invalid_order_file()
sklearn.datasets.tests.test_svmlight_format.test_load_large_qid()
sklearn.datasets.tests.test_svmlight_format.test_load_offset_exhaustive_splits(csr_container)
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_fd()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_multilabel()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_file_n_features()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_files()
sklearn.datasets.tests.test_svmlight_format.test_load_svmlight_pathlib()
sklearn.datasets.tests.test_svmlight_format.test_load_with_long_qid()
sklearn.datasets.tests.test_svmlight_format.test_load_with_offsets(sparsity,n_samples,n_features,csr_container)
sklearn.datasets.tests.test_svmlight_format.test_load_with_offsets_error()
sklearn.datasets.tests.test_svmlight_format.test_load_with_qid()
sklearn.datasets.tests.test_svmlight_format.test_load_zero_based()
sklearn.datasets.tests.test_svmlight_format.test_load_zero_based_auto()
sklearn.datasets.tests.test_svmlight_format.test_load_zeros(csr_container)
sklearn.datasets.tests.test_svmlight_format.test_multilabel_y_explicit_zeros(tmp_path,csr_container)
sklearn.datasets.tests.test_svmlight_format.test_not_a_filename()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_kddcup99.py----------------------------------------
A:sklearn.datasets.tests.test_kddcup99.data->fetch_func()
A:sklearn.datasets.tests.test_kddcup99.fetch_func->partial(fetch_kddcup99_fxt, subset='smtp')
A:sklearn.datasets.tests.test_kddcup99.bunch->fetch_kddcup99_fxt()
A:sklearn.datasets.tests.test_kddcup99.dataset->fetch_kddcup99_fxt(random_state=0, subset='SA', percent10=True)
A:sklearn.datasets.tests.test_kddcup99.dataset_shuffled->fetch_kddcup99_fxt(random_state=0, subset='SA', shuffle=True, percent10=True)
sklearn.datasets.tests.test_kddcup99.test_corrupted_file_error_message(fetch_kddcup99_fxt,tmp_path)
sklearn.datasets.tests.test_kddcup99.test_fetch_kddcup99_as_frame(fetch_kddcup99_fxt)
sklearn.datasets.tests.test_kddcup99.test_fetch_kddcup99_percent10(fetch_kddcup99_fxt,as_frame,subset,n_samples,n_features)
sklearn.datasets.tests.test_kddcup99.test_fetch_kddcup99_return_X_y(fetch_kddcup99_fxt)
sklearn.datasets.tests.test_kddcup99.test_fetch_kddcup99_shuffle(fetch_kddcup99_fxt)
sklearn.datasets.tests.test_kddcup99.test_pandas_dependency_message(fetch_kddcup99_fxt,hide_available_pandas)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_california_housing.py----------------------------------------
A:sklearn.datasets.tests.test_california_housing.data->fetch_california_housing_fxt()
A:sklearn.datasets.tests.test_california_housing.fetch_func->partial(fetch_california_housing_fxt)
A:sklearn.datasets.tests.test_california_housing.pd->pytest.importorskip('pandas')
A:sklearn.datasets.tests.test_california_housing.bunch->fetch_california_housing_fxt(as_frame=True)
sklearn.datasets.tests.test_california_housing.test_fetch(fetch_california_housing_fxt)
sklearn.datasets.tests.test_california_housing.test_fetch_asframe(fetch_california_housing_fxt)
sklearn.datasets.tests.test_california_housing.test_pandas_dependency_message(fetch_california_housing_fxt,hide_available_pandas)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_covtype.py----------------------------------------
A:sklearn.datasets.tests.test_covtype.data1->fetch_covtype_fxt(shuffle=True, random_state=global_random_seed)
A:sklearn.datasets.tests.test_covtype.data2->fetch_covtype_fxt(shuffle=True, random_state=global_random_seed + 1)
A:sklearn.datasets.tests.test_covtype.fetch_func->partial(fetch_covtype_fxt)
A:sklearn.datasets.tests.test_covtype.bunch->fetch_covtype_fxt(as_frame=True)
A:sklearn.datasets.tests.test_covtype.column_names->set(frame.columns)
sklearn.datasets.tests.test_covtype.test_fetch(fetch_covtype_fxt,global_random_seed)
sklearn.datasets.tests.test_covtype.test_fetch_asframe(fetch_covtype_fxt)
sklearn.datasets.tests.test_covtype.test_pandas_dependency_message(fetch_covtype_fxt,hide_available_pandas)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/test_openml.py----------------------------------------
A:sklearn.datasets.tests.test_openml.fetch_openml->partial(fetch_openml_orig, data_home=None)
A:sklearn.datasets.tests.test_openml.data_file_name->_file_name(url, '.json')
A:sklearn.datasets.tests.test_openml.fp->BytesIO(decompressed_f.read())
A:sklearn.datasets.tests.test_openml.decompressed_f->read_fn(f, 'rb')
A:sklearn.datasets.tests.test_openml.decoded_s->read_fn(f, 'rb').read().decode('utf-8')
A:sklearn.datasets.tests.test_openml.json_data->json.loads(decoded_s)
A:sklearn.datasets.tests.test_openml.url->request.get_full_url()
A:sklearn.datasets.tests.test_openml.pd->pytest.importorskip('pandas')
A:sklearn.datasets.tests.test_openml.bunch->fetch_openml(data_id=data_id, as_frame=True, cache=False, parser=parser)
A:sklearn.datasets.tests.test_openml.bunch_liac->fetch_openml(data_id=data_id, as_frame=True, cache=False, parser='liac-arff')
A:sklearn.datasets.tests.test_openml.bunch_pandas->fetch_openml(data_id=data_id, as_frame=as_frame, cache=False, parser='pandas')
A:sklearn.datasets.tests.test_openml.data_liac_with_fixed_dtypes->data_liac.apply(convert_numerical_dtypes)
A:sklearn.datasets.tests.test_openml.frame_liac_with_fixed_dtypes->frame_liac.apply(convert_numerical_and_categorical_dtypes)
A:sklearn.datasets.tests.test_openml.bunch_as_frame_true->fetch_openml(data_id=data_id, as_frame=True, cache=False, parser=parser)
A:sklearn.datasets.tests.test_openml.bunch_as_frame_false->fetch_openml(data_id=data_id, as_frame=False, cache=False, parser=parser)
A:sklearn.datasets.tests.test_openml.target_dtype->CategoricalDtype(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])
A:sklearn.datasets.tests.test_openml.bunch_forcing_target->fetch_openml(data_id=data_id, as_frame=True, cache=False, target_column=target_column, parser=parser)
A:sklearn.datasets.tests.test_openml.bunch_default->fetch_openml(data_id=data_id, as_frame=True, cache=False, parser=parser)
A:sklearn.datasets.tests.test_openml.(X, y)->fetch_openml(data_id=data_id, as_frame=False, cache=False, return_X_y=True, parser=parser)
A:sklearn.datasets.tests.test_openml.bunch_liac_arff->fetch_openml(data_id=data_id, as_frame=as_frame, cache=False, parser='liac-arff')
A:sklearn.datasets.tests.test_openml.n_categories->len([dtype for dtype in frame.dtypes if isinstance(dtype, CategoricalDtype)])
A:sklearn.datasets.tests.test_openml.n_floats->len([dtype for dtype in frame.dtypes if dtype.kind == 'f'])
A:sklearn.datasets.tests.test_openml.n_ints->len([dtype for dtype in frame.dtypes if dtype.kind == 'i'])
A:sklearn.datasets.tests.test_openml.frame_feature_to_n_nan->frame.isna().sum().to_dict()
A:sklearn.datasets.tests.test_openml.expected_missing->datasets_missing_values[data_id].get(name, 0)
A:sklearn.datasets.tests.test_openml.data->bytearray(orig_gzip.read())
A:sklearn.datasets.tests.test_openml.msg->expected_ignore_msg.format(target_col)
A:sklearn.datasets.tests.test_openml.penguins->fetch_openml(data_id=data_id, cache=False, as_frame=True, parser=parser)
A:sklearn.datasets.tests.test_openml.glass2->fetch_openml(cache=False, as_frame=False, parser='liac-arff', **dataset_params)
A:sklearn.datasets.tests.test_openml.adult_without_spaces->fetch_openml(**common_params)
A:sklearn.datasets.tests.test_openml.adult_with_spaces->fetch_openml(**common_params, read_csv_kwargs={'skipinitialspace': False})
A:sklearn.datasets.tests.test_openml.openml_path->sklearn.datasets._openml._DATA_FILE.format(data_id)
A:sklearn.datasets.tests.test_openml.cache_directory->str(tmpdir.mkdir('scikit_learn_data'))
A:sklearn.datasets.tests.test_openml.response1->_open_openml_url(openml_path, cache_directory)
A:sklearn.datasets.tests.test_openml.location->_get_local_path(openml_path, cache_directory)
A:sklearn.datasets.tests.test_openml.response2->_open_openml_url(openml_path, cache_directory)
A:sklearn.datasets.tests.test_openml.result->_load_data()
A:sklearn.datasets.tests.test_openml.(X_fetched, y_fetched)->fetch_openml(data_id=data_id, cache=True, data_home=cache_directory, return_X_y=True, as_frame=False, parser='liac-arff')
A:sklearn.datasets.tests.test_openml.(X_cached, y_cached)->fetch_openml(data_id=data_id, cache=True, data_home=cache_directory, return_X_y=True, as_frame=False, parser='liac-arff')
A:sklearn.datasets.tests.test_openml.orig_gzip->gzip.open(orig_file, 'rb')
A:sklearn.datasets.tests.test_openml.corrupted_data->f.read()
A:sklearn.datasets.tests.test_openml.dataset->sklearn.datasets.fetch_openml(data_id=data_id, cache=False, as_frame=False, parser=parser)
A:sklearn.datasets.tests.test_openml.mice_pandas->fetch_openml(parser='pandas', target_column='NUMB_N', **common_params)
A:sklearn.datasets.tests.test_openml.mice_liac_arff->fetch_openml(parser='liac-arff', target_column='NUMB_N', **common_params)
A:sklearn.datasets.tests.test_openml.adult_pandas->fetch_openml(parser='pandas', **common_params)
A:sklearn.datasets.tests.test_openml.adult_liac_arff->fetch_openml(parser='liac-arff', **common_params)
sklearn.datasets.tests.test_openml._MockHTTPResponse(self,data,is_gzip)
sklearn.datasets.tests.test_openml._MockHTTPResponse.__enter__(self)
sklearn.datasets.tests.test_openml._MockHTTPResponse.__exit__(self,exc_type,exc_val,exc_tb)
sklearn.datasets.tests.test_openml._MockHTTPResponse.__init__(self,data,is_gzip)
sklearn.datasets.tests.test_openml._MockHTTPResponse.__iter__(self)
sklearn.datasets.tests.test_openml._MockHTTPResponse.close(self)
sklearn.datasets.tests.test_openml._MockHTTPResponse.info(self)
sklearn.datasets.tests.test_openml._MockHTTPResponse.read(self,amt=-1)
sklearn.datasets.tests.test_openml._monkey_patch_webbased_functions(context,data_id,gzip_response)
sklearn.datasets.tests.test_openml.datasets_column_names()
sklearn.datasets.tests.test_openml.datasets_missing_values()
sklearn.datasets.tests.test_openml.test_convert_arff_data_dataframe_warning_low_memory_pandas(monkeypatch)
sklearn.datasets.tests.test_openml.test_dataset_with_openml_error(monkeypatch,gzip_response)
sklearn.datasets.tests.test_openml.test_dataset_with_openml_warning(monkeypatch,gzip_response)
sklearn.datasets.tests.test_openml.test_fetch_openml_as_frame_false(monkeypatch,data_id,dataset_params,n_samples,n_features,n_targets,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_as_frame_true(monkeypatch,data_id,dataset_params,n_samples,n_features,n_targets,parser,gzip_response)
sklearn.datasets.tests.test_openml.test_fetch_openml_auto_mode(monkeypatch,data_id,data_type)
sklearn.datasets.tests.test_openml.test_fetch_openml_cache(monkeypatch,gzip_response,tmpdir)
sklearn.datasets.tests.test_openml.test_fetch_openml_consistency_parser(monkeypatch,data_id)
sklearn.datasets.tests.test_openml.test_fetch_openml_difference_parsers(monkeypatch)
sklearn.datasets.tests.test_openml.test_fetch_openml_equivalence_array_dataframe(monkeypatch,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_equivalence_array_return_X_y(monkeypatch,data_id,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_equivalence_frame_return_X_y(monkeypatch,data_id,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_error(monkeypatch,gzip_response,data_id,params,err_type,err_msg,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_forcing_targets(monkeypatch,parser,target_column)
sklearn.datasets.tests.test_openml.test_fetch_openml_inactive(monkeypatch,gzip_response,dataset_params)
sklearn.datasets.tests.test_openml.test_fetch_openml_iris_pandas(monkeypatch,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_iris_warn_multiple_version(monkeypatch,gzip_response)
sklearn.datasets.tests.test_openml.test_fetch_openml_leading_whitespace(monkeypatch)
sklearn.datasets.tests.test_openml.test_fetch_openml_no_target(monkeypatch,gzip_response)
sklearn.datasets.tests.test_openml.test_fetch_openml_overwrite_default_params_read_csv(monkeypatch)
sklearn.datasets.tests.test_openml.test_fetch_openml_quotechar_escapechar(monkeypatch)
sklearn.datasets.tests.test_openml.test_fetch_openml_raises_illegal_argument(params,err_type,err_msg)
sklearn.datasets.tests.test_openml.test_fetch_openml_requires_pandas_error(monkeypatch,params)
sklearn.datasets.tests.test_openml.test_fetch_openml_sparse_arff_error(monkeypatch,params,err_msg)
sklearn.datasets.tests.test_openml.test_fetch_openml_strip_quotes(monkeypatch)
sklearn.datasets.tests.test_openml.test_fetch_openml_types_inference(monkeypatch,data_id,parser,expected_n_categories,expected_n_floats,expected_n_ints,gzip_response,datasets_column_names,datasets_missing_values)
sklearn.datasets.tests.test_openml.test_fetch_openml_validation_parameter(monkeypatch,params,err_msg)
sklearn.datasets.tests.test_openml.test_fetch_openml_verify_checksum(monkeypatch,as_frame,cache,tmpdir,parser)
sklearn.datasets.tests.test_openml.test_fetch_openml_with_ignored_feature(monkeypatch,gzip_response,parser)
sklearn.datasets.tests.test_openml.test_missing_values_pandas(monkeypatch,gzip_response,parser)
sklearn.datasets.tests.test_openml.test_open_openml_url_cache(monkeypatch,gzip_response,tmpdir)
sklearn.datasets.tests.test_openml.test_open_openml_url_retry_on_network_error(monkeypatch)
sklearn.datasets.tests.test_openml.test_open_openml_url_unlinks_local_path(monkeypatch,tmpdir,write_to_disk)
sklearn.datasets.tests.test_openml.test_retry_with_clean_cache(tmpdir)
sklearn.datasets.tests.test_openml.test_retry_with_clean_cache_http_error(tmpdir)
sklearn.datasets.tests.test_openml.test_warn_ignore_attribute(monkeypatch,gzip_response)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_3/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_61/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_40589/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_1590/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_292/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_62/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_40945/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_2/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_42074/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_1119/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_1/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_40675/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_42585/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_561/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/tests/data/openml/id_40966/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/data/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/descr/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/datasets/images/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py----------------------------------------
A:sklearn.gaussian_process.kernels.length_scale->_check_length_scale(X, self.length_scale)
A:sklearn.gaussian_process.kernels.bounds->numpy.repeat(bounds, n_elements, 0)
A:sklearn.gaussian_process.kernels.params->dict(kernel=self.kernel, exponent=self.exponent)
A:sklearn.gaussian_process.kernels.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.gaussian_process.kernels.init_sign->signature(init)
A:sklearn.gaussian_process.kernels.params[arg]->getattr(self, arg)
A:sklearn.gaussian_process.kernels.valid_params->self.get_params(deep=True)
A:sklearn.gaussian_process.kernels.split->key.split('__', 1)
A:sklearn.gaussian_process.kernels.cloned->clone(self)
A:sklearn.gaussian_process.kernels.params[hyperparameter.name]->numpy.exp(theta[i])
A:sklearn.gaussian_process.kernels.params_a->self.get_params()
A:sklearn.gaussian_process.kernels.params_b->b.get_params()
A:sklearn.gaussian_process.kernels.list_close->numpy.isclose(self.bounds, np.atleast_2d(self.theta).T)
A:sklearn.gaussian_process.kernels.(K_single, K_grad_single)->kernel(X, Y, eval_gradient)
A:sklearn.gaussian_process.kernels.deep_items->self.kernel.get_params().items()
A:sklearn.gaussian_process.kernels.(K1, K1_gradient)->self.k1(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.(K2, K2_gradient)->self.k2(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.(K, K_gradient)->self.kernel(X, Y, eval_gradient=True)
A:sklearn.gaussian_process.kernels.K->pairwise_kernels(X, Y, metric=self.metric, gamma=self.gamma, filter_params=True, **pairwise_kernels_kwargs)
A:sklearn.gaussian_process.kernels.X->numpy.atleast_2d(X)
A:sklearn.gaussian_process.kernels.dists->cdist(X, Y, metric='euclidean')
A:sklearn.gaussian_process.kernels.K_gradient->numpy.empty((K.shape[0], K.shape[1], 1))
A:sklearn.gaussian_process.kernels.divide_result->numpy.zeros_like(D)
A:sklearn.gaussian_process.kernels.length_scale_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.alpha_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.sin_of_arg->numpy.sin(arg)
A:sklearn.gaussian_process.kernels.cos_of_arg->numpy.cos(arg)
A:sklearn.gaussian_process.kernels.periodicity_gradient->numpy.empty((K.shape[0], K.shape[1], 0))
A:sklearn.gaussian_process.kernels.f0->f(*(xk,) + args)
A:sklearn.gaussian_process.kernels.grad->numpy.zeros((f0.shape[0], f0.shape[1], len(xk)), float)
A:sklearn.gaussian_process.kernels.ei->numpy.zeros((len(xk),), float)
sklearn.gaussian_process.kernels.CompoundKernel(self,kernels)
sklearn.gaussian_process.kernels.CompoundKernel.__eq__(self,b)
sklearn.gaussian_process.kernels.CompoundKernel.__init__(self,kernels)
sklearn.gaussian_process.kernels.CompoundKernel.bounds(self)
sklearn.gaussian_process.kernels.CompoundKernel.diag(self,X)
sklearn.gaussian_process.kernels.CompoundKernel.get_params(self,deep=True)
sklearn.gaussian_process.kernels.CompoundKernel.is_stationary(self)
sklearn.gaussian_process.kernels.CompoundKernel.requires_vector_input(self)
sklearn.gaussian_process.kernels.CompoundKernel.theta(self)
sklearn.gaussian_process.kernels.CompoundKernel.theta(self,theta)
sklearn.gaussian_process.kernels.ConstantKernel(self,constant_value=1.0,constant_value_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ConstantKernel.__init__(self,constant_value=1.0,constant_value_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ConstantKernel.__repr__(self)
sklearn.gaussian_process.kernels.ConstantKernel.diag(self,X)
sklearn.gaussian_process.kernels.ConstantKernel.hyperparameter_constant_value(self)
sklearn.gaussian_process.kernels.DotProduct(self,sigma_0=1.0,sigma_0_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.DotProduct.__init__(self,sigma_0=1.0,sigma_0_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.DotProduct.__repr__(self)
sklearn.gaussian_process.kernels.DotProduct.diag(self,X)
sklearn.gaussian_process.kernels.DotProduct.hyperparameter_sigma_0(self)
sklearn.gaussian_process.kernels.DotProduct.is_stationary(self)
sklearn.gaussian_process.kernels.ExpSineSquared(self,length_scale=1.0,periodicity=1.0,length_scale_bounds=(1e-05,100000.0),periodicity_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ExpSineSquared.__init__(self,length_scale=1.0,periodicity=1.0,length_scale_bounds=(1e-05,100000.0),periodicity_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.ExpSineSquared.__repr__(self)
sklearn.gaussian_process.kernels.ExpSineSquared.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.ExpSineSquared.hyperparameter_periodicity(self)
sklearn.gaussian_process.kernels.Exponentiation(self,kernel,exponent)
sklearn.gaussian_process.kernels.Exponentiation.__eq__(self,b)
sklearn.gaussian_process.kernels.Exponentiation.__init__(self,kernel,exponent)
sklearn.gaussian_process.kernels.Exponentiation.__repr__(self)
sklearn.gaussian_process.kernels.Exponentiation.bounds(self)
sklearn.gaussian_process.kernels.Exponentiation.diag(self,X)
sklearn.gaussian_process.kernels.Exponentiation.get_params(self,deep=True)
sklearn.gaussian_process.kernels.Exponentiation.hyperparameters(self)
sklearn.gaussian_process.kernels.Exponentiation.is_stationary(self)
sklearn.gaussian_process.kernels.Exponentiation.requires_vector_input(self)
sklearn.gaussian_process.kernels.Exponentiation.theta(self)
sklearn.gaussian_process.kernels.Exponentiation.theta(self,theta)
sklearn.gaussian_process.kernels.GenericKernelMixin
sklearn.gaussian_process.kernels.GenericKernelMixin.requires_vector_input(self)
sklearn.gaussian_process.kernels.Hyperparameter(cls,name,value_type,bounds,n_elements=1,fixed=None)
sklearn.gaussian_process.kernels.Hyperparameter.__eq__(self,other)
sklearn.gaussian_process.kernels.Hyperparameter.__new__(cls,name,value_type,bounds,n_elements=1,fixed=None)
sklearn.gaussian_process.kernels.Kernel(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Kernel.__add__(self,b)
sklearn.gaussian_process.kernels.Kernel.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Kernel.__eq__(self,b)
sklearn.gaussian_process.kernels.Kernel.__mul__(self,b)
sklearn.gaussian_process.kernels.Kernel.__pow__(self,b)
sklearn.gaussian_process.kernels.Kernel.__radd__(self,b)
sklearn.gaussian_process.kernels.Kernel.__repr__(self)
sklearn.gaussian_process.kernels.Kernel.__rmul__(self,b)
sklearn.gaussian_process.kernels.Kernel._check_bounds_params(self)
sklearn.gaussian_process.kernels.Kernel.bounds(self)
sklearn.gaussian_process.kernels.Kernel.clone_with_theta(self,theta)
sklearn.gaussian_process.kernels.Kernel.diag(self,X)
sklearn.gaussian_process.kernels.Kernel.get_params(self,deep=True)
sklearn.gaussian_process.kernels.Kernel.hyperparameters(self)
sklearn.gaussian_process.kernels.Kernel.is_stationary(self)
sklearn.gaussian_process.kernels.Kernel.n_dims(self)
sklearn.gaussian_process.kernels.Kernel.requires_vector_input(self)
sklearn.gaussian_process.kernels.Kernel.set_params(self,**params)
sklearn.gaussian_process.kernels.Kernel.theta(self)
sklearn.gaussian_process.kernels.Kernel.theta(self,theta)
sklearn.gaussian_process.kernels.KernelOperator(self,k1,k2)
sklearn.gaussian_process.kernels.KernelOperator.__eq__(self,b)
sklearn.gaussian_process.kernels.KernelOperator.__init__(self,k1,k2)
sklearn.gaussian_process.kernels.KernelOperator.bounds(self)
sklearn.gaussian_process.kernels.KernelOperator.get_params(self,deep=True)
sklearn.gaussian_process.kernels.KernelOperator.hyperparameters(self)
sklearn.gaussian_process.kernels.KernelOperator.is_stationary(self)
sklearn.gaussian_process.kernels.KernelOperator.requires_vector_input(self)
sklearn.gaussian_process.kernels.KernelOperator.theta(self)
sklearn.gaussian_process.kernels.KernelOperator.theta(self,theta)
sklearn.gaussian_process.kernels.Matern(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0),nu=1.5)
sklearn.gaussian_process.kernels.Matern.__init__(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0),nu=1.5)
sklearn.gaussian_process.kernels.Matern.__repr__(self)
sklearn.gaussian_process.kernels.NormalizedKernelMixin
sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag(self,X)
sklearn.gaussian_process.kernels.PairwiseKernel(self,gamma=1.0,gamma_bounds=(1e-05,100000.0),metric='linear',pairwise_kernels_kwargs=None)
sklearn.gaussian_process.kernels.PairwiseKernel.__init__(self,gamma=1.0,gamma_bounds=(1e-05,100000.0),metric='linear',pairwise_kernels_kwargs=None)
sklearn.gaussian_process.kernels.PairwiseKernel.__repr__(self)
sklearn.gaussian_process.kernels.PairwiseKernel.diag(self,X)
sklearn.gaussian_process.kernels.PairwiseKernel.hyperparameter_gamma(self)
sklearn.gaussian_process.kernels.PairwiseKernel.is_stationary(self)
sklearn.gaussian_process.kernels.Product(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Product.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Product.__repr__(self)
sklearn.gaussian_process.kernels.Product.diag(self,X)
sklearn.gaussian_process.kernels.RBF(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RBF.__init__(self,length_scale=1.0,length_scale_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RBF.__repr__(self)
sklearn.gaussian_process.kernels.RBF.anisotropic(self)
sklearn.gaussian_process.kernels.RBF.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.RationalQuadratic(self,length_scale=1.0,alpha=1.0,length_scale_bounds=(1e-05,100000.0),alpha_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RationalQuadratic.__init__(self,length_scale=1.0,alpha=1.0,length_scale_bounds=(1e-05,100000.0),alpha_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.RationalQuadratic.__repr__(self)
sklearn.gaussian_process.kernels.RationalQuadratic.hyperparameter_alpha(self)
sklearn.gaussian_process.kernels.RationalQuadratic.hyperparameter_length_scale(self)
sklearn.gaussian_process.kernels.StationaryKernelMixin
sklearn.gaussian_process.kernels.StationaryKernelMixin.is_stationary(self)
sklearn.gaussian_process.kernels.Sum(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Sum.__call__(self,X,Y=None,eval_gradient=False)
sklearn.gaussian_process.kernels.Sum.__repr__(self)
sklearn.gaussian_process.kernels.Sum.diag(self,X)
sklearn.gaussian_process.kernels.WhiteKernel(self,noise_level=1.0,noise_level_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.WhiteKernel.__init__(self,noise_level=1.0,noise_level_bounds=(1e-05,100000.0))
sklearn.gaussian_process.kernels.WhiteKernel.__repr__(self)
sklearn.gaussian_process.kernels.WhiteKernel.diag(self,X)
sklearn.gaussian_process.kernels.WhiteKernel.hyperparameter_noise_level(self)
sklearn.gaussian_process.kernels._approx_fprime(xk,f,epsilon,args=())
sklearn.gaussian_process.kernels._check_length_scale(X,length_scale)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/_gpc.py----------------------------------------
A:sklearn.gaussian_process._gpc.self.kernel_->clone(self.kernel)
A:sklearn.gaussian_process._gpc.self.rng->check_random_state(self.random_state)
A:sklearn.gaussian_process._gpc.label_encoder->LabelEncoder()
A:sklearn.gaussian_process._gpc.self.y_train_->LabelEncoder().fit_transform(y)
A:sklearn.gaussian_process._gpc.(lml, grad)->self.log_marginal_likelihood(theta, eval_gradient=True, clone_kernel=False)
A:sklearn.gaussian_process._gpc.theta_initial->numpy.exp(self.rng.uniform(bounds[:, 0], bounds[:, 1]))
A:sklearn.gaussian_process._gpc.lml_values->list(map(itemgetter(1), optima))
A:sklearn.gaussian_process._gpc.self.log_marginal_likelihood_value_->self.base_estimator_.log_marginal_likelihood()
A:sklearn.gaussian_process._gpc.K->kernel(self.X_train_)
A:sklearn.gaussian_process._gpc.(_, (self.pi_, self.W_sr_, self.L_, _, _))->self._posterior_mode(K, return_temporaries=True)
A:sklearn.gaussian_process._gpc.K_star->self.kernel_(self.X_train_, X)
A:sklearn.gaussian_process._gpc.f_star->self.kernel_(self.X_train_, X).T.dot(self.y_train_ - self.pi_)
A:sklearn.gaussian_process._gpc.v->solve(self.L_, self.W_sr_[:, np.newaxis] * K_star)
A:sklearn.gaussian_process._gpc.kernel->self.kernel_.clone_with_theta(theta)
A:sklearn.gaussian_process._gpc.(K, K_gradient)->kernel(self.X_train_, eval_gradient=True)
A:sklearn.gaussian_process._gpc.(Z, (pi, W_sr, L, b, a))->self._posterior_mode(K, return_temporaries=True)
A:sklearn.gaussian_process._gpc.d_Z->numpy.empty(theta.shape[0])
A:sklearn.gaussian_process._gpc.C->solve(L, W_sr[:, np.newaxis] * K)
A:sklearn.gaussian_process._gpc.b->solve(L, W_sr[:, np.newaxis] * K).dot(self.y_train_ - pi)
A:sklearn.gaussian_process._gpc.f->kernel(self.X_train_).dot(a)
A:sklearn.gaussian_process._gpc.pi->expit(f)
A:sklearn.gaussian_process._gpc.W_sr->numpy.sqrt(W)
A:sklearn.gaussian_process._gpc.L->cholesky(B, lower=True)
A:sklearn.gaussian_process._gpc.opt_res->scipy.optimize.minimize(obj_func, initial_theta, method='L-BFGS-B', jac=True, bounds=bounds)
A:sklearn.gaussian_process._gpc.(theta_opt, func_min)->self.optimizer(obj_func, initial_theta, bounds=bounds)
A:sklearn.gaussian_process._gpc.(X, y)->self._validate_data(X, y, multi_output=False, ensure_2d=False, dtype=None)
A:sklearn.gaussian_process._gpc.self.base_estimator_->OneVsOneClassifier(self.base_estimator_, n_jobs=self.n_jobs)
A:sklearn.gaussian_process._gpc.self.classes_->numpy.unique(y)
A:sklearn.gaussian_process._gpc.X->self._validate_data(X, ensure_2d=False, dtype=None, reset=False)
A:sklearn.gaussian_process._gpc.theta->numpy.asarray(theta)
sklearn.gaussian_process.GaussianProcessClassifier(self,kernel=None,*,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=None)
sklearn.gaussian_process.GaussianProcessClassifier.fit(self,X,y)
sklearn.gaussian_process.GaussianProcessClassifier.kernel_(self)
sklearn.gaussian_process.GaussianProcessClassifier.log_marginal_likelihood(self,theta=None,eval_gradient=False,clone_kernel=True)
sklearn.gaussian_process.GaussianProcessClassifier.predict(self,X)
sklearn.gaussian_process.GaussianProcessClassifier.predict_proba(self,X)
sklearn.gaussian_process._gpc.GaussianProcessClassifier(self,kernel=None,*,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=None)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.__init__(self,kernel=None,*,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None,multi_class='one_vs_rest',n_jobs=None)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.fit(self,X,y)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.kernel_(self)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.log_marginal_likelihood(self,theta=None,eval_gradient=False,clone_kernel=True)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.predict(self,X)
sklearn.gaussian_process._gpc.GaussianProcessClassifier.predict_proba(self,X)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace(self,kernel=None,*,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace.__init__(self,kernel=None,*,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,max_iter_predict=100,warm_start=False,copy_X_train=True,random_state=None)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace._posterior_mode(self,K,return_temporaries=False)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace.fit(self,X,y)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace.log_marginal_likelihood(self,theta=None,eval_gradient=False,clone_kernel=True)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace.predict(self,X)
sklearn.gaussian_process._gpc._BinaryGaussianProcessClassifierLaplace.predict_proba(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py----------------------------------------
A:sklearn.gaussian_process._gpr.self.kernel_->clone(self.kernel)
A:sklearn.gaussian_process._gpr.self._rng->check_random_state(self.random_state)
A:sklearn.gaussian_process._gpr.(X, y)->self._validate_data(X, y, multi_output=True, y_numeric=True, ensure_2d=ensure_2d, dtype=dtype)
A:sklearn.gaussian_process._gpr.self._y_train_mean->numpy.zeros(shape=shape_y_stats)
A:sklearn.gaussian_process._gpr.self._y_train_std->numpy.ones(shape=shape_y_stats)
A:sklearn.gaussian_process._gpr.(lml, grad)->self.log_marginal_likelihood(theta, eval_gradient=True, clone_kernel=False)
A:sklearn.gaussian_process._gpr.theta_initial->self._rng.uniform(bounds[:, 0], bounds[:, 1])
A:sklearn.gaussian_process._gpr.lml_values->list(map(itemgetter(1), optima))
A:sklearn.gaussian_process._gpr.self.log_marginal_likelihood_value_->self.log_marginal_likelihood(self.kernel_.theta, clone_kernel=False)
A:sklearn.gaussian_process._gpr.K->kernel(self.X_train_)
A:sklearn.gaussian_process._gpr.self.L_->cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)
A:sklearn.gaussian_process._gpr.self.alpha_->cho_solve((self.L_, GPR_CHOLESKY_LOWER), self.y_train_, check_finite=False)
A:sklearn.gaussian_process._gpr.X->self._validate_data(X, ensure_2d=ensure_2d, dtype=dtype, reset=False)
A:sklearn.gaussian_process._gpr.y_mean->numpy.squeeze(y_mean, axis=1)
A:sklearn.gaussian_process._gpr.y_cov->numpy.squeeze(y_cov, axis=2)
A:sklearn.gaussian_process._gpr.y_var->numpy.squeeze(y_var, axis=1)
A:sklearn.gaussian_process._gpr.K_trans->self.kernel_(X, self.X_train_)
A:sklearn.gaussian_process._gpr.V->solve_triangular(self.L_, K_trans.T, lower=GPR_CHOLESKY_LOWER, check_finite=False)
A:sklearn.gaussian_process._gpr.rng->check_random_state(random_state)
A:sklearn.gaussian_process._gpr.(y_mean, y_cov)->self.predict(X, return_cov=True)
A:sklearn.gaussian_process._gpr.y_samples->numpy.hstack(y_samples)
A:sklearn.gaussian_process._gpr.kernel->self.kernel_.clone_with_theta(theta)
A:sklearn.gaussian_process._gpr.(K, K_gradient)->kernel(self.X_train_, eval_gradient=True)
A:sklearn.gaussian_process._gpr.L->cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)
A:sklearn.gaussian_process._gpr.alpha->cho_solve((L, GPR_CHOLESKY_LOWER), y_train, check_finite=False)
A:sklearn.gaussian_process._gpr.log_likelihood->log_likelihood_dims.sum(axis=-1)
A:sklearn.gaussian_process._gpr.inner_term->numpy.einsum('ik,jk->ijk', alpha, alpha)
A:sklearn.gaussian_process._gpr.K_inv->cho_solve((L, GPR_CHOLESKY_LOWER), np.eye(K.shape[0]), check_finite=False)
A:sklearn.gaussian_process._gpr.log_likelihood_gradient->log_likelihood_gradient_dims.sum(axis=-1)
A:sklearn.gaussian_process._gpr.opt_res->scipy.optimize.minimize(obj_func, initial_theta, method='L-BFGS-B', jac=True, bounds=bounds)
A:sklearn.gaussian_process._gpr.(theta_opt, func_min)->self.optimizer(obj_func, initial_theta, bounds=bounds)
sklearn.gaussian_process.GaussianProcessRegressor(self,kernel=None,*,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,n_targets=None,random_state=None)
sklearn.gaussian_process.GaussianProcessRegressor._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process.GaussianProcessRegressor._more_tags(self)
sklearn.gaussian_process.GaussianProcessRegressor.fit(self,X,y)
sklearn.gaussian_process.GaussianProcessRegressor.log_marginal_likelihood(self,theta=None,eval_gradient=False,clone_kernel=True)
sklearn.gaussian_process.GaussianProcessRegressor.predict(self,X,return_std=False,return_cov=False)
sklearn.gaussian_process.GaussianProcessRegressor.sample_y(self,X,n_samples=1,random_state=0)
sklearn.gaussian_process._gpr.GaussianProcessRegressor(self,kernel=None,*,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,n_targets=None,random_state=None)
sklearn.gaussian_process._gpr.GaussianProcessRegressor.__init__(self,kernel=None,*,alpha=1e-10,optimizer='fmin_l_bfgs_b',n_restarts_optimizer=0,normalize_y=False,copy_X_train=True,n_targets=None,random_state=None)
sklearn.gaussian_process._gpr.GaussianProcessRegressor._constrained_optimization(self,obj_func,initial_theta,bounds)
sklearn.gaussian_process._gpr.GaussianProcessRegressor._more_tags(self)
sklearn.gaussian_process._gpr.GaussianProcessRegressor.fit(self,X,y)
sklearn.gaussian_process._gpr.GaussianProcessRegressor.log_marginal_likelihood(self,theta=None,eval_gradient=False,clone_kernel=True)
sklearn.gaussian_process._gpr.GaussianProcessRegressor.predict(self,X,return_std=False,return_cov=False)
sklearn.gaussian_process._gpr.GaussianProcessRegressor.sample_y(self,X,n_samples=1,random_state=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/tests/test_gpr.py----------------------------------------
A:sklearn.gaussian_process.tests.test_gpr.y->numpy.random.RandomState(0).randn(10, 2)
A:sklearn.gaussian_process.tests.test_gpr.fixed_kernel->RBF(length_scale=1.0, length_scale_bounds='fixed')
A:sklearn.gaussian_process.tests.test_gpr.gpr->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred, y_cov)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.kernel->RBF(length_scale_bounds=[1e-05, 0.001])
A:sklearn.gaussian_process.tests.test_gpr.input_theta->numpy.ones(gpr.kernel_.theta.shape, dtype=np.float64)
A:sklearn.gaussian_process.tests.test_gpr.(lml, lml_gradient)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).log_marginal_likelihood(kernel.theta, True)
A:sklearn.gaussian_process.tests.test_gpr.lml_gradient_approx->approx_fprime(kernel.theta, lambda theta: gpr.log_marginal_likelihood(theta, False), 1e-10)
A:sklearn.gaussian_process.tests.test_gpr.(y_mean, y_cov)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.samples->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).sample_y(X2, 300000)
A:sklearn.gaussian_process.tests.test_gpr.(y_mean, y_std)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.rng->numpy.random.RandomState(0)
A:sklearn.gaussian_process.tests.test_gpr.X->numpy.random.RandomState(0).randn(10, 3)
A:sklearn.gaussian_process.tests.test_gpr.gp->GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y)
A:sklearn.gaussian_process.tests.test_gpr.lml->GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=0).fit(X, y).log_marginal_likelihood(gp.kernel_.theta)
A:sklearn.gaussian_process.tests.test_gpr.y_mean->numpy.mean(y)
A:sklearn.gaussian_process.tests.test_gpr.y_std->numpy.std(y)
A:sklearn.gaussian_process.tests.test_gpr.gpr_norm->GaussianProcessRegressor(kernel=kernel, normalize_y=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred, y_pred_std)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_norm, y_pred_std_norm)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov)->GaussianProcessRegressor(n_targets=1).predict(X_test, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_norm)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.y_pred_gpy->numpy.array([15.16918303, -27.98707845, -39.31636019, 14.52605515, 69.18503589])
A:sklearn.gaussian_process.tests.test_gpr.y_pred_std_gpy->numpy.array([7.78860962, 3.83179178, 0.63149951, 0.52745188, 0.86170042])
A:sklearn.gaussian_process.tests.test_gpr.gpr_2d->GaussianProcessRegressor(kernel=kernel, normalize_y=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_1d, y_std_1d)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_2d, y_std_2d)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_1d)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov_2d)->GaussianProcessRegressor(kernel=kernel, normalize_y=True).predict(X2, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.y_sample_1d->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).sample_y(X2, n_samples=10)
A:sklearn.gaussian_process.tests.test_gpr.y_sample_2d->GaussianProcessRegressor(kernel=kernel, normalize_y=True).sample_y(X2, n_samples=10)
A:sklearn.gaussian_process.tests.test_gpr.theta->numpy.atleast_1d(rng.uniform(np.maximum(-2, bounds[:, 0]), np.minimum(1, bounds[:, 1])))
A:sklearn.gaussian_process.tests.test_gpr.f->obj_func(theta, eval_gradient=False)
A:sklearn.gaussian_process.tests.test_gpr.gpr_equal_inputs->GaussianProcessRegressor(kernel=kernel, alpha=0.01)
A:sklearn.gaussian_process.tests.test_gpr.gpr_similar_inputs->GaussianProcessRegressor(kernel=kernel, alpha=0.01)
A:sklearn.gaussian_process.tests.test_gpr.X_->numpy.vstack((X, X[0] + 1e-15))
A:sklearn.gaussian_process.tests.test_gpr.y_->numpy.hstack((y, y[0] + 1))
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_equal, y_std_equal)->GaussianProcessRegressor(kernel=kernel, alpha=0.01).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred_similar, y_std_similar)->GaussianProcessRegressor(kernel=kernel, alpha=0.01).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr1->GaussianProcessRegressor()
A:sklearn.gaussian_process.tests.test_gpr.(_, y_std1)->GaussianProcessRegressor().predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov1)->GaussianProcessRegressor().predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr2->GaussianProcessRegressor(kernel=default_kernel)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_std2)->GaussianProcessRegressor(kernel=default_kernel).predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, y_cov2)->GaussianProcessRegressor(kernel=default_kernel).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.gpr_sum->GaussianProcessRegressor(kernel=kernel_sum)
A:sklearn.gaussian_process.tests.test_gpr.X_tile->numpy.tile(X, 2)
A:sklearn.gaussian_process.tests.test_gpr.kernel_dims->RBF(length_scale=[1.0, 2.0], length_scale_bounds=[10.0, 100.0])
A:sklearn.gaussian_process.tests.test_gpr.gpr_dims->GaussianProcessRegressor(kernel=kernel_dims)
A:sklearn.gaussian_process.tests.test_gpr.k2->ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds='fixed')
A:sklearn.gaussian_process.tests.test_gpr.y_constant->numpy.ones(X.shape[0], dtype=np.float64)
A:sklearn.gaussian_process.tests.test_gpr.(Y_pred, Y_cov)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.X_train->numpy.random.RandomState(0).randn(n_samples_train, n_features)
A:sklearn.gaussian_process.tests.test_gpr.y_train->numpy.random.RandomState(0).randn(*y_train_shape)
A:sklearn.gaussian_process.tests.test_gpr.X_test->numpy.random.RandomState(0).randn(n_samples_X_test, n_features)
A:sklearn.gaussian_process.tests.test_gpr.(pred1, std)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(pred2, cov)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X_test, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.model->GaussianProcessRegressor(n_targets=1)
A:sklearn.gaussian_process.tests.test_gpr.(y_pred, y_std)->GaussianProcessRegressor(n_targets=1).predict(X_test, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.y_samples->GaussianProcessRegressor(n_targets=1).sample_y(X_test, n_samples=n_samples_y_test)
A:sklearn.gaussian_process.tests.test_gpr.(mean_prior, cov_prior)->GaussianProcessRegressor(n_targets=1).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, std_prior)->GaussianProcessRegressor(n_targets=1).predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.(mean_post, cov_post)->GaussianProcessRegressor(n_targets=1).predict(X, return_cov=True)
A:sklearn.gaussian_process.tests.test_gpr.(_, std_post)->GaussianProcessRegressor(n_targets=1).predict(X, return_std=True)
A:sklearn.gaussian_process.tests.test_gpr.X2_copy->numpy.copy(X2)
A:sklearn.gaussian_process.tests.test_gpr.(_, _)->GaussianProcessRegressor(kernel=CustomKernel()).fit(X, y).predict(X2, return_std=True)
sklearn.gaussian_process.tests.test_gpr.CustomKernel(C)
sklearn.gaussian_process.tests.test_gpr.CustomKernel.diag(self,X)
sklearn.gaussian_process.tests.test_gpr.f(x)
sklearn.gaussian_process.tests.test_gpr.test_anisotropic_kernel()
sklearn.gaussian_process.tests.test_gpr.test_bound_check_fixed_hyperparameter()
sklearn.gaussian_process.tests.test_gpr.test_constant_target(kernel)
sklearn.gaussian_process.tests.test_gpr.test_converged_to_local_maximum(kernel)
sklearn.gaussian_process.tests.test_gpr.test_custom_optimizer(kernel)
sklearn.gaussian_process.tests.test_gpr.test_duplicate_input(kernel)
sklearn.gaussian_process.tests.test_gpr.test_gpr_consistency_std_cov_non_invertible_kernel()
sklearn.gaussian_process.tests.test_gpr.test_gpr_correct_error_message()
sklearn.gaussian_process.tests.test_gpr.test_gpr_fit_error(params,TypeError,err_msg)
sklearn.gaussian_process.tests.test_gpr.test_gpr_interpolation(kernel)
sklearn.gaussian_process.tests.test_gpr.test_gpr_interpolation_structured()
sklearn.gaussian_process.tests.test_gpr.test_gpr_lml_error()
sklearn.gaussian_process.tests.test_gpr.test_gpr_predict_error()
sklearn.gaussian_process.tests.test_gpr.test_gpr_predict_input_not_modified()
sklearn.gaussian_process.tests.test_gpr.test_large_variance_y()
sklearn.gaussian_process.tests.test_gpr.test_lml_gradient(kernel)
sklearn.gaussian_process.tests.test_gpr.test_lml_improving(kernel)
sklearn.gaussian_process.tests.test_gpr.test_lml_precomputed(kernel)
sklearn.gaussian_process.tests.test_gpr.test_lml_without_cloning_kernel(kernel)
sklearn.gaussian_process.tests.test_gpr.test_n_targets_error()
sklearn.gaussian_process.tests.test_gpr.test_no_fit_default_predict()
sklearn.gaussian_process.tests.test_gpr.test_no_optimizer()
sklearn.gaussian_process.tests.test_gpr.test_predict_cov_vs_std(kernel,target)
sklearn.gaussian_process.tests.test_gpr.test_predict_shape_with_prior(n_targets)
sklearn.gaussian_process.tests.test_gpr.test_predict_shapes(normalize_y,n_targets)
sklearn.gaussian_process.tests.test_gpr.test_prior(kernel)
sklearn.gaussian_process.tests.test_gpr.test_random_starts()
sklearn.gaussian_process.tests.test_gpr.test_sample_statistics(kernel)
sklearn.gaussian_process.tests.test_gpr.test_sample_y_shape_with_prior(n_targets,n_samples)
sklearn.gaussian_process.tests.test_gpr.test_sample_y_shapes(normalize_y,n_targets)
sklearn.gaussian_process.tests.test_gpr.test_solution_inside_bounds(kernel)
sklearn.gaussian_process.tests.test_gpr.test_warning_bounds()
sklearn.gaussian_process.tests.test_gpr.test_y_multioutput()
sklearn.gaussian_process.tests.test_gpr.test_y_normalization(kernel)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/tests/test_gpc.py----------------------------------------
A:sklearn.gaussian_process.tests.test_gpc.y->numpy.array([True, False, True])
A:sklearn.gaussian_process.tests.test_gpc.fX->f(X).ravel()
A:sklearn.gaussian_process.tests.test_gpc.y_mc->numpy.empty(y.shape, dtype=int)
A:sklearn.gaussian_process.tests.test_gpc.fixed_kernel->RBF(length_scale=1.0, length_scale_bounds='fixed')
A:sklearn.gaussian_process.tests.test_gpc.gpc->GaussianProcessClassifier(**params)
A:sklearn.gaussian_process.tests.test_gpc.kernel->RBF(length_scale_bounds=[1e-05, 0.001])
A:sklearn.gaussian_process.tests.test_gpc.input_theta->numpy.ones(gpc.kernel_.theta.shape, dtype=np.float64)
A:sklearn.gaussian_process.tests.test_gpc.(lml, lml_gradient)->GaussianProcessClassifier(**params).log_marginal_likelihood(kernel.theta, True)
A:sklearn.gaussian_process.tests.test_gpc.lml_gradient_approx->approx_fprime(kernel.theta, lambda theta: gpc.log_marginal_likelihood(theta, False), 1e-10)
A:sklearn.gaussian_process.tests.test_gpc.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.gaussian_process.tests.test_gpc.gp->GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=global_random_seed).fit(X, y)
A:sklearn.gaussian_process.tests.test_gpc.lml->GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=n_restarts_optimizer, random_state=global_random_seed).fit(X, y).log_marginal_likelihood(gp.kernel_.theta)
A:sklearn.gaussian_process.tests.test_gpc.theta->numpy.atleast_1d(rng.uniform(np.maximum(-2, bounds[:, 0]), np.minimum(1, bounds[:, 1])))
A:sklearn.gaussian_process.tests.test_gpc.f->obj_func(theta, eval_gradient=False)
A:sklearn.gaussian_process.tests.test_gpc.y_prob->GaussianProcessClassifier(**params).predict_proba(X2)
A:sklearn.gaussian_process.tests.test_gpc.y_pred->GaussianProcessClassifier(**params).predict(X2)
A:sklearn.gaussian_process.tests.test_gpc.gpc_2->GaussianProcessClassifier(kernel=kernel, n_jobs=2)
A:sklearn.gaussian_process.tests.test_gpc.y_prob_2->GaussianProcessClassifier(kernel=kernel, n_jobs=2).predict_proba(X2)
A:sklearn.gaussian_process.tests.test_gpc.gpc_sum->GaussianProcessClassifier(kernel=kernel_sum)
A:sklearn.gaussian_process.tests.test_gpc.X_tile->numpy.tile(X, 2)
A:sklearn.gaussian_process.tests.test_gpc.kernel_dims->RBF(length_scale=[1.0, 2.0], length_scale_bounds=[10.0, 100.0])
A:sklearn.gaussian_process.tests.test_gpc.gpc_dims->GaussianProcessClassifier(kernel=kernel_dims)
sklearn.gaussian_process.tests.test_gpc.f(x)
sklearn.gaussian_process.tests.test_gpc.test_converged_to_local_maximum(kernel)
sklearn.gaussian_process.tests.test_gpc.test_custom_optimizer(kernel,global_random_seed)
sklearn.gaussian_process.tests.test_gpc.test_gpc_fit_error(params,error_type,err_msg)
sklearn.gaussian_process.tests.test_gpc.test_lml_gradient(kernel)
sklearn.gaussian_process.tests.test_gpc.test_lml_improving(kernel)
sklearn.gaussian_process.tests.test_gpc.test_lml_precomputed(kernel)
sklearn.gaussian_process.tests.test_gpc.test_lml_without_cloning_kernel(kernel)
sklearn.gaussian_process.tests.test_gpc.test_multi_class(kernel)
sklearn.gaussian_process.tests.test_gpc.test_multi_class_n_jobs(kernel)
sklearn.gaussian_process.tests.test_gpc.test_predict_consistent(kernel)
sklearn.gaussian_process.tests.test_gpc.test_predict_consistent_structured()
sklearn.gaussian_process.tests.test_gpc.test_random_starts(global_random_seed)
sklearn.gaussian_process.tests.test_gpc.test_warning_bounds()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/tests/test_kernels.py----------------------------------------
A:sklearn.gaussian_process.tests.test_kernels.X->numpy.random.RandomState(0).normal(0, 1, (5, 2))
A:sklearn.gaussian_process.tests.test_kernels.Y->numpy.random.RandomState(0).normal(0, 1, (6, 2))
A:sklearn.gaussian_process.tests.test_kernels.(K, K_gradient)->kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.kernel_clone->RationalQuadratic(length_scale=[1.0, 1.0]).clone_with_theta(theta)
A:sklearn.gaussian_process.tests.test_kernels.K->Matern(nu=np.inf, length_scale=1.0)(X)
A:sklearn.gaussian_process.tests.test_kernels.K_gradient_approx->_approx_fprime(kernel.theta, eval_kernel_for_theta, 1e-10)
A:sklearn.gaussian_process.tests.test_kernels.(_, K_gradient)->kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.init_sign->signature(kernel.__class__.__init__).parameters.values()
A:sklearn.gaussian_process.tests.test_kernels.theta_vars->map(lambda s: s[0:-len('_bounds')], filter(lambda s: s.endswith('_bounds'), args))
A:sklearn.gaussian_process.tests.test_kernels.params->RationalQuadratic(length_scale=[1.0, 1.0]).get_params()
A:sklearn.gaussian_process.tests.test_kernels.new_kernel->kernel_class(**params)
A:sklearn.gaussian_process.tests.test_kernels.(_, K_gradient_new)->new_kernel(X, eval_gradient=True)
A:sklearn.gaussian_process.tests.test_kernels.theta[i]->numpy.log(42)
A:sklearn.gaussian_process.tests.test_kernels.K_auto->kernel(X)
A:sklearn.gaussian_process.tests.test_kernels.K_cross->kernel(X, X)
A:sklearn.gaussian_process.tests.test_kernels.K_call_diag->numpy.diag(kernel(X))
A:sklearn.gaussian_process.tests.test_kernels.K_diag->RationalQuadratic(length_scale=[1.0, 1.0]).diag(X)
A:sklearn.gaussian_process.tests.test_kernels.X1->numpy.array(X)
A:sklearn.gaussian_process.tests.test_kernels.X2->numpy.array(X)
A:sklearn.gaussian_process.tests.test_kernels.kernel->RationalQuadratic(length_scale=[1.0, 1.0])
A:sklearn.gaussian_process.tests.test_kernels.attr_value1->getattr(kernel1, attr)
A:sklearn.gaussian_process.tests.test_kernels.attr_value2->getattr(kernel2, attr)
A:sklearn.gaussian_process.tests.test_kernels.kernel_cloned->clone(kernel)
A:sklearn.gaussian_process.tests.test_kernels.kernel_cloned_clone->clone(kernel_cloned)
A:sklearn.gaussian_process.tests.test_kernels.K_absexp->numpy.exp(-euclidean_distances(X, X, squared=False))
A:sklearn.gaussian_process.tests.test_kernels.K_rbf->RBF(length_scale=1.0)(X)
A:sklearn.gaussian_process.tests.test_kernels.K1->kernel(X, Y)
A:sklearn.gaussian_process.tests.test_kernels.K2->pairwise_kernels(X, Y, metric=kernel)
sklearn.gaussian_process.tests.test_kernels.check_hyperparameters_equal(kernel1,kernel2)
sklearn.gaussian_process.tests.test_kernels.test_auto_vs_cross(kernel)
sklearn.gaussian_process.tests.test_kernels.test_compound_kernel_input_type()
sklearn.gaussian_process.tests.test_kernels.test_kernel_anisotropic()
sklearn.gaussian_process.tests.test_kernels.test_kernel_clone(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_clone_after_set_params(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_diag(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_gradient(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_input_type(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_operator_commutative()
sklearn.gaussian_process.tests.test_kernels.test_kernel_stationary(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_theta(kernel)
sklearn.gaussian_process.tests.test_kernels.test_kernel_versus_pairwise(kernel)
sklearn.gaussian_process.tests.test_kernels.test_matern_kernel()
sklearn.gaussian_process.tests.test_kernels.test_rational_quadratic_kernel()
sklearn.gaussian_process.tests.test_kernels.test_repr_kernels(kernel)
sklearn.gaussian_process.tests.test_kernels.test_set_get_params(kernel)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/gaussian_process/tests/_mini_sequence_kernel.py----------------------------------------
A:sklearn.gaussian_process.tests._mini_sequence_kernel.cloned->clone(self)
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel(self,baseline_similarity=0.5,baseline_similarity_bounds=(1e-05,1))
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel.__init__(self,baseline_similarity=0.5,baseline_similarity_bounds=(1e-05,1))
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel._f(self,s1,s2)
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel._g(self,s1,s2)
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel.clone_with_theta(self,theta)
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel.diag(self,X)
sklearn.gaussian_process.tests._mini_sequence_kernel.MiniSeqKernel.hyperparameter_baseline_similarity(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_dbscan.py----------------------------------------
A:sklearn.cluster._dbscan.est->DBSCAN(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs)
A:sklearn.cluster._dbscan.X->X.copy().copy()
A:sklearn.cluster._dbscan.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.cluster._dbscan.neighbors_model->NearestNeighbors(radius=self.eps, algorithm=self.algorithm, leaf_size=self.leaf_size, metric=self.metric, metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
A:sklearn.cluster._dbscan.neighborhoods->NearestNeighbors(radius=self.eps, algorithm=self.algorithm, leaf_size=self.leaf_size, metric=self.metric, metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs).radius_neighbors(X, return_distance=False)
A:sklearn.cluster._dbscan.n_neighbors->numpy.array([np.sum(sample_weight[neighbors]) for neighbors in neighborhoods])
A:sklearn.cluster._dbscan.labels->numpy.full(X.shape[0], -1, dtype=np.intp)
A:sklearn.cluster._dbscan.core_samples->numpy.asarray(n_neighbors >= self.min_samples, dtype=np.uint8)
A:sklearn.cluster._dbscan.self.components_->numpy.empty((0, X.shape[1]))
sklearn.cluster.DBSCAN(self,eps=0.5,*,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=None)
sklearn.cluster.DBSCAN._more_tags(self)
sklearn.cluster.DBSCAN.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.DBSCAN.fit_predict(self,X,y=None,sample_weight=None)
sklearn.cluster._dbscan.DBSCAN(self,eps=0.5,*,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=None)
sklearn.cluster._dbscan.DBSCAN.__init__(self,eps=0.5,*,min_samples=5,metric='euclidean',metric_params=None,algorithm='auto',leaf_size=30,p=None,n_jobs=None)
sklearn.cluster._dbscan.DBSCAN._more_tags(self)
sklearn.cluster._dbscan.DBSCAN.fit(self,X,y=None,sample_weight=None)
sklearn.cluster._dbscan.DBSCAN.fit_predict(self,X,y=None,sample_weight=None)
sklearn.cluster._dbscan.dbscan(X,eps=0.5,*,min_samples=5,metric='minkowski',metric_params=None,algorithm='auto',leaf_size=30,p=2,sample_weight=None,n_jobs=None)
sklearn.cluster.dbscan(X,eps=0.5,*,min_samples=5,metric='minkowski',metric_params=None,algorithm='auto',leaf_size=30,p=2,sample_weight=None,n_jobs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_birch.py----------------------------------------
A:sklearn.cluster._birch.row->numpy.zeros(X.shape[1])
A:sklearn.cluster._birch.new_subcluster1->_CFSubcluster()
A:sklearn.cluster._birch.new_subcluster2->_CFSubcluster()
A:sklearn.cluster._birch.new_node1->_CFNode(threshold=threshold, branching_factor=branching_factor, is_leaf=node.is_leaf, n_features=node.n_features, dtype=node.init_centroids_.dtype)
A:sklearn.cluster._birch.new_node2->_CFNode(threshold=threshold, branching_factor=branching_factor, is_leaf=node.is_leaf, n_features=node.n_features, dtype=node.init_centroids_.dtype)
A:sklearn.cluster._birch.dist->euclidean_distances(node.centroids_, Y_norm_squared=node.squared_norm_, squared=True)
A:sklearn.cluster._birch.farthest_idx->numpy.unravel_index(dist.argmax(), (n_clusters, n_clusters))
A:sklearn.cluster._birch.self.init_centroids_->numpy.zeros((branching_factor + 1, n_features), dtype=dtype)
A:sklearn.cluster._birch.self.init_sq_norm_->numpy.zeros(branching_factor + 1, dtype)
A:sklearn.cluster._birch.n_samples->len(self.subclusters_)
A:sklearn.cluster._birch.ind->self.subclusters_.index(subcluster)
A:sklearn.cluster._birch.dist_matrix->numpy.dot(self.centroids_, subcluster.centroid_)
A:sklearn.cluster._birch.closest_index->numpy.argmin(dist_matrix)
A:sklearn.cluster._birch.split_child->closest_subcluster.child_.insert_cf_subcluster(subcluster)
A:sklearn.cluster._birch.(new_subcluster1, new_subcluster2)->_split_node(self.root_, threshold, branching_factor)
A:sklearn.cluster._birch.merged->closest_subcluster.merge_subcluster(subcluster, self.threshold)
A:sklearn.cluster._birch.self.squared_sum_self.sq_norm_->numpy.dot(self.linear_sum_, self.linear_sum_)
A:sklearn.cluster._birch.self.sq_norm_->numpy.dot(self.centroid_, self.centroid_)
A:sklearn.cluster._birch.new_sq_norm->numpy.dot(new_centroid, new_centroid)
A:sklearn.cluster._birch.has_root->getattr(self, 'root_', None)
A:sklearn.cluster._birch.X->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.cluster._birch.self.root_->_CFNode(threshold=threshold, branching_factor=branching_factor, is_leaf=False, n_features=n_features, dtype=X.dtype)
A:sklearn.cluster._birch.self.dummy_leaf_->_CFNode(threshold=threshold, branching_factor=branching_factor, is_leaf=True, n_features=n_features, dtype=X.dtype)
A:sklearn.cluster._birch.subcluster->_CFSubcluster(linear_sum=sample)
A:sklearn.cluster._birch.split->self.root_.insert_cf_subcluster(subcluster)
A:sklearn.cluster._birch.centroids->numpy.concatenate([leaf.centroids_ for leaf in self._get_leaves()])
A:sklearn.cluster._birch.argmin->pairwise_distances_argmin(X, self.subcluster_centers_, metric_kwargs=kwargs)
A:sklearn.cluster._birch.clusterer->AgglomerativeClustering(n_clusters=self.n_clusters)
A:sklearn.cluster._birch.self._subcluster_norms->row_norms(self.subcluster_centers_, squared=True)
A:sklearn.cluster._birch.self.subcluster_labels_->AgglomerativeClustering(n_clusters=self.n_clusters).fit_predict(self.subcluster_centers_)
A:sklearn.cluster._birch.self.labels_->self._predict(X)
sklearn.cluster.Birch(self,*,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster.Birch._check_fit(self,X)
sklearn.cluster.Birch._fit(self,X,partial)
sklearn.cluster.Birch._get_leaves(self)
sklearn.cluster.Birch._global_clustering(self,X=None)
sklearn.cluster.Birch._more_tags(self)
sklearn.cluster.Birch._predict(self,X)
sklearn.cluster.Birch.fit(self,X,y=None)
sklearn.cluster.Birch.partial_fit(self,X=None,y=None)
sklearn.cluster.Birch.predict(self,X)
sklearn.cluster.Birch.transform(self,X)
sklearn.cluster._birch.Birch(self,*,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster._birch.Birch.__init__(self,*,threshold=0.5,branching_factor=50,n_clusters=3,compute_labels=True,copy=True)
sklearn.cluster._birch.Birch._check_fit(self,X)
sklearn.cluster._birch.Birch._fit(self,X,partial)
sklearn.cluster._birch.Birch._get_leaves(self)
sklearn.cluster._birch.Birch._global_clustering(self,X=None)
sklearn.cluster._birch.Birch._more_tags(self)
sklearn.cluster._birch.Birch._predict(self,X)
sklearn.cluster._birch.Birch.fit(self,X,y=None)
sklearn.cluster._birch.Birch.partial_fit(self,X=None,y=None)
sklearn.cluster._birch.Birch.predict(self,X)
sklearn.cluster._birch.Birch.transform(self,X)
sklearn.cluster._birch._CFNode(self,*,threshold,branching_factor,is_leaf,n_features,dtype)
sklearn.cluster._birch._CFNode.__init__(self,*,threshold,branching_factor,is_leaf,n_features,dtype)
sklearn.cluster._birch._CFNode.append_subcluster(self,subcluster)
sklearn.cluster._birch._CFNode.insert_cf_subcluster(self,subcluster)
sklearn.cluster._birch._CFNode.update_split_subclusters(self,subcluster,new_subcluster1,new_subcluster2)
sklearn.cluster._birch._CFSubcluster(self,*,linear_sum=None)
sklearn.cluster._birch._CFSubcluster.__init__(self,*,linear_sum=None)
sklearn.cluster._birch._CFSubcluster.merge_subcluster(self,nominee_cluster,threshold)
sklearn.cluster._birch._CFSubcluster.radius(self)
sklearn.cluster._birch._CFSubcluster.update(self,subcluster)
sklearn.cluster._birch._iterate_sparse_X(X)
sklearn.cluster._birch._split_node(node,threshold,branching_factor)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_agglomerative.py----------------------------------------
A:sklearn.cluster._agglomerative.connectivity->check_array(connectivity, accept_sparse=['csr', 'coo', 'lil'])
A:sklearn.cluster._agglomerative.(n_connected_components, labels)->connected_components(connectivity)
A:sklearn.cluster._agglomerative.mst->_hierarchical.mst_linkage_core(X, dist_metric)
A:sklearn.cluster._agglomerative.single_linkage_tree->_hierarchical._single_linkage_label(mst_array)
A:sklearn.cluster._agglomerative.children_->out[:, :2].astype(int, copy=False)
A:sklearn.cluster._agglomerative.parent->numpy.arange(n_nodes, dtype=np.intp)
A:sklearn.cluster._agglomerative.X->self._validate_data(X, ensure_min_features=2)
A:sklearn.cluster._agglomerative.out->check_memory(self.memory).cache(tree_builder)(X, connectivity=connectivity, n_clusters=n_clusters, return_distance=return_distance, **kwargs)
A:sklearn.cluster._agglomerative.(connectivity, n_connected_components)->_fix_connectivity(X, connectivity, affinity=affinity)
A:sklearn.cluster._agglomerative.coord_row->numpy.empty(coord_col.shape, dtype=np.intp, order='C')
A:sklearn.cluster._agglomerative.coord_col->join_func(A[i], A[j], used_node, n_i, n_j)
A:sklearn.cluster._agglomerative.moments_1->numpy.zeros(n_nodes, order='C')
A:sklearn.cluster._agglomerative.moments_2->numpy.zeros((n_nodes, n_features), order='C')
A:sklearn.cluster._agglomerative.inertia->list()
A:sklearn.cluster._agglomerative.used_node->numpy.ones(n_nodes, dtype=np.intp)
A:sklearn.cluster._agglomerative.distances->numpy.empty(n_nodes - n_samples)
A:sklearn.cluster._agglomerative.not_visited->numpy.empty(n_nodes, dtype=bool, order='C')
A:sklearn.cluster._agglomerative.(inert, i, j)->heappop(inertia)
A:sklearn.cluster._agglomerative.n_additions->len(coord_row)
A:sklearn.cluster._agglomerative.ini->numpy.empty(n_additions, dtype=np.float64, order='C')
A:sklearn.cluster._agglomerative.children->numpy.array(children)
A:sklearn.cluster._agglomerative.(i, j)->numpy.triu_indices(X.shape[0], k=1)
A:sklearn.cluster._agglomerative.dist_metric->metrics.DistanceMetric.get_metric(affinity)
A:sklearn.cluster._agglomerative.A->numpy.empty(n_nodes, dtype=object)
A:sklearn.cluster._agglomerative.A[ind]->IntFloatDict(np.asarray(row, dtype=np.intp), np.asarray(data, dtype=np.float64))
A:sklearn.cluster._agglomerative.edge->heappop(inertia)
A:sklearn.cluster._agglomerative._TREE_BUILDERS->dict(ward=ward_tree, complete=_complete_linkage, average=_average_linkage, single=_single_linkage)
A:sklearn.cluster._agglomerative.label->numpy.zeros(n_leaves, dtype=np.intp)
A:sklearn.cluster._agglomerative.memory->check_memory(self.memory)
A:sklearn.cluster._agglomerative.n_samples->len(X)
A:sklearn.cluster._agglomerative.self.labels_->numpy.searchsorted(np.unique(labels), labels)
A:sklearn.cluster._agglomerative.labels->numpy.copy(labels[:n_samples])
sklearn.cluster.AgglomerativeClustering(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',distance_threshold=None,compute_distances=False)
sklearn.cluster.AgglomerativeClustering._fit(self,X)
sklearn.cluster.AgglomerativeClustering.fit(self,X,y=None)
sklearn.cluster.AgglomerativeClustering.fit_predict(self,X,y=None)
sklearn.cluster.FeatureAgglomeration(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean,distance_threshold=None,compute_distances=False)
sklearn.cluster.FeatureAgglomeration.fit(self,X,y=None)
sklearn.cluster.FeatureAgglomeration.fit_predict(self)
sklearn.cluster._agglomerative.AgglomerativeClustering(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',distance_threshold=None,compute_distances=False)
sklearn.cluster._agglomerative.AgglomerativeClustering.__init__(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',distance_threshold=None,compute_distances=False)
sklearn.cluster._agglomerative.AgglomerativeClustering._fit(self,X)
sklearn.cluster._agglomerative.AgglomerativeClustering.fit(self,X,y=None)
sklearn.cluster._agglomerative.AgglomerativeClustering.fit_predict(self,X,y=None)
sklearn.cluster._agglomerative.FeatureAgglomeration(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean,distance_threshold=None,compute_distances=False)
sklearn.cluster._agglomerative.FeatureAgglomeration.__init__(self,n_clusters=2,*,metric='euclidean',memory=None,connectivity=None,compute_full_tree='auto',linkage='ward',pooling_func=np.mean,distance_threshold=None,compute_distances=False)
sklearn.cluster._agglomerative.FeatureAgglomeration.fit(self,X,y=None)
sklearn.cluster._agglomerative.FeatureAgglomeration.fit_predict(self)
sklearn.cluster._agglomerative._average_linkage(*args,**kwargs)
sklearn.cluster._agglomerative._complete_linkage(*args,**kwargs)
sklearn.cluster._agglomerative._fix_connectivity(X,connectivity,affinity)
sklearn.cluster._agglomerative._hc_cut(n_clusters,children,n_leaves)
sklearn.cluster._agglomerative._single_linkage(*args,**kwargs)
sklearn.cluster._agglomerative._single_linkage_tree(connectivity,n_samples,n_nodes,n_clusters,n_connected_components,return_distance)
sklearn.cluster._agglomerative.linkage_tree(X,connectivity=None,n_clusters=None,linkage='complete',affinity='euclidean',return_distance=False)
sklearn.cluster._agglomerative.ward_tree(X,*,connectivity=None,n_clusters=None,return_distance=False)
sklearn.cluster.linkage_tree(X,connectivity=None,n_clusters=None,linkage='complete',affinity='euclidean',return_distance=False)
sklearn.cluster.ward_tree(X,*,connectivity=None,n_clusters=None,return_distance=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_bicluster.py----------------------------------------
A:sklearn.cluster._bicluster.X->self._validate_data(X, accept_sparse='csr', dtype=np.float64)
A:sklearn.cluster._bicluster.row_diag->numpy.where(np.isnan(row_diag), 0, row_diag)
A:sklearn.cluster._bicluster.col_diag->numpy.where(np.isnan(col_diag), 0, col_diag)
A:sklearn.cluster._bicluster.r->dia_matrix((row_diag, [0]), shape=(n_rows, n_rows))
A:sklearn.cluster._bicluster.c->dia_matrix((col_diag, [0]), shape=(n_cols, n_cols))
A:sklearn.cluster._bicluster.(X_new, _, _)->_scale_normalize(X_scaled)
A:sklearn.cluster._bicluster.dist->norm(X_scaled - X_new)
A:sklearn.cluster._bicluster.L->numpy.log(X)
A:sklearn.cluster._bicluster.col_avg->numpy.log(X).mean(axis=0)
A:sklearn.cluster._bicluster.avg->numpy.log(X).mean()
A:sklearn.cluster._bicluster.(u, _, vt)->svds(array, k=n_components, ncv=self.n_svd_vecs)
A:sklearn.cluster._bicluster.A->safe_sparse_dot(array, array.T)
A:sklearn.cluster._bicluster.random_state->check_random_state(self.random_state)
A:sklearn.cluster._bicluster.v0->check_random_state(self.random_state).uniform(-1, 1, A.shape[0])
A:sklearn.cluster._bicluster.(_, v)->eigsh(A, ncv=self.n_svd_vecs, v0=v0)
A:sklearn.cluster._bicluster.(_, u)->eigsh(A, ncv=self.n_svd_vecs, v0=v0)
A:sklearn.cluster._bicluster.model->KMeans(n_clusters, init=self.init, n_init=self.n_init, random_state=self.random_state)
A:sklearn.cluster._bicluster.(normalized_data, row_diag, col_diag)->_scale_normalize(X)
A:sklearn.cluster._bicluster.(u, v)->self._svd(normalized_data, n_sv, n_discard)
A:sklearn.cluster._bicluster.z->numpy.vstack((row_diag[:, np.newaxis] * u, col_diag[:, np.newaxis] * v))
A:sklearn.cluster._bicluster.(_, labels)->self._k_means(projected, n_clusters)
A:sklearn.cluster._bicluster.self.rows_->numpy.vstack([self.row_labels_ == label for label in range(n_row_clusters) for _ in range(n_col_clusters)])
A:sklearn.cluster._bicluster.self.columns_->numpy.vstack([self.column_labels_ == label for _ in range(n_row_clusters) for label in range(n_col_clusters)])
A:sklearn.cluster._bicluster.normalized_data->_log_normalize(X)
A:sklearn.cluster._bicluster.(normalized_data, _, _)->_scale_normalize(X)
A:sklearn.cluster._bicluster.best_ut->self._fit_best_piecewise(ut, self.n_best, n_row_clusters)
A:sklearn.cluster._bicluster.best_vt->self._fit_best_piecewise(vt, self.n_best, n_col_clusters)
A:sklearn.cluster._bicluster.self.row_labels_->self._project_and_cluster(X, best_vt.T, n_row_clusters)
A:sklearn.cluster._bicluster.self.column_labels_->self._project_and_cluster(X.T, best_ut.T, n_col_clusters)
A:sklearn.cluster._bicluster.(centroid, labels)->self._k_means(v.reshape(-1, 1), n_clusters)
A:sklearn.cluster._bicluster.piecewise_vectors->numpy.apply_along_axis(make_piecewise, axis=1, arr=vectors)
A:sklearn.cluster._bicluster.dists->numpy.apply_along_axis(norm, axis=1, arr=vectors - piecewise_vectors)
A:sklearn.cluster._bicluster.projected->safe_sparse_dot(data, vectors)
sklearn.cluster.SpectralBiclustering(self,n_clusters=3,*,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster.SpectralBiclustering._check_parameters(self,n_samples)
sklearn.cluster.SpectralBiclustering._fit(self,X)
sklearn.cluster.SpectralBiclustering._fit_best_piecewise(self,vectors,n_best,n_clusters)
sklearn.cluster.SpectralBiclustering._project_and_cluster(self,data,vectors,n_clusters)
sklearn.cluster.SpectralCoclustering(self,n_clusters=3,*,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster.SpectralCoclustering._check_parameters(self,n_samples)
sklearn.cluster.SpectralCoclustering._fit(self,X)
sklearn.cluster._bicluster.BaseSpectral(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.BaseSpectral.__init__(self,n_clusters=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.BaseSpectral._check_parameters(self,n_samples)
sklearn.cluster._bicluster.BaseSpectral._k_means(self,data,n_clusters)
sklearn.cluster._bicluster.BaseSpectral._more_tags(self)
sklearn.cluster._bicluster.BaseSpectral._svd(self,array,n_components,n_discard)
sklearn.cluster._bicluster.BaseSpectral.fit(self,X,y=None)
sklearn.cluster._bicluster.SpectralBiclustering(self,n_clusters=3,*,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.SpectralBiclustering.__init__(self,n_clusters=3,*,method='bistochastic',n_components=6,n_best=3,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.SpectralBiclustering._check_parameters(self,n_samples)
sklearn.cluster._bicluster.SpectralBiclustering._fit(self,X)
sklearn.cluster._bicluster.SpectralBiclustering._fit_best_piecewise(self,vectors,n_best,n_clusters)
sklearn.cluster._bicluster.SpectralBiclustering._project_and_cluster(self,data,vectors,n_clusters)
sklearn.cluster._bicluster.SpectralCoclustering(self,n_clusters=3,*,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.SpectralCoclustering.__init__(self,n_clusters=3,*,svd_method='randomized',n_svd_vecs=None,mini_batch=False,init='k-means++',n_init=10,random_state=None)
sklearn.cluster._bicluster.SpectralCoclustering._check_parameters(self,n_samples)
sklearn.cluster._bicluster.SpectralCoclustering._fit(self,X)
sklearn.cluster._bicluster._bistochastic_normalize(X,max_iter=1000,tol=1e-05)
sklearn.cluster._bicluster._log_normalize(X)
sklearn.cluster._bicluster._scale_normalize(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_bisect_k_means.py----------------------------------------
A:sklearn.cluster._bisect_k_means.self.left->_BisectingTree(indices=self.indices[labels == 0], center=centers[0], score=scores[0])
A:sklearn.cluster._bisect_k_means.self.right->_BisectingTree(indices=self.indices[labels == 1], center=centers[1], score=scores[1])
A:sklearn.cluster._bisect_k_means.inertia_per_cluster->numpy.empty(n_clusters)
A:sklearn.cluster._bisect_k_means.inertia_per_cluster[label]->_inertia(X, sample_weight, centers, labels, self._n_threads, single_label=label)
A:sklearn.cluster._bisect_k_means.centers_init->self._init_centroids(X, x_squared_norms=x_squared_norms, init=self.init, random_state=self._random_state, n_centroids=2, sample_weight=sample_weight)
A:sklearn.cluster._bisect_k_means.(labels, inertia, centers, _)->self._kmeans_single(X, sample_weight, centers_init, max_iter=self.max_iter, verbose=self.verbose, tol=self.tol, n_threads=self._n_threads)
A:sklearn.cluster._bisect_k_means.scores->numpy.bincount(best_labels, minlength=2)
A:sklearn.cluster._bisect_k_means.X->self._check_test_data(X)
A:sklearn.cluster._bisect_k_means.self._random_state->check_random_state(self.random_state)
A:sklearn.cluster._bisect_k_means.sample_weight->numpy.ones_like(x_squared_norms)
A:sklearn.cluster._bisect_k_means.self._n_threads->_openmp_effective_n_threads()
A:sklearn.cluster._bisect_k_means.self._X_mean->self._check_test_data(X).mean(axis=0)
A:sklearn.cluster._bisect_k_means.self._bisecting_tree->_BisectingTree(indices=np.arange(X.shape[0]), center=X.mean(axis=0), score=0)
A:sklearn.cluster._bisect_k_means.x_squared_norms->row_norms(X, squared=True)
A:sklearn.cluster._bisect_k_means.cluster_to_bisect->self._bisecting_tree.get_cluster_to_bisect()
A:sklearn.cluster._bisect_k_means.self.labels_->numpy.full(X.shape[0], -1, dtype=np.int32)
A:sklearn.cluster._bisect_k_means.self.cluster_centers_->numpy.empty((self.n_clusters, X.shape[1]), dtype=X.dtype)
A:sklearn.cluster._bisect_k_means.self.inertia_->_inertia(X, sample_weight, self.cluster_centers_, self.labels_, self._n_threads)
A:sklearn.cluster._bisect_k_means.labels->numpy.full(X.shape[0], -1, dtype=np.int32)
A:sklearn.cluster._bisect_k_means.centers->numpy.vstack((cluster_node.left.center, cluster_node.right.center))
A:sklearn.cluster._bisect_k_means.cluster_labels->_labels_inertia_threadpool_limit(X, sample_weight, centers, self._n_threads, return_inertia=False)
A:sklearn.cluster._bisect_k_means.labels[mask]->self._predict_recursive(X[mask], sample_weight[mask], cluster_node.left)
A:sklearn.cluster._bisect_k_means.labels[~mask]->self._predict_recursive(X[~mask], sample_weight[~mask], cluster_node.right)
sklearn.cluster.BisectingKMeans(self,n_clusters=8,*,init='random',n_init=1,random_state=None,max_iter=300,verbose=0,tol=0.0001,copy_x=True,algorithm='lloyd',bisecting_strategy='biggest_inertia')
sklearn.cluster.BisectingKMeans._bisect(self,X,x_squared_norms,sample_weight,cluster_to_bisect)
sklearn.cluster.BisectingKMeans._inertia_per_cluster(self,X,centers,labels,sample_weight)
sklearn.cluster.BisectingKMeans._more_tags(self)
sklearn.cluster.BisectingKMeans._predict_recursive(self,X,sample_weight,cluster_node)
sklearn.cluster.BisectingKMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster.BisectingKMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.BisectingKMeans.predict(self,X)
sklearn.cluster._bisect_k_means.BisectingKMeans(self,n_clusters=8,*,init='random',n_init=1,random_state=None,max_iter=300,verbose=0,tol=0.0001,copy_x=True,algorithm='lloyd',bisecting_strategy='biggest_inertia')
sklearn.cluster._bisect_k_means.BisectingKMeans.__init__(self,n_clusters=8,*,init='random',n_init=1,random_state=None,max_iter=300,verbose=0,tol=0.0001,copy_x=True,algorithm='lloyd',bisecting_strategy='biggest_inertia')
sklearn.cluster._bisect_k_means.BisectingKMeans._bisect(self,X,x_squared_norms,sample_weight,cluster_to_bisect)
sklearn.cluster._bisect_k_means.BisectingKMeans._inertia_per_cluster(self,X,centers,labels,sample_weight)
sklearn.cluster._bisect_k_means.BisectingKMeans._more_tags(self)
sklearn.cluster._bisect_k_means.BisectingKMeans._predict_recursive(self,X,sample_weight,cluster_node)
sklearn.cluster._bisect_k_means.BisectingKMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster._bisect_k_means.BisectingKMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster._bisect_k_means.BisectingKMeans.predict(self,X)
sklearn.cluster._bisect_k_means._BisectingTree(self,center,indices,score)
sklearn.cluster._bisect_k_means._BisectingTree.__init__(self,center,indices,score)
sklearn.cluster._bisect_k_means._BisectingTree.get_cluster_to_bisect(self)
sklearn.cluster._bisect_k_means._BisectingTree.iter_leaves(self)
sklearn.cluster._bisect_k_means._BisectingTree.split(self,labels,centers,scores)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_optics.py----------------------------------------
A:sklearn.cluster._optics.X->self._validate_data(X, dtype=dtype, accept_sparse='csr')
A:sklearn.cluster._optics.memory->check_memory(self.memory)
A:sklearn.cluster._optics.(self.ordering_, self.core_distances_, self.reachability_, self.predecessor_)->check_memory(self.memory).cache(compute_optics_graph)(X=X, min_samples=self.min_samples, algorithm=self.algorithm, leaf_size=self.leaf_size, metric=self.metric, metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs, max_eps=self.max_eps)
A:sklearn.cluster._optics.(labels_, clusters_)->cluster_optics_xi(reachability=self.reachability_, predecessor=self.predecessor_, ordering=self.ordering_, min_samples=self.min_samples, min_cluster_size=self.min_cluster_size, xi=self.xi, predecessor_correction=self.predecessor_correction)
A:sklearn.cluster._optics.labels_->cluster_optics_dbscan(reachability=self.reachability_, core_distances=self.core_distances_, ordering=self.ordering_, eps=eps)
A:sklearn.cluster._optics.core_distances->numpy.empty(n_samples)
A:sklearn.cluster._optics.chunk_n_rows->get_chunk_n_rows(row_bytes=16 * min_samples, max_n_rows=n_samples, working_memory=working_memory)
A:sklearn.cluster._optics.slices->gen_batches(n_samples, chunk_n_rows)
A:sklearn.cluster._optics.min_samples->max(2, int(min_samples * n_samples))
A:sklearn.cluster._optics.reachability_->numpy.empty(n_samples)
A:sklearn.cluster._optics.predecessor_->numpy.empty(n_samples, dtype=int)
A:sklearn.cluster._optics.nbrs->NearestNeighbors(n_neighbors=min_samples, algorithm=algorithm, leaf_size=leaf_size, metric=metric, metric_params=metric_params, p=p, n_jobs=n_jobs)
A:sklearn.cluster._optics.core_distances_->_compute_core_distances_(X=X, neighbors=nbrs, min_samples=min_samples, working_memory=None)
A:sklearn.cluster._optics.processed->numpy.zeros(X.shape[0], dtype=bool)
A:sklearn.cluster._optics.ordering->numpy.zeros(X.shape[0], dtype=int)
A:sklearn.cluster._optics.unproc->numpy.compress(~np.take(processed, indices), indices)
A:sklearn.cluster._optics.dists->pairwise_distances(P, X[unproc], metric, n_jobs=None, **_params).ravel()
A:sklearn.cluster._optics.rdists->numpy.maximum(dists, core_distances_[point_index])
A:sklearn.cluster._optics.improved->numpy.where(rdists < np.take(reachability_, unproc))
A:sklearn.cluster._optics.n_samples->len(steep_point)
A:sklearn.cluster._optics.labels->numpy.full(len(ordering), -1, dtype=int)
A:sklearn.cluster._optics.min_cluster_size->max(2, int(min_cluster_size * n_samples))
A:sklearn.cluster._optics.clusters->_xi_cluster(reachability[ordering], predecessor[ordering], ordering, xi, min_samples, min_cluster_size, predecessor_correction)
A:sklearn.cluster._optics.sda['mib']->max(sda['mib'], mib)
A:sklearn.cluster._optics.reachability_plot->numpy.hstack((reachability_plot, np.inf))
A:sklearn.cluster._optics.mib->max(mib, np.max(reachability_plot[index:steep_index + 1]))
A:sklearn.cluster._optics.sdas->_update_filter_sdas(sdas, mib, xi_complement, reachability_plot)
A:sklearn.cluster._optics.D_end->_extend_region(steep_downward, upward, D_start, min_samples)
A:sklearn.cluster._optics.U_end->_extend_region(steep_upward, downward, U_start, min_samples)
A:sklearn.cluster._optics.(c_start, c_end)->_correct_predecessor(reachability_plot, predecessor_plot, ordering, c_start, c_end)
A:sklearn.cluster._optics.labels[ordering]->numpy.full(len(ordering), -1, dtype=int).copy()
sklearn.cluster.OPTICS(self,*,min_samples=5,max_eps=np.inf,metric='minkowski',p=2,metric_params=None,cluster_method='xi',eps=None,xi=0.05,predecessor_correction=True,min_cluster_size=None,algorithm='auto',leaf_size=30,memory=None,n_jobs=None)
sklearn.cluster.OPTICS.fit(self,X,y=None)
sklearn.cluster._optics.OPTICS(self,*,min_samples=5,max_eps=np.inf,metric='minkowski',p=2,metric_params=None,cluster_method='xi',eps=None,xi=0.05,predecessor_correction=True,min_cluster_size=None,algorithm='auto',leaf_size=30,memory=None,n_jobs=None)
sklearn.cluster._optics.OPTICS.__init__(self,*,min_samples=5,max_eps=np.inf,metric='minkowski',p=2,metric_params=None,cluster_method='xi',eps=None,xi=0.05,predecessor_correction=True,min_cluster_size=None,algorithm='auto',leaf_size=30,memory=None,n_jobs=None)
sklearn.cluster._optics.OPTICS.fit(self,X,y=None)
sklearn.cluster._optics._compute_core_distances_(X,neighbors,min_samples,working_memory)
sklearn.cluster._optics._correct_predecessor(reachability_plot,predecessor_plot,ordering,s,e)
sklearn.cluster._optics._extend_region(steep_point,xward_point,start,min_samples)
sklearn.cluster._optics._extract_xi_labels(ordering,clusters)
sklearn.cluster._optics._set_reach_dist(core_distances_,reachability_,predecessor_,point_index,processed,X,nbrs,metric,metric_params,p,max_eps)
sklearn.cluster._optics._update_filter_sdas(sdas,mib,xi_complement,reachability_plot)
sklearn.cluster._optics._validate_size(size,n_samples,param_name)
sklearn.cluster._optics._xi_cluster(reachability_plot,predecessor_plot,ordering,xi,min_samples,min_cluster_size,predecessor_correction)
sklearn.cluster._optics.cluster_optics_dbscan(*,reachability,core_distances,ordering,eps)
sklearn.cluster._optics.cluster_optics_xi(*,reachability,predecessor,ordering,min_samples,min_cluster_size=None,xi=0.05,predecessor_correction=True)
sklearn.cluster._optics.compute_optics_graph(X,*,min_samples,max_eps,metric,p,metric_params,algorithm,leaf_size,n_jobs)
sklearn.cluster.cluster_optics_dbscan(*,reachability,core_distances,ordering,eps)
sklearn.cluster.cluster_optics_xi(*,reachability,predecessor,ordering,min_samples,min_cluster_size=None,xi=0.05,predecessor_correction=True)
sklearn.cluster.compute_optics_graph(X,*,min_samples,max_eps,metric,p,metric_params,algorithm,leaf_size,n_jobs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_mean_shift.py----------------------------------------
A:sklearn.cluster._mean_shift.X->self._validate_data(X, reset=False)
A:sklearn.cluster._mean_shift.random_state->check_random_state(random_state)
A:sklearn.cluster._mean_shift.n_neighbors->int(X.shape[0] * quantile)
A:sklearn.cluster._mean_shift.nbrs->NearestNeighbors(n_neighbors=1, n_jobs=self.n_jobs).fit(cluster_centers)
A:sklearn.cluster._mean_shift.(d, _)->NearestNeighbors(n_neighbors=1, n_jobs=self.n_jobs).fit(cluster_centers).kneighbors(X[batch, :], return_distance=True)
A:sklearn.cluster._mean_shift.my_mean->numpy.mean(points_within, axis=0)
A:sklearn.cluster._mean_shift.model->MeanShift(bandwidth=bandwidth, seeds=seeds, min_bin_freq=min_bin_freq, bin_seeding=bin_seeding, cluster_all=cluster_all, n_jobs=n_jobs, max_iter=max_iter).fit(X)
A:sklearn.cluster._mean_shift.bin_sizes->defaultdict(int)
A:sklearn.cluster._mean_shift.binned_point->numpy.round(point / bin_size)
A:sklearn.cluster._mean_shift.bin_seeds->numpy.array([point for (point, freq) in bin_sizes.items() if freq >= min_bin_freq], dtype=np.float32)
A:sklearn.cluster._mean_shift.bandwidth->estimate_bandwidth(X, n_jobs=self.n_jobs)
A:sklearn.cluster._mean_shift.seeds->get_bin_seeds(X, bandwidth, self.min_bin_freq)
A:sklearn.cluster._mean_shift.all_res->Parallel(n_jobs=self.n_jobs)((delayed(_mean_shift_single_seed)(seed, X, nbrs, self.max_iter) for seed in seeds))
A:sklearn.cluster._mean_shift.self.n_iter_->max([x[2] for x in all_res])
A:sklearn.cluster._mean_shift.sorted_by_intensity->sorted(center_intensity_dict.items(), key=lambda tup: (tup[1], tup[0]), reverse=True)
A:sklearn.cluster._mean_shift.sorted_centers->numpy.array([tup[0] for tup in sorted_by_intensity])
A:sklearn.cluster._mean_shift.unique->numpy.ones(len(sorted_centers), dtype=bool)
A:sklearn.cluster._mean_shift.labels->idxs.flatten()
A:sklearn.cluster._mean_shift.(distances, idxs)->NearestNeighbors(n_neighbors=1, n_jobs=self.n_jobs).fit(cluster_centers).kneighbors(X)
sklearn.cluster.MeanShift(self,*,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=None,max_iter=300)
sklearn.cluster.MeanShift.fit(self,X,y=None)
sklearn.cluster.MeanShift.predict(self,X)
sklearn.cluster._mean_shift.MeanShift(self,*,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=None,max_iter=300)
sklearn.cluster._mean_shift.MeanShift.__init__(self,*,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,n_jobs=None,max_iter=300)
sklearn.cluster._mean_shift.MeanShift.fit(self,X,y=None)
sklearn.cluster._mean_shift.MeanShift.predict(self,X)
sklearn.cluster._mean_shift._mean_shift_single_seed(my_mean,X,nbrs,max_iter)
sklearn.cluster._mean_shift.estimate_bandwidth(X,*,quantile=0.3,n_samples=None,random_state=0,n_jobs=None)
sklearn.cluster._mean_shift.get_bin_seeds(X,bin_size,min_bin_freq=1)
sklearn.cluster._mean_shift.mean_shift(X,*,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,max_iter=300,n_jobs=None)
sklearn.cluster.estimate_bandwidth(X,*,quantile=0.3,n_samples=None,random_state=0,n_jobs=None)
sklearn.cluster.get_bin_seeds(X,bin_size,min_bin_freq=1)
sklearn.cluster.mean_shift(X,*,bandwidth=None,seeds=None,bin_seeding=False,min_bin_freq=1,cluster_all=True,max_iter=300,n_jobs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_feature_agglomeration.py----------------------------------------
A:sklearn.cluster._feature_agglomeration.X->self._validate_data(X, reset=False)
A:sklearn.cluster._feature_agglomeration.size->numpy.bincount(self.labels_)
A:sklearn.cluster._feature_agglomeration.nX->numpy.array([np.bincount(self.labels_, X[i, :]) / size for i in range(n_samples)])
A:sklearn.cluster._feature_agglomeration.(unil, inverse)->numpy.unique(self.labels_, return_inverse=True)
sklearn.cluster._feature_agglomeration.AgglomerationTransform(TransformerMixin)
sklearn.cluster._feature_agglomeration.AgglomerationTransform.inverse_transform(self,Xt=None,Xred=None)
sklearn.cluster._feature_agglomeration.AgglomerationTransform.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_affinity_propagation.py----------------------------------------
A:sklearn.cluster._affinity_propagation.mask->numpy.ones(S.shape, dtype=bool)
A:sklearn.cluster._affinity_propagation.A->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster._affinity_propagation.R->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster._affinity_propagation.tmp->numpy.zeros((n_samples, n_samples))
A:sklearn.cluster._affinity_propagation.e->numpy.zeros((n_samples, convergence_iter))
A:sklearn.cluster._affinity_propagation.ind->numpy.arange(n_samples)
A:sklearn.cluster._affinity_propagation.I->numpy.flatnonzero(E)
A:sklearn.cluster._affinity_propagation.Y2->numpy.max(tmp, axis=1)
A:sklearn.cluster._affinity_propagation.dA->numpy.diag(tmp).copy()
A:sklearn.cluster._affinity_propagation.K->numpy.sum(E, axis=0)
A:sklearn.cluster._affinity_propagation.se->numpy.sum(e, axis=1)
A:sklearn.cluster._affinity_propagation.c->numpy.argmax(S[:, I], axis=1)
A:sklearn.cluster._affinity_propagation.c[I]->numpy.arange(K)
A:sklearn.cluster._affinity_propagation.j->numpy.argmax(np.sum(S[ii[:, np.newaxis], ii], axis=0))
A:sklearn.cluster._affinity_propagation.cluster_centers_indices->numpy.unique(labels)
A:sklearn.cluster._affinity_propagation.labels->numpy.array([-1] * n_samples)
A:sklearn.cluster._affinity_propagation.estimator->AffinityPropagation(damping=damping, max_iter=max_iter, convergence_iter=convergence_iter, copy=copy, preference=preference, affinity='precomputed', verbose=verbose, random_state=random_state).fit(S)
A:sklearn.cluster._affinity_propagation.X->self._validate_data(X, reset=False, accept_sparse='csr')
A:sklearn.cluster._affinity_propagation.preference->numpy.array(preference, copy=False)
A:sklearn.cluster._affinity_propagation.random_state->check_random_state(self.random_state)
A:sklearn.cluster._affinity_propagation.(self.cluster_centers_indices_, self.labels_, self.n_iter_)->_affinity_propagation(self.affinity_matrix_, max_iter=self.max_iter, convergence_iter=self.convergence_iter, preference=preference, damping=self.damping, verbose=self.verbose, return_n_iter=True, random_state=random_state)
A:sklearn.cluster._affinity_propagation.self.cluster_centers_->X[self.cluster_centers_indices_].copy()
sklearn.cluster.AffinityPropagation(self,*,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False,random_state=None)
sklearn.cluster.AffinityPropagation._more_tags(self)
sklearn.cluster.AffinityPropagation.fit(self,X,y=None)
sklearn.cluster.AffinityPropagation.fit_predict(self,X,y=None)
sklearn.cluster.AffinityPropagation.predict(self,X)
sklearn.cluster._affinity_propagation.AffinityPropagation(self,*,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False,random_state=None)
sklearn.cluster._affinity_propagation.AffinityPropagation.__init__(self,*,damping=0.5,max_iter=200,convergence_iter=15,copy=True,preference=None,affinity='euclidean',verbose=False,random_state=None)
sklearn.cluster._affinity_propagation.AffinityPropagation._more_tags(self)
sklearn.cluster._affinity_propagation.AffinityPropagation.fit(self,X,y=None)
sklearn.cluster._affinity_propagation.AffinityPropagation.fit_predict(self,X,y=None)
sklearn.cluster._affinity_propagation.AffinityPropagation.predict(self,X)
sklearn.cluster._affinity_propagation._affinity_propagation(S,*,preference,convergence_iter,max_iter,damping,verbose,return_n_iter,random_state)
sklearn.cluster._affinity_propagation._equal_similarities_and_preferences(S,preference)
sklearn.cluster._affinity_propagation.affinity_propagation(S,*,preference=None,convergence_iter=15,max_iter=200,damping=0.5,copy=True,verbose=False,return_n_iter=False,random_state=None)
sklearn.cluster.affinity_propagation(S,*,preference=None,convergence_iter=15,max_iter=200,damping=0.5,copy=True,verbose=False,return_n_iter=False,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_spectral.py----------------------------------------
A:sklearn.cluster._spectral.(_, _, piv)->qr(vectors.T, pivoting=True)
A:sklearn.cluster._spectral.(ut, _, v)->svd(vectors[piv[:k], :].T)
A:sklearn.cluster._spectral.vectors->as_float_array(vectors, copy=copy)
A:sklearn.cluster._spectral.random_state->check_random_state(self.random_state)
A:sklearn.cluster._spectral.norm_ones->numpy.sqrt(n_samples)
A:sklearn.cluster._spectral.rotation->numpy.dot(Vh.T, U.T)
A:sklearn.cluster._spectral.c->numpy.zeros(n_samples)
A:sklearn.cluster._spectral.t_discrete->numpy.dot(vectors, rotation)
A:sklearn.cluster._spectral.labels->numpy.dot(vectors, rotation).argmax(axis=1)
A:sklearn.cluster._spectral.vectors_discrete->csc_matrix((np.ones(len(labels)), (np.arange(0, n_samples), labels)), shape=(n_samples, n_components))
A:sklearn.cluster._spectral.(U, S, Vh)->numpy.linalg.svd(t_svd)
A:sklearn.cluster._spectral.clusterer->SpectralClustering(n_clusters=n_clusters, n_components=n_components, eigen_solver=eigen_solver, random_state=random_state, n_init=n_init, affinity='precomputed', eigen_tol=eigen_tol, assign_labels=assign_labels, verbose=verbose).fit(affinity)
A:sklearn.cluster._spectral.X->self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'], dtype=np.float64, ensure_min_samples=2)
A:sklearn.cluster._spectral.connectivity->NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs, metric='precomputed').fit(X).kneighbors_graph(X=X, mode='connectivity')
A:sklearn.cluster._spectral.estimator->NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs, metric='precomputed').fit(X)
A:sklearn.cluster._spectral.self.affinity_matrix_->pairwise_kernels(X, metric=self.affinity, filter_params=True, **params)
A:sklearn.cluster._spectral.maps->spectral_embedding(self.affinity_matrix_, n_components=n_components, eigen_solver=self.eigen_solver, random_state=random_state, eigen_tol=self.eigen_tol, drop_first=False)
A:sklearn.cluster._spectral.(_, self.labels_, _)->k_means(maps, self.n_clusters, random_state=random_state, n_init=self.n_init, verbose=self.verbose)
A:sklearn.cluster._spectral.self.labels_->discretize(maps, random_state=random_state)
sklearn.cluster.SpectralClustering(self,n_clusters=8,*,eigen_solver=None,n_components=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol='auto',assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=None,verbose=False)
sklearn.cluster.SpectralClustering._more_tags(self)
sklearn.cluster.SpectralClustering.fit(self,X,y=None)
sklearn.cluster.SpectralClustering.fit_predict(self,X,y=None)
sklearn.cluster._spectral.SpectralClustering(self,n_clusters=8,*,eigen_solver=None,n_components=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol='auto',assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=None,verbose=False)
sklearn.cluster._spectral.SpectralClustering.__init__(self,n_clusters=8,*,eigen_solver=None,n_components=None,random_state=None,n_init=10,gamma=1.0,affinity='rbf',n_neighbors=10,eigen_tol='auto',assign_labels='kmeans',degree=3,coef0=1,kernel_params=None,n_jobs=None,verbose=False)
sklearn.cluster._spectral.SpectralClustering._more_tags(self)
sklearn.cluster._spectral.SpectralClustering.fit(self,X,y=None)
sklearn.cluster._spectral.SpectralClustering.fit_predict(self,X,y=None)
sklearn.cluster._spectral.cluster_qr(vectors)
sklearn.cluster._spectral.discretize(vectors,*,copy=True,max_svd_restarts=30,n_iter_max=20,random_state=None)
sklearn.cluster._spectral.spectral_clustering(affinity,*,n_clusters=8,n_components=None,eigen_solver=None,random_state=None,n_init=10,eigen_tol='auto',assign_labels='kmeans',verbose=False)
sklearn.cluster.spectral_clustering(affinity,*,n_clusters=8,n_components=None,eigen_solver=None,random_state=None,n_init=10,eigen_tol='auto',assign_labels='kmeans',verbose=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py----------------------------------------
A:sklearn.cluster._kmeans.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.cluster._kmeans.x_squared_norms->row_norms(X, squared=True)
A:sklearn.cluster._kmeans.random_state->check_random_state(self.random_state)
A:sklearn.cluster._kmeans.(centers, indices)->_kmeans_plusplus(X, n_clusters, x_squared_norms, sample_weight, random_state, n_local_trials)
A:sklearn.cluster._kmeans.centers->centers.toarray().toarray()
A:sklearn.cluster._kmeans.center_id->check_random_state(self.random_state).choice(n_samples, p=sample_weight / sample_weight.sum())
A:sklearn.cluster._kmeans.indices->numpy.full(n_clusters, -1, dtype=int)
A:sklearn.cluster._kmeans.centers[0]->X[[center_id]].toarray()
A:sklearn.cluster._kmeans.closest_dist_sq->_euclidean_distances(centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms, squared=True)
A:sklearn.cluster._kmeans.candidate_ids->numpy.searchsorted(stable_cumsum(sample_weight * closest_dist_sq), rand_vals)
A:sklearn.cluster._kmeans.distance_to_candidates->_euclidean_distances(X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)
A:sklearn.cluster._kmeans.best_candidate->numpy.argmin(candidates_pot)
A:sklearn.cluster._kmeans.centers[c]->X[[best_candidate]].toarray()
A:sklearn.cluster._kmeans.variances->numpy.var(X, axis=0)
A:sklearn.cluster._kmeans.est->KMeans(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter, verbose=verbose, tol=tol, random_state=random_state, copy_x=copy_x, algorithm=algorithm).fit(X, sample_weight=sample_weight)
A:sklearn.cluster._kmeans.centers_new->numpy.empty_like(centers)
A:sklearn.cluster._kmeans.weight_in_clusters->numpy.zeros(n_clusters, dtype=X.dtype)
A:sklearn.cluster._kmeans.labels->_labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, n_threads=self._n_threads, return_inertia=False)
A:sklearn.cluster._kmeans.labels_old->_labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, n_threads=self._n_threads, return_inertia=False).copy()
A:sklearn.cluster._kmeans.upper_bounds->numpy.zeros(n_samples, dtype=X.dtype)
A:sklearn.cluster._kmeans.lower_bounds->numpy.zeros((n_samples, n_clusters), dtype=X.dtype)
A:sklearn.cluster._kmeans.center_shift->numpy.zeros(n_clusters, dtype=centers.dtype)
A:sklearn.cluster._kmeans.inertia->_inertia(X, sample_weight, centers, labels, n_threads)
A:sklearn.cluster._kmeans.center_shift_tot->(center_shift ** 2).sum()
A:sklearn.cluster._kmeans.result->_labels_inertia(X, sample_weight, centers, n_threads, return_inertia)
A:sklearn.cluster._kmeans.self._tol->_tolerance(X, self.tol)
A:sklearn.cluster._kmeans.n_active_threads->int(np.ceil(n_samples / CHUNK_SIZE))
A:sklearn.cluster._kmeans.modules->threadpool_info()
A:sklearn.cluster._kmeans.X->self._validate_data(X, accept_sparse='csr', dtype=[np.float64, np.float32], order='C', accept_large_sparse=False, reset=not has_centers)
A:sklearn.cluster._kmeans.init_indices->check_random_state(self.random_state).randint(0, n_samples, init_size)
A:sklearn.cluster._kmeans.(centers, _)->_kmeans_plusplus(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms, sample_weight=sample_weight)
A:sklearn.cluster._kmeans.seeds->check_random_state(self.random_state).choice(n_samples, size=n_clusters, replace=False, p=sample_weight / sample_weight.sum())
A:sklearn.cluster._kmeans.(_, scores)->_labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, self._n_threads)
A:sklearn.cluster._kmeans.self._n_threads->_openmp_effective_n_threads()
A:sklearn.cluster._kmeans.init_is_array_like->_is_arraylike_not_scalar(init)
A:sklearn.cluster._kmeans.init->check_array(init, dtype=X.dtype, copy=True, order='C')
A:sklearn.cluster._kmeans.X_mean->self._validate_data(X, accept_sparse='csr', dtype=[np.float64, np.float32], order='C', accept_large_sparse=False, reset=not has_centers).mean(axis=0)
A:sklearn.cluster._kmeans.centers_init->self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=random_state, sample_weight=sample_weight)
A:sklearn.cluster._kmeans.(labels, inertia, centers, n_iter_)->kmeans_single(X, sample_weight, centers_init, max_iter=self.max_iter, verbose=self.verbose, tol=self._tol, n_threads=self._n_threads)
A:sklearn.cluster._kmeans.distinct_clusters->len(set(best_labels))
A:sklearn.cluster._kmeans.(labels, inertia)->_labels_inertia(X, sample_weight, centers, n_threads=n_threads)
A:sklearn.cluster._kmeans.n_reassigns->to_reassign.sum()
A:sklearn.cluster._kmeans.new_centers->check_random_state(self.random_state).choice(X.shape[0], replace=False, size=n_reassigns)
A:sklearn.cluster._kmeans.weight_sums[to_reassign]->numpy.min(weight_sums[~to_reassign])
A:sklearn.cluster._kmeans.self._batch_size->min(self.batch_size, X.shape[0])
A:sklearn.cluster._kmeans.self._init_size->min(self._init_size, X.shape[0])
A:sklearn.cluster._kmeans.alpha->min(alpha, 1)
A:sklearn.cluster._kmeans.validation_indices->check_random_state(self.random_state).randint(0, n_samples, self._init_size)
A:sklearn.cluster._kmeans.cluster_centers->self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=random_state, init_size=self._init_size, sample_weight=sample_weight)
A:sklearn.cluster._kmeans.(_, inertia)->_labels_inertia_threadpool_limit(X_valid, sample_weight_valid, cluster_centers, n_threads=self._n_threads)
A:sklearn.cluster._kmeans.self._counts->numpy.zeros(self.n_clusters, dtype=X.dtype)
A:sklearn.cluster._kmeans.minibatch_indices->check_random_state(self.random_state).randint(0, n_samples, self._batch_size)
A:sklearn.cluster._kmeans.batch_inertia->_mini_batch_step(X=X[minibatch_indices], sample_weight=sample_weight[minibatch_indices], centers=centers, centers_new=centers_new, weight_sums=self._counts, random_state=random_state, random_reassign=self._random_reassign(), reassignment_ratio=self.reassignment_ratio, verbose=self.verbose, n_threads=self._n_threads)
A:sklearn.cluster._kmeans.centers_squared_diff->numpy.sum((centers_new - centers) ** 2)
A:sklearn.cluster._kmeans.self.n_iter_->int(np.ceil((i + 1) * self._batch_size / n_samples))
A:sklearn.cluster._kmeans.(self.labels_, self.inertia_)->_labels_inertia_threadpool_limit(X, sample_weight, self.cluster_centers_, n_threads=self._n_threads)
A:sklearn.cluster._kmeans.has_centers->hasattr(self, 'cluster_centers_')
A:sklearn.cluster._kmeans.self._random_state->getattr(self, '_random_state', check_random_state(self.random_state))
A:sklearn.cluster._kmeans.self.n_steps_->getattr(self, 'n_steps_', 0)
A:sklearn.cluster._kmeans.self.cluster_centers_->self._init_centroids(X, x_squared_norms=x_squared_norms, init=init, random_state=self._random_state, init_size=self._init_size, sample_weight=sample_weight)
sklearn.cluster.KMeans(self,n_clusters=8,*,init='k-means++',n_init='auto',max_iter=300,tol=0.0001,verbose=0,random_state=None,copy_x=True,algorithm='lloyd')
sklearn.cluster.KMeans._check_params_vs_input(self,X)
sklearn.cluster.KMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster.KMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.MiniBatchKMeans(self,n_clusters=8,*,init='k-means++',max_iter=100,batch_size=1024,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init='auto',reassignment_ratio=0.01)
sklearn.cluster.MiniBatchKMeans._check_params_vs_input(self,X)
sklearn.cluster.MiniBatchKMeans._mini_batch_convergence(self,step,n_steps,n_samples,centers_squared_diff,batch_inertia)
sklearn.cluster.MiniBatchKMeans._random_reassign(self)
sklearn.cluster.MiniBatchKMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster.MiniBatchKMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster.MiniBatchKMeans.partial_fit(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans.KMeans(self,n_clusters=8,*,init='k-means++',n_init='auto',max_iter=300,tol=0.0001,verbose=0,random_state=None,copy_x=True,algorithm='lloyd')
sklearn.cluster._kmeans.KMeans.__init__(self,n_clusters=8,*,init='k-means++',n_init='auto',max_iter=300,tol=0.0001,verbose=0,random_state=None,copy_x=True,algorithm='lloyd')
sklearn.cluster._kmeans.KMeans._check_params_vs_input(self,X)
sklearn.cluster._kmeans.KMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster._kmeans.KMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans.MiniBatchKMeans(self,n_clusters=8,*,init='k-means++',max_iter=100,batch_size=1024,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init='auto',reassignment_ratio=0.01)
sklearn.cluster._kmeans.MiniBatchKMeans.__init__(self,n_clusters=8,*,init='k-means++',max_iter=100,batch_size=1024,verbose=0,compute_labels=True,random_state=None,tol=0.0,max_no_improvement=10,init_size=None,n_init='auto',reassignment_ratio=0.01)
sklearn.cluster._kmeans.MiniBatchKMeans._check_params_vs_input(self,X)
sklearn.cluster._kmeans.MiniBatchKMeans._mini_batch_convergence(self,step,n_steps,n_samples,centers_squared_diff,batch_inertia)
sklearn.cluster._kmeans.MiniBatchKMeans._random_reassign(self)
sklearn.cluster._kmeans.MiniBatchKMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster._kmeans.MiniBatchKMeans.fit(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans.MiniBatchKMeans.partial_fit(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans._BaseKMeans(self,n_clusters,*,init,n_init,max_iter,tol,verbose,random_state)
sklearn.cluster._kmeans._BaseKMeans.__init__(self,n_clusters,*,init,n_init,max_iter,tol,verbose,random_state)
sklearn.cluster._kmeans._BaseKMeans._check_mkl_vcomp(self,X,n_samples)
sklearn.cluster._kmeans._BaseKMeans._check_params_vs_input(self,X,default_n_init=None)
sklearn.cluster._kmeans._BaseKMeans._check_test_data(self,X)
sklearn.cluster._kmeans._BaseKMeans._init_centroids(self,X,x_squared_norms,init,random_state,sample_weight,init_size=None,n_centroids=None)
sklearn.cluster._kmeans._BaseKMeans._more_tags(self)
sklearn.cluster._kmeans._BaseKMeans._transform(self,X)
sklearn.cluster._kmeans._BaseKMeans._validate_center_shape(self,X,centers)
sklearn.cluster._kmeans._BaseKMeans._warn_mkl_vcomp(self,n_active_threads)
sklearn.cluster._kmeans._BaseKMeans.fit_predict(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans._BaseKMeans.fit_transform(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans._BaseKMeans.predict(self,X,sample_weight='deprecated')
sklearn.cluster._kmeans._BaseKMeans.score(self,X,y=None,sample_weight=None)
sklearn.cluster._kmeans._BaseKMeans.transform(self,X)
sklearn.cluster._kmeans._kmeans_plusplus(X,n_clusters,x_squared_norms,sample_weight,random_state,n_local_trials=None)
sklearn.cluster._kmeans._kmeans_single_elkan(X,sample_weight,centers_init,max_iter=300,verbose=False,tol=0.0001,n_threads=1)
sklearn.cluster._kmeans._kmeans_single_lloyd(X,sample_weight,centers_init,max_iter=300,verbose=False,tol=0.0001,n_threads=1)
sklearn.cluster._kmeans._labels_inertia(X,sample_weight,centers,n_threads=1,return_inertia=True)
sklearn.cluster._kmeans._labels_inertia_threadpool_limit(X,sample_weight,centers,n_threads=1,return_inertia=True)
sklearn.cluster._kmeans._mini_batch_step(X,sample_weight,centers,centers_new,weight_sums,random_state,random_reassign=False,reassignment_ratio=0.01,verbose=False,n_threads=1)
sklearn.cluster._kmeans._tolerance(X,tol)
sklearn.cluster._kmeans.k_means(X,n_clusters,*,sample_weight=None,init='k-means++',n_init='auto',max_iter=300,verbose=False,tol=0.0001,random_state=None,copy_x=True,algorithm='lloyd',return_n_iter=False)
sklearn.cluster._kmeans.kmeans_plusplus(X,n_clusters,*,sample_weight=None,x_squared_norms=None,random_state=None,n_local_trials=None)
sklearn.cluster.k_means(X,n_clusters,*,sample_weight=None,init='k-means++',n_init='auto',max_iter=300,verbose=False,tol=0.0001,random_state=None,copy_x=True,algorithm='lloyd',return_n_iter=False)
sklearn.cluster.kmeans_plusplus(X,n_clusters,*,sample_weight=None,x_squared_norms=None,random_state=None,n_local_trials=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_k_means.py----------------------------------------
A:sklearn.cluster.tests.test_k_means.centers->numpy.random.RandomState(global_random_seed).randn(5, 10).astype(dtype, copy=False)
A:sklearn.cluster.tests.test_k_means.(X, true_labels)->make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.cluster.tests.test_k_means.X->csr_container(X)
A:sklearn.cluster.tests.test_k_means.init_centers->numpy.array([[0.5, 0.5], [3, 3]])
A:sklearn.cluster.tests.test_k_means.expected_centers->numpy.array([[0.125, 0], [0.875, 1]], dtype=dtype)
A:sklearn.cluster.tests.test_k_means.kmeans->KMeans()
A:sklearn.cluster.tests.test_k_means.sample_weight->numpy.random.RandomState(global_random_seed).uniform(size=X.shape[0])
A:sklearn.cluster.tests.test_k_means.centers_old->numpy.array([-10.0, -10, -10]).reshape(-1, 1)
A:sklearn.cluster.tests.test_k_means.centers_new->numpy.empty_like(perfect_centers)
A:sklearn.cluster.tests.test_k_means.weight_in_clusters->numpy.array([10.0, 0, 0])
A:sklearn.cluster.tests.test_k_means.labels->numpy.random.RandomState(global_random_seed).randint(5, size=100, dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.rnd->numpy.random.RandomState(global_random_seed)
A:sklearn.cluster.tests.test_k_means.(X, _)->make_blobs(n_samples=100, n_features=5, centers=5, random_state=global_random_seed)
A:sklearn.cluster.tests.test_k_means.km_lloyd->KMeans(n_clusters=5, random_state=global_random_seed, n_init=1, tol=tol)
A:sklearn.cluster.tests.test_k_means.km_elkan->KMeans(algorithm='elkan', n_clusters=5, random_state=global_random_seed, n_init=1, tol=tol)
A:sklearn.cluster.tests.test_k_means.km->Estimator(n_init=1)
A:sklearn.cluster.tests.test_k_means.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.cluster.tests.test_k_means.centers_old_csr->numpy.array([-10.0, -10, -10]).reshape(-1, 1).copy()
A:sklearn.cluster.tests.test_k_means.centers_new_csr->numpy.zeros_like(centers_old_csr)
A:sklearn.cluster.tests.test_k_means.weight_sums->numpy.zeros(centers_old.shape[0], dtype=X.dtype)
A:sklearn.cluster.tests.test_k_means.weight_sums_csr->numpy.zeros(centers_old.shape[0], dtype=X.dtype)
A:sklearn.cluster.tests.test_k_means.old_inertia->_mini_batch_step(X_mb, sample_weight_mb, centers_old, centers_new, weight_sums, np.random.RandomState(global_random_seed), random_reassign=False)
A:sklearn.cluster.tests.test_k_means.(labels, new_inertia)->_labels_inertia(X_mb, sample_weight_mb, centers_new)
A:sklearn.cluster.tests.test_k_means.old_inertia_csr->_mini_batch_step(X_mb_csr, sample_weight_mb, centers_old_csr, centers_new_csr, weight_sums_csr, np.random.RandomState(global_random_seed), random_reassign=False)
A:sklearn.cluster.tests.test_k_means.(labels_csr, new_inertia_csr)->_labels_inertia(X_mb_csr, sample_weight_mb, centers_new_csr)
A:sklearn.cluster.tests.test_k_means.init->numpy.array([[-1], [10]])
A:sklearn.cluster.tests.test_k_means.X_fortran->numpy.asfortranarray(X)
A:sklearn.cluster.tests.test_k_means.centers_fortran->numpy.asfortranarray(centers)
A:sklearn.cluster.tests.test_k_means.km_c->Estimator(n_clusters=n_clusters, init=centers, n_init=1, random_state=global_random_seed).fit(X)
A:sklearn.cluster.tests.test_k_means.km_f->Estimator(n_clusters=n_clusters, init=centers_fortran, n_init=1, random_state=global_random_seed).fit(X_fortran)
A:sklearn.cluster.tests.test_k_means.sys.stdout->StringIO()
A:sklearn.cluster.tests.test_k_means.captured->capsys.readouterr()
A:sklearn.cluster.tests.test_k_means.(zeroed_X, true_labels)->make_blobs(n_samples=100, centers=5, random_state=global_random_seed)
A:sklearn.cluster.tests.test_k_means.perfect_centers->numpy.empty((n_clusters, n_features))
A:sklearn.cluster.tests.test_k_means.perfect_centers[i]->X[true_labels == i].mean(axis=0)
A:sklearn.cluster.tests.test_k_means.(X, _, centers)->make_blobs(centers=3, random_state=0, return_centers=True)
A:sklearn.cluster.tests.test_k_means.my_X->csr_container(X).copy()
A:sklearn.cluster.tests.test_k_means.km1->KMeans(n_clusters=n_clusters).fit(input_data)
A:sklearn.cluster.tests.test_k_means.s1->KMeans(n_clusters=n_clusters).fit(input_data).fit(X).score(X)
A:sklearn.cluster.tests.test_k_means.km2->KMeans(n_clusters=n_clusters, init=km1.cluster_centers_, n_init=1).fit(input_data)
A:sklearn.cluster.tests.test_k_means.s2->KMeans(n_clusters=n_clusters, init=km1.cluster_centers_, n_init=1).fit(input_data).fit(X).score(X)
A:sklearn.cluster.tests.test_k_means.pred->Estimator(n_init=1).predict(km.cluster_centers_)
A:sklearn.cluster.tests.test_k_means.km_dense->Estimator(n_clusters=n_clusters, random_state=global_random_seed, n_init=1)
A:sklearn.cluster.tests.test_k_means.km_sparse->Estimator(n_clusters=n_clusters, random_state=global_random_seed, n_init=1)
A:sklearn.cluster.tests.test_k_means.X_dense->scipy.sparse.random(100, 10, density=0.5, format='csr', random_state=rng, dtype=dtype).toarray()
A:sklearn.cluster.tests.test_k_means.Xt->Estimator(n_init=1).transform(X)
A:sklearn.cluster.tests.test_k_means.X1->Estimator(random_state=global_random_seed, n_init=1).fit(X).transform(X)
A:sklearn.cluster.tests.test_k_means.X2->Estimator(random_state=global_random_seed, n_init=1).fit_transform(X)
A:sklearn.cluster.tests.test_k_means.(cluster_centers, labels, inertia)->k_means(X, n_clusters=n_clusters, sample_weight=None, random_state=global_random_seed)
A:sklearn.cluster.tests.test_k_means.Xt[dtype]->Estimator(n_init=1).transform(X)
A:sklearn.cluster.tests.test_k_means.X_new_type->csr_container(X).astype(dtype, copy=False)
A:sklearn.cluster.tests.test_k_means.centers_new_type->numpy.random.RandomState(global_random_seed).randn(5, 10).astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.cluster.tests.test_k_means.X_repeat->numpy.repeat(X, sample_weight, axis=0)
A:sklearn.cluster.tests.test_k_means.km_weighted->clone(km).fit(X, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_k_means.repeated_labels->numpy.repeat(km_weighted.labels_, sample_weight)
A:sklearn.cluster.tests.test_k_means.km_repeated->clone(km).fit(X_repeat)
A:sklearn.cluster.tests.test_k_means.km_none->clone(km).fit(input_data, sample_weight=None)
A:sklearn.cluster.tests.test_k_means.km_ones->clone(km).fit(input_data, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_k_means.km_orig->clone(km).fit(input_data, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_k_means.km_scaled->clone(km).fit(input_data, sample_weight=0.5 * sample_weight)
A:sklearn.cluster.tests.test_k_means.new_centers->numpy.array([[-1], [10]]).copy()
A:sklearn.cluster.tests.test_k_means.new_centers[label]->X[labels == label].mean(axis=0)
A:sklearn.cluster.tests.test_k_means.(py_labels, py_centers)->py_kmeans(X, init_centers)
A:sklearn.cluster.tests.test_k_means.cy_kmeans->KMeans(n_clusters=5, n_init=1, init=init_centers, algorithm=algo, max_iter=1).fit(X)
A:sklearn.cluster.tests.test_k_means.a_sparse->scipy.sparse.random(1, 100, density=0.5, format='csr', random_state=rng, dtype=dtype)
A:sklearn.cluster.tests.test_k_means.a_dense->scipy.sparse.random(1, 100, density=0.5, format='csr', random_state=rng, dtype=dtype).toarray().reshape(-1)
A:sklearn.cluster.tests.test_k_means.b->numpy.random.RandomState(global_random_seed).randn(100).astype(dtype, copy=False)
A:sklearn.cluster.tests.test_k_means.b_squared_norm->(b ** 2).sum()
A:sklearn.cluster.tests.test_k_means.expected->numpy.sum(distances * sample_weight[mask])
A:sklearn.cluster.tests.test_k_means.distance_dense_dense->_euclidean_dense_dense_wrapper(a_dense, b, squared)
A:sklearn.cluster.tests.test_k_means.distance_sparse_dense->_euclidean_sparse_dense_wrapper(a_sparse.data, a_sparse.indices, b, b_squared_norm, squared)
A:sklearn.cluster.tests.test_k_means.X_sparse->scipy.sparse.random(100, 10, density=0.5, format='csr', random_state=rng, dtype=dtype)
A:sklearn.cluster.tests.test_k_means.distances->((X_dense[mask] - centers[label]) ** 2).sum(axis=1)
A:sklearn.cluster.tests.test_k_means.inertia_dense->_inertia_dense(X_dense, sample_weight, centers, labels, n_threads=1, single_label=label)
A:sklearn.cluster.tests.test_k_means.inertia_sparse->_inertia_sparse(X_sparse, sample_weight, centers, labels, n_threads=1, single_label=label)
A:sklearn.cluster.tests.test_k_means.est->Klass(n_init='auto', init='random')
A:sklearn.cluster.tests.test_k_means.data->input_data.astype(dtype)
A:sklearn.cluster.tests.test_k_means.(centers, indices)->kmeans_plusplus(X, n_clusters, x_squared_norms=x_squared_norms)
A:sklearn.cluster.tests.test_k_means.(centers_c, _)->kmeans_plusplus(X, n_clusters, random_state=global_random_seed)
A:sklearn.cluster.tests.test_k_means.(centers_fortran, _)->kmeans_plusplus(X_fortran, n_clusters, random_state=global_random_seed)
A:sklearn.cluster.tests.test_k_means.labels1->numpy.array([1, 0, 0, 1, 2, 0, 2, 1], dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.labels2->numpy.array([0, 2, 2, 0, 1, 2, 1, 0], dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.labels3->numpy.array([1, 0, 0, 2, 2, 0, 2, 1], dtype=np.int32)
A:sklearn.cluster.tests.test_k_means.clustering->KMeans(n_clusters=2, **kwargs)
A:sklearn.cluster.tests.test_k_means.class_name->Klass.__name__.lower()
A:sklearn.cluster.tests.test_k_means.names_out->KMeans().get_feature_names_out()
A:sklearn.cluster.tests.test_k_means.y_pred1->KMeans().fit_predict(X)
A:sklearn.cluster.tests.test_k_means.kmeans.cluster_centers_->create_memmap_backed_data(kmeans.cluster_centers_)
A:sklearn.cluster.tests.test_k_means.kmeans.labels_->create_memmap_backed_data(kmeans.labels_)
A:sklearn.cluster.tests.test_k_means.y_pred2->KMeans().predict(X)
A:sklearn.cluster.tests.test_k_means.x_squared_norms->row_norms(X, squared=True)
A:sklearn.cluster.tests.test_k_means.clusters_weighted->KMeans()._init_centroids(X=X, x_squared_norms=x_squared_norms, init=init, sample_weight=sample_weight, n_centroids=10, random_state=np.random.RandomState(global_random_seed))
A:sklearn.cluster.tests.test_k_means.clusters->KMeans()._init_centroids(X=X, x_squared_norms=x_squared_norms, init=init, sample_weight=np.ones(X.shape[0]), n_centroids=5, random_state=np.random.RandomState(global_random_seed))
A:sklearn.cluster.tests.test_k_means.d->euclidean_distances(X[::2], clusters_weighted)
sklearn.cluster.tests.test_k_means._check_fitted_model(km)
sklearn.cluster.tests.test_k_means._sort_centers(centers)
sklearn.cluster.tests.test_k_means.test_all_init(Estimator,input_data,init)
sklearn.cluster.tests.test_k_means.test_centers_not_mutated(Estimator,dtype)
sklearn.cluster.tests.test_k_means.test_dense_sparse(Estimator,X_csr,global_random_seed)
sklearn.cluster.tests.test_k_means.test_euclidean_distance(dtype,squared,global_random_seed)
sklearn.cluster.tests.test_k_means.test_feature_names_out(Klass,method)
sklearn.cluster.tests.test_k_means.test_fit_transform(Estimator,global_random_seed)
sklearn.cluster.tests.test_k_means.test_float_precision(Estimator,input_data,global_random_seed)
sklearn.cluster.tests.test_k_means.test_fortran_aligned_data(Estimator,global_random_seed)
sklearn.cluster.tests.test_k_means.test_inertia(dtype,global_random_seed)
sklearn.cluster.tests.test_k_means.test_integer_input(Estimator,array_constr,dtype,init,global_random_seed)
sklearn.cluster.tests.test_k_means.test_is_same_clustering()
sklearn.cluster.tests.test_k_means.test_k_means_1_iteration(array_constr,algo,global_random_seed)
sklearn.cluster.tests.test_k_means.test_k_means_function(global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_convergence(algorithm,global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_copyx()
sklearn.cluster.tests.test_k_means.test_kmeans_elkan_iter_attribute()
sklearn.cluster.tests.test_k_means.test_kmeans_elkan_results(distribution,array_constr,tol,global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_empty_cluster_relocated(array_constr)
sklearn.cluster.tests.test_k_means.test_kmeans_init_auto_with_initial_centroids(Estimator,init,expected_n_init)
sklearn.cluster.tests.test_k_means.test_kmeans_init_fitted_centers(input_data)
sklearn.cluster.tests.test_k_means.test_kmeans_plusplus_dataorder(global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_plusplus_norms(x_squared_norms)
sklearn.cluster.tests.test_k_means.test_kmeans_plusplus_output(input_data,dtype,global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_plusplus_wrong_params(param,match)
sklearn.cluster.tests.test_k_means.test_kmeans_predict(Estimator,algorithm,array_constr,max_iter,global_dtype,global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_relocated_clusters(array_constr,algo)
sklearn.cluster.tests.test_k_means.test_kmeans_results(array_constr,algo,dtype)
sklearn.cluster.tests.test_k_means.test_kmeans_verbose(algorithm,tol,capsys)
sklearn.cluster.tests.test_k_means.test_kmeans_warns_less_centers_than_unique_points(global_random_seed)
sklearn.cluster.tests.test_k_means.test_kmeans_with_array_like_or_np_scalar_init(kwargs)
sklearn.cluster.tests.test_k_means.test_minibatch_declared_convergence(capsys,tol,max_no_improvement)
sklearn.cluster.tests.test_k_means.test_minibatch_iter_steps()
sklearn.cluster.tests.test_k_means.test_minibatch_kmeans_init_size()
sklearn.cluster.tests.test_k_means.test_minibatch_kmeans_partial_fit_init(init)
sklearn.cluster.tests.test_k_means.test_minibatch_kmeans_verbose()
sklearn.cluster.tests.test_k_means.test_minibatch_kmeans_warning_init_size()
sklearn.cluster.tests.test_k_means.test_minibatch_reassign(input_data,global_random_seed)
sklearn.cluster.tests.test_k_means.test_minibatch_sensible_reassign(global_random_seed)
sklearn.cluster.tests.test_k_means.test_minibatch_update_consistency(X_csr,global_random_seed)
sklearn.cluster.tests.test_k_means.test_minibatch_with_many_reassignments()
sklearn.cluster.tests.test_k_means.test_n_init(global_random_seed)
sklearn.cluster.tests.test_k_means.test_n_init_auto(Klass,default_n_init)
sklearn.cluster.tests.test_k_means.test_predict_dense_sparse(Estimator,init,X_csr)
sklearn.cluster.tests.test_k_means.test_predict_does_not_change_cluster_centers(csr_container)
sklearn.cluster.tests.test_k_means.test_predict_sample_weight_deprecation_warning(Estimator)
sklearn.cluster.tests.test_k_means.test_relocate_empty_clusters(array_constr)
sklearn.cluster.tests.test_k_means.test_result_equal_in_diff_n_threads(Estimator,global_random_seed)
sklearn.cluster.tests.test_k_means.test_sample_weight_init(init,global_random_seed)
sklearn.cluster.tests.test_k_means.test_sample_weight_unchanged(Estimator)
sklearn.cluster.tests.test_k_means.test_sample_weight_zero(init,global_random_seed)
sklearn.cluster.tests.test_k_means.test_scaled_weights(Estimator,input_data,global_random_seed)
sklearn.cluster.tests.test_k_means.test_score_max_iter(Estimator,global_random_seed)
sklearn.cluster.tests.test_k_means.test_transform(Estimator,global_random_seed)
sklearn.cluster.tests.test_k_means.test_unit_weights_vs_no_weights(Estimator,input_data,global_random_seed)
sklearn.cluster.tests.test_k_means.test_warning_elkan_1_cluster()
sklearn.cluster.tests.test_k_means.test_warning_n_init_precomputed_centers(Estimator)
sklearn.cluster.tests.test_k_means.test_weighted_vs_repeated(global_random_seed)
sklearn.cluster.tests.test_k_means.test_wrong_params(Estimator,param,match)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_optics.py----------------------------------------
A:sklearn.cluster.tests.test_optics.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_optics.X->numpy.vstack((C1, C2, C3, C4)).astype(global_dtype, copy=False)
A:sklearn.cluster.tests.test_optics.r_plot->numpy.array(r_plot)
A:sklearn.cluster.tests.test_optics.e->_extend_region(steep_upward, downward, 0, 2)
A:sklearn.cluster.tests.test_optics.labels->_extract_xi_labels(ordering, clusters)
A:sklearn.cluster.tests.test_optics.(X, expected_labels)->shuffle(X, expected_labels, random_state=rng)
A:sklearn.cluster.tests.test_optics.clust->OPTICS(cluster_method='dbscan', eps=0.5).fit(X)
A:sklearn.cluster.tests.test_optics.diff->numpy.sum(clusters - np.array([[0, 99], [0, 199]]))
A:sklearn.cluster.tests.test_optics.(X, labels_true)->make_blobs(n_samples=150, centers=centers, cluster_std=0.4, random_state=0)
A:sklearn.cluster.tests.test_optics.X_bool->numpy.random.randint(2, size=(5, 2), dtype=bool)
A:sklearn.cluster.tests.test_optics.X_num->numpy.random.randint(2, size=(5, 2), dtype=np.int32)
A:sklearn.cluster.tests.test_optics.op->OPTICS(min_samples=min_samples, cluster_method='dbscan', eps=eps, metric=metric).fit(X)
A:sklearn.cluster.tests.test_optics.db->DBSCAN(eps=eps, min_samples=min_samples).fit(X)
A:sklearn.cluster.tests.test_optics.contingency->contingency_matrix(db.labels_, op.labels_)
A:sklearn.cluster.tests.test_optics.agree->min(np.sum(np.max(contingency, axis=0)), np.sum(np.max(contingency, axis=1)))
A:sklearn.cluster.tests.test_optics.percent_mismatch->numpy.round((disagree - 1) / X.shape[0], 2)
A:sklearn.cluster.tests.test_optics.redX->X[::2].astype(global_dtype, copy=False)
A:sklearn.cluster.tests.test_optics.cluster_sizes->numpy.bincount(clust.labels_[clust.labels_ != -1])
A:sklearn.cluster.tests.test_optics.clust_frac->OPTICS(min_samples=9, min_cluster_size=min_cluster_size / redX.shape[0])
A:sklearn.cluster.tests.test_optics.clust1->OPTICS(min_samples=10, algorithm='brute', metric='precomputed').fit(dists)
A:sklearn.cluster.tests.test_optics.clust2->OPTICS(min_samples=10, algorithm='brute', metric='euclidean').fit(redX)
A:sklearn.cluster.tests.test_optics.dists->pairwise_distances(redX, metric='euclidean')
A:sklearn.cluster.tests.test_optics.X_1->numpy.array([1, 2, 3, 1, 8, 8, 7, 100]).reshape(-1, 1)
A:sklearn.cluster.tests.test_optics.optics_1->OPTICS(min_samples=3, metric='euclidean').fit(X_1)
A:sklearn.cluster.tests.test_optics.optics_2->OPTICS(min_samples=3, metric='euclidean').fit(X_2)
sklearn.cluster.tests.test_optics.test_bad_extract()
sklearn.cluster.tests.test_optics.test_bad_reachability()
sklearn.cluster.tests.test_optics.test_close_extract()
sklearn.cluster.tests.test_optics.test_cluster_hierarchy_(global_dtype)
sklearn.cluster.tests.test_optics.test_compare_to_ELKI()
sklearn.cluster.tests.test_optics.test_correct_number_of_clusters(metric,csr_container)
sklearn.cluster.tests.test_optics.test_dbscan_optics_parity(eps,min_samples,metric,global_dtype,csr_container)
sklearn.cluster.tests.test_optics.test_extend_downward(r_plot,end)
sklearn.cluster.tests.test_optics.test_extend_upward(r_plot,end)
sklearn.cluster.tests.test_optics.test_extract_dbscan(global_dtype)
sklearn.cluster.tests.test_optics.test_extract_xi(global_dtype)
sklearn.cluster.tests.test_optics.test_min_cluster_size(min_cluster_size,global_dtype)
sklearn.cluster.tests.test_optics.test_min_cluster_size_invalid2(csr_container)
sklearn.cluster.tests.test_optics.test_min_samples_edge_case(global_dtype)
sklearn.cluster.tests.test_optics.test_minimum_number_of_sample_check()
sklearn.cluster.tests.test_optics.test_nowarn_if_metric_bool_data_bool()
sklearn.cluster.tests.test_optics.test_nowarn_if_metric_no_bool()
sklearn.cluster.tests.test_optics.test_optics_predecessor_correction_ordering()
sklearn.cluster.tests.test_optics.test_precomputed_dists(global_dtype,csr_container)
sklearn.cluster.tests.test_optics.test_processing_order()
sklearn.cluster.tests.test_optics.test_the_extract_xi_labels(ordering,clusters,expected)
sklearn.cluster.tests.test_optics.test_warn_if_metric_bool_data_no_bool()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_birch.py----------------------------------------
A:sklearn.cluster.tests.test_birch.(X, y)->make_blobs(n_samples=80, centers=4, random_state=global_random_seed)
A:sklearn.cluster.tests.test_birch.X->numpy.array([[-2.6192791, -1.5053215], [-2.9993038, -1.6863596], [-2.3724914, -1.3438171], [-2.336792, -1.3417323], [-2.4089134, -1.3290224], [-2.3724914, -1.3438171], [-3.364009, -1.8846745], [-2.3724914, -1.3438171], [-2.617677, -1.5003285], [-2.2960556, -1.3260119], [-2.3724914, -1.3438171], [-2.5459878, -1.4533926], [-2.25979, -1.3003055], [-2.4089134, -1.3290224], [-2.3724914, -1.3438171], [-2.4089134, -1.3290224], [-2.5459878, -1.4533926], [-2.3724914, -1.3438171], [-2.9720619, -1.7058647], [-2.336792, -1.3417323], [-2.3724914, -1.3438171]], dtype=np.float32)
A:sklearn.cluster.tests.test_birch.brc->Birch(n_clusters=4)
A:sklearn.cluster.tests.test_birch.n_samples_root->sum([sc.n_samples_ for sc in brc.root_.subclusters_])
A:sklearn.cluster.tests.test_birch.n_samples_leaves->sum([sc.n_samples_ for leaf in brc._get_leaves() for sc in leaf.subclusters_])
A:sklearn.cluster.tests.test_birch.brc_partial->Birch(n_clusters=None)
A:sklearn.cluster.tests.test_birch.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.cluster.tests.test_birch.shuffle_indices->numpy.arange(30)
A:sklearn.cluster.tests.test_birch.brc1->Birch(n_clusters=10)
A:sklearn.cluster.tests.test_birch.gc->AgglomerativeClustering(n_clusters=10)
A:sklearn.cluster.tests.test_birch.brc2->Birch(n_clusters=gc)
A:sklearn.cluster.tests.test_birch.brc4->Birch(threshold=10000.0)
A:sklearn.cluster.tests.test_birch.csr->csr_container(X)
A:sklearn.cluster.tests.test_birch.brc_sparse->Birch(n_clusters=10)
A:sklearn.cluster.tests.test_birch.(X, _)->make_blobs(n_samples=80, n_features=4, random_state=global_random_seed)
A:sklearn.cluster.tests.test_birch.n_clusters->numpy.int64(5)
A:sklearn.cluster.tests.test_birch.names_out->Birch(n_clusters=4).get_feature_names_out()
A:sklearn.cluster.tests.test_birch.Y_64->Birch(n_clusters=4).fit_transform(X)
A:sklearn.cluster.tests.test_birch.Y_32->Birch(n_clusters=4).fit_transform(X.astype(np.float32))
sklearn.cluster.tests.test_birch.check_branching_factor(node,branching_factor)
sklearn.cluster.tests.test_birch.check_threshold(birch_instance,threshold)
sklearn.cluster.tests.test_birch.test_birch_n_clusters_long_int()
sklearn.cluster.tests.test_birch.test_birch_predict(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_both_subclusters_updated()
sklearn.cluster.tests.test_birch.test_branching_factor(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_feature_names_out()
sklearn.cluster.tests.test_birch.test_n_clusters(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_n_samples_leaves_roots(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_partial_fit(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_partial_fit_second_call_error_checks()
sklearn.cluster.tests.test_birch.test_sparse_X(global_random_seed,global_dtype,csr_container)
sklearn.cluster.tests.test_birch.test_subcluster_dtype(global_dtype)
sklearn.cluster.tests.test_birch.test_threshold(global_random_seed,global_dtype)
sklearn.cluster.tests.test_birch.test_transform_match_across_dtypes(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_dbscan.py----------------------------------------
A:sklearn.cluster.tests.test_dbscan.X->numpy.zeros((10, 10))
A:sklearn.cluster.tests.test_dbscan.D->pairwise_distances(X)
A:sklearn.cluster.tests.test_dbscan.(core_samples, labels)->dbscan(X, algorithm=algorithm, eps=1, min_samples=4)
A:sklearn.cluster.tests.test_dbscan.db->DBSCAN(leaf_size=20, eps=eps, min_samples=min_samples, algorithm='ball_tree')
A:sklearn.cluster.tests.test_dbscan.(core_sparse, labels_sparse)->dbscan(D_sparse, eps=0.8, min_samples=10, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.(core_dense, labels_dense)->dbscan(D, eps=0.8, min_samples=10, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.nn->NearestNeighbors(radius=higher_eps).fit(X)
A:sklearn.cluster.tests.test_dbscan.D_sparse->NearestNeighbors(radius=higher_eps).fit(X).radius_neighbors_graph(X, mode='distance')
A:sklearn.cluster.tests.test_dbscan.dbscan_lower->dbscan(D_sparse, eps=lower_eps, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.dbscan_higher->dbscan(D_sparse, eps=lower_eps, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.X_copy->numpy.zeros((10, 10)).copy()
A:sklearn.cluster.tests.test_dbscan.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.cluster.tests.test_dbscan.obj->DBSCAN()
A:sklearn.cluster.tests.test_dbscan.s->pickle.dumps(obj)
A:sklearn.cluster.tests.test_dbscan.(core, _)->dbscan([[0], [1], [1]], eps=0.99, min_samples=2)
A:sklearn.cluster.tests.test_dbscan.sample_weight->numpy.random.RandomState(global_random_seed).randint(0, 5, X.shape[0])
A:sklearn.cluster.tests.test_dbscan.(core1, label1)->dbscan(X, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_dbscan.X_repeated->numpy.repeat(X, sample_weight, axis=0)
A:sklearn.cluster.tests.test_dbscan.(core_repeated, label_repeated)->dbscan(X_repeated)
A:sklearn.cluster.tests.test_dbscan.core_repeated_mask->numpy.zeros(X_repeated.shape[0], dtype=bool)
A:sklearn.cluster.tests.test_dbscan.core_mask->numpy.zeros(X.shape[0], dtype=bool)
A:sklearn.cluster.tests.test_dbscan.(core3, label3)->dbscan(D, sample_weight=sample_weight, metric='precomputed')
A:sklearn.cluster.tests.test_dbscan.est->DBSCAN()
A:sklearn.cluster.tests.test_dbscan.label5->DBSCAN().fit_predict(X, sample_weight=sample_weight)
A:sklearn.cluster.tests.test_dbscan.n_samples->len(X)
A:sklearn.cluster.tests.test_dbscan.ar->numpy.array([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0], [0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.3], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1], [0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0]])
A:sklearn.cluster.tests.test_dbscan.matrix->csr_container(ar)
sklearn.cluster.tests.test_dbscan.test_boundaries()
sklearn.cluster.tests.test_dbscan.test_dbscan_balltree()
sklearn.cluster.tests.test_dbscan.test_dbscan_callable()
sklearn.cluster.tests.test_dbscan.test_dbscan_core_samples_toy(algorithm)
sklearn.cluster.tests.test_dbscan.test_dbscan_feature()
sklearn.cluster.tests.test_dbscan.test_dbscan_input_not_modified(metric,csr_container)
sklearn.cluster.tests.test_dbscan.test_dbscan_input_not_modified_precomputed_sparse_nodiag(csr_container)
sklearn.cluster.tests.test_dbscan.test_dbscan_metric_params()
sklearn.cluster.tests.test_dbscan.test_dbscan_no_core_samples(csr_container)
sklearn.cluster.tests.test_dbscan.test_dbscan_precomputed_metric_with_degenerate_input_arrays()
sklearn.cluster.tests.test_dbscan.test_dbscan_precomputed_metric_with_initial_rows_zero(csr_container)
sklearn.cluster.tests.test_dbscan.test_dbscan_similarity()
sklearn.cluster.tests.test_dbscan.test_dbscan_sparse(lil_container)
sklearn.cluster.tests.test_dbscan.test_dbscan_sparse_precomputed(include_self)
sklearn.cluster.tests.test_dbscan.test_dbscan_sparse_precomputed_different_eps()
sklearn.cluster.tests.test_dbscan.test_input_validation()
sklearn.cluster.tests.test_dbscan.test_pickle()
sklearn.cluster.tests.test_dbscan.test_weighted_dbscan(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_affinity_propagation.py----------------------------------------
A:sklearn.cluster.tests.test_affinity_propagation.(X, _)->make_blobs(n_samples=60, n_features=2, centers=centers, cluster_std=0.4, shuffle=True, random_state=0)
A:sklearn.cluster.tests.test_affinity_propagation.(cluster_centers_indices, labels)->affinity_propagation(S, preference=preference, random_state=global_random_seed)
A:sklearn.cluster.tests.test_affinity_propagation.n_clusters_->len(cluster_centers_indices)
A:sklearn.cluster.tests.test_affinity_propagation.af->AffinityPropagation(affinity='euclidean', random_state=42)
A:sklearn.cluster.tests.test_affinity_propagation.S_original->numpy.dot(X, X.T).copy()
A:sklearn.cluster.tests.test_affinity_propagation.S->numpy.dot(X, X.T)
A:sklearn.cluster.tests.test_affinity_propagation.(_, labels_no_copy)->affinity_propagation(S, preference=preference, copy=False, random_state=74)
A:sklearn.cluster.tests.test_affinity_propagation.X_->csr_container(rng.randint(0, 2, size=(5, 5))).astype(global_dtype, copy=False)
A:sklearn.cluster.tests.test_affinity_propagation.labels->AffinityPropagation(affinity='euclidean', random_state=42).fit_predict(X)
A:sklearn.cluster.tests.test_affinity_propagation.labels2->AffinityPropagation(affinity='euclidean', random_state=42).predict(X_)
A:sklearn.cluster.tests.test_affinity_propagation.X->csr_container(rng.randint(0, 2, size=(5, 5)))
A:sklearn.cluster.tests.test_affinity_propagation.(cluster_center_indices, labels)->affinity_propagation(S, preference=[-20, -10], random_state=37)
A:sklearn.cluster.tests.test_affinity_propagation.to_predict->numpy.array([[2, 2], [3, 3], [4, 4]])
A:sklearn.cluster.tests.test_affinity_propagation.y->(4 * rng.rand(40)).astype(int)
A:sklearn.cluster.tests.test_affinity_propagation.(X, labels_true)->make_blobs(n_samples=300, centers=centers, cluster_std=0.5, random_state=0)
A:sklearn.cluster.tests.test_affinity_propagation.ap->AffinityPropagation(random_state=46)
A:sklearn.cluster.tests.test_affinity_propagation.centers->container(np.zeros((1, 10)))
A:sklearn.cluster.tests.test_affinity_propagation.rng->numpy.random.RandomState(42)
A:sklearn.cluster.tests.test_affinity_propagation.afp->AffinityPropagation(preference=1, affinity='precomputed', random_state=0).fit(X)
A:sklearn.cluster.tests.test_affinity_propagation.expected->numpy.array([0, 1, 1, 2])
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation(global_random_seed,global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_affinity_shape()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_convergence_warning_dense_sparse(container,global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_equal_mutual_similarities(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_fit_non_convergence(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_no_copy()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_non_convergence_regressiontest(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_precomputed()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_precomputed_with_sparse_input(csr_container)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_predict(global_random_seed,global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_predict_error()
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_predict_non_convergence(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_affinity_propagation_random_state()
sklearn.cluster.tests.test_affinity_propagation.test_correct_clusters(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_equal_similarities_and_preferences(global_dtype)
sklearn.cluster.tests.test_affinity_propagation.test_sparse_input_for_fit_predict(csr_container)
sklearn.cluster.tests.test_affinity_propagation.test_sparse_input_for_predict(csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_spectral.py----------------------------------------
A:sklearn.cluster.tests.test_spectral.(X, _)->make_blobs(n_samples=60, n_features=2, centers=centers, cluster_std=0.4, shuffle=True, random_state=0)
A:sklearn.cluster.tests.test_spectral.S->coo_container(S)
A:sklearn.cluster.tests.test_spectral.model->SpectralClustering(random_state=0, n_clusters=2, affinity='precomputed', eigen_solver=eigen_solver, assign_labels=assign_labels).fit(mat)
A:sklearn.cluster.tests.test_spectral.model_copy->pickle.loads(pickle.dumps(model))
A:sklearn.cluster.tests.test_spectral.(X, y)->make_blobs(n_samples=20, random_state=0, centers=[[1, 1], [-1, -1]], cluster_std=0.01)
A:sklearn.cluster.tests.test_spectral.nn->NearestNeighbors(n_neighbors=n_neighbors + additional_neighbors).fit(X)
A:sklearn.cluster.tests.test_spectral.graph->img_to_graph(img, mask=mask)
A:sklearn.cluster.tests.test_spectral.sp->SpectralClustering(n_clusters=2, random_state=0)
A:sklearn.cluster.tests.test_spectral.kernels_available->kernel_metrics()
A:sklearn.cluster.tests.test_spectral.random_state->numpy.random.RandomState(seed=8)
A:sklearn.cluster.tests.test_spectral.data->numpy.random.RandomState(seed=8).randn(n_samples, n_components)
A:sklearn.cluster.tests.test_spectral.labels_float64->cluster_qr(data.astype(np.float64))
A:sklearn.cluster.tests.test_spectral.labels_float32->cluster_qr(data.astype(np.float32))
A:sklearn.cluster.tests.test_spectral.perm->numpy.random.RandomState(seed=8).permutation(n_samples)
A:sklearn.cluster.tests.test_spectral.y_true->numpy.array(y_true, float)
A:sklearn.cluster.tests.test_spectral.y_indicator->coo_container((np.ones(n_samples), (np.arange(n_samples), y_true)), shape=(n_samples, n_class + 1))
A:sklearn.cluster.tests.test_spectral.y_pred->discretize(y_true_noisy, random_state=random_state)
A:sklearn.cluster.tests.test_spectral.(x, y)->numpy.indices((40, 40))
A:sklearn.cluster.tests.test_spectral.mask->circles.copy()
A:sklearn.cluster.tests.test_spectral.img->circles.astype(float)
A:sklearn.cluster.tests.test_spectral.graph.data->numpy.exp(-graph.data / graph.data.std())
A:sklearn.cluster.tests.test_spectral.labels_arpack->spectral_clustering(graph, n_clusters=2, eigen_solver='arpack', random_state=0)
A:sklearn.cluster.tests.test_spectral.labels_amg->spectral_clustering(graph, n_clusters=2, eigen_solver='amg', random_state=0)
A:sklearn.cluster.tests.test_spectral.captured->capsys.readouterr()
A:sklearn.cluster.tests.test_spectral.X->numpy.matrix([[0.0, 2.0], [2.0, 0.0]])
A:sklearn.cluster.tests.test_spectral.vectors->numpy.ones((10, 4))
sklearn.cluster.tests.test_spectral.test_affinities()
sklearn.cluster.tests.test_spectral.test_cluster_qr()
sklearn.cluster.tests.test_spectral.test_cluster_qr_permutation_invariance()
sklearn.cluster.tests.test_spectral.test_discretize(n_samples,coo_container)
sklearn.cluster.tests.test_spectral.test_n_components()
sklearn.cluster.tests.test_spectral.test_precomputed_nearest_neighbors_filtering()
sklearn.cluster.tests.test_spectral.test_spectral_clustering(eigen_solver,assign_labels,csr_container)
sklearn.cluster.tests.test_spectral.test_spectral_clustering_not_infinite_loop(capsys,monkeypatch)
sklearn.cluster.tests.test_spectral.test_spectral_clustering_np_matrix_raises()
sklearn.cluster.tests.test_spectral.test_spectral_clustering_sparse(assign_labels,coo_container)
sklearn.cluster.tests.test_spectral.test_spectral_clustering_with_arpack_amg_solvers()
sklearn.cluster.tests.test_spectral.test_verbose(assign_labels,capsys)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/common.py----------------------------------------
A:sklearn.cluster.tests.common.prng->numpy.random.RandomState(seed)
A:sklearn.cluster.tests.common.X->numpy.empty((0, n_features))
sklearn.cluster.tests.common.generate_clustered_data(seed=0,n_clusters=3,n_features=2,n_samples_per_cluster=20,std=0.4)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_hierarchical.py----------------------------------------
A:sklearn.cluster.tests.test_hierarchical.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_hierarchical.X->numpy.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])
A:sklearn.cluster.tests.test_hierarchical.dis->cosine_distances(X)
A:sklearn.cluster.tests.test_hierarchical.res->linkage_tree(X, affinity=manhattan_distances)
A:sklearn.cluster.tests.test_hierarchical.mask->numpy.ones([10, 10], dtype=bool)
A:sklearn.cluster.tests.test_hierarchical.connectivity->grid_to_graph(*mask.shape)
A:sklearn.cluster.tests.test_hierarchical.(children, n_components, n_leaves, parent)->tree_builder(X.T, connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.(children, n_nodes, n_leaves, parent)->linkage_func(X.T, connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.clustering->AgglomerativeClustering(n_clusters=None, distance_threshold=distance_threshold, linkage='single').fit(X)
A:sklearn.cluster.tests.test_hierarchical.tempdir->mkdtemp()
A:sklearn.cluster.tests.test_hierarchical.clustering2->AgglomerativeClustering(n_clusters=10, connectivity=connectivity, metric='precomputed', linkage='complete')
A:sklearn.cluster.tests.test_hierarchical.X_dist->pairwise_distances(X)
A:sklearn.cluster.tests.test_hierarchical.Xmm->create_memmap_backed_data(X)
A:sklearn.cluster.tests.test_hierarchical.agglo->FeatureAgglomeration(n_clusters=5, connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.X_red->FeatureAgglomeration(n_clusters=5, connectivity=connectivity).transform(X)
A:sklearn.cluster.tests.test_hierarchical.X_full->FeatureAgglomeration(n_clusters=5, connectivity=connectivity).inverse_transform(X_red)
A:sklearn.cluster.tests.test_hierarchical.(moons, moon_labels)->make_moons(noise=0.05, random_state=42)
A:sklearn.cluster.tests.test_hierarchical.(circles, circle_labels)->make_circles(factor=0.5, noise=0.025, random_state=42)
A:sklearn.cluster.tests.test_hierarchical.n->len(cut)
A:sklearn.cluster.tests.test_hierarchical.ecut->numpy.zeros((n, k))
A:sklearn.cluster.tests.test_hierarchical.out->scipy.cluster.hierarchy.linkage(X, method='single')
A:sklearn.cluster.tests.test_hierarchical.children_->out[:, :2].astype(int, copy=False)
A:sklearn.cluster.tests.test_hierarchical.(children, _, n_leaves, _)->_TREE_BUILDERS['single'](X)
A:sklearn.cluster.tests.test_hierarchical.cut->_hc_cut(n_clusters, children, n_leaves)
A:sklearn.cluster.tests.test_hierarchical.cut_->_hc_cut(k, children_, n_leaves)
A:sklearn.cluster.tests.test_hierarchical.children_scipy->out[:, :2].astype(int)
A:sklearn.cluster.tests.test_hierarchical.cut_scipy->_hc_cut(n_clusters, children_scipy, n_leaves)
A:sklearn.cluster.tests.test_hierarchical.keys->numpy.unique(rng.randint(100, size=10).astype(np.intp, copy=False))
A:sklearn.cluster.tests.test_hierarchical.kwargs->dict(zip(keys, vals))
A:sklearn.cluster.tests.test_hierarchical.distance_metric->sklearn.metrics.DistanceMetric.get_metric(metric, **kwargs)
A:sklearn.cluster.tests.test_hierarchical.mst->mst_linkage_core(X, distance_metric)
A:sklearn.cluster.tests.test_hierarchical.mst_mm->mst_linkage_core(Xmm, distance_metric)
A:sklearn.cluster.tests.test_hierarchical.true_labels->numpy.array([0, 0, 1, 1, 2, 2])
A:sklearn.cluster.tests.test_hierarchical.(connectivity, n_components)->_fix_connectivity(X, connectivity, 'euclidean')
A:sklearn.cluster.tests.test_hierarchical.ward->AgglomerativeClustering(n_clusters=4, connectivity=connectivity, linkage='ward')
A:sklearn.cluster.tests.test_hierarchical.out_unstructured->ward_tree(X, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.out_structured->ward_tree(X, connectivity=connectivity, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.linkage_X_ward->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 9.10208346, 4.0], [7.0, 9.0, 24.7784379, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.linkage_X_complete->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 6.96742194, 4.0], [7.0, 9.0, 18.77445997, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.linkage_X_average->numpy.array([[3.0, 4.0, 0.36265956, 2.0], [1.0, 5.0, 1.77045373, 2.0], [0.0, 2.0, 2.55760419, 2.0], [6.0, 8.0, 6.55832839, 4.0], [7.0, 9.0, 15.44089605, 6.0]])
A:sklearn.cluster.tests.test_hierarchical.(n_samples, n_features)->numpy.shape(X)
A:sklearn.cluster.tests.test_hierarchical.connectivity_X->numpy.ones((n_samples, n_samples))
A:sklearn.cluster.tests.test_hierarchical.out_X_unstructured->linkage_tree(X, return_distance=True, linkage=linkage)
A:sklearn.cluster.tests.test_hierarchical.out_X_structured->linkage_tree(X, connectivity=connectivity_X, linkage=linkage, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.x->numpy.array([[0, 0], [1, 1]])
A:sklearn.cluster.tests.test_hierarchical.m->numpy.array([[True, False], [False, True]])
A:sklearn.cluster.tests.test_hierarchical.c->grid_to_graph(n_x=2, n_y=2, mask=m)
A:sklearn.cluster.tests.test_hierarchical.w->AgglomerativeClustering(connectivity=c, linkage='ward')
A:sklearn.cluster.tests.test_hierarchical.values->numpy.random.RandomState(0).rand(len(keys))
A:sklearn.cluster.tests.test_hierarchical.d->IntFloatDict(keys, values)
A:sklearn.cluster.tests.test_hierarchical.other->IntFloatDict(other_keys, other_values)
A:sklearn.cluster.tests.test_hierarchical.aglc1->AgglomerativeClustering(connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.aglc2->AgglomerativeClustering(connectivity=connectivity_include_self)
A:sklearn.cluster.tests.test_hierarchical.connectivity_include_self->kneighbors_graph(X, 3, include_self=True)
A:sklearn.cluster.tests.test_hierarchical.agc->AgglomerativeClustering(n_clusters=n_clusters, connectivity=connectivity)
A:sklearn.cluster.tests.test_hierarchical.fa->FakeAffinity()
A:sklearn.cluster.tests.test_hierarchical.num_clusters_produced->len(np.unique(clustering.labels_))
A:sklearn.cluster.tests.test_hierarchical.(children, n_components, n_leaves, parent, distances)->tree_builder(X, connectivity=conn, n_clusters=None, return_distance=True)
A:sklearn.cluster.tests.test_hierarchical.clusters_at_threshold->_hc_cut(n_clusters=num_clusters_produced, children=children, n_leaves=n_leaves)
A:sklearn.cluster.tests.test_hierarchical.all_distances->pairwise_distances(X, metric='minkowski', p=2)
A:sklearn.cluster.tests.test_hierarchical.D->pairwise_distances(X, metric='minkowski', p=2)
A:sklearn.cluster.tests.test_hierarchical.max_in_cluster_distance->D[in_cluster_mask][:, in_cluster_mask].min(axis=0).max()
A:sklearn.cluster.tests.test_hierarchical.min_out_cluster_distance->D[in_cluster_mask][:, ~in_cluster_mask].min(axis=0).min()
A:sklearn.cluster.tests.test_hierarchical.clusterer->AgglomerativeClustering(connectivity=connectivity_matrix, linkage='complete')
A:sklearn.cluster.tests.test_hierarchical.y_pred->AgglomerativeClustering(connectivity=connectivity_matrix, linkage='complete').fit_predict(X)
A:sklearn.cluster.tests.test_hierarchical.connectivity_matrix->numpy.array([[0, 1, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]])
A:sklearn.cluster.tests.test_hierarchical.clusterer_precomputed->AgglomerativeClustering(metric='precomputed', connectivity=connectivity_matrix, linkage='complete')
sklearn.cluster.tests.test_hierarchical.assess_same_labelling(cut1,cut2)
sklearn.cluster.tests.test_hierarchical.test_affinity_passed_to_fix_connectivity()
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering(global_random_seed,lil_container)
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering_distances(n_clusters,compute_distances,distance_threshold,linkage)
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering_memory_mapped()
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering_with_distance_threshold(linkage,global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_agglomerative_clustering_with_distance_threshold_edge_case(linkage,threshold,y_true)
sklearn.cluster.tests.test_hierarchical.test_cluster_distances_with_distance_threshold(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_compute_full_tree()
sklearn.cluster.tests.test_hierarchical.test_connectivity_callable()
sklearn.cluster.tests.test_hierarchical.test_connectivity_fixing_non_lil()
sklearn.cluster.tests.test_hierarchical.test_connectivity_ignores_diagonal()
sklearn.cluster.tests.test_hierarchical.test_connectivity_propagation()
sklearn.cluster.tests.test_hierarchical.test_deprecation_warning_metric_None(Agglomeration)
sklearn.cluster.tests.test_hierarchical.test_dist_threshold_invalid_parameters()
sklearn.cluster.tests.test_hierarchical.test_height_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_identical_points()
sklearn.cluster.tests.test_hierarchical.test_int_float_dict()
sklearn.cluster.tests.test_hierarchical.test_invalid_shape_precomputed_dist_matrix()
sklearn.cluster.tests.test_hierarchical.test_linkage_misc()
sklearn.cluster.tests.test_hierarchical.test_mst_linkage_core_memory_mapped(metric_param_grid)
sklearn.cluster.tests.test_hierarchical.test_n_components()
sklearn.cluster.tests.test_hierarchical.test_precomputed_connectivity_metric_with_2_connected_components()
sklearn.cluster.tests.test_hierarchical.test_single_linkage_clustering()
sklearn.cluster.tests.test_hierarchical.test_small_distance_threshold(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_sparse_scikit_vs_scipy(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_structured_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_unstructured_linkage_tree()
sklearn.cluster.tests.test_hierarchical.test_vector_scikit_single_vs_scipy_single(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_ward_agglomeration(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_ward_linkage_tree_return_distance(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_ward_tree_children_order(global_random_seed)
sklearn.cluster.tests.test_hierarchical.test_zero_cosine_linkage_tree()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_hdbscan.py----------------------------------------
A:sklearn.cluster.tests.test_hdbscan.(X, y)->make_blobs(n_samples, random_state=global_random_seed, centers=[[0, 0], [10, 0], [0, 10]])
A:sklearn.cluster.tests.test_hdbscan.X->numpy.random.RandomState(0).random((100, 2))
A:sklearn.cluster.tests.test_hdbscan.n_clusters->len(set(labels) - OUTLIER_SET)
A:sklearn.cluster.tests.test_hdbscan.X_outlier->numpy.random.RandomState(0).random((100, 2)).copy()
A:sklearn.cluster.tests.test_hdbscan.model->HDBSCAN().fit(X_outlier)
A:sklearn.cluster.tests.test_hdbscan.(missing_labels_idx,)->(model.labels_ == label).nonzero()
A:sklearn.cluster.tests.test_hdbscan.(missing_probs_idx,)->prob_check(model.probabilities_, prob).nonzero()
A:sklearn.cluster.tests.test_hdbscan.clean_model->HDBSCAN().fit(X_outlier[clean_idx])
A:sklearn.cluster.tests.test_hdbscan.D->sparse_constructor(D)
A:sklearn.cluster.tests.test_hdbscan.D_original->sparse_constructor(D).copy()
A:sklearn.cluster.tests.test_hdbscan.labels->_do_labelling(condensed_tree=condensed_tree, clusters={n_samples}, cluster_label_map={n_samples: 0, n_samples + 1: 1}, allow_single_cluster=True, cluster_selection_epsilon=0)
A:sklearn.cluster.tests.test_hdbscan.threshold->scipy.stats.scoreatpercentile(D.flatten(), 50)
A:sklearn.cluster.tests.test_hdbscan.metric_params->{'mahalanobis': {'V': np.eye(X.shape[1])}, 'seuclidean': {'V': np.ones(X.shape[1])}, 'minkowski': {'p': 2}, 'wminkowski': {'p': 2, 'w': np.ones(X.shape[1])}}.get(metric, None)
A:sklearn.cluster.tests.test_hdbscan.hdb->HDBSCAN(metric='precomputed')
A:sklearn.cluster.tests.test_hdbscan.clusterer->HDBSCAN().fit(X)
A:sklearn.cluster.tests.test_hdbscan.missing_labels_idx->numpy.flatnonzero(labels == missing_label)
A:sklearn.cluster.tests.test_hdbscan.infinite_labels_idx->numpy.flatnonzero(labels == infinite_label)
A:sklearn.cluster.tests.test_hdbscan.clean_idx->list(set(range(200)) - set(missing_labels_idx + infinite_labels_idx))
A:sklearn.cluster.tests.test_hdbscan.clean_labels->HDBSCAN().fit(X_outlier[clean_idx]).dbscan_clustering(cut_distance=cut_distance)
A:sklearn.cluster.tests.test_hdbscan._X_sparse->csr_container(X)
A:sklearn.cluster.tests.test_hdbscan.X_sparse->csr_container(X).copy()
A:sklearn.cluster.tests.test_hdbscan.X_dense->numpy.random.RandomState(0).random((100, 2)).copy()
A:sklearn.cluster.tests.test_hdbscan.(H, _)->make_blobs(n_samples=1000, random_state=0, centers=centers, cluster_std=0.5)
A:sklearn.cluster.tests.test_hdbscan.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_hdbscan.no_structure->numpy.random.RandomState(0).rand(150, 2)
A:sklearn.cluster.tests.test_hdbscan.(unique_labels, counts)->numpy.unique(labels, return_counts=True)
A:sklearn.cluster.tests.test_hdbscan.metrics_not_kd->list(set(BallTree.valid_metrics) - set(KDTree.valid_metrics))
A:sklearn.cluster.tests.test_hdbscan.X_nan->numpy.random.RandomState(0).random((100, 2)).copy()
A:sklearn.cluster.tests.test_hdbscan.est->HDBSCAN().fit(X)
A:sklearn.cluster.tests.test_hdbscan.condensed_tree->numpy.array([(5, 2, MAX_LAMBDA, 1), (5, 1, 0.1, 1), (5, 0, MAX_LAMBDA, 1), (5, 3, 0.2, 1), (5, 4, 0.3, 1)], dtype=CONDENSED_dtype)
A:sklearn.cluster.tests.test_hdbscan.aligned_target->numpy.vectorize(y_to_labels.get)(y)
A:sklearn.cluster.tests.test_hdbscan.X_dist->euclidean_distances(X)
sklearn.cluster.tests.test_hdbscan.check_label_quality(labels,threshold=0.99)
sklearn.cluster.tests.test_hdbscan.test_dbscan_clustering()
sklearn.cluster.tests.test_hdbscan.test_dbscan_clustering_outlier_data(cut_distance)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_algorithms(algo,metric)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_allow_single_cluster_with_epsilon()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_best_balltree_metric()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_better_than_dbscan()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_callable_metric()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_centers(algorithm)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_distance_matrix()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_error_precomputed_and_store_centers(store_centers)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_feature_array()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_min_cluster_size()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_no_clusters()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_precomputed_dense_nan()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_precomputed_non_brute(tree)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_sparse(csr_container)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_sparse_distance_matrix(sparse_constructor)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_sparse_distances_disconnected_graph(csr_container)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_sparse_distances_too_few_nonzero(csr_container)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_too_many_min_samples()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_tree_invalid_metric()
sklearn.cluster.tests.test_hdbscan.test_hdbscan_usable_inputs(X,kwargs)
sklearn.cluster.tests.test_hdbscan.test_hdbscan_warning_on_deprecated_algorithm_name()
sklearn.cluster.tests.test_hdbscan.test_labelling_distinct(global_random_seed,allow_single_cluster,epsilon)
sklearn.cluster.tests.test_hdbscan.test_labelling_thresholding()
sklearn.cluster.tests.test_hdbscan.test_outlier_data(outlier_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_bicluster.py----------------------------------------
A:sklearn.cluster.tests.test_bicluster.data->numpy.arange(25).reshape((5, 5))
A:sklearn.cluster.tests.test_bicluster.model->SpectralBiclustering(**params)
A:sklearn.cluster.tests.test_bicluster.submatrix->submatrix.toarray().toarray()
A:sklearn.cluster.tests.test_bicluster.X->numpy.random.RandomState(global_random_seed).rand(100, 100)
A:sklearn.cluster.tests.test_bicluster.(m, n)->SpectralBiclustering(**params).get_shape(i)
A:sklearn.cluster.tests.test_bicluster.(i_ind, j_ind)->SpectralBiclustering(**params).get_indices(i)
A:sklearn.cluster.tests.test_bicluster.(S, rows, cols)->make_checkerboard((30, 40), 3, noise=0, random_state=global_random_seed)
A:sklearn.cluster.tests.test_bicluster.S->numpy.where(S < 1, 0, S)
A:sklearn.cluster.tests.test_bicluster.row_sum->numpy.asarray(row_sum).squeeze()
A:sklearn.cluster.tests.test_bicluster.col_sum->numpy.asarray(col_sum).squeeze()
A:sklearn.cluster.tests.test_bicluster.generator->numpy.random.RandomState(global_random_seed)
A:sklearn.cluster.tests.test_bicluster.(scaled, _, _)->_scale_normalize(mat)
A:sklearn.cluster.tests.test_bicluster.scaled->_bistochastic_normalize(mat)
A:sklearn.cluster.tests.test_bicluster.mat->numpy.random.RandomState(global_random_seed).rand(100, 100)
A:sklearn.cluster.tests.test_bicluster.vectors->numpy.array([[1, 0], [0, 1], [0, 0]])
A:sklearn.cluster.tests.test_bicluster.best->SpectralBiclustering(**params)._fit_best_piecewise(vectors, n_best=2, n_clusters=2)
A:sklearn.cluster.tests.test_bicluster.labels->SpectralBiclustering(**params)._project_and_cluster(mat, vectors, n_clusters=2)
A:sklearn.cluster.tests.test_bicluster.(X, _, _)->make_biclusters((3, 3), 3, random_state=0)
sklearn.cluster.tests.test_bicluster.MockBiclustering(self)
sklearn.cluster.tests.test_bicluster.MockBiclustering.__init__(self)
sklearn.cluster.tests.test_bicluster.MockBiclustering.get_indices(self,i)
sklearn.cluster.tests.test_bicluster._do_bistochastic_test(scaled)
sklearn.cluster.tests.test_bicluster._do_scale_test(scaled)
sklearn.cluster.tests.test_bicluster._test_shape_indices(model)
sklearn.cluster.tests.test_bicluster.test_bistochastic_normalize(global_random_seed,csr_container)
sklearn.cluster.tests.test_bicluster.test_fit_best_piecewise(global_random_seed)
sklearn.cluster.tests.test_bicluster.test_get_submatrix(csr_container)
sklearn.cluster.tests.test_bicluster.test_log_normalize(global_random_seed)
sklearn.cluster.tests.test_bicluster.test_n_features_in_(est)
sklearn.cluster.tests.test_bicluster.test_perfect_checkerboard(global_random_seed)
sklearn.cluster.tests.test_bicluster.test_project_and_cluster(global_random_seed,csr_container)
sklearn.cluster.tests.test_bicluster.test_scale_normalize(global_random_seed,csr_container)
sklearn.cluster.tests.test_bicluster.test_spectral_biclustering(global_random_seed,csr_container)
sklearn.cluster.tests.test_bicluster.test_spectral_coclustering(global_random_seed,csr_container)
sklearn.cluster.tests.test_bicluster.test_spectralbiclustering_parameter_validation(params,type_err,err_msg)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_mean_shift.py----------------------------------------
A:sklearn.cluster.tests.test_mean_shift.(X, _)->make_blobs(n_samples=100, n_features=2, centers=[[0, 0], [1, 1]], cluster_std=0.1, random_state=0)
A:sklearn.cluster.tests.test_mean_shift.bandwidth->estimate_bandwidth(X)
A:sklearn.cluster.tests.test_mean_shift.X_with_global_dtype->numpy.array([1, 1, 1, 2, 2, 2, 3, 3], dtype=global_dtype).reshape(-1, 1).astype(global_dtype, copy=False)
A:sklearn.cluster.tests.test_mean_shift.ms->MeanShift(max_iter=max_iter).fit(X)
A:sklearn.cluster.tests.test_mean_shift.labels_unique->numpy.unique(labels)
A:sklearn.cluster.tests.test_mean_shift.n_clusters_->len(labels_unique)
A:sklearn.cluster.tests.test_mean_shift.(cluster_centers, labels_mean_shift)->mean_shift(X_with_global_dtype, cluster_all=cluster_all)
A:sklearn.cluster.tests.test_mean_shift.labels_mean_shift_unique->numpy.unique(labels_mean_shift)
A:sklearn.cluster.tests.test_mean_shift.n_clusters_mean_shift->len(labels_mean_shift_unique)
A:sklearn.cluster.tests.test_mean_shift.X->numpy.array([1, 1, 1, 2, 2, 2, 3, 3], dtype=global_dtype).reshape(-1, 1)
A:sklearn.cluster.tests.test_mean_shift.ms1->MeanShift(n_jobs=2)
A:sklearn.cluster.tests.test_mean_shift.ms2->MeanShift()
A:sklearn.cluster.tests.test_mean_shift.labels->MeanShift(max_iter=max_iter).fit(X).fit_predict(X_with_global_dtype)
A:sklearn.cluster.tests.test_mean_shift.labels2->MeanShift(max_iter=max_iter).fit(X).predict(X_with_global_dtype)
A:sklearn.cluster.tests.test_mean_shift.c1->MeanShift(bandwidth=2).fit(X)
A:sklearn.cluster.tests.test_mean_shift.c2->MeanShift(bandwidth=2).fit(X)
A:sklearn.cluster.tests.test_mean_shift.test_bins->get_bin_seeds(X, 1)
A:sklearn.cluster.tests.test_mean_shift.test_result->set((tuple(p) for p in test_bins))
A:sklearn.cluster.tests.test_mean_shift.(clusters1, _)->mean_shift(X, max_iter=max_iter)
A:sklearn.cluster.tests.test_mean_shift.ms_binning->MeanShift(bin_seeding=True, bandwidth=None).fit(X)
A:sklearn.cluster.tests.test_mean_shift.ms_nobinning->MeanShift(bin_seeding=False).fit(X)
A:sklearn.cluster.tests.test_mean_shift.expected_labels->numpy.array([0, 0, 0, 1, 1, 1, 2, 2])
sklearn.cluster.tests.test_mean_shift.test_bin_seeds(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_cluster_intensity_tie(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_estimate_bandwidth()
sklearn.cluster.tests.test_mean_shift.test_estimate_bandwidth_1sample(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_max_iter(max_iter)
sklearn.cluster.tests.test_mean_shift.test_mean_shift(global_dtype,bandwidth,cluster_all,expected,first_cluster_label)
sklearn.cluster.tests.test_mean_shift.test_mean_shift_zero_bandwidth(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_meanshift_all_orphans()
sklearn.cluster.tests.test_mean_shift.test_meanshift_predict(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_parallel(global_dtype)
sklearn.cluster.tests.test_mean_shift.test_unfitted()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_feature_agglomeration.py----------------------------------------
A:sklearn.cluster.tests.test_feature_agglomeration.X->numpy.array([0, 0, 1]).reshape(1, 3)
A:sklearn.cluster.tests.test_feature_agglomeration.agglo_mean->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.mean)
A:sklearn.cluster.tests.test_feature_agglomeration.agglo_median->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.median)
A:sklearn.cluster.tests.test_feature_agglomeration.Xt_mean->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.mean).transform(X)
A:sklearn.cluster.tests.test_feature_agglomeration.Xt_median->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.median).transform(X)
A:sklearn.cluster.tests.test_feature_agglomeration.X_full_mean->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.mean).inverse_transform(Xt_mean)
A:sklearn.cluster.tests.test_feature_agglomeration.X_full_median->FeatureAgglomeration(n_clusters=n_clusters, pooling_func=np.median).inverse_transform(Xt_median)
A:sklearn.cluster.tests.test_feature_agglomeration.(X, _)->make_blobs(n_features=6, random_state=0)
A:sklearn.cluster.tests.test_feature_agglomeration.agglo->FeatureAgglomeration(n_clusters=3)
A:sklearn.cluster.tests.test_feature_agglomeration.names_out->FeatureAgglomeration(n_clusters=3).get_feature_names_out()
A:sklearn.cluster.tests.test_feature_agglomeration.est->FeatureAgglomeration(n_clusters=1, pooling_func=np.mean)
A:sklearn.cluster.tests.test_feature_agglomeration.Xt->FeatureAgglomeration(n_clusters=1, pooling_func=np.mean).transform(X)
sklearn.cluster.tests.test_feature_agglomeration.test_feature_agglomeration()
sklearn.cluster.tests.test_feature_agglomeration.test_feature_agglomeration_feature_names_out()
sklearn.cluster.tests.test_feature_agglomeration.test_inverse_transform_Xred_deprecation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/tests/test_bisect_k_means.py----------------------------------------
A:sklearn.cluster.tests.test_bisect_k_means.X->numpy.random.normal(size=(128, 1))
A:sklearn.cluster.tests.test_bisect_k_means.bisect_means->BisectingKMeans(n_clusters=3, random_state=0)
A:sklearn.cluster.tests.test_bisect_k_means.rng->numpy.random.RandomState(0)
A:sklearn.cluster.tests.test_bisect_k_means.X_csr->csr_container(X)
A:sklearn.cluster.tests.test_bisect_k_means.km->BisectingKMeans(n_clusters=3, random_state=0)
A:sklearn.cluster.tests.test_bisect_k_means.km64->BisectingKMeans(n_clusters=3, random_state=0).fit(X)
A:sklearn.cluster.tests.test_bisect_k_means.km32->BisectingKMeans(n_clusters=3, random_state=0).fit(X.astype(np.float32))
A:sklearn.cluster.tests.test_bisect_k_means.X_train->numpy.random.RandomState(0).rand(3000, 10)
A:sklearn.cluster.tests.test_bisect_k_means.bkm->BisectingKMeans(n_clusters=10, algorithm=algorithm).fit(X_train)
A:sklearn.cluster.tests.test_bisect_k_means.labels->BisectingKMeans(n_clusters=10, algorithm=algorithm).fit(X_train).predict(X_test)
sklearn.cluster.tests.test_bisect_k_means.test_dtype_preserved(csr_container,global_dtype)
sklearn.cluster.tests.test_bisect_k_means.test_fit_predict(csr_container)
sklearn.cluster.tests.test_bisect_k_means.test_float32_float64_equivalence(csr_container)
sklearn.cluster.tests.test_bisect_k_means.test_n_clusters(n_clusters)
sklearn.cluster.tests.test_bisect_k_means.test_no_crash_on_empty_bisections(algorithm)
sklearn.cluster.tests.test_bisect_k_means.test_one_cluster()
sklearn.cluster.tests.test_bisect_k_means.test_one_feature()
sklearn.cluster.tests.test_bisect_k_means.test_sparse(csr_container)
sklearn.cluster.tests.test_bisect_k_means.test_three_clusters(bisecting_strategy,init)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_hdbscan/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_hdbscan/hdbscan.py----------------------------------------
A:sklearn.cluster._hdbscan.hdbscan.FAST_METRICS->set(KDTree.valid_metrics + BallTree.valid_metrics)
A:sklearn.cluster._hdbscan.hdbscan.n_components->scipy.sparse.csgraph.connected_components(mutual_reachability, directed=False, return_labels=False)
A:sklearn.cluster._hdbscan.hdbscan.sparse_min_spanning_tree->scipy.sparse.csgraph.minimum_spanning_tree(mutual_reachability)
A:sklearn.cluster._hdbscan.hdbscan.(rows, cols)->scipy.sparse.csgraph.minimum_spanning_tree(mutual_reachability).nonzero()
A:sklearn.cluster._hdbscan.hdbscan.mst->numpy.rec.fromarrays([rows, cols, sparse_min_spanning_tree.data], dtype=MST_edge_dtype)
A:sklearn.cluster._hdbscan.hdbscan.row_order->numpy.argsort(min_spanning_tree['distance'])
A:sklearn.cluster._hdbscan.hdbscan.distance_matrix->distance_matrix.tocsr().tocsr()
A:sklearn.cluster._hdbscan.hdbscan.max_distance->metric_params.get('max_distance', 0.0)
A:sklearn.cluster._hdbscan.hdbscan.mutual_reachability_->mutual_reachability_graph(distance_matrix, min_samples=min_samples, max_distance=max_distance)
A:sklearn.cluster._hdbscan.hdbscan.min_spanning_tree->mst_from_data_matrix(X, core_distances, dist_metric, alpha)
A:sklearn.cluster._hdbscan.hdbscan.X->self._validate_data(X, force_all_finite=False, dtype=np.float64)
A:sklearn.cluster._hdbscan.hdbscan.nbrs->NearestNeighbors(n_neighbors=min_samples, algorithm=algo, leaf_size=leaf_size, metric=metric, metric_params=metric_params, n_jobs=n_jobs, p=None).fit(X)
A:sklearn.cluster._hdbscan.hdbscan.(neighbors_distances, _)->NearestNeighbors(n_neighbors=min_samples, algorithm=algo, leaf_size=leaf_size, metric=metric, metric_params=metric_params, n_jobs=n_jobs, p=None).fit(X).kneighbors(X, min_samples, return_distance=True)
A:sklearn.cluster._hdbscan.hdbscan.core_distances->numpy.ascontiguousarray(neighbors_distances[:, -1])
A:sklearn.cluster._hdbscan.hdbscan.dist_metric->metrics._dist_metrics.DistanceMetric.get_metric(metric, **metric_params)
A:sklearn.cluster._hdbscan.hdbscan.finite_count->len(internal_to_raw)
A:sklearn.cluster._hdbscan.hdbscan.outlier_count->len(non_finite)
A:sklearn.cluster._hdbscan.hdbscan.outlier_tree->numpy.zeros(len(non_finite), dtype=HIERARCHY_dtype)
A:sklearn.cluster._hdbscan.hdbscan.last_cluster_id->max(tree[tree.shape[0] - 1]['left_node'], tree[tree.shape[0] - 1]['right_node'])
A:sklearn.cluster._hdbscan.hdbscan.tree->numpy.concatenate([tree, outlier_tree])
A:sklearn.cluster._hdbscan.hdbscan.row_indices->numpy.array([i for (i, row) in enumerate(matrix.tolil().data) if np.all(np.isfinite(row))])
A:sklearn.cluster._hdbscan.hdbscan.(row_indices,)->numpy.isfinite(matrix.sum(axis=1)).nonzero()
A:sklearn.cluster._hdbscan.hdbscan.reduced_X->self._validate_data(X, force_all_finite=False, dtype=np.float64).sum(axis=1)
A:sklearn.cluster._hdbscan.hdbscan.finite_index->_get_finite_row_indices(X)
A:sklearn.cluster._hdbscan.hdbscan.kwargs->dict(X=X, min_samples=self._min_samples, alpha=self.alpha, metric=self.metric, n_jobs=self.n_jobs, **self._metric_params)
A:sklearn.cluster._hdbscan.hdbscan.self._single_linkage_tree_->remap_single_linkage_tree(self._single_linkage_tree_, internal_to_raw, non_finite=set(np.hstack([infinite_index, missing_index])))
A:sklearn.cluster._hdbscan.hdbscan.(self.labels_, self.probabilities_)->tree_to_labels(self._single_linkage_tree_, self.min_cluster_size, self.cluster_selection_method, self.allow_single_cluster, self.cluster_selection_epsilon, self.max_cluster_size)
A:sklearn.cluster._hdbscan.hdbscan.new_labels->numpy.empty(self._raw_data.shape[0], dtype=np.int32)
A:sklearn.cluster._hdbscan.hdbscan.new_probabilities->numpy.zeros(self._raw_data.shape[0], dtype=np.float64)
A:sklearn.cluster._hdbscan.hdbscan.n_clusters->len(set(self.labels_) - {-1, -2})
A:sklearn.cluster._hdbscan.hdbscan.mask->numpy.empty((X.shape[0],), dtype=np.bool_)
A:sklearn.cluster._hdbscan.hdbscan.self.centroids_->numpy.empty((n_clusters, X.shape[1]), dtype=np.float64)
A:sklearn.cluster._hdbscan.hdbscan.self.medoids_->numpy.empty((n_clusters, X.shape[1]), dtype=np.float64)
A:sklearn.cluster._hdbscan.hdbscan.self.centroids_[idx]->numpy.average(data, weights=strength, axis=0)
A:sklearn.cluster._hdbscan.hdbscan.dist_mat->pairwise_distances(data, metric=self.metric, **self._metric_params)
A:sklearn.cluster._hdbscan.hdbscan.medoid_index->numpy.argmin(dist_mat.sum(axis=1))
A:sklearn.cluster._hdbscan.hdbscan.labels->labelling_at_cut(self._single_linkage_tree_, cut_distance, min_cluster_size)
sklearn.cluster.HDBSCAN(self,min_cluster_size=5,min_samples=None,cluster_selection_epsilon=0.0,max_cluster_size=None,metric='euclidean',metric_params=None,alpha=1.0,algorithm='auto',leaf_size=40,n_jobs=None,cluster_selection_method='eom',allow_single_cluster=False,store_centers=None,copy=False)
sklearn.cluster.HDBSCAN._more_tags(self)
sklearn.cluster.HDBSCAN._weighted_cluster_center(self,X)
sklearn.cluster.HDBSCAN.dbscan_clustering(self,cut_distance,min_cluster_size=5)
sklearn.cluster.HDBSCAN.fit(self,X,y=None)
sklearn.cluster.HDBSCAN.fit_predict(self,X,y=None)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN(self,min_cluster_size=5,min_samples=None,cluster_selection_epsilon=0.0,max_cluster_size=None,metric='euclidean',metric_params=None,alpha=1.0,algorithm='auto',leaf_size=40,n_jobs=None,cluster_selection_method='eom',allow_single_cluster=False,store_centers=None,copy=False)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN.__init__(self,min_cluster_size=5,min_samples=None,cluster_selection_epsilon=0.0,max_cluster_size=None,metric='euclidean',metric_params=None,alpha=1.0,algorithm='auto',leaf_size=40,n_jobs=None,cluster_selection_method='eom',allow_single_cluster=False,store_centers=None,copy=False)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN._more_tags(self)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN._weighted_cluster_center(self,X)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN.dbscan_clustering(self,cut_distance,min_cluster_size=5)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN.fit(self,X,y=None)
sklearn.cluster._hdbscan.hdbscan.HDBSCAN.fit_predict(self,X,y=None)
sklearn.cluster._hdbscan.hdbscan._brute_mst(mutual_reachability,min_samples)
sklearn.cluster._hdbscan.hdbscan._get_finite_row_indices(matrix)
sklearn.cluster._hdbscan.hdbscan._hdbscan_brute(X,min_samples=5,alpha=None,metric='euclidean',n_jobs=None,copy=False,**metric_params)
sklearn.cluster._hdbscan.hdbscan._hdbscan_prims(X,algo,min_samples=5,alpha=1.0,metric='euclidean',leaf_size=40,n_jobs=None,**metric_params)
sklearn.cluster._hdbscan.hdbscan._process_mst(min_spanning_tree)
sklearn.cluster._hdbscan.hdbscan.remap_single_linkage_tree(tree,internal_to_raw,non_finite)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_hdbscan/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cluster/_hdbscan/tests/test_reachibility.py----------------------------------------
A:sklearn.cluster._hdbscan.tests.test_reachibility.rng->numpy.random.RandomState(0)
A:sklearn.cluster._hdbscan.tests.test_reachibility.X->_convert_container(X, array_type)
A:sklearn.cluster._hdbscan.tests.test_reachibility.mr_graph->mutual_reachability_graph(X)
A:sklearn.cluster._hdbscan.tests.test_reachibility.X_sparse->_convert_container(X_dense, 'sparse_csr')
A:sklearn.cluster._hdbscan.tests.test_reachibility.mr_graph_dense->mutual_reachability_graph(X_dense, min_samples=3)
A:sklearn.cluster._hdbscan.tests.test_reachibility.mr_graph_sparse->mutual_reachability_graph(X_sparse, min_samples=3)
sklearn.cluster._hdbscan.tests.test_reachibility.test_mutual_reachability_graph_equivalence_dense_sparse()
sklearn.cluster._hdbscan.tests.test_reachibility.test_mutual_reachability_graph_error_sparse_format()
sklearn.cluster._hdbscan.tests.test_reachibility.test_mutual_reachability_graph_inplace(array_type)
sklearn.cluster._hdbscan.tests.test_reachibility.test_mutual_reachability_graph_preserve_dtype(array_type,dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_ranking.py----------------------------------------
A:sklearn.metrics._ranking.x->column_or_1d(x)
A:sklearn.metrics._ranking.y->column_or_1d(y)
A:sklearn.metrics._ranking.dx->numpy.diff(x)
A:sklearn.metrics._ranking.area->area.dtype.type(area).dtype.type(area)
A:sklearn.metrics._ranking.(precision, recall, _)->precision_recall_curve(y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
A:sklearn.metrics._ranking.y_type->type_of_target(y_true, input_name='y_true')
A:sklearn.metrics._ranking.present_labels->numpy.unique(y_true).tolist()
A:sklearn.metrics._ranking.y_true->column_or_1d(y_true)
A:sklearn.metrics._ranking.average_precision->partial(_binary_uninterpolated_average_precision, pos_label=pos_label)
A:sklearn.metrics._ranking.(fps, tps, thresholds)->_binary_clf_curve(y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)
A:sklearn.metrics._ranking.sl->slice(None, None, -1)
A:sklearn.metrics._ranking.(fpr, tpr, _)->roc_curve(y_true, y_score, sample_weight=sample_weight)
A:sklearn.metrics._ranking.stop->numpy.searchsorted(fpr, max_fpr, 'right')
A:sklearn.metrics._ranking.tpr->numpy.repeat(np.nan, tps.shape)
A:sklearn.metrics._ranking.fpr->numpy.repeat(np.nan, fps.shape)
A:sklearn.metrics._ranking.partial_auc->auc(fpr, tpr)
A:sklearn.metrics._ranking.y_score->column_or_1d(y_score)
A:sklearn.metrics._ranking.labels->column_or_1d(labels)
A:sklearn.metrics._ranking.classes->_unique(labels)
A:sklearn.metrics._ranking.y_true_encoded->_encode(y_true, uniques=classes)
A:sklearn.metrics._ranking.y_true_multilabel->label_binarize(y_true, classes=classes)
A:sklearn.metrics._ranking.sample_weight->_check_sample_weight(sample_weight, y_true)
A:sklearn.metrics._ranking.pos_label->_check_pos_label_consistency(pos_label, y_true)
A:sklearn.metrics._ranking.precision->numpy.zeros_like(tps)
A:sklearn.metrics._ranking.recall->numpy.ones_like(tps)
A:sklearn.metrics._ranking.L->rankdata(scores_i[relevant], 'max')
A:sklearn.metrics._ranking.aux->(L / rank).mean()
A:sklearn.metrics._ranking.y_score_mask->numpy.ma.masked_array(y_score, mask=np.logical_not(y_true))
A:sklearn.metrics._ranking.y_min_relevant->numpy.ma.masked_array(y_score, mask=np.logical_not(y_true)).min(axis=1).reshape((-1, 1))
A:sklearn.metrics._ranking.coverage->coverage.filled(0).filled(0)
A:sklearn.metrics._ranking.loss->numpy.zeros(n_samples)
A:sklearn.metrics._ranking.(unique_scores, unique_inverse)->numpy.unique(y_score[i], return_inverse=True)
A:sklearn.metrics._ranking.true_at_reversed_rank->numpy.bincount(unique_inverse[y_true.indices[start:stop]], minlength=len(unique_scores))
A:sklearn.metrics._ranking.all_at_reversed_rank->numpy.bincount(unique_inverse, minlength=len(unique_scores))
A:sklearn.metrics._ranking.loss[i]->numpy.dot(true_at_reversed_rank.cumsum(), false_at_reversed_rank)
A:sklearn.metrics._ranking.n_positives->count_nonzero(y_true, axis=1)
A:sklearn.metrics._ranking.cumulative_gains->numpy.asarray(cumulative_gains)
A:sklearn.metrics._ranking.discount_cumsum->numpy.cumsum(discount)
A:sklearn.metrics._ranking.(_, inv, counts)->numpy.unique(-y_score, return_inverse=True, return_counts=True)
A:sklearn.metrics._ranking.ranked->numpy.zeros(len(counts))
A:sklearn.metrics._ranking.discount_sums->numpy.empty(len(counts))
A:sklearn.metrics._ranking.discount_sums[1:]->numpy.diff(discount_cumsum[groups])
A:sklearn.metrics._ranking.gain->_ndcg_sample_scores(y_true, y_score, k=k, ignore_ties=ignore_ties)
A:sklearn.metrics._ranking.normalizing_gain->_dcg_sample_scores(y_true, y_true, k, ignore_ties=True)
A:sklearn.metrics._ranking.n_classes->len(classes)
A:sklearn.metrics._ranking.n_labels->len(labels)
A:sklearn.metrics._ranking.y_pred->(y_score > threshold).astype(np.int64)
A:sklearn.metrics._ranking.hits->(y_true_encoded == sorted_pred[:, :k].T).any(axis=0)
sklearn.metrics._ranking._binary_clf_curve(y_true,y_score,pos_label=None,sample_weight=None)
sklearn.metrics._ranking._binary_roc_auc_score(y_true,y_score,sample_weight=None,max_fpr=None)
sklearn.metrics._ranking._check_dcg_target_type(y_true)
sklearn.metrics._ranking._dcg_sample_scores(y_true,y_score,k=None,log_base=2,ignore_ties=False)
sklearn.metrics._ranking._multiclass_roc_auc_score(y_true,y_score,labels,multi_class,average,sample_weight)
sklearn.metrics._ranking._ndcg_sample_scores(y_true,y_score,k=None,ignore_ties=False)
sklearn.metrics._ranking._tie_averaged_dcg(y_true,y_score,discount_cumsum)
sklearn.metrics._ranking.auc(x,y)
sklearn.metrics._ranking.average_precision_score(y_true,y_score,*,average='macro',pos_label=1,sample_weight=None)
sklearn.metrics._ranking.coverage_error(y_true,y_score,*,sample_weight=None)
sklearn.metrics._ranking.dcg_score(y_true,y_score,*,k=None,log_base=2,sample_weight=None,ignore_ties=False)
sklearn.metrics._ranking.det_curve(y_true,y_score,pos_label=None,sample_weight=None)
sklearn.metrics._ranking.label_ranking_average_precision_score(y_true,y_score,*,sample_weight=None)
sklearn.metrics._ranking.label_ranking_loss(y_true,y_score,*,sample_weight=None)
sklearn.metrics._ranking.ndcg_score(y_true,y_score,*,k=None,sample_weight=None,ignore_ties=False)
sklearn.metrics._ranking.precision_recall_curve(y_true,probas_pred,*,pos_label=None,sample_weight=None,drop_intermediate=False)
sklearn.metrics._ranking.roc_auc_score(y_true,y_score,*,average='macro',sample_weight=None,max_fpr=None,multi_class='raise',labels=None)
sklearn.metrics._ranking.roc_curve(y_true,y_score,*,pos_label=None,sample_weight=None,drop_intermediate=True)
sklearn.metrics._ranking.top_k_accuracy_score(y_true,y_score,*,k=2,normalize=True,sample_weight=None,labels=None)
sklearn.metrics.auc(x,y)
sklearn.metrics.average_precision_score(y_true,y_score,*,average='macro',pos_label=1,sample_weight=None)
sklearn.metrics.coverage_error(y_true,y_score,*,sample_weight=None)
sklearn.metrics.dcg_score(y_true,y_score,*,k=None,log_base=2,sample_weight=None,ignore_ties=False)
sklearn.metrics.det_curve(y_true,y_score,pos_label=None,sample_weight=None)
sklearn.metrics.label_ranking_average_precision_score(y_true,y_score,*,sample_weight=None)
sklearn.metrics.label_ranking_loss(y_true,y_score,*,sample_weight=None)
sklearn.metrics.ndcg_score(y_true,y_score,*,k=None,sample_weight=None,ignore_ties=False)
sklearn.metrics.precision_recall_curve(y_true,probas_pred,*,pos_label=None,sample_weight=None,drop_intermediate=False)
sklearn.metrics.roc_auc_score(y_true,y_score,*,average='macro',sample_weight=None,max_fpr=None,multi_class='raise',labels=None)
sklearn.metrics.roc_curve(y_true,y_score,*,pos_label=None,sample_weight=None,drop_intermediate=True)
sklearn.metrics.top_k_accuracy_score(y_true,y_score,*,k=2,normalize=True,sample_weight=None,labels=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_classification.py----------------------------------------
A:sklearn.metrics._classification.type_true->type_of_target(y_true, input_name='y_true')
A:sklearn.metrics._classification.type_pred->type_of_target(y_pred, input_name='y_pred')
A:sklearn.metrics._classification.y_type->type_of_target(y_true, input_name='y_true')
A:sklearn.metrics._classification.(xp, _)->get_namespace(y_true, y_pred)
A:sklearn.metrics._classification.y_true->numpy.array(y_true == pos_label, int)
A:sklearn.metrics._classification.y_pred->numpy.append(1 - y_pred, y_pred, axis=1)
A:sklearn.metrics._classification.unique_values->_union1d(y_true, y_pred, xp)
A:sklearn.metrics._classification.(y_type, y_true, y_pred)->_check_targets(y_true, y_pred)
A:sklearn.metrics._classification.differing_labels->count_nonzero(y_true - y_pred, axis=1)
A:sklearn.metrics._classification.labels->numpy.asarray(labels)
A:sklearn.metrics._classification.sample_weight->numpy.array(sample_weight)
A:sklearn.metrics._classification.ind->numpy.logical_and(y_pred < n_labels, y_true < n_labels)
A:sklearn.metrics._classification.cm->confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels)
A:sklearn.metrics._classification.present_labels->unique_labels(y_true, y_pred).tolist()
A:sklearn.metrics._classification.n_labels->len(labels)
A:sklearn.metrics._classification.le->LabelEncoder()
A:sklearn.metrics._classification.tp_sum->numpy.array([tp_sum.sum()])
A:sklearn.metrics._classification.true_sumpred_sumtp_sum->numpy.zeros(len(labels))
A:sklearn.metrics._classification.pred_sum->numpy.array([pred_sum.sum()])
A:sklearn.metrics._classification.true_sum->numpy.array([true_sum.sum()])
A:sklearn.metrics._classification.indices->numpy.searchsorted(sorted_labels, labels[:n_labels])
A:sklearn.metrics._classification.true_and_pred->numpy.array(y_true == pos_label, int).multiply(y_pred)
A:sklearn.metrics._classification.tp->numpy.array(tp)
A:sklearn.metrics._classification.fp->numpy.array(fp)
A:sklearn.metrics._classification.fn->numpy.array(fn)
A:sklearn.metrics._classification.confusion->confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)
A:sklearn.metrics._classification.sum0->numpy.sum(confusion, axis=0)
A:sklearn.metrics._classification.sum1->numpy.sum(confusion, axis=1)
A:sklearn.metrics._classification.w_mat->numpy.abs(w_mat - w_mat.T)
A:sklearn.metrics._classification.MCM->multilabel_confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels, samplewise=samplewise)
A:sklearn.metrics._classification.numerator->numpy.array([numerator.sum()])
A:sklearn.metrics._classification.denominator->denominator.copy().copy()
A:sklearn.metrics._classification.jaccard->_prf_divide(numerator, denominator, 'jaccard', 'true or predicted', average, ('jaccard',), zero_division=zero_division)
A:sklearn.metrics._classification.lb->LabelBinarizer()
A:sklearn.metrics._classification.C->confusion_matrix(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics._classification.t_sum->confusion_matrix(y_true, y_pred, sample_weight=sample_weight).sum(axis=1, dtype=np.float64)
A:sklearn.metrics._classification.p_sum->confusion_matrix(y_true, y_pred, sample_weight=sample_weight).sum(axis=0, dtype=np.float64)
A:sklearn.metrics._classification.n_correct->numpy.trace(C, dtype=np.float64)
A:sklearn.metrics._classification.n_samples->_num_samples(y_true)
A:sklearn.metrics._classification.score->numpy.mean(per_class)
A:sklearn.metrics._classification.(_, _, f, _)->precision_recall_fscore_support(y_true, y_pred, beta=beta, labels=labels, pos_label=pos_label, average=average, warn_for=('f-score',), sample_weight=sample_weight, zero_division=zero_division)
A:sklearn.metrics._classification.zero_division_value->_check_zero_division(zero_division)
A:sklearn.metrics._classification.msg->msg.format('in {0}s with'.format(axis1)).format('in {0}s with'.format(axis1))
A:sklearn.metrics._classification.average_options->list(average_options)
A:sklearn.metrics._classification.precision->_nanaverage(precision, weights=weights)
A:sklearn.metrics._classification.recall->_nanaverage(recall, weights=weights)
A:sklearn.metrics._classification.f_score->_nanaverage(f_score, weights=weights)
A:sklearn.metrics._classification.(tn, fp, fn, tp)->confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels).ravel()
A:sklearn.metrics._classification.(p, _, _, _)->precision_recall_fscore_support(y_true, y_pred, labels=labels, pos_label=pos_label, average=average, warn_for=('precision',), sample_weight=sample_weight, zero_division=zero_division)
A:sklearn.metrics._classification.(_, r, _, _)->precision_recall_fscore_support(y_true, y_pred, labels=labels, pos_label=pos_label, average=average, warn_for=('recall',), sample_weight=sample_weight, zero_division=zero_division)
A:sklearn.metrics._classification.n_classes->len(per_class)
A:sklearn.metrics._classification.(p, r, f1, s)->precision_recall_fscore_support(y_true, y_pred, labels=labels, average=None, sample_weight=sample_weight, zero_division=zero_division)
A:sklearn.metrics._classification.rows->zip(target_names, p, r, f1, s)
A:sklearn.metrics._classification.report_dict[label]->dict(zip(headers, [float(i) for i in scores]))
A:sklearn.metrics._classification.name_width->max((len(cn) for cn in target_names))
A:sklearn.metrics._classification.width->max(name_width, len(longest_last_line_heading), digits)
A:sklearn.metrics._classification.report->head_fmt.format('', *headers, width=width)
A:sklearn.metrics._classification.(avg_p, avg_r, avg_f1, _)->precision_recall_fscore_support(y_true, y_pred, labels=labels, average=average, sample_weight=sample_weight, zero_division=zero_division)
A:sklearn.metrics._classification.report_dict[line_heading]->dict(zip(headers, [float(i) for i in avg]))
A:sklearn.metrics._classification.weight_average->numpy.mean(sample_weight)
A:sklearn.metrics._classification.n_differences->count_nonzero(y_true - y_pred, sample_weight=sample_weight)
A:sklearn.metrics._classification.transformed_labels->check_array(transformed_labels)
A:sklearn.metrics._classification.y_pred_sum->numpy.append(1 - y_pred, y_pred, axis=1).sum(axis=1)
A:sklearn.metrics._classification.pred_decision->numpy.ravel(pred_decision)
A:sklearn.metrics._classification.y_true_unique->numpy.unique(labels if labels is not None else y_true)
A:sklearn.metrics._classification.mask->numpy.ones_like(pred_decision, dtype=bool)
A:sklearn.metrics._classification.lbin->LabelBinarizer(neg_label=-1)
A:sklearn.metrics._classification.y_prob->column_or_1d(y_prob)
A:sklearn.metrics._classification.pos_label->_check_pos_label_consistency(pos_label, y_true)
A:sklearn.metrics._classification.classes->numpy.unique(y_true)
sklearn.metrics._classification._check_set_wise_labels(y_true,y_pred,average,labels,pos_label)
sklearn.metrics._classification._check_targets(y_true,y_pred)
sklearn.metrics._classification._check_zero_division(zero_division)
sklearn.metrics._classification._prf_divide(numerator,denominator,metric,modifier,average,warn_for,zero_division='warn')
sklearn.metrics._classification._warn_prf(average,modifier,msg_start,result_size)
sklearn.metrics._classification.accuracy_score(y_true,y_pred,*,normalize=True,sample_weight=None)
sklearn.metrics._classification.balanced_accuracy_score(y_true,y_pred,*,sample_weight=None,adjusted=False)
sklearn.metrics._classification.brier_score_loss(y_true,y_prob,*,sample_weight=None,pos_label=None)
sklearn.metrics._classification.class_likelihood_ratios(y_true,y_pred,*,labels=None,sample_weight=None,raise_warning=True)
sklearn.metrics._classification.classification_report(y_true,y_pred,*,labels=None,target_names=None,sample_weight=None,digits=2,output_dict=False,zero_division='warn')
sklearn.metrics._classification.cohen_kappa_score(y1,y2,*,labels=None,weights=None,sample_weight=None)
sklearn.metrics._classification.confusion_matrix(y_true,y_pred,*,labels=None,sample_weight=None,normalize=None)
sklearn.metrics._classification.f1_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics._classification.fbeta_score(y_true,y_pred,*,beta,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics._classification.hamming_loss(y_true,y_pred,*,sample_weight=None)
sklearn.metrics._classification.hinge_loss(y_true,pred_decision,*,labels=None,sample_weight=None)
sklearn.metrics._classification.jaccard_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics._classification.log_loss(y_true,y_pred,*,eps='auto',normalize=True,sample_weight=None,labels=None)
sklearn.metrics._classification.matthews_corrcoef(y_true,y_pred,*,sample_weight=None)
sklearn.metrics._classification.multilabel_confusion_matrix(y_true,y_pred,*,sample_weight=None,labels=None,samplewise=False)
sklearn.metrics._classification.precision_recall_fscore_support(y_true,y_pred,*,beta=1.0,labels=None,pos_label=1,average=None,warn_for=('precision','recall','f-score'),sample_weight=None,zero_division='warn')
sklearn.metrics._classification.precision_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics._classification.recall_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics._classification.zero_one_loss(y_true,y_pred,*,normalize=True,sample_weight=None)
sklearn.metrics.accuracy_score(y_true,y_pred,*,normalize=True,sample_weight=None)
sklearn.metrics.balanced_accuracy_score(y_true,y_pred,*,sample_weight=None,adjusted=False)
sklearn.metrics.brier_score_loss(y_true,y_prob,*,sample_weight=None,pos_label=None)
sklearn.metrics.class_likelihood_ratios(y_true,y_pred,*,labels=None,sample_weight=None,raise_warning=True)
sklearn.metrics.classification_report(y_true,y_pred,*,labels=None,target_names=None,sample_weight=None,digits=2,output_dict=False,zero_division='warn')
sklearn.metrics.cohen_kappa_score(y1,y2,*,labels=None,weights=None,sample_weight=None)
sklearn.metrics.confusion_matrix(y_true,y_pred,*,labels=None,sample_weight=None,normalize=None)
sklearn.metrics.f1_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics.fbeta_score(y_true,y_pred,*,beta,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics.hamming_loss(y_true,y_pred,*,sample_weight=None)
sklearn.metrics.hinge_loss(y_true,pred_decision,*,labels=None,sample_weight=None)
sklearn.metrics.jaccard_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics.log_loss(y_true,y_pred,*,eps='auto',normalize=True,sample_weight=None,labels=None)
sklearn.metrics.matthews_corrcoef(y_true,y_pred,*,sample_weight=None)
sklearn.metrics.multilabel_confusion_matrix(y_true,y_pred,*,sample_weight=None,labels=None,samplewise=False)
sklearn.metrics.precision_recall_fscore_support(y_true,y_pred,*,beta=1.0,labels=None,pos_label=1,average=None,warn_for=('precision','recall','f-score'),sample_weight=None,zero_division='warn')
sklearn.metrics.precision_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics.recall_score(y_true,y_pred,*,labels=None,pos_label=1,average='binary',sample_weight=None,zero_division='warn')
sklearn.metrics.zero_one_loss(y_true,y_pred,*,normalize=True,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_regression.py----------------------------------------
A:sklearn.metrics._regression.y_true->y_true.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics._regression.y_pred->y_pred.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics._regression.multioutput->check_array(multioutput, ensure_2d=False)
A:sklearn.metrics._regression.(y_type, y_true, y_pred, multioutput)->_check_reg_targets(y_true, y_pred, multioutput)
A:sklearn.metrics._regression.output_errors->_weighted_percentile(np.abs(y_pred - y_true), sample_weight=sample_weight)
A:sklearn.metrics._regression.sign->(diff >= 0).astype(diff.dtype)
A:sklearn.metrics._regression.(_, y_true, y_pred, multioutput)->_check_reg_targets(y_true, y_pred, multioutput)
A:sklearn.metrics._regression.sample_weight->_check_sample_weight(sample_weight, y_true)
A:sklearn.metrics._regression.output_scores->numpy.ones(y_true.shape[1])
A:sklearn.metrics._regression.y_diff_avg->numpy.average(y_true - y_pred, weights=sample_weight, axis=0)
A:sklearn.metrics._regression.numerator->mean_pinball_loss(y_true, y_pred, sample_weight=sample_weight, alpha=alpha, multioutput='raw_values')
A:sklearn.metrics._regression.y_true_avg->numpy.average(y_true, weights=sample_weight, axis=0)
A:sklearn.metrics._regression.denominator->mean_pinball_loss(y_true, y_quantile, sample_weight=sample_weight, alpha=alpha, multioutput='raw_values')
A:sklearn.metrics._regression.(y_type, y_true, y_pred, _)->_check_reg_targets(y_true, y_pred, None, dtype=[np.float64, np.float32])
A:sklearn.metrics._regression.y_avg->numpy.average(y_true, weights=sample_weight)
A:sklearn.metrics._regression.y_quantile->numpy.tile(_weighted_percentile(y_true, sample_weight=sample_weight, percentile=alpha * 100), (len(y_true), 1))
sklearn.metrics._regression._assemble_r2_explained_variance(numerator,denominator,n_outputs,multioutput,force_finite)
sklearn.metrics._regression._check_reg_targets(y_true,y_pred,multioutput,dtype='numeric')
sklearn.metrics._regression._mean_tweedie_deviance(y_true,y_pred,sample_weight,power)
sklearn.metrics._regression.d2_absolute_error_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics._regression.d2_pinball_score(y_true,y_pred,*,sample_weight=None,alpha=0.5,multioutput='uniform_average')
sklearn.metrics._regression.d2_tweedie_score(y_true,y_pred,*,sample_weight=None,power=0)
sklearn.metrics._regression.explained_variance_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',force_finite=True)
sklearn.metrics._regression.max_error(y_true,y_pred)
sklearn.metrics._regression.mean_absolute_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics._regression.mean_absolute_percentage_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics._regression.mean_gamma_deviance(y_true,y_pred,*,sample_weight=None)
sklearn.metrics._regression.mean_pinball_loss(y_true,y_pred,*,sample_weight=None,alpha=0.5,multioutput='uniform_average')
sklearn.metrics._regression.mean_poisson_deviance(y_true,y_pred,*,sample_weight=None)
sklearn.metrics._regression.mean_squared_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',squared='deprecated')
sklearn.metrics._regression.mean_squared_log_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',squared='deprecated')
sklearn.metrics._regression.mean_tweedie_deviance(y_true,y_pred,*,sample_weight=None,power=0)
sklearn.metrics._regression.median_absolute_error(y_true,y_pred,*,multioutput='uniform_average',sample_weight=None)
sklearn.metrics._regression.r2_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',force_finite=True)
sklearn.metrics._regression.root_mean_squared_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics._regression.root_mean_squared_log_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.d2_absolute_error_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.d2_pinball_score(y_true,y_pred,*,sample_weight=None,alpha=0.5,multioutput='uniform_average')
sklearn.metrics.d2_tweedie_score(y_true,y_pred,*,sample_weight=None,power=0)
sklearn.metrics.explained_variance_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',force_finite=True)
sklearn.metrics.max_error(y_true,y_pred)
sklearn.metrics.mean_absolute_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.mean_absolute_percentage_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.mean_gamma_deviance(y_true,y_pred,*,sample_weight=None)
sklearn.metrics.mean_pinball_loss(y_true,y_pred,*,sample_weight=None,alpha=0.5,multioutput='uniform_average')
sklearn.metrics.mean_poisson_deviance(y_true,y_pred,*,sample_weight=None)
sklearn.metrics.mean_squared_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',squared='deprecated')
sklearn.metrics.mean_squared_log_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',squared='deprecated')
sklearn.metrics.mean_tweedie_deviance(y_true,y_pred,*,sample_weight=None,power=0)
sklearn.metrics.median_absolute_error(y_true,y_pred,*,multioutput='uniform_average',sample_weight=None)
sklearn.metrics.r2_score(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average',force_finite=True)
sklearn.metrics.root_mean_squared_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')
sklearn.metrics.root_mean_squared_log_error(y_true,y_pred,*,sample_weight=None,multioutput='uniform_average')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_base.py----------------------------------------
A:sklearn.metrics._base.y_type->type_of_target(y_true)
A:sklearn.metrics._base.y_true->y_true.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics._base.y_score->y_score.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.metrics._base.score_weight->numpy.repeat(score_weight, y_true.shape[1])
A:sklearn.metrics._base.average_weight->numpy.asarray(average_weight)
A:sklearn.metrics._base.score->numpy.zeros((n_classes,))
A:sklearn.metrics._base.y_true_c->y_true.reshape((-1, 1)).reshape((-1, 1)).take([c], axis=not_average_axis).ravel()
A:sklearn.metrics._base.y_score_c->y_score.reshape((-1, 1)).reshape((-1, 1)).take([c], axis=not_average_axis).ravel()
A:sklearn.metrics._base.score[c]->binary_metric(y_true_c, y_score_c, sample_weight=score_weight)
A:sklearn.metrics._base.y_true_unique->numpy.unique(y_true)
A:sklearn.metrics._base.pair_scores->numpy.empty(n_pairs)
A:sklearn.metrics._base.ab_mask->numpy.logical_or(a_mask, b_mask)
A:sklearn.metrics._base.prevalence[ix]->numpy.average(ab_mask)
A:sklearn.metrics._base.a_true_score->binary_metric(a_true, y_score[ab_mask, a])
A:sklearn.metrics._base.b_true_score->binary_metric(b_true, y_score[ab_mask, b])
sklearn.metrics._base._average_binary_score(binary_metric,y_true,y_score,average,sample_weight=None)
sklearn.metrics._base._average_multiclass_ovo_score(binary_metric,y_true,y_score,average='macro')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_scorer.py----------------------------------------
A:sklearn.metrics._scorer.(result, _)->_get_response_values(estimator, *args, response_method=response_method, **kwargs)
A:sklearn.metrics._scorer.cached_call->partial(_cached_call, cache)
A:sklearn.metrics._scorer.routed_params->Bunch(**{name: Bunch(score=kwargs) for name in self._scorers})
A:sklearn.metrics._scorer.score->scorer(estimator, *args, **routed_params.get(name).score)
A:sklearn.metrics._scorer.scores[name]->format_exc()
A:sklearn.metrics._scorer.counter->Counter([_check_response_method(estimator, scorer._response_method).__name__ for scorer in self._scorers.values() if isinstance(scorer, _BaseScorer)])
A:sklearn.metrics._scorer.kwargs_string->''.join([f', {k}={v}' for (k, v) in self._kwargs.items()])
A:sklearn.metrics._scorer._kwargs->copy.deepcopy(kwargs)
A:sklearn.metrics._scorer.overlap->copy.deepcopy(kwargs).intersection(kwargs.keys())
A:sklearn.metrics._scorer.self._metadata_request->MetadataRequest(owner=self.__class__.__name__)
A:sklearn.metrics._scorer.response_method->_get_response_method(response_method, needs_threshold, needs_proba)
A:sklearn.metrics._scorer.y_pred->method_caller(estimator, response_method.__name__, X, pos_label=pos_label)
A:sklearn.metrics._scorer.scorer->copy.deepcopy(_SCORERS[scoring])
A:sklearn.metrics._scorer.keys->set(scoring)
A:sklearn.metrics._scorer.explained_variance_scorer->make_scorer(explained_variance_score)
A:sklearn.metrics._scorer.r2_scorer->make_scorer(r2_score)
A:sklearn.metrics._scorer.max_error_scorer->make_scorer(max_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_squared_error_scorer->make_scorer(mean_squared_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_squared_log_error_scorer->make_scorer(mean_squared_log_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_absolute_error_scorer->make_scorer(mean_absolute_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_absolute_percentage_error_scorer->make_scorer(mean_absolute_percentage_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_median_absolute_error_scorer->make_scorer(median_absolute_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_root_mean_squared_error_scorer->make_scorer(root_mean_squared_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_root_mean_squared_log_error_scorer->make_scorer(root_mean_squared_log_error, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_poisson_deviance_scorer->make_scorer(mean_poisson_deviance, greater_is_better=False)
A:sklearn.metrics._scorer.neg_mean_gamma_deviance_scorer->make_scorer(mean_gamma_deviance, greater_is_better=False)
A:sklearn.metrics._scorer.accuracy_scorer->make_scorer(accuracy_score)
A:sklearn.metrics._scorer.balanced_accuracy_scorer->make_scorer(balanced_accuracy_score)
A:sklearn.metrics._scorer.matthews_corrcoef_scorer->make_scorer(matthews_corrcoef)
A:sklearn.metrics._scorer.positive_likelihood_ratio_scorer->make_scorer(positive_likelihood_ratio)
A:sklearn.metrics._scorer.neg_negative_likelihood_ratio_scorer->make_scorer(negative_likelihood_ratio, greater_is_better=False)
A:sklearn.metrics._scorer.top_k_accuracy_scorer->make_scorer(top_k_accuracy_score, greater_is_better=True, response_method=('decision_function', 'predict_proba'))
A:sklearn.metrics._scorer.roc_auc_scorer->make_scorer(roc_auc_score, greater_is_better=True, response_method=('decision_function', 'predict_proba'))
A:sklearn.metrics._scorer.average_precision_scorer->make_scorer(average_precision_score, response_method=('decision_function', 'predict_proba'))
A:sklearn.metrics._scorer.roc_auc_ovo_scorer->make_scorer(roc_auc_score, response_method='predict_proba', multi_class='ovo')
A:sklearn.metrics._scorer.roc_auc_ovo_weighted_scorer->make_scorer(roc_auc_score, response_method='predict_proba', multi_class='ovo', average='weighted')
A:sklearn.metrics._scorer.roc_auc_ovr_scorer->make_scorer(roc_auc_score, response_method='predict_proba', multi_class='ovr')
A:sklearn.metrics._scorer.roc_auc_ovr_weighted_scorer->make_scorer(roc_auc_score, response_method='predict_proba', multi_class='ovr', average='weighted')
A:sklearn.metrics._scorer.neg_log_loss_scorer->make_scorer(log_loss, greater_is_better=False, response_method='predict_proba')
A:sklearn.metrics._scorer.neg_brier_score_scorer->make_scorer(brier_score_loss, greater_is_better=False, response_method='predict_proba')
A:sklearn.metrics._scorer.brier_score_loss_scorer->make_scorer(brier_score_loss, greater_is_better=False, response_method='predict_proba')
A:sklearn.metrics._scorer.adjusted_rand_scorer->make_scorer(adjusted_rand_score)
A:sklearn.metrics._scorer.rand_scorer->make_scorer(rand_score)
A:sklearn.metrics._scorer.homogeneity_scorer->make_scorer(homogeneity_score)
A:sklearn.metrics._scorer.completeness_scorer->make_scorer(completeness_score)
A:sklearn.metrics._scorer.v_measure_scorer->make_scorer(v_measure_score)
A:sklearn.metrics._scorer.mutual_info_scorer->make_scorer(mutual_info_score)
A:sklearn.metrics._scorer.adjusted_mutual_info_scorer->make_scorer(adjusted_mutual_info_score)
A:sklearn.metrics._scorer.normalized_mutual_info_scorer->make_scorer(normalized_mutual_info_score)
A:sklearn.metrics._scorer.fowlkes_mallows_scorer->make_scorer(fowlkes_mallows_score)
A:sklearn.metrics._scorer._SCORERS->dict(explained_variance=explained_variance_scorer, r2=r2_scorer, max_error=max_error_scorer, matthews_corrcoef=matthews_corrcoef_scorer, neg_median_absolute_error=neg_median_absolute_error_scorer, neg_mean_absolute_error=neg_mean_absolute_error_scorer, neg_mean_absolute_percentage_error=neg_mean_absolute_percentage_error_scorer, neg_mean_squared_error=neg_mean_squared_error_scorer, neg_mean_squared_log_error=neg_mean_squared_log_error_scorer, neg_root_mean_squared_error=neg_root_mean_squared_error_scorer, neg_root_mean_squared_log_error=neg_root_mean_squared_log_error_scorer, neg_mean_poisson_deviance=neg_mean_poisson_deviance_scorer, neg_mean_gamma_deviance=neg_mean_gamma_deviance_scorer, accuracy=accuracy_scorer, top_k_accuracy=top_k_accuracy_scorer, roc_auc=roc_auc_scorer, roc_auc_ovr=roc_auc_ovr_scorer, roc_auc_ovo=roc_auc_ovo_scorer, roc_auc_ovr_weighted=roc_auc_ovr_weighted_scorer, roc_auc_ovo_weighted=roc_auc_ovo_weighted_scorer, balanced_accuracy=balanced_accuracy_scorer, average_precision=average_precision_scorer, neg_log_loss=neg_log_loss_scorer, neg_brier_score=neg_brier_score_scorer, positive_likelihood_ratio=positive_likelihood_ratio_scorer, neg_negative_likelihood_ratio=neg_negative_likelihood_ratio_scorer, adjusted_rand_score=adjusted_rand_scorer, rand_score=rand_scorer, homogeneity_score=homogeneity_scorer, completeness_score=completeness_scorer, v_measure_score=v_measure_scorer, mutual_info_score=mutual_info_scorer, adjusted_mutual_info_score=adjusted_mutual_info_scorer, normalized_mutual_info_score=normalized_mutual_info_scorer, fowlkes_mallows_score=fowlkes_mallows_scorer)
A:sklearn.metrics._scorer._SCORERS[name]->make_scorer(metric, average='binary')
A:sklearn.metrics._scorer.qualified_name->'{0}_{1}'.format(name, average)
A:sklearn.metrics._scorer._SCORERS[qualified_name]->make_scorer(metric, pos_label=None, average=average)
A:sklearn.metrics._scorer.module->getattr(scoring, '__module__', None)
sklearn.metrics._scorer._BaseScorer(self,score_func,sign,kwargs,response_method='predict')
sklearn.metrics._scorer._BaseScorer.__init__(self,score_func,sign,kwargs,response_method='predict')
sklearn.metrics._scorer._BaseScorer.__repr__(self)
sklearn.metrics._scorer._BaseScorer._get_pos_label(self)
sklearn.metrics._scorer._BaseScorer._warn_overlap(self,message,kwargs)
sklearn.metrics._scorer._BaseScorer.set_score_request(self,**kwargs)
sklearn.metrics._scorer._MultimetricScorer(self,*,scorers,raise_exc=True)
sklearn.metrics._scorer._MultimetricScorer.__init__(self,*,scorers,raise_exc=True)
sklearn.metrics._scorer._MultimetricScorer._use_cache(self,estimator)
sklearn.metrics._scorer._MultimetricScorer.get_metadata_routing(self)
sklearn.metrics._scorer._PassthroughScorer(self,estimator)
sklearn.metrics._scorer._PassthroughScorer.__init__(self,estimator)
sklearn.metrics._scorer._PassthroughScorer.get_metadata_routing(self)
sklearn.metrics._scorer._Scorer(_BaseScorer)
sklearn.metrics._scorer._Scorer._score(self,method_caller,estimator,X,y_true,**kwargs)
sklearn.metrics._scorer._cached_call(cache,estimator,response_method,*args,**kwargs)
sklearn.metrics._scorer._check_multimetric_scoring(estimator,scoring)
sklearn.metrics._scorer._get_response_method(response_method,needs_threshold,needs_proba)
sklearn.metrics._scorer.check_scoring(estimator,scoring=None,*,allow_none=False)
sklearn.metrics._scorer.get_scorer(scoring)
sklearn.metrics._scorer.get_scorer_names()
sklearn.metrics._scorer.make_scorer(score_func,*,response_method=None,greater_is_better=True,needs_proba='deprecated',needs_threshold='deprecated',**kwargs)
sklearn.metrics._scorer.negative_likelihood_ratio(y_true,y_pred)
sklearn.metrics._scorer.positive_likelihood_ratio(y_true,y_pred)
sklearn.metrics.check_scoring(estimator,scoring=None,*,allow_none=False)
sklearn.metrics.get_scorer(scoring)
sklearn.metrics.get_scorer_names()
sklearn.metrics.make_scorer(score_func,*,response_method=None,greater_is_better=True,needs_proba='deprecated',needs_threshold='deprecated',**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/pairwise.py----------------------------------------
A:sklearn.metrics.pairwise.X->csr_matrix(X, copy=False)
A:sklearn.metrics.pairwise.Y->csr_matrix(Y, copy=False)
A:sklearn.metrics.pairwise.(X, Y, dtype_float)->_return_float_dtype(X, Y)
A:sklearn.metrics.pairwise.XY->check_array(X, accept_sparse=accept_sparse, dtype=dtype, copy=copy, force_all_finite=force_all_finite, estimator=estimator)
A:sklearn.metrics.pairwise.(X, Y)->check_pairwise_arrays(X, Y, dtype=dtype, force_all_finite=force_all_finite)
A:sklearn.metrics.pairwise.X_norm_squared->X_norm_squared.reshape(-1, 1).reshape(-1, 1)
A:sklearn.metrics.pairwise.Y_norm_squared->Y_norm_squared.reshape(1, -1).reshape(1, -1)
A:sklearn.metrics.pairwise.XX->X_norm_squared.reshape(-1, 1).reshape(-1, 1).reshape(-1, 1)
A:sklearn.metrics.pairwise.YY->Y_norm_squared.reshape(1, -1).reshape(1, -1).reshape(1, -1)
A:sklearn.metrics.pairwise.distances->numpy.zeros(len(X))
A:sklearn.metrics.pairwise.missing_X->_get_mask(X, missing_values)
A:sklearn.metrics.pairwise.present_count->numpy.dot(present_X, present_Y.T)
A:sklearn.metrics.pairwise.maxmem->max(((x_density * n_samples_X + y_density * n_samples_Y) * n_features + x_density * n_samples_X * y_density * n_samples_Y) / 10, 10 * 2 ** 17)
A:sklearn.metrics.pairwise.batch_size->max(int(batch_size), 1)
A:sklearn.metrics.pairwise.x_batches->gen_batches(n_samples_X, batch_size)
A:sklearn.metrics.pairwise.X_chunk->X[x_slice].astype(np.float64)
A:sklearn.metrics.pairwise.y_batches->gen_batches(n_samples_Y, batch_size)
A:sklearn.metrics.pairwise.Y_chunk->Y[y_slice].astype(np.float64)
A:sklearn.metrics.pairwise.distances[x_slice, y_slice]->d.astype(np.float32, copy=False)
A:sklearn.metrics.pairwise.indices->numpy.concatenate(list(pairwise_distances_chunked(X, Y, reduce_func=_argmin_reduce, metric=metric, **metric_kwargs)))
A:sklearn.metrics.pairwise.(values, indices)->_pairwise_distances_reduction.ArgKmin.compute(X=X, Y=Y, k=1, metric=metric, metric_kwargs=metric_kwargs, strategy='auto', return_distance=True)
A:sklearn.metrics.pairwise.values->numpy.concatenate(values)
A:sklearn.metrics.pairwise.(indices, values)->zip(*pairwise_distances_chunked(X, Y, reduce_func=_argmin_min_reduce, metric=metric, **metric_kwargs))
A:sklearn.metrics.pairwise.D->numpy.zeros((X.shape[0], Y.shape[0]))
A:sklearn.metrics.pairwise.S->cosine_similarity(X, Y)
A:sklearn.metrics.pairwise.diff.data->numpy.abs(diff.data)
A:sklearn.metrics.pairwise.distances[i]->metric(X[i], Y[i])
A:sklearn.metrics.pairwise.K->additive_chi2_kernel(X, Y)
A:sklearn.metrics.pairwise.X_normalized->normalize(X, copy=True)
A:sklearn.metrics.pairwise.Y_normalized->normalize(Y, copy=True)
A:sklearn.metrics.pairwise.result->numpy.zeros((X.shape[0], Y.shape[0]), dtype=X.dtype)
A:sklearn.metrics.pairwise.dist_matrix[:, slice_]->dist_func(*args, **kwargs)
A:sklearn.metrics.pairwise.(X, Y, dtype)->_return_float_dtype(X, Y)
A:sklearn.metrics.pairwise.fd->delayed(_dist_wrapper)
A:sklearn.metrics.pairwise.ret->numpy.empty((X.shape[0], Y.shape[0]), dtype=dtype, order='F')
A:sklearn.metrics.pairwise.out->numpy.empty((X.shape[0], Y.shape[0]), dtype='float')
A:sklearn.metrics.pairwise.iterator->itertools.product(range(X.shape[0]), range(Y.shape[0]))
A:sklearn.metrics.pairwise.out[i, j]->metric(x, y, **kwds)
A:sklearn.metrics.pairwise.out[i, i]->metric(x, x, **kwds)
A:sklearn.metrics.pairwise.is_tuple->isinstance(reduced, tuple)
A:sklearn.metrics.pairwise.actual_size->tuple((_num_samples(r) for r in reduced))
A:sklearn.metrics.pairwise.V->numpy.var(X, axis=0, ddof=1)
A:sklearn.metrics.pairwise.n_samples_X->_num_samples(X)
A:sklearn.metrics.pairwise.chunk_n_rows->get_chunk_n_rows(row_bytes=8 * _num_samples(Y), max_n_rows=n_samples_X, working_memory=working_memory)
A:sklearn.metrics.pairwise.slices->gen_batches(n_samples_X, chunk_n_rows)
A:sklearn.metrics.pairwise.params->_precompute_metric_params(X, Y, metric=metric, **kwds)
A:sklearn.metrics.pairwise.D_chunk->reduce_func(D_chunk, sl.start)
A:sklearn.metrics.pairwise.(X, _)->check_pairwise_arrays(X, Y, precomputed=True)
A:sklearn.metrics.pairwise.func->partial(_pairwise_callable, metric=metric, **kwds)
sklearn.metrics.euclidean_distances(X,Y=None,*,Y_norm_squared=None,squared=False,X_norm_squared=None)
sklearn.metrics.nan_euclidean_distances(X,Y=None,*,squared=False,missing_values=np.nan,copy=True)
sklearn.metrics.pairwise._argmin_min_reduce(dist,start)
sklearn.metrics.pairwise._argmin_reduce(dist,start)
sklearn.metrics.pairwise._check_chunk_size(reduced,chunk_size)
sklearn.metrics.pairwise._dist_wrapper(dist_func,dist_matrix,slice_,*args,**kwargs)
sklearn.metrics.pairwise._euclidean_distances(X,Y,X_norm_squared=None,Y_norm_squared=None,squared=False)
sklearn.metrics.pairwise._euclidean_distances_upcast(X,XX=None,Y=None,YY=None,batch_size=None)
sklearn.metrics.pairwise._pairwise_callable(X,Y,metric,force_all_finite=True,**kwds)
sklearn.metrics.pairwise._parallel_pairwise(X,Y,func,n_jobs,**kwds)
sklearn.metrics.pairwise._precompute_metric_params(X,Y,metric=None,**kwds)
sklearn.metrics.pairwise._return_float_dtype(X,Y)
sklearn.metrics.pairwise.additive_chi2_kernel(X,Y=None)
sklearn.metrics.pairwise.check_paired_arrays(X,Y)
sklearn.metrics.pairwise.check_pairwise_arrays(X,Y,*,precomputed=False,dtype=None,accept_sparse='csr',force_all_finite=True,copy=False)
sklearn.metrics.pairwise.chi2_kernel(X,Y=None,gamma=1.0)
sklearn.metrics.pairwise.cosine_distances(X,Y=None)
sklearn.metrics.pairwise.cosine_similarity(X,Y=None,dense_output=True)
sklearn.metrics.pairwise.distance_metrics()
sklearn.metrics.pairwise.euclidean_distances(X,Y=None,*,Y_norm_squared=None,squared=False,X_norm_squared=None)
sklearn.metrics.pairwise.haversine_distances(X,Y=None)
sklearn.metrics.pairwise.kernel_metrics()
sklearn.metrics.pairwise.laplacian_kernel(X,Y=None,gamma=None)
sklearn.metrics.pairwise.linear_kernel(X,Y=None,dense_output=True)
sklearn.metrics.pairwise.manhattan_distances(X,Y=None)
sklearn.metrics.pairwise.nan_euclidean_distances(X,Y=None,*,squared=False,missing_values=np.nan,copy=True)
sklearn.metrics.pairwise.paired_cosine_distances(X,Y)
sklearn.metrics.pairwise.paired_distances(X,Y,*,metric='euclidean',**kwds)
sklearn.metrics.pairwise.paired_euclidean_distances(X,Y)
sklearn.metrics.pairwise.paired_manhattan_distances(X,Y)
sklearn.metrics.pairwise.pairwise_distances(X,Y=None,metric='euclidean',*,n_jobs=None,force_all_finite=True,**kwds)
sklearn.metrics.pairwise.pairwise_distances_argmin(X,Y,*,axis=1,metric='euclidean',metric_kwargs=None)
sklearn.metrics.pairwise.pairwise_distances_argmin_min(X,Y,*,axis=1,metric='euclidean',metric_kwargs=None)
sklearn.metrics.pairwise.pairwise_distances_chunked(X,Y=None,*,reduce_func=None,metric='euclidean',n_jobs=None,working_memory=None,**kwds)
sklearn.metrics.pairwise.pairwise_kernels(X,Y=None,metric='linear',*,filter_params=False,n_jobs=None,**kwds)
sklearn.metrics.pairwise.polynomial_kernel(X,Y=None,degree=3,gamma=None,coef0=1)
sklearn.metrics.pairwise.rbf_kernel(X,Y=None,gamma=None)
sklearn.metrics.pairwise.sigmoid_kernel(X,Y=None,gamma=None,coef0=1)
sklearn.metrics.pairwise_distances(X,Y=None,metric='euclidean',*,n_jobs=None,force_all_finite=True,**kwds)
sklearn.metrics.pairwise_distances_argmin(X,Y,*,axis=1,metric='euclidean',metric_kwargs=None)
sklearn.metrics.pairwise_distances_argmin_min(X,Y,*,axis=1,metric='euclidean',metric_kwargs=None)
sklearn.metrics.pairwise_distances_chunked(X,Y=None,*,reduce_func=None,metric='euclidean',n_jobs=None,working_memory=None,**kwds)
sklearn.metrics.pairwise_kernels(X,Y=None,metric='linear',*,filter_params=False,n_jobs=None,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/confusion_matrix.py----------------------------------------
A:sklearn.metrics._plot.confusion_matrix.(fig, ax)->matplotlib.pyplot.subplots()
A:sklearn.metrics._plot.confusion_matrix.default_im_kw->dict(interpolation='nearest', cmap=cmap)
A:sklearn.metrics._plot.confusion_matrix.self.im_->ax.imshow(cm, **im_kw)
A:sklearn.metrics._plot.confusion_matrix.self.text_->numpy.empty_like(cm, dtype=object)
A:sklearn.metrics._plot.confusion_matrix.text_cm->format(cm[i, j], values_format)
A:sklearn.metrics._plot.confusion_matrix.text_d->format(cm[i, j], 'd')
A:sklearn.metrics._plot.confusion_matrix.default_text_kwargs->dict(ha='center', va='center', color=color)
A:sklearn.metrics._plot.confusion_matrix.self.text_[i, j]->ax.text(j, i, text_cm, **text_kwargs)
A:sklearn.metrics._plot.confusion_matrix.display_labels->unique_labels(y_true, y_pred)
A:sklearn.metrics._plot.confusion_matrix.y_pred->estimator.predict(X)
A:sklearn.metrics._plot.confusion_matrix.cm->confusion_matrix(y_true, y_pred, sample_weight=sample_weight, labels=labels, normalize=normalize)
A:sklearn.metrics._plot.confusion_matrix.disp->cls(confusion_matrix=cm, display_labels=display_labels)
sklearn.metrics.ConfusionMatrixDisplay(self,confusion_matrix,*,display_labels=None)
sklearn.metrics.ConfusionMatrixDisplay.from_estimator(cls,estimator,X,y,*,labels=None,sample_weight=None,normalize=None,display_labels=None,include_values=True,xticks_rotation='horizontal',values_format=None,cmap='viridis',ax=None,colorbar=True,im_kw=None,text_kw=None)
sklearn.metrics.ConfusionMatrixDisplay.from_predictions(cls,y_true,y_pred,*,labels=None,sample_weight=None,normalize=None,display_labels=None,include_values=True,xticks_rotation='horizontal',values_format=None,cmap='viridis',ax=None,colorbar=True,im_kw=None,text_kw=None)
sklearn.metrics.ConfusionMatrixDisplay.plot(self,*,include_values=True,cmap='viridis',xticks_rotation='horizontal',values_format=None,ax=None,colorbar=True,im_kw=None,text_kw=None)
sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay(self,confusion_matrix,*,display_labels=None)
sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay.__init__(self,confusion_matrix,*,display_labels=None)
sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay.from_estimator(cls,estimator,X,y,*,labels=None,sample_weight=None,normalize=None,display_labels=None,include_values=True,xticks_rotation='horizontal',values_format=None,cmap='viridis',ax=None,colorbar=True,im_kw=None,text_kw=None)
sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay.from_predictions(cls,y_true,y_pred,*,labels=None,sample_weight=None,normalize=None,display_labels=None,include_values=True,xticks_rotation='horizontal',values_format=None,cmap='viridis',ax=None,colorbar=True,im_kw=None,text_kw=None)
sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay.plot(self,*,include_values=True,cmap='viridis',xticks_rotation='horizontal',values_format=None,ax=None,colorbar=True,im_kw=None,text_kw=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/roc_curve.py----------------------------------------
A:sklearn.metrics._plot.roc_curve.(self.ax_, self.figure_, name)->self._validate_plot_params(ax=ax, name=name)
A:sklearn.metrics._plot.roc_curve.(self.line_,)->self.ax_.plot(self.fpr, self.tpr, **line_kwargs)
A:sklearn.metrics._plot.roc_curve.(self.chance_level_,)->self.ax_.plot((0, 1), (0, 1), **chance_level_line_kw)
A:sklearn.metrics._plot.roc_curve.(y_pred, pos_label, name)->cls._validate_and_get_response_values(estimator, X, y, response_method=response_method, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.roc_curve.(pos_label_validated, name)->cls._validate_from_predictions_params(y_true, y_pred, sample_weight=sample_weight, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.roc_curve.(fpr, tpr, _)->roc_curve(y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight, drop_intermediate=drop_intermediate)
A:sklearn.metrics._plot.roc_curve.roc_auc->auc(fpr, tpr)
A:sklearn.metrics._plot.roc_curve.viz->cls(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name, pos_label=pos_label_validated)
sklearn.metrics.RocCurveDisplay(self,*,fpr,tpr,roc_auc=None,estimator_name=None,pos_label=None)
sklearn.metrics.RocCurveDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,drop_intermediate=True,response_method='auto',pos_label=None,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics.RocCurveDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,drop_intermediate=True,pos_label=None,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics.RocCurveDisplay.plot(self,ax=None,*,name=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.roc_curve.RocCurveDisplay(self,*,fpr,tpr,roc_auc=None,estimator_name=None,pos_label=None)
sklearn.metrics._plot.roc_curve.RocCurveDisplay.__init__(self,*,fpr,tpr,roc_auc=None,estimator_name=None,pos_label=None)
sklearn.metrics._plot.roc_curve.RocCurveDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,drop_intermediate=True,response_method='auto',pos_label=None,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.roc_curve.RocCurveDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,drop_intermediate=True,pos_label=None,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.roc_curve.RocCurveDisplay.plot(self,ax=None,*,name=None,plot_chance_level=False,chance_level_kw=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/det_curve.py----------------------------------------
A:sklearn.metrics._plot.det_curve.(y_pred, pos_label, name)->cls._validate_and_get_response_values(estimator, X, y, response_method=response_method, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.det_curve.(pos_label_validated, name)->cls._validate_from_predictions_params(y_true, y_pred, sample_weight=sample_weight, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.det_curve.(fpr, fnr, _)->det_curve(y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight)
A:sklearn.metrics._plot.det_curve.viz->cls(fpr=fpr, fnr=fnr, estimator_name=name, pos_label=pos_label_validated)
A:sklearn.metrics._plot.det_curve.(self.ax_, self.figure_, name)->self._validate_plot_params(ax=ax, name=name)
A:sklearn.metrics._plot.det_curve.(self.line_,)->self.ax_.plot(sp.stats.norm.ppf(self.fpr), sp.stats.norm.ppf(self.fnr), **line_kwargs)
A:sklearn.metrics._plot.det_curve.tick_locations->scipy.stats.norm.ppf(ticks)
sklearn.metrics.DetCurveDisplay(self,*,fpr,fnr,estimator_name=None,pos_label=None)
sklearn.metrics.DetCurveDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,response_method='auto',pos_label=None,name=None,ax=None,**kwargs)
sklearn.metrics.DetCurveDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,pos_label=None,name=None,ax=None,**kwargs)
sklearn.metrics.DetCurveDisplay.plot(self,ax=None,*,name=None,**kwargs)
sklearn.metrics._plot.det_curve.DetCurveDisplay(self,*,fpr,fnr,estimator_name=None,pos_label=None)
sklearn.metrics._plot.det_curve.DetCurveDisplay.__init__(self,*,fpr,fnr,estimator_name=None,pos_label=None)
sklearn.metrics._plot.det_curve.DetCurveDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,response_method='auto',pos_label=None,name=None,ax=None,**kwargs)
sklearn.metrics._plot.det_curve.DetCurveDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,pos_label=None,name=None,ax=None,**kwargs)
sklearn.metrics._plot.det_curve.DetCurveDisplay.plot(self,ax=None,*,name=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/precision_recall_curve.py----------------------------------------
A:sklearn.metrics._plot.precision_recall_curve.(self.ax_, self.figure_, name)->self._validate_plot_params(ax=ax, name=name)
A:sklearn.metrics._plot.precision_recall_curve.(self.line_,)->self.ax_.plot(self.recall, self.precision, **line_kwargs)
A:sklearn.metrics._plot.precision_recall_curve.(self.chance_level_,)->self.ax_.plot((0, 1), (self.prevalence_pos_label, self.prevalence_pos_label), **chance_level_line_kw)
A:sklearn.metrics._plot.precision_recall_curve.(y_pred, pos_label, name)->cls._validate_and_get_response_values(estimator, X, y, response_method=response_method, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.precision_recall_curve.(pos_label, name)->cls._validate_from_predictions_params(y_true, y_pred, sample_weight=sample_weight, pos_label=pos_label, name=name)
A:sklearn.metrics._plot.precision_recall_curve.(precision, recall, _)->precision_recall_curve(y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight, drop_intermediate=drop_intermediate)
A:sklearn.metrics._plot.precision_recall_curve.average_precision->average_precision_score(y_true, y_pred, pos_label=pos_label, sample_weight=sample_weight)
A:sklearn.metrics._plot.precision_recall_curve.class_count->Counter(y_true)
A:sklearn.metrics._plot.precision_recall_curve.viz->cls(precision=precision, recall=recall, average_precision=average_precision, estimator_name=name, pos_label=pos_label, prevalence_pos_label=prevalence_pos_label)
sklearn.metrics.PrecisionRecallDisplay(self,precision,recall,*,average_precision=None,estimator_name=None,pos_label=None,prevalence_pos_label=None)
sklearn.metrics.PrecisionRecallDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,pos_label=None,drop_intermediate=False,response_method='auto',name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics.PrecisionRecallDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,pos_label=None,drop_intermediate=False,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics.PrecisionRecallDisplay.plot(self,ax=None,*,name=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay(self,precision,recall,*,average_precision=None,estimator_name=None,pos_label=None,prevalence_pos_label=None)
sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay.__init__(self,precision,recall,*,average_precision=None,estimator_name=None,pos_label=None,prevalence_pos_label=None)
sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay.from_estimator(cls,estimator,X,y,*,sample_weight=None,pos_label=None,drop_intermediate=False,response_method='auto',name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay.from_predictions(cls,y_true,y_pred,*,sample_weight=None,pos_label=None,drop_intermediate=False,name=None,ax=None,plot_chance_level=False,chance_level_kw=None,**kwargs)
sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay.plot(self,ax=None,*,name=None,plot_chance_level=False,chance_level_kw=None,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/regression.py----------------------------------------
A:sklearn.metrics._plot.regression.(_, ax)->matplotlib.pyplot.subplots()
A:sklearn.metrics._plot.regression.max_value->max(np.max(self.y_true), np.max(self.y_pred))
A:sklearn.metrics._plot.regression.min_value->min(np.min(self.y_true), np.min(self.y_pred))
A:sklearn.metrics._plot.regression.self.scatter_->ax.scatter(self.y_pred, self.y_true - self.y_pred, **scatter_kwargs)
A:sklearn.metrics._plot.regression.y_pred->_safe_indexing(y_pred, indices, axis=0)
A:sklearn.metrics._plot.regression.random_state->check_random_state(random_state)
A:sklearn.metrics._plot.regression.n_samples->len(y_true)
A:sklearn.metrics._plot.regression.subsample->int(n_samples * subsample)
A:sklearn.metrics._plot.regression.indices->check_random_state(random_state).choice(np.arange(n_samples), size=subsample)
A:sklearn.metrics._plot.regression.y_true->_safe_indexing(y_true, indices, axis=0)
A:sklearn.metrics._plot.regression.viz->cls(y_true=y_true, y_pred=y_pred)
sklearn.metrics.PredictionErrorDisplay(self,*,y_true,y_pred)
sklearn.metrics.PredictionErrorDisplay.from_estimator(cls,estimator,X,y,*,kind='residual_vs_predicted',subsample=1000,random_state=None,ax=None,scatter_kwargs=None,line_kwargs=None)
sklearn.metrics.PredictionErrorDisplay.from_predictions(cls,y_true,y_pred,*,kind='residual_vs_predicted',subsample=1000,random_state=None,ax=None,scatter_kwargs=None,line_kwargs=None)
sklearn.metrics.PredictionErrorDisplay.plot(self,ax=None,*,kind='residual_vs_predicted',scatter_kwargs=None,line_kwargs=None)
sklearn.metrics._plot.regression.PredictionErrorDisplay(self,*,y_true,y_pred)
sklearn.metrics._plot.regression.PredictionErrorDisplay.__init__(self,*,y_true,y_pred)
sklearn.metrics._plot.regression.PredictionErrorDisplay.from_estimator(cls,estimator,X,y,*,kind='residual_vs_predicted',subsample=1000,random_state=None,ax=None,scatter_kwargs=None,line_kwargs=None)
sklearn.metrics._plot.regression.PredictionErrorDisplay.from_predictions(cls,y_true,y_pred,*,kind='residual_vs_predicted',subsample=1000,random_state=None,ax=None,scatter_kwargs=None,line_kwargs=None)
sklearn.metrics._plot.regression.PredictionErrorDisplay.plot(self,ax=None,*,kind='residual_vs_predicted',scatter_kwargs=None,line_kwargs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_predict_error_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_predict_error_display.(X, y)->load_diabetes(return_X_y=True)
A:sklearn.metrics._plot.tests.test_predict_error_display.y_pred->regressor_fitted.predict(X)
A:sklearn.metrics._plot.tests.test_predict_error_display.regressor->Ridge()
A:sklearn.metrics._plot.tests.test_predict_error_display.display->sklearn.metrics.PredictionErrorDisplay.from_predictions(y_true=y, y_pred=y_pred)
A:sklearn.metrics._plot.tests.test_predict_error_display.(_, ax)->pyplot.subplots()
sklearn.metrics._plot.tests.test_predict_error_display.regressor_fitted()
sklearn.metrics._plot.tests.test_predict_error_display.test_from_estimator_not_fitted(pyplot)
sklearn.metrics._plot.tests.test_predict_error_display.test_plot_prediction_error_ax(pyplot,regressor_fitted,class_method)
sklearn.metrics._plot.tests.test_predict_error_display.test_plot_prediction_error_subsample(pyplot,regressor_fitted,class_method,subsample,expected_size)
sklearn.metrics._plot.tests.test_predict_error_display.test_prediction_error_custom_artist(pyplot,regressor_fitted,class_method)
sklearn.metrics._plot.tests.test_predict_error_display.test_prediction_error_display(pyplot,regressor_fitted,class_method,kind)
sklearn.metrics._plot.tests.test_predict_error_display.test_prediction_error_display_raise_error(pyplot,class_method,regressor,params,err_type,err_msg)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_precision_recall_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_precision_recall_display.pytestmark->pytest.mark.filterwarnings("ignore:In future, it will be an error for 'np.bool_':DeprecationWarning:matplotlib.*")
A:sklearn.metrics._plot.tests.test_precision_recall_display.(X, y)->make_classification(n_classes=2, n_samples=50, random_state=0)
A:sklearn.metrics._plot.tests.test_precision_recall_display.classifier->LogisticRegression()
A:sklearn.metrics._plot.tests.test_precision_recall_display.y_pred->getattr(classifier, response_method)(X_test)
A:sklearn.metrics._plot.tests.test_precision_recall_display.display->PrecisionRecallDisplay(precision, recall)
A:sklearn.metrics._plot.tests.test_precision_recall_display.(precision, recall, _)->precision_recall_curve(y, y_pred, pos_label=pos_label, drop_intermediate=drop_intermediate)
A:sklearn.metrics._plot.tests.test_precision_recall_display.average_precision->average_precision_score(y, y_pred, pos_label=pos_label)
A:sklearn.metrics._plot.tests.test_precision_recall_display.lr->LogisticRegression()
A:sklearn.metrics._plot.tests.test_precision_recall_display.cancer->load_breast_cancer()
A:sklearn.metrics._plot.tests.test_precision_recall_display.avg_prec->average_precision_score(y, y_pred, pos_label=lr.classes_[1])
A:sklearn.metrics._plot.tests.test_precision_recall_display.precision->numpy.array([1, 0.5, 0])
A:sklearn.metrics._plot.tests.test_precision_recall_display.recall->numpy.array([0, 0.5, 1])
A:sklearn.metrics._plot.tests.test_precision_recall_display.idx_positive->numpy.flatnonzero(y == 1)
A:sklearn.metrics._plot.tests.test_precision_recall_display.idx_negative->numpy.flatnonzero(y == 0)
A:sklearn.metrics._plot.tests.test_precision_recall_display.idx_selected->numpy.hstack([idx_negative, idx_positive[:25]])
A:sklearn.metrics._plot.tests.test_precision_recall_display.y->numpy.array(['cancer' if c == 1 else 'not cancer' for c in y], dtype=object)
A:sklearn.metrics._plot.tests.test_precision_recall_display.(X_train, X_test, y_train, y_test)->train_test_split(X, y, stratify=y, random_state=0)
sklearn.metrics._plot.tests.test_precision_recall_display.test_default_labels(pyplot,average_precision,estimator_name,expected_label)
sklearn.metrics._plot.tests.test_precision_recall_display.test_plot_precision_recall_pos_label(pyplot,constructor_name,response_method)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_chance_level_line(pyplot,chance_level_kw,constructor_name)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_display_name(pyplot,constructor_name,default_label)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_display_pipeline(pyplot,clf)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_display_plotting(pyplot,constructor_name,response_method,drop_intermediate)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_display_string_labels(pyplot)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_prevalence_pos_label_reusable(pyplot,constructor_name)
sklearn.metrics._plot.tests.test_precision_recall_display.test_precision_recall_raise_no_prevalence(pyplot)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_roc_curve_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_roc_curve_display.rng->numpy.random.RandomState(42)
A:sklearn.metrics._plot.tests.test_roc_curve_display.sample_weight->numpy.random.RandomState(42).randint(1, 4, size=X.shape[0])
A:sklearn.metrics._plot.tests.test_roc_curve_display.lr->LogisticRegression()
A:sklearn.metrics._plot.tests.test_roc_curve_display.y_pred->getattr(classifier, response_method)(X_test)
A:sklearn.metrics._plot.tests.test_roc_curve_display.display->sklearn.metrics.RocCurveDisplay.from_predictions(y_test, y_pred_not_cancer, pos_label='not cancer')
A:sklearn.metrics._plot.tests.test_roc_curve_display.(fpr, tpr, _)->roc_curve(y, y_pred, sample_weight=sample_weight, drop_intermediate=drop_intermediate, pos_label=pos_label)
A:sklearn.metrics._plot.tests.test_roc_curve_display.fpr->numpy.array([0, 0.5, 1])
A:sklearn.metrics._plot.tests.test_roc_curve_display.tpr->numpy.array([0, 0.5, 1])
A:sklearn.metrics._plot.tests.test_roc_curve_display.disp->RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=estimator_name).plot()
A:sklearn.metrics._plot.tests.test_roc_curve_display.(X, y)->shuffle(X, y, random_state=42)
A:sklearn.metrics._plot.tests.test_roc_curve_display.idx_positive->numpy.flatnonzero(y == 1)
A:sklearn.metrics._plot.tests.test_roc_curve_display.idx_negative->numpy.flatnonzero(y == 0)
A:sklearn.metrics._plot.tests.test_roc_curve_display.idx_selected->numpy.hstack([idx_negative, idx_positive[:25]])
A:sklearn.metrics._plot.tests.test_roc_curve_display.y->numpy.array(['cancer' if c == 1 else 'not cancer' for c in y], dtype=object)
A:sklearn.metrics._plot.tests.test_roc_curve_display.(X_train, X_test, y_train, y_test)->train_test_split(X, y, stratify=y, random_state=0)
A:sklearn.metrics._plot.tests.test_roc_curve_display.classifier->LogisticRegression()
sklearn.metrics._plot.tests.test_roc_curve_display.data()
sklearn.metrics._plot.tests.test_roc_curve_display.data_binary(data)
sklearn.metrics._plot.tests.test_roc_curve_display.test_plot_roc_curve_pos_label(pyplot,response_method,constructor_name)
sklearn.metrics._plot.tests.test_roc_curve_display.test_roc_curve_chance_level_line(pyplot,data_binary,plot_chance_level,chance_level_kw,constructor_name)
sklearn.metrics._plot.tests.test_roc_curve_display.test_roc_curve_display_complex_pipeline(pyplot,data_binary,clf,constructor_name)
sklearn.metrics._plot.tests.test_roc_curve_display.test_roc_curve_display_default_labels(pyplot,roc_auc,estimator_name,expected_label)
sklearn.metrics._plot.tests.test_roc_curve_display.test_roc_curve_display_plotting(pyplot,response_method,data_binary,with_sample_weight,drop_intermediate,with_strings,constructor_name,default_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_common_curve_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_common_curve_display.clf->LogisticRegression().fit(X, y)
A:sklearn.metrics._plot.tests.test_common_curve_display.regressor->DecisionTreeRegressor().fit(X, y)
A:sklearn.metrics._plot.tests.test_common_curve_display.classifier->LogisticRegression().fit(X, y)
A:sklearn.metrics._plot.tests.test_common_curve_display.disp->Display.from_estimator(model, X, y)
A:sklearn.metrics._plot.tests.test_common_curve_display.model->clone(clf)
A:sklearn.metrics._plot.tests.test_common_curve_display.curve->SubclassOfDisplay.from_estimator(classifier, X, y)
sklearn.metrics._plot.tests.test_common_curve_display.data()
sklearn.metrics._plot.tests.test_common_curve_display.data_binary(data)
sklearn.metrics._plot.tests.test_common_curve_display.test_classifier_display_curve_named_constructor_return_type(pyplot,data_binary,Display,constructor)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_error_classifier(pyplot,data,data_binary,Display)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_error_no_response(pyplot,data_binary,response_method,msg,Display)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_error_pos_label(pyplot,data_binary,Display)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_error_regression(pyplot,data_binary,Display)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_estimator_name_multiple_calls(pyplot,data_binary,Display,constructor_name)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_n_samples_consistency(pyplot,data_binary,Display)
sklearn.metrics._plot.tests.test_common_curve_display.test_display_curve_not_fitted_errors(pyplot,data_binary,clf,Display)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_confusion_matrix_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.pytestmark->pytest.mark.filterwarnings("ignore:In future, it will be an error for 'np.bool_':DeprecationWarning:matplotlib.*")
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.(X, y)->make_classification(random_state=0)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.regressor->SVR().fit(X, y)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.y_pred_regressor->SVR().fit(X, y).predict(X)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.y_pred_classifier->SVC().fit(X, y).predict(X)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.classifier->SVC().fit(X, y)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.y_pred->SVC().fit(X, y).predict(X)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.ax->pyplot.gca()
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.cm->confusion_matrix(y, y_pred)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.disp->sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y, y_pred, text_kw={'fontsize': font_size})
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.expected_display_labels->list(range(n_classes))
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.expected_text->numpy.array([format(v, 'e') for v in cm.ravel(order='C')])
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.text_text->numpy.array([t.get_text() for t in disp.text_.ravel(order='C')])
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.min_color->pyplot.cm.Blues(0)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.max_color->pyplot.cm.Blues(255)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.gray->pyplot.get_cmap('gray', 1024)
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.confusion_matrix->numpy.array([[0.48, 0.04], [0.08, 0.4]])
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.color->sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y, y_pred, text_kw={'fontsize': font_size}).text_[1, 0].get_color()
A:sklearn.metrics._plot.tests.test_confusion_matrix_display.clim->sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y, y_pred, text_kw={'fontsize': font_size}).im_.get_clim()
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_colormap_max(pyplot)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_contrast(pyplot)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_display(pyplot,constructor_name)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_display_custom_labels(pyplot,constructor_name,with_labels,with_display_labels)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_display_plotting(pyplot,constructor_name,normalize,include_values)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_display_validation(pyplot)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_pipeline(pyplot,clf)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_text_kw(pyplot)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_confusion_matrix_with_unknown_labels(pyplot,constructor_name)
sklearn.metrics._plot.tests.test_confusion_matrix_display.test_im_kw_adjust_vmin_vmax(pyplot)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_plot/tests/test_det_curve_display.py----------------------------------------
A:sklearn.metrics._plot.tests.test_det_curve_display.(X, y)->load_iris(return_X_y=True)
A:sklearn.metrics._plot.tests.test_det_curve_display.rng->numpy.random.RandomState(42)
A:sklearn.metrics._plot.tests.test_det_curve_display.sample_weight->numpy.random.RandomState(42).randint(1, 4, size=X.shape[0])
A:sklearn.metrics._plot.tests.test_det_curve_display.lr->LogisticRegression().fit(X, y)
A:sklearn.metrics._plot.tests.test_det_curve_display.y_pred->getattr(lr, response_method)(X)
A:sklearn.metrics._plot.tests.test_det_curve_display.disp->sklearn.metrics.DetCurveDisplay.from_predictions(y, y_pred)
A:sklearn.metrics._plot.tests.test_det_curve_display.(fpr, fnr, _)->det_curve(y, y_pred, sample_weight=sample_weight, pos_label=pos_label)
sklearn.metrics._plot.tests.test_det_curve_display.test_det_curve_display(pyplot,constructor_name,response_method,with_sample_weight,with_strings)
sklearn.metrics._plot.tests.test_det_curve_display.test_det_curve_display_default_name(pyplot,constructor_name,expected_clf_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/_bicluster.py----------------------------------------
A:sklearn.metrics.cluster._bicluster.(a_rows, a_cols)->map(checks, a)
A:sklearn.metrics.cluster._bicluster.(b_rows, b_cols)->map(checks, b)
A:sklearn.metrics.cluster._bicluster.(a_rows, a_cols, b_rows, b_cols)->_check_rows_and_columns(a, b)
A:sklearn.metrics.cluster._bicluster.result->numpy.array([[similarity(a_rows[i], a_cols[i], b_rows[j], b_cols[j]) for j in range(n_b)] for i in range(n_a)])
A:sklearn.metrics.cluster._bicluster.matrix->_pairwise_similarity(a, b, similarity)
A:sklearn.metrics.cluster._bicluster.(row_indices, col_indices)->linear_sum_assignment(1.0 - matrix)
A:sklearn.metrics.cluster._bicluster.n_a->len(a[0])
A:sklearn.metrics.cluster._bicluster.n_b->len(b[0])
sklearn.metrics.cluster._bicluster._check_rows_and_columns(a,b)
sklearn.metrics.cluster._bicluster._jaccard(a_rows,a_cols,b_rows,b_cols)
sklearn.metrics.cluster._bicluster._pairwise_similarity(a,b,similarity)
sklearn.metrics.cluster._bicluster.consensus_score(a,b,*,similarity='jaccard')
sklearn.metrics.consensus_score(a,b,*,similarity='jaccard')


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/_supervised.py----------------------------------------
A:sklearn.metrics.cluster._supervised.labels_true->check_array(labels_true, ensure_2d=False, ensure_min_samples=0, dtype=None)
A:sklearn.metrics.cluster._supervised.labels_pred->check_array(labels_pred, ensure_2d=False, ensure_min_samples=0, dtype=None)
A:sklearn.metrics.cluster._supervised.type_label->type_of_target(labels_true)
A:sklearn.metrics.cluster._supervised.type_pred->type_of_target(labels_pred)
A:sklearn.metrics.cluster._supervised.(classes, class_idx)->numpy.unique(labels_true, return_inverse=True)
A:sklearn.metrics.cluster._supervised.(clusters, cluster_idx)->numpy.unique(labels_pred, return_inverse=True)
A:sklearn.metrics.cluster._supervised.contingency->contingency.astype(np.float64, copy=False).astype(np.float64, copy=False)
A:sklearn.metrics.cluster._supervised.(labels_true, labels_pred)->check_clusterings(labels_true, labels_pred)
A:sklearn.metrics.cluster._supervised.n_samples->numpy.int64(labels_true.shape[0])
A:sklearn.metrics.cluster._supervised.n_c->numpy.ravel(contingency.sum(axis=1))
A:sklearn.metrics.cluster._supervised.n_k->numpy.ravel(contingency.sum(axis=0))
A:sklearn.metrics.cluster._supervised.sum_squares->(contingency.data ** 2).sum()
A:sklearn.metrics.cluster._supervised.C->numpy.empty((2, 2), dtype=np.int64)
A:sklearn.metrics.cluster._supervised.numerator->contingency.astype(np.float64, copy=False).astype(np.float64, copy=False).diagonal().sum()
A:sklearn.metrics.cluster._supervised.denominator->max(denominator, np.finfo('float64').eps)
A:sklearn.metrics.cluster._supervised.((tn, fp), (fn, tp))->pair_confusion_matrix(labels_true, labels_pred)
A:sklearn.metrics.cluster._supervised.entropy_C->entropy(labels_true)
A:sklearn.metrics.cluster._supervised.entropy_K->entropy(labels_pred)
A:sklearn.metrics.cluster._supervised.MI->mutual_info_score(None, None, contingency=contingency)
A:sklearn.metrics.cluster._supervised.(nzx, nzy)->numpy.nonzero(contingency)
A:sklearn.metrics.cluster._supervised.(nzx, nzy, nz_val)->scipy.sparse.find(contingency)
A:sklearn.metrics.cluster._supervised.contingency_sum->contingency.astype(np.float64, copy=False).astype(np.float64, copy=False).sum()
A:sklearn.metrics.cluster._supervised.pi->numpy.bincount(label_idx).astype(np.float64)
A:sklearn.metrics.cluster._supervised.pj->numpy.ravel(contingency.sum(axis=0))
A:sklearn.metrics.cluster._supervised.log_contingency_nm->numpy.log(nz_val)
A:sklearn.metrics.cluster._supervised.mi->mutual_info_score(labels_true, labels_pred, contingency=contingency)
A:sklearn.metrics.cluster._supervised.classes->numpy.unique(labels_true)
A:sklearn.metrics.cluster._supervised.clusters->numpy.unique(labels_pred)
A:sklearn.metrics.cluster._supervised.emi->expected_mutual_information(contingency, n_samples)
A:sklearn.metrics.cluster._supervised.normalizer->_generalized_average(h_true, h_pred, average_method)
A:sklearn.metrics.cluster._supervised.c->c.astype(np.int64, copy=False).astype(np.int64, copy=False)
A:sklearn.metrics.cluster._supervised.pi_sum->numpy.sum(pi)
sklearn.metrics.adjusted_mutual_info_score(labels_true,labels_pred,*,average_method='arithmetic')
sklearn.metrics.adjusted_rand_score(labels_true,labels_pred)
sklearn.metrics.cluster._supervised._generalized_average(U,V,average_method)
sklearn.metrics.cluster._supervised.adjusted_mutual_info_score(labels_true,labels_pred,*,average_method='arithmetic')
sklearn.metrics.cluster._supervised.adjusted_rand_score(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.check_clusterings(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.completeness_score(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.contingency_matrix(labels_true,labels_pred,*,eps=None,sparse=False,dtype=np.int64)
sklearn.metrics.cluster._supervised.entropy(labels)
sklearn.metrics.cluster._supervised.fowlkes_mallows_score(labels_true,labels_pred,*,sparse=False)
sklearn.metrics.cluster._supervised.homogeneity_completeness_v_measure(labels_true,labels_pred,*,beta=1.0)
sklearn.metrics.cluster._supervised.homogeneity_score(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.mutual_info_score(labels_true,labels_pred,*,contingency=None)
sklearn.metrics.cluster._supervised.normalized_mutual_info_score(labels_true,labels_pred,*,average_method='arithmetic')
sklearn.metrics.cluster._supervised.pair_confusion_matrix(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.rand_score(labels_true,labels_pred)
sklearn.metrics.cluster._supervised.v_measure_score(labels_true,labels_pred,*,beta=1.0)
sklearn.metrics.cluster.contingency_matrix(labels_true,labels_pred,*,eps=None,sparse=False,dtype=np.int64)
sklearn.metrics.cluster.entropy(labels)
sklearn.metrics.completeness_score(labels_true,labels_pred)
sklearn.metrics.fowlkes_mallows_score(labels_true,labels_pred,*,sparse=False)
sklearn.metrics.homogeneity_completeness_v_measure(labels_true,labels_pred,*,beta=1.0)
sklearn.metrics.homogeneity_score(labels_true,labels_pred)
sklearn.metrics.mutual_info_score(labels_true,labels_pred,*,contingency=None)
sklearn.metrics.normalized_mutual_info_score(labels_true,labels_pred,*,average_method='arithmetic')
sklearn.metrics.pair_confusion_matrix(labels_true,labels_pred)
sklearn.metrics.rand_score(labels_true,labels_pred)
sklearn.metrics.v_measure_score(labels_true,labels_pred,*,beta=1.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/_unsupervised.py----------------------------------------
A:sklearn.metrics.cluster._unsupervised.(X, labels)->check_X_y(X, labels)
A:sklearn.metrics.cluster._unsupervised.random_state->check_random_state(random_state)
A:sklearn.metrics.cluster._unsupervised.cluster_distances->numpy.zeros((n_chunk_samples, len(label_freqs)), dtype=D_chunk.dtype)
A:sklearn.metrics.cluster._unsupervised.sample_labels->numpy.take(labels, indices)
A:sklearn.metrics.cluster._unsupervised.inter_cluster_distances->numpy.zeros((n_chunk_samples, len(label_freqs)), dtype=D_chunk.dtype).min(axis=1)
A:sklearn.metrics.cluster._unsupervised.error_msg->ValueError('The precomputed distance matrix contains non-zero elements on the diagonal. Use np.fill_diagonal(X, 0).')
A:sklearn.metrics.cluster._unsupervised.le->LabelEncoder()
A:sklearn.metrics.cluster._unsupervised.labels->LabelEncoder().fit_transform(labels)
A:sklearn.metrics.cluster._unsupervised.n_samples->len(labels)
A:sklearn.metrics.cluster._unsupervised.label_freqs->numpy.bincount(labels)
A:sklearn.metrics.cluster._unsupervised.reduce_func->functools.partial(_silhouette_reduce, labels=labels, label_freqs=label_freqs)
A:sklearn.metrics.cluster._unsupervised.results->zip(*pairwise_distances_chunked(X, reduce_func=reduce_func, **kwds))
A:sklearn.metrics.cluster._unsupervised.intra_clust_dists->numpy.concatenate(intra_clust_dists)
A:sklearn.metrics.cluster._unsupervised.inter_clust_dists->numpy.concatenate(inter_clust_dists)
A:sklearn.metrics.cluster._unsupervised.denom->(label_freqs - 1).take(labels, mode='clip')
A:sklearn.metrics.cluster._unsupervised.n_labels->len(le.classes_)
A:sklearn.metrics.cluster._unsupervised.mean->numpy.mean(X, axis=0)
A:sklearn.metrics.cluster._unsupervised.mean_k->numpy.mean(cluster_k, axis=0)
A:sklearn.metrics.cluster._unsupervised.intra_dists->numpy.zeros(n_labels)
A:sklearn.metrics.cluster._unsupervised.centroids->numpy.zeros((n_labels, len(X[0])), dtype=float)
A:sklearn.metrics.cluster._unsupervised.cluster_k->_safe_indexing(X, labels == k)
A:sklearn.metrics.cluster._unsupervised.centroid->_safe_indexing(X, labels == k).mean(axis=0)
A:sklearn.metrics.cluster._unsupervised.intra_dists[k]->numpy.average(pairwise_distances(cluster_k, [centroid]))
A:sklearn.metrics.cluster._unsupervised.centroid_distances->pairwise_distances(centroids)
A:sklearn.metrics.cluster._unsupervised.scores->numpy.max(combined_intra_dists / centroid_distances, axis=1)
sklearn.metrics.calinski_harabasz_score(X,labels)
sklearn.metrics.cluster._unsupervised._silhouette_reduce(D_chunk,start,labels,label_freqs)
sklearn.metrics.cluster._unsupervised.calinski_harabasz_score(X,labels)
sklearn.metrics.cluster._unsupervised.check_number_of_labels(n_labels,n_samples)
sklearn.metrics.cluster._unsupervised.davies_bouldin_score(X,labels)
sklearn.metrics.cluster._unsupervised.silhouette_samples(X,labels,*,metric='euclidean',**kwds)
sklearn.metrics.cluster._unsupervised.silhouette_score(X,labels,*,metric='euclidean',sample_size=None,random_state=None,**kwds)
sklearn.metrics.davies_bouldin_score(X,labels)
sklearn.metrics.silhouette_samples(X,labels,*,metric='euclidean',**kwds)
sklearn.metrics.silhouette_score(X,labels,*,metric='euclidean',sample_size=None,random_state=None,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/tests/test_unsupervised.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_unsupervised.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.cluster.tests.test_unsupervised.X->sparse_container(X)
A:sklearn.metrics.cluster.tests.test_unsupervised.D->numpy.zeros((12, 12))
A:sklearn.metrics.cluster.tests.test_unsupervised.score_precomputed->silhouette_score(D, y, metric='precomputed', sample_size=sample_size, random_state=0)
A:sklearn.metrics.cluster.tests.test_unsupervised.score_euclidean->silhouette_score(X, y, metric='euclidean', sample_size=sample_size, random_state=0)
A:sklearn.metrics.cluster.tests.test_unsupervised.labels->numpy.array([0, 1, 1, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_unsupervised.silhouette->silhouette_score(X, labels)
A:sklearn.metrics.cluster.tests.test_unsupervised.ss->silhouette_samples(X, labels)
A:sklearn.metrics.cluster.tests.test_unsupervised.y->numpy.zeros(X.shape[0])
A:sklearn.metrics.cluster.tests.test_unsupervised.dists->pairwise_distances(np.array([[0.2, 0.1, 0.12, 1.34, 1.11, 1.6]], dtype=dtype).T)
A:sklearn.metrics.cluster.tests.test_unsupervised.pdist_dense->pairwise_distances(X)
A:sklearn.metrics.cluster.tests.test_unsupervised.pdist_sparse->sparse_container(pdist_dense)
A:sklearn.metrics.cluster.tests.test_unsupervised.output_with_sparse_input->silhouette_samples(pdist_sparse, y)
A:sklearn.metrics.cluster.tests.test_unsupervised.output_with_dense_input->silhouette_samples(pdist_dense, y)
A:sklearn.metrics.cluster.tests.test_unsupervised.label_freqs->numpy.bincount(y)
A:sklearn.metrics.cluster.tests.test_unsupervised.rng->numpy.random.RandomState(seed=0)
A:sklearn.metrics.cluster.tests.test_unsupervised.result->silhouette_score([[0, 1, 2], [1, 0, 1], [2, 1, 0]], [0, 0, 1], metric='precomputed')
sklearn.metrics.cluster.tests.test_unsupervised.assert_raises_on_all_points_same_cluster(func)
sklearn.metrics.cluster.tests.test_unsupervised.assert_raises_on_only_one_label(func)
sklearn.metrics.cluster.tests.test_unsupervised.test_calinski_harabasz_score()
sklearn.metrics.cluster.tests.test_unsupervised.test_cluster_size_1()
sklearn.metrics.cluster.tests.test_unsupervised.test_correct_labelsize()
sklearn.metrics.cluster.tests.test_unsupervised.test_davies_bouldin_score()
sklearn.metrics.cluster.tests.test_unsupervised.test_non_encoded_labels()
sklearn.metrics.cluster.tests.test_unsupervised.test_non_numpy_labels()
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette(sparse_container,sample_size)
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_nonzero_diag(dtype)
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_paper_example()
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_reduce(sparse_container)
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_samples_euclidean_sparse(sparse_container)
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_samples_precomputed_sparse(sparse_container)
sklearn.metrics.cluster.tests.test_unsupervised.test_silhouette_score_integer_precomputed()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/tests/test_common.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_common.rng->numpy.random.RandomState(0)
A:sklearn.metrics.cluster.tests.test_common.y1->numpy.random.RandomState(0).randint(3, size=30)
A:sklearn.metrics.cluster.tests.test_common.y2->numpy.random.RandomState(0).randint(3, size=30)
A:sklearn.metrics.cluster.tests.test_common.score->numpy.array([metric(lower_bound_1, lower_bound_2), metric(lower_bound_2, lower_bound_1)])
A:sklearn.metrics.cluster.tests.test_common.y_label->numpy.array([0, 0, 0, 1, 1, 0, 1])
A:sklearn.metrics.cluster.tests.test_common.y_pred->numpy.array([1, 0, 1, 0, 1, 1, 0])
A:sklearn.metrics.cluster.tests.test_common.score_1->metric(X, y_true)
A:sklearn.metrics.cluster.tests.test_common.X->numpy.random.randint(10, size=(2, 10))
A:sklearn.metrics.cluster.tests.test_common.y->numpy.array(y)
A:sklearn.metrics.cluster.tests.test_common.y_true_gen->generate_formats(y_true)
A:sklearn.metrics.cluster.tests.test_common.y_pred_gen->generate_formats(y_pred)
sklearn.metrics.cluster.tests.test_common.test_format_invariance(metric_name)
sklearn.metrics.cluster.tests.test_common.test_inf_nan_input(metric_name,metric_func)
sklearn.metrics.cluster.tests.test_common.test_non_symmetry(metric_name,y1,y2)
sklearn.metrics.cluster.tests.test_common.test_normalized_output(metric_name)
sklearn.metrics.cluster.tests.test_common.test_permute_labels(metric_name)
sklearn.metrics.cluster.tests.test_common.test_single_sample(metric)
sklearn.metrics.cluster.tests.test_common.test_symmetric_non_symmetric_union()
sklearn.metrics.cluster.tests.test_common.test_symmetry(metric_name,y1,y2)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/tests/test_supervised.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_supervised.(h, c, v)->homogeneity_completeness_v_measure([0, 0, 0, 1, 1, 1], [0, 4, 0, 4, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.v->v_measure_score([0, 0, 0, 1, 1, 1], [0, 1, 0, 1, 2, 2], beta=beta_test)
A:sklearn.metrics.cluster.tests.test_supervised.ari_1->adjusted_rand_score([0, 0, 0, 1, 1, 1], [0, 1, 0, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.ari_2->adjusted_rand_score([0, 0, 0, 1, 1, 1], [0, 4, 0, 4, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.ri_1->rand_score([0, 0, 0, 1, 1, 1], [0, 1, 0, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.ri_2->rand_score([0, 0, 0, 1, 1, 1], [0, 4, 0, 4, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.scores->uniform_labelings_scores(adjusted_rand_score, n_samples, n_clusters_range, n_runs)
A:sklearn.metrics.cluster.tests.test_supervised.labels_a->numpy.array([0, 0, 0, 1, 1, 2])
A:sklearn.metrics.cluster.tests.test_supervised.labels_b->numpy.array([1, 1, 2, 2, 0, 0])
A:sklearn.metrics.cluster.tests.test_supervised.scores[i, j]->score_func(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.max_abs_scores->numpy.abs(scores).max(axis=1)
A:sklearn.metrics.cluster.tests.test_supervised.mi->mutual_info_score(labels_a, labels_b, contingency=C)
A:sklearn.metrics.cluster.tests.test_supervised.C->contingency_matrix(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.n_samples->contingency_matrix(labels_a, labels_b).sum()
A:sklearn.metrics.cluster.tests.test_supervised.emi->expected_mutual_information(C, n_samples)
A:sklearn.metrics.cluster.tests.test_supervised.ami->adjusted_mutual_info_score(a110, b110)
A:sklearn.metrics.cluster.tests.test_supervised.a110->numpy.array([list(labels_a) * 110]).flatten()
A:sklearn.metrics.cluster.tests.test_supervised.b110->numpy.array([list(labels_b) * 110]).flatten()
A:sklearn.metrics.cluster.tests.test_supervised.x->numpy.array([1] * (52632 + 2529) + [2] * (14660 + 793) + [3] * (3271 + 204) + [4] * (814 + 39) + [5] * (316 + 20))
A:sklearn.metrics.cluster.tests.test_supervised.y->numpy.array([0] * 52632 + [1] * 2529 + [0] * 14660 + [1] * 793 + [0] * 3271 + [1] * 204 + [0] * 814 + [1] * 39 + [0] * 316 + [1] * 20)
A:sklearn.metrics.cluster.tests.test_supervised.ent->entropy([0, 0, 42.0])
A:sklearn.metrics.cluster.tests.test_supervised.C_sparse->contingency_matrix(labels_a, labels_b, sparse=True).toarray()
A:sklearn.metrics.cluster.tests.test_supervised.random_state->numpy.random.RandomState(seed)
A:sklearn.metrics.cluster.tests.test_supervised.score->fowlkes_mallows_score([0, 0, 0, 1, 1, 1], [0, 0, 1, 1, 2, 2])
A:sklearn.metrics.cluster.tests.test_supervised.perfect_score->fowlkes_mallows_score([0, 0, 0, 1, 1, 1], [1, 1, 1, 0, 0, 0])
A:sklearn.metrics.cluster.tests.test_supervised.worst_score->fowlkes_mallows_score([0, 0, 0, 0, 0, 0], [0, 1, 2, 3, 4, 5])
A:sklearn.metrics.cluster.tests.test_supervised.score_original->fowlkes_mallows_score(labels_a, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.score_symmetric->fowlkes_mallows_score(labels_b, labels_a)
A:sklearn.metrics.cluster.tests.test_supervised.score_permuted->fowlkes_mallows_score((labels_a + 1) % 3, labels_b)
A:sklearn.metrics.cluster.tests.test_supervised.score_both->fowlkes_mallows_score(labels_b, (labels_a + 2) % 3)
A:sklearn.metrics.cluster.tests.test_supervised.rng->numpy.random.RandomState(0)
A:sklearn.metrics.cluster.tests.test_supervised.noise->numpy.random.RandomState(0).rand(500)
A:sklearn.metrics.cluster.tests.test_supervised.clustering1->numpy.hstack([[i + 1] * n for i in range(n)])
A:sklearn.metrics.cluster.tests.test_supervised.expected->numpy.zeros(shape=(2, 2), dtype=np.int64)
A:sklearn.metrics.cluster.tests.test_supervised.same_cluster_1->int(clustering1[i] == clustering1[j])
A:sklearn.metrics.cluster.tests.test_supervised.same_cluster_2->int(clustering2[i] == clustering2[j])
A:sklearn.metrics.cluster.tests.test_supervised.y_true->numpy.random.RandomState(0).randint(0, 2, 100000, dtype=np.int8)
A:sklearn.metrics.cluster.tests.test_supervised.y_pred->numpy.random.RandomState(0).randint(0, 2, 100000, dtype=np.int8)
A:sklearn.metrics.cluster.tests.test_supervised.nmi->normalized_mutual_info_score(labels2, labels3, average_method=average_method)
sklearn.metrics.cluster.tests.test_supervised.test_adjusted_mutual_info_score()
sklearn.metrics.cluster.tests.test_supervised.test_adjusted_rand_score_overflow()
sklearn.metrics.cluster.tests.test_supervised.test_adjustment_for_chance()
sklearn.metrics.cluster.tests.test_supervised.test_beta_parameter()
sklearn.metrics.cluster.tests.test_supervised.test_check_clustering_error()
sklearn.metrics.cluster.tests.test_supervised.test_complete_but_not_homogeneous_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_contingency_matrix()
sklearn.metrics.cluster.tests.test_supervised.test_contingency_matrix_sparse()
sklearn.metrics.cluster.tests.test_supervised.test_entropy()
sklearn.metrics.cluster.tests.test_supervised.test_error_messages_on_wrong_input()
sklearn.metrics.cluster.tests.test_supervised.test_exactly_zero_info_score()
sklearn.metrics.cluster.tests.test_supervised.test_expected_mutual_info_overflow()
sklearn.metrics.cluster.tests.test_supervised.test_fowlkes_mallows_score()
sklearn.metrics.cluster.tests.test_supervised.test_fowlkes_mallows_score_properties()
sklearn.metrics.cluster.tests.test_supervised.test_generalized_average()
sklearn.metrics.cluster.tests.test_supervised.test_homogeneous_but_not_complete_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_int_overflow_mutual_info_fowlkes_mallows_score()
sklearn.metrics.cluster.tests.test_supervised.test_mutual_info_score_positive_constant_label(labels_true,labels_pred)
sklearn.metrics.cluster.tests.test_supervised.test_non_consecutive_labels()
sklearn.metrics.cluster.tests.test_supervised.test_normalized_mutual_info_score_bounded(average_method)
sklearn.metrics.cluster.tests.test_supervised.test_not_complete_and_not_homogeneous_labeling()
sklearn.metrics.cluster.tests.test_supervised.test_pair_confusion_matrix()
sklearn.metrics.cluster.tests.test_supervised.test_pair_confusion_matrix_fully_dispersed()
sklearn.metrics.cluster.tests.test_supervised.test_pair_confusion_matrix_single_cluster()
sklearn.metrics.cluster.tests.test_supervised.test_perfect_matches()
sklearn.metrics.cluster.tests.test_supervised.test_rand_score()
sklearn.metrics.cluster.tests.test_supervised.test_rand_score_edge_cases(clustering1,clustering2)
sklearn.metrics.cluster.tests.test_supervised.test_v_measure_and_mutual_information(seed=36)
sklearn.metrics.cluster.tests.test_supervised.uniform_labelings_scores(score_func,n_samples,k_range,n_runs=10,seed=42)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/cluster/tests/test_bicluster.py----------------------------------------
A:sklearn.metrics.cluster.tests.test_bicluster.a1->numpy.array([True, True, False, False])
A:sklearn.metrics.cluster.tests.test_bicluster.a2->numpy.array([True, True, True, True])
A:sklearn.metrics.cluster.tests.test_bicluster.a3->numpy.array([False, True, True, False])
A:sklearn.metrics.cluster.tests.test_bicluster.a4->numpy.array([False, False, True, True])
A:sklearn.metrics.cluster.tests.test_bicluster.a_rows->numpy.array([[True, True, False, False], [False, False, True, True], [False, False, False, True]])
A:sklearn.metrics.cluster.tests.test_bicluster.a_cols->numpy.array([[True, True, False, False], [False, False, True, True], [False, False, False, True]])
A:sklearn.metrics.cluster.tests.test_bicluster.s->consensus_score((a_rows, a_cols), (a_rows[idx], a_cols[idx]))
sklearn.metrics.cluster.tests.test_bicluster.test_consensus_score()
sklearn.metrics.cluster.tests.test_bicluster.test_consensus_score_issue2445()
sklearn.metrics.cluster.tests.test_bicluster.test_jaccard()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_classification.py----------------------------------------
A:sklearn.metrics.tests.test_classification.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_classification.p->numpy.arange(n_samples)
A:sklearn.metrics.tests.test_classification.rng->numpy.random.RandomState(42)
A:sklearn.metrics.tests.test_classification.half->int(n_samples / 2)
A:sklearn.metrics.tests.test_classification.clf->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0)
A:sklearn.metrics.tests.test_classification.probas_pred->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X[:half], y[:half]).predict_proba(X[half:])
A:sklearn.metrics.tests.test_classification.y_pred->rng.choice(classes, size=n_samples, replace=True).copy()
A:sklearn.metrics.tests.test_classification.iris->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_classification.(y_true, y_pred, _)->make_prediction(binary=False)
A:sklearn.metrics.tests.test_classification.report->classification_report(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.y1->numpy.array([[0, 1, 1], [1, 0, 1]])
A:sklearn.metrics.tests.test_classification.y2->numpy.array([[0, 0, 1], [1, 0, 1]])
A:sklearn.metrics.tests.test_classification.(p, r, f, s)->precision_recall_fscore_support(y_true, y_pred, average=None, beta=1)
A:sklearn.metrics.tests.test_classification.ps->precision_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.rs->recall_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.fs->f1_score(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.y_true_bin->LabelBinarizer().transform(y_true)
A:sklearn.metrics.tests.test_classification.y_pred_bin->LabelBinarizer().transform(y_pred)
A:sklearn.metrics.tests.test_classification.actual->recall_score(y_true, y_pred, labels=[0, 1, 2, 3, 4], average='macro')
A:sklearn.metrics.tests.test_classification.y_true->numpy.random.RandomState(42).choice(classes, size=n_samples, replace=True)
A:sklearn.metrics.tests.test_classification.(p, r, f, _)->precision_recall_fscore_support(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_classification.recall_13->partial(recall_score, y_true, y_pred, labels=[1, 3])
A:sklearn.metrics.tests.test_classification.recall_all->partial(recall_score, y_true, y_pred, labels=None)
A:sklearn.metrics.tests.test_classification.y_score->numpy.array([[0.1, 0.9], [0.1, 0.9]])
A:sklearn.metrics.tests.test_classification.cm->confusion_matrix(y, y, sample_weight=weight)
A:sklearn.metrics.tests.test_classification.(tp, fp, fn, tn)->confusion_matrix(y, y, sample_weight=weight).flatten()
A:sklearn.metrics.tests.test_classification.den->numpy.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
A:sklearn.metrics.tests.test_classification.mcc->matthews_corrcoef(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.y_true_csr->csr_container(y_true)
A:sklearn.metrics.tests.test_classification.y_pred_csr->csr_container(y_pred)
A:sklearn.metrics.tests.test_classification.y_true_csc->csc_container(y_true)
A:sklearn.metrics.tests.test_classification.y_pred_csc->csc_container(y_pred)
A:sklearn.metrics.tests.test_classification.sample_weight->numpy.random.RandomState(42).rand(20)
A:sklearn.metrics.tests.test_classification.cm_true->confusion_matrix(y_test, y_pred, normalize='true')
A:sklearn.metrics.tests.test_classification.cm_pred->confusion_matrix(y_test, y_pred, normalize='pred')
A:sklearn.metrics.tests.test_classification.(pos, neg)->class_likelihood_ratios(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_classification.kappa->cohen_kappa_score(y1, y2)
A:sklearn.metrics.tests.test_classification.result->metric(y_true, y_pred, pos_label=pos_label)
A:sklearn.metrics.tests.test_classification.C->confusion_matrix(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_classification.N->len(C)
A:sklearn.metrics.tests.test_classification.cov_ytyp->sum([C[k, k] * C[m, l] - C[l, k] * C[k, m] for k in range(N) for m in range(N) for l in range(N)])
A:sklearn.metrics.tests.test_classification.cov_ytyt->sum([C[:, k].sum() * np.sum([C[g, f] for f in range(N) for g in range(N) if f != k]) for k in range(N)])
A:sklearn.metrics.tests.test_classification.cov_ypyp->numpy.sum([C[k, :].sum() * np.sum([C[f, g] for f in range(N) for g in range(N) if f != k]) for k in range(N)])
A:sklearn.metrics.tests.test_classification.mcc_ours->matthews_corrcoef(y_true, y_pred, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_classification.y_true_inv2->numpy.where(y_true_inv2, 'a', 'b')
A:sklearn.metrics.tests.test_classification.ord_a->ord('a')
A:sklearn.metrics.tests.test_classification.conf_matrix->confusion_matrix(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.n_points->len(y_true)
A:sklearn.metrics.tests.test_classification.x_true->numpy.random.RandomState(42).random_sample(n_points)
A:sklearn.metrics.tests.test_classification.arr->numpy.repeat([0.0, 1.0, 2.0], n_points)
A:sklearn.metrics.tests.test_classification.(y_true, y_pred)->random_ys(n_points)
A:sklearn.metrics.tests.test_classification.(ps, rs, fs, _)->precision_recall_fscore_support(y_true, y_pred, average=None)
A:sklearn.metrics.tests.test_classification.support->numpy.bincount(y_true)
A:sklearn.metrics.tests.test_classification.old_error_settings->numpy.seterr(all='raise')
A:sklearn.metrics.tests.test_classification.expected->numpy.zeros((expected_n_classes, expected_n_classes), dtype=int)
A:sklearn.metrics.tests.test_classification.weight->numpy.full(len(y), 9223372036854775807, dtype=np.int64)
A:sklearn.metrics.tests.test_classification.pd->pytest.importorskip('pandas')
A:sklearn.metrics.tests.test_classification.y_ndarray->numpy.array([1, 0, 0, 1, 0, 1, 1, 0, 1])
A:sklearn.metrics.tests.test_classification.y_predicted->pytest.importorskip('pandas').Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype='int64')
A:sklearn.metrics.tests.test_classification.output->confusion_matrix(y_true, y_predicted)
A:sklearn.metrics.tests.test_classification.expected_output->confusion_matrix(y_ndarray, y_predicted)
A:sklearn.metrics.tests.test_classification.labels->numpy.array([0, 1, 2])
A:sklearn.metrics.tests.test_classification.(_, y_true)->make_multilabel_classification(n_features=1, n_samples=n_samples, n_classes=n_classes, random_state=0)
A:sklearn.metrics.tests.test_classification.(_, y_pred)->make_multilabel_classification(n_features=1, n_samples=n_samples, n_classes=n_classes, random_state=1)
A:sklearn.metrics.tests.test_classification.w->numpy.array([1, 3])
A:sklearn.metrics.tests.test_classification.lb->LabelBinarizer()
A:sklearn.metrics.tests.test_classification.multi_jaccard_score->partial(jaccard_score, y_true, y_pred)
A:sklearn.metrics.tests.test_classification.bin_jaccard_score->partial(jaccard_score, y_true_bin, y_pred_bin)
A:sklearn.metrics.tests.test_classification.score->jaccard_score(y_true, y_pred, average='samples', zero_division=zero_division)
A:sklearn.metrics.tests.test_classification.f2->fbeta_score(y_true, y_pred, beta=2, average=None, zero_division=zero_division)
A:sklearn.metrics.tests.test_classification.fbeta->fbeta_score(y_true, y_pred, beta=1, average=None)
A:sklearn.metrics.tests.test_classification.zero_division->numpy.float64(zero_division)
A:sklearn.metrics.tests.test_classification.y_true_ind->numpy.array([[0, 1, 1], [1, 0, 0], [0, 0, 1]])
A:sklearn.metrics.tests.test_classification.y_pred_ind->numpy.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])
A:sklearn.metrics.tests.test_classification.err_msg->'{0} is not supported'.format(type1)
A:sklearn.metrics.tests.test_classification.(merged_type, y1out, y2out)->_check_targets(y1, y2)
A:sklearn.metrics.tests.test_classification.pred_decision->numpy.array([[+0.36, -0.17, -0.58], [-0.15, -0.58, -0.48], [-1.45, -0.58, -0.38], [-0.55, -0.78, -0.42], [-1.45, -0.58, -0.38]])
A:sklearn.metrics.tests.test_classification.dummy_losses->numpy.array([1 - pred_decision[0][0] + pred_decision[0][1], 1 - pred_decision[1][1] + pred_decision[1][2], 1 - pred_decision[2][2] + pred_decision[2][3], 1 - pred_decision[3][1] + pred_decision[3][2], 1 - pred_decision[4][3] + pred_decision[4][2], 1 - pred_decision[5][2] + pred_decision[5][3]])
A:sklearn.metrics.tests.test_classification.dummy_hinge_loss->numpy.mean(dummy_losses)
A:sklearn.metrics.tests.test_classification.loss->log_loss(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.calculated_log_loss->log_loss(y_true, y_score, labels=[1, 2])
A:sklearn.metrics.tests.test_classification.y_tr->numpy.array(['ham', 'spam', 'spam', 'ham'])
A:sklearn.metrics.tests.test_classification.y_pr->numpy.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])
A:sklearn.metrics.tests.test_classification.macro_recall->recall_score(y_true, y_pred, average='macro', labels=np.unique(y_true))
A:sklearn.metrics.tests.test_classification.balanced->balanced_accuracy_score(y_true, y_pred)
A:sklearn.metrics.tests.test_classification.adjusted->balanced_accuracy_score(y_true, y_pred, adjusted=True)
A:sklearn.metrics.tests.test_classification.chance->balanced_accuracy_score(y_true, np.full_like(y_true, y_true[0]))
A:sklearn.metrics.tests.test_classification.(X, y)->sklearn.datasets.make_classification(random_state=0)
A:sklearn.metrics.tests.test_classification.classifier->DecisionTreeClassifier(max_depth=3, random_state=0).fit(X, y)
sklearn.metrics.tests.test_classification.make_prediction(dataset=None,binary=False)
sklearn.metrics.tests.test_classification.test__check_targets()
sklearn.metrics.tests.test_classification.test__check_targets_multiclass_with_both_y_true_and_y_pred_binary()
sklearn.metrics.tests.test_classification.test_average_binary_jaccard_score(recwarn)
sklearn.metrics.tests.test_classification.test_average_precision_score_duplicate_values(y_true,y_score)
sklearn.metrics.tests.test_classification.test_average_precision_score_non_binary_class()
sklearn.metrics.tests.test_classification.test_average_precision_score_tied_values(y_true,y_score)
sklearn.metrics.tests.test_classification.test_balanced_accuracy_score(y_true,y_pred)
sklearn.metrics.tests.test_classification.test_balanced_accuracy_score_unseen()
sklearn.metrics.tests.test_classification.test_brier_score_loss()
sklearn.metrics.tests.test_classification.test_classification_metric_division_by_zero_nan_validaton(scoring)
sklearn.metrics.tests.test_classification.test_classification_metric_pos_label_types(metric,classes)
sklearn.metrics.tests.test_classification.test_classification_report_dictionary_output()
sklearn.metrics.tests.test_classification.test_classification_report_labels_target_names_unequal_length()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_balanced()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_digits()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_label_detection()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_long_string_label()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_string_label()
sklearn.metrics.tests.test_classification.test_classification_report_multiclass_with_unicode_label()
sklearn.metrics.tests.test_classification.test_classification_report_no_labels_target_names_unequal_length()
sklearn.metrics.tests.test_classification.test_classification_report_output_dict_empty_input()
sklearn.metrics.tests.test_classification.test_classification_report_zero_division_warning(zero_division)
sklearn.metrics.tests.test_classification.test_cohen_kappa()
sklearn.metrics.tests.test_classification.test_confusion_matrix_binary()
sklearn.metrics.tests.test_classification.test_confusion_matrix_dtype()
sklearn.metrics.tests.test_classification.test_confusion_matrix_error(labels,err_msg)
sklearn.metrics.tests.test_classification.test_confusion_matrix_multiclass_subset_labels()
sklearn.metrics.tests.test_classification.test_confusion_matrix_normalize(normalize,cm_dtype,expected_results)
sklearn.metrics.tests.test_classification.test_confusion_matrix_normalize_single_class()
sklearn.metrics.tests.test_classification.test_confusion_matrix_on_zero_length_input(labels)
sklearn.metrics.tests.test_classification.test_confusion_matrix_pandas_nullable(dtype)
sklearn.metrics.tests.test_classification.test_confusion_matrix_single_label()
sklearn.metrics.tests.test_classification.test_f1_for_small_binary_inputs_with_zero_division(y_true,y_pred,expected_score)
sklearn.metrics.tests.test_classification.test_fscore_warnings(zero_division)
sklearn.metrics.tests.test_classification.test_hinge_loss_binary()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_invariance_lists()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_missing_labels_only_two_unq_in_y_true()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_missing_labels_with_labels_none()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_no_consistent_pred_decision_shape()
sklearn.metrics.tests.test_classification.test_hinge_loss_multiclass_with_missing_labels()
sklearn.metrics.tests.test_classification.test_jaccard_score_validation()
sklearn.metrics.tests.test_classification.test_jaccard_score_zero_division_set_value(zero_division,expected_score)
sklearn.metrics.tests.test_classification.test_jaccard_score_zero_division_warning()
sklearn.metrics.tests.test_classification.test_likelihood_ratios()
sklearn.metrics.tests.test_classification.test_likelihood_ratios_errors(params,err_msg)
sklearn.metrics.tests.test_classification.test_likelihood_ratios_warnings(params,warn_msg)
sklearn.metrics.tests.test_classification.test_log_loss()
sklearn.metrics.tests.test_classification.test_log_loss_eps_auto(global_dtype)
sklearn.metrics.tests.test_classification.test_log_loss_eps_auto_float16()
sklearn.metrics.tests.test_classification.test_log_loss_pandas_input()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_against_jurman()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_against_numpy_corrcoef()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_multiclass()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_nan()
sklearn.metrics.tests.test_classification.test_matthews_corrcoef_overflow(n_points)
sklearn.metrics.tests.test_classification.test_multiclass_jaccard_score(recwarn)
sklearn.metrics.tests.test_classification.test_multilabel_accuracy_score_subset_accuracy()
sklearn.metrics.tests.test_classification.test_multilabel_classification_report()
sklearn.metrics.tests.test_classification.test_multilabel_confusion_matrix_binary()
sklearn.metrics.tests.test_classification.test_multilabel_confusion_matrix_errors()
sklearn.metrics.tests.test_classification.test_multilabel_confusion_matrix_multiclass()
sklearn.metrics.tests.test_classification.test_multilabel_confusion_matrix_multilabel(csc_container,csr_container)
sklearn.metrics.tests.test_classification.test_multilabel_hamming_loss()
sklearn.metrics.tests.test_classification.test_multilabel_jaccard_score(recwarn)
sklearn.metrics.tests.test_classification.test_multilabel_zero_one_loss_subset()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_no_labels(beta,average,zero_division)
sklearn.metrics.tests.test_classification.test_precision_recall_f1_no_labels_average_none(zero_division)
sklearn.metrics.tests.test_classification.test_precision_recall_f1_no_labels_average_none_warn()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_no_labels_check_warnings(average)
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_binary()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_binary_averaged()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multiclass()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multilabel_1()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_multilabel_2()
sklearn.metrics.tests.test_classification.test_precision_recall_f1_score_with_an_empty_prediction(zero_division,zero_division_expected)
sklearn.metrics.tests.test_classification.test_precision_recall_f_binary_single_class()
sklearn.metrics.tests.test_classification.test_precision_recall_f_extra_labels()
sklearn.metrics.tests.test_classification.test_precision_recall_f_ignored_labels()
sklearn.metrics.tests.test_classification.test_precision_recall_f_unused_pos_label()
sklearn.metrics.tests.test_classification.test_precision_refcall_f1_score_multilabel_unordered_labels(average)
sklearn.metrics.tests.test_classification.test_precision_warnings(zero_division)
sklearn.metrics.tests.test_classification.test_prf_average_binary_data_non_binary()
sklearn.metrics.tests.test_classification.test_prf_no_warnings_if_zero_division_set(zero_division)
sklearn.metrics.tests.test_classification.test_prf_warnings()
sklearn.metrics.tests.test_classification.test_recall_warnings(zero_division)
sklearn.metrics.tests.test_classification.test_zero_division_nan_no_warning(metric,y_true,y_pred,zero_division)
sklearn.metrics.tests.test_classification.test_zero_division_nan_warning(metric,y_true,y_pred)
sklearn.metrics.tests.test_classification.test_zero_precision_recall()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_ranking.py----------------------------------------
A:sklearn.metrics.tests.test_ranking.dataset->sklearn.datasets.load_iris()
A:sklearn.metrics.tests.test_ranking.p->numpy.sum(y_true)
A:sklearn.metrics.tests.test_ranking.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.metrics.tests.test_ranking.half->int(n_samples / 2)
A:sklearn.metrics.tests.test_ranking.clf->LogisticRegression(random_state=0)
A:sklearn.metrics.tests.test_ranking.y_score->numpy.random.RandomState(global_random_seed).rand(10)
A:sklearn.metrics.tests.test_ranking.y_pred->numpy.array([[0.5, 0.2, 0.1], [0.4, 0.5, 0.3], [0.1, 0.2, 0.6], [0.2, 0.3, 0.5], [0.2, 0.3, 0.5], [0.2, 0.3, 0.5]])
A:sklearn.metrics.tests.test_ranking.n_correct->numpy.sum(diff_matrix > 0)
A:sklearn.metrics.tests.test_ranking.n_pos->numpy.sum(y_true == pos_label)
A:sklearn.metrics.tests.test_ranking.(precision, recall, threshold)->precision_recall_curve(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.precision->list(reversed(precision))
A:sklearn.metrics.tests.test_ranking.recall->list(reversed(recall))
A:sklearn.metrics.tests.test_ranking.(fpr, tpr, _)->roc_curve(y_onehot.ravel(), y_pred.ravel())
A:sklearn.metrics.tests.test_ranking.new_fpr->numpy.append(new_fpr, max_fpr)
A:sklearn.metrics.tests.test_ranking.idx_out->numpy.argmax(fpr > max_fpr)
A:sklearn.metrics.tests.test_ranking.new_tpr->numpy.append(new_tpr, np.interp(max_fpr, x_interp, y_interp))
A:sklearn.metrics.tests.test_ranking.(new_fpr, new_tpr)->_partial_roc(y_true, y_predict, max_fpr)
A:sklearn.metrics.tests.test_ranking.partial_auc->auc(new_fpr, new_tpr)
A:sklearn.metrics.tests.test_ranking.(y_true, _, y_score)->make_prediction(binary=True)
A:sklearn.metrics.tests.test_ranking.expected_auc->_auc(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.(fpr, tpr, thresholds)->roc_curve([1 - x for x in y_true], y_pred)
A:sklearn.metrics.tests.test_ranking.roc_auc->roc_auc_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.y_true->numpy.random.RandomState(global_random_seed).randint(0, 2, size=10)
A:sklearn.metrics.tests.test_ranking.(fpr, tpr, thr)->roc_curve(y_true, y_pred, drop_intermediate=True)
A:sklearn.metrics.tests.test_ranking.tp->numpy.sum((y_score >= t) & y_true)
A:sklearn.metrics.tests.test_ranking.(y_true, pred, y_score)->make_prediction(binary=True)
A:sklearn.metrics.tests.test_ranking.trivial_pred->numpy.zeros(y_true.shape)
A:sklearn.metrics.tests.test_ranking.(tpr, fpr, _)->roc_curve(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.(tpr, fpr, thresholds)->roc_curve(y_true, y_score, drop_intermediate=True)
A:sklearn.metrics.tests.test_ranking.sample_weight->numpy.array([1.0, 1.0, 0.0])
A:sklearn.metrics.tests.test_ranking.error_message->'x is neither increasing nor decreasing : {}'.format(np.array(x))
A:sklearn.metrics.tests.test_ranking.y_scores->numpy.array([0.1, 0, 0.1, 0.01])
A:sklearn.metrics.tests.test_ranking.score_01->roc_auc_score([1, 0, 1, 0], [0.2, 0.6, 0.55, 0.4])
A:sklearn.metrics.tests.test_ranking.score_10->roc_auc_score([0, 1, 0, 1], [0.8, 0.4, 0.45, 0.6])
A:sklearn.metrics.tests.test_ranking.score_02->roc_auc_score([1, 1, 0], [0.1, 0.35, 0])
A:sklearn.metrics.tests.test_ranking.score_20->roc_auc_score([0, 0, 1], [0.1, 0.15, 0.8])
A:sklearn.metrics.tests.test_ranking.score_12->roc_auc_score([1, 0], [0.4, 0.2])
A:sklearn.metrics.tests.test_ranking.score_21->roc_auc_score([0, 1], [0.3, 0.8])
A:sklearn.metrics.tests.test_ranking.ovo_weighted_score->numpy.average(pair_scores, weights=prevalence)
A:sklearn.metrics.tests.test_ranking.out_0->roc_auc_score([1, 0, 0, 0], y_scores[:, 0])
A:sklearn.metrics.tests.test_ranking.out_1->roc_auc_score([0, 1, 0, 0], y_scores[:, 1])
A:sklearn.metrics.tests.test_ranking.out_2->roc_auc_score([0, 0, 1, 1], y_scores[:, 2])
A:sklearn.metrics.tests.test_ranking.y_onehot->label_binarize(y_true, classes=[0, 1, 2])
A:sklearn.metrics.tests.test_ranking.roc_auc_by_hand->auc(fpr, tpr)
A:sklearn.metrics.tests.test_ranking.roc_auc_auto->roc_auc_score(y_true, y_pred, multi_class='ovr', average='micro')
A:sklearn.metrics.tests.test_ranking.y_prob->softmax(y_score)
A:sklearn.metrics.tests.test_ranking.int_curve->curve_func([0, 1, 1, 0], y_pred)
A:sklearn.metrics.tests.test_ranking.float_curve->curve_func([0.0, 1.0, 1.0, 0.0], y_pred)
A:sklearn.metrics.tests.test_ranking.result_1->curve_func(y_true, y_score, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_ranking.result_2->curve_func(y_true[:-1], y_score[:-1], sample_weight=sample_weight[:-1])
A:sklearn.metrics.tests.test_ranking.(p, r, t)->precision_recall_curve(labels, predict_probas, drop_intermediate=drop)
A:sklearn.metrics.tests.test_ranking.y_true_copy->numpy.random.RandomState(global_random_seed).randint(0, 2, size=10).copy()
A:sklearn.metrics.tests.test_ranking.(p, r, thresholds)->precision_recall_curve(y_true, np.zeros_like(y_score), drop_intermediate=drop)
A:sklearn.metrics.tests.test_ranking.precision_recall_auc->_average_precision_slow(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.(p, r, _)->precision_recall_curve(y_true, y_score, drop_intermediate=drop)
A:sklearn.metrics.tests.test_ranking.auc_prc->average_precision_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.(precision, recall, thresholds)->precision_recall_curve(y_true, y_score, drop_intermediate=True)
A:sklearn.metrics.tests.test_ranking.roc_auc_scaled_up->roc_auc_score(y_true, 100 * y_score)
A:sklearn.metrics.tests.test_ranking.roc_auc_scaled_down->roc_auc_score(y_true, 1e-06 * y_score)
A:sklearn.metrics.tests.test_ranking.roc_auc_shifted->roc_auc_score(y_true, y_score - 10)
A:sklearn.metrics.tests.test_ranking.pr_auc->average_precision_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.pr_auc_scaled_up->average_precision_score(y_true, 100 * y_score)
A:sklearn.metrics.tests.test_ranking.pr_auc_scaled_down->average_precision_score(y_true, 1e-06 * y_score)
A:sklearn.metrics.tests.test_ranking.pr_auc_shifted->average_precision_score(y_true, y_score - 10)
A:sklearn.metrics.tests.test_ranking.(fpr, fnr, _)->det_curve(y_true=y_true, y_score=y_true)
A:sklearn.metrics.tests.test_ranking.(fpr, fnr, threshold)->det_curve(y_true=[0, 1, 0, 1, 0, 1], y_score=np.full(6, y_score))
A:sklearn.metrics.tests.test_ranking.y_pred_pos_not_cancer->numpy.array([0.1, 0.4, 0.6, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9])
A:sklearn.metrics.tests.test_ranking.(fpr_pos_cancer, fnr_pos_cancer, th_pos_cancer)->det_curve(y_true, y_pred_pos_cancer, pos_label='cancer')
A:sklearn.metrics.tests.test_ranking.(fpr_pos_not_cancer, fnr_pos_not_cancer, th_pos_not_cancer)->det_curve(y_true, y_pred_pos_not_cancer, pos_label='not cancer')
A:sklearn.metrics.tests.test_ranking.random_state->check_random_state(random_state)
A:sklearn.metrics.tests.test_ranking.y_score_ties->numpy.zeros_like(y_score)
A:sklearn.metrics.tests.test_ranking.score->top_k_accuracy_score(y_true, y_score, k=k)
A:sklearn.metrics.tests.test_ranking.(unique_rank, inv_rank)->numpy.unique(y_score[i], return_inverse=True)
A:sklearn.metrics.tests.test_ranking.corr_rank->numpy.bincount(rank, minlength=n_ranks + 1).cumsum()
A:sklearn.metrics.tests.test_ranking.n_ranked_above->sum((rank[r] <= rank[label] for r in relevant))
A:sklearn.metrics.tests.test_ranking.(_, y_true)->make_multilabel_classification(random_state=0, n_classes=10)
A:sklearn.metrics.tests.test_ranking.score_lrap->label_ranking_average_precision_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.score_my_lrap->_my_lrap(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.samplewise_lraps->numpy.array([0.5, 0.75, 1.0])
A:sklearn.metrics.tests.test_ranking.(y_true, y_score)->numpy.random.RandomState(0).random_sample((2, 100, 10))
A:sklearn.metrics.tests.test_ranking.discount->numpy.log2(np.arange(y_true.shape[1]) + 2)
A:sklearn.metrics.tests.test_ranking.ideal->_ndcg_sample_scores(y_true, y_true)
A:sklearn.metrics.tests.test_ranking.dcg->_dcg_sample_scores(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.dcg_ignore_ties->_dcg_sample_scores(y_true, y_score, ignore_ties=True)
A:sklearn.metrics.tests.test_ranking.a->numpy.arange(12).reshape((2, 6))
A:sklearn.metrics.tests.test_ranking.ndcg->ndcg_score(y_true, y_score)
A:sklearn.metrics.tests.test_ranking.ndcg_no_ties->ndcg_score(y_true, y_score, ignore_ties=True)
A:sklearn.metrics.tests.test_ranking.expected_dcg_score->(3 / np.log2(np.arange(2, 9))).sum()
A:sklearn.metrics.tests.test_ranking.all_zero->(y_true == 0).all(axis=1)
A:sklearn.metrics.tests.test_ranking.roc_auc_with_max_fpr_one->roc_auc_score(y_true, y_scores, max_fpr=1)
A:sklearn.metrics.tests.test_ranking.unconstrained_roc_auc->roc_auc_score(y_true, y_scores)
A:sklearn.metrics.tests.test_ranking.(y_true, y_pred, _)->make_prediction(binary=True)
A:sklearn.metrics.tests.test_ranking.score_acc->accuracy_score(y_true, y_pred)
A:sklearn.metrics.tests.test_ranking.labels->numpy.asarray(labels)
A:sklearn.metrics.tests.test_ranking.(X, y)->sklearn.datasets.make_classification(n_classes=10, n_samples=1000, n_informative=10, random_state=0)
A:sklearn.metrics.tests.test_ranking.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.metrics.tests.test_ranking.result->metric(y_true, y_proba, pos_label=pos_label)
A:sklearn.metrics.tests.test_ranking.y_proba->numpy.random.RandomState(global_random_seed).rand(n_samples)
A:sklearn.metrics.tests.test_ranking.(_, _, thresholds)->roc_curve(y_true, y_score)
sklearn.metrics.tests.test_ranking._auc(y_true,y_score)
sklearn.metrics.tests.test_ranking._average_precision(y_true,y_score)
sklearn.metrics.tests.test_ranking._average_precision_slow(y_true,y_score)
sklearn.metrics.tests.test_ranking._my_lrap(y_true,y_score)
sklearn.metrics.tests.test_ranking._partial_roc_auc_score(y_true,y_predict,max_fpr)
sklearn.metrics.tests.test_ranking._test_dcg_score_for(y_true,y_score)
sklearn.metrics.tests.test_ranking._test_ndcg_score_for(y_true,y_score)
sklearn.metrics.tests.test_ranking._test_precision_recall_curve(y_true,y_score,drop)
sklearn.metrics.tests.test_ranking.check_alternative_lrap_implementation(lrap_score,n_classes=5,n_samples=20,random_state=0)
sklearn.metrics.tests.test_ranking.check_lrap_error_raised(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_only_ties(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_toy(lrap_score)
sklearn.metrics.tests.test_ranking.check_lrap_without_tie_and_increasing_score(lrap_score)
sklearn.metrics.tests.test_ranking.check_zero_or_all_relevant_labels(lrap_score)
sklearn.metrics.tests.test_ranking.make_prediction(dataset=None,binary=False)
sklearn.metrics.tests.test_ranking.test_alternative_lrap_implementation(n_samples,n_classes,random_state)
sklearn.metrics.tests.test_ranking.test_auc()
sklearn.metrics.tests.test_ranking.test_auc_errors()
sklearn.metrics.tests.test_ranking.test_auc_score_non_binary_class()
sklearn.metrics.tests.test_ranking.test_average_precision_constant_values()
sklearn.metrics.tests.test_ranking.test_average_precision_score_binary_pos_label_errors()
sklearn.metrics.tests.test_ranking.test_average_precision_score_multiclass_pos_label_errors()
sklearn.metrics.tests.test_ranking.test_average_precision_score_multilabel_pos_label_errors()
sklearn.metrics.tests.test_ranking.test_binary_clf_curve_implicit_pos_label(curve_func)
sklearn.metrics.tests.test_ranking.test_binary_clf_curve_multiclass_error(curve_func)
sklearn.metrics.tests.test_ranking.test_binary_clf_curve_zero_sample_weight(curve_func)
sklearn.metrics.tests.test_ranking.test_coverage_1d_error_message(y_true,y_score)
sklearn.metrics.tests.test_ranking.test_coverage_error()
sklearn.metrics.tests.test_ranking.test_coverage_tie_handling()
sklearn.metrics.tests.test_ranking.test_dcg_score()
sklearn.metrics.tests.test_ranking.test_dcg_ties()
sklearn.metrics.tests.test_ranking.test_det_curve_bad_input(y_true,y_pred,err_msg)
sklearn.metrics.tests.test_ranking.test_det_curve_constant_scores(y_score)
sklearn.metrics.tests.test_ranking.test_det_curve_perfect_scores(y_true)
sklearn.metrics.tests.test_ranking.test_det_curve_pos_label()
sklearn.metrics.tests.test_ranking.test_det_curve_sanity_check()
sklearn.metrics.tests.test_ranking.test_det_curve_tie_handling(y_true,y_score,expected_fpr,expected_fnr)
sklearn.metrics.tests.test_ranking.test_det_curve_toydata(y_true,y_score,expected_fpr,expected_fnr)
sklearn.metrics.tests.test_ranking.test_label_ranking_avg_precision_score_should_allow_csr_matrix_for_y_true_input(csr_container)
sklearn.metrics.tests.test_ranking.test_label_ranking_avp(check,func)
sklearn.metrics.tests.test_ranking.test_label_ranking_loss()
sklearn.metrics.tests.test_ranking.test_label_ranking_loss_sparse(csr_container)
sklearn.metrics.tests.test_ranking.test_lrap_error_raised()
sklearn.metrics.tests.test_ranking.test_lrap_sample_weighting_zero_labels()
sklearn.metrics.tests.test_ranking.test_micro_averaged_ovr_roc_auc(global_random_seed)
sklearn.metrics.tests.test_ranking.test_multiclass_ovo_roc_auc_toydata(y_true,labels)
sklearn.metrics.tests.test_ranking.test_multiclass_ovo_roc_auc_toydata_binary(y_true,labels)
sklearn.metrics.tests.test_ranking.test_multiclass_ovr_roc_auc_toydata(y_true,labels)
sklearn.metrics.tests.test_ranking.test_ndcg_error_single_document()
sklearn.metrics.tests.test_ranking.test_ndcg_ignore_ties_with_k()
sklearn.metrics.tests.test_ranking.test_ndcg_invariant()
sklearn.metrics.tests.test_ranking.test_ndcg_negative_ndarray_error()
sklearn.metrics.tests.test_ranking.test_ndcg_score()
sklearn.metrics.tests.test_ranking.test_ndcg_toy_examples(ignore_ties)
sklearn.metrics.tests.test_ranking.test_partial_roc_auc_score()
sklearn.metrics.tests.test_ranking.test_perfect_imperfect_chance_multiclass_roc_auc(multi_class,average)
sklearn.metrics.tests.test_ranking.test_precision_recall_curve(drop)
sklearn.metrics.tests.test_ranking.test_precision_recall_curve_drop_intermediate()
sklearn.metrics.tests.test_ranking.test_precision_recall_curve_toydata(drop)
sklearn.metrics.tests.test_ranking.test_ranking_appropriate_input_shape()
sklearn.metrics.tests.test_ranking.test_ranking_loss_ties_handling()
sklearn.metrics.tests.test_ranking.test_ranking_metric_pos_label_types(metric,classes)
sklearn.metrics.tests.test_ranking.test_roc_auc_score_multiclass_error(msg,kwargs)
sklearn.metrics.tests.test_ranking.test_roc_auc_score_multiclass_labels_error(msg,y_true,labels,multi_class)
sklearn.metrics.tests.test_ranking.test_roc_curve(drop)
sklearn.metrics.tests.test_ranking.test_roc_curve_confidence()
sklearn.metrics.tests.test_ranking.test_roc_curve_drop_intermediate()
sklearn.metrics.tests.test_ranking.test_roc_curve_end_points()
sklearn.metrics.tests.test_ranking.test_roc_curve_fpr_tpr_increasing()
sklearn.metrics.tests.test_ranking.test_roc_curve_hard()
sklearn.metrics.tests.test_ranking.test_roc_curve_multi()
sklearn.metrics.tests.test_ranking.test_roc_curve_one_label()
sklearn.metrics.tests.test_ranking.test_roc_curve_toydata()
sklearn.metrics.tests.test_ranking.test_roc_curve_with_probablity_estimates(global_random_seed)
sklearn.metrics.tests.test_ranking.test_roc_returns_consistency()
sklearn.metrics.tests.test_ranking.test_score_scale_invariance()
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score(y_true,k,true_score)
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_binary(y_score,k,true_score)
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_error(y_true,y_score,labels,msg)
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_increasing()
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_multiclass_with_labels(y_true,true_score,labels,labels_as_ndarray)
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_ties(y_true,k,true_score)
sklearn.metrics.tests.test_ranking.test_top_k_accuracy_score_warning(y_true,k)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_dist_metrics.py----------------------------------------
A:sklearn.metrics.tests.test_dist_metrics.rng->check_random_state(0)
A:sklearn.metrics.tests.test_dist_metrics.X64->check_random_state(0).random_sample((n1, d))
A:sklearn.metrics.tests.test_dist_metrics.Y64->check_random_state(0).random_sample((n2, d))
A:sklearn.metrics.tests.test_dist_metrics.X32->check_random_state(0).random_sample((n1, d)).astype('float32')
A:sklearn.metrics.tests.test_dist_metrics.Y32->check_random_state(0).random_sample((n2, d)).astype('float32')
A:sklearn.metrics.tests.test_dist_metrics.[X_mmap, Y_mmap]->create_memmap_backed_data([X64, Y64])
A:sklearn.metrics.tests.test_dist_metrics.X_bool->(X64 < 0.3).astype(np.float64)
A:sklearn.metrics.tests.test_dist_metrics.Y_bool->(Y64 < 0.7).astype(np.float64)
A:sklearn.metrics.tests.test_dist_metrics.[X_bool_mmap, Y_bool_mmap]->create_memmap_backed_data([X_bool, Y_bool])
A:sklearn.metrics.tests.test_dist_metrics.V->check_random_state(0).random_sample((d, d))
A:sklearn.metrics.tests.test_dist_metrics.VI->check_random_state(0).rand(10, 10)
A:sklearn.metrics.tests.test_dist_metrics.keys->param_grid.keys()
A:sklearn.metrics.tests.test_dist_metrics.kwargs->dict(zip(keys, vals))
A:sklearn.metrics.tests.test_dist_metrics.D_scipy_cdist->cdist(X_bool, Y_bool, metric)
A:sklearn.metrics.tests.test_dist_metrics.dm->sklearn.metrics.DistanceMetric.get_metric('minkowski', p=3, w=w2)
A:sklearn.metrics.tests.test_dist_metrics.D_sklearn->sklearn.metrics.DistanceMetric.get_metric('haversine', X.dtype).pairwise(X, Y_csr)
A:sklearn.metrics.tests.test_dist_metrics.X_csr->csr_container(X)
A:sklearn.metrics.tests.test_dist_metrics.D_scipy_pdist->cdist(X_bool, X_bool, metric)
A:sklearn.metrics.tests.test_dist_metrics.D_sklearn_csr->sklearn.metrics.DistanceMetric.get_metric('minkowski', p=3, w=w2).pairwise(X_csr, X_csr)
A:sklearn.metrics.tests.test_dist_metrics.dm64->sklearn.metrics.DistanceMetric.get_metric(metric, np.float64, **kwargs)
A:sklearn.metrics.tests.test_dist_metrics.dm32->sklearn.metrics.DistanceMetric.get_metric(metric, np.float32, **kwargs)
A:sklearn.metrics.tests.test_dist_metrics.D64->sklearn.metrics.DistanceMetric.get_metric(metric, np.float64, **kwargs).pairwise(X64, Y64)
A:sklearn.metrics.tests.test_dist_metrics.D32->sklearn.metrics.DistanceMetric.get_metric(metric, np.float32, **kwargs).pairwise(X32, Y32)
A:sklearn.metrics.tests.test_dist_metrics.X_bool_csr->csr_container(X_bool)
A:sklearn.metrics.tests.test_dist_metrics.vals->copy.deepcopy(vals)
A:sklearn.metrics.tests.test_dist_metrics.D1->sklearn.metrics.DistanceMetric.get_metric('euclidean').pairwise(X)
A:sklearn.metrics.tests.test_dist_metrics.dm2->pickle.loads(pickle.dumps(dm))
A:sklearn.metrics.tests.test_dist_metrics.D2->sklearn.metrics.DistanceMetric.get_metric('pyfunc', func=custom_metric).pairwise(X)
A:sklearn.metrics.tests.test_dist_metrics.X->check_random_state(0).rand(10, 3)
A:sklearn.metrics.tests.test_dist_metrics.Y->numpy.asarray(Y[:, :2])
A:sklearn.metrics.tests.test_dist_metrics.D_reference->numpy.zeros((X_csr.shape[0], Y_csr.shape[0]))
A:sklearn.metrics.tests.test_dist_metrics.D_reference[i, j]->haversine_slow(xi, yj)
A:sklearn.metrics.tests.test_dist_metrics.haversine->sklearn.metrics.DistanceMetric.get_metric('haversine', X.dtype)
A:sklearn.metrics.tests.test_dist_metrics.euclidean->sklearn.metrics.DistanceMetric.get_metric('euclidean')
A:sklearn.metrics.tests.test_dist_metrics.pyfunc->sklearn.metrics.DistanceMetric.get_metric('pyfunc', func=custom_metric)
A:sklearn.metrics.tests.test_dist_metrics.euclidean_pkl->pickle.loads(pickle.dumps(euclidean))
A:sklearn.metrics.tests.test_dist_metrics.pyfunc_pkl->pickle.loads(pickle.dumps(pyfunc))
A:sklearn.metrics.tests.test_dist_metrics.D1_pkl->pickle.loads(pickle.dumps(euclidean)).pairwise(X)
A:sklearn.metrics.tests.test_dist_metrics.D2_pkl->pickle.loads(pickle.dumps(pyfunc)).pairwise(X)
A:sklearn.metrics.tests.test_dist_metrics.eucl->sklearn.metrics.DistanceMetric.get_metric('euclidean')
A:sklearn.metrics.tests.test_dist_metrics.weights->check_random_state(0).rand(100)
A:sklearn.metrics.tests.test_dist_metrics.w2->check_random_state(0).random_sample(d + 1)
A:sklearn.metrics.tests.test_dist_metrics.generic_type->type(DistanceMetric.get_metric(metric, dtype, **metric_kwargs))
A:sklearn.metrics.tests.test_dist_metrics.specialized_type->type(specialized_cls.get_metric(metric, **metric_kwargs))
sklearn.metrics.tests.test_dist_metrics.dist_func(x1,x2,p)
sklearn.metrics.tests.test_dist_metrics.test_cdist(metric_param_grid,X,Y,csr_container)
sklearn.metrics.tests.test_dist_metrics.test_cdist_bool_metric(metric,X_bool,Y_bool,csr_container)
sklearn.metrics.tests.test_dist_metrics.test_distance_metrics_dtype_consistency(metric_param_grid)
sklearn.metrics.tests.test_dist_metrics.test_get_metric_bad_dtype()
sklearn.metrics.tests.test_dist_metrics.test_get_metric_dtype(metric,metric_kwargs,dtype)
sklearn.metrics.tests.test_dist_metrics.test_haversine_metric(X,Y,csr_container)
sklearn.metrics.tests.test_dist_metrics.test_input_data_size()
sklearn.metrics.tests.test_dist_metrics.test_minkowski_metric_validate_bad_p_parameter()
sklearn.metrics.tests.test_dist_metrics.test_minkowski_metric_validate_weights_size()
sklearn.metrics.tests.test_dist_metrics.test_minkowski_metric_validate_weights_values(w,err_type,err_msg)
sklearn.metrics.tests.test_dist_metrics.test_pdist(metric_param_grid,X,csr_container)
sklearn.metrics.tests.test_dist_metrics.test_pdist_bool_metrics(metric,X_bool,csr_container)
sklearn.metrics.tests.test_dist_metrics.test_pickle(writable_kwargs,metric_param_grid,X)
sklearn.metrics.tests.test_dist_metrics.test_pickle_bool_metrics(metric,X_bool)
sklearn.metrics.tests.test_dist_metrics.test_pyfunc_metric()
sklearn.metrics.tests.test_dist_metrics.test_readonly_kwargs()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_common.py----------------------------------------
A:sklearn.metrics.tests.test_common.(precision, recall, thresholds)->precision_recall_curve(*args, **kwargs)
A:sklearn.metrics.tests.test_common.ALL_METRICS->dict()
A:sklearn.metrics.tests.test_common.METRIC_UNDEFINED_BINARY_MULTICLASS->METRIC_UNDEFINED_BINARY.union(METRIC_UNDEFINED_MULTICLASS)
A:sklearn.metrics.tests.test_common.random_state->check_random_state(0)
A:sklearn.metrics.tests.test_common.y_true->check_random_state(0).randint(0, n_classes, size=n_samples)
A:sklearn.metrics.tests.test_common.y_pred->numpy.vstack([ya, ya])
A:sklearn.metrics.tests.test_common.(y_true, y_pred)->_require_positive_targets(y_true, y_pred)
A:sklearn.metrics.tests.test_common.y_true_bin->check_random_state(0).randint(0, 2, size=(20, 25))
A:sklearn.metrics.tests.test_common.y_pred_bin->check_random_state(0).randint(0, 2, size=(20, 25))
A:sklearn.metrics.tests.test_common.(y_true_shuffle, y_pred_shuffle)->shuffle(y_true, y_pred, random_state=0)
A:sklearn.metrics.tests.test_common.y_score->check_random_state(0).rand(n_samples, n_classes)
A:sklearn.metrics.tests.test_common.(y_true_shuffle, y_pred_shuffle, y_score_shuffle)->shuffle(y_true, y_pred, y_score, random_state=0)
A:sklearn.metrics.tests.test_common.y1->numpy.array(['spam'] * 3 + ['eggs'] * 2, dtype=dtype_y_str)
A:sklearn.metrics.tests.test_common.y2->numpy.random.RandomState(42).randint(0, 2, size=y1.size)
A:sklearn.metrics.tests.test_common.(y1, y2)->_require_positive_targets(y1, y2)
A:sklearn.metrics.tests.test_common.y1_list->list(y1)
A:sklearn.metrics.tests.test_common.y2_list->list(y2)
A:sklearn.metrics.tests.test_common.y1_column->numpy.reshape(y1_1d, (-1, 1))
A:sklearn.metrics.tests.test_common.y2_column->numpy.reshape(y2_1d, (-1, 1))
A:sklearn.metrics.tests.test_common.y1_row->numpy.reshape(y1_1d, (1, -1))
A:sklearn.metrics.tests.test_common.y2_row->numpy.reshape(y2_1d, (1, -1))
A:sklearn.metrics.tests.test_common.measure->metric(y1, y2)
A:sklearn.metrics.tests.test_common.measure_with_number->metric(y1, y2)
A:sklearn.metrics.tests.test_common.metric_str->partial(metric_str, pos_label=pos_label_str)
A:sklearn.metrics.tests.test_common.measure_with_str->metric_str(y1_str, y2)
A:sklearn.metrics.tests.test_common.measure_with_strobj->metric_str(y1_str.astype('O'), y2)
A:sklearn.metrics.tests.test_common.error->metric(y_true, y_pred)
A:sklearn.metrics.tests.test_common.perm->check_random_state(0).permutation(y_true.shape[1])
A:sklearn.metrics.tests.test_common.(_, y1)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, n_samples=n_samples, allow_unlabeled=True)
A:sklearn.metrics.tests.test_common.(_, y2)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, n_samples=n_samples, allow_unlabeled=True)
A:sklearn.metrics.tests.test_common.y1_sparse_indicator->coo_container(y1)
A:sklearn.metrics.tests.test_common.y2_sparse_indicator->coo_container(y2)
A:sklearn.metrics.tests.test_common.y1_list_array_indicator->list(y1)
A:sklearn.metrics.tests.test_common.y2_list_array_indicator->list(y2)
A:sklearn.metrics.tests.test_common.measure_normalized->metrics(y_true, pred, normalize=True)
A:sklearn.metrics.tests.test_common.measure_not_normalized->metrics(y_true, pred, normalize=False)
A:sklearn.metrics.tests.test_common.(_, y_true)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=0, allow_unlabeled=True, n_samples=n_samples)
A:sklearn.metrics.tests.test_common.(_, y_pred)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=1, allow_unlabeled=True, n_samples=n_samples)
A:sklearn.metrics.tests.test_common.label_measure->metric(y_true, y_pred, average=None)
A:sklearn.metrics.tests.test_common.micro_measure->metric(y_true, y_pred, average='micro')
A:sklearn.metrics.tests.test_common.macro_measure->metric(y_true, y_pred, average='macro')
A:sklearn.metrics.tests.test_common.weights->numpy.sum(y_true_binarize, axis=0, dtype=int)
A:sklearn.metrics.tests.test_common.weighted_measure->metric(y_true, y_pred, average='weighted')
A:sklearn.metrics.tests.test_common.sample_measure->metric(y_true, y_pred, average='samples')
A:sklearn.metrics.tests.test_common.is_multilabel->type_of_target(y_true).startswith('multilabel')
A:sklearn.metrics.tests.test_common.lb->LabelBinarizer().fit(y_true)
A:sklearn.metrics.tests.test_common.y_true_binarize->LabelBinarizer().fit(y_true).transform(y_true)
A:sklearn.metrics.tests.test_common.y_pred_binarize->LabelBinarizer().fit(y_true).transform(y_pred)
A:sklearn.metrics.tests.test_common.(_, y)->make_multilabel_classification(n_features=1, n_classes=n_classes, random_state=5, n_samples=n_samples, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.rng->numpy.random.RandomState(42)
A:sklearn.metrics.tests.test_common.sample_weight->numpy.array([0.0, 0.1, 2.0, 1.0], dtype=dtype_name)
A:sklearn.metrics.tests.test_common.unweighted_score->metric(y1, y2, sample_weight=None)
A:sklearn.metrics.tests.test_common.weighted_score->metric(y1, y2, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_common.weighted_score_list->metric(y1, y2, sample_weight=sample_weight.tolist())
A:sklearn.metrics.tests.test_common.repeat_weighted_score->metric(np.repeat(y1, sample_weight, axis=0), np.repeat(y2, sample_weight, axis=0), sample_weight=None)
A:sklearn.metrics.tests.test_common.sample_weight_zeroed->numpy.copy(sample_weight)
A:sklearn.metrics.tests.test_common.weighted_score_subset->metric(y1_subset, y2_subset, sample_weight=sample_weight_subset)
A:sklearn.metrics.tests.test_common.weighted_score_zeroed->metric(y1, y2, sample_weight=sample_weight_zeroed)
A:sklearn.metrics.tests.test_common.error_message->'Found input variables with inconsistent numbers of samples: \\[{}, {}, {}\\]'.format(_num_samples(y1), _num_samples(y2), _num_samples(sample_weight) * 2)
A:sklearn.metrics.tests.test_common.temp->numpy.exp(-y_score)
A:sklearn.metrics.tests.test_common.(_, ya)->make_multilabel_classification(n_features=1, n_classes=10, random_state=0, n_samples=50, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.(_, yb)->make_multilabel_classification(n_features=1, n_classes=10, random_state=1, n_samples=50, allow_unlabeled=False)
A:sklearn.metrics.tests.test_common.y_true_multilabel->numpy.array([[1, 1, 0, 0], [1, 1, 0, 0]])
A:sklearn.metrics.tests.test_common.y_pred_multilabel->numpy.array([[0, 0, 1, 1], [0, 1, 1, 0]])
A:sklearn.metrics.tests.test_common.y_true_multiclass->numpy.array([0, 1, 2])
A:sklearn.metrics.tests.test_common.y_pred_multiclass->numpy.array([0, 2, 3])
A:sklearn.metrics.tests.test_common.labels->numpy.array([3, 0, 1, 2])
A:sklearn.metrics.tests.test_common.(_, inverse_labels)->numpy.unique(labels, return_inverse=True)
A:sklearn.metrics.tests.test_common.score_labels->metric(y_true, y_pred, labels=labels, average=None)
A:sklearn.metrics.tests.test_common.score->metric(y_true, y_score)
A:sklearn.metrics.tests.test_common.current_score->metric(y_true_perm, y_score_perm)
A:sklearn.metrics.tests.test_common.inverse_perm->numpy.zeros(n_classes, dtype=int)
A:sklearn.metrics.tests.test_common.inverse_perm[list(perm)]->numpy.arange(n_classes)
A:sklearn.metrics.tests.test_common.y_true_perm->numpy.take(perm, y_true)
A:sklearn.metrics.tests.test_common.xp->_array_api_for_tests(array_namespace, device)
A:sklearn.metrics.tests.test_common.y_true_xp->_array_api_for_tests(array_namespace, device).asarray(y_true_np, device=device)
A:sklearn.metrics.tests.test_common.y_pred_xp->_array_api_for_tests(array_namespace, device).asarray(y_pred_np, device=device)
A:sklearn.metrics.tests.test_common.metric_np->metric(y_true_np, y_pred_np, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_common.metric_xp->metric(y_true_xp, y_pred_xp, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_common.y_true_np->numpy.array([0, 1, 2, 3])
A:sklearn.metrics.tests.test_common.y_pred_np->numpy.array([0, 1, 0, 2])
sklearn.metrics.tests.test_common._check_averaging(metric,y_true,y_pred,y_true_binarize,y_pred_binarize,is_multilabel)
sklearn.metrics.tests.test_common._require_positive_targets(y1,y2)
sklearn.metrics.tests.test_common.check_array_api_binary_classification_metric(metric,array_namespace,device,dtype_name)
sklearn.metrics.tests.test_common.check_array_api_metric(metric,array_namespace,device,dtype_name,y_true_np,y_pred_np,sample_weight)
sklearn.metrics.tests.test_common.check_array_api_multiclass_classification_metric(metric,array_namespace,device,dtype_name)
sklearn.metrics.tests.test_common.check_averaging(name,y_true,y_true_binarize,y_pred,y_pred_binarize,y_score)
sklearn.metrics.tests.test_common.check_sample_weight_invariance(name,metric,y1,y2)
sklearn.metrics.tests.test_common.check_single_sample(name)
sklearn.metrics.tests.test_common.check_single_sample_multioutput(name)
sklearn.metrics.tests.test_common.precision_recall_curve_padded_thresholds(*args,**kwargs)
sklearn.metrics.tests.test_common.test_array_api_compliance(metric,array_namespace,device,dtype_name,check_func)
sklearn.metrics.tests.test_common.test_averaging_binary_multilabel_all_zeroes()
sklearn.metrics.tests.test_common.test_averaging_multiclass(name)
sklearn.metrics.tests.test_common.test_averaging_multilabel(name)
sklearn.metrics.tests.test_common.test_averaging_multilabel_all_ones(name)
sklearn.metrics.tests.test_common.test_averaging_multilabel_all_zeroes(name)
sklearn.metrics.tests.test_common.test_binary_sample_weight_invariance(name)
sklearn.metrics.tests.test_common.test_classification_binary_continuous_input(metric)
sklearn.metrics.tests.test_common.test_classification_inf_nan_input(metric,y_true,y_score)
sklearn.metrics.tests.test_common.test_classification_invariance_string_vs_numbers_labels(name)
sklearn.metrics.tests.test_common.test_format_invariance_with_1d_vectors(name)
sklearn.metrics.tests.test_common.test_metrics_consistent_type_error(metric_name)
sklearn.metrics.tests.test_common.test_metrics_pos_label_error_str(metric,y_pred_threshold,dtype_y_str)
sklearn.metrics.tests.test_common.test_multiclass_sample_weight_invariance(name)
sklearn.metrics.tests.test_common.test_multilabel_label_permutations_invariance(name)
sklearn.metrics.tests.test_common.test_multilabel_representation_invariance(coo_container)
sklearn.metrics.tests.test_common.test_multilabel_sample_weight_invariance(name)
sklearn.metrics.tests.test_common.test_multioutput_number_of_output_differ(name)
sklearn.metrics.tests.test_common.test_multioutput_regression_invariance_to_dimension_shuffling(name)
sklearn.metrics.tests.test_common.test_no_averaging_labels()
sklearn.metrics.tests.test_common.test_normalize_option_binary_classification(name)
sklearn.metrics.tests.test_common.test_normalize_option_multiclass_classification(name)
sklearn.metrics.tests.test_common.test_normalize_option_multilabel_classification(name)
sklearn.metrics.tests.test_common.test_not_symmetric_metric(name)
sklearn.metrics.tests.test_common.test_raise_value_error_multilabel_sequences(name)
sklearn.metrics.tests.test_common.test_regression_sample_weight_invariance(name)
sklearn.metrics.tests.test_common.test_regression_thresholded_inf_nan_input(metric,y_true,y_score)
sklearn.metrics.tests.test_common.test_sample_order_invariance(name)
sklearn.metrics.tests.test_common.test_sample_order_invariance_multilabel_and_multioutput()
sklearn.metrics.tests.test_common.test_single_sample(name)
sklearn.metrics.tests.test_common.test_single_sample_multioutput(name)
sklearn.metrics.tests.test_common.test_symmetric_metric(name)
sklearn.metrics.tests.test_common.test_symmetry_consistency()
sklearn.metrics.tests.test_common.test_thresholded_invariance_string_vs_numbers_labels(name)
sklearn.metrics.tests.test_common.test_thresholded_metric_permutation_invariance(name)
sklearn.metrics.tests.test_common.test_thresholded_multilabel_multioutput_permutations_invariance(name)
sklearn.metrics.tests.test_common.yield_metric_checker_combinations(metric_checkers=array_api_metric_checkers)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_regression.py----------------------------------------
A:sklearn.metrics.tests.test_regression.y_true->numpy.array([[0.5, 1], [1, 2], [7, 6]])
A:sklearn.metrics.tests.test_regression.mape->mean_absolute_percentage_error(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.dev_median->numpy.abs(y_true - np.median(y_true)).sum()
A:sklearn.metrics.tests.test_regression.y_quantile->numpy.percentile(y_true, q=alpha * 100)
A:sklearn.metrics.tests.test_regression.mse->mean_squared_error(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.rmse->root_mean_squared_error([[1]], [[10]], multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.y_pred->numpy.array([[0.5, 2], [1, 2.5], [8, 8]])
A:sklearn.metrics.tests.test_regression.error->r2_score(yc, [5.0, 6.0], multioutput='variance_weighted', force_finite=False)
A:sklearn.metrics.tests.test_regression.score->metric(y_true, y_pred)
A:sklearn.metrics.tests.test_regression.raw_expected_score->numpy.where(np.isnan(raw_expected_score), 1, raw_expected_score)
A:sklearn.metrics.tests.test_regression.error2->r2_score(y_true, y_pred, multioutput='uniform_average', force_finite=False)
A:sklearn.metrics.tests.test_regression.(y_type, y_check1, y_check2, multioutput)->_check_reg_targets(y1, y2, None)
A:sklearn.metrics.tests.test_regression.expected_message->"Allowed 'multioutput' string values are.+You provided multioutput={!r}".format(invalid_multioutput)
A:sklearn.metrics.tests.test_regression.mae->mean_absolute_error(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.pbl->mean_pinball_loss(data, constant_pred, alpha=target_quantile)
A:sklearn.metrics.tests.test_regression.r->r2_score([[0, -1], [0, 1]], [[2, 2], [1, 1]], multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.evs->explained_variance_score(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.d2ps->d2_pinball_score(y_true, y_pred, alpha=0.5, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.evs2->explained_variance_score(y_true, y_pred, multioutput='raw_values', force_finite=False)
A:sklearn.metrics.tests.test_regression.r2->r2_score(y_true, y_pred, multioutput='raw_values')
A:sklearn.metrics.tests.test_regression.r22->r2_score(y_true, y_pred, multioutput='raw_values', force_finite=False)
A:sklearn.metrics.tests.test_regression.msle->mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])
A:sklearn.metrics.tests.test_regression.msle2->mean_squared_error(np.log(1 + y_true), np.log(1 + y_pred), multioutput=[0.3, 0.7])
A:sklearn.metrics.tests.test_regression.msew->mean_squared_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.rmsew->root_mean_squared_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.maew->mean_absolute_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.mapew->mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.rw->r2_score(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.evsw->explained_variance_score(y_true, y_pred, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.d2psw->d2_pinball_score(y_true, y_pred, alpha=0.5, multioutput=[0.4, 0.6])
A:sklearn.metrics.tests.test_regression.evsw2->explained_variance_score(y_true, y_pred, multioutput=[0.4, 0.6], force_finite=False)
A:sklearn.metrics.tests.test_regression.random_number_generator->numpy.random.RandomState(42)
A:sklearn.metrics.tests.test_regression.rng->numpy.random.RandomState(714)
A:sklearn.metrics.tests.test_regression.data->getattr(rng, distribution)(size=n_samples)
A:sklearn.metrics.tests.test_regression.best_pred->numpy.quantile(data, target_quantile)
A:sklearn.metrics.tests.test_regression.best_constant_pred->numpy.full(n_samples, fill_value=best_pred)
A:sklearn.metrics.tests.test_regression.best_pbl->mean_pinball_loss(data, best_constant_pred, alpha=target_quantile)
A:sklearn.metrics.tests.test_regression.candidate_predictions->numpy.quantile(data, np.linspace(0, 1, 100))
A:sklearn.metrics.tests.test_regression.constant_pred->numpy.full(n_samples, fill_value=x)
A:sklearn.metrics.tests.test_regression.result->scipy.optimize.minimize(objective_func, data.mean(), method='Nelder-Mead')
A:sklearn.metrics.tests.test_regression.X->numpy.random.RandomState(714).normal(size=(n_samples, 5))
A:sklearn.metrics.tests.test_regression.y->numpy.random.RandomState(714).exponential(size=n_samples)
A:sklearn.metrics.tests.test_regression.neg_mean_pinball_loss->make_scorer(mean_pinball_loss, alpha=alpha, greater_is_better=False)
A:sklearn.metrics.tests.test_regression.regressor->DummyRegressor(strategy='quantile', quantile=0.25)
A:sklearn.metrics.tests.test_regression.grid_search->GridSearchCV(regressor, param_grid=dict(quantile=all_quantiles), scoring=neg_mean_pinball_loss).fit(X, y)
A:sklearn.metrics.tests.test_regression.sw->numpy.arange(len(y_true))
A:sklearn.metrics.tests.test_regression.expected->old_func(y_true, y_pred, sample_weight=sw, multioutput='raw_values', squared=False)
A:sklearn.metrics.tests.test_regression.actual->new_func(y_true, y_pred, sample_weight=sw, multioutput='raw_values')
sklearn.metrics.tests.test_regression.test__check_reg_targets()
sklearn.metrics.tests.test_regression.test__check_reg_targets_exception()
sklearn.metrics.tests.test_regression.test_dummy_quantile_parameter_tuning()
sklearn.metrics.tests.test_regression.test_mean_absolute_percentage_error()
sklearn.metrics.tests.test_regression.test_mean_pinball_loss_on_constant_predictions(distribution,target_quantile)
sklearn.metrics.tests.test_regression.test_mean_squared_deprecation_squared(metric)
sklearn.metrics.tests.test_regression.test_multioutput_regression()
sklearn.metrics.tests.test_regression.test_pinball_loss_relation_with_mae()
sklearn.metrics.tests.test_regression.test_regression_custom_weights()
sklearn.metrics.tests.test_regression.test_regression_metrics(n_samples=50)
sklearn.metrics.tests.test_regression.test_regression_metrics_at_limits()
sklearn.metrics.tests.test_regression.test_regression_multioutput_array()
sklearn.metrics.tests.test_regression.test_regression_single_sample(metric)
sklearn.metrics.tests.test_regression.test_rmse_rmsle_parameter(old_func,new_func)
sklearn.metrics.tests.test_regression.test_root_mean_squared_error_multioutput_raw_value()
sklearn.metrics.tests.test_regression.test_tweedie_deviance_continuity()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_pairwise.py----------------------------------------
A:sklearn.metrics.tests.test_pairwise.rng->numpy.random.RandomState(0)
A:sklearn.metrics.tests.test_pairwise.X->numpy.random.RandomState(0).random_sample((5, 4)).astype(global_dtype, copy=False)
A:sklearn.metrics.tests.test_pairwise.S->pairwise_distances(X, Y, metric=metric)
A:sklearn.metrics.tests.test_pairwise.S2->paired_distances(X, Y, metric=lambda x, y: np.abs(x - y).sum(axis=0))
A:sklearn.metrics.tests.test_pairwise.Y->numpy.random.RandomState(0).random_sample((5, 4)).astype(global_dtype, copy=False)
A:sklearn.metrics.tests.test_pairwise.X_masked->numpy.random.RandomState(0).random_sample((5, 4)).astype(global_dtype, copy=False)
A:sklearn.metrics.tests.test_pairwise.Y_masked->numpy.random.RandomState(0).random_sample((2, 4)).astype(global_dtype, copy=False)
A:sklearn.metrics.tests.test_pairwise.S_masked->pairwise_distances(X_masked, Y_masked, metric='nan_euclidean')
A:sklearn.metrics.tests.test_pairwise.S2_masked->nan_euclidean_distances(X_masked, Y_masked)
A:sklearn.metrics.tests.test_pairwise.X_tuples->tuple([tuple([v for v in row]) for row in X])
A:sklearn.metrics.tests.test_pairwise.Y_tuples->tuple([tuple([v for v in row]) for row in Y])
A:sklearn.metrics.tests.test_pairwise.X_sparse->csr_container(X)
A:sklearn.metrics.tests.test_pairwise.Y_sparse->csr_container(Y)
A:sklearn.metrics.tests.test_pairwise.res->pairwise_distances(X, Z, metric=metric)
A:sklearn.metrics.tests.test_pairwise.K->laplacian_kernel(X, X)
A:sklearn.metrics.tests.test_pairwise.K1->pairwise_kernels(X_, Y=Y_, metric='cosine')
A:sklearn.metrics.tests.test_pairwise.K2->pairwise_kernels(X_, Y=Y_, metric='linear')
A:sklearn.metrics.tests.test_pairwise.S3->func(csr_container(X), csr_container(Y))
A:sklearn.metrics.tests.test_pairwise.distances->euclidean_distances(X, Y)
A:sklearn.metrics.tests.test_pairwise.Xsp->dok_container(X)
A:sklearn.metrics.tests.test_pairwise.Ysp->csr_container(Y, dtype=global_dtype)
A:sklearn.metrics.tests.test_pairwise.(idx, vals)->pairwise_distances_argmin_min(X, Y, metric='minkowski', metric_kwargs={'p': 2})
A:sklearn.metrics.tests.test_pairwise.idx2->pairwise_distances_argmin(X, Y, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.(idxsp, valssp)->pairwise_distances_argmin_min(Xsp, Ysp, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.idxsp2->pairwise_distances_argmin(Xsp, Ysp, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.(idx2, vals2)->pairwise_distances_argmin_min(X, Y, metric='euclidean', metric_kwargs={'squared': True})
A:sklearn.metrics.tests.test_pairwise.idx3->pairwise_distances_argmin(X, Y, metric='sqeuclidean')
A:sklearn.metrics.tests.test_pairwise.idx4->pairwise_distances_argmin(X, Y, metric='euclidean', metric_kwargs={'squared': True})
A:sklearn.metrics.tests.test_pairwise.dist->pairwise_distances(X, Y, metric=metric, **params)
A:sklearn.metrics.tests.test_pairwise.dist_orig_ind->pairwise_distances(X, Y, metric=metric, **params).argmin(axis=0)
A:sklearn.metrics.tests.test_pairwise.(dist_chunked_ind, dist_chunked_val)->pairwise_distances_argmin_min(X, Y, axis=0, metric='manhattan')
A:sklearn.metrics.tests.test_pairwise.(argmin_0, dist_0)->pairwise_distances_argmin_min(X, X, axis=0)
A:sklearn.metrics.tests.test_pairwise.(argmin_1, dist_1)->pairwise_distances_argmin_min(X, X, axis=1)
A:sklearn.metrics.tests.test_pairwise.argmin_0->pairwise_distances_argmin(X, X, axis=0)
A:sklearn.metrics.tests.test_pairwise.argmin_1->pairwise_distances_argmin(X, X, axis=1)
A:sklearn.metrics.tests.test_pairwise.argmin_C_contiguous->pairwise_distances_argmin(X, Y)
A:sklearn.metrics.tests.test_pairwise.argmin_F_contiguous->pairwise_distances_argmin(np.asfortranarray(X), np.asfortranarray(Y))
A:sklearn.metrics.tests.test_pairwise.S_chunks->pairwise_distances_chunked(X, None, reduce_func=bad_reduce, working_memory=64)
A:sklearn.metrics.tests.test_pairwise.gen->pairwise_distances_chunked(D, working_memory=2 ** (-16), metric='precomputed')
A:sklearn.metrics.tests.test_pairwise.blockwise_distances->numpy.vstack(blockwise_distances)
A:sklearn.metrics.tests.test_pairwise.chunks->list(pairwise_distances_chunked(X, working_memory=1, metric=metric))
A:sklearn.metrics.tests.test_pairwise.D->paired_cosine_distances(X, Y)
A:sklearn.metrics.tests.test_pairwise.X_norm_sq->(X.astype(np.float32) ** 2).sum(axis=1).reshape(1, -1)
A:sklearn.metrics.tests.test_pairwise.Y_norm_sq->(Y.astype(np.float32) ** 2).sum(axis=1).reshape(1, -1)
A:sklearn.metrics.tests.test_pairwise.D1->numpy.array([[slow_haversine_distances(x, y) for y in Y] for x in X])
A:sklearn.metrics.tests.test_pairwise.D2->haversine_distances(X, Y)
A:sklearn.metrics.tests.test_pairwise.D3->nan_euclidean_distances(X, missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.D4->nan_euclidean_distances(X, X, missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.wrong_D->euclidean_distances(X, Y, X_norm_squared=np.zeros_like(X_norm_sq), Y_norm_squared=np.zeros_like(Y_norm_sq))
A:sklearn.metrics.tests.test_pairwise.X_norm_squared->(X ** 2).sum(axis=1)
A:sklearn.metrics.tests.test_pairwise.Y_norm_squared->(Y ** 2).sum(axis=1)
A:sklearn.metrics.tests.test_pairwise.expected->cdist(X, Y)
A:sklearn.metrics.tests.test_pairwise.normal_distance->euclidean_distances(X, Y=Y, squared=squared)
A:sklearn.metrics.tests.test_pairwise.nan_distance->nan_euclidean_distances(X, Y=Y, squared=squared)
A:sklearn.metrics.tests.test_pairwise.exp_dist->numpy.array([[np.nan, np.nan], [np.nan, 0]])
A:sklearn.metrics.tests.test_pairwise.dist_sq->nan_euclidean_distances(X, squared=True, missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.dist_two->nan_euclidean_distances(X, X, missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.dist_two_copy->nan_euclidean_distances(X, X.copy(), missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.D5->nan_euclidean_distances(X, X.copy(), missing_values=missing_value)
A:sklearn.metrics.tests.test_pairwise.D6->nan_euclidean_distances(X, Y, copy=True)
A:sklearn.metrics.tests.test_pairwise.D7->nan_euclidean_distances(X, Y, copy=False)
A:sklearn.metrics.tests.test_pairwise.dist_squared->nan_euclidean_distances(X, missing_values=missing_value, squared=True)
A:sklearn.metrics.tests.test_pairwise.x->numpy.abs(rng.rand(910))
A:sklearn.metrics.tests.test_pairwise.XA->numpy.resize(np.arange(40), (5, 8)).astype(np.float32)
A:sklearn.metrics.tests.test_pairwise.XB->numpy.resize(np.arange(40), (5, 8)).astype(np.float32)
A:sklearn.metrics.tests.test_pairwise.K_add->additive_chi2_kernel(X, Y)
A:sklearn.metrics.tests.test_pairwise.chi2_exp->numpy.exp(gamma * chi2)
A:sklearn.metrics.tests.test_pairwise.Xcsr->csr_container(X)
A:sklearn.metrics.tests.test_pairwise.Ycsr->csr_container(Y)
A:sklearn.metrics.tests.test_pairwise.K3->pairwise_kernels(X, Y=Y, metric=metric)
A:sklearn.metrics.tests.test_pairwise.X_->normalize(X_)
A:sklearn.metrics.tests.test_pairwise.Y_->normalize(Y_)
A:sklearn.metrics.tests.test_pairwise.(XA_checked, XB_checked)->check_pairwise_arrays(XA, XB.astype(float))
A:sklearn.metrics.tests.test_pairwise.XA_sparse->csr_container(XA)
A:sklearn.metrics.tests.test_pairwise.XB_sparse->csr_container(XB)
A:sklearn.metrics.tests.test_pairwise.(XA_checked, XA_2_checked)->check_pairwise_arrays(XA_sparse, XA_sparse)
A:sklearn.metrics.tests.test_pairwise.XA_tuples->tuplify(XA)
A:sklearn.metrics.tests.test_pairwise.XB_tuples->tuplify(XB)
A:sklearn.metrics.tests.test_pairwise.expected_dist->cdist(X, Y, metric=metric)
sklearn.metrics.tests.test_pairwise._reduce_func(dist,start)
sklearn.metrics.tests.test_pairwise.callable_rbf_kernel(x,y,**kwds)
sklearn.metrics.tests.test_pairwise.check_pairwise_distances_chunked(X,Y,working_memory,metric='euclidean')
sklearn.metrics.tests.test_pairwise.test_check_XB_returned()
sklearn.metrics.tests.test_pairwise.test_check_dense_matrices()
sklearn.metrics.tests.test_pairwise.test_check_different_dimensions()
sklearn.metrics.tests.test_pairwise.test_check_invalid_dimensions()
sklearn.metrics.tests.test_pairwise.test_check_preserve_type()
sklearn.metrics.tests.test_pairwise.test_check_sparse_arrays(csr_container)
sklearn.metrics.tests.test_pairwise.test_check_tuple_input()
sklearn.metrics.tests.test_pairwise.test_chi_square_kernel()
sklearn.metrics.tests.test_pairwise.test_cosine_distances()
sklearn.metrics.tests.test_pairwise.test_cosine_similarity(csr_container)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances(global_dtype,x_array_constr,y_array_constr)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_extreme_values(dtype,eps,rtol,dim)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_float32_norms(global_random_seed,symmetric)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_known_result(x_array_constr,y_array_constr)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_norm_shapes()
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_sym(global_dtype,x_array_constr)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_upcast(batch_size,x_array_constr,y_array_constr)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_upcast_sym(batch_size,x_array_constr)
sklearn.metrics.tests.test_pairwise.test_euclidean_distances_with_norms(global_dtype,y_array_constr)
sklearn.metrics.tests.test_pairwise.test_haversine_distances()
sklearn.metrics.tests.test_pairwise.test_kernel_sparse(kernel,csr_container)
sklearn.metrics.tests.test_pairwise.test_kernel_symmetry(kernel)
sklearn.metrics.tests.test_pairwise.test_laplacian_kernel()
sklearn.metrics.tests.test_pairwise.test_linear_kernel()
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_2x2(X,X_diag,missing_value)
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_complete_nan(missing_value)
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_equal_to_euclidean_distance(squared)
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_infinite_values(X,Y)
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_not_trival(missing_value)
sklearn.metrics.tests.test_pairwise.test_nan_euclidean_distances_one_feature_match_positive(missing_value)
sklearn.metrics.tests.test_pairwise.test_no_data_conversion_warning()
sklearn.metrics.tests.test_pairwise.test_numeric_pairwise_distances_datatypes(metric,global_dtype,y_is_x)
sklearn.metrics.tests.test_pairwise.test_paired_cosine_distances()
sklearn.metrics.tests.test_pairwise.test_paired_distances(metric,func,csr_container)
sklearn.metrics.tests.test_pairwise.test_paired_distances_callable(global_dtype)
sklearn.metrics.tests.test_pairwise.test_paired_euclidean_distances()
sklearn.metrics.tests.test_pairwise.test_paired_manhattan_distances()
sklearn.metrics.tests.test_pairwise.test_pairwise_boolean_distance(metric)
sklearn.metrics.tests.test_pairwise.test_pairwise_callable_nonstrict_metric()
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_argmin_min(dok_container,csr_container,global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked(global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked_diagonal(metric,global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked_reduce(global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked_reduce_invalid(global_dtype,bad_reduce,err_type,message)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked_reduce_none(global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_chunked_reduce_valid(good_reduce)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_data_derived_params(n_jobs,metric,dist_function)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_data_derived_params_error(metric)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_for_dense_data(global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_distances_for_sparse_data(coo_container,csc_container,bsr_container,csr_container,global_dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_kernels(metric,csr_container)
sklearn.metrics.tests.test_pairwise.test_pairwise_kernels_callable()
sklearn.metrics.tests.test_pairwise.test_pairwise_kernels_filter_param()
sklearn.metrics.tests.test_pairwise.test_pairwise_parallel(func,metric,kwds,dtype)
sklearn.metrics.tests.test_pairwise.test_pairwise_precomputed(func)
sklearn.metrics.tests.test_pairwise.test_pairwise_precomputed_non_negative()
sklearn.metrics.tests.test_pairwise.test_pairwise_similarity_sparse_output(metric,pairwise_func,csr_container)
sklearn.metrics.tests.test_pairwise.test_parallel_pairwise_distances_diagonal(metric,global_dtype)
sklearn.metrics.tests.test_pairwise.test_rbf_kernel()
sklearn.metrics.tests.test_pairwise.test_sparse_manhattan_readonly_dataset(csr_container)
sklearn.metrics.tests.test_pairwise.tuplify(X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_score_objects.py----------------------------------------
A:sklearn.metrics.tests.test_score_objects.sensible_regr->DecisionTreeRegressor(random_state=0)
A:sklearn.metrics.tests.test_score_objects.sensible_clf->DecisionTreeClassifier(random_state=0)
A:sklearn.metrics.tests.test_score_objects.sensible_ml_clf->DecisionTreeClassifier(random_state=0)
A:sklearn.metrics.tests.test_score_objects.TEMP_FOLDER->tempfile.mkdtemp(prefix='sklearn_test_score_objects_')
A:sklearn.metrics.tests.test_score_objects.(X, y)->make_classification(n_samples=150, n_features=10, random_state=0)
A:sklearn.metrics.tests.test_score_objects.(_, y_ml)->make_multilabel_classification(n_samples=X.shape[0], random_state=0)
A:sklearn.metrics.tests.test_score_objects.filename->os.path.join(TEMP_FOLDER, 'test_data.pkl')
A:sklearn.metrics.tests.test_score_objects.(X_mm, y_mm, y_ml_mm)->joblib.load(filename, mmap_mode='r')
A:sklearn.metrics.tests.test_score_objects.ESTIMATORS->_make_estimators(X_mm, y_mm, y_ml_mm)
A:sklearn.metrics.tests.test_score_objects.estimator->KNeighborsClassifier().fit(X_train, Y_train)
A:sklearn.metrics.tests.test_score_objects.scorer->make_scorer(score)
A:sklearn.metrics.tests.test_score_objects.scorers->_check_multimetric_scoring(estimator, scoring)
A:sklearn.metrics.tests.test_score_objects.grid->GridSearchCV(LinearSVC(dual='auto'), param_grid={'C': [0.1, 1]}, cv=3)
A:sklearn.metrics.tests.test_score_objects.pipe->make_pipeline(LinearSVC(dual='auto'))
A:sklearn.metrics.tests.test_score_objects.scores->cross_val_score(EstimatorWithFit(), [[1], [2], [3]], [1, 0, 1], scoring=DummyScorer(), cv=3)
A:sklearn.metrics.tests.test_score_objects.(X_train, X_test, y_train, y_test)->train_test_split(X, y, stratify=y, random_state=0)
A:sklearn.metrics.tests.test_score_objects.clf->DecisionTreeClassifier().fit(X, y)
A:sklearn.metrics.tests.test_score_objects.score->get_scorer('average_precision')(estimator, X_test, Y_test)
A:sklearn.metrics.tests.test_score_objects.expected_score->roc_auc_score(y_binary, y_proba, multi_class='ovo', labels=[0, 1, 2])
A:sklearn.metrics.tests.test_score_objects.score1->get_scorer(name)(km, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects.unpickled_scorer->pickle.loads(pickle.dumps(scorer))
A:sklearn.metrics.tests.test_score_objects.score2->getattr(cluster_module, name)(y_test, km.predict(X_test))
A:sklearn.metrics.tests.test_score_objects.diabetes->load_diabetes()
A:sklearn.metrics.tests.test_score_objects.score3->roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
A:sklearn.metrics.tests.test_score_objects.logscore->get_scorer('neg_log_loss')(clf, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects.logloss->log_loss(y_test, clf.predict_proba(X_test))
A:sklearn.metrics.tests.test_score_objects.reg->DecisionTreeRegressor(random_state=0)
A:sklearn.metrics.tests.test_score_objects.y_proba->LogisticRegression().fit(X, y).predict_proba(X)
A:sklearn.metrics.tests.test_score_objects.km->KMeans(n_clusters=3, n_init='auto')
A:sklearn.metrics.tests.test_score_objects.f1_scorer_no_average->make_scorer(f1_score, average=None)
A:sklearn.metrics.tests.test_score_objects.grid_search->GridSearchCV(clf, scoring=f1_scorer_no_average, param_grid={'max_depth': [1, 2]})
A:sklearn.metrics.tests.test_score_objects.split->train_test_split(X, y, y_ml, random_state=0)
A:sklearn.metrics.tests.test_score_objects.sample_weight->numpy.ones_like(y_test)
A:sklearn.metrics.tests.test_score_objects.weighted->scorer(reg, X_test, y_test, sample_weight=sample_weight)
A:sklearn.metrics.tests.test_score_objects.ignored->scorer(reg, X_test[11:], y_test[11:])
A:sklearn.metrics.tests.test_score_objects.unweighted->scorer(reg, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects._->scorer(estimator[name], X_test[:10], target[:10], sample_weight=None)
A:sklearn.metrics.tests.test_score_objects.y->numpy.array(['cancer' if c == 1 else 'not cancer' for c in y], dtype=object)
A:sklearn.metrics.tests.test_score_objects.y_mm_1->_require_positive_y(y_mm)
A:sklearn.metrics.tests.test_score_objects.y_ml_mm_1->_require_positive_y(y_ml_mm)
A:sklearn.metrics.tests.test_score_objects.mock_est->Mock()
A:sklearn.metrics.tests.test_score_objects.fit_func->Mock(return_value=mock_est, name='fit')
A:sklearn.metrics.tests.test_score_objects.predict_func->Mock(return_value=y, name='predict')
A:sklearn.metrics.tests.test_score_objects.pos_proba->numpy.random.rand(X.shape[0])
A:sklearn.metrics.tests.test_score_objects.predict_proba_func->Mock(return_value=proba, name='predict_proba')
A:sklearn.metrics.tests.test_score_objects.decision_function_func->Mock(return_value=pos_proba, name='decision_function')
A:sklearn.metrics.tests.test_score_objects.mock_est.classes_->numpy.array([0, 1])
A:sklearn.metrics.tests.test_score_objects.scorer_dict->_check_multimetric_scoring(clf, scorers)
A:sklearn.metrics.tests.test_score_objects.multi_scorer->_MultimetricScorer(scorers=scorer_dict)
A:sklearn.metrics.tests.test_score_objects.results->multi_scorer(mock_est, X, y)
A:sklearn.metrics.tests.test_score_objects.result->multi_scorer(clf, X, y)
A:sklearn.metrics.tests.test_score_objects.lr->LogisticRegression().fit(X, y)
A:sklearn.metrics.tests.test_score_objects.idx_positive->numpy.flatnonzero(y == 1)
A:sklearn.metrics.tests.test_score_objects.idx_negative->numpy.flatnonzero(y == 0)
A:sklearn.metrics.tests.test_score_objects.idx_selected->numpy.hstack([idx_negative, idx_positive[:25]])
A:sklearn.metrics.tests.test_score_objects.classifier->LogisticRegression().fit(X, y)
A:sklearn.metrics.tests.test_score_objects.y_pred->LogisticRegression().fit(X, y).predict(X_test)
A:sklearn.metrics.tests.test_score_objects.y_pred_proba->LogisticRegression().fit(X, y).predict_proba(X_test)
A:sklearn.metrics.tests.test_score_objects.y_pred_decision->LogisticRegression().fit(X, y).decision_function(X_test)
A:sklearn.metrics.tests.test_score_objects.ap_proba->average_precision_score(y_test, y_pred_proba, pos_label=pos_label)
A:sklearn.metrics.tests.test_score_objects.ap_decision_function->average_precision_score(y_test, y_pred_decision, pos_label=pos_label)
A:sklearn.metrics.tests.test_score_objects.average_precision_scorer->make_scorer(average_precision_score, response_method=('decision_function', 'predict_proba'), pos_label=pos_label)
A:sklearn.metrics.tests.test_score_objects.ap_scorer->average_precision_scorer(clf_without_predict_proba, X_test, y_test)
A:sklearn.metrics.tests.test_score_objects.clf_without_predict_proba->deepcopy(clf)
A:sklearn.metrics.tests.test_score_objects.clf_without_predict_proba.predict_proba->partial(_predict_proba, clf_without_predict_proba)
A:sklearn.metrics.tests.test_score_objects.brier_pos_cancer->brier_score_loss(y_test, y_pred_proba[:, 0], pos_label='cancer')
A:sklearn.metrics.tests.test_score_objects.brier_pos_not_cancer->brier_score_loss(y_test, y_pred_proba[:, 1], pos_label='not cancer')
A:sklearn.metrics.tests.test_score_objects.brier_scorer->make_scorer(brier_score_loss, response_method='predict_proba', pos_label=pos_label)
A:sklearn.metrics.tests.test_score_objects.score_pos_cancer->score_func(y_test, y_pred, pos_label='cancer')
A:sklearn.metrics.tests.test_score_objects.score_pos_not_cancer->score_func(y_test, y_pred, pos_label='not cancer')
A:sklearn.metrics.tests.test_score_objects.weighted_scorer->make_scorer(score).set_score_request(sample_weight=True)
A:sklearn.metrics.tests.test_score_objects.router->MetadataRouter(owner='test').add(scorer=weighted_scorer, method_mapping='score')
A:sklearn.metrics.tests.test_score_objects.routed_params->MetadataRouter(owner='test').add(scorer=weighted_scorer, method_mapping='score').route_params(params={'sample_weight': 1}, caller='score')
A:sklearn.metrics.tests.test_score_objects.(X, Y)->make_multilabel_classification(n_samples=72, n_classes=3, random_state=0)
A:sklearn.metrics.tests.test_score_objects.(X_train, X_test, Y_train, Y_test)->train_test_split(X, Y, random_state=0)
A:sklearn.metrics.tests.test_score_objects.deprecated_roc_auc_scorer->make_scorer(roc_auc_score, **deprecated_params)
A:sklearn.metrics.tests.test_score_objects.roc_auc_scorer->make_scorer(roc_auc_score, **new_params)
sklearn.metrics.tests.test_score_objects.DummyScorer(self,est,X,y)
sklearn.metrics.tests.test_score_objects.DummyScorer.__call__(self,est,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFit(BaseEstimator)
sklearn.metrics.tests.test_score_objects.EstimatorWithFit.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndPredict.predict(self,X)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore.fit(self,X,y)
sklearn.metrics.tests.test_score_objects.EstimatorWithFitAndScore.score(self,X,y)
sklearn.metrics.tests.test_score_objects._make_estimators(X_train,y_train,y_ml_train)
sklearn.metrics.tests.test_score_objects._require_positive_y(y)
sklearn.metrics.tests.test_score_objects.check_scoring_validator_for_single_metric_usecases(scoring_validator)
sklearn.metrics.tests.test_score_objects.setup_module()
sklearn.metrics.tests.test_score_objects.string_labeled_classification_problem()
sklearn.metrics.tests.test_score_objects.teardown_module()
sklearn.metrics.tests.test_score_objects.test_PassthroughScorer_metadata_request()
sklearn.metrics.tests.test_score_objects.test_all_scorers_repr()
sklearn.metrics.tests.test_score_objects.test_average_precision_pos_label(string_labeled_classification_problem)
sklearn.metrics.tests.test_score_objects.test_brier_score_loss_pos_label(string_labeled_classification_problem)
sklearn.metrics.tests.test_score_objects.test_check_scoring_and_check_multimetric_scoring(scoring)
sklearn.metrics.tests.test_score_objects.test_check_scoring_and_check_multimetric_scoring_errors(scoring,msg)
sklearn.metrics.tests.test_score_objects.test_check_scoring_gridsearchcv()
sklearn.metrics.tests.test_score_objects.test_classification_binary_scores(scorer_name,metric)
sklearn.metrics.tests.test_score_objects.test_classification_multiclass_scores(scorer_name,metric)
sklearn.metrics.tests.test_score_objects.test_classification_scorer_sample_weight()
sklearn.metrics.tests.test_score_objects.test_custom_scorer_pickling()
sklearn.metrics.tests.test_score_objects.test_get_scorer_multilabel_indicator()
sklearn.metrics.tests.test_score_objects.test_get_scorer_return_copy()
sklearn.metrics.tests.test_score_objects.test_kwargs_without_metadata_routing_error()
sklearn.metrics.tests.test_score_objects.test_make_scorer_deprecation(deprecated_params,new_params,warn_msg)
sklearn.metrics.tests.test_score_objects.test_make_scorer_error(params,err_type,err_msg)
sklearn.metrics.tests.test_score_objects.test_make_scorer_repr(scorer,expected_repr)
sklearn.metrics.tests.test_score_objects.test_metadata_kwarg_conflict()
sklearn.metrics.tests.test_score_objects.test_multiclass_roc_no_proba_scorer_errors(scorer_name)
sklearn.metrics.tests.test_score_objects.test_multiclass_roc_proba_scorer(scorer_name,metric)
sklearn.metrics.tests.test_score_objects.test_multiclass_roc_proba_scorer_label()
sklearn.metrics.tests.test_score_objects.test_multimetric_scorer_calls_method_once(scorers,expected_predict_count,expected_predict_proba_count,expected_decision_func_count)
sklearn.metrics.tests.test_score_objects.test_multimetric_scorer_calls_method_once_classifier_no_decision(scorers)
sklearn.metrics.tests.test_score_objects.test_multimetric_scorer_calls_method_once_regressor_threshold()
sklearn.metrics.tests.test_score_objects.test_multimetric_scorer_exception_handling(raise_exc)
sklearn.metrics.tests.test_score_objects.test_multimetric_scorer_sanity_check()
sklearn.metrics.tests.test_score_objects.test_multimetric_scoring_metadata_routing()
sklearn.metrics.tests.test_score_objects.test_non_symmetric_metric_pos_label(score_func,string_labeled_classification_problem)
sklearn.metrics.tests.test_score_objects.test_raises_on_score_list()
sklearn.metrics.tests.test_score_objects.test_regression_scorer_sample_weight()
sklearn.metrics.tests.test_score_objects.test_regression_scorers()
sklearn.metrics.tests.test_score_objects.test_scorer_memmap_input(name)
sklearn.metrics.tests.test_score_objects.test_scorer_metadata_request(name)
sklearn.metrics.tests.test_score_objects.test_scorer_no_op_multiclass_select_proba()
sklearn.metrics.tests.test_score_objects.test_scorer_select_proba_error(scorer)
sklearn.metrics.tests.test_score_objects.test_scorer_set_score_request_raises(name)
sklearn.metrics.tests.test_score_objects.test_scoring_is_not_metric()
sklearn.metrics.tests.test_score_objects.test_supervised_cluster_scorers()
sklearn.metrics.tests.test_score_objects.test_thresholded_scorers()
sklearn.metrics.tests.test_score_objects.test_thresholded_scorers_multilabel_indicator_data()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/tests/test_pairwise_distances_reduction.py----------------------------------------
A:sklearn.metrics.tests.test_pairwise_distances_reduction.rng->numpy.random.RandomState(1)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.indices_to_dist_a->dict(zip(indices_row_a, dist_row_a))
A:sklearn.metrics.tests.test_pairwise_distances_reduction.indices_to_dist_b->dict(zip(indices_row_b, dist_row_b))
A:sklearn.metrics.tests.test_pairwise_distances_reduction.common_indices->set(indices_row_a).intersection(set(indices_row_b))
A:sklearn.metrics.tests.test_pairwise_distances_reduction.missing_from_b->numpy.setdiff1d(indices_row_a[mask_a], indices_row_b)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.missing_from_a->numpy.setdiff1d(indices_row_b[mask_b], indices_row_a)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.sampled_dists->precomputed_dists[:n_subsampled_queries].copy()
A:sklearn.metrics.tests.test_pairwise_distances_reduction.n_queries->len(neighbors_dists_a)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.max_dist_a->numpy.max(dist_row_a)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.max_dist_b->numpy.max(dist_row_b)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.tols->dict(atol=atol, rtol=rtol)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.ref_dist->numpy.array([[1.2, 2.5, _6_1m, 6.1, _6_1p], [_1m, _1m, 1, _1p, _1p]])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.ref_indices->numpy.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.msg->re.escape('Largest returned distance 6.100000033333333 not within requested radius 6.1 on row 0')
A:sklearn.metrics.tests.test_pairwise_distances_reduction.X->numpy.random.RandomState(1).rand(100, 10)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.Y->numpy.random.RandomState(1).rand(100, 10)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.X_csr->csr_container(X)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.Y_csr->csr_container(Y)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.X_csr_0_nnz->csr_container(X * 0)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.X_csr_int64->csr_container(X)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.X_csr_int64.indices->csr_container(X).indices.astype(np.int64)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.Y_labels->numpy.random.RandomState(1).randint(low=0, high=10, size=100)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.unique_Y_labels->numpy.unique(Y_labels)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(n_samples_X, n_samples_Y)->numpy.random.RandomState(1).choice([97, 100, 101, 500], size=2, replace=False)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.radius->_non_trivial_radius(precomputed_dists=dist_matrix)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(ref_dist, ref_indices)->Dispatcher.compute(X, Y, parameter, metric=metric, return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(dist, indices)->Dispatcher.compute(_X, _Y, parameter, chunk_size=50, return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(dist_dense, indices_dense)->Dispatcher.compute(X, Y, parameter, chunk_size=50, return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.metric->numpy.random.RandomState(1).choice(np.array(['euclidean', 'minkowski', 'manhattan', 'haversine'], dtype=object))
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(dist_par_X, indices_par_X)->Dispatcher.compute(X, Y, parameter, metric=metric, metric_kwargs=_get_metric_params_list(metric, n_features, seed=global_random_seed)[0], chunk_size=n_samples_X // 4, strategy='parallel_on_X', return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(dist_par_Y, indices_par_Y)->Dispatcher.compute(X, Y, parameter, metric=metric, metric_kwargs=_get_metric_params_list(metric, n_features, seed=global_random_seed)[0], chunk_size=n_samples_Y // 4, strategy='parallel_on_Y', return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.n_features->numpy.random.RandomState(1).choice([5, 10, 100])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.translation->numpy.random.RandomState(1).choice([0, 1000000.0])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.dist_matrix->cdist(X, Y, metric=metric, **metric_kwargs)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.argkmin_distances_ref->numpy.zeros(argkmin_indices_ref.shape, dtype=np.float64)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(argkmin_distances, argkmin_indices)->sklearn.metrics._pairwise_distances_reduction.ArgKmin.compute(_X, _Y, k, metric=metric, metric_kwargs=metric_kwargs, return_distance=True, chunk_size=n_samples // 4, strategy=strategy)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.sort->numpy.argsort(dist)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(neigh_distances, neigh_indices)->sklearn.metrics._pairwise_distances_reduction.RadiusNeighbors.compute(X, Y, radius, metric=metric, metric_kwargs=metric_kwargs, return_distance=True, chunk_size=n_samples // 4, strategy=strategy, sort_results=True)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(X_mm, Y_mm)->create_memmap_backed_data([X, Y])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.(dist_mm, indices_mm)->Dispatcher.compute(X_mm, Y_mm, parameter, metric=metric, return_distance=True, **compute_parameters)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.n_samples->numpy.random.RandomState(1).choice([97, 100, 101, 1000])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.num_threads->numpy.random.RandomState(1).choice([1, 2, 8])
A:sklearn.metrics.tests.test_pairwise_distances_reduction.sq_row_norm->sqeuclidean_row_norms(X, num_threads=num_threads)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.sq_row_norm_csr->sqeuclidean_row_norms(X_csr, num_threads=num_threads)
A:sklearn.metrics.tests.test_pairwise_distances_reduction.results_X->sklearn.metrics._pairwise_distances_reduction.RadiusNeighborsClassMode.compute(X=X, Y=Y, radius=radius, metric=metric, weights=weights, Y_labels=Y_labels, unique_Y_labels=unique_Y_labels, outlier_label=outlier_label, strategy='parallel_on_X')
A:sklearn.metrics.tests.test_pairwise_distances_reduction.results_Y->sklearn.metrics._pairwise_distances_reduction.RadiusNeighborsClassMode.compute(X=X, Y=Y, radius=radius, metric=metric, weights=weights, Y_labels=Y_labels, unique_Y_labels=unique_Y_labels, outlier_label=outlier_label, strategy='parallel_on_Y')
sklearn.metrics.tests.test_pairwise_distances_reduction._get_metric_params_list(metric:str,n_features:int,seed:int=1)
sklearn.metrics.tests.test_pairwise_distances_reduction._non_trivial_radius(*,X=None,Y=None,metric=None,precomputed_dists=None,expected_n_neighbors=10,n_subsampled_queries=10,**metric_kwargs)
sklearn.metrics.tests.test_pairwise_distances_reduction.assert_compatible_argkmin_results(neighbors_dists_a,neighbors_dists_b,neighbors_indices_a,neighbors_indices_b,rtol=1e-05,atol=1e-06)
sklearn.metrics.tests.test_pairwise_distances_reduction.assert_compatible_radius_results(neighbors_dists_a,neighbors_dists_b,neighbors_indices_a,neighbors_indices_b,radius,check_sorted=True,rtol=1e-05,atol=1e-06)
sklearn.metrics.tests.test_pairwise_distances_reduction.assert_no_missing_neighbors(query_idx,dist_row_a,dist_row_b,indices_row_a,indices_row_b,threshold)
sklearn.metrics.tests.test_pairwise_distances_reduction.assert_same_distances_for_common_neighbors(query_idx,dist_row_a,dist_row_b,indices_row_a,indices_row_b,rtol,atol)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_argkmin_classmode_factory_method_wrong_usages()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_argkmin_classmode_strategy_consistent()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_argkmin_factory_method_wrong_usages()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_assert_compatible_argkmin_results()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_assert_compatible_radius_results(check_sorted)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_chunk_size_agnosticism(global_random_seed,Dispatcher,dtype,n_features=100)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_format_agnosticism(global_random_seed,Dispatcher,dtype,csr_container)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_memmap_backed_data(metric,Dispatcher,dtype)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_n_threads_agnosticism(global_random_seed,Dispatcher,dtype,n_features=100)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_pairwise_distances_argkmin(global_random_seed,metric,strategy,dtype,csr_container,n_queries=5,n_samples=100,k=10)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_pairwise_distances_radius_neighbors(global_random_seed,metric,strategy,dtype,n_queries=5,n_samples=100)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_pairwise_distances_reduction_is_usable_for(csr_container)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_radius_neighbors_classmode_factory_method_wrong_usages()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_radius_neighbors_classmode_strategy_consistent(outlier_label)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_radius_neighbors_factory_method_wrong_usages()
sklearn.metrics.tests.test_pairwise_distances_reduction.test_sqeuclidean_row_norms(global_random_seed,dtype,csr_container)
sklearn.metrics.tests.test_pairwise_distances_reduction.test_strategies_consistency(global_random_seed,global_dtype,Dispatcher,n_features=10)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py----------------------------------------
sklearn.metrics._pairwise_distances_reduction.ArgKmin(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction.ArgKmin.compute(cls,X,Y,k,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None,return_distance=False)
sklearn.metrics._pairwise_distances_reduction.ArgKminClassMode(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction.ArgKminClassMode.compute(cls,X,Y,k,weights,Y_labels,unique_Y_labels,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None)
sklearn.metrics._pairwise_distances_reduction.ArgKminClassMode.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction.BaseDistancesReductionDispatcher
sklearn.metrics._pairwise_distances_reduction.BaseDistancesReductionDispatcher.compute(cls,X,Y,**kwargs)
sklearn.metrics._pairwise_distances_reduction.BaseDistancesReductionDispatcher.is_usable_for(cls,X,Y,metric)->bool
sklearn.metrics._pairwise_distances_reduction.BaseDistancesReductionDispatcher.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction.RadiusNeighbors(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction.RadiusNeighbors.compute(cls,X,Y,radius,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None,return_distance=False,sort_results=False)
sklearn.metrics._pairwise_distances_reduction.RadiusNeighborsClassMode(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction.RadiusNeighborsClassMode.compute(cls,X,Y,radius,weights,Y_labels,unique_Y_labels,outlier_label,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None)
sklearn.metrics._pairwise_distances_reduction.RadiusNeighborsClassMode.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction._dispatcher.ArgKmin(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction._dispatcher.ArgKmin.compute(cls,X,Y,k,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None,return_distance=False)
sklearn.metrics._pairwise_distances_reduction._dispatcher.ArgKminClassMode(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction._dispatcher.ArgKminClassMode.compute(cls,X,Y,k,weights,Y_labels,unique_Y_labels,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None)
sklearn.metrics._pairwise_distances_reduction._dispatcher.ArgKminClassMode.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction._dispatcher.BaseDistancesReductionDispatcher
sklearn.metrics._pairwise_distances_reduction._dispatcher.BaseDistancesReductionDispatcher.compute(cls,X,Y,**kwargs)
sklearn.metrics._pairwise_distances_reduction._dispatcher.BaseDistancesReductionDispatcher.is_usable_for(cls,X,Y,metric)->bool
sklearn.metrics._pairwise_distances_reduction._dispatcher.BaseDistancesReductionDispatcher.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction._dispatcher.RadiusNeighbors(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction._dispatcher.RadiusNeighbors.compute(cls,X,Y,radius,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None,return_distance=False,sort_results=False)
sklearn.metrics._pairwise_distances_reduction._dispatcher.RadiusNeighborsClassMode(BaseDistancesReductionDispatcher)
sklearn.metrics._pairwise_distances_reduction._dispatcher.RadiusNeighborsClassMode.compute(cls,X,Y,radius,weights,Y_labels,unique_Y_labels,outlier_label,metric='euclidean',chunk_size=None,metric_kwargs=None,strategy=None)
sklearn.metrics._pairwise_distances_reduction._dispatcher.RadiusNeighborsClassMode.valid_metrics(cls)->List[str]
sklearn.metrics._pairwise_distances_reduction._dispatcher.sqeuclidean_row_norms(X,num_threads)
sklearn.metrics._pairwise_distances_reduction.sqeuclidean_row_norms(X,num_threads)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cross_decomposition/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cross_decomposition/_pls.py----------------------------------------
A:sklearn.cross_decomposition._pls.(u, s, vh)->svd(a, full_matrices=False, check_finite=False)
A:sklearn.cross_decomposition._pls.t->u.dtype.char.lower()
A:sklearn.cross_decomposition._pls.rank->numpy.sum(s > cond)
A:sklearn.cross_decomposition._pls.y_score->next((col for col in Y.T if np.any(np.abs(col) > eps)))
A:sklearn.cross_decomposition._pls.x_weights->numpy.dot(X_pinv, y_score)
A:sklearn.cross_decomposition._pls.x_score->numpy.dot(X, x_weights)
A:sklearn.cross_decomposition._pls.y_weights->numpy.dot(Y_pinv, x_score)
A:sklearn.cross_decomposition._pls.C->numpy.dot(X.T, Y)
A:sklearn.cross_decomposition._pls.(U, _, Vt)->svd(C, full_matrices=False)
A:sklearn.cross_decomposition._pls.x_mean->self._validate_data(X, dtype=np.float64, reset=False).mean(axis=0)
A:sklearn.cross_decomposition._pls.y_mean->Y.reshape(-1, 1).mean(axis=0)
A:sklearn.cross_decomposition._pls.x_std->numpy.ones(X.shape[1])
A:sklearn.cross_decomposition._pls.y_std->numpy.ones(Y.shape[1])
A:sklearn.cross_decomposition._pls.biggest_abs_val_idx->numpy.argmax(np.abs(u))
A:sklearn.cross_decomposition._pls.sign->numpy.sign(u[biggest_abs_val_idx])
A:sklearn.cross_decomposition._pls.X->self._validate_data(X, dtype=np.float64, reset=False)
A:sklearn.cross_decomposition._pls.Y->Y.reshape(-1, 1).reshape(-1, 1)
A:sklearn.cross_decomposition._pls.(Xk, Yk, self._x_mean, self._y_mean, self._x_std, self._y_std)->_center_scale_xy(X, Y, self.scale)
A:sklearn.cross_decomposition._pls.self.x_weights_->numpy.zeros((p, n_components))
A:sklearn.cross_decomposition._pls.self.y_weights_->numpy.zeros((q, n_components))
A:sklearn.cross_decomposition._pls.self._x_scores->numpy.zeros((n, n_components))
A:sklearn.cross_decomposition._pls.self._y_scores->numpy.zeros((n, n_components))
A:sklearn.cross_decomposition._pls.self.x_loadings_->numpy.zeros((p, n_components))
A:sklearn.cross_decomposition._pls.self.y_loadings_->numpy.zeros((q, n_components))
A:sklearn.cross_decomposition._pls.Yk_mask->numpy.all(np.abs(Yk) < 10 * Y_eps, axis=0)
A:sklearn.cross_decomposition._pls.(x_weights, y_weights, n_iter_)->_get_first_singular_vectors_power_method(Xk, Yk, mode=self.mode, max_iter=self.max_iter, tol=self.tol, norm_y_weights=norm_y_weights)
A:sklearn.cross_decomposition._pls.(x_weights, y_weights)->_get_first_singular_vectors_svd(Xk, Yk)
A:sklearn.cross_decomposition._pls.x_scores->numpy.dot(Xr, self.x_weights_)
A:sklearn.cross_decomposition._pls.y_ss->numpy.dot(y_weights, y_weights)
A:sklearn.cross_decomposition._pls.self.x_rotations_->numpy.dot(self.x_weights_, pinv2(np.dot(self.x_loadings_.T, self.x_weights_), check_finite=False))
A:sklearn.cross_decomposition._pls.self.y_rotations_->numpy.dot(self.y_weights_, pinv2(np.dot(self.y_loadings_.T, self.y_weights_), check_finite=False))
A:sklearn.cross_decomposition._pls.self.coef_->numpy.dot(self.x_rotations_, self.y_loadings_.T)
A:sklearn.cross_decomposition._pls.y_scores->numpy.dot(Yr, self.y_weights_)
A:sklearn.cross_decomposition._pls.X_reconstructed->numpy.matmul(X, self.x_loadings_.T)
A:sklearn.cross_decomposition._pls.Y_reconstructed->numpy.matmul(Y, self.y_loadings_.T)
A:sklearn.cross_decomposition._pls.rank_upper_bound->min(X.shape[0], X.shape[1], Y.shape[1])
A:sklearn.cross_decomposition._pls.(X, Y, self._x_mean, self._y_mean, self._x_std, self._y_std)->_center_scale_xy(X, Y, self.scale)
A:sklearn.cross_decomposition._pls.(U, s, Vt)->svd(C, full_matrices=False)
A:sklearn.cross_decomposition._pls.(U, Vt)->svd_flip(U, Vt)
sklearn.cross_decomposition.CCA(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.PLSCanonical(self,n_components=2,*,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.PLSRegression(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition.PLSRegression.fit(self,X,Y)
sklearn.cross_decomposition.PLSSVD(self,n_components=2,*,scale=True,copy=True)
sklearn.cross_decomposition.PLSSVD.fit(self,X,Y)
sklearn.cross_decomposition.PLSSVD.fit_transform(self,X,y=None)
sklearn.cross_decomposition.PLSSVD.transform(self,X,Y=None)
sklearn.cross_decomposition._pls.CCA(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.CCA.__init__(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.PLSCanonical(self,n_components=2,*,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.PLSCanonical.__init__(self,n_components=2,*,scale=True,algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.PLSRegression(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.PLSRegression.__init__(self,n_components=2,*,scale=True,max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls.PLSRegression.fit(self,X,Y)
sklearn.cross_decomposition._pls.PLSSVD(self,n_components=2,*,scale=True,copy=True)
sklearn.cross_decomposition._pls.PLSSVD.__init__(self,n_components=2,*,scale=True,copy=True)
sklearn.cross_decomposition._pls.PLSSVD.fit(self,X,Y)
sklearn.cross_decomposition._pls.PLSSVD.fit_transform(self,X,y=None)
sklearn.cross_decomposition._pls.PLSSVD.transform(self,X,Y=None)
sklearn.cross_decomposition._pls._PLS(self,n_components=2,*,scale=True,deflation_mode='regression',mode='A',algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls._PLS.__init__(self,n_components=2,*,scale=True,deflation_mode='regression',mode='A',algorithm='nipals',max_iter=500,tol=1e-06,copy=True)
sklearn.cross_decomposition._pls._PLS._more_tags(self)
sklearn.cross_decomposition._pls._PLS.fit(self,X,Y)
sklearn.cross_decomposition._pls._PLS.fit_transform(self,X,y=None)
sklearn.cross_decomposition._pls._PLS.inverse_transform(self,X,Y=None)
sklearn.cross_decomposition._pls._PLS.predict(self,X,copy=True)
sklearn.cross_decomposition._pls._PLS.transform(self,X,Y=None,copy=True)
sklearn.cross_decomposition._pls._center_scale_xy(X,Y,scale=True)
sklearn.cross_decomposition._pls._get_first_singular_vectors_power_method(X,Y,mode='A',max_iter=500,tol=1e-06,norm_y_weights=False)
sklearn.cross_decomposition._pls._get_first_singular_vectors_svd(X,Y)
sklearn.cross_decomposition._pls._pinv2_old(a)
sklearn.cross_decomposition._pls._svd_flip_1d(u,v)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cross_decomposition/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/cross_decomposition/tests/test_pls.py----------------------------------------
A:sklearn.cross_decomposition.tests.test_pls.K->numpy.dot(M.T, M)
A:sklearn.cross_decomposition.tests.test_pls.d->load_linnerud()
A:sklearn.cross_decomposition.tests.test_pls.pls->PLSEstimator(copy=True, scale=scale).fit(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.(Xc, Yc, x_mean, y_mean, x_std, y_std)->_center_scale_xy(X.copy(), Y.copy(), scale=True)
A:sklearn.cross_decomposition.tests.test_pls.Xt->PLSEstimator(copy=True, scale=scale).fit(X, Y).transform(X)
A:sklearn.cross_decomposition.tests.test_pls.(Xt, Yt)->PLSEstimator(copy=True, scale=scale).fit(X, Y).transform(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.X_back->PLSEstimator(copy=True, scale=scale).fit(X, Y).inverse_transform(Xt)
A:sklearn.cross_decomposition.tests.test_pls.(_, Y_back)->PLSEstimator(copy=True, scale=scale).fit(X, Y).inverse_transform(Xt, Yt)
A:sklearn.cross_decomposition.tests.test_pls.(X_trans, _)->PLSEstimator(copy=True, scale=scale).fit(X, Y).fit_transform(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.expected_x_weights->numpy.array([[0.65803719, 0.19197924, 0.21769083], [0.7009113, 0.13303969, -0.15376699], [0.13528197, -0.68636408, 0.13856546], [0.16854574, -0.66788088, -0.12485304], [-0.03232333, -0.04189855, 0.40690153], [0.1148816, -0.09643158, 0.1613305], [0.04792138, -0.02384992, 0.17175319], [-0.06781, -0.01666137, -0.18556747], [-0.00266945, -0.00160224, 0.11893098], [-0.00849528, -0.07706095, 0.1570547], [-0.00949471, -0.02964127, 0.34657036], [-0.03572177, 0.0945091, 0.3414855], [0.05584937, -0.02028961, -0.57682568], [0.05744254, -0.01482333, -0.17431274]])
A:sklearn.cross_decomposition.tests.test_pls.expected_x_loadings->numpy.array([[0.65649254, 0.1847647, 0.15270699], [0.67554234, 0.15237508, -0.09182247], [0.19219925, -0.67750975, 0.08673128], [0.2133631, -0.67034809, -0.08835483], [-0.03178912, -0.06668336, 0.43395268], [0.15684588, -0.13350241, 0.20578984], [0.03337736, -0.03807306, 0.09871553], [-0.06199844, 0.01559854, -0.1881785], [0.00406146, -0.00587025, 0.16413253], [-0.00374239, -0.05848466, 0.19140336], [0.00139214, -0.01033161, 0.32239136], [-0.05292828, 0.0953533, 0.31916881], [0.04031924, -0.01961045, -0.65174036], [0.06172484, -0.06597366, -0.1244497]])
A:sklearn.cross_decomposition.tests.test_pls.expected_y_weights->numpy.array([[0.66101097, 0.18672553, 0.22826092], [0.69347861, 0.18463471, -0.23995597], [0.14462724, -0.66504085, 0.17082434], [0.22247955, -0.6932605, -0.09832993], [0.07035859, 0.00714283, 0.67810124], [0.07765351, -0.0105204, -0.44108074], [-0.00917056, 0.04322147, 0.10062478], [-0.01909512, 0.06182718, 0.28830475], [0.01756709, 0.04797666, 0.32225745]])
A:sklearn.cross_decomposition.tests.test_pls.expected_y_loadings->numpy.array([[0.68568625, 0.1674376, 0.0969508], [0.68782064, 0.20375837, -0.1164448], [0.11712173, -0.68046903, 0.12001505], [0.17860457, -0.6798319, -0.05089681], [0.06265739, -0.0277703, 0.74729584], [0.0914178, 0.00403751, -0.5135078], [-0.02196918, -0.01377169, 0.09564505], [-0.03288952, 0.09039729, 0.31858973], [0.04287624, 0.05254676, 0.27836841]])
A:sklearn.cross_decomposition.tests.test_pls.x_loadings_sign_flip->numpy.sign(pls.x_loadings_ / expected_x_loadings)
A:sklearn.cross_decomposition.tests.test_pls.x_weights_sign_flip->numpy.sign(pls.x_weights_ / expected_x_weights)
A:sklearn.cross_decomposition.tests.test_pls.y_weights_sign_flip->numpy.sign(pls.y_weights_ / expected_y_weights)
A:sklearn.cross_decomposition.tests.test_pls.y_loadings_sign_flip->numpy.sign(pls.y_loadings_ / expected_y_loadings)
A:sklearn.cross_decomposition.tests.test_pls.expected_x_rotations->numpy.array([[-0.61330704, 0.41591889, -0.62297525], [-0.74697144, 0.31388326, 0.77368233], [-0.25668686, -0.89237972, -0.24121788]])
A:sklearn.cross_decomposition.tests.test_pls.expected_y_rotations->numpy.array([[+0.58989127, 0.7168115, 0.30665872], [+0.77134053, -0.70791757, 0.19786539], [-0.2388767, -0.00343595, 0.94162826]])
A:sklearn.cross_decomposition.tests.test_pls.x_rotations_sign_flip->numpy.sign(pls.x_rotations_ / expected_x_rotations)
A:sklearn.cross_decomposition.tests.test_pls.y_rotations_sign_flip->numpy.sign(pls.y_rotations_ / expected_y_rotations)
A:sklearn.cross_decomposition.tests.test_pls.rng->numpy.random.RandomState(42)
A:sklearn.cross_decomposition.tests.test_pls.l1->numpy.random.RandomState(42).normal(size=n)
A:sklearn.cross_decomposition.tests.test_pls.l2->numpy.random.RandomState(42).normal(size=n)
A:sklearn.cross_decomposition.tests.test_pls.X->numpy.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])
A:sklearn.cross_decomposition.tests.test_pls.Y->numpy.random.RandomState(42).randn(10, 3)
A:sklearn.cross_decomposition.tests.test_pls.pls_nipals->PLSCanonical(n_components=X.shape[1], max_iter=2)
A:sklearn.cross_decomposition.tests.test_pls.est->Klass().set_output(transform='pandas').fit(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.X_orig->numpy.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]]).copy()
A:sklearn.cross_decomposition.tests.test_pls.Q->numpy.random.RandomState(42).randn(n_targets, n_features)
A:sklearn.cross_decomposition.tests.test_pls.(X, Y)->load_linnerud(return_X_y=True, as_frame=True)
A:sklearn.cross_decomposition.tests.test_pls.(X_s, Y_s, *_)->_center_scale_xy(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.(X_score, Y_score)->Est(scale=True).fit_transform(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.(X_s_score, Y_s_score)->Est(scale=False).fit_transform(X_s, Y_s)
A:sklearn.cross_decomposition.tests.test_pls.(u1, v1, _)->_get_first_singular_vectors_power_method(X, Y, norm_y_weights=True)
A:sklearn.cross_decomposition.tests.test_pls.(u2, v2)->_get_first_singular_vectors_svd(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.svd->PLSSVD(n_components=1).fit(X, Y).transform(X)
A:sklearn.cross_decomposition.tests.test_pls.reg->PLSRegression(n_components=1).fit(X, Y).transform(X)
A:sklearn.cross_decomposition.tests.test_pls.canonical->PLSCanonical(n_components=1).fit(X, Y).transform(X)
A:sklearn.cross_decomposition.tests.test_pls.u->numpy.array([1, -4, 2])
A:sklearn.cross_decomposition.tests.test_pls.v->numpy.array([1, 2, 3])
A:sklearn.cross_decomposition.tests.test_pls.(u_expected, v_expected)->svd_flip(u.reshape(-1, 1), v.reshape(1, -1))
A:sklearn.cross_decomposition.tests.test_pls.(X, y)->make_regression(n_samples=200, n_features=20, n_targets=20, random_state=global_random_seed)
A:sklearn.cross_decomposition.tests.test_pls.cca->CCA(n_components=10, max_iter=500)
A:sklearn.cross_decomposition.tests.test_pls.x->numpy.random.RandomState(42).rand(100, 3)
A:sklearn.cross_decomposition.tests.test_pls.y->numpy.array([2, 6, 12, 20, 30, 42])
A:sklearn.cross_decomposition.tests.test_pls.Y_pred->PLSEstimator(copy=True, scale=scale).fit(X, Y).predict(X, copy=True)
A:sklearn.cross_decomposition.tests.test_pls.y_mean->numpy.random.RandomState(42).randn(10, 3).mean(axis=0)
A:sklearn.cross_decomposition.tests.test_pls.names_out->Klass().set_output(transform='pandas').fit(X, Y).get_feature_names_out()
A:sklearn.cross_decomposition.tests.test_pls.class_name_lower->Klass.__name__.lower()
A:sklearn.cross_decomposition.tests.test_pls.expected_names_out->numpy.array([f'{class_name_lower}{i}' for i in range(est.x_weights_.shape[1])], dtype=object)
A:sklearn.cross_decomposition.tests.test_pls.pd->pytest.importorskip('pandas')
A:sklearn.cross_decomposition.tests.test_pls.(X_trans, y_trans)->Klass().set_output(transform='pandas').fit(X, Y).transform(X, Y)
A:sklearn.cross_decomposition.tests.test_pls.expected->numpy.array([2, 6, 12, 20, 30, 42]).copy()
A:sklearn.cross_decomposition.tests.test_pls.plsr->PLSRegression().fit(X, y)
A:sklearn.cross_decomposition.tests.test_pls.y_pred->VotingRegressor([('lr', lr), ('plsr', plsr)]).fit(X, y).predict(X)
A:sklearn.cross_decomposition.tests.test_pls.lr->LinearRegression().fit(X, y)
A:sklearn.cross_decomposition.tests.test_pls.vr->VotingRegressor([('lr', lr), ('plsr', plsr)])
sklearn.cross_decomposition.tests.test_pls._generate_test_scale_and_stability_datasets()
sklearn.cross_decomposition.tests.test_pls.assert_matrix_orthogonal(M)
sklearn.cross_decomposition.tests.test_pls.test_attibutes_shapes(Est)
sklearn.cross_decomposition.tests.test_pls.test_convergence_fail()
sklearn.cross_decomposition.tests.test_pls.test_copy(Est)
sklearn.cross_decomposition.tests.test_pls.test_loadings_converges(global_random_seed)
sklearn.cross_decomposition.tests.test_pls.test_n_components_upper_bounds(Estimator)
sklearn.cross_decomposition.tests.test_pls.test_one_component_equivalence(global_random_seed)
sklearn.cross_decomposition.tests.test_pls.test_pls_canonical_basics()
sklearn.cross_decomposition.tests.test_pls.test_pls_coef_shape(PLSEstimator)
sklearn.cross_decomposition.tests.test_pls.test_pls_constant_y()
sklearn.cross_decomposition.tests.test_pls.test_pls_feature_names_out(Klass)
sklearn.cross_decomposition.tests.test_pls.test_pls_prediction(PLSEstimator,scale)
sklearn.cross_decomposition.tests.test_pls.test_pls_regression_fit_1d_y()
sklearn.cross_decomposition.tests.test_pls.test_pls_set_output(Klass)
sklearn.cross_decomposition.tests.test_pls.test_sanity_check_pls_canonical()
sklearn.cross_decomposition.tests.test_pls.test_sanity_check_pls_canonical_random()
sklearn.cross_decomposition.tests.test_pls.test_sanity_check_pls_regression()
sklearn.cross_decomposition.tests.test_pls.test_sanity_check_pls_regression_constant_column_Y()
sklearn.cross_decomposition.tests.test_pls.test_scale_and_stability(Est,X,Y)
sklearn.cross_decomposition.tests.test_pls.test_singular_value_helpers(n_samples,n_features,global_random_seed)
sklearn.cross_decomposition.tests.test_pls.test_svd_flip_1d()
sklearn.cross_decomposition.tests.test_pls.test_univariate_equivalence(Est)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/_label_propagation.py----------------------------------------
A:sklearn.semi_supervised._label_propagation.self.nn_fit->NearestNeighbors(n_neighbors=self.n_neighbors, n_jobs=self.n_jobs).fit(X)
A:sklearn.semi_supervised._label_propagation.probas->self.predict_proba(X)
A:sklearn.semi_supervised._label_propagation.X_2d->self._validate_data(X, accept_sparse=['csc', 'csr', 'coo', 'dok', 'bsr', 'lil', 'dia'], reset=False)
A:sklearn.semi_supervised._label_propagation.weight_matrices->self._get_kernel(self.X_, X_2d)
A:sklearn.semi_supervised._label_propagation.probabilities->safe_sparse_dot(weight_matrices, self.label_distributions_)
A:sklearn.semi_supervised._label_propagation.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], reset=True)
A:sklearn.semi_supervised._label_propagation.graph_matrix->graph_matrix.tocsr().tocsr()
A:sklearn.semi_supervised._label_propagation.classes->numpy.unique(y)
A:sklearn.semi_supervised._label_propagation.y->numpy.asarray(y)
A:sklearn.semi_supervised._label_propagation.self.label_distributions_->numpy.where(unlabeled, self.label_distributions_, y_static)
A:sklearn.semi_supervised._label_propagation.y_static->numpy.copy(self.label_distributions_)
A:sklearn.semi_supervised._label_propagation.l_previous->numpy.zeros((self.X_.shape[0], n_classes))
A:sklearn.semi_supervised._label_propagation.self.transduction_->transduction.ravel()
A:sklearn.semi_supervised._label_propagation.affinity_matrix->self._get_kernel(self.X_)
A:sklearn.semi_supervised._label_propagation.normalizer->self._get_kernel(self.X_).sum(axis=0)
A:sklearn.semi_supervised._label_propagation.laplacian->csgraph_laplacian(affinity_matrix, normed=True)
sklearn.semi_supervised.LabelPropagation(self,kernel='rbf',*,gamma=20,n_neighbors=7,max_iter=1000,tol=0.001,n_jobs=None)
sklearn.semi_supervised.LabelPropagation._build_graph(self)
sklearn.semi_supervised.LabelPropagation.fit(self,X,y)
sklearn.semi_supervised.LabelSpreading(self,kernel='rbf',*,gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=None)
sklearn.semi_supervised.LabelSpreading._build_graph(self)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation(self,kernel='rbf',*,gamma=20,n_neighbors=7,alpha=1,max_iter=30,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation.__init__(self,kernel='rbf',*,gamma=20,n_neighbors=7,alpha=1,max_iter=30,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation._build_graph(self)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation._get_kernel(self,X,y=None)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation.fit(self,X,y)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation.predict(self,X)
sklearn.semi_supervised._label_propagation.BaseLabelPropagation.predict_proba(self,X)
sklearn.semi_supervised._label_propagation.LabelPropagation(self,kernel='rbf',*,gamma=20,n_neighbors=7,max_iter=1000,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.LabelPropagation.__init__(self,kernel='rbf',*,gamma=20,n_neighbors=7,max_iter=1000,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.LabelPropagation._build_graph(self)
sklearn.semi_supervised._label_propagation.LabelPropagation.fit(self,X,y)
sklearn.semi_supervised._label_propagation.LabelSpreading(self,kernel='rbf',*,gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.LabelSpreading.__init__(self,kernel='rbf',*,gamma=20,n_neighbors=7,alpha=0.2,max_iter=30,tol=0.001,n_jobs=None)
sklearn.semi_supervised._label_propagation.LabelSpreading._build_graph(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/_self_training.py----------------------------------------
A:sklearn.semi_supervised._self_training.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc', 'lil', 'dok'], force_all_finite=False)
A:sklearn.semi_supervised._self_training.self.base_estimator_->clone(self.base_estimator)
A:sklearn.semi_supervised._self_training.self.transduction_->numpy.copy(y)
A:sklearn.semi_supervised._self_training.self.labeled_iter_->numpy.full_like(y, -1)
A:sklearn.semi_supervised._self_training.prob->self.base_estimator_.predict_proba(X[safe_mask(X, ~has_label)])
A:sklearn.semi_supervised._self_training.max_proba->numpy.max(prob, axis=1)
A:sklearn.semi_supervised._self_training.n_to_select->min(self.k_best, max_proba.shape[0])
A:sklearn.semi_supervised._self_training.selected->numpy.ones_like(max_proba, dtype=bool)
A:sklearn.semi_supervised._self_training.X->self._validate_data(X, accept_sparse=True, force_all_finite=False, reset=False)
sklearn.semi_supervised.SelfTrainingClassifier(self,base_estimator,threshold=0.75,criterion='threshold',k_best=10,max_iter=10,verbose=False)
sklearn.semi_supervised.SelfTrainingClassifier.decision_function(self,X)
sklearn.semi_supervised.SelfTrainingClassifier.fit(self,X,y)
sklearn.semi_supervised.SelfTrainingClassifier.predict(self,X)
sklearn.semi_supervised.SelfTrainingClassifier.predict_log_proba(self,X)
sklearn.semi_supervised.SelfTrainingClassifier.predict_proba(self,X)
sklearn.semi_supervised.SelfTrainingClassifier.score(self,X,y)
sklearn.semi_supervised._self_training.SelfTrainingClassifier(self,base_estimator,threshold=0.75,criterion='threshold',k_best=10,max_iter=10,verbose=False)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.__init__(self,base_estimator,threshold=0.75,criterion='threshold',k_best=10,max_iter=10,verbose=False)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.decision_function(self,X)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.fit(self,X,y)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.predict(self,X)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.predict_log_proba(self,X)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.predict_proba(self,X)
sklearn.semi_supervised._self_training.SelfTrainingClassifier.score(self,X,y)
sklearn.semi_supervised._self_training._estimator_has(attr)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/tests/test_label_propagation.py----------------------------------------
A:sklearn.semi_supervised.tests.test_label_propagation.samples->numpy.asarray([[1.0, 0.0], [0.0, 1.0], [1.0, 2.5]], dtype=global_dtype)
A:sklearn.semi_supervised.tests.test_label_propagation.clf->Estimator(**parameters).fit(X, labels)
A:sklearn.semi_supervised.tests.test_label_propagation.(X, y)->make_classification(n_classes=n_classes, n_samples=n_samples, n_features=20, n_informative=20, n_redundant=0, n_repeated=0, random_state=0)
A:sklearn.semi_supervised.tests.test_label_propagation.X->X.astype(global_dtype).astype(global_dtype)
A:sklearn.semi_supervised.tests.test_label_propagation.S->Estimator(**parameters).fit(X, labels)._build_graph()
A:sklearn.semi_supervised.tests.test_label_propagation.Y->numpy.zeros((len(y), n_classes + 1))
A:sklearn.semi_supervised.tests.test_label_propagation.expected->numpy.zeros((len(y), n_classes + 1)).copy()
A:sklearn.semi_supervised.tests.test_label_propagation.T_bar->Estimator(**parameters).fit(X, labels)._build_graph()
A:sklearn.semi_supervised.tests.test_label_propagation.Y_u->numpy.dot(np.dot(np.linalg.inv(np.eye(Tuu.shape[0]) - Tuu), Tul), Y_l)
A:sklearn.semi_supervised.tests.test_label_propagation.X.data->X.astype(global_dtype).astype(global_dtype).data.astype(dtype, copy=False)
A:sklearn.semi_supervised.tests.test_label_propagation.X.indices->X.astype(global_dtype).astype(global_dtype).indices.astype(index_dtype, copy=False)
A:sklearn.semi_supervised.tests.test_label_propagation.X.indptr->X.astype(global_dtype).astype(global_dtype).indptr.astype(index_dtype, copy=False)
A:sklearn.semi_supervised.tests.test_label_propagation.y->numpy.array([0, 1, -1, -1])
A:sklearn.semi_supervised.tests.test_label_propagation.mdl->LabelPropagationCls(kernel='knn', max_iter=100, n_neighbors=1)
A:sklearn.semi_supervised.tests.test_label_propagation.nn->NearestNeighbors(n_neighbors=10, metric='euclidean', n_jobs=2)
A:sklearn.semi_supervised.tests.test_label_propagation.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=n_test, random_state=0)
A:sklearn.semi_supervised.tests.test_label_propagation.model->sklearn.semi_supervised._label_propagation.LabelPropagation(kernel=topk_rbf)
sklearn.semi_supervised.tests.test_label_propagation.test_convergence_speed(constructor_type)
sklearn.semi_supervised.tests.test_label_propagation.test_convergence_warning()
sklearn.semi_supervised.tests.test_label_propagation.test_distribution(global_dtype,Estimator,parameters)
sklearn.semi_supervised.tests.test_label_propagation.test_fit_transduction(global_dtype,Estimator,parameters)
sklearn.semi_supervised.tests.test_label_propagation.test_label_propagation_closed_form(global_dtype)
sklearn.semi_supervised.tests.test_label_propagation.test_label_propagation_non_zero_normalizer(LabelPropagationCls)
sklearn.semi_supervised.tests.test_label_propagation.test_label_spreading_closed_form(global_dtype,Estimator,parameters,alpha)
sklearn.semi_supervised.tests.test_label_propagation.test_predict(global_dtype,Estimator,parameters)
sklearn.semi_supervised.tests.test_label_propagation.test_predict_proba(global_dtype,Estimator,parameters)
sklearn.semi_supervised.tests.test_label_propagation.test_predict_sparse_callable_kernel(global_dtype)
sklearn.semi_supervised.tests.test_label_propagation.test_sparse_input_types(accepted_sparse_type,index_dtype,dtype,Estimator,parameters)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/semi_supervised/tests/test_self_training.py----------------------------------------
A:sklearn.semi_supervised.tests.test_self_training.iris->load_iris()
A:sklearn.semi_supervised.tests.test_self_training.(X_train, X_test, y_train, y_test)->train_test_split(iris.data, iris.target, random_state=0)
A:sklearn.semi_supervised.tests.test_self_training.y_train_missing_labels->y_train.copy()
A:sklearn.semi_supervised.tests.test_self_training.y_train_missing_strings->numpy.vectorize(mapping.get)(y_train_missing_labels).astype(object)
A:sklearn.semi_supervised.tests.test_self_training.st->SelfTrainingClassifier(svc, criterion='k_best', max_iter=1, k_best=10)
A:sklearn.semi_supervised.tests.test_self_training.pred->SVC(gamma='scale', probability=True, random_state=0).predict_proba(X_train[~has_label])
A:sklearn.semi_supervised.tests.test_self_training.proba->SelfTrainingClassifier(svc, criterion='k_best', max_iter=1, k_best=10).predict_proba(X_test)
A:sklearn.semi_supervised.tests.test_self_training.st_string->SelfTrainingClassifier(base_estimator, max_iter=max_iter, criterion=selection_crit, threshold=threshold)
A:sklearn.semi_supervised.tests.test_self_training.pred_string->SelfTrainingClassifier(base_estimator, max_iter=max_iter, criterion=selection_crit, threshold=threshold).predict(X_test)
A:sklearn.semi_supervised.tests.test_self_training.proba_string->SelfTrainingClassifier(base_estimator, max_iter=max_iter, criterion=selection_crit, threshold=threshold).predict_proba(X_test)
A:sklearn.semi_supervised.tests.test_self_training.y_train_only_one_label->numpy.copy(y_train)
A:sklearn.semi_supervised.tests.test_self_training.n_expected_iter->ceil((n_samples - 1) / 10)
A:sklearn.semi_supervised.tests.test_self_training.base_estimator->SVC(probability=False, gamma='scale')
A:sklearn.semi_supervised.tests.test_self_training.score_supervised->accuracy_score(base_estimator.predict(X_test), y_test)
A:sklearn.semi_supervised.tests.test_self_training.score_self_training->accuracy_score(st.predict(X_test), y_test)
A:sklearn.semi_supervised.tests.test_self_training.clf1->SelfTrainingClassifier(base_estimator, max_iter=0)
A:sklearn.semi_supervised.tests.test_self_training.clf2->SVC(probability=False, gamma='scale').fit(X_train[:n_labeled_samples], y[:n_labeled_samples])
A:sklearn.semi_supervised.tests.test_self_training.knn->KNeighborsClassifier()
A:sklearn.semi_supervised.tests.test_self_training.amount_iter_0->len(st.labeled_iter_[st.labeled_iter_ == 0])
A:sklearn.semi_supervised.tests.test_self_training.svc->SVC(gamma='scale', probability=True, random_state=0)
A:sklearn.semi_supervised.tests.test_self_training.clf->SelfTrainingClassifier(base_estimator=base_estimator)
A:sklearn.semi_supervised.tests.test_self_training.(X, y)->make_blobs(n_samples=30, random_state=0, cluster_std=0.1)
A:sklearn.semi_supervised.tests.test_self_training.y_strings->numpy.take(labels_multiclass, y)
A:sklearn.semi_supervised.tests.test_self_training.captured->capsys.readouterr()
A:sklearn.semi_supervised.tests.test_self_training.max_proba->numpy.max(pred, axis=1)
A:sklearn.semi_supervised.tests.test_self_training.added_by_st->X_train[np.where(got_label)].tolist()
A:sklearn.semi_supervised.tests.test_self_training.self_training->SelfTrainingClassifier(base_estimator)
sklearn.semi_supervised.tests.test_self_training.test_base_estimator_meta_estimator()
sklearn.semi_supervised.tests.test_self_training.test_classification(base_estimator,selection_crit)
sklearn.semi_supervised.tests.test_self_training.test_early_stopping()
sklearn.semi_supervised.tests.test_self_training.test_k_best()
sklearn.semi_supervised.tests.test_self_training.test_k_best_selects_best()
sklearn.semi_supervised.tests.test_self_training.test_labeled_iter(max_iter)
sklearn.semi_supervised.tests.test_self_training.test_missing_predict_proba()
sklearn.semi_supervised.tests.test_self_training.test_no_unlabeled()
sklearn.semi_supervised.tests.test_self_training.test_none_iter()
sklearn.semi_supervised.tests.test_self_training.test_prefitted_throws_error()
sklearn.semi_supervised.tests.test_self_training.test_sanity_classification()
sklearn.semi_supervised.tests.test_self_training.test_strings_dtype()
sklearn.semi_supervised.tests.test_self_training.test_verbose(capsys,verbose)
sklearn.semi_supervised.tests.test_self_training.test_verbose_k_best(capsys)
sklearn.semi_supervised.tests.test_self_training.test_warns_k_best()
sklearn.semi_supervised.tests.test_self_training.test_zero_iterations(base_estimator,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_kernel_approximation.py----------------------------------------
A:sklearn.tests.test_kernel_approximation.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_kernel_approximation.X->numpy.random.RandomState(0).random_sample(size=(300, 3))
A:sklearn.tests.test_kernel_approximation.Y->numpy.random.RandomState(0).random_sample(size=(300, 50))
A:sklearn.tests.test_kernel_approximation.kernel->rbf_kernel(X, Y, gamma=gamma)
A:sklearn.tests.test_kernel_approximation.ps_transform->PolynomialCountSketch(n_components=n_components, gamma=gamma, coef0=coef0, degree=degree, random_state=42)
A:sklearn.tests.test_kernel_approximation.X_trans->Estimator().fit(X).transform(X)
A:sklearn.tests.test_kernel_approximation.Y_trans->RBFSampler(gamma=gamma, n_components=1000, random_state=42).transform(Y)
A:sklearn.tests.test_kernel_approximation.kernel_approx->numpy.dot(X_trans, Y_trans.T)
A:sklearn.tests.test_kernel_approximation.ps_dense->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42)
A:sklearn.tests.test_kernel_approximation.Xt_dense->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42).fit_transform(X)
A:sklearn.tests.test_kernel_approximation.Yt_dense->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42).transform(Y)
A:sklearn.tests.test_kernel_approximation.ps_sparse->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42)
A:sklearn.tests.test_kernel_approximation.Xt_sparse->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42).fit_transform(csr_container(X))
A:sklearn.tests.test_kernel_approximation.Yt_sparse->PolynomialCountSketch(n_components=500, gamma=gamma, degree=degree, coef0=coef0, random_state=42).transform(csr_container(Y))
A:sklearn.tests.test_kernel_approximation.transform->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42)
A:sklearn.tests.test_kernel_approximation.X_sp_trans->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42).fit_transform(csr_container(X))
A:sklearn.tests.test_kernel_approximation.Y_sp_trans->SkewedChi2Sampler(skewedness=c, n_components=1000, random_state=42).transform(csr_container(Y))
A:sklearn.tests.test_kernel_approximation.Y_neg->numpy.random.RandomState(0).random_sample(size=(300, 50)).copy()
A:sklearn.tests.test_kernel_approximation.transformer->AdditiveChi2Sampler()
A:sklearn.tests.test_kernel_approximation.msg->re.escape('If sample_steps is not in [1, 2, 3], you need to provide sample_interval')
A:sklearn.tests.test_kernel_approximation.X_neg->numpy.random.RandomState(0).random_sample(size=(300, 3)).copy()
A:sklearn.tests.test_kernel_approximation.rbf_transform->RBFSampler(gamma=gamma, n_components=1000, random_state=42)
A:sklearn.tests.test_kernel_approximation.rbf->RBFSampler(gamma='scale')
A:sklearn.tests.test_kernel_approximation.rbf32->RBFSampler(random_state=42)
A:sklearn.tests.test_kernel_approximation.X32->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)
A:sklearn.tests.test_kernel_approximation.rbf64->RBFSampler(random_state=42)
A:sklearn.tests.test_kernel_approximation.X64->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float64)
A:sklearn.tests.test_kernel_approximation.skewed_chi2_sampler->SkewedChi2Sampler()
A:sklearn.tests.test_kernel_approximation.skewed_chi2_sampler_32->SkewedChi2Sampler(random_state=42)
A:sklearn.tests.test_kernel_approximation.X_32->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)
A:sklearn.tests.test_kernel_approximation.skewed_chi2_sampler_64->SkewedChi2Sampler(random_state=42)
A:sklearn.tests.test_kernel_approximation.X_64->numpy.array([[1, 2], [3, 4], [5, 6]], dtype=np.float64)
A:sklearn.tests.test_kernel_approximation.rnd->numpy.random.RandomState(12)
A:sklearn.tests.test_kernel_approximation.X_transformed->Nystroem(kernel='precomputed', n_components=X.shape[0]).fit_transform(K)
A:sklearn.tests.test_kernel_approximation.K->polynomial_kernel(X, degree=2, coef0=0.1)
A:sklearn.tests.test_kernel_approximation.trans->Nystroem(n_components=2, kernel=kern, random_state=rnd)
A:sklearn.tests.test_kernel_approximation.kernels_available->kernel_metrics()
A:sklearn.tests.test_kernel_approximation.nystroem->Nystroem(kernel='precomputed', n_components=X.shape[0])
A:sklearn.tests.test_kernel_approximation.K2->numpy.dot(X_transformed, X_transformed.T)
A:sklearn.tests.test_kernel_approximation.N->Nystroem(gamma=gamma, n_components=X.shape[0]).fit(X)
A:sklearn.tests.test_kernel_approximation.ny->Nystroem(kernel='precomputed', n_components=X.shape[0], **param)
A:sklearn.tests.test_kernel_approximation.(X, _)->make_classification(n_samples=100, n_features=20)
A:sklearn.tests.test_kernel_approximation.feature_map_nystroem->Nystroem(n_components=10, random_state=0)
A:sklearn.tests.test_kernel_approximation.est->Estimator().fit(X)
A:sklearn.tests.test_kernel_approximation.names_out->AdditiveChi2Sampler(sample_steps=3).fit(X).get_feature_names_out(input_features=input_names)
A:sklearn.tests.test_kernel_approximation.class_name->Estimator.__name__.lower()
A:sklearn.tests.test_kernel_approximation.chi2_sampler->AdditiveChi2Sampler(sample_steps=3).fit(X)
sklearn.tests.test_kernel_approximation._linear_kernel(X,Y)
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler(csr_container)
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler_exceptions()
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler_future_warnings()
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler_sample_steps(method,sample_steps)
sklearn.tests.test_kernel_approximation.test_additive_chi2_sampler_wrong_sample_steps(method)
sklearn.tests.test_kernel_approximation.test_additivechi2sampler_get_feature_names_out()
sklearn.tests.test_kernel_approximation.test_get_feature_names_out(Estimator)
sklearn.tests.test_kernel_approximation.test_input_validation(csr_container)
sklearn.tests.test_kernel_approximation.test_nystroem_approximation()
sklearn.tests.test_kernel_approximation.test_nystroem_callable()
sklearn.tests.test_kernel_approximation.test_nystroem_component_indices()
sklearn.tests.test_kernel_approximation.test_nystroem_default_parameters()
sklearn.tests.test_kernel_approximation.test_nystroem_poly_kernel_params()
sklearn.tests.test_kernel_approximation.test_nystroem_precomputed_kernel()
sklearn.tests.test_kernel_approximation.test_nystroem_singular_kernel()
sklearn.tests.test_kernel_approximation.test_polynomial_count_sketch(gamma,degree,coef0,n_components)
sklearn.tests.test_kernel_approximation.test_polynomial_count_sketch_dense_sparse(gamma,degree,coef0,csr_container)
sklearn.tests.test_kernel_approximation.test_rbf_sampler()
sklearn.tests.test_kernel_approximation.test_rbf_sampler_dtype_equivalence()
sklearn.tests.test_kernel_approximation.test_rbf_sampler_fitted_attributes_dtype(global_dtype)
sklearn.tests.test_kernel_approximation.test_rbf_sampler_gamma_scale()
sklearn.tests.test_kernel_approximation.test_skewed_chi2_sampler()
sklearn.tests.test_kernel_approximation.test_skewed_chi2_sampler_dtype_equivalence()
sklearn.tests.test_kernel_approximation.test_skewed_chi2_sampler_fitted_attributes_dtype(global_dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_base.py----------------------------------------
A:sklearn.tests.test_base.self.a->a.copy()
A:sklearn.tests.test_base.selector->SelectFpr(f_classif, alpha=0.1)
A:sklearn.tests.test_base.new_selector->clone(selector)
A:sklearn.tests.test_base.buggy->Buggy()
A:sklearn.tests.test_base.no_estimator->NoEstimator()
A:sklearn.tests.test_base.varg_est->VargEstimator()
A:sklearn.tests.test_base.est->MyEstimator()
A:sklearn.tests.test_base.clf->Pipeline([('svc', SVC())])
A:sklearn.tests.test_base.clf2->clone(clf)
A:sklearn.tests.test_base.cloned->clone(orig)
A:sklearn.tests.test_base.sparse_matrix->cls(np.eye(5))
A:sklearn.tests.test_base.clf_cloned->clone(clf)
A:sklearn.tests.test_base.my_estimator->MyEstimator()
A:sklearn.tests.test_base.test->T(K(), K)
A:sklearn.tests.test_base.some_est->T(a=['long_params'] * 1000)
A:sklearn.tests.test_base.svc->SVC()
A:sklearn.tests.test_base.gscv->GridSearchCV(DecisionTreeClassifier(), {})
A:sklearn.tests.test_base.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_base.sample_weight->numpy.random.RandomState(0).randint(1, 10, size=len(y))
A:sklearn.tests.test_base.score_unweighted->DecisionTreeClassifier().score(X, y)
A:sklearn.tests.test_base.score_weighted->DecisionTreeClassifier().score(X, y, sample_weight=sample_weight)
A:sklearn.tests.test_base.d->numpy.arange(10)
A:sklearn.tests.test_base.df->_convert_container(data, constructor_name, columns_name=columns, minversion=minversion)
A:sklearn.tests.test_base.e->DummyEstimator(df, scalar_param=1)
A:sklearn.tests.test_base.cloned_e->clone(e)
A:sklearn.tests.test_base.X->numpy.array([[-1, -1], [-2, -1], [-3, -2]])
A:sklearn.tests.test_base.pca->PCA().fit(X)
A:sklearn.tests.test_base.frozen_pca->FrozenEstimator(pca)
A:sklearn.tests.test_base.X_new->numpy.asarray([[-1, 2], [3, 4], [1, 2]])
A:sklearn.tests.test_base.clone_frozen_pca->clone(frozen_pca)
A:sklearn.tests.test_base.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_base.tree->DecisionTreeClassifier()
A:sklearn.tests.test_base.tree_pickle->pickle.dumps(tree)
A:sklearn.tests.test_base.tree_restored->assert_no_warnings(pickle.loads, tree_pickle)
A:sklearn.tests.test_base.score_of_original->DecisionTreeClassifier().score(iris.data, iris.target)
A:sklearn.tests.test_base.score_of_restored->assert_no_warnings(pickle.loads, tree_pickle).score(iris.data, iris.target)
A:sklearn.tests.test_base.tree_pickle_other->pickle.dumps(tree)
A:sklearn.tests.test_base.message->pickle_error_message.format(estimator='TreeNoVersion', old_version='pre-0.18', current_version=sklearn.__version__)
A:sklearn.tests.test_base.tree_pickle_noversion->pickle.dumps(tree)
A:sklearn.tests.test_base.data->self.__dict__.copy()
A:sklearn.tests.test_base.estimator->SingleInheritanceEstimator()
A:sklearn.tests.test_base.serialized->pickle.dumps(estimator)
A:sklearn.tests.test_base.estimator_restored->pickle.loads(serialized)
A:sklearn.tests.test_base.nan_tag_est->NaNTag()
A:sklearn.tests.test_base.no_nan_tag_est->NoNaNTag()
A:sklearn.tests.test_base.redefine_tags_est->OverrideTag()
A:sklearn.tests.test_base.diamond_tag_est->DiamondOverwriteTag()
A:sklearn.tests.test_base.inherit_diamond_tag_est->InheritDiamondOverwriteTag()
A:sklearn.tests.test_base.output->DecisionTreeClassifier()._repr_html_()
A:sklearn.tests.test_base.pd->pytest.importorskip('pandas')
A:sklearn.tests.test_base.trans->NoOpTransformer()
A:sklearn.tests.test_base.df_bad->_convert_container(data, constructor_name, columns_name=bad_names)
A:sklearn.tests.test_base.df_int_names->pytest.importorskip('pandas').DataFrame(X_np)
A:sklearn.tests.test_base.df_mixed->pytest.importorskip('pandas').DataFrame(X_np, columns=['a', 'b', 1, 2])
A:sklearn.tests.test_base.msg->re.escape("Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.")
A:sklearn.tests.test_base.y->pytest.importorskip('pandas').Series(iris.target)
A:sklearn.tests.test_base.no_op->NoOpTransformer()
A:sklearn.tests.test_base.X_np_out->NoOpTransformer()._validate_data(df, cast_to_ndarray=True)
A:sklearn.tests.test_base.X_df_out->NoOpTransformer()._validate_data(df, cast_to_ndarray=False)
A:sklearn.tests.test_base.y_np_out->NoOpTransformer()._validate_data(y=y, cast_to_ndarray=True)
A:sklearn.tests.test_base.y_series_out->NoOpTransformer()._validate_data(y=y, cast_to_ndarray=False)
A:sklearn.tests.test_base.(X_np_out, y_np_out)->NoOpTransformer()._validate_data(df, y, cast_to_ndarray=True)
A:sklearn.tests.test_base.(X_df_out, y_series_out)->NoOpTransformer()._validate_data(df, y, cast_to_ndarray=False)
A:sklearn.tests.test_base.ss->StandardScaler().set_output(transform='pandas')
A:sklearn.tests.test_base.config->_get_output_config('transform', ss)
A:sklearn.tests.test_base.ss_clone->clone(ss)
A:sklearn.tests.test_base.config_clone->_get_output_config('transform', ss_clone)
A:sklearn.tests.test_base.state->SingleInheritanceEstimator().__getstate__()
A:sklearn.tests.test_base.X_out->NoOpTransformer().transform(df)
sklearn.tests.test_base.Buggy(self,a=None)
sklearn.tests.test_base.Buggy.__init__(self,a=None)
sklearn.tests.test_base.DiamondOverwriteTag(NaNTag,NoNaNTag)
sklearn.tests.test_base.DiamondOverwriteTag._more_tags(self)
sklearn.tests.test_base.DontPickleAttributeMixin
sklearn.tests.test_base.DontPickleAttributeMixin.__getstate__(self)
sklearn.tests.test_base.DontPickleAttributeMixin.__setstate__(self,state)
sklearn.tests.test_base.EmptyEstimator(_Empty,BaseEstimator)
sklearn.tests.test_base.InheritDiamondOverwriteTag(DiamondOverwriteTag)
sklearn.tests.test_base.K(self,c=None,d=None)
sklearn.tests.test_base.K.__init__(self,c=None,d=None)
sklearn.tests.test_base.ModifyInitParams(self,a=np.array([0]))
sklearn.tests.test_base.ModifyInitParams.__init__(self,a=np.array([0]))
sklearn.tests.test_base.MultiInheritanceEstimator(self,attribute_pickled=5)
sklearn.tests.test_base.MultiInheritanceEstimator.__init__(self,attribute_pickled=5)
sklearn.tests.test_base.MyEstimator(self,l1=0,empty=None)
sklearn.tests.test_base.MyEstimator.__init__(self,l1=0,empty=None)
sklearn.tests.test_base.NaNTag(BaseEstimator)
sklearn.tests.test_base.NaNTag._more_tags(self)
sklearn.tests.test_base.NoEstimator(self)
sklearn.tests.test_base.NoEstimator.__init__(self)
sklearn.tests.test_base.NoEstimator.fit(self,X=None,y=None)
sklearn.tests.test_base.NoEstimator.predict(self,X=None)
sklearn.tests.test_base.NoNaNTag(BaseEstimator)
sklearn.tests.test_base.NoNaNTag._more_tags(self)
sklearn.tests.test_base.OverrideTag(NaNTag)
sklearn.tests.test_base.OverrideTag._more_tags(self)
sklearn.tests.test_base.SingleInheritanceEstimator(self,attribute_pickled=5)
sklearn.tests.test_base.SingleInheritanceEstimator.__getstate__(self)
sklearn.tests.test_base.SingleInheritanceEstimator.__init__(self,attribute_pickled=5)
sklearn.tests.test_base.T(self,a=None,b=None)
sklearn.tests.test_base.T.__init__(self,a=None,b=None)
sklearn.tests.test_base.TreeBadVersion(DecisionTreeClassifier)
sklearn.tests.test_base.TreeBadVersion.__getstate__(self)
sklearn.tests.test_base.TreeNoVersion(DecisionTreeClassifier)
sklearn.tests.test_base.TreeNoVersion.__getstate__(self)
sklearn.tests.test_base.VargEstimator(self,*vargs)
sklearn.tests.test_base.VargEstimator.__init__(self,*vargs)
sklearn.tests.test_base._Empty
sklearn.tests.test_base.test_clone()
sklearn.tests.test_base.test_clone_2()
sklearn.tests.test_base.test_clone_buggy()
sklearn.tests.test_base.test_clone_class_rather_than_instance()
sklearn.tests.test_base.test_clone_dict()
sklearn.tests.test_base.test_clone_empty_array()
sklearn.tests.test_base.test_clone_estimator_types()
sklearn.tests.test_base.test_clone_keeps_output_config()
sklearn.tests.test_base.test_clone_nan()
sklearn.tests.test_base.test_clone_pandas_dataframe()
sklearn.tests.test_base.test_clone_protocol()
sklearn.tests.test_base.test_clone_sparse_matrices()
sklearn.tests.test_base.test_dataframe_protocol(constructor_name,minversion)
sklearn.tests.test_base.test_estimator_empty_instance_dict(estimator)
sklearn.tests.test_base.test_estimator_getstate_using_slots_error_message()
sklearn.tests.test_base.test_feature_names_in()
sklearn.tests.test_base.test_get_params()
sklearn.tests.test_base.test_is_classifier()
sklearn.tests.test_base.test_n_features_in_no_validation()
sklearn.tests.test_base.test_n_features_in_validation()
sklearn.tests.test_base.test_outlier_mixin_fit_predict_with_metadata_in_predict()
sklearn.tests.test_base.test_pickle_version_no_warning_is_issued_with_non_sklearn_estimator()
sklearn.tests.test_base.test_pickle_version_warning_is_issued_upon_different_version()
sklearn.tests.test_base.test_pickle_version_warning_is_issued_when_no_version_info_in_pickle()
sklearn.tests.test_base.test_pickle_version_warning_is_not_raised_with_matching_version()
sklearn.tests.test_base.test_pickling_when_getstate_is_overwritten_by_mixin()
sklearn.tests.test_base.test_pickling_when_getstate_is_overwritten_by_mixin_outside_of_sklearn()
sklearn.tests.test_base.test_pickling_works_when_getstate_is_overwritten_in_the_child_class()
sklearn.tests.test_base.test_raises_on_get_params_non_attribute()
sklearn.tests.test_base.test_repr()
sklearn.tests.test_base.test_repr_html_wraps()
sklearn.tests.test_base.test_repr_mimebundle_()
sklearn.tests.test_base.test_score_sample_weight(tree,dataset)
sklearn.tests.test_base.test_set_params()
sklearn.tests.test_base.test_set_params_passes_all_parameters()
sklearn.tests.test_base.test_set_params_updates_valid_params()
sklearn.tests.test_base.test_str()
sklearn.tests.test_base.test_tag_inheritance()
sklearn.tests.test_base.test_transformer_fit_transform_with_metadata_in_transform()
sklearn.tests.test_base.test_validate_data_cast_to_ndarray()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_calibration.py----------------------------------------
A:sklearn.tests.test_calibration.(X, y)->load_iris(return_X_y=True)
A:sklearn.tests.test_calibration.sample_weight->numpy.zeros_like(y)
A:sklearn.tests.test_calibration.clf->ClfWithoutSampleWeight()
A:sklearn.tests.test_calibration.cal_clf->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble)
A:sklearn.tests.test_calibration.calib_clf->CalibratedClassifierCV(estimator=vote, cv='prefit')
A:sklearn.tests.test_calibration.kfold->KFold(n_splits=splits)
A:sklearn.tests.test_calibration.estimator->LogisticRegression()
A:sklearn.tests.test_calibration.calibrated_clf->CalibratedClassifierCV(MockTensorClassifier())
A:sklearn.tests.test_calibration.probs_with_sw->CalibratedClassifierCV(MockTensorClassifier()).predict_proba(X_test)
A:sklearn.tests.test_calibration.probs_without_sw->CalibratedClassifierCV(MockTensorClassifier()).predict_proba(X_test)
A:sklearn.tests.test_calibration.diff->numpy.linalg.norm(probs_with_sw - probs_without_sw)
A:sklearn.tests.test_calibration.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=42)
A:sklearn.tests.test_calibration.cal_clf_parallel->CalibratedClassifierCV(estimator, method=method, n_jobs=2, ensemble=ensemble)
A:sklearn.tests.test_calibration.probs_parallel->CalibratedClassifierCV(estimator, method=method, n_jobs=2, ensemble=ensemble).predict_proba(X_test)
A:sklearn.tests.test_calibration.cal_clf_sequential->CalibratedClassifierCV(estimator, method=method, n_jobs=1, ensemble=ensemble)
A:sklearn.tests.test_calibration.probs_sequential->CalibratedClassifierCV(estimator, method=method, n_jobs=1, ensemble=ensemble).predict_proba(X_test)
A:sklearn.tests.test_calibration.probas->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble).predict_proba(X)
A:sklearn.tests.test_calibration.uncalibrated_brier->multiclass_brier(y_test, clf_probs, n_classes=n_classes)
A:sklearn.tests.test_calibration.calibrated_brier->multiclass_brier(y_test, cal_clf_probs, n_classes=n_classes)
A:sklearn.tests.test_calibration.clf_probs->ClfWithoutSampleWeight().predict_proba(X_test)
A:sklearn.tests.test_calibration.cal_clf_probs->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble).predict_proba(X_test)
A:sklearn.tests.test_calibration.calibrator->_SigmoidCalibration()
A:sklearn.tests.test_calibration.unfit_clf->CalibratedClassifierCV(clf, cv='prefit')
A:sklearn.tests.test_calibration.y_prob->numpy.array([])
A:sklearn.tests.test_calibration.y_pred->numpy.array([0.1, 0.2, 0.3, 0.4, 0.65, 0.7, 0.8, 0.9, 1.0])
A:sklearn.tests.test_calibration.cal_probas->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble).predict_proba(X)
A:sklearn.tests.test_calibration.unbiased_preds->cross_val_predict(clf, X, y, cv=3, method='decision_function')
A:sklearn.tests.test_calibration.clf_df->ClfWithoutSampleWeight().decision_function(X)
A:sklearn.tests.test_calibration.manual_probas->_SigmoidCalibration().predict(clf_df)
A:sklearn.tests.test_calibration.exF->numpy.array([5, -4, 1.0])
A:sklearn.tests.test_calibration.exY->numpy.array([1, -1, -1])
A:sklearn.tests.test_calibration.AB_lin_libsvm->numpy.array([-0.20261354391187855, 0.6523631498001051])
A:sklearn.tests.test_calibration.sk_prob->_SigmoidCalibration().fit(exF, exY).predict(exF)
A:sklearn.tests.test_calibration.y_true->numpy.array([0, 0, 0, 1, 1, 1, 1, 1, 1])
A:sklearn.tests.test_calibration.(prob_true, prob_pred)->calibration_curve(y, y_prob, pos_label=pos_label)
A:sklearn.tests.test_calibration.y_true2->numpy.array([0, 0, 0, 0, 1, 1])
A:sklearn.tests.test_calibration.y_pred2->numpy.array([0.0, 0.1, 0.2, 0.5, 0.9, 1.0])
A:sklearn.tests.test_calibration.(prob_true_quantile, prob_pred_quantile)->calibration_curve(y_true2, y_pred2, n_bins=2, strategy='quantile')
A:sklearn.tests.test_calibration.clf_c->CalibratedClassifierCV(clf, cv=2, method='isotonic', ensemble=ensemble)
A:sklearn.tests.test_calibration.clf_prob->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble)
A:sklearn.tests.test_calibration.probs->CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut(), ensemble=ensemble).predict_proba(X)
A:sklearn.tests.test_calibration.X->numpy.vstack((X[:40], X[50:90]))
A:sklearn.tests.test_calibration.y->numpy.random.RandomState(seed=global_random_seed).randint(0, 2, size=n)
A:sklearn.tests.test_calibration.proba->calibrated_classifier.predict_proba(X)
A:sklearn.tests.test_calibration.self.classes_->numpy.unique(y)
A:sklearn.tests.test_calibration.pipeline_prefit->Pipeline([('vectorizer', DictVectorizer()), ('clf', RandomForestClassifier())])
A:sklearn.tests.test_calibration.vote->VotingClassifier(estimators=[('lr' + str(i), LogisticRegression()) for i in range(3)], voting='soft')
A:sklearn.tests.test_calibration.lr->LogisticRegression().fit(X, y)
A:sklearn.tests.test_calibration.viz->sklearn.calibration.CalibrationDisplay.from_estimator(lr, X, y, pos_label=pos_label)
A:sklearn.tests.test_calibration.legend_labels->sklearn.calibration.CalibrationDisplay.from_estimator(lr, X, y, pos_label=pos_label).ax_.get_legend().get_texts()
A:sklearn.tests.test_calibration.prob_true->numpy.array([0, 1, 1, 0])
A:sklearn.tests.test_calibration.prob_pred->numpy.array([0.2, 0.8, 0.8, 0.4])
A:sklearn.tests.test_calibration.constructor->getattr(CalibrationDisplay, constructor_name)
A:sklearn.tests.test_calibration.dt->DecisionTreeClassifier().fit(X, y)
A:sklearn.tests.test_calibration.viz2->sklearn.calibration.CalibrationDisplay.from_estimator(dt, X, y, ax=viz.ax_)
A:sklearn.tests.test_calibration.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_calibration.y1->numpy.array(['spam'] * 3 + ['eggs'] * 2, dtype=dtype_y_str)
A:sklearn.tests.test_calibration.y2->numpy.random.RandomState(42).randint(0, 2, size=y1.size)
A:sklearn.tests.test_calibration.classes->numpy.array(['spam', 'egg'], dtype=dtype_y_str)
A:sklearn.tests.test_calibration.(prob_true, _)->calibration_curve(y_true_str, 1 - y_pred, n_bins=4, pos_label='spam')
A:sklearn.tests.test_calibration.X_twice->numpy.zeros((X.shape[0] * 2, X.shape[1]), dtype=X.dtype)
A:sklearn.tests.test_calibration.y_twice->numpy.zeros(y.shape[0] * 2, dtype=y.dtype)
A:sklearn.tests.test_calibration.calibrated_clf_without_weights->CalibratedClassifierCV(estimator, method=method, ensemble=ensemble, cv=2)
A:sklearn.tests.test_calibration.calibrated_clf_with_weights->clone(calibrated_clf_without_weights)
A:sklearn.tests.test_calibration.y_pred_with_weights->clone(calibrated_clf_without_weights).predict_proba(X)
A:sklearn.tests.test_calibration.y_pred_without_weights->CalibratedClassifierCV(estimator, method=method, ensemble=ensemble, cv=2).predict_proba(X)
A:sklearn.tests.test_calibration.pc_clf->CalibratedClassifierCV(clf)
A:sklearn.tests.test_calibration.random_noise->numpy.random.default_rng(global_random_seed).normal(size=n)
A:sklearn.tests.test_calibration.cv->check_cv(cv=None, y=y, classifier=True)
A:sklearn.tests.test_calibration.indices->check_cv(cv=None, y=y, classifier=True).split(X, y)
A:sklearn.tests.test_calibration.sgd_clf->SGDClassifier(loss='squared_hinge', random_state=global_random_seed)
A:sklearn.tests.test_calibration.predictions->SGDClassifier(loss='squared_hinge', random_state=global_random_seed).decision_function(X_test)
A:sklearn.tests.test_calibration.clf_sigmoid->CalibratedClassifierCV(SGDClassifier(loss='squared_hinge', random_state=global_random_seed), method='sigmoid')
A:sklearn.tests.test_calibration.score_sigmoid->cross_val_score(clf_sigmoid, X, y, scoring='roc_auc')
A:sklearn.tests.test_calibration.clf_isotonic->CalibratedClassifierCV(SGDClassifier(loss='squared_hinge', random_state=global_random_seed), method='isotonic')
A:sklearn.tests.test_calibration.score_isotonic->cross_val_score(clf_isotonic, X, y, scoring='roc_auc')
A:sklearn.tests.test_calibration.random_state->numpy.random.RandomState(seed=global_random_seed)
A:sklearn.tests.test_calibration.predictions_small->numpy.random.RandomState(seed=global_random_seed).uniform(low=-2, high=2, size=100)
A:sklearn.tests.test_calibration.(a1, b1)->_sigmoid_calibration(predictions=predictions_small, y=y, max_abs_prediction_threshold=threshold_1)
A:sklearn.tests.test_calibration.(a2, b2)->_sigmoid_calibration(predictions=predictions_small, y=y, max_abs_prediction_threshold=threshold_2)
A:sklearn.tests.test_calibration.(a3, b3)->_sigmoid_calibration(predictions=predictions_small, y=y)
sklearn.tests.test_calibration.data()
sklearn.tests.test_calibration.dict_data()
sklearn.tests.test_calibration.dict_data_pipeline(dict_data)
sklearn.tests.test_calibration.iris_data()
sklearn.tests.test_calibration.iris_data_binary(iris_data)
sklearn.tests.test_calibration.test_calibrated_classifier_cv_double_sample_weights_equivalence(method,ensemble)
sklearn.tests.test_calibration.test_calibrated_classifier_cv_works_with_large_confidence_scores(global_random_seed)
sklearn.tests.test_calibration.test_calibrated_classifier_cv_zeros_sample_weights_equivalence(method,ensemble)
sklearn.tests.test_calibration.test_calibration(data,method,csr_container,ensemble)
sklearn.tests.test_calibration.test_calibration_accepts_ndarray(X)
sklearn.tests.test_calibration.test_calibration_attributes(clf,cv)
sklearn.tests.test_calibration.test_calibration_curve()
sklearn.tests.test_calibration.test_calibration_curve_pos_label(dtype_y_str)
sklearn.tests.test_calibration.test_calibration_curve_pos_label_error_str(dtype_y_str)
sklearn.tests.test_calibration.test_calibration_cv_splitter(data,ensemble)
sklearn.tests.test_calibration.test_calibration_default_estimator(data)
sklearn.tests.test_calibration.test_calibration_dict_pipeline(dict_data,dict_data_pipeline)
sklearn.tests.test_calibration.test_calibration_display_compute(pyplot,iris_data_binary,n_bins,strategy)
sklearn.tests.test_calibration.test_calibration_display_default_labels(pyplot,name,expected_label)
sklearn.tests.test_calibration.test_calibration_display_label_class_plot(pyplot)
sklearn.tests.test_calibration.test_calibration_display_name_multiple_calls(constructor_name,pyplot,iris_data_binary)
sklearn.tests.test_calibration.test_calibration_display_pos_label(pyplot,iris_data_binary,pos_label,expected_pos_label)
sklearn.tests.test_calibration.test_calibration_display_ref_line(pyplot,iris_data_binary)
sklearn.tests.test_calibration.test_calibration_ensemble_false(data,method)
sklearn.tests.test_calibration.test_calibration_inconsistent_prefit_n_features_in()
sklearn.tests.test_calibration.test_calibration_less_classes(ensemble)
sklearn.tests.test_calibration.test_calibration_multiclass(method,ensemble,seed)
sklearn.tests.test_calibration.test_calibration_nan_imputer(ensemble)
sklearn.tests.test_calibration.test_calibration_prefit(csr_container)
sklearn.tests.test_calibration.test_calibration_prob_sum(ensemble)
sklearn.tests.test_calibration.test_calibration_votingclassifier()
sklearn.tests.test_calibration.test_calibration_with_fit_params(fit_params_type,data)
sklearn.tests.test_calibration.test_calibration_with_non_sample_aligned_fit_param(data)
sklearn.tests.test_calibration.test_calibration_with_sample_weight_estimator(sample_weight,data)
sklearn.tests.test_calibration.test_calibration_without_sample_weight_estimator(data)
sklearn.tests.test_calibration.test_calibration_zero_probability()
sklearn.tests.test_calibration.test_parallel_execution(data,method,ensemble)
sklearn.tests.test_calibration.test_plot_calibration_curve_pipeline(pyplot,iris_data_binary)
sklearn.tests.test_calibration.test_sample_weight(data,method,ensemble)
sklearn.tests.test_calibration.test_sigmoid_calibration()
sklearn.tests.test_calibration.test_sigmoid_calibration_max_abs_prediction_threshold(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_build.py----------------------------------------
A:sklearn.tests.test_build.err_msg->textwrap.dedent('\n        This test fails because scikit-learn has been built without OpenMP.\n        This is not recommended since some estimators will run in sequential\n        mode instead of leveraging thread-based parallelism.\n\n        You can find instructions to build scikit-learn with OpenMP at this\n        address:\n\n            https://scikit-learn.org/{}/developers/advanced_installation.html\n\n        You can skip this test by setting the environment variable\n        SKLEARN_SKIP_OPENMP_TEST to any value.\n        ').format(base_url)
sklearn.tests.test_build.test_openmp_parallelism_enabled()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_discriminant_analysis.py----------------------------------------
A:sklearn.tests.test_discriminant_analysis.X->numpy.array([[0.5, 0.6], [0.6, 0.5]])
A:sklearn.tests.test_discriminant_analysis.y->numpy.array(['a', 'b'])
A:sklearn.tests.test_discriminant_analysis.y3->numpy.array([1, 1, 2, 2, 3, 3])
A:sklearn.tests.test_discriminant_analysis.X1->numpy.array([[-2], [-1], [-1], [1], [1], [2]], dtype='f')
A:sklearn.tests.test_discriminant_analysis.X6->numpy.array([[0, 0], [-2, -2], [-2, -1], [-1, -1], [-1, -2], [1, 3], [1, 2], [2, 1], [2, 2]])
A:sklearn.tests.test_discriminant_analysis.y6->numpy.array([1, 1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.tests.test_discriminant_analysis.y7->numpy.array([1, 2, 3, 2, 3, 1, 2, 3, 1])
A:sklearn.tests.test_discriminant_analysis.X7->numpy.array([[-3], [-2], [-1], [-1], [0], [1], [1], [2], [3]])
A:sklearn.tests.test_discriminant_analysis.X2->numpy.array([[-3, 0], [-2, 0], [-1, 0], [-1, 0], [0, 0], [1, 0], [1, 0], [2, 0], [3, 0]])
A:sklearn.tests.test_discriminant_analysis.y4->numpy.array([1, 1, 1, 1, 1, 1, 1, 1, 2])
A:sklearn.tests.test_discriminant_analysis.y5->numpy.array([0, 0, 0, 0, 0, 1, 1, 1])
A:sklearn.tests.test_discriminant_analysis.clf->LinearDiscriminantAnalysis(solver=solver)
A:sklearn.tests.test_discriminant_analysis.y_pred->LinearDiscriminantAnalysis(solver=solver).predict(X2)
A:sklearn.tests.test_discriminant_analysis.y_pred1->LinearDiscriminantAnalysis(solver=solver).fit(X7, y6).predict(X7)
A:sklearn.tests.test_discriminant_analysis.y_proba_pred1->LinearDiscriminantAnalysis(solver=solver).predict_proba(X7)
A:sklearn.tests.test_discriminant_analysis.y_log_proba_pred1->LinearDiscriminantAnalysis(solver=solver).predict_log_proba(X7)
A:sklearn.tests.test_discriminant_analysis.y_pred3->LinearDiscriminantAnalysis(solver=solver).fit(X6, y7).predict(X6)
A:sklearn.tests.test_discriminant_analysis.rng->check_random_state(0)
A:sklearn.tests.test_discriminant_analysis.blob_stds->numpy.array([[[10, 10], [10, 100]]] * len(blob_centers))
A:sklearn.tests.test_discriminant_analysis.(X, y)->make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=11)
A:sklearn.tests.test_discriminant_analysis.lda->LinearDiscriminantAnalysis(n_components=n_components)
A:sklearn.tests.test_discriminant_analysis.precision->scipy.linalg.inv(blob_stds[0])
A:sklearn.tests.test_discriminant_analysis.sample->numpy.array([[-22, 22]])
A:sklearn.tests.test_discriminant_analysis.prob->numpy.array([float(discriminant_func(sample, alpha_k, alpha_k_0, clazz) / (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz) for clazz in range(n_classes - 1)]))) for clazz in range(n_classes - 1)])
A:sklearn.tests.test_discriminant_analysis.prob_ref_2->float(1 / (1 + sum([discriminant_func(sample, alpha_k, alpha_k_0, clazz) for clazz in range(n_classes - 1)])))
A:sklearn.tests.test_discriminant_analysis.priors->numpy.array([0.5, 0.5])
A:sklearn.tests.test_discriminant_analysis.prior_norm->numpy.array([0.45, 0.55])
A:sklearn.tests.test_discriminant_analysis.clf_lda_svd->LinearDiscriminantAnalysis(solver='svd')
A:sklearn.tests.test_discriminant_analysis.clf_lda_lsqr->LinearDiscriminantAnalysis(solver='lsqr')
A:sklearn.tests.test_discriminant_analysis.clf_lda_eigen->LinearDiscriminantAnalysis(solver='eigen')
A:sklearn.tests.test_discriminant_analysis.X_transformed->LinearDiscriminantAnalysis(solver=solver).fit(X, y).transform(X)
A:sklearn.tests.test_discriminant_analysis.state->numpy.random.RandomState(0)
A:sklearn.tests.test_discriminant_analysis.means->numpy.array([[0, 0, -1], [0, 2, 0], [0, -2, 0], [0, 0, 5]])
A:sklearn.tests.test_discriminant_analysis.scatter->numpy.array([[0.1, 0, 0], [-0.1, 0, 0], [0, 0.1, 0], [0, -0.1, 0], [0, 0, 0.1], [0, 0, -0.1]])
A:sklearn.tests.test_discriminant_analysis.means_transformed->LinearDiscriminantAnalysis(solver=solver).transform(means)
A:sklearn.tests.test_discriminant_analysis.c1->LinearDiscriminantAnalysis(store_covariance=True, shrinkage='auto', solver='lsqr')
A:sklearn.tests.test_discriminant_analysis.c2->LinearDiscriminantAnalysis(store_covariance=True, covariance_estimator=StandardizedLedoitWolf(), solver='lsqr')
A:sklearn.tests.test_discriminant_analysis.sc->StandardScaler()
A:sklearn.tests.test_discriminant_analysis.X_sc->StandardScaler().fit_transform(X)
A:sklearn.tests.test_discriminant_analysis.max_components->min(n_features, n_classes - 1)
A:sklearn.tests.test_discriminant_analysis.clf_32->LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)
A:sklearn.tests.test_discriminant_analysis.clf_64->LinearDiscriminantAnalysis(solver=solver, shrinkage=shrinkage)
A:sklearn.tests.test_discriminant_analysis.n_pos->numpy.sum(y_pred == 2)
A:sklearn.tests.test_discriminant_analysis.n_pos2->numpy.sum(y_pred == 2)
A:sklearn.tests.test_discriminant_analysis.qda->QuadraticDiscriminantAnalysis(priors=priors).fit(X, y)
A:sklearn.tests.test_discriminant_analysis.y_pred5->LinearDiscriminantAnalysis(solver=solver).predict(X5)
A:sklearn.tests.test_discriminant_analysis.(x, y)->make_blobs(n_samples=100, n_features=5, centers=1, random_state=42)
A:sklearn.tests.test_discriminant_analysis.x->numpy.dot(x, np.arange(x.shape[1] ** 2).reshape(x.shape[1], x.shape[1]))
A:sklearn.tests.test_discriminant_analysis.c_e->_cov(x, 'empirical')
A:sklearn.tests.test_discriminant_analysis.c_s->_cov(x, 'auto')
A:sklearn.tests.test_discriminant_analysis.est->LinearDiscriminantAnalysis().fit(X, y)
A:sklearn.tests.test_discriminant_analysis.names_out->LinearDiscriminantAnalysis().fit(X, y).get_feature_names_out()
A:sklearn.tests.test_discriminant_analysis.class_name_lower->'LinearDiscriminantAnalysis'.lower()
A:sklearn.tests.test_discriminant_analysis.expected_names_out->numpy.array([f'{class_name_lower}{i}' for i in range(est.explained_variance_ratio_.shape[0])], dtype=object)
sklearn.tests.test_discriminant_analysis.test_covariance()
sklearn.tests.test_discriminant_analysis.test_get_feature_names_out()
sklearn.tests.test_discriminant_analysis.test_lda_coefs()
sklearn.tests.test_discriminant_analysis.test_lda_dimension_warning(n_classes,n_features)
sklearn.tests.test_discriminant_analysis.test_lda_dtype_match(data_type,expected_type)
sklearn.tests.test_discriminant_analysis.test_lda_explained_variance_ratio()
sklearn.tests.test_discriminant_analysis.test_lda_ledoitwolf()
sklearn.tests.test_discriminant_analysis.test_lda_numeric_consistency_float32_float64()
sklearn.tests.test_discriminant_analysis.test_lda_orthogonality()
sklearn.tests.test_discriminant_analysis.test_lda_predict()
sklearn.tests.test_discriminant_analysis.test_lda_predict_proba(solver,n_classes)
sklearn.tests.test_discriminant_analysis.test_lda_priors()
sklearn.tests.test_discriminant_analysis.test_lda_scaling()
sklearn.tests.test_discriminant_analysis.test_lda_shrinkage(seed)
sklearn.tests.test_discriminant_analysis.test_lda_store_covariance()
sklearn.tests.test_discriminant_analysis.test_lda_transform()
sklearn.tests.test_discriminant_analysis.test_qda()
sklearn.tests.test_discriminant_analysis.test_qda_prior_copy()
sklearn.tests.test_discriminant_analysis.test_qda_prior_type(priors_type)
sklearn.tests.test_discriminant_analysis.test_qda_priors()
sklearn.tests.test_discriminant_analysis.test_qda_regularization()
sklearn.tests.test_discriminant_analysis.test_qda_store_covariance()
sklearn.tests.test_discriminant_analysis.test_raises_value_error_on_same_number_of_classes_and_samples(solver)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_kernel_ridge.py----------------------------------------
A:sklearn.tests.test_kernel_ridge.(X, y)->make_regression(n_features=10, random_state=0)
A:sklearn.tests.test_kernel_ridge.pred->Ridge(alpha=1, fit_intercept=False).fit(X, Y).predict(X)
A:sklearn.tests.test_kernel_ridge.pred2->KernelRidge(kernel='linear', alpha=1).fit(X, Y).predict(X)
A:sklearn.tests.test_kernel_ridge.X_sparse->sparse_container(X)
A:sklearn.tests.test_kernel_ridge.kr->KernelRidge(kernel='linear', alpha=0)
A:sklearn.tests.test_kernel_ridge.K->numpy.dot(X, X.T)
A:sklearn.tests.test_kernel_ridge.K2->numpy.dot(X, X.T).copy()
A:sklearn.tests.test_kernel_ridge.sw->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.tests.test_kernel_ridge.pred3->KernelRidge(kernel='linear', alpha=1).fit(X, y).predict(X)
sklearn.tests.test_kernel_ridge.test_kernel_ridge()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_multi_output()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_precomputed()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_precomputed_kernel_unchanged()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_sample_weights()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_singular_kernel()
sklearn.tests.test_kernel_ridge.test_kernel_ridge_sparse(sparse_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_random_projection.py----------------------------------------
A:sklearn.tests.test_random_projection.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_random_projection.data_coo->coo_container((rng.randn(n_nonzeros), (rng.randint(n_samples, size=n_nonzeros), rng.randint(n_features, size=n_nonzeros))), shape=(n_samples, n_features))
A:sklearn.tests.test_random_projection.n_nonzeros->int(n_features / 4)
A:sklearn.tests.test_random_projection.A->densify(A)
A:sklearn.tests.test_random_projection.random_matrix_dense->functools.partial(random_matrix, density=1.0)
A:sklearn.tests.test_random_projection.values->numpy.unique(A)
A:sklearn.tests.test_random_projection.data->make_sparse_random_data(coo_container, n_samples, n_features, n_nonzeros, random_state=global_random_seed, sparse_format=None)
A:sklearn.tests.test_random_projection.rp->random_projection_cls(random_state=0)
A:sklearn.tests.test_random_projection.original_distances->original_distances.ravel().ravel()
A:sklearn.tests.test_random_projection.projected->projected.toarray().toarray()
A:sklearn.tests.test_random_projection.projected_distances->projected_distances.ravel().ravel()
A:sklearn.tests.test_random_projection.dense_data->make_sparse_random_data(coo_container, n_samples, n_features, n_nonzeros, random_state=global_random_seed, sparse_format=None)
A:sklearn.tests.test_random_projection.sparse_data->make_sparse_random_data(coo_container, n_samples, n_features, n_nonzeros, random_state=global_random_seed, sparse_format='csr')
A:sklearn.tests.test_random_projection.projected_1->random_projection_cls(random_state=0).transform(data)
A:sklearn.tests.test_random_projection.projected_2->random_projection_cls(random_state=0).transform(data)
A:sklearn.tests.test_random_projection.rp2->RandomProjection(random_state=0, eps=0.5)
A:sklearn.tests.test_random_projection.projected_3->RandomProjection(random_state=0, eps=0.5).fit_transform(data)
A:sklearn.tests.test_random_projection.rp_dense->RandomProjection(n_components=3, random_state=1).fit(dense_data)
A:sklearn.tests.test_random_projection.rp_sparse->RandomProjection(n_components=3, random_state=1).fit(sparse_data)
A:sklearn.tests.test_random_projection.random_projection->random_projection_cls(n_components=n_components, compute_inverse_components=compute_inverse_components, random_state=global_random_seed)
A:sklearn.tests.test_random_projection.names_out->random_projection_cls(n_components=n_components, compute_inverse_components=compute_inverse_components, random_state=global_random_seed).get_feature_names_out()
A:sklearn.tests.test_random_projection.class_name_lower->random_projection_cls.__name__.lower()
A:sklearn.tests.test_random_projection.expected_names_out->numpy.array([f'{class_name_lower}{i}' for i in range(random_projection.n_components_)], dtype=object)
A:sklearn.tests.test_random_projection.X_dense->make_sparse_random_data(coo_container, n_samples, n_features, n_nonzeros=n_samples * n_features // 100 + 1, random_state=global_random_seed, sparse_format=None)
A:sklearn.tests.test_random_projection.X_csr->make_sparse_random_data(coo_container, n_samples, n_features, n_nonzeros=n_samples * n_features // 100 + 1, random_state=global_random_seed, sparse_format='csr')
A:sklearn.tests.test_random_projection.projected_back->random_projection_cls(n_components=n_components, compute_inverse_components=compute_inverse_components, random_state=global_random_seed).inverse_transform(projected)
A:sklearn.tests.test_random_projection.projected_again->random_projection_cls(n_components=n_components, compute_inverse_components=compute_inverse_components, random_state=global_random_seed).transform(projected_back)
A:sklearn.tests.test_random_projection.X->numpy.random.RandomState(42).rand(25, 3000)
A:sklearn.tests.test_random_projection.transformed->random_projection_cls(random_state=0).fit_transform(X.astype(input_dtype))
A:sklearn.tests.test_random_projection.rp_32->random_projection_cls(random_state=0)
A:sklearn.tests.test_random_projection.rp_64->random_projection_cls(random_state=0)
A:sklearn.tests.test_random_projection.projection_32->random_projection_cls(random_state=0).fit_transform(X.astype(np.float32))
A:sklearn.tests.test_random_projection.projection_64->random_projection_cls(random_state=0).fit_transform(X.astype(np.float64))
sklearn.tests.test_random_projection.check_input_size_random_matrix(random_matrix)
sklearn.tests.test_random_projection.check_input_with_sparse_random_matrix(random_matrix)
sklearn.tests.test_random_projection.check_size_generated(random_matrix)
sklearn.tests.test_random_projection.check_zero_mean_and_unit_norm(random_matrix)
sklearn.tests.test_random_projection.densify(matrix)
sklearn.tests.test_random_projection.make_sparse_random_data(coo_container,n_samples,n_features,n_nonzeros,random_state=None,sparse_format='csr')
sklearn.tests.test_random_projection.test_SparseRandomProj_output_representation(coo_container)
sklearn.tests.test_random_projection.test_basic_property_of_random_matrix(random_matrix)
sklearn.tests.test_random_projection.test_basic_property_of_sparse_random_matrix(random_matrix)
sklearn.tests.test_random_projection.test_correct_RandomProjection_dimensions_embedding(coo_container,global_random_seed)
sklearn.tests.test_random_projection.test_gaussian_random_matrix()
sklearn.tests.test_random_projection.test_input_size_jl_min_dim()
sklearn.tests.test_random_projection.test_invalid_jl_domain(n_samples,eps)
sklearn.tests.test_random_projection.test_inverse_transform(coo_container,n_samples,n_features,random_projection_cls,compute_inverse_components,global_random_seed)
sklearn.tests.test_random_projection.test_johnson_lindenstrauss_min_dim()
sklearn.tests.test_random_projection.test_random_projection_dtype_match(random_projection_cls,input_dtype,expected_dtype)
sklearn.tests.test_random_projection.test_random_projection_embedding_quality(coo_container)
sklearn.tests.test_random_projection.test_random_projection_feature_names_out(coo_container,random_projection_cls,global_random_seed)
sklearn.tests.test_random_projection.test_random_projection_numerical_consistency(random_projection_cls)
sklearn.tests.test_random_projection.test_random_projection_transformer_invalid_input()
sklearn.tests.test_random_projection.test_sparse_random_matrix()
sklearn.tests.test_random_projection.test_too_many_samples_to_find_a_safe_embedding(coo_container,global_random_seed)
sklearn.tests.test_random_projection.test_try_to_transform_before_fit(coo_container,global_random_seed)
sklearn.tests.test_random_projection.test_warning_n_components_greater_than_n_features(coo_container,global_random_seed)
sklearn.tests.test_random_projection.test_works_with_sparse_data(coo_container,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/metadata_routing_common.py----------------------------------------
A:sklearn.tests.metadata_routing_common.records->getattr(obj, '_records', dict()).get(method, dict())
A:sklearn.tests.metadata_routing_common.record_metadata_not_default->partial(record_metadata, record_default=False)
A:sklearn.tests.metadata_routing_common.mmr->getattr(request, method)
A:sklearn.tests.metadata_routing_common.self.classes_->numpy.unique(y)
A:sklearn.tests.metadata_routing_common.sample_weight->kwargs.get('sample_weight', None)
A:sklearn.tests.metadata_routing_common.train_indices->list(range(0, split_index))
A:sklearn.tests.metadata_routing_common.test_indices->list(range(split_index, len(X)))
A:sklearn.tests.metadata_routing_common.params->process_routing(self, 'transform', **transform_params)
A:sklearn.tests.metadata_routing_common.self.estimator_->clone(self.estimator).fit(X, y, **params.estimator.fit)
A:sklearn.tests.metadata_routing_common.router->MetadataRouter(owner=self.__class__.__name__).add_self_request(self).add(estimator=self.estimator, method_mapping='fit')
A:sklearn.tests.metadata_routing_common.self.transformer_->clone(self.transformer).fit(X, y, **params.transformer.fit)
sklearn.tests.metadata_routing_common.ConsumingClassifier(self,registry=None,alpha=0.0)
sklearn.tests.metadata_routing_common.ConsumingClassifier.__init__(self,registry=None,alpha=0.0)
sklearn.tests.metadata_routing_common.ConsumingClassifier.decision_function(self,X,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingClassifier.fit(self,X,y,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingClassifier.partial_fit(self,X,y,classes=None,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingClassifier.predict(self,X,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingClassifier.predict_log_proba(self,X,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingClassifier.predict_proba(self,X,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingRegressor(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingRegressor.__init__(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingRegressor.fit(self,X,y,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingRegressor.partial_fit(self,X,y,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingRegressor.predict(self,X,sample_weight='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingScorer(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingScorer.__init__(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingScorer._score(self,method_caller,clf,X,y,**kwargs)
sklearn.tests.metadata_routing_common.ConsumingSplitter(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingSplitter.__init__(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingSplitter._iter_test_indices(self,X=None,y=None,groups=None)
sklearn.tests.metadata_routing_common.ConsumingSplitter.get_n_splits(self,X=None,y=None,groups=None,metadata=None)
sklearn.tests.metadata_routing_common.ConsumingSplitter.split(self,X,y=None,groups='default',metadata='default')
sklearn.tests.metadata_routing_common.ConsumingTransformer(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingTransformer.__init__(self,registry=None)
sklearn.tests.metadata_routing_common.ConsumingTransformer.fit(self,X,y=None,sample_weight=None,metadata=None)
sklearn.tests.metadata_routing_common.ConsumingTransformer.fit_transform(self,X,y,sample_weight=None,metadata=None)
sklearn.tests.metadata_routing_common.ConsumingTransformer.inverse_transform(self,X,sample_weight=None,metadata=None)
sklearn.tests.metadata_routing_common.ConsumingTransformer.transform(self,X,sample_weight=None,metadata=None)
sklearn.tests.metadata_routing_common.MetaRegressor(self,estimator)
sklearn.tests.metadata_routing_common.MetaRegressor.__init__(self,estimator)
sklearn.tests.metadata_routing_common.MetaRegressor.fit(self,X,y,**fit_params)
sklearn.tests.metadata_routing_common.MetaRegressor.get_metadata_routing(self)
sklearn.tests.metadata_routing_common.MetaTransformer(self,transformer)
sklearn.tests.metadata_routing_common.MetaTransformer.__init__(self,transformer)
sklearn.tests.metadata_routing_common.MetaTransformer.fit(self,X,y=None,**fit_params)
sklearn.tests.metadata_routing_common.MetaTransformer.get_metadata_routing(self)
sklearn.tests.metadata_routing_common.MetaTransformer.transform(self,X,y=None,**transform_params)
sklearn.tests.metadata_routing_common.NonConsumingClassifier(self,registry=None)
sklearn.tests.metadata_routing_common.NonConsumingClassifier.__init__(self,registry=None)
sklearn.tests.metadata_routing_common.NonConsumingClassifier.fit(self,X,y)
sklearn.tests.metadata_routing_common.NonConsumingClassifier.predict(self,X)
sklearn.tests.metadata_routing_common.WeightedMetaClassifier(self,estimator,registry=None)
sklearn.tests.metadata_routing_common.WeightedMetaClassifier.__init__(self,estimator,registry=None)
sklearn.tests.metadata_routing_common.WeightedMetaClassifier.fit(self,X,y,sample_weight=None,**kwargs)
sklearn.tests.metadata_routing_common.WeightedMetaClassifier.get_metadata_routing(self)
sklearn.tests.metadata_routing_common.WeightedMetaRegressor(self,estimator,registry=None)
sklearn.tests.metadata_routing_common.WeightedMetaRegressor.__init__(self,estimator,registry=None)
sklearn.tests.metadata_routing_common.WeightedMetaRegressor.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.tests.metadata_routing_common.WeightedMetaRegressor.get_metadata_routing(self)
sklearn.tests.metadata_routing_common.WeightedMetaRegressor.predict(self,X,**predict_params)
sklearn.tests.metadata_routing_common._Registry(list)
sklearn.tests.metadata_routing_common._Registry.__copy__(self)
sklearn.tests.metadata_routing_common._Registry.__deepcopy__(self,memo)
sklearn.tests.metadata_routing_common.assert_request_equal(request,dictionary)
sklearn.tests.metadata_routing_common.assert_request_is_empty(metadata_request,exclude=None)
sklearn.tests.metadata_routing_common.check_recorded_metadata(obj,method,split_params=tuple(),**kwargs)
sklearn.tests.metadata_routing_common.record_metadata(obj,method,record_default=True,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_multioutput.py----------------------------------------
A:sklearn.tests.test_multioutput.(X, y)->make_classification(n_samples=50)
A:sklearn.tests.test_multioutput.references->numpy.zeros_like(y_test)
A:sklearn.tests.test_multioutput.rgr->MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
A:sklearn.tests.test_multioutput.references[:, n]->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5)).predict(X_test)
A:sklearn.tests.test_multioutput.y_pred->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5)).predict(X_test)
A:sklearn.tests.test_multioutput.sgr->MultiOutputRegressor(SGDRegressor(random_state=0, max_iter=5))
A:sklearn.tests.test_multioutput.rgr_sparse->MultiOutputRegressor(Lasso(random_state=0))
A:sklearn.tests.test_multioutput.rgr_w->MultiOutputRegressor(GradientBoostingRegressor(random_state=0))
A:sklearn.tests.test_multioutput.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_multioutput.y2->numpy.array(['d', 'e', 'f', 'e', 'd']).reshape(5, 1)
A:sklearn.tests.test_multioutput.y3->shuffle(y1, random_state=2)
A:sklearn.tests.test_multioutput.y->numpy.column_stack((y1, y2, y3))
A:sklearn.tests.test_multioutput.n_classes->len(np.unique(y1))
A:sklearn.tests.test_multioutput.classes->list(map(np.unique, (y1, y2, y3)))
A:sklearn.tests.test_multioutput.sgd_linear_clf->SGDClassifier(random_state=1, max_iter=20)
A:sklearn.tests.test_multioutput.mor->MultiOutputClassifier(sgd_linear_clf, n_jobs=4)
A:sklearn.tests.test_multioutput.multi_target_linear->MultiOutputClassifier(sgd_linear_clf)
A:sklearn.tests.test_multioutput.grid_clf->GridSearchCV(sgd_linear_clf, param_grid=param, scoring=custom_scorer, cv=3, error_score='raise')
A:sklearn.tests.test_multioutput.first_predictions->MultiOutputClassifier(sgd_linear_clf).predict(X)
A:sklearn.tests.test_multioutput.second_predictions->MultiOutputClassifier(sgd_linear_clf).predict(X)
A:sklearn.tests.test_multioutput.forest->RandomForestClassifier(n_estimators=10, random_state=1)
A:sklearn.tests.test_multioutput.multi_target_forest->MultiOutputClassifier(forest)
A:sklearn.tests.test_multioutput.predictions->MultiOutputClassifier(multi_class_svc).predict(X)
A:sklearn.tests.test_multioutput.predict_proba->MultiOutputClassifier(forest).predict_proba(X)
A:sklearn.tests.test_multioutput.forest_->clone(forest)
A:sklearn.tests.test_multioutput.svc->LinearSVC(dual='auto', random_state=0)
A:sklearn.tests.test_multioutput.multi_class_svc->OneVsRestClassifier(svc)
A:sklearn.tests.test_multioutput.multi_target_svc->MultiOutputClassifier(multi_class_svc)
A:sklearn.tests.test_multioutput.multi_class_svc_->clone(multi_class_svc)
A:sklearn.tests.test_multioutput.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_multioutput.X->numpy.random.RandomState(42).normal(size=(5, 5))
A:sklearn.tests.test_multioutput.y1->numpy.array(['b', 'a', 'a', 'b', 'a']).reshape(5, 1)
A:sklearn.tests.test_multioutput.Y->numpy.concatenate([y1, y2], axis=1)
A:sklearn.tests.test_multioutput.clf->Cls(PassiveAggressiveClassifier())
A:sklearn.tests.test_multioutput.y_result->Cls(PassiveAggressiveClassifier()).predict_proba(X)
A:sklearn.tests.test_multioutput.w->numpy.asarray([2.0, 1.0, 1.0])
A:sklearn.tests.test_multioutput.clf_w->MultiOutputClassifier(sgd_linear_clf)
A:sklearn.tests.test_multioutput.moc->MultiOutputClassifier(LinearSVC(dual='auto'))
A:sklearn.tests.test_multioutput.y_new->numpy.column_stack((y1, y2))
A:sklearn.tests.test_multioutput.Y_multi->numpy.array([[int(yyy) for yyy in format(yy, '#06b')[2:]] for yy in y])
A:sklearn.tests.test_multioutput.(X, Y)->generate_multilabel_dataset_with_correlations()
A:sklearn.tests.test_multioutput.classifier_chain->ClassifierChain(LogisticRegression())
A:sklearn.tests.test_multioutput.Y_pred->ClassifierChain(RandomForestClassifier(), order=order).predict(X)
A:sklearn.tests.test_multioutput.Y_decision->ClassifierChain(LogisticRegression()).decision_function(X)
A:sklearn.tests.test_multioutput.X_sparse->csr_container(X)
A:sklearn.tests.test_multioutput.Y_pred_sparse->ClassifierChain(LogisticRegression()).predict(X_sparse)
A:sklearn.tests.test_multioutput.Y_pred_dense->ClassifierChain(LogisticRegression()).predict(X)
A:sklearn.tests.test_multioutput.ovr->OneVsRestClassifier(LogisticRegression())
A:sklearn.tests.test_multioutput.Y_pred_ovr->OneVsRestClassifier(LogisticRegression()).predict(X_test)
A:sklearn.tests.test_multioutput.chain->ClassifierChain(RandomForestClassifier(), order=order)
A:sklearn.tests.test_multioutput.Y_pred_chain->ClassifierChain(RandomForestClassifier(), order=order).predict(X_test)
A:sklearn.tests.test_multioutput.Y_prob->numpy.exp(Y_prob)
A:sklearn.tests.test_multioutput.chain_random->clone(chain).set_params(order='random', random_state=42)
A:sklearn.tests.test_multioutput.chain_fixed->clone(chain).set_params(order=chain_random.order_)
A:sklearn.tests.test_multioutput.chain_cv->clone(chain).set_params(cv=3)
A:sklearn.tests.test_multioutput.Y_pred_cv->clone(chain).set_params(cv=3).predict(X)
A:sklearn.tests.test_multioutput.some_param->numpy.zeros_like(X)
A:sklearn.tests.test_multioutput.weight->numpy.random.RandomState(42).rand(y.shape[0])
A:sklearn.tests.test_multioutput.model->RegressorChain(MySGD())
A:sklearn.tests.test_multioutput.mask->numpy.random.RandomState(42).choice([1, 0], X.shape, p=[0.01, 0.99]).astype(bool)
A:sklearn.tests.test_multioutput.pipe->make_pipeline(SimpleImputer(), Estimator())
A:sklearn.tests.test_multioutput.order->tuple([1, 2])
A:sklearn.tests.test_multioutput.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.tests.test_multioutput.classifier->ClassifierChain(DecisionTreeClassifier(), order=[0, 1, 2], random_state=0, verbose=True)
A:sklearn.tests.test_multioutput.regressor->RegressorChain(LinearRegression(), order=[1, 0, 2], random_state=0, verbose=True)
A:sklearn.tests.test_multioutput.stacker->StackingRegressor(estimators=[('sgd', SGDRegressor(random_state=1))], final_estimator=Ridge(), cv=2)
A:sklearn.tests.test_multioutput.reg->MultiOutputRegressor(estimator=stacker).fit(X, y)
A:sklearn.tests.test_multioutput.est->MultiOutputRegressor(LinearRegression())
sklearn.tests.test_multioutput.DummyClassifierWithFitParams(DummyClassifier)
sklearn.tests.test_multioutput.DummyClassifierWithFitParams.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.tests.test_multioutput.DummyRegressorWithFitParams(DummyRegressor)
sklearn.tests.test_multioutput.DummyRegressorWithFitParams.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.tests.test_multioutput.generate_multilabel_dataset_with_correlations()
sklearn.tests.test_multioutput.test_base_chain_crossval_fit_and_predict()
sklearn.tests.test_multioutput.test_base_chain_fit_and_predict(response_method)
sklearn.tests.test_multioutput.test_base_chain_fit_and_predict_with_sparse_data_and_cv(csr_container)
sklearn.tests.test_multioutput.test_base_chain_random_order()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_linear_svc()
sklearn.tests.test_multioutput.test_classifier_chain_fit_and_predict_with_sparse_data(csr_container)
sklearn.tests.test_multioutput.test_classifier_chain_tuple_invalid_order()
sklearn.tests.test_multioutput.test_classifier_chain_tuple_order(order_type)
sklearn.tests.test_multioutput.test_classifier_chain_verbose(capsys)
sklearn.tests.test_multioutput.test_classifier_chain_vs_independent_models()
sklearn.tests.test_multioutput.test_fit_params_no_routing(Cls,method)
sklearn.tests.test_multioutput.test_hasattr_multi_output_predict_proba()
sklearn.tests.test_multioutput.test_multi_output_classes_(estimator)
sklearn.tests.test_multioutput.test_multi_output_classification()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit_no_first_classes_exception()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit_parallelism()
sklearn.tests.test_multioutput.test_multi_output_classification_partial_fit_sample_weights()
sklearn.tests.test_multioutput.test_multi_output_classification_sample_weights()
sklearn.tests.test_multioutput.test_multi_output_delegate_predict_proba()
sklearn.tests.test_multioutput.test_multi_output_exceptions()
sklearn.tests.test_multioutput.test_multi_output_not_fitted_error(response_method)
sklearn.tests.test_multioutput.test_multi_output_predict_proba()
sklearn.tests.test_multioutput.test_multi_target_regression()
sklearn.tests.test_multioutput.test_multi_target_regression_one_target()
sklearn.tests.test_multioutput.test_multi_target_regression_partial_fit()
sklearn.tests.test_multioutput.test_multi_target_sample_weight_partial_fit()
sklearn.tests.test_multioutput.test_multi_target_sample_weights()
sklearn.tests.test_multioutput.test_multi_target_sample_weights_api()
sklearn.tests.test_multioutput.test_multi_target_sparse_regression(sparse_container)
sklearn.tests.test_multioutput.test_multiclass_multioutput_estimator()
sklearn.tests.test_multioutput.test_multiclass_multioutput_estimator_predict_proba()
sklearn.tests.test_multioutput.test_multioutput_estimator_with_fit_params(estimator,dataset)
sklearn.tests.test_multioutput.test_multioutput_regressor_has_partial_fit()
sklearn.tests.test_multioutput.test_multioutputregressor_ducktypes_fitted_estimator()
sklearn.tests.test_multioutput.test_regressor_chain_verbose(capsys)
sklearn.tests.test_multioutput.test_regressor_chain_w_fit_params()
sklearn.tests.test_multioutput.test_support_missing_values(MultiOutputEstimator,Estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_dummy.py----------------------------------------
A:sklearn.tests.test_dummy.proba->DummyClassifier(strategy=strategy, random_state=0).predict_proba(X)
A:sklearn.tests.test_dummy.log_proba->DummyClassifier(strategy=strategy, random_state=0).predict_log_proba(X)
A:sklearn.tests.test_dummy.y->numpy.array([2, 2, 2])
A:sklearn.tests.test_dummy.n_samples->len(X)
A:sklearn.tests.test_dummy.X->numpy.zeros(4)
A:sklearn.tests.test_dummy.est->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight)
A:sklearn.tests.test_dummy.y_pred->DummyRegressor().predict(X)
A:sklearn.tests.test_dummy.clf->DummyClassifier(strategy=strategy, random_state=0)
A:sklearn.tests.test_dummy.clf_1d->DummyClassifier(strategy=strategy, random_state=0)
A:sklearn.tests.test_dummy.clf_2d->DummyClassifier(strategy=strategy, random_state=0)
A:sklearn.tests.test_dummy.clf1->DummyClassifier(strategy=strategy, random_state=global_random_seed, constant=0)
A:sklearn.tests.test_dummy.predictions1->DummyRegressor(strategy=strategy, constant=0, quantile=0.7).predict(X1)
A:sklearn.tests.test_dummy.clf2->DummyClassifier(strategy=strategy, random_state=global_random_seed, constant=0)
A:sklearn.tests.test_dummy.predictions2->DummyRegressor(strategy=strategy, constant=0, quantile=0.7).predict(X2)
A:sklearn.tests.test_dummy.random_state->numpy.random.RandomState(seed=global_random_seed)
A:sklearn.tests.test_dummy.reg->DummyRegressor()
A:sklearn.tests.test_dummy.X_learn->numpy.random.RandomState(seed=global_random_seed).randn(10, 10)
A:sklearn.tests.test_dummy.y_learn->numpy.random.RandomState(seed=global_random_seed).randn(10, 5)
A:sklearn.tests.test_dummy.mean->numpy.mean(y_learn, axis=0).reshape((1, -1))
A:sklearn.tests.test_dummy.X_test->numpy.random.RandomState(seed=global_random_seed).randn(20, 10)
A:sklearn.tests.test_dummy.y_test->numpy.random.RandomState(seed=global_random_seed).randn(20, 5)
A:sklearn.tests.test_dummy.y_pred_learn->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight).predict(X_learn)
A:sklearn.tests.test_dummy.y_pred_test->DummyRegressor(strategy='quantile', quantile=0.95).fit(X, y, sample_weight).predict(X_test)
A:sklearn.tests.test_dummy.median->numpy.median(y_learn, axis=0).reshape((1, -1))
A:sklearn.tests.test_dummy.quantile_values->numpy.percentile(y_learn, axis=0, q=80).reshape((1, -1))
A:sklearn.tests.test_dummy.constants->numpy.random.RandomState(seed=global_random_seed).randn(5)
A:sklearn.tests.test_dummy.y_expected->numpy.array([2, 2, 2])
A:sklearn.tests.test_dummy.sample_weight->numpy.random.RandomState(seed=global_random_seed).rand(n_samples)
A:sklearn.tests.test_dummy.cls->DummyRegressor()
A:sklearn.tests.test_dummy.y_pred_proba->DummyRegressor().predict_proba(X)
A:sklearn.tests.test_dummy.y_std_expected->numpy.array([0, 0, 0])
A:sklearn.tests.test_dummy.y_pred_list->DummyRegressor().predict(X, return_std=True)
A:sklearn.tests.test_dummy.reg1->DummyRegressor(strategy=strategy, constant=0, quantile=0.7)
A:sklearn.tests.test_dummy.reg2->DummyRegressor(strategy=strategy, constant=0, quantile=0.7)
A:sklearn.tests.test_dummy.model->DummyClassifier(strategy=strategy, random_state=0, constant=0)
A:sklearn.tests.test_dummy.probas->DummyClassifier(strategy=strategy, random_state=0, constant=0).fit(X, y).predict_proba(X)
sklearn.tests.test_dummy._check_behavior_2d(clf)
sklearn.tests.test_dummy._check_behavior_2d_for_constant(clf)
sklearn.tests.test_dummy._check_equality_regressor(statistic,y_learn,y_pred_learn,y_test,y_pred_test)
sklearn.tests.test_dummy._check_predict_proba(clf,X,y)
sklearn.tests.test_dummy.test_classification_sample_weight()
sklearn.tests.test_dummy.test_classifier_prediction_independent_of_X(strategy,global_random_seed)
sklearn.tests.test_dummy.test_classifier_score_with_None(y,y_test)
sklearn.tests.test_dummy.test_constant_size_multioutput_regressor(global_random_seed)
sklearn.tests.test_dummy.test_constant_strategy()
sklearn.tests.test_dummy.test_constant_strategy_exceptions(y,params,err_msg)
sklearn.tests.test_dummy.test_constant_strategy_multioutput()
sklearn.tests.test_dummy.test_constant_strategy_multioutput_regressor(global_random_seed)
sklearn.tests.test_dummy.test_constant_strategy_regressor(global_random_seed)
sklearn.tests.test_dummy.test_constant_strategy_sparse_target(csc_container)
sklearn.tests.test_dummy.test_constants_not_specified_regressor()
sklearn.tests.test_dummy.test_dtype_of_classifier_probas(strategy)
sklearn.tests.test_dummy.test_dummy_classifier_on_3D_array()
sklearn.tests.test_dummy.test_dummy_regressor_on_3D_array()
sklearn.tests.test_dummy.test_dummy_regressor_return_std()
sklearn.tests.test_dummy.test_dummy_regressor_sample_weight(global_random_seed,n_samples=10)
sklearn.tests.test_dummy.test_mean_strategy_multioutput_regressor(global_random_seed)
sklearn.tests.test_dummy.test_mean_strategy_regressor(global_random_seed)
sklearn.tests.test_dummy.test_median_strategy_multioutput_regressor(global_random_seed)
sklearn.tests.test_dummy.test_median_strategy_regressor(global_random_seed)
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy()
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy_multioutput()
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy_sparse_target(csc_container)
sklearn.tests.test_dummy.test_most_frequent_and_prior_strategy_with_2d_column_y()
sklearn.tests.test_dummy.test_quantile_invalid()
sklearn.tests.test_dummy.test_quantile_strategy_empty_train()
sklearn.tests.test_dummy.test_quantile_strategy_multioutput_regressor(global_random_seed)
sklearn.tests.test_dummy.test_quantile_strategy_regressor(global_random_seed)
sklearn.tests.test_dummy.test_regressor_exceptions()
sklearn.tests.test_dummy.test_regressor_prediction_independent_of_X(strategy)
sklearn.tests.test_dummy.test_regressor_score_with_None(y,y_test)
sklearn.tests.test_dummy.test_stratified_strategy(global_random_seed)
sklearn.tests.test_dummy.test_stratified_strategy_multioutput(global_random_seed)
sklearn.tests.test_dummy.test_stratified_strategy_sparse_target(global_random_seed,csc_container)
sklearn.tests.test_dummy.test_string_labels()
sklearn.tests.test_dummy.test_uniform_strategy(global_random_seed)
sklearn.tests.test_dummy.test_uniform_strategy_multioutput(global_random_seed)
sklearn.tests.test_dummy.test_uniform_strategy_sparse_target_warning(global_random_seed,csc_container)
sklearn.tests.test_dummy.test_y_mean_attribute_regressor()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_metadata_routing.py----------------------------------------
A:sklearn.tests.test_metadata_routing.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_metadata_routing.X->numpy.random.RandomState(42).rand(N, M)
A:sklearn.tests.test_metadata_routing.y->numpy.random.RandomState(42).randint(0, 2, size=N)
A:sklearn.tests.test_metadata_routing.my_groups->numpy.random.RandomState(42).randint(0, 10, size=N)
A:sklearn.tests.test_metadata_routing.my_weights->numpy.random.RandomState(42).rand(N)
A:sklearn.tests.test_metadata_routing.my_other_weights->numpy.random.RandomState(42).rand(N)
A:sklearn.tests.test_metadata_routing.params->process_routing(self, 'predict', **predict_params)
A:sklearn.tests.test_metadata_routing.transformer->clone(step).fit(X_transformed, y, **params.get(f'step_{i}').fit)
A:sklearn.tests.test_metadata_routing.X_transformed->step.transform(X, **params.get(f'step_{i}').transform)
A:sklearn.tests.test_metadata_routing.router->MetadataRouter(owner='test').add_self_request(WeightedMetaRegressor(estimator=ConsumingRegressor()).set_fit_request(sample_weight='self_weights')).add(method_mapping='fit', trs=ConsumingTransformer().set_fit_request(sample_weight='transform_weights'))
A:sklearn.tests.test_metadata_routing.requests->MetadataRequest(owner='test')
A:sklearn.tests.test_metadata_routing.odd_request->get_routing_for_object(OddEstimator())
A:sklearn.tests.test_metadata_routing.trs_request->get_routing_for_object(ConsumingTransformer())
A:sklearn.tests.test_metadata_routing.est_request->get_routing_for_object(ConsumingClassifier())
A:sklearn.tests.test_metadata_routing.clf->WeightedMetaClassifier(estimator=ConsumingClassifier().set_fit_request(sample_weight='alternative_weight'))
A:sklearn.tests.test_metadata_routing.pipeline->SimplePipeline([MetaTransformer(transformer=ConsumingTransformer().set_fit_request(metadata=True, sample_weight=False).set_transform_request(sample_weight=True)), WeightedMetaRegressor(estimator=ConsumingRegressor().set_fit_request(sample_weight=True)).set_fit_request(sample_weight='outer_weights')])
A:sklearn.tests.test_metadata_routing.trs->MetaTransformer(transformer=ConsumingTransformer().set_transform_request(sample_weight=False))
A:sklearn.tests.test_metadata_routing.est->SimpleEstimator()
A:sklearn.tests.test_metadata_routing.test_cases->dict()
A:sklearn.tests.test_metadata_routing.mmr->MethodMetadataRequest(owner='test', method='fit')
A:sklearn.tests.test_metadata_routing.mr->get_routing_for_object(Consumer())
A:sklearn.tests.test_metadata_routing.mr_factory->get_routing_for_object(mr)
A:sklearn.tests.test_metadata_routing.request->MetadataRequest(owner='nested')
A:sklearn.tests.test_metadata_routing.mm->sklearn.utils.metadata_routing.MethodMapping.from_str('score')
A:sklearn.tests.test_metadata_routing.mm_list->list(mm)
sklearn.tests.test_metadata_routing.SimplePipeline(self,steps)
sklearn.tests.test_metadata_routing.SimplePipeline.__init__(self,steps)
sklearn.tests.test_metadata_routing.SimplePipeline.fit(self,X,y,**fit_params)
sklearn.tests.test_metadata_routing.SimplePipeline.get_metadata_routing(self)
sklearn.tests.test_metadata_routing.SimplePipeline.predict(self,X,**predict_params)
sklearn.tests.test_metadata_routing.enable_slep006()
sklearn.tests.test_metadata_routing.test_assert_request_is_empty()
sklearn.tests.test_metadata_routing.test_composite_methods()
sklearn.tests.test_metadata_routing.test_default_requests()
sklearn.tests.test_metadata_routing.test_estimator_puts_self_in_registry(estimator)
sklearn.tests.test_metadata_routing.test_estimator_warnings()
sklearn.tests.test_metadata_routing.test_get_metadata_routing()
sklearn.tests.test_metadata_routing.test_get_routing_for_object()
sklearn.tests.test_metadata_routing.test_invalid_metadata()
sklearn.tests.test_metadata_routing.test_metadata_request_consumes_method()
sklearn.tests.test_metadata_routing.test_metadata_router_consumes_method()
sklearn.tests.test_metadata_routing.test_metadata_routing_add()
sklearn.tests.test_metadata_routing.test_metadata_routing_get_param_names()
sklearn.tests.test_metadata_routing.test_metadatarouter_add_self_request()
sklearn.tests.test_metadata_routing.test_metaestimator_warnings()
sklearn.tests.test_metadata_routing.test_method_generation()
sklearn.tests.test_metadata_routing.test_method_metadata_request()
sklearn.tests.test_metadata_routing.test_methodmapping()
sklearn.tests.test_metadata_routing.test_nested_routing()
sklearn.tests.test_metadata_routing.test_nested_routing_conflict()
sklearn.tests.test_metadata_routing.test_no_feature_flag_raises_error()
sklearn.tests.test_metadata_routing.test_none_metadata_passed()
sklearn.tests.test_metadata_routing.test_process_routing_invalid_method()
sklearn.tests.test_metadata_routing.test_process_routing_invalid_object()
sklearn.tests.test_metadata_routing.test_removing_non_existing_param_raises()
sklearn.tests.test_metadata_routing.test_request_type_is_alias(val,res)
sklearn.tests.test_metadata_routing.test_request_type_is_valid(val,res)
sklearn.tests.test_metadata_routing.test_setting_default_requests()
sklearn.tests.test_metadata_routing.test_simple_metadata_routing()
sklearn.tests.test_metadata_routing.test_string_representations(obj,string)
sklearn.tests.test_metadata_routing.test_validations(obj,method,inputs,err_cls,err_msg)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_metaestimators_metadata_routing.py----------------------------------------
A:sklearn.tests.test_metaestimators_metadata_routing.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_metaestimators_metadata_routing.X->numpy.random.RandomState(42).rand(N, M)
A:sklearn.tests.test_metaestimators_metadata_routing.y->numpy.random.RandomState(42).randint(0, 3, size=N)
A:sklearn.tests.test_metaestimators_metadata_routing.classes->numpy.unique(y)
A:sklearn.tests.test_metaestimators_metadata_routing.y_multi->numpy.random.RandomState(42).randint(0, 3, size=(N, 3))
A:sklearn.tests.test_metaestimators_metadata_routing.metadata->numpy.random.RandomState(42).randint(0, 10, size=N)
A:sklearn.tests.test_metaestimators_metadata_routing.sample_weight->numpy.random.RandomState(42).rand(N)
A:sklearn.tests.test_metaestimators_metadata_routing.groups->numpy.array([0, 1] * (len(y) // 2))
A:sklearn.tests.test_metaestimators_metadata_routing.kwargs->metaestimator_info.get('init_args', {})
A:sklearn.tests.test_metaestimators_metadata_routing.estimator_registry->_Registry()
A:sklearn.tests.test_metaestimators_metadata_routing.estimator->metaestimator_info['estimator'](estimator_registry)
A:sklearn.tests.test_metaestimators_metadata_routing.scorer_registry->_Registry()
A:sklearn.tests.test_metaestimators_metadata_routing.scorer->ConsumingScorer(registry=scorer_registry)
A:sklearn.tests.test_metaestimators_metadata_routing.cv_registry->_Registry()
A:sklearn.tests.test_metaestimators_metadata_routing.cv->ConsumingSplitter(registry=cv_registry)
A:sklearn.tests.test_metaestimators_metadata_routing.a->_Registry()
A:sklearn.tests.test_metaestimators_metadata_routing.b->_Registry()
A:sklearn.tests.test_metaestimators_metadata_routing.(kwargs, *_)->get_init_args(metaestimator)
A:sklearn.tests.test_metaestimators_metadata_routing.instance->cls(**kwargs)
A:sklearn.tests.test_metaestimators_metadata_routing.(kwargs, (estimator, _), (scorer, _), *_)->get_init_args(metaestimator)
A:sklearn.tests.test_metaestimators_metadata_routing.method->getattr(instance, method_name)
A:sklearn.tests.test_metaestimators_metadata_routing.set_request_for_method->getattr(estimator, f'set_{method_name}_request')
A:sklearn.tests.test_metaestimators_metadata_routing.preserves_metadata->metaestimator.get('preserves_metadata', True)
A:sklearn.tests.test_metaestimators_metadata_routing.(kwargs, (estimator, registry), (scorer, _), (cv, _))->get_init_args(metaestimator)
A:sklearn.tests.test_metaestimators_metadata_routing.extra_method_args->metaestimator.get('method_args', {}).get(method_name, {})
A:sklearn.tests.test_metaestimators_metadata_routing.(kwargs, (estimator, _), (scorer, registry), (cv, _))->get_init_args(metaestimator)
A:sklearn.tests.test_metaestimators_metadata_routing.(kwargs, (estimator, _), (scorer, _), (cv, registry))->get_init_args(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.enable_slep006()
sklearn.tests.test_metaestimators_metadata_routing.get_init_args(metaestimator_info)
sklearn.tests.test_metaestimators_metadata_routing.test_default_request(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.test_error_on_missing_requests_for_sub_estimator(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.test_metadata_is_routed_correctly_to_scorer(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.test_metadata_is_routed_correctly_to_splitter(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.test_registry_copy()
sklearn.tests.test_metaestimators_metadata_routing.test_setting_request_on_sub_estimator_removes_error(metaestimator)
sklearn.tests.test_metaestimators_metadata_routing.test_unsupported_estimators_fit_with_metadata(estimator)
sklearn.tests.test_metaestimators_metadata_routing.test_unsupported_estimators_get_metadata_routing(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_isotonic.py----------------------------------------
A:sklearn.tests.test_isotonic.ir->IsotonicRegression()
A:sklearn.tests.test_isotonic.(x_s, y_s, sample_weight_s)->shuffle(x, y, sample_weight, random_state=0)
A:sklearn.tests.test_isotonic.y_transformed->IsotonicRegression().fit_transform(x, y, sample_weight=sample_weight)
A:sklearn.tests.test_isotonic.y_transformed_s->IsotonicRegression().fit(x_s, y_s, sample_weight=sample_weight_s).transform(x)
A:sklearn.tests.test_isotonic.is_increasing->check_increasing(x, y)
A:sklearn.tests.test_isotonic.y->numpy.arange(10)
A:sklearn.tests.test_isotonic.y_->IsotonicRegression().fit_transform(x, y)
A:sklearn.tests.test_isotonic.x->numpy.array([0, 1e-16, 1, 1 + 1e-14], dtype=dtype)
A:sklearn.tests.test_isotonic.perm->numpy.random.permutation(len(y))
A:sklearn.tests.test_isotonic.y_true->numpy.array([0.0, 0.25, 0.25, 0.25, 0.25, 1.0])
A:sklearn.tests.test_isotonic.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_isotonic.weights->numpy.array([0.9, 0.9, 0.9, 0.9, 0.9], dtype=np.float64)
A:sklearn.tests.test_isotonic.y_set_value->IsotonicRegression().fit_transform(x, y, sample_weight=weights)
A:sklearn.tests.test_isotonic.y_default_value->IsotonicRegression().fit_transform(x, y)
A:sklearn.tests.test_isotonic.y_result->numpy.round(ir.fit_transform(x, y))
A:sklearn.tests.test_isotonic.received_y->IsotonicRegression().fit_transform(x, y, sample_weight=sample_weight)
A:sklearn.tests.test_isotonic.y1->IsotonicRegression().predict([min(x) - 10, max(x) + 10])
A:sklearn.tests.test_isotonic.y2->IsotonicRegression().predict(x)
A:sklearn.tests.test_isotonic.ir_ser->pickle.dumps(ir, pickle.HIGHEST_PROTOCOL)
A:sklearn.tests.test_isotonic.ir2->pickle.loads(ir_ser)
A:sklearn.tests.test_isotonic.all_predictions_finite->numpy.all(np.isfinite(ir.predict(x)))
A:sklearn.tests.test_isotonic.regression->IsotonicRegression()
A:sklearn.tests.test_isotonic.w->numpy.ones_like(x)
A:sklearn.tests.test_isotonic.y_train->numpy.less(rng.rand(n_samples), expit(X_train)).astype('int64').astype('float64')
A:sklearn.tests.test_isotonic.slow_model->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')
A:sklearn.tests.test_isotonic.fast_model->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')
A:sklearn.tests.test_isotonic.(X_train_fit, y_train_fit)->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')._build_y(X_train, y_train, sample_weight=weights, trim_duplicates=False)
A:sklearn.tests.test_isotonic.y_pred_slow->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip').predict(X_test)
A:sklearn.tests.test_isotonic.y_pred_fast->IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip').predict(X_test)
A:sklearn.tests.test_isotonic.reg->IsotonicRegression()
A:sklearn.tests.test_isotonic.y_np->numpy.array(y, dtype=dtype)
A:sklearn.tests.test_isotonic.res->IsotonicRegression().predict(X)
A:sklearn.tests.test_isotonic.X->X.reshape(-1, 1).reshape(-1, 1)
A:sklearn.tests.test_isotonic.(x, y, w)->_make_unique(x, y, w)
A:sklearn.tests.test_isotonic.x_out->numpy.array([0, 1])
A:sklearn.tests.test_isotonic.ireg->IsotonicRegression(increasing=increasing).fit(X, y)
A:sklearn.tests.test_isotonic.y_pred->IsotonicRegression().predict(X)
A:sklearn.tests.test_isotonic.X_2d->X.reshape(-1, 1).reshape(-1, 1).reshape(-1, 1)
A:sklearn.tests.test_isotonic.iso_reg->IsotonicRegression().fit(X, y)
A:sklearn.tests.test_isotonic.iso_reg_2d->IsotonicRegression().fit(X_2d, y)
A:sklearn.tests.test_isotonic.y_pred1->IsotonicRegression().fit(X, y).predict(X)
A:sklearn.tests.test_isotonic.y_pred2->IsotonicRegression().fit(X_2d, y).predict(X_2d)
A:sklearn.tests.test_isotonic.(X, y)->make_regression(n_samples=10, n_features=1, random_state=42)
A:sklearn.tests.test_isotonic.sample_weight_original->numpy.ones_like(y)
A:sklearn.tests.test_isotonic.sample_weight_fit->numpy.ones_like(y).copy()
A:sklearn.tests.test_isotonic.iso->IsotonicRegression().fit(X, y)
A:sklearn.tests.test_isotonic.names->IsotonicRegression().fit(X, y).get_feature_names_out()
A:sklearn.tests.test_isotonic.pd->pytest.importorskip('pandas')
A:sklearn.tests.test_isotonic.regressor->IsotonicRegression()
A:sklearn.tests.test_isotonic.X_trans->IsotonicRegression().transform(X)
sklearn.tests.test_isotonic.test_assert_raises_exceptions()
sklearn.tests.test_isotonic.test_check_ci_warn()
sklearn.tests.test_isotonic.test_check_increasing_down()
sklearn.tests.test_isotonic.test_check_increasing_down_extreme()
sklearn.tests.test_isotonic.test_check_increasing_small_number_of_samples()
sklearn.tests.test_isotonic.test_check_increasing_up()
sklearn.tests.test_isotonic.test_check_increasing_up_extreme()
sklearn.tests.test_isotonic.test_fast_predict()
sklearn.tests.test_isotonic.test_get_feature_names_out(shape)
sklearn.tests.test_isotonic.test_input_shape_validation()
sklearn.tests.test_isotonic.test_isotonic_2darray_more_than_1_feature()
sklearn.tests.test_isotonic.test_isotonic_copy_before_fit()
sklearn.tests.test_isotonic.test_isotonic_dtype()
sklearn.tests.test_isotonic.test_isotonic_duplicate_min_entry()
sklearn.tests.test_isotonic.test_isotonic_make_unique_tolerance()
sklearn.tests.test_isotonic.test_isotonic_min_max_boundaries()
sklearn.tests.test_isotonic.test_isotonic_mismatched_dtype(y_dtype)
sklearn.tests.test_isotonic.test_isotonic_non_regression_inf_slope()
sklearn.tests.test_isotonic.test_isotonic_regression()
sklearn.tests.test_isotonic.test_isotonic_regression_auto_decreasing()
sklearn.tests.test_isotonic.test_isotonic_regression_auto_increasing()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_clip()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_nan()
sklearn.tests.test_isotonic.test_isotonic_regression_oob_raise()
sklearn.tests.test_isotonic.test_isotonic_regression_output_predict()
sklearn.tests.test_isotonic.test_isotonic_regression_pickle()
sklearn.tests.test_isotonic.test_isotonic_regression_reversed()
sklearn.tests.test_isotonic.test_isotonic_regression_sample_weight_not_overwritten()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_max()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_min()
sklearn.tests.test_isotonic.test_isotonic_regression_ties_secondary_()
sklearn.tests.test_isotonic.test_isotonic_regression_with_ties_in_differently_sized_groups()
sklearn.tests.test_isotonic.test_isotonic_sample_weight()
sklearn.tests.test_isotonic.test_isotonic_sample_weight_parameter_default_value()
sklearn.tests.test_isotonic.test_isotonic_thresholds(increasing)
sklearn.tests.test_isotonic.test_isotonic_ymin_ymax()
sklearn.tests.test_isotonic.test_isotonic_zero_weight_loop()
sklearn.tests.test_isotonic.test_make_unique_dtype()
sklearn.tests.test_isotonic.test_make_unique_tolerance(dtype)
sklearn.tests.test_isotonic.test_permutation_invariance()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_check_build.py----------------------------------------
sklearn.tests.test_check_build.test_raise_build_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_config.py----------------------------------------
A:sklearn.tests.test_config.items->Parallel(backend=backend, n_jobs=2)((delayed(set_assume_finite)(assume_finite, sleep_dur) for (assume_finite, sleep_dur) in zip(assume_finites, sleep_durations)))
sklearn.tests.test_config.set_assume_finite(assume_finite,sleep_duration)
sklearn.tests.test_config.test_config_array_api_dispatch_error(monkeypatch)
sklearn.tests.test_config.test_config_array_api_dispatch_error_numpy(monkeypatch)
sklearn.tests.test_config.test_config_context()
sklearn.tests.test_config.test_config_context_exception()
sklearn.tests.test_config.test_config_threadsafe()
sklearn.tests.test_config.test_config_threadsafe_joblib(backend)
sklearn.tests.test_config.test_set_config()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_multiclass.py----------------------------------------
A:sklearn.tests.test_multiclass.pytestmark->pytest.mark.filterwarnings(f'ignore:{msg}:FutureWarning')
A:sklearn.tests.test_multiclass.iris->sklearn.datasets.load_iris()
A:sklearn.tests.test_multiclass.rng->numpy.random.RandomState(42)
A:sklearn.tests.test_multiclass.perm->numpy.random.RandomState(42).permutation(iris.target.size)
A:sklearn.tests.test_multiclass.ovr->OneVsRestClassifier(LogisticRegression())
A:sklearn.tests.test_multiclass.X->numpy.ones((10, 2))
A:sklearn.tests.test_multiclass.y->make_y((10, 1), dtype=np.int32)
A:sklearn.tests.test_multiclass.msg->type_of_target(y)
A:sklearn.tests.test_multiclass.pred->OneVsOneClassifier(LinearSVC(dual='auto', random_state=0)).estimators_[k].predict(iris.data)
A:sklearn.tests.test_multiclass.clf->KNeighborsClassifier(n_neighbors=8, weights='distance')
A:sklearn.tests.test_multiclass.pred2->OneVsOneClassifier(MultinomialNB()).fit(X, y).predict(X)
A:sklearn.tests.test_multiclass.(X, y)->load_breast_cancer(return_X_y=True)
A:sklearn.tests.test_multiclass.ovr2->OneVsRestClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.ovr1->OneVsRestClassifier(SGDClassifier(max_iter=1, tol=None, shuffle=False, random_state=0))
A:sklearn.tests.test_multiclass.pred1->OneVsOneClassifier(MultinomialNB()).predict(X)
A:sklearn.tests.test_multiclass.base_clf->MultinomialNB(alpha=1)
A:sklearn.tests.test_multiclass.(X, Y)->sklearn.datasets.make_classification(n_samples=100, n_features=20, random_state=0)
A:sklearn.tests.test_multiclass.Y_pred->KNeighborsClassifier(n_neighbors=8, weights='distance').predict(X_test)
A:sklearn.tests.test_multiclass.clf_sprs->OneVsRestClassifier(clf).fit(X_train, sparse_container(Y_train))
A:sklearn.tests.test_multiclass.Y_pred_sprs->OneVsRestClassifier(clf).fit(X_train, sparse_container(Y_train)).predict(X_test)
A:sklearn.tests.test_multiclass.Y_proba->KNeighborsClassifier(n_neighbors=8, weights='distance').predict_proba(X_test)
A:sklearn.tests.test_multiclass.dec_pred->(clf_sprs.decision_function(X_test) > 0).astype(int)
A:sklearn.tests.test_multiclass.y_pred->OneVsRestClassifier(LogisticRegression()).predict_proba(X)
A:sklearn.tests.test_multiclass.Y->numpy.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1], [1, 0, 0]])
A:sklearn.tests.test_multiclass.classes->set('eggs spam'.split())
A:sklearn.tests.test_multiclass.dec->KNeighborsClassifier(n_neighbors=8, weights='distance').decision_function(X)
A:sklearn.tests.test_multiclass.X_test->numpy.array([[0, 0, 4]])
A:sklearn.tests.test_multiclass.probabilities->KNeighborsClassifier(n_neighbors=8, weights='distance').predict_proba(X_test)
A:sklearn.tests.test_multiclass.decision_only->OneVsRestClassifier(svm.SVR()).fit(X_train, Y_train)
A:sklearn.tests.test_multiclass.gs->GridSearchCV(svm.SVC(probability=False), param_grid={'probability': [True]})
A:sklearn.tests.test_multiclass.proba_after_fit->OneVsRestClassifier(gs)
A:sklearn.tests.test_multiclass.cv->GridSearchCV(ecoc, {'estimator__C': Cs})
A:sklearn.tests.test_multiclass.ovr_pipe->OneVsRestClassifier(clf)
A:sklearn.tests.test_multiclass.ovo->OneVsOneClassifier(clf)
A:sklearn.tests.test_multiclass.prediction_from_array->OneVsOneClassifier(clf).fit(iris.data, iris.target).predict(iris.data)
A:sklearn.tests.test_multiclass.prediction_from_list->OneVsOneClassifier(clf).fit(iris_data_list, list(iris.target)).predict(iris_data_list)
A:sklearn.tests.test_multiclass.temp->sklearn.datasets.load_iris()
A:sklearn.tests.test_multiclass.ovo1->OneVsOneClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.ovo2->OneVsOneClassifier(MultinomialNB())
A:sklearn.tests.test_multiclass.message_re->escape('Mini-batch contains {0} while it must be subset of {1}'.format(np.unique(error_y), np.unique(y)))
A:sklearn.tests.test_multiclass.ovo_clf->OneVsOneClassifier(LinearSVC(dual='auto', random_state=0))
A:sklearn.tests.test_multiclass.decisions->OneVsOneClassifier(LinearSVC(dual='auto', random_state=0)).decision_function(iris.data)
A:sklearn.tests.test_multiclass.votes->numpy.round(ovo_decision)
A:sklearn.tests.test_multiclass.multi_clf->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None))
A:sklearn.tests.test_multiclass.ovo_prediction->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None)).fit(X, y).predict(X)
A:sklearn.tests.test_multiclass.ovo_decision->OneVsOneClassifier(Perceptron(shuffle=False, max_iter=4, tol=None)).decision_function(X)
A:sklearn.tests.test_multiclass.y_ref->numpy.array([2, 0, 1, 2])
A:sklearn.tests.test_multiclass.ecoc->OutputCodeClassifier(LinearSVC(dual='auto', random_state=0))
A:sklearn.tests.test_multiclass.X_sp->csc_container(X)
A:sklearn.tests.test_multiclass.base_estimator->CheckingClassifier(check_X=check_array, check_X_params={'ensure_2d': True, 'accept_sparse': False})
A:sklearn.tests.test_multiclass.clf_precomputed->sklearn.svm.SVC(kernel='precomputed')
A:sklearn.tests.test_multiclass.ovr_false->MultiClassClassifier(clf_notprecomputed)
A:sklearn.tests.test_multiclass.linear_kernel->numpy.dot(X, X.T)
A:sklearn.tests.test_multiclass.n_estimators->len(ovr_false.estimators_)
A:sklearn.tests.test_multiclass.clf_notprecomputed->sklearn.svm.SVC(kernel='linear')
A:sklearn.tests.test_multiclass.ovr_notprecomputed->OneVsRestClassifier(clf_notprecomputed).fit(X, y)
A:sklearn.tests.test_multiclass.ovo_notprecomputed->OneVsOneClassifier(clf_notprecomputed).fit(X, y)
A:sklearn.tests.test_multiclass.ovr_precomputed->OneVsRestClassifier(clf_precomputed).fit(K, y)
A:sklearn.tests.test_multiclass.ovo_precomputed->OneVsOneClassifier(clf_precomputed).fit(K, y)
A:sklearn.tests.test_multiclass.ovr_true->MultiClassClassifier(clf_precomputed)
A:sklearn.tests.test_multiclass.multiclass_clf_notprecomputed->MultiClassClassifier(clf_notprecomputed)
A:sklearn.tests.test_multiclass.multiclass_clf_precomputed->MultiClassClassifier(clf_precomputed)
A:sklearn.tests.test_multiclass.score_not_precomputed->cross_val_score(multiclass_clf_notprecomputed, X, y, error_score='raise')
A:sklearn.tests.test_multiclass.score_precomputed->cross_val_score(multiclass_clf_precomputed, linear_kernel, y, error_score='raise')
A:sklearn.tests.test_multiclass.mask->numpy.random.RandomState(42).choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool)
A:sklearn.tests.test_multiclass.lr->make_pipeline(SimpleImputer(), LogisticRegression(random_state=rng))
A:sklearn.tests.test_multiclass.expected->numpy.zeros((X.shape[0], 2))
sklearn.tests.test_multiclass.test_check_classification_targets()
sklearn.tests.test_multiclass.test_constant_int_target(make_y)
sklearn.tests.test_multiclass.test_ecoc_delegate_sparse_base_estimator(csc_container)
sklearn.tests.test_multiclass.test_ecoc_exceptions()
sklearn.tests.test_multiclass.test_ecoc_fit_predict()
sklearn.tests.test_multiclass.test_ecoc_float_y()
sklearn.tests.test_multiclass.test_ecoc_gridsearch()
sklearn.tests.test_multiclass.test_ovo_consistent_binary_classification()
sklearn.tests.test_multiclass.test_ovo_decision_function()
sklearn.tests.test_multiclass.test_ovo_exceptions()
sklearn.tests.test_multiclass.test_ovo_fit_on_list()
sklearn.tests.test_multiclass.test_ovo_fit_predict()
sklearn.tests.test_multiclass.test_ovo_float_y()
sklearn.tests.test_multiclass.test_ovo_gridsearch()
sklearn.tests.test_multiclass.test_ovo_one_class()
sklearn.tests.test_multiclass.test_ovo_partial_fit_predict()
sklearn.tests.test_multiclass.test_ovo_string_y()
sklearn.tests.test_multiclass.test_ovo_ties()
sklearn.tests.test_multiclass.test_ovo_ties2()
sklearn.tests.test_multiclass.test_ovr_always_present()
sklearn.tests.test_multiclass.test_ovr_binary()
sklearn.tests.test_multiclass.test_ovr_exceptions()
sklearn.tests.test_multiclass.test_ovr_fit_predict()
sklearn.tests.test_multiclass.test_ovr_fit_predict_sparse(sparse_container)
sklearn.tests.test_multiclass.test_ovr_fit_predict_svc()
sklearn.tests.test_multiclass.test_ovr_gridsearch()
sklearn.tests.test_multiclass.test_ovr_multiclass()
sklearn.tests.test_multiclass.test_ovr_multilabel()
sklearn.tests.test_multiclass.test_ovr_multilabel_dataset()
sklearn.tests.test_multiclass.test_ovr_multilabel_decision_function()
sklearn.tests.test_multiclass.test_ovr_multilabel_predict_proba()
sklearn.tests.test_multiclass.test_ovr_ovo_regressor()
sklearn.tests.test_multiclass.test_ovr_partial_fit()
sklearn.tests.test_multiclass.test_ovr_partial_fit_exceptions()
sklearn.tests.test_multiclass.test_ovr_pipeline()
sklearn.tests.test_multiclass.test_ovr_single_label_decision_function()
sklearn.tests.test_multiclass.test_ovr_single_label_predict_proba()
sklearn.tests.test_multiclass.test_pairwise_cross_val_score(MultiClassClassifier)
sklearn.tests.test_multiclass.test_pairwise_indices()
sklearn.tests.test_multiclass.test_pairwise_n_features_in()
sklearn.tests.test_multiclass.test_pairwise_tag(MultiClassClassifier)
sklearn.tests.test_multiclass.test_support_missing_values(MultiClassClassifier)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_common.py----------------------------------------
A:sklearn.tests.test_common.msg->'Base estimators such as {0} should not be included in all_estimators'.format(name)
A:sklearn.tests.test_common.estimator->_construct_instance(Estimator)
A:sklearn.tests.test_common.all_instance_gen_checks->check_estimator(LogisticRegression(), generate_only=True)
A:sklearn.tests.test_common.cwd->os.getcwd()
A:sklearn.tests.test_common.setup_path->os.path.abspath(os.path.join(sklearn.__path__[0], '..'))
A:sklearn.tests.test_common.setup_filename->os.path.join(setup_path, 'setup.py')
A:sklearn.tests.test_common.classifiers->all_estimators(type_filter='classifier')
A:sklearn.tests.test_common.required_parameters->getattr(clazz, '_required_parameters', [])
A:sklearn.tests.test_common.pkgs->pkgutil.walk_packages(path=sklearn.__path__, prefix='sklearn.', onerror=lambda _: None)
A:sklearn.tests.test_common.package->__import__(modname, fromlist='dummy')
A:sklearn.tests.test_common.HAS_TESTS_EXCEPTIONS->re.compile('(?x)\n                                      \\.externals(\\.|$)|\n                                      \\.tests(\\.|$)|\n                                      \\._\n                                      ')
A:sklearn.tests.test_common.search_cv->SearchCV(make_pipeline(PCA(), Estimator()), param_grid, cv=2, **extra_params).set_params(error_score='raise')
A:sklearn.tests.test_common.tags->_safe_tags(estimator)
A:sklearn.tests.test_common.correct_tags->type(_DEFAULT_TAGS[name])
A:sklearn.tests.test_common.est_params->set(estimator.get_params())
A:sklearn.tests.test_common.est->Estimator()
A:sklearn.tests.test_common.column_name_estimators->list(chain(_tested_estimators(), [make_pipeline(LogisticRegression(C=1))], list(_generate_search_cv_instances()), _estimators_that_predict_in_fit()))
A:sklearn.tests.test_common.(X, _)->make_blobs(n_samples=80, n_features=4, random_state=0)
A:sklearn.tests.test_common.X->numpy.asfortranarray(X)
A:sklearn.tests.test_common.y->numpy.round(X[:, 0])
A:sklearn.tests.test_common.SET_OUTPUT_ESTIMATORS->list(chain(_tested_estimators('transformer'), [make_pipeline(StandardScaler(), MinMaxScaler()), OneHotEncoder(sparse_output=False), FunctionTransformer(feature_names_out='one-to-one')]))
sklearn.tests.test_common._estimators_that_predict_in_fit()
sklearn.tests.test_common._generate_column_transformer_instances()
sklearn.tests.test_common._generate_pipeline()
sklearn.tests.test_common._generate_search_cv_instances()
sklearn.tests.test_common._include_in_get_feature_names_out_check(transformer)
sklearn.tests.test_common._sample_func(x,y=1)
sklearn.tests.test_common._tested_estimators(type_filter=None)
sklearn.tests.test_common._tested_linear_classifiers()
sklearn.tests.test_common.test_all_estimator_no_base_class()
sklearn.tests.test_common.test_all_tests_are_importable()
sklearn.tests.test_common.test_check_estimator_generate_only()
sklearn.tests.test_common.test_check_n_features_in_after_fitting(estimator)
sklearn.tests.test_common.test_check_param_validation(estimator)
sklearn.tests.test_common.test_class_support_removed()
sklearn.tests.test_common.test_class_weight_balanced_linear_classifiers(name,Classifier)
sklearn.tests.test_common.test_configure()
sklearn.tests.test_common.test_estimators(estimator,check,request)
sklearn.tests.test_common.test_estimators_do_not_raise_errors_in_init_or_set_params(Estimator)
sklearn.tests.test_common.test_estimators_get_feature_names_out_error(estimator)
sklearn.tests.test_common.test_f_contiguous_array_estimator(Estimator)
sklearn.tests.test_common.test_get_check_estimator_ids(val,expected)
sklearn.tests.test_common.test_import_all_consistency()
sklearn.tests.test_common.test_pandas_column_name_consistency(estimator)
sklearn.tests.test_common.test_root_import_all_completeness()
sklearn.tests.test_common.test_search_cv(estimator,check,request)
sklearn.tests.test_common.test_set_output_transform(estimator)
sklearn.tests.test_common.test_set_output_transform_configured(estimator,check_func)
sklearn.tests.test_common.test_transformers_get_feature_names_out(transformer)
sklearn.tests.test_common.test_valid_tag_types(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/random_seed.py----------------------------------------
A:sklearn.tests.random_seed.random_seeds->list(range(int(start), int(stop) + 1))
A:sklearn.tests.random_seed.RANDOM_SEED_RANGE->list(range(100))
A:sklearn.tests.random_seed.random_seed_var->os.environ.get('SKLEARN_TESTS_GLOBAL_RANDOM_SEED')
A:sklearn.tests.random_seed.(start, stop)->os.environ.get('SKLEARN_TESTS_GLOBAL_RANDOM_SEED').split('-')
sklearn.tests.random_seed.XDistHooks
sklearn.tests.random_seed.XDistHooks.pytest_configure_node(self,node)->None
sklearn.tests.random_seed.pytest_configure(config)
sklearn.tests.random_seed.pytest_report_header(config)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_min_dependencies_readme.py----------------------------------------
A:sklearn.tests.test_min_dependencies_readme.pattern->re.compile('(\\.\\. \\|)' + '(([A-Za-z]+\\-?)+)' + '(MinVersion\\| replace::)' + '( [0-9]+\\.[0-9]+(\\.[0-9]+)?)')
A:sklearn.tests.test_min_dependencies_readme.matched->re.compile('(\\.\\. \\|)' + '(([A-Za-z]+\\-?)+)' + '(MinVersion\\| replace::)' + '( [0-9]+\\.[0-9]+(\\.[0-9]+)?)').match(line)
A:sklearn.tests.test_min_dependencies_readme.package->package.lower().lower()
A:sklearn.tests.test_min_dependencies_readme.version->parse_version(version)
A:sklearn.tests.test_min_dependencies_readme.min_version->parse_version(dependent_packages[package][0])
A:sklearn.tests.test_min_dependencies_readme.tomllib->pytest.importorskip('tomllib')
A:sklearn.tests.test_min_dependencies_readme.pyproject_toml->pytest.importorskip('tomllib').load(f)
A:sklearn.tests.test_min_dependencies_readme.(package, version)->requirement.split('>=')
A:sklearn.tests.test_min_dependencies_readme.expected_min_version->parse_version(dependent_packages[package][0])
sklearn.tests.test_min_dependencies_readme.test_min_dependencies_pyproject_toml()
sklearn.tests.test_min_dependencies_readme.test_min_dependencies_readme()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_metaestimators.py----------------------------------------
A:sklearn.tests.test_metaestimators.self.coef_->numpy.arange(X.shape[1])
A:sklearn.tests.test_metaestimators.delegate->SubEstimator(hidden_method=method)
A:sklearn.tests.test_metaestimators.delegator->delegator_data.construct(delegate)
A:sklearn.tests.test_metaestimators.sig->set(signature(Estimator).parameters)
A:sklearn.tests.test_metaestimators.estimator->make_pipeline(TfidfVectorizer(), LogisticRegression())
A:sklearn.tests.test_metaestimators.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_metaestimators.X->_enforce_estimator_tags_X(estimator, X).tolist()
A:sklearn.tests.test_metaestimators.y->_enforce_estimator_tags_y(estimator, y).tolist()
sklearn.tests.test_metaestimators.DelegatorData(self,name,construct,skip_methods=(),fit_args=make_classification(random_state=0))
sklearn.tests.test_metaestimators.DelegatorData.__init__(self,name,construct,skip_methods=(),fit_args=make_classification(random_state=0))
sklearn.tests.test_metaestimators._generate_meta_estimator_instances_with_pipeline()
sklearn.tests.test_metaestimators._get_meta_estimator_id(estimator)
sklearn.tests.test_metaestimators.test_meta_estimators_delegate_data_validation(estimator)
sklearn.tests.test_metaestimators.test_metaestimator_delegation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_init.py----------------------------------------
sklearn.tests.test_init.test_import_skl()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_docstrings.py----------------------------------------
A:sklearn.tests.test_docstrings.numpydoc_validation->pytest.importorskip('numpydoc.validate')
A:sklearn.tests.test_docstrings.estimators->all_estimators()
A:sklearn.tests.test_docstrings.displays->all_displays()
A:sklearn.tests.test_docstrings.method_obj->getattr(Klass, method)
A:sklearn.tests.test_docstrings.functions->all_functions()
A:sklearn.tests.test_docstrings.obj->getattr(Klass, method)
A:sklearn.tests.test_docstrings.obj_signature->str(signature(obj))
A:sklearn.tests.test_docstrings.res->pytest.importorskip('numpydoc.validate').validate(args.import_path)
A:sklearn.tests.test_docstrings.res['errors']->list(filter_errors(res['errors'], method))
A:sklearn.tests.test_docstrings.msg->repr_errors(res, method=args.import_path)
A:sklearn.tests.test_docstrings.import_path->'.'.join(import_path)
A:sklearn.tests.test_docstrings.parser->argparse.ArgumentParser(description='Validate docstring with numpydoc.')
A:sklearn.tests.test_docstrings.args->argparse.ArgumentParser(description='Validate docstring with numpydoc.').parse_args()
A:sklearn.tests.test_docstrings.import_path_sections->argparse.ArgumentParser(description='Validate docstring with numpydoc.').parse_args().import_path.split('.')
sklearn.tests.test_docstrings.filter_errors(errors,method,Klass=None)
sklearn.tests.test_docstrings.get_all_functions_names()
sklearn.tests.test_docstrings.get_all_methods()
sklearn.tests.test_docstrings.repr_errors(res,Klass=None,method:Optional[str]=None)->str
sklearn.tests.test_docstrings.test_docstring(Klass,method,request)
sklearn.tests.test_docstrings.test_function_docstring(function_name,request)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_public_functions.py----------------------------------------
A:sklearn.tests.test_public_functions.(module_name, func_name)->func_module.rsplit('.', 1)
A:sklearn.tests.test_public_functions.module->import_module(module_name)
A:sklearn.tests.test_public_functions.func->getattr(module, func_name)
A:sklearn.tests.test_public_functions.func_sig->signature(func)
A:sklearn.tests.test_public_functions.valid_required_params[param_name]->generate_valid_param(make_constraint(parameter_constraints[param_name][0]))
A:sklearn.tests.test_public_functions.validation_params->getattr(func, '_skl_parameter_constraints').keys()
A:sklearn.tests.test_public_functions.param_with_bad_type->type('BadType', (), {})()
A:sklearn.tests.test_public_functions.bad_value->generate_invalid_param_val(constraint)
A:sklearn.tests.test_public_functions.(func, func_name, func_params, required_params)->_get_func_info(func_module)
A:sklearn.tests.test_public_functions.parameter_constraints->getattr(func, '_skl_parameter_constraints')
A:sklearn.tests.test_public_functions.(module_name, class_name)->class_module.rsplit('.', 1)
A:sklearn.tests.test_public_functions.klass->getattr(module, class_name)
A:sklearn.tests.test_public_functions.parameter_constraints_func->getattr(func, '_skl_parameter_constraints')
A:sklearn.tests.test_public_functions.parameter_constraints_class->getattr(klass, '_parameter_constraints')
sklearn.tests.test_public_functions._check_function_param_validation(func,func_name,func_params,required_params,parameter_constraints)
sklearn.tests.test_public_functions._get_func_info(func_module)
sklearn.tests.test_public_functions.test_class_wrapper_param_validation(func_module,class_module)
sklearn.tests.test_public_functions.test_function_param_validation(func_module)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_naive_bayes.py----------------------------------------
A:sklearn.tests.test_naive_bayes.pytestmark->pytest.mark.filterwarnings(f'ignore:{msg}:FutureWarning')
A:sklearn.tests.test_naive_bayes.X->numpy.array([[1, 0], [1, 1]])
A:sklearn.tests.test_naive_bayes.y->numpy.array([0, 1])
A:sklearn.tests.test_naive_bayes.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.tests.test_naive_bayes.X1->numpy.random.RandomState(global_random_seed).normal(size=(10, 3))
A:sklearn.tests.test_naive_bayes.y1->(rng.normal(size=10) > 0).astype(int)
A:sklearn.tests.test_naive_bayes.X2->numpy.random.RandomState(global_random_seed).randint(5, size=(6, 100))
A:sklearn.tests.test_naive_bayes.y2->numpy.array([1, 1, 2, 2, 3, 3])
A:sklearn.tests.test_naive_bayes.clf->CategoricalNB(alpha=1, fit_prior=False, min_categories=min_categories)
A:sklearn.tests.test_naive_bayes.y_pred->CategoricalNB(alpha=1, fit_prior=False, min_categories=min_categories).fit(X2, y2).predict(X2)
A:sklearn.tests.test_naive_bayes.y_pred_proba->CategoricalNB(alpha=1, fit_prior=False, min_categories=min_categories).predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba->CategoricalNB(alpha=1, fit_prior=False, min_categories=min_categories).predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.(X1, y1)->get_random_normal_x_binary_y(global_random_seed)
A:sklearn.tests.test_naive_bayes.sw->numpy.random.RandomState(global_random_seed).rand(y.shape[0])
A:sklearn.tests.test_naive_bayes.clf_sw->GaussianNB().fit(X, y, sample_weight)
A:sklearn.tests.test_naive_bayes.clf1->DiscreteNaiveBayes()
A:sklearn.tests.test_naive_bayes.clf2->MultinomialNB()
A:sklearn.tests.test_naive_bayes.ind->numpy.random.RandomState(global_random_seed).randint(0, X.shape[0], 20)
A:sklearn.tests.test_naive_bayes.sample_weight->numpy.array([1, 1, 2, 2], dtype=np.float64)
A:sklearn.tests.test_naive_bayes.clf_dupl->GaussianNB().fit(X[ind], y[ind])
A:sklearn.tests.test_naive_bayes.priors->numpy.array([0.08, 0.14, 0.03, 0.16, 0.11, 0.16, 0.07, 0.14, 0.11, 0.0])
A:sklearn.tests.test_naive_bayes.Y->numpy.array([0, 0, 0, 1])
A:sklearn.tests.test_naive_bayes.x_empty->numpy.empty((0, X.shape[1]))
A:sklearn.tests.test_naive_bayes.(tmean, tvar)->sklearn.naive_bayes.GaussianNB._update_mean_variance(prev_points, mean, var, x_empty)
A:sklearn.tests.test_naive_bayes.clf_pf->GaussianNB().partial_fit(X, y, np.unique(y))
A:sklearn.tests.test_naive_bayes.clf_pf2->GaussianNB().partial_fit(X[0::2, :], y[0::2], np.unique(y))
A:sklearn.tests.test_naive_bayes.iris->load_iris()
A:sklearn.tests.test_naive_bayes.(X2, y2)->get_random_integer_x_three_classes_y(global_random_seed)
A:sklearn.tests.test_naive_bayes.clf3->MultinomialNB()
A:sklearn.tests.test_naive_bayes.prior->numpy.exp(clf.class_log_prior_)
A:sklearn.tests.test_naive_bayes.(iris_data1, iris_data2, iris_target1, iris_target2)->train_test_split(iris.data, iris.target, test_size=0.4, random_state=415)
A:sklearn.tests.test_naive_bayes.clf_full->DiscreteNaiveBayes(class_prior=prior)
A:sklearn.tests.test_naive_bayes.clf_partial->DiscreteNaiveBayes(class_prior=prior)
A:sklearn.tests.test_naive_bayes.classes->sorted(list(set(y)))
A:sklearn.tests.test_naive_bayes.num_classes->len(classes)
A:sklearn.tests.test_naive_bayes.attribute->getattr(clf, attribute_name, None)
A:sklearn.tests.test_naive_bayes.y_pred2->MultinomialNB().predict(X)
A:sklearn.tests.test_naive_bayes.y_pred_proba2->MultinomialNB().predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba2->MultinomialNB().predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred3->MultinomialNB().predict(X)
A:sklearn.tests.test_naive_bayes.y_pred_proba3->MultinomialNB().predict_proba(X)
A:sklearn.tests.test_naive_bayes.y_pred_log_proba3->MultinomialNB().predict_log_proba(X)
A:sklearn.tests.test_naive_bayes.class_prior->numpy.array([0.75, 0.25])
A:sklearn.tests.test_naive_bayes.feature_prob->numpy.array([[1 / 2, 1 / 2], [2 / 5, 3 / 5]])
A:sklearn.tests.test_naive_bayes.X_test->numpy.array([[0, 1, 1, 0, 0, 1]])
A:sklearn.tests.test_naive_bayes.unnorm_predict_proba->numpy.array([[0.005183999999999999, 0.02194787379972565]])
A:sklearn.tests.test_naive_bayes.num->numpy.log(clf.feature_count_ + 1.0)
A:sklearn.tests.test_naive_bayes.theta->numpy.array([[(0 + 1) / (3 + 6), (1 + 1) / (3 + 6), (1 + 1) / (3 + 6), (0 + 1) / (3 + 6), (0 + 1) / (3 + 6), (1 + 1) / (3 + 6)], [(1 + 1) / (6 + 6), (3 + 1) / (6 + 6), (0 + 1) / (6 + 6), (1 + 1) / (6 + 6), (1 + 1) / (6 + 6), (0 + 1) / (6 + 6)]])
A:sklearn.tests.test_naive_bayes.weights->numpy.zeros(theta.shape)
A:sklearn.tests.test_naive_bayes.normed_weights->numpy.zeros(theta.shape)
A:sklearn.tests.test_naive_bayes.msg->re.escape('Negative values in data passed to ComplementNB (input X)')
A:sklearn.tests.test_naive_bayes.feature_count->numpy.array([[1, 3, 0, 1, 1, 0], [0, 1, 1, 0, 0, 1]])
A:sklearn.tests.test_naive_bayes.class_count->numpy.array([3, 1])
A:sklearn.tests.test_naive_bayes.feature_all->numpy.array([1, 4, 1, 1, 1, 1])
A:sklearn.tests.test_naive_bayes.X3->numpy.array([[1, 4], [2, 5]])
A:sklearn.tests.test_naive_bayes.y3->numpy.array([1, 2])
A:sklearn.tests.test_naive_bayes.error_msg->re.escape('Negative values in data passed to CategoricalNB (input X)')
A:sklearn.tests.test_naive_bayes.X3_test->numpy.array([[2, 5]])
A:sklearn.tests.test_naive_bayes.bayes_numerator->numpy.array([[1 / 3 * 1 / 3, 2 / 3 * 2 / 3]])
A:sklearn.tests.test_naive_bayes.bayes_denominator->numpy.array([[1 / 3 * 1 / 3, 2 / 3 * 2 / 3]]).sum()
A:sklearn.tests.test_naive_bayes.X_n_categories->numpy.array([[0, 0], [0, 1], [0, 0], [1, 1]])
A:sklearn.tests.test_naive_bayes.y_n_categories->numpy.array([1, 1, 2, 2])
A:sklearn.tests.test_naive_bayes.expected_prediction->numpy.array([1])
A:sklearn.tests.test_naive_bayes.predictions->CategoricalNB(alpha=1, fit_prior=False, min_categories=min_categories).predict(new_X)
A:sklearn.tests.test_naive_bayes.nb->MultinomialNB(alpha=alpha, force_alpha=False)
A:sklearn.tests.test_naive_bayes.prob->numpy.array([[5 / 9, 4 / 9], [25 / 49, 24 / 49]])
A:sklearn.tests.test_naive_bayes.alpha->numpy.array([1.0, 2.0, 3.0])
A:sklearn.tests.test_naive_bayes.m_nb->MultinomialNB(alpha=alpha, force_alpha=False)
A:sklearn.tests.test_naive_bayes.(X, y)->load_digits(return_X_y=True)
A:sklearn.tests.test_naive_bayes.binary_3v8->numpy.logical_or(y == 3, y == 8)
A:sklearn.tests.test_naive_bayes.scores->cross_val_score(GaussianNB(), X_3v8, y_3v8, cv=10)
A:sklearn.tests.test_naive_bayes.b->BernoulliNB(alpha=alphas, force_alpha=False)
A:sklearn.tests.test_naive_bayes.alphas->numpy.array([0.0, 1.0])
A:sklearn.tests.test_naive_bayes.est->Estimator().fit(X2, y2)
A:sklearn.tests.test_naive_bayes.jll->Estimator().fit(X2, y2).predict_joint_log_proba(X2)
A:sklearn.tests.test_naive_bayes.log_prob_x->logsumexp(jll, axis=1)
sklearn.tests.test_naive_bayes.get_random_integer_x_three_classes_y(global_random_seed)
sklearn.tests.test_naive_bayes.get_random_normal_x_binary_y(global_random_seed)
sklearn.tests.test_naive_bayes.test_NB_partial_fit_no_first_classes(NaiveBayes,global_random_seed)
sklearn.tests.test_naive_bayes.test_alpha(csr_container)
sklearn.tests.test_naive_bayes.test_alpha_vector()
sklearn.tests.test_naive_bayes.test_bnb()
sklearn.tests.test_naive_bayes.test_bnb_feature_log_prob()
sklearn.tests.test_naive_bayes.test_categoricalnb(global_random_seed)
sklearn.tests.test_naive_bayes.test_categoricalnb_min_categories_errors(min_categories,error_msg)
sklearn.tests.test_naive_bayes.test_categoricalnb_with_min_categories(min_categories,exp_X1_count,exp_X2_count,new_X,exp_n_categories_)
sklearn.tests.test_naive_bayes.test_check_accuracy_on_digits()
sklearn.tests.test_naive_bayes.test_check_alpha()
sklearn.tests.test_naive_bayes.test_cnb()
sklearn.tests.test_naive_bayes.test_discretenb_degenerate_one_class_case(DiscreteNaiveBayes,use_partial_fit,train_on_single_class_y)
sklearn.tests.test_naive_bayes.test_discretenb_partial_fit(DiscreteNaiveBayes)
sklearn.tests.test_naive_bayes.test_discretenb_predict_proba()
sklearn.tests.test_naive_bayes.test_discretenb_prior(DiscreteNaiveBayes,global_random_seed)
sklearn.tests.test_naive_bayes.test_discretenb_provide_prior(DiscreteNaiveBayes)
sklearn.tests.test_naive_bayes.test_discretenb_provide_prior_with_partial_fit(DiscreteNaiveBayes)
sklearn.tests.test_naive_bayes.test_discretenb_sample_weight_multiclass(DiscreteNaiveBayes)
sklearn.tests.test_naive_bayes.test_discretenb_uniform_prior(DiscreteNaiveBayes)
sklearn.tests.test_naive_bayes.test_gnb()
sklearn.tests.test_naive_bayes.test_gnb_check_update_with_no_data()
sklearn.tests.test_naive_bayes.test_gnb_naive_bayes_scale_invariance()
sklearn.tests.test_naive_bayes.test_gnb_neg_priors()
sklearn.tests.test_naive_bayes.test_gnb_partial_fit()
sklearn.tests.test_naive_bayes.test_gnb_prior(global_random_seed)
sklearn.tests.test_naive_bayes.test_gnb_prior_greater_one()
sklearn.tests.test_naive_bayes.test_gnb_prior_large_bias()
sklearn.tests.test_naive_bayes.test_gnb_priors()
sklearn.tests.test_naive_bayes.test_gnb_priors_sum_isclose()
sklearn.tests.test_naive_bayes.test_gnb_sample_weight(global_random_seed)
sklearn.tests.test_naive_bayes.test_gnb_wrong_nb_priors()
sklearn.tests.test_naive_bayes.test_mnb_prior_unobserved_targets()
sklearn.tests.test_naive_bayes.test_mnnb(kind,global_random_seed,csr_container)
sklearn.tests.test_naive_bayes.test_predict_joint_proba(Estimator,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_pipeline.py----------------------------------------
A:sklearn.tests.test_pipeline.iris->load_iris()
A:sklearn.tests.test_pipeline.self.means_->numpy.mean(X, axis=0)
A:sklearn.tests.test_pipeline.self.timestamp_->time.time()
A:sklearn.tests.test_pipeline.pipeline->Pipeline([('estimator', est)])
A:sklearn.tests.test_pipeline.clf->make_pipeline(LogisticRegression())
A:sklearn.tests.test_pipeline.pipe->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)])
A:sklearn.tests.test_pipeline.filter1->SelectKBest(f_classif, k=2)
A:sklearn.tests.test_pipeline.msg->re.escape('Transformer tr1 (type Transf) does not provide get_feature_names_out')
A:sklearn.tests.test_pipeline.pipe2->clone(pipe)
A:sklearn.tests.test_pipeline.params->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).get_params(deep=True)
A:sklearn.tests.test_pipeline.params2->clone(pipe).get_params(deep=True)
A:sklearn.tests.test_pipeline.X->numpy.asarray([[1]])
A:sklearn.tests.test_pipeline.error_msg->re.escape("Invalid parameter 'invalid_param' for estimator LinearRegression(). Valid parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'positive'].")
A:sklearn.tests.test_pipeline.pca->PCA()
A:sklearn.tests.test_pipeline.lof->LocalOutlierFactor(novelty=True)
A:sklearn.tests.test_pipeline.y->numpy.array([1])
A:sklearn.tests.test_pipeline.n_classes->len(np.unique(y))
A:sklearn.tests.test_pipeline.scaler->StandardScaler()
A:sklearn.tests.test_pipeline.predict->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).predict(X)
A:sklearn.tests.test_pipeline.proba->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).predict_proba(X)
A:sklearn.tests.test_pipeline.log_proba->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).predict_log_proba(X)
A:sklearn.tests.test_pipeline.decision_function->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).decision_function(X)
A:sklearn.tests.test_pipeline.km->KMeans(random_state=0, n_init='auto')
A:sklearn.tests.test_pipeline.scaler_for_pipeline->StandardScaler()
A:sklearn.tests.test_pipeline.km_for_pipeline->KMeans(random_state=0, n_init='auto')
A:sklearn.tests.test_pipeline.scaled->StandardScaler().fit_transform(iris.data)
A:sklearn.tests.test_pipeline.separate_pred->KMeans(random_state=0, n_init='auto').fit_predict(scaled)
A:sklearn.tests.test_pipeline.pipeline_pred->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).fit_predict(iris.data)
A:sklearn.tests.test_pipeline.method->getattr(pipe, method_name)
A:sklearn.tests.test_pipeline.svd->TruncatedSVD(n_components=2, random_state=0)
A:sklearn.tests.test_pipeline.select->SelectKBest(k=1)
A:sklearn.tests.test_pipeline.fs->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))])
A:sklearn.tests.test_pipeline.X_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).transform(X)
A:sklearn.tests.test_pipeline.X_sp->csr_container(X)
A:sklearn.tests.test_pipeline.X_sp_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X_sp, y)
A:sklearn.tests.test_pipeline.fs2->clone(fs)
A:sklearn.tests.test_pipeline.transf->DummyTransf()
A:sklearn.tests.test_pipeline.noinvtransf->NoInvTransf()
A:sklearn.tests.test_pipeline.mock->Transf()
A:sklearn.tests.test_pipeline.fu->make_union(ss)
A:sklearn.tests.test_pipeline.(names, transformers)->zip(*fu.transformer_list)
A:sklearn.tests.test_pipeline.X_trans->FeatureUnion([('pass', 'passthrough')]).transform(X_test)
A:sklearn.tests.test_pipeline.X_trans2->DummyTransf().fit(X, y).transform(X)
A:sklearn.tests.test_pipeline.X_trans3->PCA().fit_transform(X)
A:sklearn.tests.test_pipeline.X_back->Pipeline([('estimator', est)]).inverse_transform(X_trans)
A:sklearn.tests.test_pipeline.X_back2->PCA().inverse_transform(X_trans)
A:sklearn.tests.test_pipeline.pipe_params->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).get_params(deep=False)
A:sklearn.tests.test_pipeline.pipe_slice_params->pipe_slice.get_params(deep=False)
A:sklearn.tests.test_pipeline.transf1->Transf()
A:sklearn.tests.test_pipeline.transf2->Transf()
A:sklearn.tests.test_pipeline.mult2->Mult(2)
A:sklearn.tests.test_pipeline.mult3->Mult(3)
A:sklearn.tests.test_pipeline.mult5->Mult(5)
A:sklearn.tests.test_pipeline.t1->Transf()
A:sklearn.tests.test_pipeline.t2->Transf()
A:sklearn.tests.test_pipeline.X_fit_transformed->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X, y)
A:sklearn.tests.test_pipeline.X_fit_transformed_wo_method->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))]).fit_transform(X, y)
A:sklearn.tests.test_pipeline.fs_parallel->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2)
A:sklearn.tests.test_pipeline.fs_parallel2->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2)
A:sklearn.tests.test_pipeline.X_transformed_parallel->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2).transform(X)
A:sklearn.tests.test_pipeline.X_transformed_parallel2->FeatureUnion([('words', CountVectorizer(analyzer='word')), ('chars', CountVectorizer(analyzer='char'))], n_jobs=2).transform(X)
A:sklearn.tests.test_pipeline.word_vect->CountVectorizer(analyzer='word')
A:sklearn.tests.test_pipeline.char_vect->CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))
A:sklearn.tests.test_pipeline.ft->FeatureUnion([('pca', pca), ('passthrough', 'passthrough')])
A:sklearn.tests.test_pipeline.feature_names->FeatureUnion([('pca', pca), ('passthrough', 'passthrough')]).get_feature_names_out()
A:sklearn.tests.test_pipeline.reg->make_pipeline(SelectKBest(k=1), LinearRegression())
A:sklearn.tests.test_pipeline.X_ft->FeatureUnion([('pca', pca), ('passthrough', 'passthrough')]).fit_transform(X)
A:sklearn.tests.test_pipeline.est->SimpleEstimator()
A:sklearn.tests.test_pipeline.estimator->Pipeline([('a', Pipeline([('b', DummyRegressor())]))])
A:sklearn.tests.test_pipeline.cachedir->mkdtemp()
A:sklearn.tests.test_pipeline.memory->joblib.Memory(location=cachedir, verbose=10)
A:sklearn.tests.test_pipeline.cached_pipe->Pipeline([('transf', transf), ('svc', clf)], memory=memory)
A:sklearn.tests.test_pipeline.clf_2->SVC(probability=True, random_state=0)
A:sklearn.tests.test_pipeline.transf_2->DummyTransf()
A:sklearn.tests.test_pipeline.cached_pipe_2->Pipeline([('transf_2', transf_2), ('svc', clf_2)], memory=memory)
A:sklearn.tests.test_pipeline.func->getattr(est, method)
A:sklearn.tests.test_pipeline.ss->StandardScaler()
A:sklearn.tests.test_pipeline.gbdt->HistGradientBoostingClassifier()
A:sklearn.tests.test_pipeline.t->FeatureUnion([('dummy0', Dummy()), ('dummy1', Dummy())])
A:sklearn.tests.test_pipeline.mask->numpy.random.choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool)
A:sklearn.tests.test_pipeline.union->FeatureUnion([('pass', 'passthrough')])
A:sklearn.tests.test_pipeline.rng->numpy.random.RandomState(0)
A:sklearn.tests.test_pipeline.model->Pipeline([('transformer', MinimalTransformer()), ('predictor', Predictor())])
A:sklearn.tests.test_pipeline.y_pred->Pipeline([('transformer', MinimalTransformer()), ('predictor', Predictor())]).predict(X)
A:sklearn.tests.test_pipeline.names->super().get_feature_names_out(input_features=input_features)
A:sklearn.tests.test_pipeline.feature_names_out->Pipeline([('trs', FunctionTransformer()), ('estimator', last_step)]).get_feature_names_out(input_names)
A:sklearn.tests.test_pipeline.(X, y)->load_iris(as_frame=True, return_X_y=True)
A:sklearn.tests.test_pipeline.feature_names_in_->pipe[:-1].get_feature_names_out()
A:sklearn.tests.test_pipeline.pd->pytest.importorskip('pandas')
A:sklearn.tests.test_pipeline.(X, _)->load_iris(as_frame=True, return_X_y=True)
A:sklearn.tests.test_pipeline.(X_train, X_test)->train_test_split(X, random_state=0)
A:sklearn.tests.test_pipeline.scalar->StandardScaler()
A:sklearn.tests.test_pipeline.X_array->numpy.asarray([[1]]).to_numpy()
A:sklearn.tests.test_pipeline.trs->ConsumingTransformer().set_fit_request(sample_weight=True, metadata=True).set_transform_request(sample_weight=True, metadata=True).set_inverse_transform_request(sample_weight=True, metadata=True)
sklearn.tests.test_pipeline.DummyEstimatorParams(BaseEstimator)
sklearn.tests.test_pipeline.DummyEstimatorParams.fit(self,X,y)
sklearn.tests.test_pipeline.DummyEstimatorParams.predict(self,X,got_attribute=False)
sklearn.tests.test_pipeline.DummyEstimatorParams.predict_log_proba(self,X,got_attribute=False)
sklearn.tests.test_pipeline.DummyEstimatorParams.predict_proba(self,X,got_attribute=False)
sklearn.tests.test_pipeline.DummyTransf(Transf)
sklearn.tests.test_pipeline.DummyTransf.fit(self,X,y)
sklearn.tests.test_pipeline.FeatureNameSaver(BaseEstimator)
sklearn.tests.test_pipeline.FeatureNameSaver.fit(self,X,y=None)
sklearn.tests.test_pipeline.FeatureNameSaver.get_feature_names_out(self,input_features=None)
sklearn.tests.test_pipeline.FeatureNameSaver.transform(self,X,y=None)
sklearn.tests.test_pipeline.FitParamT(self)
sklearn.tests.test_pipeline.FitParamT.__init__(self)
sklearn.tests.test_pipeline.FitParamT.fit(self,X,y,should_succeed=False)
sklearn.tests.test_pipeline.FitParamT.fit_predict(self,X,y,should_succeed=False)
sklearn.tests.test_pipeline.FitParamT.predict(self,X)
sklearn.tests.test_pipeline.FitParamT.score(self,X,y=None,sample_weight=None)
sklearn.tests.test_pipeline.Mult(self,mult=1)
sklearn.tests.test_pipeline.Mult.__init__(self,mult=1)
sklearn.tests.test_pipeline.Mult.fit(self,X,y)
sklearn.tests.test_pipeline.Mult.inverse_transform(self,X)
sklearn.tests.test_pipeline.Mult.predict(self,X)
sklearn.tests.test_pipeline.Mult.score(self,X,y=None)
sklearn.tests.test_pipeline.Mult.transform(self,X)
sklearn.tests.test_pipeline.NoFit(self,a=None,b=None)
sklearn.tests.test_pipeline.NoFit.__init__(self,a=None,b=None)
sklearn.tests.test_pipeline.NoInvTransf(NoTrans)
sklearn.tests.test_pipeline.NoInvTransf.transform(self,X)
sklearn.tests.test_pipeline.NoTrans(NoFit)
sklearn.tests.test_pipeline.NoTrans.fit(self,X,y)
sklearn.tests.test_pipeline.NoTrans.get_params(self,deep=False)
sklearn.tests.test_pipeline.NoTrans.set_params(self,**params)
sklearn.tests.test_pipeline.SimpleEstimator(BaseEstimator)
sklearn.tests.test_pipeline.SimpleEstimator.decision_function(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.fit(self,X,y,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.fit_predict(self,X,y,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.fit_transform(self,X,y,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.inverse_transform(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.predict(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.predict_log_proba(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.predict_proba(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.score(self,X,y,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.SimpleEstimator.transform(self,X,sample_weight=None,prop=None)
sklearn.tests.test_pipeline.Transf(NoInvTransf)
sklearn.tests.test_pipeline.Transf.inverse_transform(self,X)
sklearn.tests.test_pipeline.Transf.transform(self,X)
sklearn.tests.test_pipeline.TransfFitParams(Transf)
sklearn.tests.test_pipeline.TransfFitParams.fit(self,X,y,**fit_params)
sklearn.tests.test_pipeline.test_classes_property()
sklearn.tests.test_pipeline.test_feature_names_count_vectorizer()
sklearn.tests.test_pipeline.test_feature_union(csr_container)
sklearn.tests.test_pipeline.test_feature_union_check_if_fitted()
sklearn.tests.test_pipeline.test_feature_union_feature_names()
sklearn.tests.test_pipeline.test_feature_union_feature_names_in_()
sklearn.tests.test_pipeline.test_feature_union_fit_params()
sklearn.tests.test_pipeline.test_feature_union_getitem()
sklearn.tests.test_pipeline.test_feature_union_getitem_error(key)
sklearn.tests.test_pipeline.test_feature_union_named_transformers()
sklearn.tests.test_pipeline.test_feature_union_parallel()
sklearn.tests.test_pipeline.test_feature_union_passthrough_get_feature_names_out()
sklearn.tests.test_pipeline.test_feature_union_set_output()
sklearn.tests.test_pipeline.test_feature_union_warns_unknown_transformer_weight()
sklearn.tests.test_pipeline.test_feature_union_weights()
sklearn.tests.test_pipeline.test_features_names_passthrough()
sklearn.tests.test_pipeline.test_fit_predict_on_pipeline()
sklearn.tests.test_pipeline.test_fit_predict_on_pipeline_without_fit_predict()
sklearn.tests.test_pipeline.test_fit_predict_with_intermediate_fit_params()
sklearn.tests.test_pipeline.test_make_pipeline()
sklearn.tests.test_pipeline.test_make_pipeline_memory()
sklearn.tests.test_pipeline.test_make_union()
sklearn.tests.test_pipeline.test_make_union_kwargs()
sklearn.tests.test_pipeline.test_metadata_routing_error_for_pipeline(method)
sklearn.tests.test_pipeline.test_metadata_routing_for_pipeline(method)
sklearn.tests.test_pipeline.test_n_features_in_feature_union()
sklearn.tests.test_pipeline.test_n_features_in_pipeline()
sklearn.tests.test_pipeline.test_pipeline_check_if_fitted()
sklearn.tests.test_pipeline.test_pipeline_correctly_adjusts_steps(passthrough)
sklearn.tests.test_pipeline.test_pipeline_ducktyping()
sklearn.tests.test_pipeline.test_pipeline_feature_names_out_error_without_definition()
sklearn.tests.test_pipeline.test_pipeline_fit_params()
sklearn.tests.test_pipeline.test_pipeline_fit_transform()
sklearn.tests.test_pipeline.test_pipeline_get_feature_names_out_passes_names_through()
sklearn.tests.test_pipeline.test_pipeline_get_tags_none(passthrough)
sklearn.tests.test_pipeline.test_pipeline_index()
sklearn.tests.test_pipeline.test_pipeline_init_tuple()
sklearn.tests.test_pipeline.test_pipeline_invalid_parameters()
sklearn.tests.test_pipeline.test_pipeline_memory()
sklearn.tests.test_pipeline.test_pipeline_methods_anova()
sklearn.tests.test_pipeline.test_pipeline_methods_pca_svm()
sklearn.tests.test_pipeline.test_pipeline_methods_preprocessing_svm()
sklearn.tests.test_pipeline.test_pipeline_missing_values_leniency()
sklearn.tests.test_pipeline.test_pipeline_named_steps()
sklearn.tests.test_pipeline.test_pipeline_param_error()
sklearn.tests.test_pipeline.test_pipeline_raise_set_params_error()
sklearn.tests.test_pipeline.test_pipeline_sample_weight_supported()
sklearn.tests.test_pipeline.test_pipeline_sample_weight_unsupported()
sklearn.tests.test_pipeline.test_pipeline_score_samples_pca_lof()
sklearn.tests.test_pipeline.test_pipeline_set_output_integration()
sklearn.tests.test_pipeline.test_pipeline_slice(start,end)
sklearn.tests.test_pipeline.test_pipeline_transform()
sklearn.tests.test_pipeline.test_pipeline_with_estimator_with_len()
sklearn.tests.test_pipeline.test_pipeline_with_no_last_step(last_step)
sklearn.tests.test_pipeline.test_predict_methods_with_predict_params(method_name)
sklearn.tests.test_pipeline.test_routing_passed_metadata_not_supported(method)
sklearn.tests.test_pipeline.test_score_samples_on_pipeline_without_score_samples()
sklearn.tests.test_pipeline.test_search_cv_using_minimal_compatible_estimator(Predictor)
sklearn.tests.test_pipeline.test_set_feature_union_passthrough()
sklearn.tests.test_pipeline.test_set_feature_union_step_drop()
sklearn.tests.test_pipeline.test_set_feature_union_steps()
sklearn.tests.test_pipeline.test_set_params_nested_pipeline()
sklearn.tests.test_pipeline.test_set_pipeline_step_passthrough(passthrough)
sklearn.tests.test_pipeline.test_set_pipeline_steps()
sklearn.tests.test_pipeline.test_step_name_validation()
sklearn.tests.test_pipeline.test_verbose(est,method,pattern,capsys)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tests/test_docstring_parameters.py----------------------------------------
A:sklearn.tests.test_docstring_parameters.PUBLIC_MODULES->set([pckg[1] for pckg in walk_packages(prefix='sklearn.', path=sklearn_path) if not ('._' in pckg[1] or '.tests.' in pckg[1])])
A:sklearn.tests.test_docstring_parameters.module->importlib.import_module(name)
A:sklearn.tests.test_docstring_parameters.classes->inspect.getmembers(module, inspect.isclass)
A:sklearn.tests.test_docstring_parameters.cdoc->numpydoc.docscrape.ClassDoc(cls)
A:sklearn.tests.test_docstring_parameters.method->getattr(cls, method_name)
A:sklearn.tests.test_docstring_parameters.sig->signature(method)
A:sklearn.tests.test_docstring_parameters.result->check_docstring_parameters(method, ignore=param_ignore)
A:sklearn.tests.test_docstring_parameters.functions->inspect.getmembers(module, inspect.isfunction)
A:sklearn.tests.test_docstring_parameters.name_->_get_func_name(func)
A:sklearn.tests.test_docstring_parameters.msg->'\n'.join(incorrect)
A:sklearn.tests.test_docstring_parameters.dictionary->numpy.array([[0, 1, 0], [-1, -1, 2], [1, 1, 1], [0, 1, 1], [0, 2, 1]], dtype=np.float64)
A:sklearn.tests.test_docstring_parameters.doc->numpydoc.docscrape.ClassDoc(Estimator)
A:sklearn.tests.test_docstring_parameters.est->_construct_instance(Estimator)
A:sklearn.tests.test_docstring_parameters.(X, y)->make_classification(n_samples=20, n_features=3, n_redundant=0, n_classes=2, random_state=2)
A:sklearn.tests.test_docstring_parameters.y->_enforce_estimator_tags_y(est, y)
A:sklearn.tests.test_docstring_parameters.X->_enforce_estimator_tags_X(est, X)
A:sklearn.tests.test_docstring_parameters.desc->' '.join(attr.desc).lower()
A:sklearn.tests.test_docstring_parameters.fit_attr->list(estimator.__dict__.keys())
A:sklearn.tests.test_docstring_parameters.undocumented_attrs->set(undocumented_attrs).difference(skipped_attributes)
A:sklearn.tests.test_docstring_parameters.obj->getattr(estimator.__class__, name)
sklearn.tests.test_docstring_parameters._construct_compose_pipeline_instance(Estimator)
sklearn.tests.test_docstring_parameters._construct_searchcv_instance(SearchCV)
sklearn.tests.test_docstring_parameters._construct_sparse_coder(Estimator)
sklearn.tests.test_docstring_parameters._get_all_fitted_attributes(estimator)
sklearn.tests.test_docstring_parameters.test_docstring_parameters()
sklearn.tests.test_docstring_parameters.test_fit_docstring_attributes(name,Estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/__check_build/__init__.py----------------------------------------
A:sklearn.__check_build.__init__.dir_content->list()
sklearn.__check_build.__init__.raise_build_error(e)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/_export.py----------------------------------------
A:sklearn.tree._export.SENTINEL->Sentinel()
A:sklearn.tree._export.exporter->_DOTTreeExporter(out_file=out_file, max_depth=max_depth, feature_names=feature_names, class_names=class_names, label=label, filled=filled, leaves_parallel=leaves_parallel, impurity=impurity, node_ids=node_ids, proportion=proportion, rotate=rotate, rounded=rounded, special_characters=special_characters, precision=precision, fontname=fontname)
A:sklearn.tree._export.color->list(self.colors['rgb'][0])
A:sklearn.tree._export.sorted_values->sorted(value, reverse=True)
A:sklearn.tree._export.self.colors['rgb']->_color_brew(tree.n_classes[0])
A:sklearn.tree._export.node_val->node_val.item().item()
A:sklearn.tree._export.value_text->value_text.replace('\n ', characters[4]).replace('\n ', characters[4])
A:sklearn.tree._export.self.bbox_args->dict()
A:sklearn.tree._export.self.arrow_args->dict(arrowstyle='<-')
A:sklearn.tree._export.name->self.node_to_str(et, node_id, criterion=criterion)
A:sklearn.tree._export.ax->matplotlib.pyplot.gca()
A:sklearn.tree._export.my_tree->self._make_tree(0, decision_tree.tree_, decision_tree.criterion)
A:sklearn.tree._export.draw_tree->buchheim(my_tree)
A:sklearn.tree._export.renderer->matplotlib.pyplot.gca().figure.canvas.get_renderer()
A:sklearn.tree._export.max_width->max([extent.width for extent in extents])
A:sklearn.tree._export.max_height->max([extent.height for extent in extents])
A:sklearn.tree._export.kwargs->dict(bbox=self.bbox_args.copy(), ha='center', va='center', zorder=100 - 10 * depth, xycoords='axes fraction', arrowprops=self.arrow_args.copy())
A:sklearn.tree._export.kwargs['bbox']['fc']->matplotlib.pyplot.gca().get_facecolor()
A:sklearn.tree._export.feature_names->check_array(feature_names, ensure_2d=False, dtype=None, ensure_min_samples=0)
A:sklearn.tree._export.class_names->check_array(class_names, ensure_2d=False, dtype=None, ensure_min_samples=0)
A:sklearn.tree._export.out_file->StringIO()
A:sklearn.tree._export.class_name->numpy.argmax(value)
A:sklearn.tree._export.threshold->'{1:.{0}f}'.format(decimals, threshold)
A:sklearn.tree._export.subtree_depth->_compute_depth(tree_, node)
sklearn.tree._export.Sentinel
sklearn.tree._export.Sentinel.__repr__(self)
sklearn.tree._export._BaseTreeExporter(self,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,fontsize=None)
sklearn.tree._export._BaseTreeExporter.__init__(self,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,fontsize=None)
sklearn.tree._export._BaseTreeExporter.get_color(self,value)
sklearn.tree._export._BaseTreeExporter.get_fill_color(self,tree,node_id)
sklearn.tree._export._BaseTreeExporter.node_to_str(self,tree,node_id,criterion)
sklearn.tree._export._DOTTreeExporter(self,out_file=SENTINEL,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3,fontname='helvetica')
sklearn.tree._export._DOTTreeExporter.__init__(self,out_file=SENTINEL,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3,fontname='helvetica')
sklearn.tree._export._DOTTreeExporter.export(self,decision_tree)
sklearn.tree._export._DOTTreeExporter.head(self)
sklearn.tree._export._DOTTreeExporter.recurse(self,tree,node_id,criterion,parent=None,depth=0)
sklearn.tree._export._DOTTreeExporter.tail(self)
sklearn.tree._export._MPLTreeExporter(self,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,fontsize=None)
sklearn.tree._export._MPLTreeExporter.__init__(self,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,fontsize=None)
sklearn.tree._export._MPLTreeExporter._make_tree(self,node_id,et,criterion,depth=0)
sklearn.tree._export._MPLTreeExporter.export(self,decision_tree,ax=None)
sklearn.tree._export._MPLTreeExporter.recurse(self,node,tree,ax,max_x,max_y,depth=0)
sklearn.tree._export._color_brew(n)
sklearn.tree._export._compute_depth(tree,node)
sklearn.tree._export.export_graphviz(decision_tree,out_file=None,*,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3,fontname='helvetica')
sklearn.tree._export.export_text(decision_tree,*,feature_names=None,class_names=None,max_depth=10,spacing=3,decimals=2,show_weights=False)
sklearn.tree._export.plot_tree(decision_tree,*,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,ax=None,fontsize=None)
sklearn.tree.export_graphviz(decision_tree,out_file=None,*,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,leaves_parallel=False,impurity=True,node_ids=False,proportion=False,rotate=False,rounded=False,special_characters=False,precision=3,fontname='helvetica')
sklearn.tree.export_text(decision_tree,*,feature_names=None,class_names=None,max_depth=10,spacing=3,decimals=2,show_weights=False)
sklearn.tree.plot_tree(decision_tree,*,max_depth=None,feature_names=None,class_names=None,label='all',filled=False,impurity=True,node_ids=False,proportion=False,rounded=False,precision=3,ax=None,fontsize=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/_reingold_tilford.py----------------------------------------
A:sklearn.tree._reingold_tilford.lmost_sibling->property(get_lmost_sibling)
A:sklearn.tree._reingold_tilford.dt->first_walk(DrawTree(tree))
A:sklearn.tree._reingold_tilford.min->second_walk(w, m + v.mod, depth + 1, min)
A:sklearn.tree._reingold_tilford.default_ancestor->apportion(w, default_ancestor, distance)
A:sklearn.tree._reingold_tilford.w->v.lbrother()
A:sklearn.tree._reingold_tilford.vil->vil.right().right()
A:sklearn.tree._reingold_tilford.vir->vir.left().left()
A:sklearn.tree._reingold_tilford.vol->vol.left().left()
A:sklearn.tree._reingold_tilford.vor->vor.right().right()
A:sklearn.tree._reingold_tilford.vor.thread->vil.right().right().right()
A:sklearn.tree._reingold_tilford.vol.thread->vir.left().left().left()
sklearn.tree._reingold_tilford.DrawTree(self,tree,parent=None,depth=0,number=1)
sklearn.tree._reingold_tilford.DrawTree.__init__(self,tree,parent=None,depth=0,number=1)
sklearn.tree._reingold_tilford.DrawTree.__repr__(self)
sklearn.tree._reingold_tilford.DrawTree.__str__(self)
sklearn.tree._reingold_tilford.DrawTree.get_lmost_sibling(self)
sklearn.tree._reingold_tilford.DrawTree.lbrother(self)
sklearn.tree._reingold_tilford.DrawTree.left(self)
sklearn.tree._reingold_tilford.DrawTree.max_extents(self)
sklearn.tree._reingold_tilford.DrawTree.right(self)
sklearn.tree._reingold_tilford.Tree(self,label='',node_id=-1,*children)
sklearn.tree._reingold_tilford.Tree.__init__(self,label='',node_id=-1,*children)
sklearn.tree._reingold_tilford.ancestor(vil,v,default_ancestor)
sklearn.tree._reingold_tilford.apportion(v,default_ancestor,distance)
sklearn.tree._reingold_tilford.buchheim(tree)
sklearn.tree._reingold_tilford.execute_shifts(v)
sklearn.tree._reingold_tilford.first_walk(v,distance=1.0)
sklearn.tree._reingold_tilford.move_subtree(wl,wr,shift)
sklearn.tree._reingold_tilford.second_walk(v,m=0,depth=0,min=None)
sklearn.tree._reingold_tilford.third_walk(tree,n)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/_classes.py----------------------------------------
A:sklearn.tree._classes.common_kwargs->dict(estimator_name=estimator_name, input_name='X')
A:sklearn.tree._classes.overall_sum->numpy.sum(X)
A:sklearn.tree._classes.missing_values_in_feature_mask->self._compute_missing_values_in_feature_mask(X)
A:sklearn.tree._classes.random_state->check_random_state(self.random_state)
A:sklearn.tree._classes.check_X_params->dict(dtype=DTYPE, accept_sparse='csc', force_all_finite=False)
A:sklearn.tree._classes.check_y_params->dict(ensure_2d=False, dtype=None)
A:sklearn.tree._classes.(X, y)->self._validate_data(X, y, validate_separately=(check_X_params, check_y_params))
A:sklearn.tree._classes.is_classification->is_classifier(self)
A:sklearn.tree._classes.y->numpy.ascontiguousarray(y, dtype=DOUBLE)
A:sklearn.tree._classes.y_original->numpy.copy(y)
A:sklearn.tree._classes.y_encoded->numpy.zeros(y.shape, dtype=int)
A:sklearn.tree._classes.(classes_k, y_encoded[:, k])->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.tree._classes.expanded_class_weight->compute_sample_weight(self.class_weight, y_original)
A:sklearn.tree._classes.self.n_classes_->numpy.array(self.n_classes_, dtype=np.intp)
A:sklearn.tree._classes.min_samples_leaf->int(ceil(self.min_samples_leaf * n_samples))
A:sklearn.tree._classes.min_samples_split->max(min_samples_split, 2 * min_samples_leaf)
A:sklearn.tree._classes.max_features->max(1, int(self.max_features * self.n_features_in_))
A:sklearn.tree._classes.sample_weight->_check_sample_weight(sample_weight, X, DOUBLE)
A:sklearn.tree._classes.criterion->copy.deepcopy(criterion)
A:sklearn.tree._classes.monotonic_cst->numpy.asarray(monotonic_cst, dtype=np.int8)
A:sklearn.tree._classes.valid_constraints->numpy.isin(monotonic_cst, (-1, 0, 1))
A:sklearn.tree._classes.unique_constaints_value->numpy.unique(monotonic_cst)
A:sklearn.tree._classes.splitter->SPLITTERS[self.splitter](criterion, self.max_features_, min_samples_leaf, min_weight_leaf, random_state, monotonic_cst)
A:sklearn.tree._classes.self.tree_->Tree(self.n_features_in_, np.array([1] * self.n_outputs_, dtype=np.intp), self.n_outputs_)
A:sklearn.tree._classes.builder->BestFirstTreeBuilder(splitter, min_samples_split, min_samples_leaf, min_weight_leaf, max_depth, max_leaf_nodes, self.min_impurity_decrease)
A:sklearn.tree._classes.X->self._validate_X_predict(X, check_input)
A:sklearn.tree._classes.proba->self.predict_proba(X)
A:sklearn.tree._classes.predictions->numpy.zeros((n_samples, self.n_outputs_), dtype=class_type)
A:sklearn.tree._classes.predictions[:, k]->self.classes_[k].take(np.argmax(proba[:, k], axis=1), axis=0)
A:sklearn.tree._classes.n_classes->numpy.atleast_1d(self.n_classes_)
A:sklearn.tree._classes.pruned_tree->Tree(self.n_features_in_, np.array([1] * self.n_outputs_, dtype=np.intp), self.n_outputs_)
A:sklearn.tree._classes.est->clone(self).set_params(ccp_alpha=0.0)
A:sklearn.tree._classes.proba[k]->numpy.log(proba[k])
A:sklearn.tree._classes.grid->numpy.asarray(grid, dtype=DTYPE, order='C')
A:sklearn.tree._classes.averaged_predictions->numpy.zeros(shape=grid.shape[0], dtype=np.float64, order='C')
sklearn.tree.BaseDecisionTree(self,*,criterion,splitter,max_depth,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_features,max_leaf_nodes,random_state,min_impurity_decrease,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree.BaseDecisionTree._compute_missing_values_in_feature_mask(self,X,estimator_name=None)
sklearn.tree.BaseDecisionTree._fit(self,X,y,sample_weight=None,check_input=True,missing_values_in_feature_mask=None)
sklearn.tree.BaseDecisionTree._prune_tree(self)
sklearn.tree.BaseDecisionTree._support_missing_values(self,X)
sklearn.tree.BaseDecisionTree._validate_X_predict(self,X,check_input)
sklearn.tree.BaseDecisionTree.apply(self,X,check_input=True)
sklearn.tree.BaseDecisionTree.cost_complexity_pruning_path(self,X,y,sample_weight=None)
sklearn.tree.BaseDecisionTree.decision_path(self,X,check_input=True)
sklearn.tree.BaseDecisionTree.feature_importances_(self)
sklearn.tree.BaseDecisionTree.get_depth(self)
sklearn.tree.BaseDecisionTree.get_n_leaves(self)
sklearn.tree.BaseDecisionTree.predict(self,X,check_input=True)
sklearn.tree.DecisionTreeClassifier(self,*,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree.DecisionTreeClassifier._more_tags(self)
sklearn.tree.DecisionTreeClassifier.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.tree.DecisionTreeClassifier.predict_log_proba(self,X)
sklearn.tree.DecisionTreeClassifier.predict_proba(self,X,check_input=True)
sklearn.tree.DecisionTreeRegressor(self,*,criterion='squared_error',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree.DecisionTreeRegressor._compute_partial_dependence_recursion(self,grid,target_features)
sklearn.tree.DecisionTreeRegressor._more_tags(self)
sklearn.tree.DecisionTreeRegressor.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.tree.ExtraTreeClassifier(self,*,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree.ExtraTreeRegressor(self,*,criterion='squared_error',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,random_state=None,min_impurity_decrease=0.0,max_leaf_nodes=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.BaseDecisionTree(self,*,criterion,splitter,max_depth,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_features,max_leaf_nodes,random_state,min_impurity_decrease,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.BaseDecisionTree.__init__(self,*,criterion,splitter,max_depth,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_features,max_leaf_nodes,random_state,min_impurity_decrease,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.BaseDecisionTree._compute_missing_values_in_feature_mask(self,X,estimator_name=None)
sklearn.tree._classes.BaseDecisionTree._fit(self,X,y,sample_weight=None,check_input=True,missing_values_in_feature_mask=None)
sklearn.tree._classes.BaseDecisionTree._prune_tree(self)
sklearn.tree._classes.BaseDecisionTree._support_missing_values(self,X)
sklearn.tree._classes.BaseDecisionTree._validate_X_predict(self,X,check_input)
sklearn.tree._classes.BaseDecisionTree.apply(self,X,check_input=True)
sklearn.tree._classes.BaseDecisionTree.cost_complexity_pruning_path(self,X,y,sample_weight=None)
sklearn.tree._classes.BaseDecisionTree.decision_path(self,X,check_input=True)
sklearn.tree._classes.BaseDecisionTree.feature_importances_(self)
sklearn.tree._classes.BaseDecisionTree.get_depth(self)
sklearn.tree._classes.BaseDecisionTree.get_n_leaves(self)
sklearn.tree._classes.BaseDecisionTree.predict(self,X,check_input=True)
sklearn.tree._classes.DecisionTreeClassifier(self,*,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.DecisionTreeClassifier.__init__(self,*,criterion='gini',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.DecisionTreeClassifier._more_tags(self)
sklearn.tree._classes.DecisionTreeClassifier.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.tree._classes.DecisionTreeClassifier.predict_log_proba(self,X)
sklearn.tree._classes.DecisionTreeClassifier.predict_proba(self,X,check_input=True)
sklearn.tree._classes.DecisionTreeRegressor(self,*,criterion='squared_error',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.DecisionTreeRegressor.__init__(self,*,criterion='squared_error',splitter='best',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=None,random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.DecisionTreeRegressor._compute_partial_dependence_recursion(self,grid,target_features)
sklearn.tree._classes.DecisionTreeRegressor._more_tags(self)
sklearn.tree._classes.DecisionTreeRegressor.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.tree._classes.ExtraTreeClassifier(self,*,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.ExtraTreeClassifier.__init__(self,*,criterion='gini',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',random_state=None,max_leaf_nodes=None,min_impurity_decrease=0.0,class_weight=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.ExtraTreeRegressor(self,*,criterion='squared_error',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,random_state=None,min_impurity_decrease=0.0,max_leaf_nodes=None,ccp_alpha=0.0,monotonic_cst=None)
sklearn.tree._classes.ExtraTreeRegressor.__init__(self,*,criterion='squared_error',splitter='random',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,random_state=None,min_impurity_decrease=0.0,max_leaf_nodes=None,ccp_alpha=0.0,monotonic_cst=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/tests/test_monotonic_tree.py----------------------------------------
A:sklearn.tree.tests.test_monotonic_tree.(X, y)->make_classification(n_samples=100, n_features=5, n_classes=2, n_informative=3, random_state=0)
A:sklearn.tree.tests.test_monotonic_tree.monotonic_cst->numpy.zeros(X.shape[1])
A:sklearn.tree.tests.test_monotonic_tree.est->TreeClassifier(max_depth=None, monotonic_cst=np.array([-1, 0.8]), random_state=0)
A:sklearn.tree.tests.test_monotonic_tree.X_train->csc_container(X_train)
A:sklearn.tree.tests.test_monotonic_tree.proba_test->TreeClassifier(max_depth=None, monotonic_cst=np.array([-1, 0.8]), random_state=0).predict_proba(X_test)
A:sklearn.tree.tests.test_monotonic_tree.train->numpy.arange(n_samples_train)
A:sklearn.tree.tests.test_monotonic_tree.test->numpy.arange(n_samples_train, n_samples)
A:sklearn.tree.tests.test_monotonic_tree.X_test->numpy.copy(X[test])
A:sklearn.tree.tests.test_monotonic_tree.X_test_incr->numpy.copy(X_test)
A:sklearn.tree.tests.test_monotonic_tree.X_test_decr->numpy.copy(X_test)
A:sklearn.tree.tests.test_monotonic_tree.y->numpy.random.RandomState(global_random_seed).rand(n_samples)
A:sklearn.tree.tests.test_monotonic_tree.y_incr->TreeClassifier(max_depth=None, monotonic_cst=np.array([-1, 0.8]), random_state=0).predict(X_test_incr)
A:sklearn.tree.tests.test_monotonic_tree.y_decr->TreeClassifier(max_depth=None, monotonic_cst=np.array([-1, 0.8]), random_state=0).predict(X_test_decr)
A:sklearn.tree.tests.test_monotonic_tree.X->numpy.random.RandomState(global_random_seed).rand(n_samples, n_features)
A:sklearn.tree.tests.test_monotonic_tree.reg->DecisionTreeRegressor(max_depth=None, random_state=0).fit(X, -y)
A:sklearn.tree.tests.test_monotonic_tree.X_grid->numpy.linspace(min_x, max_x, n_steps).reshape(-1, 1)
A:sklearn.tree.tests.test_monotonic_tree.y_pred_grid->TreeRegressor(monotonic_cst=monotonic_cst, max_leaf_nodes=n_samples, criterion=criterion, random_state=global_random_seed).predict(X_grid)
A:sklearn.tree.tests.test_monotonic_tree.clf->TreeRegressor(monotonic_cst=monotonic_cst, max_leaf_nodes=n_samples, criterion=criterion, random_state=global_random_seed)
A:sklearn.tree.tests.test_monotonic_tree.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.tree.tests.test_monotonic_tree.upper_bound->numpy.full(tree_.node_count, np.inf)
A:sklearn.tree.tests.test_monotonic_tree.lower_bound->numpy.full(tree_.node_count, -np.inf)
sklearn.tree.tests.test_monotonic_tree.assert_1d_reg_monotonic(clf,monotonic_sign,min_x,max_x,n_steps)
sklearn.tree.tests.test_monotonic_tree.assert_1d_reg_tree_children_monotonic_bounded(tree_,monotonic_sign)
sklearn.tree.tests.test_monotonic_tree.assert_nd_reg_tree_children_monotonic_bounded(tree_,monotonic_cst)
sklearn.tree.tests.test_monotonic_tree.test_1d_opposite_monotonicity_cst_data(TreeRegressor)
sklearn.tree.tests.test_monotonic_tree.test_1d_tree_nodes_values(TreeRegressor,monotonic_sign,depth_first_builder,criterion,global_random_seed)
sklearn.tree.tests.test_monotonic_tree.test_assert_1d_reg_tree_children_monotonic_bounded()
sklearn.tree.tests.test_monotonic_tree.test_assert_nd_reg_tree_children_monotonic_bounded()
sklearn.tree.tests.test_monotonic_tree.test_bad_monotonic_cst_raises(TreeClassifier)
sklearn.tree.tests.test_monotonic_tree.test_missing_values_raises(DecisionTreeEstimator)
sklearn.tree.tests.test_monotonic_tree.test_monotonic_constraints_classifications(TreeClassifier,depth_first_builder,sparse_splitter,global_random_seed,csc_container)
sklearn.tree.tests.test_monotonic_tree.test_monotonic_constraints_regressions(TreeRegressor,depth_first_builder,sparse_splitter,criterion,global_random_seed,csc_container)
sklearn.tree.tests.test_monotonic_tree.test_multiclass_raises(TreeClassifier)
sklearn.tree.tests.test_monotonic_tree.test_multiple_output_raises(TreeClassifier)
sklearn.tree.tests.test_monotonic_tree.test_nd_tree_nodes_values(TreeRegressor,monotonic_sign,depth_first_builder,criterion,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/tests/test_export.py----------------------------------------
A:sklearn.tree.tests.test_export.clf->DecisionTreeRegressor()
A:sklearn.tree.tests.test_export.contents1->export_graphviz(clf, class_names=constructor(['yes', 'no']), out_file=None)
A:sklearn.tree.tests.test_export.out->StringIO()
A:sklearn.tree.tests.test_export.dot_data->export_graphviz(clf, out_file=None, precision=precision, proportion=True)
A:sklearn.tree.tests.test_export.rng_reg->RandomState(2)
A:sklearn.tree.tests.test_export.rng_clf->RandomState(8)
A:sklearn.tree.tests.test_export.expected_report->dedent('\n    |--- feature_1 <= 0.00\n    |   |--- class: cat\n    |--- feature_1 >  0.00\n    |   |--- class: dog\n    ').lstrip()
A:sklearn.tree.tests.test_export.reg->DecisionTreeRegressor(max_depth=2, random_state=0)
A:sklearn.tree.tests.test_export.nodes->plot_tree(clf, feature_names=feature_names)
sklearn.tree.tests.test_export.test_export_text()
sklearn.tree.tests.test_export.test_export_text_errors()
sklearn.tree.tests.test_export.test_export_text_feature_class_names_array_support(constructor)
sklearn.tree.tests.test_export.test_friedman_mse_in_graphviz()
sklearn.tree.tests.test_export.test_graphviz_errors()
sklearn.tree.tests.test_export.test_graphviz_feature_class_names_array_support(constructor)
sklearn.tree.tests.test_export.test_graphviz_toy()
sklearn.tree.tests.test_export.test_not_fitted_tree(pyplot)
sklearn.tree.tests.test_export.test_plot_tree_entropy(pyplot)
sklearn.tree.tests.test_export.test_plot_tree_gini(pyplot)
sklearn.tree.tests.test_export.test_precision()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/tests/test_reingold_tilford.py----------------------------------------
A:sklearn.tree.tests.test_reingold_tilford.simple_tree->Tree('', 0, Tree('', 1), Tree('', 2))
A:sklearn.tree.tests.test_reingold_tilford.bigger_tree->Tree('', 0, Tree('', 1, Tree('', 3), Tree('', 4, Tree('', 7), Tree('', 8))), Tree('', 2, Tree('', 5), Tree('', 6)))
A:sklearn.tree.tests.test_reingold_tilford.layout->buchheim(tree)
A:sklearn.tree.tests.test_reingold_tilford.coordinates->walk_tree(layout)
sklearn.tree.tests.test_reingold_tilford.test_buchheim(tree,n_nodes)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/tree/tests/test_tree.py----------------------------------------
A:sklearn.tree.tests.test_tree.X_small->dataset['X'].astype(tree._tree.DTYPE, copy=False)
A:sklearn.tree.tests.test_tree.iris->sklearn.datasets.load_iris()
A:sklearn.tree.tests.test_tree.rng->numpy.random.RandomState(0)
A:sklearn.tree.tests.test_tree.perm->numpy.random.RandomState(0).permutation(digits.target.size)
A:sklearn.tree.tests.test_tree.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.tree.tests.test_tree.digits->sklearn.datasets.load_digits()
A:sklearn.tree.tests.test_tree.random_state->check_random_state(0)
A:sklearn.tree.tests.test_tree.(X_multilabel, y_multilabel)->sklearn.datasets.make_multilabel_classification(random_state=0, n_samples=30, n_features=10)
A:sklearn.tree.tests.test_tree.X_sparse_pos->check_random_state(0).uniform(size=(20, 5))
A:sklearn.tree.tests.test_tree.y_random->check_random_state(0).randint(0, 4, size=(20,))
A:sklearn.tree.tests.test_tree.X_sparse_mix->_sparse_random_matrix(20, 10, density=0.25, random_state=0).toarray()
A:sklearn.tree.tests.test_tree.internal->numpy.logical_not(external)
A:sklearn.tree.tests.test_tree.clf->DecisionTreeClassifier(random_state=0)
A:sklearn.tree.tests.test_tree.reg->DecisionTreeRegressor(criterion='poisson', random_state=42)
A:sklearn.tree.tests.test_tree.y->numpy.random.RandomState(0).randint(0, high=2, size=n_samples)
A:sklearn.tree.tests.test_tree.(gridx, gridy)->numpy.indices(y.shape)
A:sklearn.tree.tests.test_tree.score->DecisionTreeClassifier(random_state=0).score(X, y)
A:sklearn.tree.tests.test_tree.loss->metric(diabetes.target, reg.predict(diabetes.data))
A:sklearn.tree.tests.test_tree.prob_predict->DecisionTreeClassifier(random_state=0).predict_proba(iris.data)
A:sklearn.tree.tests.test_tree.X->numpy.random.RandomState(0).standard_normal(size=(n_samples, 10))
A:sklearn.tree.tests.test_tree.(X, y)->make_data(n_samples=n_samples, n_features=n_features, random_state=rng)
A:sklearn.tree.tests.test_tree.n_important->numpy.sum(importances > 0.1)
A:sklearn.tree.tests.test_tree.clf2->DecisionTreeClassifier(random_state=0, ccp_alpha=10)
A:sklearn.tree.tests.test_tree.est->ALL_TREES[name](splitter=splitter)
A:sklearn.tree.tests.test_tree.Xf->numpy.asfortranarray(X)
A:sklearn.tree.tests.test_tree.t->numpy.asarray(T)
A:sklearn.tree.tests.test_tree.out->ALL_TREES[name](splitter=splitter).tree_.apply(X)
A:sklearn.tree.tests.test_tree.node_counts->numpy.bincount(out)
A:sklearn.tree.tests.test_tree.weights->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.tree.tests.test_tree.total_weight->numpy.sum(weights)
A:sklearn.tree.tests.test_tree.node_weights->numpy.bincount(out)
A:sklearn.tree.tests.test_tree.est1->TreeEstimator(max_leaf_nodes=max_leaf_nodes, random_state=0)
A:sklearn.tree.tests.test_tree.est2->pickle.loads(serialized_object)
A:sklearn.tree.tests.test_tree.est3->TreeEstimator(max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.0001, random_state=0)
A:sklearn.tree.tests.test_tree.est4->TreeEstimator(max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=0.1, random_state=0)
A:sklearn.tree.tests.test_tree.serialized_object->pickle.dumps(est)
A:sklearn.tree.tests.test_tree.score2->pickle.loads(serialized_object).score(X, y)
A:sklearn.tree.tests.test_tree.y_hat->DecisionTreeRegressor(criterion='poisson', random_state=42).fit(X, y).predict(T)
A:sklearn.tree.tests.test_tree.proba->DecisionTreeClassifier(random_state=0).predict_proba(T)
A:sklearn.tree.tests.test_tree.log_proba->DecisionTreeClassifier(random_state=0).predict_log_proba(T)
A:sklearn.tree.tests.test_tree.sample_weight->numpy.ones(X.shape[0])
A:sklearn.tree.tests.test_tree.duplicates->numpy.random.RandomState(0).randint(0, X.shape[0], 100)
A:sklearn.tree.tests.test_tree.clf1->DecisionTreeClassifier(random_state=0)
A:sklearn.tree.tests.test_tree.clf3->TreeClassifier(class_weight=[{0: 2.0, 1: 2.0, 2: 1.0}, {0: 2.0, 1: 1.0, 2: 2.0}, {0: 1.0, 1: 2.0, 2: 2.0}], random_state=0)
A:sklearn.tree.tests.test_tree.clf4->TreeClassifier(class_weight='balanced', random_state=0)
A:sklearn.tree.tests.test_tree.value->getattr(DecisionTreeClassifier().fit([[0], [1]], [0, 1]).tree_, attr)
A:sklearn.tree.tests.test_tree.X_sparse->csc_container((data, indices, indptr), shape=(n_samples, n_features))
A:sklearn.tree.tests.test_tree.d->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y)
A:sklearn.tree.tests.test_tree.s->TreeEstimator(random_state=0, max_depth=max_depth).fit(X_sparse, y)
A:sklearn.tree.tests.test_tree.y_pred->DecisionTreeRegressor(criterion='poisson', random_state=42).predict(X)
A:sklearn.tree.tests.test_tree.y_proba->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y).predict_proba(X)
A:sklearn.tree.tests.test_tree.y_log_proba->TreeEstimator(random_state=0, max_depth=max_depth).fit(X, y).predict_log_proba(X)
A:sklearn.tree.tests.test_tree.X_sparse_test->X_sparse_test.copy().copy()
A:sklearn.tree.tests.test_tree.samples->numpy.arange(n_samples)
A:sklearn.tree.tests.test_tree.n_nonzero_i->check_random_state(0).binomial(n_samples, 0.5)
A:sklearn.tree.tests.test_tree.indices->numpy.concatenate(indices).astype(np.int32)
A:sklearn.tree.tests.test_tree.indptr->numpy.array(indptr, dtype=np.int32)
A:sklearn.tree.tests.test_tree.data->numpy.nan_to_num(data.astype('float32'))
A:sklearn.tree.tests.test_tree.X_test->X_sparse_test.copy().copy().toarray()
A:sklearn.tree.tests.test_tree.X_2d->sklearn.datasets.load_iris().data[:, 0].reshape((-1, 1))
A:sklearn.tree.tests.test_tree.X_small32->csr_container(X_small.astype(tree._tree.DTYPE, copy=False))
A:sklearn.tree.tests.test_tree.node_indicator->est.decision_path(X).toarray()
A:sklearn.tree.tests.test_tree.node_indicator_csr->ALL_TREES[name](splitter=splitter).decision_path(X)
A:sklearn.tree.tests.test_tree.leaves->ALL_TREES[name](splitter=splitter).apply(X)
A:sklearn.tree.tests.test_tree.max_depth->est.decision_path(X).toarray().sum(axis=1).max()
A:sklearn.tree.tests.test_tree.dt_mae->DecisionTreeRegressor(random_state=0, criterion='absolute_error', max_leaf_nodes=2)
A:sklearn.tree.tests.test_tree.n_classes->numpy.array([0, 1], dtype=expected_dtype)
A:sklearn.tree.tests.test_tree.criteria->typename(n_outputs, n_samples)
A:sklearn.tree.tests.test_tree.result->copy_func(criteria).__reduce__()
A:sklearn.tree.tests.test_tree.tree->DecisionTreeClassifier(random_state=rng).fit(X_train, y_train)
A:sklearn.tree.tests.test_tree.terminal_regions->DecisionTreeClassifier(random_state=rng).fit(X_train, y_train).apply(X)
A:sklearn.tree.tests.test_tree.left_leaf->set(np.where(tree.tree_.children_left == TREE_LEAF)[0])
A:sklearn.tree.tests.test_tree.empty_leaf->set(np.where(tree.tree_.children_left == TREE_LEAF)[0]).difference(terminal_regions)
A:sklearn.tree.tests.test_tree.info->ALL_TREES[name](splitter=splitter).cost_complexity_pruning_path(X, y)
A:sklearn.tree.tests.test_tree.(tree_node_idx, subtree_node_idx)->stack.pop()
A:sklearn.tree.tests.test_tree.X_readonly->sparse_container(dataset['X'])
A:sklearn.tree.tests.test_tree.X_readonly.data->numpy.array(X_readonly.data, dtype=tree._tree.DTYPE)
A:sklearn.tree.tests.test_tree.(X_readonly.data, X_readonly.indices, X_readonly.indptr)->create_memmap_backed_data((X_readonly.data, X_readonly.indices, X_readonly.indptr))
A:sklearn.tree.tests.test_tree.y_readonly->create_memmap_backed_data(np.array(y_small, dtype=tree._tree.DTYPE))
A:sklearn.tree.tests.test_tree.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=rng)
A:sklearn.tree.tests.test_tree.tree_poi->DecisionTreeRegressor(criterion='poisson', min_samples_split=10, random_state=rng)
A:sklearn.tree.tests.test_tree.tree_mse->DecisionTreeRegressor(criterion='squared_error', min_samples_split=10, random_state=rng)
A:sklearn.tree.tests.test_tree.dummy->DummyRegressor(strategy='mean').fit(X_train, y_train)
A:sklearn.tree.tests.test_tree.metric_poi->mean_poisson_deviance(y, tree_poi.predict(X))
A:sklearn.tree.tests.test_tree.metric_mse->mean_poisson_deviance(y, np.clip(tree_mse.predict(X), 1e-15, None))
A:sklearn.tree.tests.test_tree.metric_dummy->mean_poisson_deviance(y, dummy.predict(X))
A:sklearn.tree.tests.test_tree.tree_params->dict(criterion=criterion)
A:sklearn.tree.tests.test_tree.X2->numpy.concatenate([X, X[:n_samples // 2]], axis=0)
A:sklearn.tree.tests.test_tree.y2->numpy.concatenate([y, y[:n_samples // 2]])
A:sklearn.tree.tests.test_tree.sample_weight_1->numpy.ones(len(y))
A:sklearn.tree.tests.test_tree.tree1->DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)
A:sklearn.tree.tests.test_tree.tree2->DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)
A:sklearn.tree.tests.test_tree.tree_log_loss->Tree(criterion='log_loss', random_state=43).fit(X, y)
A:sklearn.tree.tests.test_tree.tree_entropy->Tree(criterion='entropy', random_state=43).fit(X, y)
A:sklearn.tree.tests.test_tree.f->io.BytesIO()
A:sklearn.tree.tests.test_tree.p->NumpyPickler(f)
A:sklearn.tree.tests.test_tree.p.dispatch_table->copyreg.dispatch_table.copy()
A:sklearn.tree.tests.test_tree.new_clf->joblib.load(joblib_dump_with_different_bitness())
A:sklearn.tree.tests.test_tree.new_score->joblib.load(joblib_dump_with_different_bitness()).score(X, y)
A:sklearn.tree.tests.test_tree.obj->obj.byteswap().view(obj.dtype.newbyteorder()).byteswap().view(obj.dtype.newbyteorder())
A:sklearn.tree.tests.test_tree.new_dtype->numpy.dtype({'names': list(new_dtype_dict.keys()), 'formats': list(new_dtype_dict.values())})
A:sklearn.tree.tests.test_tree.(tree_cls, (n_features, n_classes, n_outputs), state)->DecisionTreeClassifier(random_state=rng).fit(X_train, y_train).__reduce__()
A:sklearn.tree.tests.test_tree.new_n_classes->numpy.array([0, 1], dtype=expected_dtype).astype(new_dtype, casting='same_kind')
A:sklearn.tree.tests.test_tree.new_state->state.copy()
A:sklearn.tree.tests.test_tree.new_state['nodes']->get_different_bitness_node_ndarray(new_state['nodes'])
A:sklearn.tree.tests.test_tree.wrong_dim_n_classes->numpy.array([[0, 1]], dtype=expected_dtype)
A:sklearn.tree.tests.test_tree.wrong_dtype_n_classes->numpy.array([0, 1], dtype=expected_dtype).astype(np.float64)
A:sklearn.tree.tests.test_tree.expected_dtype->numpy.dtype(np.float64)
A:sklearn.tree.tests.test_tree.value_ndarray->numpy.zeros(expected_shape, dtype=expected_dtype)
A:sklearn.tree.tests.test_tree.node_ndarray->numpy.zeros((5,), dtype=expected_dtype)
A:sklearn.tree.tests.test_tree.problematic_node_ndarray->numpy.zeros((5,), dtype=expected_dtype).astype(new_dtype)
A:sklearn.tree.tests.test_tree.new_dtype_dict->dtype_dict.copy()
A:sklearn.tree.tests.test_tree.criterion->CRITERIA_CLF['gini'](n_outputs, n_classes)
A:sklearn.tree.tests.test_tree.splitter->Splitter(criterion, max_features, 5, 0.5, rng, monotonic_cst=None)
A:sklearn.tree.tests.test_tree.splitter_serialize->pickle.dumps(splitter)
A:sklearn.tree.tests.test_tree.splitter_back->pickle.loads(splitter_serialize)
A:sklearn.tree.tests.test_tree.pickle_path->str(tmpdir.join('clf.joblib'))
A:sklearn.tree.tests.test_tree.loaded_clf->joblib.load(pickle_path, mmap_mode='r')
A:sklearn.tree.tests.test_tree.dtc->DecisionTreeClassifier(random_state=42, max_depth=1, criterion=criterion)
A:sklearn.tree.tests.test_tree.y_nan_pred->DecisionTreeClassifier(random_state=42, max_depth=1, criterion=criterion).predict(X_test)
A:sklearn.tree.tests.test_tree.X_missing->numpy.random.RandomState(0).standard_normal(size=(n_samples, 10)).copy()
A:sklearn.tree.tests.test_tree.(X_missing_train, X_missing_test, y_train, y_test)->train_test_split(X_missing, y, random_state=0)
A:sklearn.tree.tests.test_tree.sample_weight_train->numpy.ones(X_missing_train.shape[0])
A:sklearn.tree.tests.test_tree.tree_with_missing->Tree(random_state=rng)
A:sklearn.tree.tests.test_tree.score_with_missing->Tree(random_state=rng).score(X_missing_test, y_test)
A:sklearn.tree.tests.test_tree.score_without_missing->DecisionTreeClassifier(random_state=rng).fit(X_train, y_train).score(X_test, y_test)
A:sklearn.tree.tests.test_tree.X_random_mask->numpy.random.RandomState(0).choice([False, True], size=n_samples, p=[0.95, 0.05])
A:sklearn.tree.tests.test_tree.y_mask->numpy.random.RandomState(0).randint(0, high=2, size=n_samples).copy().astype(bool)
A:sklearn.tree.tests.test_tree.X_predictive->numpy.random.RandomState(0).standard_normal(size=n_samples)
A:sklearn.tree.tests.test_tree.tree_with_sw->Tree(random_state=0)
A:sklearn.tree.tests.test_tree.tree_samples_removed->Tree(random_state=0)
A:sklearn.tree.tests.test_tree.pickle1->pickle.dumps(tree1)
A:sklearn.tree.tests.test_tree.pickle2->pickle.dumps(tree2)
sklearn.tree.tests.test_tree.assert_is_subtree(tree,subtree)
sklearn.tree.tests.test_tree.assert_pruning_creates_subtree(estimator_cls,X,y,pruning_path)
sklearn.tree.tests.test_tree.assert_tree_equal(d,s,message)
sklearn.tree.tests.test_tree.check_min_weight_fraction_leaf(name,datasets,sparse_container=None)
sklearn.tree.tests.test_tree.check_min_weight_fraction_leaf_with_min_samples_leaf(name,datasets,sparse_container=None)
sklearn.tree.tests.test_tree.check_raise_error_on_1d_input(name)
sklearn.tree.tests.test_tree.check_sparse_input(tree,dataset,max_depth=None)
sklearn.tree.tests.test_tree.get_different_alignment_node_ndarray(node_ndarray)
sklearn.tree.tests.test_tree.get_different_bitness_node_ndarray(node_ndarray)
sklearn.tree.tests.test_tree.reduce_tree_with_different_bitness(tree)
sklearn.tree.tests.test_tree.test_1d_input(name)
sklearn.tree.tests.test_tree.test_apply_path_readonly_all_trees(name,splitter,sparse_container)
sklearn.tree.tests.test_tree.test_arrayrepr()
sklearn.tree.tests.test_tree.test_arrays_persist()
sklearn.tree.tests.test_tree.test_balance_property(criterion,Tree)
sklearn.tree.tests.test_tree.test_behaviour_constant_feature_after_splits()
sklearn.tree.tests.test_tree.test_big_input()
sklearn.tree.tests.test_tree.test_check_n_classes()
sklearn.tree.tests.test_tree.test_check_node_ndarray()
sklearn.tree.tests.test_tree.test_check_value_ndarray()
sklearn.tree.tests.test_tree.test_class_weight_errors(name)
sklearn.tree.tests.test_tree.test_class_weights(name)
sklearn.tree.tests.test_tree.test_classes_shape()
sklearn.tree.tests.test_tree.test_classification_toy()
sklearn.tree.tests.test_tree.test_criterion_copy()
sklearn.tree.tests.test_tree.test_criterion_entropy_same_as_log_loss(Tree,n_classes)
sklearn.tree.tests.test_tree.test_decision_path(name)
sklearn.tree.tests.test_tree.test_decision_path_hardcoded()
sklearn.tree.tests.test_tree.test_decision_tree_regressor_sample_weight_consistency(criterion)
sklearn.tree.tests.test_tree.test_deterministic_pickle()
sklearn.tree.tests.test_tree.test_diabetes_overfit(name,Tree,criterion)
sklearn.tree.tests.test_tree.test_diabetes_underfit(name,Tree,criterion,max_depth,metric,max_loss)
sklearn.tree.tests.test_tree.test_different_bitness_joblib_pickle()
sklearn.tree.tests.test_tree.test_different_bitness_pickle()
sklearn.tree.tests.test_tree.test_different_endianness_joblib_pickle()
sklearn.tree.tests.test_tree.test_different_endianness_pickle()
sklearn.tree.tests.test_tree.test_empty_leaf_infinite_threshold(sparse_container)
sklearn.tree.tests.test_tree.test_error()
sklearn.tree.tests.test_tree.test_explicit_sparse_zeros(tree_type,csc_container,csr_container)
sklearn.tree.tests.test_tree.test_huge_allocations()
sklearn.tree.tests.test_tree.test_importances()
sklearn.tree.tests.test_tree.test_importances_gini_equal_squared_error()
sklearn.tree.tests.test_tree.test_importances_raises()
sklearn.tree.tests.test_tree.test_iris()
sklearn.tree.tests.test_tree.test_mae()
sklearn.tree.tests.test_tree.test_max_features()
sklearn.tree.tests.test_tree.test_max_leaf_nodes()
sklearn.tree.tests.test_tree.test_max_leaf_nodes_max_depth()
sklearn.tree.tests.test_tree.test_memory_layout()
sklearn.tree.tests.test_tree.test_min_impurity_decrease(global_random_seed)
sklearn.tree.tests.test_tree.test_min_sample_split_1_error(Tree)
sklearn.tree.tests.test_tree.test_min_samples_leaf()
sklearn.tree.tests.test_tree.test_min_samples_split()
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf_on_dense_input(name)
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf_on_sparse_input(name,csc_container)
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf_with_min_samples_leaf_on_dense_input(name)
sklearn.tree.tests.test_tree.test_min_weight_fraction_leaf_with_min_samples_leaf_on_sparse_input(name,csc_container)
sklearn.tree.tests.test_tree.test_min_weight_leaf_split_level(name,sparse_container)
sklearn.tree.tests.test_tree.test_missing_value_errors(sparse_container,tree)
sklearn.tree.tests.test_tree.test_missing_value_is_predictive()
sklearn.tree.tests.test_tree.test_missing_values_best_splitter_three_classes(criterion)
sklearn.tree.tests.test_tree.test_missing_values_best_splitter_to_left(criterion)
sklearn.tree.tests.test_tree.test_missing_values_best_splitter_to_right(criterion)
sklearn.tree.tests.test_tree.test_missing_values_is_resilience(make_data,Tree,sample_weight_train)
sklearn.tree.tests.test_tree.test_missing_values_missing_both_classes_has_nan(criterion)
sklearn.tree.tests.test_tree.test_missing_values_on_equal_nodes_no_missing(criterion)
sklearn.tree.tests.test_tree.test_missing_values_poisson()
sklearn.tree.tests.test_tree.test_multioutput()
sklearn.tree.tests.test_tree.test_no_sparse_y_support(name,csr_container)
sklearn.tree.tests.test_tree.test_numerical_stability()
sklearn.tree.tests.test_tree.test_only_constant_features()
sklearn.tree.tests.test_tree.test_pickle()
sklearn.tree.tests.test_tree.test_poisson_vs_mse()
sklearn.tree.tests.test_tree.test_poisson_zero_nodes(seed)
sklearn.tree.tests.test_tree.test_probability()
sklearn.tree.tests.test_tree.test_prune_single_node_tree()
sklearn.tree.tests.test_tree.test_prune_tree_classifier_are_subtrees(dataset,tree_cls)
sklearn.tree.tests.test_tree.test_prune_tree_regression_are_subtrees(dataset,tree_cls)
sklearn.tree.tests.test_tree.test_public_apply_all_trees(name)
sklearn.tree.tests.test_tree.test_public_apply_sparse_trees(name,csr_container)
sklearn.tree.tests.test_tree.test_pure_set()
sklearn.tree.tests.test_tree.test_realloc()
sklearn.tree.tests.test_tree.test_regression_toy(Tree,criterion)
sklearn.tree.tests.test_tree.test_sample_weight()
sklearn.tree.tests.test_tree.test_sample_weight_invalid()
sklearn.tree.tests.test_tree.test_sample_weight_non_uniform(make_data,Tree)
sklearn.tree.tests.test_tree.test_sparse_criteria(tree_type,dataset,csc_container,criterion)
sklearn.tree.tests.test_tree.test_sparse_input(tree_type,dataset)
sklearn.tree.tests.test_tree.test_sparse_input_reg_trees(tree_type,dataset)
sklearn.tree.tests.test_tree.test_sparse_parameters(tree_type,dataset,csc_container)
sklearn.tree.tests.test_tree.test_splitter_serializable(Splitter)
sklearn.tree.tests.test_tree.test_tree_deserialization_from_read_only_buffer(tmpdir)
sklearn.tree.tests.test_tree.test_unbalanced_iris()
sklearn.tree.tests.test_tree.test_weighted_classification_toy()
sklearn.tree.tests.test_tree.test_with_only_one_non_constant_features()
sklearn.tree.tests.test_tree.test_xor()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/__init__.py----------------------------------------
sklearn.impute.__init__.__getattr__(name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/_iterative.py----------------------------------------
A:sklearn.impute._iterative._ImputerTriplet->namedtuple('_ImputerTriplet', ['feat_idx', 'neighbor_feat_idx', 'estimator'])
A:sklearn.impute._iterative.estimator->clone(self._estimator)
A:sklearn.impute._iterative.X_train->_safe_indexing(_safe_indexing(X_filled, neighbor_feat_idx, axis=1), ~missing_row_mask, axis=0)
A:sklearn.impute._iterative.y_train->_safe_indexing(_safe_indexing(X_filled, feat_idx, axis=1), ~missing_row_mask, axis=0)
A:sklearn.impute._iterative.X_test->_safe_indexing(_safe_indexing(X_filled, neighbor_feat_idx, axis=1), missing_row_mask, axis=0)
A:sklearn.impute._iterative.(mus, sigmas)->clone(self._estimator).predict(X_test, return_std=True)
A:sklearn.impute._iterative.imputed_values->numpy.clip(imputed_values, self._min_value[feat_idx], self._max_value[feat_idx])
A:sklearn.impute._iterative.truncated_normal->scipy.stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas)
A:sklearn.impute._iterative.imputed_values[inrange_mask]->scipy.stats.truncnorm(a=a, b=b, loc=mus, scale=sigmas).rvs(random_state=self.random_state_)
A:sklearn.impute._iterative.neighbor_feat_idx->self._get_neighbor_feat_idx(n_features, feat_idx, abs_corr_mat)
A:sklearn.impute._iterative.inds_left->numpy.arange(feat_idx)
A:sklearn.impute._iterative.inds_right->numpy.arange(feat_idx + 1, n_features)
A:sklearn.impute._iterative.frac_of_missing_values->X_missing_mask.copy().mean(axis=0)
A:sklearn.impute._iterative.missing_values_idx->numpy.arange(np.shape(frac_of_missing_values)[0])
A:sklearn.impute._iterative.abs_corr_mat->self._get_abs_corr_mat(Xt)
A:sklearn.impute._iterative.X->self._validate_data(X, dtype=FLOAT_DTYPES, order='F', reset=in_fit, force_all_finite=force_all_finite)
A:sklearn.impute._iterative.X_missing_mask->_get_mask(X, self.missing_values)
A:sklearn.impute._iterative.mask_missing_values->_get_mask(X, self.missing_values).copy()
A:sklearn.impute._iterative.self.initial_imputer_->SimpleImputer(missing_values=self.missing_values, strategy=self.initial_strategy, fill_value=self.fill_value, keep_empty_features=self.keep_empty_features).set_output(transform='default')
A:sklearn.impute._iterative.X_filled->self.initial_imputer_.transform(X)
A:sklearn.impute._iterative.valid_mask->numpy.flatnonzero(np.logical_not(np.isnan(self.initial_imputer_.statistics_)))
A:sklearn.impute._iterative.limit->check_array(limit, force_all_finite=False, copy=False, ensure_2d=False)
A:sklearn.impute._iterative.self.random_state_->getattr(self, 'random_state_', check_random_state(self.random_state))
A:sklearn.impute._iterative.self._estimator->clone(self.estimator)
A:sklearn.impute._iterative.(X, Xt, mask_missing_values, complete_mask)->self._initial_imputation(X, in_fit=False)
A:sklearn.impute._iterative.X_indicator->super()._transform_indicator(complete_mask)
A:sklearn.impute._iterative.self._min_value->self._validate_limit(self.min_value, 'min', X.shape[1])
A:sklearn.impute._iterative.self._max_value->self._validate_limit(self.max_value, 'max', X.shape[1])
A:sklearn.impute._iterative.ordered_idx->self._get_ordered_idx(mask_missing_values)
A:sklearn.impute._iterative.self.n_features_with_missing_->len(ordered_idx)
A:sklearn.impute._iterative.start_t->time()
A:sklearn.impute._iterative.Xt_previous->Xt.copy()
A:sklearn.impute._iterative.(Xt, estimator)->self._impute_one_feature(Xt, mask_missing_values, feat_idx, neighbor_feat_idx, estimator=None, fit_mode=True)
A:sklearn.impute._iterative.estimator_triplet->_ImputerTriplet(feat_idx, neighbor_feat_idx, estimator)
A:sklearn.impute._iterative.inf_norm->numpy.linalg.norm(Xt - Xt_previous, ord=np.inf, axis=None)
A:sklearn.impute._iterative.(Xt, _)->self._impute_one_feature(Xt, mask_missing_values, estimator_triplet.feat_idx, estimator_triplet.neighbor_feat_idx, estimator=estimator_triplet.estimator, fit_mode=False)
A:sklearn.impute._iterative.input_features->_check_feature_names_in(self, input_features)
A:sklearn.impute._iterative.names->self.initial_imputer_.get_feature_names_out(input_features)
sklearn.impute.IterativeImputer(self,estimator=None,*,missing_values=np.nan,sample_posterior=False,max_iter=10,tol=0.001,n_nearest_features=None,initial_strategy='mean',fill_value=None,imputation_order='ascending',skip_complete=False,min_value=-np.inf,max_value=np.inf,verbose=0,random_state=None,add_indicator=False,keep_empty_features=False)
sklearn.impute.IterativeImputer._get_abs_corr_mat(self,X_filled,tolerance=1e-06)
sklearn.impute.IterativeImputer._get_neighbor_feat_idx(self,n_features,feat_idx,abs_corr_mat)
sklearn.impute.IterativeImputer._get_ordered_idx(self,mask_missing_values)
sklearn.impute.IterativeImputer._impute_one_feature(self,X_filled,mask_missing_values,feat_idx,neighbor_feat_idx,estimator=None,fit_mode=True)
sklearn.impute.IterativeImputer._initial_imputation(self,X,in_fit=False)
sklearn.impute.IterativeImputer._validate_limit(limit,limit_type,n_features)
sklearn.impute.IterativeImputer.fit(self,X,y=None)
sklearn.impute.IterativeImputer.fit_transform(self,X,y=None)
sklearn.impute.IterativeImputer.get_feature_names_out(self,input_features=None)
sklearn.impute.IterativeImputer.transform(self,X)
sklearn.impute._iterative.IterativeImputer(self,estimator=None,*,missing_values=np.nan,sample_posterior=False,max_iter=10,tol=0.001,n_nearest_features=None,initial_strategy='mean',fill_value=None,imputation_order='ascending',skip_complete=False,min_value=-np.inf,max_value=np.inf,verbose=0,random_state=None,add_indicator=False,keep_empty_features=False)
sklearn.impute._iterative.IterativeImputer.__init__(self,estimator=None,*,missing_values=np.nan,sample_posterior=False,max_iter=10,tol=0.001,n_nearest_features=None,initial_strategy='mean',fill_value=None,imputation_order='ascending',skip_complete=False,min_value=-np.inf,max_value=np.inf,verbose=0,random_state=None,add_indicator=False,keep_empty_features=False)
sklearn.impute._iterative.IterativeImputer._get_abs_corr_mat(self,X_filled,tolerance=1e-06)
sklearn.impute._iterative.IterativeImputer._get_neighbor_feat_idx(self,n_features,feat_idx,abs_corr_mat)
sklearn.impute._iterative.IterativeImputer._get_ordered_idx(self,mask_missing_values)
sklearn.impute._iterative.IterativeImputer._impute_one_feature(self,X_filled,mask_missing_values,feat_idx,neighbor_feat_idx,estimator=None,fit_mode=True)
sklearn.impute._iterative.IterativeImputer._initial_imputation(self,X,in_fit=False)
sklearn.impute._iterative.IterativeImputer._validate_limit(limit,limit_type,n_features)
sklearn.impute._iterative.IterativeImputer.fit(self,X,y=None)
sklearn.impute._iterative.IterativeImputer.fit_transform(self,X,y=None)
sklearn.impute._iterative.IterativeImputer.get_feature_names_out(self,input_features=None)
sklearn.impute._iterative.IterativeImputer.transform(self,X)
sklearn.impute._iterative._assign_where(X1,X2,cond)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/_knn.py----------------------------------------
A:sklearn.impute._knn.weight_matrix->_get_weights(donors_dist, self.weights)
A:sklearn.impute._knn.donors->numpy.ma.array(donors, mask=donors_mask)
A:sklearn.impute._knn.donors_mask->mask_fit_X_col.take(donors_idx)
A:sklearn.impute._knn.X->self._validate_data(X, accept_sparse=False, dtype=FLOAT_DTYPES, force_all_finite=force_all_finite, copy=self.copy, reset=False)
A:sklearn.impute._knn.self._mask_fit_X->_get_mask(self._fit_X, self.missing_values)
A:sklearn.impute._knn.mask->_get_mask(X, self.missing_values)
A:sklearn.impute._knn.X_indicator->super()._transform_indicator(mask)
A:sklearn.impute._knn.row_missing_idx->numpy.flatnonzero(mask.any(axis=1))
A:sklearn.impute._knn.non_missing_fix_X->numpy.logical_not(mask_fit_X)
A:sklearn.impute._knn.dist_idx_map->numpy.zeros(X.shape[0], dtype=int)
A:sklearn.impute._knn.dist_idx_map[row_missing_idx]->numpy.arange(row_missing_idx.shape[0])
A:sklearn.impute._knn.(potential_donors_idx,)->numpy.nonzero(non_missing_fix_X[:, col])
A:sklearn.impute._knn.all_nan_dist_mask->numpy.isnan(dist_subset).all(axis=1)
A:sklearn.impute._knn.col_mean->numpy.ma.array(self._fit_X[:, col], mask=mask_fit_X[:, col]).mean()
A:sklearn.impute._knn.n_neighbors->min(self.n_neighbors, len(potential_donors_idx))
A:sklearn.impute._knn.value->self._calc_impute(dist_subset, n_neighbors, self._fit_X[potential_donors_idx, col], mask_fit_X[potential_donors_idx, col])
A:sklearn.impute._knn.gen->pairwise_distances_chunked(X[row_missing_idx, :], self._fit_X, metric=self.metric, missing_values=self.missing_values, force_all_finite=force_all_finite, reduce_func=process_chunk)
A:sklearn.impute._knn.input_features->_check_feature_names_in(self, input_features)
sklearn.impute.KNNImputer(self,*,missing_values=np.nan,n_neighbors=5,weights='uniform',metric='nan_euclidean',copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute.KNNImputer._calc_impute(self,dist_pot_donors,n_neighbors,fit_X_col,mask_fit_X_col)
sklearn.impute.KNNImputer.fit(self,X,y=None)
sklearn.impute.KNNImputer.get_feature_names_out(self,input_features=None)
sklearn.impute.KNNImputer.transform(self,X)
sklearn.impute._knn.KNNImputer(self,*,missing_values=np.nan,n_neighbors=5,weights='uniform',metric='nan_euclidean',copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute._knn.KNNImputer.__init__(self,*,missing_values=np.nan,n_neighbors=5,weights='uniform',metric='nan_euclidean',copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute._knn.KNNImputer._calc_impute(self,dist_pot_donors,n_neighbors,fit_X_col,mask_fit_X_col)
sklearn.impute._knn.KNNImputer.fit(self,X,y=None)
sklearn.impute._knn.KNNImputer.get_feature_names_out(self,input_features=None)
sklearn.impute._knn.KNNImputer.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/_base.py----------------------------------------
A:sklearn.impute._base.counter->Counter(array)
A:sklearn.impute._base.most_frequent_value->min((value for (value, count) in counter.items() if count == most_frequent_count))
A:sklearn.impute._base.mode->_mode(array)
A:sklearn.impute._base.self.indicator_->MissingIndicator(missing_values=self.missing_values, error_on_new=False)
A:sklearn.impute._base.hstack->partial(sp.hstack, format=X_imputed.format)
A:sklearn.impute._base.indicator_names->self.indicator_.get_feature_names_out(input_features)
A:sklearn.impute._base.X->self._validate_input(X, in_fit=False)
A:sklearn.impute._base.new_ve->ValueError('Cannot use {} strategy with non-numeric data:\n{}'.format(self.strategy, ve))
A:sklearn.impute._base.self.statistics_->self._dense_fit(X, self.strategy, self.missing_values, fill_value)
A:sklearn.impute._base.missing_mask->X[:, non_empty_feature_count:].astype(bool)
A:sklearn.impute._base.statistics->numpy.empty(X.shape[1])
A:sklearn.impute._base.mask_zeros->_get_mask(column, 0)
A:sklearn.impute._base.n_explicit_zeros->_get_mask(column, 0).sum()
A:sklearn.impute._base.statistics[i]->_most_frequent(column, 0, n_zeros)
A:sklearn.impute._base.masked_X->numpy.ma.masked_array(X, mask=missing_mask)
A:sklearn.impute._base.mean_masked->numpy.ma.mean(masked_X, axis=0)
A:sklearn.impute._base.mean->numpy.ma.getdata(mean_masked)
A:sklearn.impute._base.median_masked->numpy.ma.median(masked_X, axis=0)
A:sklearn.impute._base.median->numpy.ma.getdata(median_masked)
A:sklearn.impute._base.mask->_get_mask(X.data, self.missing_values)
A:sklearn.impute._base.most_frequent->numpy.empty(X.shape[0])
A:sklearn.impute._base.row_mask->numpy.logical_not(row_mask).astype(bool)
A:sklearn.impute._base.most_frequent[i]->_most_frequent(row, np.nan, 0)
A:sklearn.impute._base.invalid_mask->_get_mask(statistics, np.nan)
A:sklearn.impute._base.valid_mask->numpy.logical_not(invalid_mask)
A:sklearn.impute._base.valid_statistics_indexes->numpy.flatnonzero(valid_mask)
A:sklearn.impute._base.X.data[mask]->valid_statistics[indexes].astype(X.dtype, copy=False)
A:sklearn.impute._base.n_missing->self._fit(X, y).sum(axis=0)
A:sklearn.impute._base.values->numpy.repeat(valid_statistics, n_missing)
A:sklearn.impute._base.X_indicator->super()._transform_indicator(missing_mask)
A:sklearn.impute._base.n_features_missing->len(self.indicator_.features_)
A:sklearn.impute._base.array_imputed->X[:, :non_empty_feature_count].copy()
A:sklearn.impute._base.n_features_original->len(self.statistics_)
A:sklearn.impute._base.X_original->numpy.zeros(shape_original)
A:sklearn.impute._base.full_mask->numpy.zeros(shape_original).astype(bool)
A:sklearn.impute._base.input_features->_check_feature_names_in(self, input_features)
A:sklearn.impute._base.non_missing_mask->numpy.logical_not(_get_mask(self.statistics_, np.nan))
A:sklearn.impute._base.imputer_mask->self._fit(X, y)
A:sklearn.impute._base.features_indices->numpy.flatnonzero(n_missing)
A:sklearn.impute._base.missing_features_info->self._get_missing_features_info(X)
A:sklearn.impute._base.(imputer_mask, features)->self._get_missing_features_info(X)
A:sklearn.impute._base.features_diff_fit_trans->numpy.setdiff1d(features, self.features_)
A:sklearn.impute._base.prefix->self.__class__.__name__.lower()
sklearn.impute.MissingIndicator(self,*,missing_values=np.nan,features='missing-only',sparse='auto',error_on_new=True)
sklearn.impute.MissingIndicator._fit(self,X,y=None,precomputed=False)
sklearn.impute.MissingIndicator._get_missing_features_info(self,X)
sklearn.impute.MissingIndicator._more_tags(self)
sklearn.impute.MissingIndicator._validate_input(self,X,in_fit)
sklearn.impute.MissingIndicator.fit(self,X,y=None)
sklearn.impute.MissingIndicator.fit_transform(self,X,y=None)
sklearn.impute.MissingIndicator.get_feature_names_out(self,input_features=None)
sklearn.impute.MissingIndicator.transform(self,X)
sklearn.impute.SimpleImputer(self,*,missing_values=np.nan,strategy='mean',fill_value=None,copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute.SimpleImputer._dense_fit(self,X,strategy,missing_values,fill_value)
sklearn.impute.SimpleImputer._more_tags(self)
sklearn.impute.SimpleImputer._sparse_fit(self,X,strategy,missing_values,fill_value)
sklearn.impute.SimpleImputer._validate_input(self,X,in_fit)
sklearn.impute.SimpleImputer.fit(self,X,y=None)
sklearn.impute.SimpleImputer.get_feature_names_out(self,input_features=None)
sklearn.impute.SimpleImputer.inverse_transform(self,X)
sklearn.impute.SimpleImputer.transform(self,X)
sklearn.impute._base.MissingIndicator(self,*,missing_values=np.nan,features='missing-only',sparse='auto',error_on_new=True)
sklearn.impute._base.MissingIndicator.__init__(self,*,missing_values=np.nan,features='missing-only',sparse='auto',error_on_new=True)
sklearn.impute._base.MissingIndicator._fit(self,X,y=None,precomputed=False)
sklearn.impute._base.MissingIndicator._get_missing_features_info(self,X)
sklearn.impute._base.MissingIndicator._more_tags(self)
sklearn.impute._base.MissingIndicator._validate_input(self,X,in_fit)
sklearn.impute._base.MissingIndicator.fit(self,X,y=None)
sklearn.impute._base.MissingIndicator.fit_transform(self,X,y=None)
sklearn.impute._base.MissingIndicator.get_feature_names_out(self,input_features=None)
sklearn.impute._base.MissingIndicator.transform(self,X)
sklearn.impute._base.SimpleImputer(self,*,missing_values=np.nan,strategy='mean',fill_value=None,copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute._base.SimpleImputer.__init__(self,*,missing_values=np.nan,strategy='mean',fill_value=None,copy=True,add_indicator=False,keep_empty_features=False)
sklearn.impute._base.SimpleImputer._dense_fit(self,X,strategy,missing_values,fill_value)
sklearn.impute._base.SimpleImputer._more_tags(self)
sklearn.impute._base.SimpleImputer._sparse_fit(self,X,strategy,missing_values,fill_value)
sklearn.impute._base.SimpleImputer._validate_input(self,X,in_fit)
sklearn.impute._base.SimpleImputer.fit(self,X,y=None)
sklearn.impute._base.SimpleImputer.get_feature_names_out(self,input_features=None)
sklearn.impute._base.SimpleImputer.inverse_transform(self,X)
sklearn.impute._base.SimpleImputer.transform(self,X)
sklearn.impute._base._BaseImputer(self,*,missing_values=np.nan,add_indicator=False,keep_empty_features=False)
sklearn.impute._base._BaseImputer.__init__(self,*,missing_values=np.nan,add_indicator=False,keep_empty_features=False)
sklearn.impute._base._BaseImputer._concatenate_indicator(self,X_imputed,X_indicator)
sklearn.impute._base._BaseImputer._concatenate_indicator_feature_names_out(self,names,input_features)
sklearn.impute._base._BaseImputer._fit_indicator(self,X)
sklearn.impute._base._BaseImputer._more_tags(self)
sklearn.impute._base._BaseImputer._transform_indicator(self,X)
sklearn.impute._base._check_inputs_dtype(X,missing_values)
sklearn.impute._base._most_frequent(array,extra_value,n_repeat)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/tests/test_base.py----------------------------------------
A:sklearn.impute.tests.test_base.X->numpy.random.randn(10, 2)
A:sklearn.impute.tests.test_base.mask->numpy.random.RandomState(0).randint(0, 2, size=(n_samples, n_features)).astype(bool)
A:sklearn.impute.tests.test_base.imputer->NoPrecomputedMaskTransform(add_indicator=True)
A:sklearn.impute.tests.test_base.rng->numpy.random.RandomState(0)
A:sklearn.impute.tests.test_base.X1->X1.to_numpy().to_numpy()
A:sklearn.impute.tests.test_base.X2->numpy.random.RandomState(0).randn(n_samples, n_features)
sklearn.impute.tests.test_base.NoFitIndicatorImputer(_BaseImputer)
sklearn.impute.tests.test_base.NoFitIndicatorImputer.fit(self,X,y=None)
sklearn.impute.tests.test_base.NoFitIndicatorImputer.transform(self,X,y=None)
sklearn.impute.tests.test_base.NoPrecomputedMaskFit(_BaseImputer)
sklearn.impute.tests.test_base.NoPrecomputedMaskFit.fit(self,X,y=None)
sklearn.impute.tests.test_base.NoPrecomputedMaskFit.transform(self,X)
sklearn.impute.tests.test_base.NoPrecomputedMaskTransform(_BaseImputer)
sklearn.impute.tests.test_base.NoPrecomputedMaskTransform.fit(self,X,y=None)
sklearn.impute.tests.test_base.NoPrecomputedMaskTransform.transform(self,X)
sklearn.impute.tests.test_base.NoTransformIndicatorImputer(_BaseImputer)
sklearn.impute.tests.test_base.NoTransformIndicatorImputer.fit(self,X,y=None)
sklearn.impute.tests.test_base.NoTransformIndicatorImputer.transform(self,X,y=None)
sklearn.impute.tests.test_base.data()
sklearn.impute.tests.test_base.test_assign_where(X1_type)
sklearn.impute.tests.test_base.test_base_imputer_not_fit(data)
sklearn.impute.tests.test_base.test_base_imputer_not_transform(data)
sklearn.impute.tests.test_base.test_base_no_precomputed_mask_fit(data)
sklearn.impute.tests.test_base.test_base_no_precomputed_mask_transform(data)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/tests/test_knn.py----------------------------------------
A:sklearn.impute.tests.test_knn.X->numpy.array([[3, na], [2, na], [na, 4], [5, 6], [6, 8], [na, 5]])
A:sklearn.impute.tests.test_knn.imputer->KNNImputer(n_neighbors=2, metric=custom_callable)
A:sklearn.impute.tests.test_knn.X_imputed->numpy.array([[0, r0c1, 0, r0c3], [1, 1, 1, r1c3], [2, 2, r2c2, 2], [3, 3, 3, 3], [4, 4, 4, 4], [5, 5, 5, 5], [6, 6, 6, 6], [r7c0, 7, 7, 7]])
A:sklearn.impute.tests.test_knn.X_fit->numpy.array([[0, 1, 1, 2, na], [2, 1, 2, 2, 3], [3, 2, 3, 3, 8], [na, 6, 0, 5, 13], [na, 7, 0, 7, 8], [6, 6, 2, 5, 7]])
A:sklearn.impute.tests.test_knn.knn->KNNImputer(missing_values=na)
A:sklearn.impute.tests.test_knn.X_transform->KNNImputer(missing_values=na).transform(X_test)
A:sklearn.impute.tests.test_knn.X_test->numpy.arange(0, 12).reshape(2, 6)
A:sklearn.impute.tests.test_knn.X_zero->numpy.array([[1, 0, 1, 1, 1.0], [2, 2, 2, 2, 2], [3, 3, 3, 3, 0], [6, 6, 0, 6, 6]])
A:sklearn.impute.tests.test_knn.X_nan->numpy.array([[1, na, 1, 1, 1.0], [2, 2, 2, 2, 2], [3, 3, 3, 3, na], [6, 6, na, 6, 6]])
A:sklearn.impute.tests.test_knn.imputer_zero->KNNImputer(missing_values=0, n_neighbors=2, weights='uniform')
A:sklearn.impute.tests.test_knn.imputer_nan->KNNImputer(missing_values=na, n_neighbors=2, weights='uniform')
A:sklearn.impute.tests.test_knn.X1->numpy.array([[na, 1], [na, 2]])
A:sklearn.impute.tests.test_knn.X1_imputed->numpy.array([[3, 11], [3, 1], [3, 6]])
A:sklearn.impute.tests.test_knn.imputer_plus1->KNNImputer(n_neighbors=n_neighbors, missing_values=na)
A:sklearn.impute.tests.test_knn.X_imputed_uniform->numpy.array([[0, 0], [5, 2], [4, 3], [5, 6], [7, 7], [9, 8], [11, 10]])
A:sklearn.impute.tests.test_knn.nn->KNeighborsRegressor(metric='euclidean', weights='distance')
A:sklearn.impute.tests.test_knn.dist->pairwise_distances(X, metric='nan_euclidean', squared=False, missing_values=na)
A:sklearn.impute.tests.test_knn.manual_imputed_value->numpy.average(X[X_neighbors_idx, 0], weights=weights)
A:sklearn.impute.tests.test_knn.X_imputed_distance1->numpy.array([[0, 0], [manual_imputed_value, 2], [4, 3], [5, 6], [7, 7], [9, 8], [11, 10]])
A:sklearn.impute.tests.test_knn.X_imputed_distance2->numpy.array([[0, 0], [knn_imputed_value, 2], [4, 3], [5, 6], [7, 7], [9, 8], [11, 10]])
A:sklearn.impute.tests.test_knn.dist_0_1->numpy.sqrt(3 / 2 * ((1 - 0) ** 2 + (2 - 0) ** 2))
A:sklearn.impute.tests.test_knn.dist_0_2->numpy.sqrt(3 / 2 * ((2 - 0) ** 2 + (3 - 0) ** 2))
A:sklearn.impute.tests.test_knn.imputed_value->numpy.average([2, 3], weights=[1 / dist_0_1, 1 / dist_0_2])
A:sklearn.impute.tests.test_knn.col1_donor_values->numpy.ma.masked_invalid(X[[0, 2, 3, 4, 5], 1]).copy()
A:sklearn.impute.tests.test_knn.col3_donor_values->numpy.ma.masked_invalid(X[[0, 3, 4, 5, 6], 3]).copy()
A:sklearn.impute.tests.test_knn.r1c1_imp->numpy.ma.average(col1_donor_values, weights=r1c1_nbor_wt)
A:sklearn.impute.tests.test_knn.r1c3_imp->numpy.ma.average(col3_donor_values, weights=r1c3_nbor_wt)
A:sklearn.impute.tests.test_knn.r2c3_imp->numpy.ma.average(col3_donor_values, weights=r2c3_nbor_wt)
A:sklearn.impute.tests.test_knn.r0c3->numpy.mean(X[2:-1, -1])
A:sklearn.impute.tests.test_knn.r1c3->numpy.mean(X[2:-1, -1])
A:sklearn.impute.tests.test_knn.r2c2->numpy.mean(X[[0, 1, 3, 4, 5], 2])
A:sklearn.impute.tests.test_knn.r7c0->numpy.mean(X[2:-1, 0])
A:sklearn.impute.tests.test_knn.imputer_comp_wt->KNNImputer(missing_values=na, weights='distance')
A:sklearn.impute.tests.test_knn.x->numpy.ma.array(x, mask=np.isnan(x))
A:sklearn.impute.tests.test_knn.y->numpy.ma.array(y, mask=np.isnan(y))
A:sklearn.impute.tests.test_knn.r0c1->numpy.mean(X[1:6, 1])
A:sklearn.impute.tests.test_knn.imputer_comp->KNNImputer(missing_values=na)
A:sklearn.impute.tests.test_knn.X2->numpy.array([[1, 2], [3, na]])
A:sklearn.impute.tests.test_knn.X2_imputed->numpy.array([[4, 6]])
A:sklearn.impute.tests.test_knn.X1_expected->numpy.array([[1], [2]])
A:sklearn.impute.tests.test_knn.X2_expected->numpy.array([[2], [1.5]])
A:sklearn.impute.tests.test_knn.X_01->numpy.average(X[3:5, 1], weights=1 / dist[0, 3:5])
A:sklearn.impute.tests.test_knn.X_11->numpy.average(X[3:5, 1], weights=1 / dist[1, 3:5])
A:sklearn.impute.tests.test_knn.X_20->numpy.average(X[3:5, 0], weights=1 / dist[2, 3:5])
A:sklearn.impute.tests.test_knn.X_50->numpy.average(X[3:5, 0], weights=1 / dist[5, 3:5])
A:sklearn.impute.tests.test_knn.X_expected->numpy.array([[3, X_01], [2, X_11], [X_20, 4], [5, 6], [6, 8], [X_50, 5]])
A:sklearn.impute.tests.test_knn.knn_3->KNNImputer(missing_values=na, n_neighbors=3, weights='distance')
A:sklearn.impute.tests.test_knn.knn_4->KNNImputer(missing_values=na, n_neighbors=4, weights='distance')
sklearn.impute.tests.test_knn.test_knn_imputer_all_samples_are_neighbors(na)
sklearn.impute.tests.test_knn.test_knn_imputer_callable_metric()
sklearn.impute.tests.test_knn.test_knn_imputer_default_with_invalid_input(na)
sklearn.impute.tests.test_knn.test_knn_imputer_distance_weighted_not_enough_neighbors(na,working_memory)
sklearn.impute.tests.test_knn.test_knn_imputer_drops_all_nan_features(na)
sklearn.impute.tests.test_knn.test_knn_imputer_not_enough_valid_distances(na,weights)
sklearn.impute.tests.test_knn.test_knn_imputer_one_n_neighbors(na)
sklearn.impute.tests.test_knn.test_knn_imputer_removes_all_na_features(na)
sklearn.impute.tests.test_knn.test_knn_imputer_shape(weights,n_neighbors)
sklearn.impute.tests.test_knn.test_knn_imputer_verify(na)
sklearn.impute.tests.test_knn.test_knn_imputer_weight_distance(na)
sklearn.impute.tests.test_knn.test_knn_imputer_weight_uniform(na)
sklearn.impute.tests.test_knn.test_knn_imputer_with_simple_example(na,working_memory)
sklearn.impute.tests.test_knn.test_knn_imputer_zero_nan_imputes_the_same(na)
sklearn.impute.tests.test_knn.test_knn_tags(na,allow_nan)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/tests/test_impute.py----------------------------------------
A:sklearn.impute.tests.test_impute.imputer->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features)
A:sklearn.impute.tests.test_impute.X_trans->SimpleImputer().fit(X).transform(X_test)
A:sklearn.impute.tests.test_impute.X->_convert_container(X, array_type)
A:sklearn.impute.tests.test_impute.X_imputed->getattr(imputer, method)(X)
A:sklearn.impute.tests.test_impute.iterative_imputer->IterativeImputer(initial_strategy=strategy)
A:sklearn.impute.tests.test_impute.pd->pytest.importorskip('pandas')
A:sklearn.impute.tests.test_impute.feature_names->MissingIndicator(missing_values=missing_values).fit(X).get_feature_names_out()
A:sklearn.impute.tests.test_impute.rng->numpy.random.RandomState(42)
A:sklearn.impute.tests.test_impute.zeros->numpy.zeros(shape[0])
A:sklearn.impute.tests.test_impute.values->numpy.arange(1, shape[0] + 1)
A:sklearn.impute.tests.test_impute.X_true->numpy.array([['a', 'b'], ['c', expected]], dtype=object)
A:sklearn.impute.tests.test_impute.true_statistics->numpy.empty(shape[1])
A:sklearn.impute.tests.test_impute.nb_missing_values->max(shape[0] + dec * dec - (j + dec) * (j + dec), 0)
A:sklearn.impute.tests.test_impute.p->numpy.repeat(test_missing_values, nb_missing_values)
A:sklearn.impute.tests.test_impute.true_statistics[j]->true_value_fun(z, v, p)
A:sklearn.impute.tests.test_impute.X[:, j]->numpy.hstack((v, z, p))
A:sklearn.impute.tests.test_impute.X_true[:, j]->numpy.hstack((v, z, np.repeat(true_statistics[j], nb_missing_values)))
A:sklearn.impute.tests.test_impute.X_imputed_median->numpy.array([[0, 0, 0], [5, 5, 5], [0, 0, 0], [-5, 0, -2.5], [0, 5, 2.5], [4, 5, 4.5], [-4, -5, -4.5], [-1, 2, 0.5]]).transpose()
A:sklearn.impute.tests.test_impute.f->io.StringIO('Cat1,Cat2,Cat3,Cat4\n,i,x,\na,,y,\na,j,,\nb,j,x,')
A:sklearn.impute.tests.test_impute.df->pytest.importorskip('pandas').DataFrame({'feature': pd.Series([1.0, None, 2.0, 3.0], dtype='float64')})
A:sklearn.impute.tests.test_impute.pipeline->Pipeline([('imputer', SimpleImputer(missing_values=missing_values)), ('tree', tree.DecisionTreeRegressor(random_state=0))])
A:sklearn.impute.tests.test_impute.Y->_sparse_random_matrix(100, 1, density=0.1).toarray()
A:sklearn.impute.tests.test_impute.gs->GridSearchCV(pipeline, parameters)
A:sklearn.impute.tests.test_impute.X_orig->_sparse_random_matrix(5, 5, density=0.75, random_state=0)
A:sklearn.impute.tests.test_impute.Xt->MissingIndicator(features='all', missing_values=1).fit_transform(X)
A:sklearn.impute.tests.test_impute.imputations->numpy.array([imputer.transform(X)[0][0] for _ in range(100)])
A:sklearn.impute.tests.test_impute.(ks_statistic, p_value)->kstest((imputations - mu) / sigma, 'norm')
A:sklearn.impute.tests.test_impute.X_train->numpy.zeros((10, 3))
A:sklearn.impute.tests.test_impute.X_test->numpy.asarray([[np.nan, np.nan, np.nan]], dtype=dtype_test)
A:sklearn.impute.tests.test_impute.initial_imputer->SimpleImputer(missing_values=0, strategy=strategy).fit(X_train)
A:sklearn.impute.tests.test_impute.rng1->numpy.random.RandomState(0)
A:sklearn.impute.tests.test_impute.rng2->numpy.random.RandomState(1)
A:sklearn.impute.tests.test_impute.X_fitted_1->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).transform(X)
A:sklearn.impute.tests.test_impute.X_fitted_2->IterativeImputer(min_value=min_max_2[0], max_value=min_max_2[1], random_state=0).transform(X)
A:sklearn.impute.tests.test_impute.imputer1->IterativeImputer(min_value=min_max_1[0], max_value=min_max_1[1], random_state=0)
A:sklearn.impute.tests.test_impute.imputer2->IterativeImputer(min_value=min_max_2[0], max_value=min_max_2[1], random_state=0)
A:sklearn.impute.tests.test_impute.X_fitted_1a->IterativeImputer(min_value=min_max_1[0], max_value=min_max_1[1], random_state=0).transform(X)
A:sklearn.impute.tests.test_impute.X_fitted_1b->IterativeImputer(min_value=min_max_1[0], max_value=min_max_1[1], random_state=0).transform(X)
A:sklearn.impute.tests.test_impute.m1->IterativeImputer(max_iter=10, random_state=rng)
A:sklearn.impute.tests.test_impute.m2->IterativeImputer(max_iter=10, random_state=rng)
A:sklearn.impute.tests.test_impute.pred1->IterativeImputer(max_iter=10, random_state=rng).fit(X).transform(X)
A:sklearn.impute.tests.test_impute.pred2->IterativeImputer(max_iter=10, random_state=rng).fit_transform(X)
A:sklearn.impute.tests.test_impute.A->numpy.random.RandomState(42).rand(n, 1)
A:sklearn.impute.tests.test_impute.B->numpy.random.RandomState(42).rand(1, d)
A:sklearn.impute.tests.test_impute.X_missing->_convert_container(X, array_type).copy()
A:sklearn.impute.tests.test_impute.X_filled->numpy.zeros(A.shape)
A:sklearn.impute.tests.test_impute.X_test_est->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).fit(X_train).transform(X_test)
A:sklearn.impute.tests.test_impute.X_filled_100->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).fit_transform(X_missing)
A:sklearn.impute.tests.test_impute.X_filled_early->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).fit_transform(X_missing)
A:sklearn.impute.tests.test_impute.(X, y)->load_diabetes(return_X_y=True)
A:sklearn.impute.tests.test_impute.sample_idx->numpy.random.RandomState(42).choice(np.arange(n_samples), size=int(n_samples * missing_rate), replace=False)
A:sklearn.impute.tests.test_impute.X_fill->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).fit_transform(X, y)
A:sklearn.impute.tests.test_impute.X_test_imputed1->IterativeImputer(min_value=min_max_1[0], max_value=min_max_1[1], random_state=0).fit(X_train).transform(X_test)
A:sklearn.impute.tests.test_impute.X_test_imputed2->IterativeImputer(min_value=min_max_2[0], max_value=min_max_2[1], random_state=0).fit(X_train).transform(X_test)
A:sklearn.impute.tests.test_impute.estimator->ZeroEstimator(random_state=rs_estimator)
A:sklearn.impute.tests.test_impute.indicator->MissingIndicator(missing_values=missing_values).fit(X)
A:sklearn.impute.tests.test_impute.X_fit->arr_type(X_fit).astype(np.float64)
A:sklearn.impute.tests.test_impute.X_fit_expected->X_fit_expected.astype(dtype).astype(dtype)
A:sklearn.impute.tests.test_impute.X_trans_expected->X_trans_expected.astype(dtype).astype(dtype)
A:sklearn.impute.tests.test_impute.X_fit_mask->MissingIndicator(missing_values=missing_values).fit(X).fit_transform(X_fit)
A:sklearn.impute.tests.test_impute.X_trans_mask->MissingIndicator(missing_values=missing_values).fit(X).transform(X_trans)
A:sklearn.impute.tests.test_impute.X_fit_mask_sparse->MissingIndicator(missing_values=missing_values).fit(X).fit_transform(X_fit)
A:sklearn.impute.tests.test_impute.X_trans_mask_sparse->MissingIndicator(missing_values=missing_values).fit(X).transform(X_trans)
A:sklearn.impute.tests.test_impute.X_fit_sparse->arr_type(X_fit)
A:sklearn.impute.tests.test_impute.X_trans_sparse->arr_type(X_trans)
A:sklearn.impute.tests.test_impute.trans->make_union(SimpleImputer(missing_values=missing_values, strategy='most_frequent'), MissingIndicator(missing_values=missing_values))
A:sklearn.impute.tests.test_impute.mi->MissingIndicator(features='all', missing_values=1)
A:sklearn.impute.tests.test_impute.X_sparse->arr_type([[np.nan, 1, 5], [2, np.nan, 1], [6, 3, np.nan], [1, 2, 9]])
A:sklearn.impute.tests.test_impute.trs->IterativeImputer(max_iter=1, imputation_order=order, random_state=0).fit(X)
A:sklearn.impute.tests.test_impute.X_1->numpy.array([[9, missing_value, 3, -1], [4, -1, 5, 4], [6, 7, missing_value, -1], [8, 9, 0, missing_value]])
A:sklearn.impute.tests.test_impute.X_2->numpy.array([[5, 4, 2, 1], [2, 1, missing_value, 3], [9, missing_value, 7, 1], [6, 4, 2, missing_value]])
A:sklearn.impute.tests.test_impute.X_3->numpy.array([[1, missing_value, 5, 9], [missing_value, 4, missing_value, missing_value], [2, missing_value, 7, missing_value], [missing_value, 3, missing_value, 8]])
A:sklearn.impute.tests.test_impute.X_4->numpy.array([[1, 1, 1, 3], [missing_value, 2, missing_value, 1], [2, 3, 3, 4], [missing_value, 4, missing_value, 2]])
A:sklearn.impute.tests.test_impute.X_1_trans->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).fit_transform(X_1)
A:sklearn.impute.tests.test_impute.X_1_inv_trans->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).inverse_transform(X_1_trans)
A:sklearn.impute.tests.test_impute.X_2_trans->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).transform(X_2)
A:sklearn.impute.tests.test_impute.X_2_inv_trans->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).inverse_transform(X_2_trans)
A:sklearn.impute.tests.test_impute.X_inv_trans->SimpleImputer(strategy=strategy, keep_empty_features=keep_empty_features).inverse_transform(X_trans)
A:sklearn.impute.tests.test_impute.imp_frequent->SimpleImputer(strategy='most_frequent').fit(X)
A:sklearn.impute.tests.test_impute.imp->SimpleImputer().fit(X)
sklearn.impute.tests.test_impute._assert_allclose_and_same_dtype(x,y)
sklearn.impute.tests.test_impute._assert_array_equal_and_same_dtype(x,y)
sklearn.impute.tests.test_impute._check_statistics(X,X_true,strategy,statistics,missing_values,sparse_container)
sklearn.impute.tests.test_impute._generate_missing_indicator_cases()
sklearn.impute.tests.test_impute.safe_mean(arr,*args,**kwargs)
sklearn.impute.tests.test_impute.safe_median(arr,*args,**kwargs)
sklearn.impute.tests.test_impute.test_imputation_const_mostf_error_invalid_types(strategy,dtype)
sklearn.impute.tests.test_impute.test_imputation_constant_error_invalid_type(X_data,missing_value)
sklearn.impute.tests.test_impute.test_imputation_constant_float(array_constructor)
sklearn.impute.tests.test_impute.test_imputation_constant_integer()
sklearn.impute.tests.test_impute.test_imputation_constant_object(marker)
sklearn.impute.tests.test_impute.test_imputation_constant_pandas(dtype)
sklearn.impute.tests.test_impute.test_imputation_copy()
sklearn.impute.tests.test_impute.test_imputation_deletion_warning(strategy)
sklearn.impute.tests.test_impute.test_imputation_deletion_warning_feature_names(strategy)
sklearn.impute.tests.test_impute.test_imputation_error_sparse_0(strategy,csc_container)
sklearn.impute.tests.test_impute.test_imputation_mean_median(csc_container)
sklearn.impute.tests.test_impute.test_imputation_mean_median_error_invalid_type(strategy,dtype)
sklearn.impute.tests.test_impute.test_imputation_mean_median_error_invalid_type_list_pandas(strategy,type)
sklearn.impute.tests.test_impute.test_imputation_median_special_cases(csc_container)
sklearn.impute.tests.test_impute.test_imputation_most_frequent(csc_container)
sklearn.impute.tests.test_impute.test_imputation_most_frequent_objects(marker)
sklearn.impute.tests.test_impute.test_imputation_most_frequent_pandas(dtype)
sklearn.impute.tests.test_impute.test_imputation_order(order,idx_order)
sklearn.impute.tests.test_impute.test_imputation_pipeline_grid_search()
sklearn.impute.tests.test_impute.test_imputation_shape(strategy,csr_container)
sklearn.impute.tests.test_impute.test_imputer_lists_fit_transform()
sklearn.impute.tests.test_impute.test_imputer_transform_preserves_numeric_dtype(dtype_test)
sklearn.impute.tests.test_impute.test_imputer_without_indicator(imputer_constructor)
sklearn.impute.tests.test_impute.test_inconsistent_dtype_X_missing_values(imputer_constructor,imputer_missing_values,missing_value,err_msg)
sklearn.impute.tests.test_impute.test_iterative_imputer_additive_matrix()
sklearn.impute.tests.test_impute.test_iterative_imputer_all_missing()
sklearn.impute.tests.test_impute.test_iterative_imputer_catch_min_max_error(min_value,max_value,err_msg)
sklearn.impute.tests.test_impute.test_iterative_imputer_catch_warning()
sklearn.impute.tests.test_impute.test_iterative_imputer_clip()
sklearn.impute.tests.test_impute.test_iterative_imputer_clip_truncnorm()
sklearn.impute.tests.test_impute.test_iterative_imputer_constant_fill_value()
sklearn.impute.tests.test_impute.test_iterative_imputer_dont_set_random_state(rs_imputer,rs_estimator)
sklearn.impute.tests.test_impute.test_iterative_imputer_early_stopping()
sklearn.impute.tests.test_impute.test_iterative_imputer_estimators(estimator)
sklearn.impute.tests.test_impute.test_iterative_imputer_imputation_order(imputation_order)
sklearn.impute.tests.test_impute.test_iterative_imputer_keep_empty_features(initial_strategy)
sklearn.impute.tests.test_impute.test_iterative_imputer_min_max_array_like(min_value,max_value,correct_output)
sklearn.impute.tests.test_impute.test_iterative_imputer_min_max_array_like_imputation(min_max_1,min_max_2)
sklearn.impute.tests.test_impute.test_iterative_imputer_missing_at_transform(strategy)
sklearn.impute.tests.test_impute.test_iterative_imputer_no_missing()
sklearn.impute.tests.test_impute.test_iterative_imputer_one_feature(X)
sklearn.impute.tests.test_impute.test_iterative_imputer_rank_one()
sklearn.impute.tests.test_impute.test_iterative_imputer_skip_non_missing(skip_complete)
sklearn.impute.tests.test_impute.test_iterative_imputer_transform_recovery(rank)
sklearn.impute.tests.test_impute.test_iterative_imputer_transform_stochasticity()
sklearn.impute.tests.test_impute.test_iterative_imputer_truncated_normal_posterior()
sklearn.impute.tests.test_impute.test_iterative_imputer_verbose()
sklearn.impute.tests.test_impute.test_iterative_imputer_zero_iters()
sklearn.impute.tests.test_impute.test_knn_imputer_keep_empty_features(keep_empty_features)
sklearn.impute.tests.test_impute.test_missing_indicator_error(X_fit,X_trans,params,msg_err)
sklearn.impute.tests.test_impute.test_missing_indicator_feature_names_out()
sklearn.impute.tests.test_impute.test_missing_indicator_new(missing_values,arr_type,dtype,param_features,n_features,features_indices)
sklearn.impute.tests.test_impute.test_missing_indicator_no_missing()
sklearn.impute.tests.test_impute.test_missing_indicator_raise_on_sparse_with_missing_0(arr_type)
sklearn.impute.tests.test_impute.test_missing_indicator_sparse_no_explicit_zeros(csr_container)
sklearn.impute.tests.test_impute.test_missing_indicator_sparse_param(arr_type,missing_values,param_sparse)
sklearn.impute.tests.test_impute.test_missing_indicator_string()
sklearn.impute.tests.test_impute.test_missing_indicator_with_imputer(X,missing_values,X_trans_exp)
sklearn.impute.tests.test_impute.test_most_frequent(expected,array,dtype,extra_value,n_repeat)
sklearn.impute.tests.test_impute.test_simple_imputation_add_indicator_sparse_matrix(arr_type)
sklearn.impute.tests.test_impute.test_simple_imputation_inverse_transform(missing_value)
sklearn.impute.tests.test_impute.test_simple_imputation_inverse_transform_exceptions(missing_value)
sklearn.impute.tests.test_impute.test_simple_imputation_string_list(strategy,expected)
sklearn.impute.tests.test_impute.test_simple_impute_pd_na()
sklearn.impute.tests.test_impute.test_simple_imputer_constant_keep_empty_features(array_type,keep_empty_features)
sklearn.impute.tests.test_impute.test_simple_imputer_keep_empty_features(strategy,array_type,keep_empty_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/impute/tests/test_common.py----------------------------------------
A:sklearn.impute.tests.test_common.X->numpy.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])
A:sklearn.impute.tests.test_common.X_true_indicator->csr_container([[1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 1.0], [0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0]])
A:sklearn.impute.tests.test_common.X_trans->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).fit_transform(X_df)
A:sklearn.impute.tests.test_common.X_trans_no_indicator->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).fit_transform(X)
A:sklearn.impute.tests.test_common.pd->pytest.importorskip('pandas')
A:sklearn.impute.tests.test_common.imputer->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).set_params(add_indicator=False, keep_empty_features=keep_empty_features)
A:sklearn.impute.tests.test_common.X_trans_expected->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).set_params(add_indicator=False, keep_empty_features=keep_empty_features).fit_transform(X)
A:sklearn.impute.tests.test_common.X_df->pytest.importorskip('pandas').DataFrame(X, columns=['a', 'b', 'c', 'd', 'e', 'f'])
A:sklearn.impute.tests.test_common.names->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).set_params(add_indicator=False, keep_empty_features=keep_empty_features).get_feature_names_out()
A:sklearn.impute.tests.test_common.X_imputed->getattr(imputer, method)(X)
A:sklearn.impute.tests.test_common.X_train->numpy.array([[0, np.nan], [1, 2]])
A:sklearn.impute.tests.test_common.X_test->numpy.array([[0, missing_value_test], [1, 2]])
A:sklearn.impute.tests.test_common.X_test_imputed_with_indicator->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).set_params(add_indicator=False, keep_empty_features=keep_empty_features).transform(X_test)
A:sklearn.impute.tests.test_common.X_test_imputed_without_indicator->imputer.set_params(add_indicator=False, keep_empty_features=keep_empty_features).set_params(add_indicator=False, keep_empty_features=keep_empty_features).transform(X_test)
sklearn.impute.tests.test_common.imputers()
sklearn.impute.tests.test_common.sparse_imputers()
sklearn.impute.tests.test_common.test_imputation_adds_missing_indicator_if_add_indicator_is_true(imputer,missing_value_test)
sklearn.impute.tests.test_common.test_imputation_missing_value_in_test_array(imputer)
sklearn.impute.tests.test_common.test_imputers_add_indicator(marker,imputer)
sklearn.impute.tests.test_common.test_imputers_add_indicator_sparse(imputer,marker,csr_container)
sklearn.impute.tests.test_common.test_imputers_feature_names_out_pandas(imputer,add_indicator)
sklearn.impute.tests.test_common.test_imputers_pandas_na_integer_array_support(imputer,add_indicator)
sklearn.impute.tests.test_common.test_keep_empty_features(imputer,keep_empty_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/loss.py----------------------------------------
A:sklearn._loss.loss.self.interval_y_true->Interval(0, 1, True, True)
A:sklearn._loss.loss.loss_out->numpy.empty_like(y_true, dtype=gradient_out.dtype)
A:sklearn._loss.loss.raw_prediction->raw_prediction.squeeze(1).squeeze(1)
A:sklearn._loss.loss.gradient_out->numpy.empty_like(proba_out)
A:sklearn._loss.loss.hessian_out->hessian_out.squeeze(1).squeeze(1)
A:sklearn._loss.loss.y_pred->numpy.average(y_true, weights=sample_weight, axis=0)
A:sklearn._loss.loss.gradient->numpy.empty(shape=shape, dtype=dtype, order=order)
A:sklearn._loss.loss.hessian->numpy.empty(shape=shape, dtype=dtype, order=order)
A:sklearn._loss.loss.median->_weighted_percentile(y_true, sample_weight, 50)
A:sklearn._loss.loss.self.interval_y_pred->Interval(0, 1, False, False)
A:sklearn._loss.loss.proba->numpy.empty((raw_prediction.shape[0], 2), dtype=raw_prediction.dtype)
A:sklearn._loss.loss.proba[:, 1]->self.link.inverse(raw_prediction)
A:sklearn._loss.loss.out->numpy.zeros(self.n_classes, dtype=y_true.dtype)
A:sklearn._loss.loss.out[k]->numpy.clip(out[k], eps, 1 - eps)
A:sklearn._loss.loss.proba_out->numpy.empty_like(gradient_out)
sklearn._loss.AbsoluteError(self,sample_weight=None)
sklearn._loss.AbsoluteError.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.HalfBinomialLoss(self,sample_weight=None)
sklearn._loss.HalfBinomialLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.HalfBinomialLoss.predict_proba(self,raw_prediction)
sklearn._loss.HalfGammaLoss(self,sample_weight=None)
sklearn._loss.HalfGammaLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.HalfMultinomialLoss(self,sample_weight=None,n_classes=3)
sklearn._loss.HalfMultinomialLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.HalfMultinomialLoss.gradient_proba(self,y_true,raw_prediction,sample_weight=None,gradient_out=None,proba_out=None,n_threads=1)
sklearn._loss.HalfMultinomialLoss.in_y_true_range(self,y)
sklearn._loss.HalfMultinomialLoss.predict_proba(self,raw_prediction)
sklearn._loss.HalfPoissonLoss(self,sample_weight=None)
sklearn._loss.HalfPoissonLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.HalfSquaredError(self,sample_weight=None)
sklearn._loss.HalfTweedieLoss(self,sample_weight=None,power=1.5)
sklearn._loss.HalfTweedieLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.HalfTweedieLossIdentity(self,sample_weight=None,power=1.5)
sklearn._loss.HuberLoss(self,sample_weight=None,quantile=0.9,delta=0.5)
sklearn._loss.HuberLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.PinballLoss(self,sample_weight=None,quantile=0.5)
sklearn._loss.PinballLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.loss.AbsoluteError(self,sample_weight=None)
sklearn._loss.loss.AbsoluteError.__init__(self,sample_weight=None)
sklearn._loss.loss.AbsoluteError.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.loss.BaseLoss(self,closs,link,n_classes=None)
sklearn._loss.loss.BaseLoss.__init__(self,closs,link,n_classes=None)
sklearn._loss.loss.BaseLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.BaseLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.loss.BaseLoss.gradient(self,y_true,raw_prediction,sample_weight=None,gradient_out=None,n_threads=1)
sklearn._loss.loss.BaseLoss.gradient_hessian(self,y_true,raw_prediction,sample_weight=None,gradient_out=None,hessian_out=None,n_threads=1)
sklearn._loss.loss.BaseLoss.in_y_pred_range(self,y)
sklearn._loss.loss.BaseLoss.in_y_true_range(self,y)
sklearn._loss.loss.BaseLoss.init_gradient_and_hessian(self,n_samples,dtype=np.float64,order='F')
sklearn._loss.loss.BaseLoss.loss(self,y_true,raw_prediction,sample_weight=None,loss_out=None,n_threads=1)
sklearn._loss.loss.BaseLoss.loss_gradient(self,y_true,raw_prediction,sample_weight=None,loss_out=None,gradient_out=None,n_threads=1)
sklearn._loss.loss.ExponentialLoss(self,sample_weight=None)
sklearn._loss.loss.ExponentialLoss.__init__(self,sample_weight=None)
sklearn._loss.loss.ExponentialLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.ExponentialLoss.predict_proba(self,raw_prediction)
sklearn._loss.loss.HalfBinomialLoss(self,sample_weight=None)
sklearn._loss.loss.HalfBinomialLoss.__init__(self,sample_weight=None)
sklearn._loss.loss.HalfBinomialLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.HalfBinomialLoss.predict_proba(self,raw_prediction)
sklearn._loss.loss.HalfGammaLoss(self,sample_weight=None)
sklearn._loss.loss.HalfGammaLoss.__init__(self,sample_weight=None)
sklearn._loss.loss.HalfGammaLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.HalfMultinomialLoss(self,sample_weight=None,n_classes=3)
sklearn._loss.loss.HalfMultinomialLoss.__init__(self,sample_weight=None,n_classes=3)
sklearn._loss.loss.HalfMultinomialLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.loss.HalfMultinomialLoss.gradient_proba(self,y_true,raw_prediction,sample_weight=None,gradient_out=None,proba_out=None,n_threads=1)
sklearn._loss.loss.HalfMultinomialLoss.in_y_true_range(self,y)
sklearn._loss.loss.HalfMultinomialLoss.predict_proba(self,raw_prediction)
sklearn._loss.loss.HalfPoissonLoss(self,sample_weight=None)
sklearn._loss.loss.HalfPoissonLoss.__init__(self,sample_weight=None)
sklearn._loss.loss.HalfPoissonLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.HalfSquaredError(self,sample_weight=None)
sklearn._loss.loss.HalfSquaredError.__init__(self,sample_weight=None)
sklearn._loss.loss.HalfTweedieLoss(self,sample_weight=None,power=1.5)
sklearn._loss.loss.HalfTweedieLoss.__init__(self,sample_weight=None,power=1.5)
sklearn._loss.loss.HalfTweedieLoss.constant_to_optimal_zero(self,y_true,sample_weight=None)
sklearn._loss.loss.HalfTweedieLossIdentity(self,sample_weight=None,power=1.5)
sklearn._loss.loss.HalfTweedieLossIdentity.__init__(self,sample_weight=None,power=1.5)
sklearn._loss.loss.HuberLoss(self,sample_weight=None,quantile=0.9,delta=0.5)
sklearn._loss.loss.HuberLoss.__init__(self,sample_weight=None,quantile=0.9,delta=0.5)
sklearn._loss.loss.HuberLoss.fit_intercept_only(self,y_true,sample_weight=None)
sklearn._loss.loss.PinballLoss(self,sample_weight=None,quantile=0.5)
sklearn._loss.loss.PinballLoss.__init__(self,sample_weight=None,quantile=0.5)
sklearn._loss.loss.PinballLoss.fit_intercept_only(self,y_true,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/link.py----------------------------------------
A:sklearn._loss.link.low->numpy.greater(x, self.low)
A:sklearn._loss.link.high->numpy.less(x, self.high)
A:sklearn._loss.link.interval_y_pred->Interval(0, 1, False, False)
A:sklearn._loss.link.out->logit(y_pred, out=out)
A:sklearn._loss.link.gm->gmean(y_pred, axis=1)
sklearn._loss.link.BaseLink(ABC)
sklearn._loss.link.BaseLink.inverse(self,raw_prediction,out=None)
sklearn._loss.link.BaseLink.link(self,y_pred,out=None)
sklearn._loss.link.HalfLogitLink(BaseLink)
sklearn._loss.link.HalfLogitLink.inverse(self,raw_prediction,out=None)
sklearn._loss.link.HalfLogitLink.link(self,y_pred,out=None)
sklearn._loss.link.IdentityLink(BaseLink)
sklearn._loss.link.IdentityLink.link(self,y_pred,out=None)
sklearn._loss.link.Interval
sklearn._loss.link.Interval.__post_init__(self)
sklearn._loss.link.Interval.includes(self,x)
sklearn._loss.link.LogLink(BaseLink)
sklearn._loss.link.LogLink.inverse(self,raw_prediction,out=None)
sklearn._loss.link.LogLink.link(self,y_pred,out=None)
sklearn._loss.link.LogitLink(BaseLink)
sklearn._loss.link.LogitLink.inverse(self,raw_prediction,out=None)
sklearn._loss.link.LogitLink.link(self,y_pred,out=None)
sklearn._loss.link.MultinomialLogit(BaseLink)
sklearn._loss.link.MultinomialLogit.inverse(self,raw_prediction,out=None)
sklearn._loss.link.MultinomialLogit.link(self,y_pred,out=None)
sklearn._loss.link.MultinomialLogit.symmetrize_raw_prediction(self,raw_prediction)
sklearn._loss.link._inclusive_low_high(interval,dtype=np.float64)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/tests/test_loss.py----------------------------------------
A:sklearn._loss.tests.test_loss.ALL_LOSSES->list(_LOSSES.values())
A:sklearn._loss.tests.test_loss.rng->numpy.random.RandomState(global_random_seed)
A:sklearn._loss.tests.test_loss.raw_prediction->numpy.random.RandomState(global_random_seed).normal(size=n_samples)
A:sklearn._loss.tests.test_loss.raw_prediction.flat[:]->numpy.random.RandomState(global_random_seed).uniform(low=raw_bound[0], high=raw_bound[1], size=n_samples * loss.n_classes)
A:sklearn._loss.tests.test_loss.(low, high)->_inclusive_low_high(loss.interval_y_pred)
A:sklearn._loss.tests.test_loss.low->max(low, y_bound[0])
A:sklearn._loss.tests.test_loss.high->min(high, y_bound[1])
A:sklearn._loss.tests.test_loss.y_true->y_true.astype(global_dtype).astype(global_dtype)
A:sklearn._loss.tests.test_loss.h->numpy.full_like(x, fill_value=eps)
A:sklearn._loss.tests.test_loss.f_minus_2h->func(x - 2 * h)
A:sklearn._loss.tests.test_loss.f_minus_1h->func(x - h)
A:sklearn._loss.tests.test_loss.f_plus_1h->func(x + h)
A:sklearn._loss.tests.test_loss.f_plus_2h->func(x + 2 * h)
A:sklearn._loss.tests.test_loss.y_pred->HalfTweedieLoss(power=p).link.inverse(raw_prediction)
A:sklearn._loss.tests.test_loss.y_pred[:, 0]->numpy.linspace(low, high, num=n)
A:sklearn._loss.tests.test_loss.loss1->loss(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))
A:sklearn._loss.tests.test_loss.grad1->loss().gradient(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))
A:sklearn._loss.tests.test_loss.(loss2, grad2)->loss().loss_gradient(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))
A:sklearn._loss.tests.test_loss.(grad3, hess)->loss().gradient_hessian(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))
A:sklearn._loss.tests.test_loss.loss->loss()
A:sklearn._loss.tests.test_loss.(y_true, raw_prediction)->random_y_true_raw_prediction(loss=half_tweedie_log, n_samples=n_samples, seed=42)
A:sklearn._loss.tests.test_loss.sample_weight->numpy.ones(n_samples)
A:sklearn._loss.tests.test_loss.out1->numpy.empty_like(raw_prediction, dtype=dtype_out)
A:sklearn._loss.tests.test_loss.out2->numpy.empty_like(raw_prediction, dtype=dtype_out)
A:sklearn._loss.tests.test_loss.out_l1->numpy.empty_like(y_true)
A:sklearn._loss.tests.test_loss.out_l2->numpy.empty_like(y_true)
A:sklearn._loss.tests.test_loss.out_g1->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_g2->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_h1->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_h2->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_g3->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_h3->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.l1->loss().loss(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, loss_out=out_l1)
A:sklearn._loss.tests.test_loss.g1->loss().gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=out_g1)
A:sklearn._loss.tests.test_loss.(l2, g2)->loss().loss_gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, loss_out=out_l2, gradient_out=out_g2)
A:sklearn._loss.tests.test_loss.(g3, h3)->loss().gradient_hessian(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=out_g3, hessian_out=out_h3)
A:sklearn._loss.tests.test_loss.out_g4->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.out_proba->numpy.empty_like(raw_prediction)
A:sklearn._loss.tests.test_loss.(g4, proba)->loss().gradient_proba(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight, gradient_out=out_g4, proba_out=out_proba)
A:sklearn._loss.tests.test_loss.(losses, gradient)->loss().loss_gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=None)
A:sklearn._loss.tests.test_loss.(losses_sw, gradient_sw)->loss().loss_gradient(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.(gradient, hessian)->loss().init_gradient_and_hessian(n_samples=5, **params)
A:sklearn._loss.tests.test_loss.(gradient_sw, hessian_sw)->loss().gradient_hessian(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.raw_prediction.flat[::loss.n_classes + 1]->numpy.exp(10)
A:sklearn._loss.tests.test_loss.loss_value->loss().loss(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.constant_term->loss().constant_to_optimal_zero(y_true=y_true, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.(g, h)->loss().gradient_hessian(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.g_numeric->numerical_derivative(loss_func, raw_prediction[:, k], eps=1e-05)
A:sklearn._loss.tests.test_loss.h_numeric->numerical_derivative(grad_func, raw_prediction[:, k], eps=1e-06)
A:sklearn._loss.tests.test_loss.raw->numpy.random.RandomState(global_random_seed).normal(size=n_samples).copy()
A:sklearn._loss.tests.test_loss.x0->numpy.array([x0], dtype=np.float64)
A:sklearn._loss.tests.test_loss.optimum->optimum.ravel().ravel()
A:sklearn._loss.tests.test_loss.a->loss().fit_intercept_only(y_true=y_true, sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.opt->minimize(fun, np.zeros(loss.n_classes), tol=1e-13, options={'maxiter': 100}, method='SLSQP', constraints=LinearConstraint(np.ones((1, loss.n_classes)), 0, 0))
A:sklearn._loss.tests.test_loss.grad->loss().gradient(y_true=y_true, raw_prediction=np.tile(a, (n_samples, 1)), sample_weight=sample_weight)
A:sklearn._loss.tests.test_loss.y_train->numpy.random.RandomState(global_random_seed).randint(0, 2, size=n_samples).astype(np.float64)
A:sklearn._loss.tests.test_loss.baseline_prediction->loss().fit_intercept_only(y_true=y_train)
A:sklearn._loss.tests.test_loss.p->numpy.zeros(n_classes, dtype=y_train.dtype)
A:sklearn._loss.tests.test_loss.p[k]->(y_train == k).mean()
A:sklearn._loss.tests.test_loss.binom->HalfBinomialLoss()
A:sklearn._loss.tests.test_loss.multinom->HalfMultinomialLoss(n_classes=2)
A:sklearn._loss.tests.test_loss.raw_multinom->numpy.empty((n_samples, 2))
A:sklearn._loss.tests.test_loss.bin_loss->HalfBinomialLoss()
A:sklearn._loss.tests.test_loss.proba->loss().predict_proba(raw_prediction)
A:sklearn._loss.tests.test_loss.(grad, proba)->loss().gradient_proba(y_true=y_true, raw_prediction=raw_prediction, sample_weight=None, gradient_out=grad, proba_out=proba)
A:sklearn._loss.tests.test_loss.pickled_loss->pickle.dumps(loss)
A:sklearn._loss.tests.test_loss.unpickled_loss->pickle.loads(pickled_loss)
A:sklearn._loss.tests.test_loss.half_tweedie_log->HalfTweedieLoss(power=p)
A:sklearn._loss.tests.test_loss.half_tweedie_identity->HalfTweedieLossIdentity(power=p)
A:sklearn._loss.tests.test_loss.(gradient_log, hessian_log)->HalfTweedieLoss(power=p).gradient_hessian(y_true=y_true, raw_prediction=raw_prediction)
A:sklearn._loss.tests.test_loss.(gradient_identity, hessian_identity)->HalfTweedieLossIdentity(power=p).gradient_hessian(y_true=y_true, raw_prediction=y_pred)
sklearn._loss.tests.test_loss.loss_instance_name(param)
sklearn._loss.tests.test_loss.numerical_derivative(func,x,eps)
sklearn._loss.tests.test_loss.random_y_true_raw_prediction(loss,n_samples,y_bound=(-100,100),raw_bound=(-5,5),seed=42)
sklearn._loss.tests.test_loss.test_binomial_and_multinomial_loss(global_random_seed)
sklearn._loss.tests.test_loss.test_binomial_vs_alternative_formulation(y_true,y_pred,global_dtype)
sklearn._loss.tests.test_loss.test_derivatives(loss,x0,y_true)
sklearn._loss.tests.test_loss.test_graceful_squeezing(loss)
sklearn._loss.tests.test_loss.test_gradients_hessians_numerically(loss,sample_weight,global_random_seed)
sklearn._loss.tests.test_loss.test_init_gradient_and_hessian_raises(loss,params,err_msg)
sklearn._loss.tests.test_loss.test_init_gradient_and_hessians(loss,sample_weight,dtype,order)
sklearn._loss.tests.test_loss.test_loss_boundary(loss)
sklearn._loss.tests.test_loss.test_loss_boundary_y_pred(loss,y_pred_success,y_pred_fail)
sklearn._loss.tests.test_loss.test_loss_boundary_y_true(loss,y_true_success,y_true_fail)
sklearn._loss.tests.test_loss.test_loss_dtype(loss,readonly_memmap,dtype_in,dtype_out,sample_weight,out1,out2,n_threads)
sklearn._loss.tests.test_loss.test_loss_gradients_are_the_same(loss,sample_weight,global_random_seed)
sklearn._loss.tests.test_loss.test_loss_init_parameter_validation(loss,params,err_type,err_msg)
sklearn._loss.tests.test_loss.test_loss_intercept_only(loss,sample_weight)
sklearn._loss.tests.test_loss.test_loss_of_perfect_prediction(loss,sample_weight)
sklearn._loss.tests.test_loss.test_loss_on_specific_values(loss,y_true,raw_prediction,loss_true,gradient_true,hessian_true)
sklearn._loss.tests.test_loss.test_loss_pickle(loss)
sklearn._loss.tests.test_loss.test_loss_same_as_C_functions(loss,sample_weight)
sklearn._loss.tests.test_loss.test_multinomial_loss_fit_intercept_only()
sklearn._loss.tests.test_loss.test_predict_proba(loss,global_random_seed)
sklearn._loss.tests.test_loss.test_sample_weight_multiplies(loss,sample_weight,global_random_seed)
sklearn._loss.tests.test_loss.test_specific_fit_intercept_only(loss,func,random_dist,global_random_seed)
sklearn._loss.tests.test_loss.test_tweedie_log_identity_consistency(p)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/_loss/tests/test_link.py----------------------------------------
A:sklearn._loss.tests.test_link.LINK_FUNCTIONS->list(_LINKS.values())
A:sklearn._loss.tests.test_link.(low, high)->_inclusive_low_high(interval)
A:sklearn._loss.tests.test_link.x->numpy.linspace(low, high, num=10)
A:sklearn._loss.tests.test_link.rng->numpy.random.RandomState(42)
A:sklearn._loss.tests.test_link.link->link()
A:sklearn._loss.tests.test_link.raw_prediction->numpy.random.RandomState(42).uniform(low=-10, high=10, size=n_samples)
A:sklearn._loss.tests.test_link.y_pred->link().inverse(raw_prediction, out=None)
A:sklearn._loss.tests.test_link.out->numpy.empty_like(y_pred)
A:sklearn._loss.tests.test_link.y_pred_2->link().inverse(raw_prediction, out=out)
A:sklearn._loss.tests.test_link.raw_prediction_2->link().link(y_pred, out=out)
sklearn._loss.tests.test_link.test_interval_raises()
sklearn._loss.tests.test_link.test_is_in_range(interval)
sklearn._loss.tests.test_link.test_link_inverse_identity(link,global_random_seed)
sklearn._loss.tests.test_link.test_link_out_argument(link)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/_empirical_covariance.py----------------------------------------
A:sklearn.covariance._empirical_covariance.X->self._validate_data(X, reset=False)
A:sklearn.covariance._empirical_covariance.covariance->empirical_covariance(X, assume_centered=self.assume_centered)
A:sklearn.covariance._empirical_covariance.self.precision_->scipy.linalg.pinvh(covariance, check_finite=False)
A:sklearn.covariance._empirical_covariance.precision->self.get_precision()
A:sklearn.covariance._empirical_covariance.self.location_->self._validate_data(X, reset=False).mean(0)
A:sklearn.covariance._empirical_covariance.X_test->self._validate_data(X_test, reset=False)
A:sklearn.covariance._empirical_covariance.test_cov->empirical_covariance(X_test - self.location_, assume_centered=True)
A:sklearn.covariance._empirical_covariance.res->log_likelihood(test_cov, self.get_precision())
A:sklearn.covariance._empirical_covariance.squared_norm->numpy.amax(linalg.svdvals(np.dot(error.T, error)))
A:sklearn.covariance._empirical_covariance.result->numpy.sqrt(squared_norm)
A:sklearn.covariance._empirical_covariance.dist->pairwise_distances(X, self.location_[np.newaxis, :], metric='mahalanobis', VI=precision)
sklearn.covariance.EmpiricalCovariance(self,*,store_precision=True,assume_centered=False)
sklearn.covariance.EmpiricalCovariance._set_covariance(self,covariance)
sklearn.covariance.EmpiricalCovariance.error_norm(self,comp_cov,norm='frobenius',scaling=True,squared=True)
sklearn.covariance.EmpiricalCovariance.fit(self,X,y=None)
sklearn.covariance.EmpiricalCovariance.get_precision(self)
sklearn.covariance.EmpiricalCovariance.mahalanobis(self,X)
sklearn.covariance.EmpiricalCovariance.score(self,X_test,y=None)
sklearn.covariance._empirical_covariance.EmpiricalCovariance(self,*,store_precision=True,assume_centered=False)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.__init__(self,*,store_precision=True,assume_centered=False)
sklearn.covariance._empirical_covariance.EmpiricalCovariance._set_covariance(self,covariance)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.error_norm(self,comp_cov,norm='frobenius',scaling=True,squared=True)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.fit(self,X,y=None)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.get_precision(self)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.mahalanobis(self,X)
sklearn.covariance._empirical_covariance.EmpiricalCovariance.score(self,X_test,y=None)
sklearn.covariance._empirical_covariance.empirical_covariance(X,*,assume_centered=False)
sklearn.covariance._empirical_covariance.log_likelihood(emp_cov,precision)
sklearn.covariance.empirical_covariance(X,*,assume_centered=False)
sklearn.covariance.log_likelihood(emp_cov,precision)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/_robust_covariance.py----------------------------------------
A:sklearn.covariance._robust_covariance.X->self._validate_data(X, ensure_min_samples=2, estimator='MinCovDet')
A:sklearn.covariance._robust_covariance.random_state->check_random_state(self.random_state)
A:sklearn.covariance._robust_covariance.support->numpy.zeros(n_samples, dtype=bool)
A:sklearn.covariance._robust_covariance.precision->scipy.linalg.pinvh(raw_covariance)
A:sklearn.covariance._robust_covariance.dist->numpy.zeros(n_samples)
A:sklearn.covariance._robust_covariance.location->numpy.asarray([np.mean(X)])
A:sklearn.covariance._robust_covariance.covariance->numpy.asarray([[np.var(X)]])
A:sklearn.covariance._robust_covariance.det->fast_logdet(covariance)
A:sklearn.covariance._robust_covariance.(all_locs_sub, all_covs_sub, all_dets_sub, all_supports_sub, all_ds_sub)->zip(*all_estimates)
A:sklearn.covariance._robust_covariance.n_support->numpy.sum(self.support_)
A:sklearn.covariance._robust_covariance.X_sorted->numpy.sort(np.ravel(X))
A:sklearn.covariance._robust_covariance.samples_shuffle->check_random_state(self.random_state).permutation(n_samples)
A:sklearn.covariance._robust_covariance.h_subset->int(np.ceil(n_samples_subsets * (n_support / float(n_samples))))
A:sklearn.covariance._robust_covariance.n_trials->max(10, n_trials_tot // n_subsets)
A:sklearn.covariance._robust_covariance.all_best_locations->numpy.zeros((n_best_tot, n_features))
A:sklearn.covariance._robust_covariance.all_best_covariances->numpy.zeros((n_best_tot, n_features, n_features))
A:sklearn.covariance._robust_covariance.(best_locations_sub, best_covariances_sub, _, _)->select_candidates(current_subset, h_subset, n_trials, select=n_best_sub, n_iter=2, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance._robust_covariance.subset_slice->numpy.arange(i * n_best_sub, (i + 1) * n_best_sub)
A:sklearn.covariance._robust_covariance.n_samples_merged->min(1500, n_samples)
A:sklearn.covariance._robust_covariance.h_merged->int(np.ceil(n_samples_merged * (n_support / float(n_samples))))
A:sklearn.covariance._robust_covariance.(locations_merged, covariances_merged, supports_merged, d)->select_candidates(X[selection], h_merged, n_trials=(all_best_locations, all_best_covariances), select=n_best_merged, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance._robust_covariance.(locations_full, covariances_full, supports_full, d)->select_candidates(X, n_support, n_trials=(locations_best, covariances_best), select=1, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance._robust_covariance.(locations_best, covariances_best, _, _)->select_candidates(X, n_support, n_trials=n_trials, select=n_best, n_iter=2, cov_computation_method=cov_computation_method, random_state=random_state)
A:sklearn.covariance._robust_covariance._nonrobust_covariance->staticmethod(empirical_covariance)
A:sklearn.covariance._robust_covariance.(raw_location, raw_covariance, raw_support, raw_dist)->fast_mcd(X, support_fraction=self.support_fraction, cov_computation_method=self._nonrobust_covariance, random_state=random_state)
A:sklearn.covariance._robust_covariance.raw_location->numpy.zeros(n_features)
A:sklearn.covariance._robust_covariance.raw_covariance->self._nonrobust_covariance(X[raw_support], assume_centered=True)
A:sklearn.covariance._robust_covariance.raw_dist->numpy.sum(np.dot(X, precision) * X, 1)
A:sklearn.covariance._robust_covariance.n_samples->len(self.dist_)
A:sklearn.covariance._robust_covariance.location_reweighted->data[mask].mean(0)
A:sklearn.covariance._robust_covariance.covariance_reweighted->self._nonrobust_covariance(data[mask], assume_centered=self.assume_centered)
A:sklearn.covariance._robust_covariance.support_reweighted->numpy.zeros(n_samples, dtype=bool)
A:sklearn.covariance._robust_covariance.self.dist_->numpy.sum(np.dot(X_centered, self.get_precision()) * X_centered, 1)
sklearn.covariance.MinCovDet(self,*,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance.MinCovDet.correct_covariance(self,data)
sklearn.covariance.MinCovDet.fit(self,X,y=None)
sklearn.covariance.MinCovDet.reweight_covariance(self,data)
sklearn.covariance._robust_covariance.MinCovDet(self,*,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance._robust_covariance.MinCovDet.__init__(self,*,store_precision=True,assume_centered=False,support_fraction=None,random_state=None)
sklearn.covariance._robust_covariance.MinCovDet.correct_covariance(self,data)
sklearn.covariance._robust_covariance.MinCovDet.fit(self,X,y=None)
sklearn.covariance._robust_covariance.MinCovDet.reweight_covariance(self,data)
sklearn.covariance._robust_covariance._c_step(X,n_support,random_state,remaining_iterations=30,initial_estimates=None,verbose=False,cov_computation_method=empirical_covariance)
sklearn.covariance._robust_covariance.c_step(X,n_support,remaining_iterations=30,initial_estimates=None,verbose=False,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance._robust_covariance.fast_mcd(X,support_fraction=None,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance._robust_covariance.select_candidates(X,n_support,n_trials,select=1,n_iter=30,verbose=False,cov_computation_method=empirical_covariance,random_state=None)
sklearn.covariance.fast_mcd(X,support_fraction=None,cov_computation_method=empirical_covariance,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/_elliptic_envelope.py----------------------------------------
A:sklearn.covariance._elliptic_envelope.self.offset_->numpy.percentile(-self.dist_, 100.0 * self.contamination)
A:sklearn.covariance._elliptic_envelope.negative_mahal_dist->self.score_samples(X)
A:sklearn.covariance._elliptic_envelope.values->self.decision_function(X)
A:sklearn.covariance._elliptic_envelope.is_inlier->numpy.full(values.shape[0], -1, dtype=int)
sklearn.covariance.EllipticEnvelope(self,*,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance.EllipticEnvelope.decision_function(self,X)
sklearn.covariance.EllipticEnvelope.fit(self,X,y=None)
sklearn.covariance.EllipticEnvelope.predict(self,X)
sklearn.covariance.EllipticEnvelope.score(self,X,y,sample_weight=None)
sklearn.covariance.EllipticEnvelope.score_samples(self,X)
sklearn.covariance._elliptic_envelope.EllipticEnvelope(self,*,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.__init__(self,*,store_precision=True,assume_centered=False,support_fraction=None,contamination=0.1,random_state=None)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.decision_function(self,X)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.fit(self,X,y=None)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.predict(self,X)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.score(self,X,y,sample_weight=None)
sklearn.covariance._elliptic_envelope.EllipticEnvelope.score_samples(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/_shrunk_covariance.py----------------------------------------
A:sklearn.covariance._shrunk_covariance.shrinkage->ledoit_wolf_shrinkage(X, assume_centered=assume_centered, block_size=block_size)
A:sklearn.covariance._shrunk_covariance.emp_cov->check_array(emp_cov, allow_nd=True)
A:sklearn.covariance._shrunk_covariance.alpha->numpy.mean(emp_cov ** 2)
A:sklearn.covariance._shrunk_covariance.mu->numpy.expand_dims(mu, axis=tuple(range(mu.ndim, emp_cov.ndim)))
A:sklearn.covariance._shrunk_covariance.X->self._validate_data(X)
A:sklearn.covariance._shrunk_covariance.self.location_->self._validate_data(X).mean(0)
A:sklearn.covariance._shrunk_covariance.covariance->shrunk_covariance(covariance, self.shrinkage)
A:sklearn.covariance._shrunk_covariance.n_splits->int(n_features / block_size)
A:sklearn.covariance._shrunk_covariance.rows->slice(block_size * i, block_size * (i + 1))
A:sklearn.covariance._shrunk_covariance.cols->slice(block_size * j, block_size * (j + 1))
A:sklearn.covariance._shrunk_covariance.beta->min(beta, delta)
A:sklearn.covariance._shrunk_covariance.estimator->OAS(assume_centered=assume_centered).fit(X)
A:sklearn.covariance._shrunk_covariance.(covariance, shrinkage)->_oas(X - self.location_, assume_centered=True)
sklearn.covariance.LedoitWolf(self,*,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance.LedoitWolf.fit(self,X,y=None)
sklearn.covariance.OAS(EmpiricalCovariance)
sklearn.covariance.OAS.fit(self,X,y=None)
sklearn.covariance.ShrunkCovariance(self,*,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance.ShrunkCovariance.fit(self,X,y=None)
sklearn.covariance._shrunk_covariance.LedoitWolf(self,*,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance._shrunk_covariance.LedoitWolf.__init__(self,*,store_precision=True,assume_centered=False,block_size=1000)
sklearn.covariance._shrunk_covariance.LedoitWolf.fit(self,X,y=None)
sklearn.covariance._shrunk_covariance.OAS(EmpiricalCovariance)
sklearn.covariance._shrunk_covariance.OAS.fit(self,X,y=None)
sklearn.covariance._shrunk_covariance.ShrunkCovariance(self,*,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance._shrunk_covariance.ShrunkCovariance.__init__(self,*,store_precision=True,assume_centered=False,shrinkage=0.1)
sklearn.covariance._shrunk_covariance.ShrunkCovariance.fit(self,X,y=None)
sklearn.covariance._shrunk_covariance._ledoit_wolf(X,*,assume_centered,block_size)
sklearn.covariance._shrunk_covariance._oas(X,*,assume_centered=False)
sklearn.covariance._shrunk_covariance.ledoit_wolf(X,*,assume_centered=False,block_size=1000)
sklearn.covariance._shrunk_covariance.ledoit_wolf_shrinkage(X,assume_centered=False,block_size=1000)
sklearn.covariance._shrunk_covariance.oas(X,*,assume_centered=False)
sklearn.covariance._shrunk_covariance.shrunk_covariance(emp_cov,shrinkage=0.1)
sklearn.covariance.ledoit_wolf(X,*,assume_centered=False,block_size=1000)
sklearn.covariance.ledoit_wolf_shrinkage(X,assume_centered=False,block_size=1000)
sklearn.covariance.oas(X,*,assume_centered=False)
sklearn.covariance.shrunk_covariance(emp_cov,shrinkage=0.1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/_graph_lasso.py----------------------------------------
A:sklearn.covariance._graph_lasso.gap->numpy.sum(emp_cov * precision_)
A:sklearn.covariance._graph_lasso.precision_->scipy.linalg.pinvh(covariance_)
A:sklearn.covariance._graph_lasso.covariance_->empirical_covariance(X, assume_centered=self.assume_centered).copy()
A:sklearn.covariance._graph_lasso.indices->numpy.arange(n_features)
A:sklearn.covariance._graph_lasso.costs->list()
A:sklearn.covariance._graph_lasso.errors->dict(invalid='raise')
A:sklearn.covariance._graph_lasso.sub_covariance->numpy.copy(covariance_[1:, 1:], order='C')
A:sklearn.covariance._graph_lasso.(coefs, _, _, _)->linear_model._cd_fast.enet_coordinate_descent_gram(coefs, alpha, 0, sub_covariance, row, row, max_iter, enet_tol, check_random_state(None), False)
A:sklearn.covariance._graph_lasso.(_, _, coefs)->lars_path_gram(Xy=row, Gram=sub_covariance, n_samples=row.size, alpha_min=alpha / (n_features - 1), copy_Gram=True, eps=eps, method='lars', return_path=False)
A:sklearn.covariance._graph_lasso.coefs->numpy.dot(sub_covariance, coefs)
A:sklearn.covariance._graph_lasso.d_gap->_dual_gap(emp_cov, precision_, alpha)
A:sklearn.covariance._graph_lasso.cost->_objective(emp_cov, precision_, alpha)
A:sklearn.covariance._graph_lasso.A->numpy.copy(emp_cov)
A:sklearn.covariance._graph_lasso.model->GraphicalLasso(alpha=alpha, mode=mode, covariance='precomputed', tol=tol, enet_tol=enet_tol, max_iter=max_iter, verbose=verbose, eps=eps, assume_centered=True).fit(emp_cov)
A:sklearn.covariance._graph_lasso.X->self._validate_data(X, ensure_min_features=2)
A:sklearn.covariance._graph_lasso.emp_cov->empirical_covariance(X, assume_centered=self.assume_centered)
A:sklearn.covariance._graph_lasso.self.location_->self._validate_data(X, ensure_min_features=2).mean(0)
A:sklearn.covariance._graph_lasso.(self.covariance_, self.precision_, self.costs_, self.n_iter_)->_graphical_lasso(emp_cov, alpha=best_alpha, mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=self.max_iter, verbose=inner_verbose, eps=self.eps)
A:sklearn.covariance._graph_lasso.inner_verbose->max(0, self.verbose - 1)
A:sklearn.covariance._graph_lasso.covariances_->list()
A:sklearn.covariance._graph_lasso.precisions_->list()
A:sklearn.covariance._graph_lasso.scores_->list()
A:sklearn.covariance._graph_lasso.test_emp_cov->empirical_covariance(X_test)
A:sklearn.covariance._graph_lasso.(covariance_, precision_, _, _)->_graphical_lasso(emp_cov, alpha=alpha, cov_init=covariance_, mode=mode, tol=tol, enet_tol=enet_tol, max_iter=max_iter, verbose=inner_verbose, eps=eps)
A:sklearn.covariance._graph_lasso.this_score->numpy.mean(scores)
A:sklearn.covariance._graph_lasso.cv->check_cv(self.cv, y, classifier=False)
A:sklearn.covariance._graph_lasso.path->list(zip(*path))
A:sklearn.covariance._graph_lasso.alpha_1->alpha_max(emp_cov)
A:sklearn.covariance._graph_lasso.t0->time.time()
A:sklearn.covariance._graph_lasso.this_path->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(graphical_lasso_path)(X[train], alphas=alphas, X_test=X[test], mode=self.mode, tol=self.tol, enet_tol=self.enet_tol, max_iter=int(0.1 * self.max_iter), verbose=inner_verbose, eps=self.eps) for (train, test) in cv.split(X, y)))
A:sklearn.covariance._graph_lasso.(covs, _, scores)->zip(*this_path)
A:sklearn.covariance._graph_lasso.covs->zip(*covs)
A:sklearn.covariance._graph_lasso.scores->zip(*scores)
A:sklearn.covariance._graph_lasso.alphas->list(path[0])
A:sklearn.covariance._graph_lasso.grid_scores->numpy.array(grid_scores)
A:sklearn.covariance._graph_lasso.self.cv_results_['mean_test_score']->numpy.mean(grid_scores, axis=1)
A:sklearn.covariance._graph_lasso.self.cv_results_['std_test_score']->numpy.std(grid_scores, axis=1)
sklearn.covariance.GraphicalLasso(self,alpha=0.01,*,mode='cd',covariance=None,tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance.GraphicalLasso.fit(self,X,y=None)
sklearn.covariance.GraphicalLassoCV(self,*,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=None,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance.GraphicalLassoCV.fit(self,X,y=None)
sklearn.covariance._graph_lasso.BaseGraphicalLasso(self,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.BaseGraphicalLasso.__init__(self,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.GraphicalLasso(self,alpha=0.01,*,mode='cd',covariance=None,tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.GraphicalLasso.__init__(self,alpha=0.01,*,mode='cd',covariance=None,tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.GraphicalLasso.fit(self,X,y=None)
sklearn.covariance._graph_lasso.GraphicalLassoCV(self,*,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=None,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.GraphicalLassoCV.__init__(self,*,alphas=4,n_refinements=4,cv=None,tol=0.0001,enet_tol=0.0001,max_iter=100,mode='cd',n_jobs=None,verbose=False,eps=np.finfo(np.float64).eps,assume_centered=False)
sklearn.covariance._graph_lasso.GraphicalLassoCV.fit(self,X,y=None)
sklearn.covariance._graph_lasso._dual_gap(emp_cov,precision_,alpha)
sklearn.covariance._graph_lasso._graphical_lasso(emp_cov,alpha,*,cov_init=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps)
sklearn.covariance._graph_lasso._objective(mle,precision_,alpha)
sklearn.covariance._graph_lasso.alpha_max(emp_cov)
sklearn.covariance._graph_lasso.graphical_lasso(emp_cov,alpha,*,cov_init=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,return_costs=False,eps=np.finfo(np.float64).eps,return_n_iter=False)
sklearn.covariance._graph_lasso.graphical_lasso_path(X,alphas,cov_init=None,X_test=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps)
sklearn.covariance.graphical_lasso(emp_cov,alpha,*,cov_init=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,return_costs=False,eps=np.finfo(np.float64).eps,return_n_iter=False)
sklearn.covariance.graphical_lasso_path(X,alphas,cov_init=None,X_test=None,mode='cd',tol=0.0001,enet_tol=0.0001,max_iter=100,verbose=False,eps=np.finfo(np.float64).eps)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/tests/test_elliptic_envelope.py----------------------------------------
A:sklearn.covariance.tests.test_elliptic_envelope.rnd->numpy.random.RandomState(global_random_seed)
A:sklearn.covariance.tests.test_elliptic_envelope.X->numpy.random.RandomState(global_random_seed).randn(100, 10)
A:sklearn.covariance.tests.test_elliptic_envelope.clf->EllipticEnvelope(contamination=0.1)
A:sklearn.covariance.tests.test_elliptic_envelope.y_pred->EllipticEnvelope(contamination=0.1).predict(X)
A:sklearn.covariance.tests.test_elliptic_envelope.scores->EllipticEnvelope(contamination=0.1).score_samples(X)
A:sklearn.covariance.tests.test_elliptic_envelope.decisions->EllipticEnvelope(contamination=0.1).decision_function(X)
A:sklearn.covariance.tests.test_elliptic_envelope.clf1->EllipticEnvelope(contamination=0.2).fit(X_train)
A:sklearn.covariance.tests.test_elliptic_envelope.clf2->EllipticEnvelope().fit(X_train)
sklearn.covariance.tests.test_elliptic_envelope.test_elliptic_envelope(global_random_seed)
sklearn.covariance.tests.test_elliptic_envelope.test_score_samples()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/tests/test_graphical_lasso.py----------------------------------------
A:sklearn.covariance.tests.test_graphical_lasso.random_state->check_random_state(random_state)
A:sklearn.covariance.tests.test_graphical_lasso.prec->make_sparse_spd_matrix(dim, alpha=0.95, random_state=0)
A:sklearn.covariance.tests.test_graphical_lasso.cov->scipy.linalg.inv(prec)
A:sklearn.covariance.tests.test_graphical_lasso.X->numpy.random.RandomState(0).multivariate_normal(np.zeros(dim), cov, size=n_samples)
A:sklearn.covariance.tests.test_graphical_lasso.emp_cov->empirical_covariance(X)
A:sklearn.covariance.tests.test_graphical_lasso.covs->dict()
A:sklearn.covariance.tests.test_graphical_lasso.icovs->dict()
A:sklearn.covariance.tests.test_graphical_lasso.(cov_, icov_, costs)->graphical_lasso(emp_cov, return_costs=True, alpha=alpha, mode=method)
A:sklearn.covariance.tests.test_graphical_lasso.model->GraphicalLasso(alpha=0, covariance='precomputed').fit(emp_cov)
A:sklearn.covariance.tests.test_graphical_lasso.precs->list()
A:sklearn.covariance.tests.test_graphical_lasso.(_, precision)->graphical_lasso(emp_cov, alpha=0)
A:sklearn.covariance.tests.test_graphical_lasso.(X, _)->sklearn.datasets.make_classification(n_samples=5000, n_features=20, random_state=0)
A:sklearn.covariance.tests.test_graphical_lasso.(_, _, n_iter)->graphical_lasso(emp_cov, 0.2, mode=mode, max_iter=2, return_n_iter=True)
A:sklearn.covariance.tests.test_graphical_lasso.cov_R->numpy.array([[0.08, 0.056666662595, 0.00229729713223, 0.00153153142149], [0.056666662595, 0.082222222222, 0.00333333333333, 0.00222222222222], [0.002297297132, 0.003333333333, 0.00666666666667, 9.009009009e-05], [0.001531531421, 0.002222222222, 9.009009009e-05, 0.00222222222222]])
A:sklearn.covariance.tests.test_graphical_lasso.icov_R->numpy.array([[24.42244057, -16.831679593, 0.0, 0.0], [-16.83168201, 24.351841681, -6.206896552, -12.5], [0.0, -6.206896171, 153.103448276, 0.0], [0.0, -12.499999143, 0.0, 462.5]])
A:sklearn.covariance.tests.test_graphical_lasso.(cov, icov)->graphical_lasso(emp_cov, alpha=0.01, return_costs=False, mode=method)
A:sklearn.covariance.tests.test_graphical_lasso.cov_skggm->numpy.array([[3.09550269, 1.186972], [1.186972, 0.57713289]])
A:sklearn.covariance.tests.test_graphical_lasso.icov_skggm->numpy.array([[1.52836773, -3.14334831], [-3.14334831, 8.19753385]])
A:sklearn.covariance.tests.test_graphical_lasso.indices->numpy.arange(10, 13)
A:sklearn.covariance.tests.test_graphical_lasso.sys.stdout->StringIO()
A:sklearn.covariance.tests.test_graphical_lasso.true_cov->numpy.array([[0.8, 0.0, 0.2, 0.0], [0.0, 0.4, 0.0, 0.0], [0.2, 0.0, 0.3, 0.1], [0.0, 0.0, 0.1, 0.7]])
A:sklearn.covariance.tests.test_graphical_lasso.rng->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_graphical_lasso.alphas->_convert_container([0.02, 0.03], alphas_container_type)
A:sklearn.covariance.tests.test_graphical_lasso.cv_scores->numpy.asarray([cov.cv_results_[key] for key in split_keys])
A:sklearn.covariance.tests.test_graphical_lasso.expected_mean->numpy.asarray([cov.cv_results_[key] for key in split_keys]).mean(axis=0)
A:sklearn.covariance.tests.test_graphical_lasso.expected_std->numpy.asarray([cov.cv_results_[key] for key in split_keys]).std(axis=0)
sklearn.covariance.tests.test_graphical_lasso.test_graph_lasso_2D()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_cov_init_deprecation()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_cv(random_state=1)
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_cv_alphas_invalid_array(alphas,err_type,err_msg)
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_cv_alphas_iterable(alphas_container_type)
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_cv_scores()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_iris()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_iris_singular()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_n_iter(mode)
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lasso_when_alpha_equals_0()
sklearn.covariance.tests.test_graphical_lasso.test_graphical_lassos(random_state=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/tests/test_robust_covariance.py----------------------------------------
A:sklearn.covariance.tests.test_robust_covariance.X->numpy.random.RandomState(0).normal(size=(3, 1))
A:sklearn.covariance.tests.test_robust_covariance.mcd->MinCovDet(support_fraction=0.5, random_state=global_random_seed)
A:sklearn.covariance.tests.test_robust_covariance.rand_gen->numpy.random.RandomState(global_random_seed)
A:sklearn.covariance.tests.test_robust_covariance.data->numpy.hstack((data, np.zeros((data.shape[0], 1))))
A:sklearn.covariance.tests.test_robust_covariance.inliers_mask->numpy.ones(n_samples).astype(bool)
A:sklearn.covariance.tests.test_robust_covariance.mcd_fit->MinCovDet(random_state=seed).fit(data)
A:sklearn.covariance.tests.test_robust_covariance.error_location->numpy.mean((pure_data.mean(0) - T) ** 2)
A:sklearn.covariance.tests.test_robust_covariance.error_cov->numpy.mean((empirical_covariance(pure_data) - S) ** 2)
A:sklearn.covariance.tests.test_robust_covariance.rnd->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_robust_covariance.data_values->numpy.linspace(-5, 5, 10).tolist()
A:sklearn.covariance.tests.test_robust_covariance.X_1->X_1.reshape(-1, 1).reshape(-1, 1)
A:sklearn.covariance.tests.test_robust_covariance.X_2->X_2.reshape(-1, 1).reshape(-1, 1)
sklearn.covariance.tests.test_robust_covariance.launch_mcd_on_dataset(n_samples,n_features,n_outliers,tol_loc,tol_cov,tol_support,seed)
sklearn.covariance.tests.test_robust_covariance.test_fast_mcd_on_invalid_input()
sklearn.covariance.tests.test_robust_covariance.test_mcd(global_random_seed)
sklearn.covariance.tests.test_robust_covariance.test_mcd_class_on_invalid_input()
sklearn.covariance.tests.test_robust_covariance.test_mcd_increasing_det_warning(global_random_seed)
sklearn.covariance.tests.test_robust_covariance.test_mcd_issue1127()
sklearn.covariance.tests.test_robust_covariance.test_mcd_issue3367(global_random_seed)
sklearn.covariance.tests.test_robust_covariance.test_mcd_support_covariance_is_zero()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/covariance/tests/test_covariance.py----------------------------------------
A:sklearn.covariance.tests.test_covariance.(X, _)->sklearn.datasets.load_diabetes(return_X_y=True)
A:sklearn.covariance.tests.test_covariance.cov->EmpiricalCovariance().fit(X)
A:sklearn.covariance.tests.test_covariance.emp_cov->empirical_covariance(X, assume_centered=False)
A:sklearn.covariance.tests.test_covariance.mahal_dist->EmpiricalCovariance().fit(X).mahalanobis(X)
A:sklearn.covariance.tests.test_covariance.X_1d->X[:, 0].reshape((-1, 1))
A:sklearn.covariance.tests.test_covariance.X_1sample->numpy.arange(5).reshape(1, 5)
A:sklearn.covariance.tests.test_covariance.X_integer->numpy.asarray([[0, 1], [1, 0]])
A:sklearn.covariance.tests.test_covariance.result->numpy.asarray([[0.25, -0.25], [-0.25, 0.25]])
A:sklearn.covariance.tests.test_covariance.cov_target->numpy.repeat(cov_target[np.newaxis, ...], n_matrices, axis=0)
A:sklearn.covariance.tests.test_covariance.cov_shrunk->shrunk_covariance(cov, 0.5)
A:sklearn.covariance.tests.test_covariance.lw->LedoitWolf(block_size=25).fit(X)
A:sklearn.covariance.tests.test_covariance.score_->OAS().score(X_centered)
A:sklearn.covariance.tests.test_covariance.(lw_cov_from_mle, lw_shrinkage_from_mle)->ledoit_wolf(X_1d)
A:sklearn.covariance.tests.test_covariance.scov->ShrunkCovariance(shrinkage=oa.shrinkage_)
A:sklearn.covariance.tests.test_covariance.delta_->empirical_covariance(X, assume_centered=False).copy()
A:sklearn.covariance.tests.test_covariance.beta->min(beta_, delta)
A:sklearn.covariance.tests.test_covariance.rng->numpy.random.RandomState(0)
A:sklearn.covariance.tests.test_covariance.X->numpy.random.RandomState(0).normal(size=(10, 20))
A:sklearn.covariance.tests.test_covariance.X_empty->numpy.zeros((0, 2))
A:sklearn.covariance.tests.test_covariance.oa->OAS()
A:sklearn.covariance.tests.test_covariance.(oa_cov_from_mle, oa_shrinkage_from_mle)->oas(X_1d)
A:sklearn.covariance.tests.test_covariance.(_oa_cov_from_mle, _oa_shrinkage_from_mle)->_oas(X_1f)
sklearn.covariance.tests.test_covariance._naive_ledoit_wolf_shrinkage(X)
sklearn.covariance.tests.test_covariance.test_EmpiricalCovariance_validates_mahalanobis()
sklearn.covariance.tests.test_covariance.test_covariance()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf_empty_array(ledoit_wolf_fitting_function)
sklearn.covariance.tests.test_covariance.test_ledoit_wolf_large()
sklearn.covariance.tests.test_covariance.test_ledoit_wolf_small()
sklearn.covariance.tests.test_covariance.test_oas()
sklearn.covariance.tests.test_covariance.test_shrunk_covariance()
sklearn.covariance.tests.test_covariance.test_shrunk_covariance_func(n_matrices)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py----------------------------------------
A:sklearn.compose._column_transformer.transformers->chain(self.transformers, [('remainder', self.remainder, '')])
A:sklearn.compose._column_transformer.columns_is_scalar->numpy.isscalar(columns)
A:sklearn.compose._column_transformer.(names, transformers, _)->zip(*self.transformers)
A:sklearn.compose._column_transformer.columns->columns(X)
A:sklearn.compose._column_transformer.transformer_to_input_indices[name]->_get_column_indices(X, columns)
A:sklearn.compose._column_transformer.cols->set(chain(*self._transformer_to_input_indices.values()))
A:sklearn.compose._column_transformer.remaining->sorted(set(range(self.n_features_in_)) - cols)
A:sklearn.compose._column_transformer.input_features->_check_feature_names_in(self, input_features)
A:sklearn.compose._column_transformer.feature_names_out->self._get_feature_name_out_for_transformer(name, trans, input_features)
A:sklearn.compose._column_transformer.names->list(chain.from_iterable(((f'{name}__{i}' for i in feature_names_out) for (name, feature_names_out) in transformer_with_feature_names_out)))
A:sklearn.compose._column_transformer.feature_names_count->Counter(chain.from_iterable((s for (_, s) in transformer_with_feature_names_out)))
A:sklearn.compose._column_transformer.names_repr->str(top_6_overlap)
A:sklearn.compose._column_transformer.fitted_transformers->iter(transformers)
A:sklearn.compose._column_transformer.trans->FunctionTransformer(accept_sparse=True, check_inverse=False, feature_names_out='one-to-one').set_output(transform=output_config['dense'])
A:sklearn.compose._column_transformer.self.output_indices_[name]->slice(0, 0)
A:sklearn.compose._column_transformer.output_config->_get_output_config('transform', self)
A:sklearn.compose._column_transformer.extra_args->dict(message_clsname='ColumnTransformer', message=self._log_message(name, idx, len(transformers)))
A:sklearn.compose._column_transformer.X->_check_X(X)
A:sklearn.compose._column_transformer.n_samples->_num_samples(X)
A:sklearn.compose._column_transformer.routed_params->self._get_empty_routing()
A:sklearn.compose._column_transformer.result->self._call_func_on_transformers(X, y, _fit_transform_one, column_as_labels=False, routed_params=routed_params)
A:sklearn.compose._column_transformer.(Xs, transformers)->zip(*result)
A:sklearn.compose._column_transformer.nnz->sum((X.nnz if sparse.issparse(X) else X.size for X in Xs))
A:sklearn.compose._column_transformer.total->sum((X.shape[0] * X.shape[1] if sparse.issparse(X) else X.size for X in Xs))
A:sklearn.compose._column_transformer.column_names->_get_feature_names(X)
A:sklearn.compose._column_transformer.all_indices->set(chain(*non_dropped_indices))
A:sklearn.compose._column_transformer.all_names->set((self.feature_names_in_[ind] for ind in all_indices))
A:sklearn.compose._column_transformer.Xs->self._call_func_on_transformers(X, None, _transform_one, column_as_labels=fit_dataframe_and_transform_dataframe, routed_params=routed_params)
A:sklearn.compose._column_transformer.adapter->_get_container_adapter('transform', self)
A:sklearn.compose._column_transformer.output->_get_container_adapter('transform', self).hstack(Xs)
A:sklearn.compose._column_transformer.names_out->self._add_prefix_for_feature_names_out(list(zip(transformer_names, feature_names_outs)))
A:sklearn.compose._column_transformer.remainder_columns->self.feature_names_in_[remainder_columns].tolist()
A:sklearn.compose._column_transformer.(names, transformers, name_details)->zip(*transformers)
A:sklearn.compose._column_transformer.router->MetadataRouter(owner=self.__class__.__name__)
A:sklearn.compose._column_transformer.method_mapping->MethodMapping()
A:sklearn.compose._column_transformer.(transformers, columns)->zip(*estimators)
A:sklearn.compose._column_transformer.(names, _)->zip(*_name_estimators(transformers))
A:sklearn.compose._column_transformer.transformer_list->_get_transformer_list(transformers)
A:sklearn.compose._column_transformer.df_row->df_row.select_dtypes(include=self.dtype_include, exclude=self.dtype_exclude).select_dtypes(include=self.dtype_include, exclude=self.dtype_exclude)
sklearn.compose.ColumnTransformer(self,transformers,*,remainder='drop',sparse_threshold=0.3,n_jobs=None,transformer_weights=None,verbose=False,verbose_feature_names_out=True)
sklearn.compose.ColumnTransformer._add_prefix_for_feature_names_out(self,transformer_with_feature_names_out)
sklearn.compose.ColumnTransformer._call_func_on_transformers(self,X,y,func,column_as_labels,routed_params)
sklearn.compose.ColumnTransformer._get_empty_routing(self)
sklearn.compose.ColumnTransformer._get_feature_name_out_for_transformer(self,name,trans,feature_names_in)
sklearn.compose.ColumnTransformer._hstack(self,Xs)
sklearn.compose.ColumnTransformer._iter(self,fitted,column_as_labels,skip_drop,skip_empty_columns)
sklearn.compose.ColumnTransformer._log_message(self,name,idx,total)
sklearn.compose.ColumnTransformer._record_output_indices(self,Xs)
sklearn.compose.ColumnTransformer._sk_visual_block_(self)
sklearn.compose.ColumnTransformer._transformers(self)
sklearn.compose.ColumnTransformer._transformers(self,value)
sklearn.compose.ColumnTransformer._update_fitted_transformers(self,transformers)
sklearn.compose.ColumnTransformer._validate_column_callables(self,X)
sklearn.compose.ColumnTransformer._validate_output(self,result)
sklearn.compose.ColumnTransformer._validate_remainder(self,X)
sklearn.compose.ColumnTransformer._validate_transformers(self)
sklearn.compose.ColumnTransformer.fit(self,X,y=None,**params)
sklearn.compose.ColumnTransformer.fit_transform(self,X,y=None,**params)
sklearn.compose.ColumnTransformer.get_feature_names_out(self,input_features=None)
sklearn.compose.ColumnTransformer.get_metadata_routing(self)
sklearn.compose.ColumnTransformer.get_params(self,deep=True)
sklearn.compose.ColumnTransformer.named_transformers_(self)
sklearn.compose.ColumnTransformer.set_output(self,*,transform=None)
sklearn.compose.ColumnTransformer.set_params(self,**kwargs)
sklearn.compose.ColumnTransformer.transform(self,X,**params)
sklearn.compose._column_transformer.ColumnTransformer(self,transformers,*,remainder='drop',sparse_threshold=0.3,n_jobs=None,transformer_weights=None,verbose=False,verbose_feature_names_out=True)
sklearn.compose._column_transformer.ColumnTransformer.__init__(self,transformers,*,remainder='drop',sparse_threshold=0.3,n_jobs=None,transformer_weights=None,verbose=False,verbose_feature_names_out=True)
sklearn.compose._column_transformer.ColumnTransformer._add_prefix_for_feature_names_out(self,transformer_with_feature_names_out)
sklearn.compose._column_transformer.ColumnTransformer._call_func_on_transformers(self,X,y,func,column_as_labels,routed_params)
sklearn.compose._column_transformer.ColumnTransformer._get_empty_routing(self)
sklearn.compose._column_transformer.ColumnTransformer._get_feature_name_out_for_transformer(self,name,trans,feature_names_in)
sklearn.compose._column_transformer.ColumnTransformer._hstack(self,Xs)
sklearn.compose._column_transformer.ColumnTransformer._iter(self,fitted,column_as_labels,skip_drop,skip_empty_columns)
sklearn.compose._column_transformer.ColumnTransformer._log_message(self,name,idx,total)
sklearn.compose._column_transformer.ColumnTransformer._record_output_indices(self,Xs)
sklearn.compose._column_transformer.ColumnTransformer._sk_visual_block_(self)
sklearn.compose._column_transformer.ColumnTransformer._transformers(self)
sklearn.compose._column_transformer.ColumnTransformer._transformers(self,value)
sklearn.compose._column_transformer.ColumnTransformer._update_fitted_transformers(self,transformers)
sklearn.compose._column_transformer.ColumnTransformer._validate_column_callables(self,X)
sklearn.compose._column_transformer.ColumnTransformer._validate_output(self,result)
sklearn.compose._column_transformer.ColumnTransformer._validate_remainder(self,X)
sklearn.compose._column_transformer.ColumnTransformer._validate_transformers(self)
sklearn.compose._column_transformer.ColumnTransformer.fit(self,X,y=None,**params)
sklearn.compose._column_transformer.ColumnTransformer.fit_transform(self,X,y=None,**params)
sklearn.compose._column_transformer.ColumnTransformer.get_feature_names_out(self,input_features=None)
sklearn.compose._column_transformer.ColumnTransformer.get_metadata_routing(self)
sklearn.compose._column_transformer.ColumnTransformer.get_params(self,deep=True)
sklearn.compose._column_transformer.ColumnTransformer.named_transformers_(self)
sklearn.compose._column_transformer.ColumnTransformer.set_output(self,*,transform=None)
sklearn.compose._column_transformer.ColumnTransformer.set_params(self,**kwargs)
sklearn.compose._column_transformer.ColumnTransformer.transform(self,X,**params)
sklearn.compose._column_transformer._check_X(X)
sklearn.compose._column_transformer._get_transformer_list(estimators)
sklearn.compose._column_transformer._is_empty_column_selection(column)
sklearn.compose._column_transformer.make_column_selector(self,pattern=None,*,dtype_include=None,dtype_exclude=None)
sklearn.compose._column_transformer.make_column_selector.__init__(self,pattern=None,*,dtype_include=None,dtype_exclude=None)
sklearn.compose._column_transformer.make_column_transformer(*transformers,remainder='drop',sparse_threshold=0.3,n_jobs=None,verbose=False,verbose_feature_names_out=True)
sklearn.compose.make_column_selector(self,pattern=None,*,dtype_include=None,dtype_exclude=None)
sklearn.compose.make_column_transformer(*transformers,remainder='drop',sparse_threshold=0.3,n_jobs=None,verbose=False,verbose_feature_names_out=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/_target.py----------------------------------------
A:sklearn.compose._target.self.transformer_->FunctionTransformer(func=self.func, inverse_func=self.inverse_func, validate=True, check_inverse=self.check_inverse)
A:sklearn.compose._target.idx_selected->slice(None, None, max(1, y.shape[0] // 10))
A:sklearn.compose._target.y_sel->_safe_indexing(y, idx_selected)
A:sklearn.compose._target.y_sel_t->self.transformer_.transform(y_sel)
A:sklearn.compose._target.y->check_array(y, input_name='y', accept_sparse=False, force_all_finite=True, ensure_2d=False, dtype='numeric', allow_nd=True)
A:sklearn.compose._target.y_2d->check_array(y, input_name='y', accept_sparse=False, force_all_finite=True, ensure_2d=False, dtype='numeric', allow_nd=True).reshape(-1, 1)
A:sklearn.compose._target.y_trans->y_trans.squeeze(axis=1).squeeze(axis=1)
A:sklearn.compose._target.self.regressor_->clone(self.regressor)
A:sklearn.compose._target.pred->self.regressor_.predict(X, **predict_params)
A:sklearn.compose._target.pred_trans->pred_trans.squeeze(axis=1).squeeze(axis=1)
A:sklearn.compose._target.regressor->LinearRegression()
sklearn.compose.TransformedTargetRegressor(self,regressor=None,*,transformer=None,func=None,inverse_func=None,check_inverse=True)
sklearn.compose.TransformedTargetRegressor._fit_transformer(self,y)
sklearn.compose.TransformedTargetRegressor._more_tags(self)
sklearn.compose.TransformedTargetRegressor.fit(self,X,y,**fit_params)
sklearn.compose.TransformedTargetRegressor.n_features_in_(self)
sklearn.compose.TransformedTargetRegressor.predict(self,X,**predict_params)
sklearn.compose._target.TransformedTargetRegressor(self,regressor=None,*,transformer=None,func=None,inverse_func=None,check_inverse=True)
sklearn.compose._target.TransformedTargetRegressor.__init__(self,regressor=None,*,transformer=None,func=None,inverse_func=None,check_inverse=True)
sklearn.compose._target.TransformedTargetRegressor._fit_transformer(self,y)
sklearn.compose._target.TransformedTargetRegressor._more_tags(self)
sklearn.compose._target.TransformedTargetRegressor.fit(self,X,y,**fit_params)
sklearn.compose._target.TransformedTargetRegressor.n_features_in_(self)
sklearn.compose._target.TransformedTargetRegressor.predict(self,X,**predict_params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/tests/test_target.py----------------------------------------
A:sklearn.compose.tests.test_target.friedman->sklearn.datasets.make_friedman1(random_state=0)
A:sklearn.compose.tests.test_target.regr->TransformedTargetRegressor(regressor=DummyRegressorWithExtraPredictParams(), transformer=DummyTransformer())
A:sklearn.compose.tests.test_target.sample_weight->numpy.ones((y.shape[0],))
A:sklearn.compose.tests.test_target.y_mean->numpy.mean(y, axis=0)
A:sklearn.compose.tests.test_target.y_std->numpy.std(y, axis=0)
A:sklearn.compose.tests.test_target.y_pred->TransformedTargetRegressor(regressor=DummyRegressorWithExtraPredictParams(), transformer=DummyTransformer()).fit(X, y).predict(X)
A:sklearn.compose.tests.test_target.y_tran->TransformedTargetRegressor(regressor=DummyRegressorWithExtraPredictParams(), transformer=DummyTransformer()).transformer_.transform(y)
A:sklearn.compose.tests.test_target.lr->LinearRegression()
A:sklearn.compose.tests.test_target.transformer->FunctionTransformer(func=flatten_data, inverse_func=unflatten_data)
A:sklearn.compose.tests.test_target.transformer2->clone(transformer)
A:sklearn.compose.tests.test_target.y_lr_pred->LinearRegression().predict(X)
A:sklearn.compose.tests.test_target.y_pred2->clone(transformer).inverse_transform(y_lr_pred)
A:sklearn.compose.tests.test_target.y->numpy.transpose([friedman[1], friedman[1] ** 2 + 1])
A:sklearn.compose.tests.test_target.out->numpy.sqrt(y[:, 0] ** 2 + y[:, 1] ** 2)
A:sklearn.compose.tests.test_target.tt->TransformedTargetRegressor(transformer=DummyCheckerArrayTransformer(), regressor=DummyCheckerListRegressor(), check_inverse=False)
A:sklearn.compose.tests.test_target.y_pred_2d_func->TransformedTargetRegressor(transformer=DummyCheckerArrayTransformer(), regressor=DummyCheckerListRegressor(), check_inverse=False).predict(X)
A:sklearn.compose.tests.test_target.y_pred_1d_func->TransformedTargetRegressor(transformer=DummyCheckerArrayTransformer(), regressor=DummyCheckerListRegressor(), check_inverse=False).predict(X)
A:sklearn.compose.tests.test_target.ttr->TransformedTargetRegressor(transformer=DummyTransformer(), check_inverse=check_inverse)
A:sklearn.compose.tests.test_target.pip->Pipeline(estimators)
sklearn.compose.tests.test_target.DummyCheckerArrayTransformer(TransformerMixin,BaseEstimator)
sklearn.compose.tests.test_target.DummyCheckerArrayTransformer.fit(self,X,y=None)
sklearn.compose.tests.test_target.DummyCheckerArrayTransformer.inverse_transform(self,X)
sklearn.compose.tests.test_target.DummyCheckerArrayTransformer.transform(self,X)
sklearn.compose.tests.test_target.DummyCheckerListRegressor(DummyRegressor)
sklearn.compose.tests.test_target.DummyCheckerListRegressor.fit(self,X,y,sample_weight=None)
sklearn.compose.tests.test_target.DummyCheckerListRegressor.predict(self,X)
sklearn.compose.tests.test_target.DummyRegressorWithExtraFitParams(DummyRegressor)
sklearn.compose.tests.test_target.DummyRegressorWithExtraFitParams.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.compose.tests.test_target.DummyRegressorWithExtraPredictParams(DummyRegressor)
sklearn.compose.tests.test_target.DummyRegressorWithExtraPredictParams.predict(self,X,check_input=True)
sklearn.compose.tests.test_target.DummyTransformer(self,fit_counter=0)
sklearn.compose.tests.test_target.DummyTransformer.__init__(self,fit_counter=0)
sklearn.compose.tests.test_target.DummyTransformer.fit(self,X,y=None)
sklearn.compose.tests.test_target.DummyTransformer.inverse_transform(self,X)
sklearn.compose.tests.test_target.DummyTransformer.transform(self,X)
sklearn.compose.tests.test_target._check_shifted_by_one(y,y_pred)
sklearn.compose.tests.test_target._check_standard_scaled(y,y_pred)
sklearn.compose.tests.test_target.test_transform_target_regressor_1d_transformer(X,y)
sklearn.compose.tests.test_target.test_transform_target_regressor_2d_transformer(X,y)
sklearn.compose.tests.test_target.test_transform_target_regressor_2d_transformer_multioutput()
sklearn.compose.tests.test_target.test_transform_target_regressor_3d_target()
sklearn.compose.tests.test_target.test_transform_target_regressor_count_fit(check_inverse)
sklearn.compose.tests.test_target.test_transform_target_regressor_ensure_y_array()
sklearn.compose.tests.test_target.test_transform_target_regressor_error()
sklearn.compose.tests.test_target.test_transform_target_regressor_functions()
sklearn.compose.tests.test_target.test_transform_target_regressor_functions_multioutput()
sklearn.compose.tests.test_target.test_transform_target_regressor_invertible()
sklearn.compose.tests.test_target.test_transform_target_regressor_multi_to_single()
sklearn.compose.tests.test_target.test_transform_target_regressor_pass_extra_predict_parameters()
sklearn.compose.tests.test_target.test_transform_target_regressor_pass_fit_parameters()
sklearn.compose.tests.test_target.test_transform_target_regressor_route_pipeline()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/compose/tests/test_column_transformer.py----------------------------------------
A:sklearn.compose.tests.test_column_transformer.n_samples->len(X)
A:sklearn.compose.tests.test_column_transformer.X_res_first1D->numpy.array([0, 1, 2])
A:sklearn.compose.tests.test_column_transformer.X_res_second1D->numpy.array([2, 4, 6])
A:sklearn.compose.tests.test_column_transformer.X_res_first->numpy.array([0, 1, 2]).reshape(-1, 1)
A:sklearn.compose.tests.test_column_transformer.ct->make_column_transformer((Trans(), [0, 1]))
A:sklearn.compose.tests.test_column_transformer.both->ColumnTransformer([('trans', Trans(), [0, 1])], transformer_weights={'trans': 0.1})
A:sklearn.compose.tests.test_column_transformer.ct_with_list->ColumnTransformer(transformers)
A:sklearn.compose.tests.test_column_transformer.ct_with_tuple->ColumnTransformer(tuple(transformers))
A:sklearn.compose.tests.test_column_transformer.dataframe_lib->pytest.importorskip(constructor_name)
A:sklearn.compose.tests.test_column_transformer.X_df->pytest.importorskip('pandas').DataFrame({'feat0': [1.0, 2.0, 3.0], 'feat1': [2.0, 3.0, 4.0]})
A:sklearn.compose.tests.test_column_transformer.X->pytest.importorskip('pandas').DataFrame([[1.0, 2.2], [3.0, 1.0]], columns=['a', 'b'], index=[8, 3])
A:sklearn.compose.tests.test_column_transformer.X_df2->pytest.importorskip('pandas').DataFrame({'feat0': [1.0, 2.0, 3.0], 'feat1': [2.0, 3.0, 4.0]}).copy()
A:sklearn.compose.tests.test_column_transformer.pd->pytest.importorskip('pandas')
A:sklearn.compose.tests.test_column_transformer.fixture->numpy.array([[], [], []])
A:sklearn.compose.tests.test_column_transformer.X_array->X_fit_array.copy()
A:sklearn.compose.tests.test_column_transformer.X_trans->make_column_transformer((Trans(), [0, 1])).fit_transform(df)
A:sklearn.compose.tests.test_column_transformer.X_sparse->csr_container(sparse.eye(3, 2))
A:sklearn.compose.tests.test_column_transformer.expected_result->numpy.array([[1, float('nan'), 1, 0], [-1, 0, 0, 1]])
A:sklearn.compose.tests.test_column_transformer.col_trans->ColumnTransformer([('trans', TransRaise(), 0)])
A:sklearn.compose.tests.test_column_transformer.df->df.convert_dtypes().convert_dtypes()
A:sklearn.compose.tests.test_column_transformer.res->ColumnTransformer([('trans', TransRaise(), 0)]).fit_transform(X_array)
A:sklearn.compose.tests.test_column_transformer.scaler->StandardScaler()
A:sklearn.compose.tests.test_column_transformer.norm->Normalizer()
A:sklearn.compose.tests.test_column_transformer.(names, transformers, columns)->zip(*ct.transformers)
A:sklearn.compose.tests.test_column_transformer.ct1->ColumnTransformer([('norm', Normalizer(), X_df.columns)])
A:sklearn.compose.tests.test_column_transformer.ct2->make_column_transformer((norm, X_df.columns))
A:sklearn.compose.tests.test_column_transformer.msg->re.escape(f'Output feature names: {colliding_columns} are not unique. Please set verbose_feature_names_out=True to add prefixes to feature names')
A:sklearn.compose.tests.test_column_transformer.remainder->StandardScaler()
A:sklearn.compose.tests.test_column_transformer.exp->numpy.array([[0.0], [1.0], [2.0]])
A:sklearn.compose.tests.test_column_transformer.X_res_second->numpy.array([2, 4, 6]).reshape(-1, 1)
A:sklearn.compose.tests.test_column_transformer.key->pytest.importorskip('pandas').Index(['first'])
A:sklearn.compose.tests.test_column_transformer.X_res_both->X_fit_array.copy().copy()
A:sklearn.compose.tests.test_column_transformer.exp_array->numpy.hstack((X_array[:, 0].reshape(-1, 1), np.eye(3)))
A:sklearn.compose.tests.test_column_transformer.params->make_column_transformer((Trans(), [0, 1])).get_params()
A:sklearn.compose.tests.test_column_transformer.func->getattr(est, method)
A:sklearn.compose.tests.test_column_transformer.X_categories->numpy.array([[1], [2]])
A:sklearn.compose.tests.test_column_transformer.ohe->OneHotEncoder()
A:sklearn.compose.tests.test_column_transformer.tf_1->ColumnTransformer([('ohe', ohe, [-1])], remainder='passthrough')
A:sklearn.compose.tests.test_column_transformer.tf_2->ColumnTransformer([('ohe', ohe, [2])], remainder='passthrough')
A:sklearn.compose.tests.test_column_transformer.column_transformer->ColumnTransformer([('identity', FunctionTransformer(), [False, True, False, True])])
A:sklearn.compose.tests.test_column_transformer.selector->make_column_selector(dtype_include=[object])
A:sklearn.compose.tests.test_column_transformer.X_df['col_str']->X_df['col_str'].astype('category').astype('category')
A:sklearn.compose.tests.test_column_transformer.cat_selector->make_column_selector(dtype_include=['category', object])
A:sklearn.compose.tests.test_column_transformer.num_selector->make_column_selector(dtype_include=np.number)
A:sklearn.compose.tests.test_column_transformer.ct_selector->make_column_transformer((ohe, cat_selector), (scaler, num_selector))
A:sklearn.compose.tests.test_column_transformer.ct_direct->make_column_transformer((ohe, ['col_cat', 'col_str']), (scaler, ['col_float', 'col_int']))
A:sklearn.compose.tests.test_column_transformer.X_selector->make_column_transformer((ohe, cat_selector), (scaler, num_selector)).fit_transform(X_df)
A:sklearn.compose.tests.test_column_transformer.X_direct->make_column_transformer((ohe, ['col_cat', 'col_str']), (scaler, ['col_float', 'col_int'])).fit_transform(X_df)
A:sklearn.compose.tests.test_column_transformer.selector_picked->pickle.loads(pickle.dumps(selector))
A:sklearn.compose.tests.test_column_transformer.visual_block->make_column_transformer((Trans(), [0, 1]))._sk_visual_block_()
A:sklearn.compose.tests.test_column_transformer.X_fit_df->pytest.importorskip('pandas').DataFrame(X_fit_array, columns=['first', 'second'])
A:sklearn.compose.tests.test_column_transformer.X_trans_df->make_column_transformer((Trans(), [0, 1])).transform(df)
A:sklearn.compose.tests.test_column_transformer.tf->ColumnTransformer([('bycol', 'drop', ['c'])], remainder='passthrough')
A:sklearn.compose.tests.test_column_transformer.X_fit_trans->ColumnTransformer([('bycol', 'drop', ['c'])], remainder='passthrough').transform(X_fit_df)
A:sklearn.compose.tests.test_column_transformer.X_extended_df->pytest.importorskip('pandas').DataFrame(X_fit_array, columns=['first', 'second']).copy()
A:sklearn.compose.tests.test_column_transformer.df_dropped->df.convert_dtypes().convert_dtypes().drop('c', axis=1)
A:sklearn.compose.tests.test_column_transformer.df_dropped_trans->ColumnTransformer([('bycol', 'drop', ['c'])], remainder='passthrough').transform(df_dropped)
A:sklearn.compose.tests.test_column_transformer.df_fit_trans->ColumnTransformer([('bycol', 'drop', ['c'])], remainder='passthrough').transform(df)
A:sklearn.compose.tests.test_column_transformer.names->make_column_transformer((Trans(), [0, 1])).get_feature_names_out()
A:sklearn.compose.tests.test_column_transformer.df_test->pytest.importorskip('pandas').DataFrame([[1, 2, 3, 4]], columns=df.columns, index=[20])
A:sklearn.compose.tests.test_column_transformer.feature_names_out->make_column_transformer((Trans(), [0, 1])).get_feature_names_out()
A:sklearn.compose.tests.test_column_transformer.X_trans_np->make_column_transformer((Trans(), [0, 1])).fit_transform(X_df)
A:sklearn.compose.tests.test_column_transformer.X_trans_df0->make_column_transformer((Trans(), [0, 1])).fit_transform(X_df)
A:sklearn.compose.tests.test_column_transformer.X_trans_df1->make_column_transformer((Trans(), [0, 1])).fit_transform(X_df)
A:sklearn.compose.tests.test_column_transformer.X_out->make_column_transformer((Trans(), [0, 1])).fit_transform(X)
A:sklearn.compose.tests.test_column_transformer.reset_index_transformer->FunctionTransformer(lambda x: x.reset_index(drop=True), feature_names_out='one-to-one')
A:sklearn.compose.tests.test_column_transformer.out->make_column_transformer((Trans(), [0, 1])).fit_transform(df)
A:sklearn.compose.tests.test_column_transformer.pl->pytest.importorskip('polars')
A:sklearn.compose.tests.test_column_transformer.X_train_np->numpy.array([[0, 1], [2, 4], [4, 5]])
A:sklearn.compose.tests.test_column_transformer.X_test_np->numpy.array([[1, 2], [1, 3], [2, 3]])
A:sklearn.compose.tests.test_column_transformer.X_train_pd->pytest.importorskip('pandas').DataFrame(X_train_np, columns=['a', 'b'])
A:sklearn.compose.tests.test_column_transformer.X_test_pl->pytest.importorskip('polars').DataFrame(X_test_np, schema=['a', 'b'])
A:sklearn.compose.tests.test_column_transformer.out_pl_in->make_column_transformer((Trans(), [0, 1])).transform(X_test_pl)
A:sklearn.compose.tests.test_column_transformer.X_train_pl->pytest.importorskip('polars').DataFrame(X_train_np, schema=['a', 'b'])
A:sklearn.compose.tests.test_column_transformer.X_test_pd->pytest.importorskip('pandas').DataFrame(X_test_np, columns=['a', 'b'])
A:sklearn.compose.tests.test_column_transformer.out_pd_in->make_column_transformer((Trans(), [0, 1])).transform(X_test_pd)
A:sklearn.compose.tests.test_column_transformer.trs->ColumnTransformer([('trans', ConsumingTransformer(), [0])])
A:sklearn.compose.tests.test_column_transformer.registry->_Registry()
sklearn.compose.tests.test_column_transformer.DoubleTrans(BaseEstimator)
sklearn.compose.tests.test_column_transformer.DoubleTrans.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.DoubleTrans.transform(self,X)
sklearn.compose.tests.test_column_transformer.PandasOutTransformer(self,offset=1.0)
sklearn.compose.tests.test_column_transformer.PandasOutTransformer.__init__(self,offset=1.0)
sklearn.compose.tests.test_column_transformer.PandasOutTransformer.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.PandasOutTransformer.set_output(self,transform=None)
sklearn.compose.tests.test_column_transformer.PandasOutTransformer.transform(self,X,y=None)
sklearn.compose.tests.test_column_transformer.SparseMatrixTrans(self,csr_container)
sklearn.compose.tests.test_column_transformer.SparseMatrixTrans.__init__(self,csr_container)
sklearn.compose.tests.test_column_transformer.SparseMatrixTrans.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.SparseMatrixTrans.transform(self,X,y=None)
sklearn.compose.tests.test_column_transformer.Trans(TransformerMixin,BaseEstimator)
sklearn.compose.tests.test_column_transformer.Trans.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.Trans.transform(self,X,y=None)
sklearn.compose.tests.test_column_transformer.TransNo2D(BaseEstimator)
sklearn.compose.tests.test_column_transformer.TransNo2D.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.TransNo2D.transform(self,X,y=None)
sklearn.compose.tests.test_column_transformer.TransRaise(BaseEstimator)
sklearn.compose.tests.test_column_transformer.TransRaise.fit(self,X,y=None)
sklearn.compose.tests.test_column_transformer.TransRaise.transform(self,X,y=None)
sklearn.compose.tests.test_column_transformer.TransWithNames(self,feature_names_out=None)
sklearn.compose.tests.test_column_transformer.TransWithNames.__init__(self,feature_names_out=None)
sklearn.compose.tests.test_column_transformer.TransWithNames.get_feature_names_out(self,input_features=None)
sklearn.compose.tests.test_column_transformer.test_2D_transformer_output()
sklearn.compose.tests.test_column_transformer.test_2D_transformer_output_pandas()
sklearn.compose.tests.test_column_transformer.test_column_transform_set_output_after_fitting(remainder)
sklearn.compose.tests.test_column_transformer.test_column_transform_set_output_mixed(remainder,fit_transform)
sklearn.compose.tests.test_column_transformer.test_column_transformer()
sklearn.compose.tests.test_column_transformer.test_column_transformer_callable_specifier()
sklearn.compose.tests.test_column_transformer.test_column_transformer_callable_specifier_dataframe()
sklearn.compose.tests.test_column_transformer.test_column_transformer_cloning()
sklearn.compose.tests.test_column_transformer.test_column_transformer_dataframe(constructor_name)
sklearn.compose.tests.test_column_transformer.test_column_transformer_drop_all_sparse_remainder_transformer(csr_container)
sklearn.compose.tests.test_column_transformer.test_column_transformer_drops_all_remainder_transformer()
sklearn.compose.tests.test_column_transformer.test_column_transformer_empty_columns(pandas,column_selection,callable_column)
sklearn.compose.tests.test_column_transformer.test_column_transformer_error_msg_1D()
sklearn.compose.tests.test_column_transformer.test_column_transformer_get_feature_names()
sklearn.compose.tests.test_column_transformer.test_column_transformer_get_set_params()
sklearn.compose.tests.test_column_transformer.test_column_transformer_get_set_params_with_remainder()
sklearn.compose.tests.test_column_transformer.test_column_transformer_invalid_columns(remainder)
sklearn.compose.tests.test_column_transformer.test_column_transformer_invalid_transformer()
sklearn.compose.tests.test_column_transformer.test_column_transformer_list()
sklearn.compose.tests.test_column_transformer.test_column_transformer_mask_indexing(array_type)
sklearn.compose.tests.test_column_transformer.test_column_transformer_mixed_cols_sparse()
sklearn.compose.tests.test_column_transformer.test_column_transformer_named_estimators()
sklearn.compose.tests.test_column_transformer.test_column_transformer_negative_column_indexes()
sklearn.compose.tests.test_column_transformer.test_column_transformer_no_estimators()
sklearn.compose.tests.test_column_transformer.test_column_transformer_no_estimators_set_params()
sklearn.compose.tests.test_column_transformer.test_column_transformer_no_remaining_remainder_transformer()
sklearn.compose.tests.test_column_transformer.test_column_transformer_output_indices()
sklearn.compose.tests.test_column_transformer.test_column_transformer_output_indices_df()
sklearn.compose.tests.test_column_transformer.test_column_transformer_remainder()
sklearn.compose.tests.test_column_transformer.test_column_transformer_remainder_numpy(key)
sklearn.compose.tests.test_column_transformer.test_column_transformer_remainder_pandas(key)
sklearn.compose.tests.test_column_transformer.test_column_transformer_remainder_transformer(key)
sklearn.compose.tests.test_column_transformer.test_column_transformer_reordered_column_names_remainder(explicit_colname,remainder)
sklearn.compose.tests.test_column_transformer.test_column_transformer_set_output(verbose_feature_names_out,remainder)
sklearn.compose.tests.test_column_transformer.test_column_transformer_sparse_array(csr_container)
sklearn.compose.tests.test_column_transformer.test_column_transformer_sparse_remainder_transformer(csr_container)
sklearn.compose.tests.test_column_transformer.test_column_transformer_sparse_stacking(csr_container)
sklearn.compose.tests.test_column_transformer.test_column_transformer_sparse_threshold()
sklearn.compose.tests.test_column_transformer.test_column_transformer_special_strings()
sklearn.compose.tests.test_column_transformer.test_column_transformer_tuple_transformers_parameter()
sklearn.compose.tests.test_column_transformer.test_column_transformer_verbose(est,pattern,method,capsys)
sklearn.compose.tests.test_column_transformer.test_column_transformer_with_make_column_selector()
sklearn.compose.tests.test_column_transformer.test_dataframe_different_dataframe_libraries()
sklearn.compose.tests.test_column_transformer.test_empty_selection_pandas_output(empty_selection)
sklearn.compose.tests.test_column_transformer.test_feature_name_validation_missing_columns_drop_passthough()
sklearn.compose.tests.test_column_transformer.test_feature_names_empty_columns(empty_col)
sklearn.compose.tests.test_column_transformer.test_feature_names_in_()
sklearn.compose.tests.test_column_transformer.test_feature_names_out_non_pandas(selector)
sklearn.compose.tests.test_column_transformer.test_feature_names_out_pandas(selector)
sklearn.compose.tests.test_column_transformer.test_make_column_selector_error()
sklearn.compose.tests.test_column_transformer.test_make_column_selector_pickle()
sklearn.compose.tests.test_column_transformer.test_make_column_selector_with_select_dtypes(cols,pattern,include,exclude)
sklearn.compose.tests.test_column_transformer.test_make_column_transformer()
sklearn.compose.tests.test_column_transformer.test_make_column_transformer_kwargs()
sklearn.compose.tests.test_column_transformer.test_make_column_transformer_pandas()
sklearn.compose.tests.test_column_transformer.test_make_column_transformer_remainder_transformer()
sklearn.compose.tests.test_column_transformer.test_metadata_routing_error_for_column_transformer(method)
sklearn.compose.tests.test_column_transformer.test_metadata_routing_for_column_transformer(method)
sklearn.compose.tests.test_column_transformer.test_metadata_routing_no_fit_transform()
sklearn.compose.tests.test_column_transformer.test_n_features_in()
sklearn.compose.tests.test_column_transformer.test_raise_error_if_index_not_aligned()
sklearn.compose.tests.test_column_transformer.test_remainder_set_output()
sklearn.compose.tests.test_column_transformer.test_routing_passed_metadata_not_supported(method)
sklearn.compose.tests.test_column_transformer.test_sk_visual_block_remainder(remainder)
sklearn.compose.tests.test_column_transformer.test_sk_visual_block_remainder_drop()
sklearn.compose.tests.test_column_transformer.test_sk_visual_block_remainder_fitted_numpy(remainder)
sklearn.compose.tests.test_column_transformer.test_sk_visual_block_remainder_fitted_pandas(remainder)
sklearn.compose.tests.test_column_transformer.test_transform_pd_na()
sklearn.compose.tests.test_column_transformer.test_transformers_with_pandas_out_but_not_feature_names_out(trans_1,expected_verbose_names,expected_non_verbose_names)
sklearn.compose.tests.test_column_transformer.test_verbose_feature_names_out_false(transformers,remainder,expected_names)
sklearn.compose.tests.test_column_transformer.test_verbose_feature_names_out_false_errors(transformers,remainder,colliding_columns)
sklearn.compose.tests.test_column_transformer.test_verbose_feature_names_out_true(transformers,remainder,expected_names)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/image.py----------------------------------------
A:sklearn.feature_extraction.image.vertices->numpy.arange(n_x * n_y * n_z).reshape((n_x, n_y, n_z))
A:sklearn.feature_extraction.image.edges_deep->numpy.vstack((vertices[:, :, :-1].ravel(), vertices[:, :, 1:].ravel()))
A:sklearn.feature_extraction.image.edges_right->numpy.vstack((vertices[:, :-1].ravel(), vertices[:, 1:].ravel()))
A:sklearn.feature_extraction.image.edges_down->numpy.vstack((vertices[:-1].ravel(), vertices[1:].ravel()))
A:sklearn.feature_extraction.image.edges->_mask_edges_weights(mask, edges)
A:sklearn.feature_extraction.image.gradient->numpy.abs(img[edges[0] // (n_y * n_z), edges[0] % (n_y * n_z) // n_z, edges[0] % (n_y * n_z) % n_z] - img[edges[1] // (n_y * n_z), edges[1] % (n_y * n_z) // n_z, edges[1] % (n_y * n_z) % n_z])
A:sklearn.feature_extraction.image.inds->numpy.arange(mask.size)
A:sklearn.feature_extraction.image.ind_mask->numpy.logical_and(np.isin(edges[0], inds), np.isin(edges[1], inds))
A:sklearn.feature_extraction.image.maxval->_mask_edges_weights(mask, edges).max()
A:sklearn.feature_extraction.image.order->numpy.searchsorted(np.flatnonzero(mask), np.arange(maxval + 1))
A:sklearn.feature_extraction.image.img->numpy.zeros(image_size)
A:sklearn.feature_extraction.image.weights->numpy.ones(edges.shape[1], dtype=dtype)
A:sklearn.feature_extraction.image.(edges, weights)->_mask_edges_weights(mask, edges, weights)
A:sklearn.feature_extraction.image.diag->numpy.ones(n_voxels, dtype=dtype)
A:sklearn.feature_extraction.image.mask->mask.astype(dtype=bool, copy=False).astype(dtype=bool, copy=False)
A:sklearn.feature_extraction.image.n_voxels->numpy.sum(mask)
A:sklearn.feature_extraction.image.diag_idx->numpy.arange(n_voxels)
A:sklearn.feature_extraction.image.i_idx->numpy.hstack((edges[0], edges[1]))
A:sklearn.feature_extraction.image.j_idx->numpy.hstack((edges[1], edges[0]))
A:sklearn.feature_extraction.image.graph->scipy.sparse.coo_matrix((np.hstack((weights, weights, diag)), (np.hstack((i_idx, diag_idx)), np.hstack((j_idx, diag_idx)))), (n_voxels, n_voxels), dtype=dtype)
A:sklearn.feature_extraction.image.patch_shape->tuple([patch_shape] * arr_ndim)
A:sklearn.feature_extraction.image.extraction_step->tuple([extraction_step] * arr_ndim)
A:sklearn.feature_extraction.image.slices->tuple((slice(None, None, st) for st in extraction_step))
A:sklearn.feature_extraction.image.shape->tuple(list(patch_indices_shape) + list(patch_shape))
A:sklearn.feature_extraction.image.strides->tuple(list(indexing_strides) + list(patch_strides))
A:sklearn.feature_extraction.image.patches->numpy.empty(patches_shape)
A:sklearn.feature_extraction.image.image->image.reshape((i_h, i_w, -1)).reshape((i_h, i_w, -1))
A:sklearn.feature_extraction.image.extracted_patches->_extract_patches(image, patch_shape=(p_h, p_w, n_colors), extraction_step=1)
A:sklearn.feature_extraction.image.n_patches->_compute_n_patches(img_height, img_width, patch_height, patch_width, self.max_patches)
A:sklearn.feature_extraction.image.rng->check_random_state(random_state)
A:sklearn.feature_extraction.image.i_s->check_random_state(random_state).randint(i_h - p_h + 1, size=n_patches)
A:sklearn.feature_extraction.image.j_s->check_random_state(random_state).randint(i_w - p_w + 1, size=n_patches)
A:sklearn.feature_extraction.image.X->numpy.reshape(X, (n_imgs, img_height, img_width, -1))
A:sklearn.feature_extraction.image.random_state->check_random_state(self.random_state)
A:sklearn.feature_extraction.image.patches[ii * n_patches:(ii + 1) * n_patches]->extract_patches_2d(image, patch_size, max_patches=self.max_patches, random_state=random_state)
sklearn.feature_extraction.grid_to_graph(n_x,n_y,n_z=1,*,mask=None,return_as=sparse.coo_matrix,dtype=int)
sklearn.feature_extraction.image.PatchExtractor(self,*,patch_size=None,max_patches=None,random_state=None)
sklearn.feature_extraction.image.PatchExtractor.__init__(self,*,patch_size=None,max_patches=None,random_state=None)
sklearn.feature_extraction.image.PatchExtractor._more_tags(self)
sklearn.feature_extraction.image.PatchExtractor.fit(self,X,y=None)
sklearn.feature_extraction.image.PatchExtractor.transform(self,X)
sklearn.feature_extraction.image._compute_gradient_3d(edges,img)
sklearn.feature_extraction.image._compute_n_patches(i_h,i_w,p_h,p_w,max_patches=None)
sklearn.feature_extraction.image._extract_patches(arr,patch_shape=8,extraction_step=1)
sklearn.feature_extraction.image._make_edges_3d(n_x,n_y,n_z=1)
sklearn.feature_extraction.image._mask_edges_weights(mask,edges,weights=None)
sklearn.feature_extraction.image._to_graph(n_x,n_y,n_z,mask=None,img=None,return_as=sparse.coo_matrix,dtype=None)
sklearn.feature_extraction.image.extract_patches_2d(image,patch_size,*,max_patches=None,random_state=None)
sklearn.feature_extraction.image.grid_to_graph(n_x,n_y,n_z=1,*,mask=None,return_as=sparse.coo_matrix,dtype=int)
sklearn.feature_extraction.image.img_to_graph(img,*,mask=None,return_as=sparse.coo_matrix,dtype=None)
sklearn.feature_extraction.image.reconstruct_from_patches_2d(patches,image_size)
sklearn.feature_extraction.img_to_graph(img,*,mask=None,return_as=sparse.coo_matrix,dtype=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/text.py----------------------------------------
A:sklearn.feature_extraction.text.doc->doc.decode(self.encoding, self.decode_error).decode(self.encoding, self.decode_error)
A:sklearn.feature_extraction.text.normalized->unicodedata.normalize('NFKD', s)
A:sklearn.feature_extraction.text.nkfd_form->unicodedata.normalize('NFKD', s)
A:sklearn.feature_extraction.text._white_spaces->re.compile('\\s\\s+')
A:sklearn.feature_extraction.text.tokens->list(tokenize(preprocess(w)))
A:sklearn.feature_extraction.text.n_original_tokens->len(original_tokens)
A:sklearn.feature_extraction.text.text_document->self._white_spaces.sub(' ', text_document)
A:sklearn.feature_extraction.text.text_len->len(text_document)
A:sklearn.feature_extraction.text.ngrams->list(text_document)
A:sklearn.feature_extraction.text.w_len->len(w)
A:sklearn.feature_extraction.text.token_pattern->re.compile(self.token_pattern)
A:sklearn.feature_extraction.text.inconsistent->set()
A:sklearn.feature_extraction.text.self._stop_words_id->id(self.stop_words)
A:sklearn.feature_extraction.text.preprocess->self.build_preprocessor()
A:sklearn.feature_extraction.text.stop_words->self.get_stop_words()
A:sklearn.feature_extraction.text.tokenize->self.build_tokenizer()
A:sklearn.feature_extraction.text.vocabulary->dict(vocabulary)
A:sklearn.feature_extraction.text.indices->numpy.array(list(self.vocabulary_.values()))
A:sklearn.feature_extraction.text.self.vocabulary_->dict(vocabulary)
A:sklearn.feature_extraction.text.analyzer->self.build_analyzer()
A:sklearn.feature_extraction.text.X->super().transform(raw_documents)
A:sklearn.feature_extraction.text.sorted_features->sorted(vocabulary.items())
A:sklearn.feature_extraction.text.map_index->numpy.empty(len(sorted_features), dtype=X.indices.dtype)
A:sklearn.feature_extraction.text.X.indices->numpy.empty(len(sorted_features), dtype=X.indices.dtype).take(X.indices, mode='clip')
A:sklearn.feature_extraction.text.dfs->_document_frequency(X)
A:sklearn.feature_extraction.text.mask->numpy.ones(len(dfs), dtype=bool)
A:sklearn.feature_extraction.text.tfs->numpy.asarray(X.sum(axis=0)).ravel()
A:sklearn.feature_extraction.text.new_mask->numpy.zeros(len(dfs), dtype=bool)
A:sklearn.feature_extraction.text.removed_terms->set()
A:sklearn.feature_extraction.text.analyze->self.build_analyzer()
A:sklearn.feature_extraction.text.values->numpy.frombuffer(values, dtype=np.intc)
A:sklearn.feature_extraction.text.j_indices->numpy.asarray(j_indices, dtype=indices_dtype)
A:sklearn.feature_extraction.text.indptr->numpy.asarray(indptr, dtype=indices_dtype)
A:sklearn.feature_extraction.text.(vocabulary, X)->self._count_vocab(raw_documents, self.fixed_vocabulary_)
A:sklearn.feature_extraction.text.(X, self.stop_words_)->self._limit_features(X, vocabulary, max_doc_count, min_doc_count, max_features)
A:sklearn.feature_extraction.text.(_, X)->self._count_vocab(raw_documents, fixed_vocab=True)
A:sklearn.feature_extraction.text.terms->numpy.array(list(self.vocabulary_.keys()))
A:sklearn.feature_extraction.text.df->df.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.feature_extraction.text.self._idf_diag->scipy.sparse.spdiags(value, diags=0, m=n_features, n=n_features, format='csr')
A:sklearn.feature_extraction.text.value->numpy.asarray(value, dtype=np.float64)
A:sklearn.feature_extraction.text.self._tfidf->TfidfTransformer(norm=self.norm, use_idf=self.use_idf, smooth_idf=self.smooth_idf, sublinear_tf=self.sublinear_tf)
sklearn.feature_extraction.text.CountVectorizer(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.feature_extraction.text.CountVectorizer.__init__(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.feature_extraction.text.CountVectorizer._count_vocab(self,raw_documents,fixed_vocab)
sklearn.feature_extraction.text.CountVectorizer._limit_features(self,X,vocabulary,high=None,low=None,limit=None)
sklearn.feature_extraction.text.CountVectorizer._more_tags(self)
sklearn.feature_extraction.text.CountVectorizer._sort_features(self,X,vocabulary)
sklearn.feature_extraction.text.CountVectorizer.fit(self,raw_documents,y=None)
sklearn.feature_extraction.text.CountVectorizer.fit_transform(self,raw_documents,y=None)
sklearn.feature_extraction.text.CountVectorizer.get_feature_names_out(self,input_features=None)
sklearn.feature_extraction.text.CountVectorizer.inverse_transform(self,X)
sklearn.feature_extraction.text.CountVectorizer.transform(self,raw_documents)
sklearn.feature_extraction.text.HashingVectorizer(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',n_features=2**20,binary=False,norm='l2',alternate_sign=True,dtype=np.float64)
sklearn.feature_extraction.text.HashingVectorizer.__init__(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',n_features=2**20,binary=False,norm='l2',alternate_sign=True,dtype=np.float64)
sklearn.feature_extraction.text.HashingVectorizer._get_hasher(self)
sklearn.feature_extraction.text.HashingVectorizer._more_tags(self)
sklearn.feature_extraction.text.HashingVectorizer.fit(self,X,y=None)
sklearn.feature_extraction.text.HashingVectorizer.fit_transform(self,X,y=None)
sklearn.feature_extraction.text.HashingVectorizer.partial_fit(self,X,y=None)
sklearn.feature_extraction.text.HashingVectorizer.transform(self,X)
sklearn.feature_extraction.text.TfidfTransformer(self,*,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfTransformer.__init__(self,*,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfTransformer._more_tags(self)
sklearn.feature_extraction.text.TfidfTransformer.fit(self,X,y=None)
sklearn.feature_extraction.text.TfidfTransformer.idf_(self)
sklearn.feature_extraction.text.TfidfTransformer.idf_(self,value)
sklearn.feature_extraction.text.TfidfTransformer.transform(self,X,copy=True)
sklearn.feature_extraction.text.TfidfVectorizer(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,analyzer='word',stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.float64,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfVectorizer.__init__(self,*,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,analyzer='word',stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.float64,norm='l2',use_idf=True,smooth_idf=True,sublinear_tf=False)
sklearn.feature_extraction.text.TfidfVectorizer._check_params(self)
sklearn.feature_extraction.text.TfidfVectorizer._more_tags(self)
sklearn.feature_extraction.text.TfidfVectorizer.fit(self,raw_documents,y=None)
sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(self,raw_documents,y=None)
sklearn.feature_extraction.text.TfidfVectorizer.idf_(self)
sklearn.feature_extraction.text.TfidfVectorizer.idf_(self,value)
sklearn.feature_extraction.text.TfidfVectorizer.transform(self,raw_documents)
sklearn.feature_extraction.text._VectorizerMixin
sklearn.feature_extraction.text._VectorizerMixin._char_ngrams(self,text_document)
sklearn.feature_extraction.text._VectorizerMixin._char_wb_ngrams(self,text_document)
sklearn.feature_extraction.text._VectorizerMixin._check_stop_words_consistency(self,stop_words,preprocess,tokenize)
sklearn.feature_extraction.text._VectorizerMixin._check_vocabulary(self)
sklearn.feature_extraction.text._VectorizerMixin._validate_ngram_range(self)
sklearn.feature_extraction.text._VectorizerMixin._validate_vocabulary(self)
sklearn.feature_extraction.text._VectorizerMixin._warn_for_unused_params(self)
sklearn.feature_extraction.text._VectorizerMixin._word_ngrams(self,tokens,stop_words=None)
sklearn.feature_extraction.text._VectorizerMixin.build_analyzer(self)
sklearn.feature_extraction.text._VectorizerMixin.build_preprocessor(self)
sklearn.feature_extraction.text._VectorizerMixin.build_tokenizer(self)
sklearn.feature_extraction.text._VectorizerMixin.decode(self,doc)
sklearn.feature_extraction.text._VectorizerMixin.get_stop_words(self)
sklearn.feature_extraction.text._analyze(doc,analyzer=None,tokenizer=None,ngrams=None,preprocessor=None,decoder=None,stop_words=None)
sklearn.feature_extraction.text._check_stop_list(stop)
sklearn.feature_extraction.text._document_frequency(X)
sklearn.feature_extraction.text._make_int_array()
sklearn.feature_extraction.text._preprocess(doc,accent_function=None,lower=False)
sklearn.feature_extraction.text.strip_accents_ascii(s)
sklearn.feature_extraction.text.strip_accents_unicode(s)
sklearn.feature_extraction.text.strip_tags(s)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/_hash.py----------------------------------------
A:sklearn.feature_extraction._hash.raw_X->iter(raw_X)
A:sklearn.feature_extraction._hash.first_raw_X->next(raw_X)
A:sklearn.feature_extraction._hash.raw_X_->chain([first_raw_X], raw_X)
A:sklearn.feature_extraction._hash.(indices, indptr, values)->_hashing_transform(raw_X, self.n_features, self.dtype, self.alternate_sign, seed=0)
A:sklearn.feature_extraction._hash.X->scipy.sparse.csr_matrix((values, indices, indptr), dtype=self.dtype, shape=(n_samples, self.n_features))
sklearn.feature_extraction.FeatureHasher(self,n_features=2**20,*,input_type='dict',dtype=np.float64,alternate_sign=True)
sklearn.feature_extraction.FeatureHasher._more_tags(self)
sklearn.feature_extraction.FeatureHasher.fit(self,X=None,y=None)
sklearn.feature_extraction.FeatureHasher.transform(self,raw_X)
sklearn.feature_extraction._hash.FeatureHasher(self,n_features=2**20,*,input_type='dict',dtype=np.float64,alternate_sign=True)
sklearn.feature_extraction._hash.FeatureHasher.__init__(self,n_features=2**20,*,input_type='dict',dtype=np.float64,alternate_sign=True)
sklearn.feature_extraction._hash.FeatureHasher._more_tags(self)
sklearn.feature_extraction._hash.FeatureHasher.fit(self,X=None,y=None)
sklearn.feature_extraction._hash.FeatureHasher.transform(self,raw_X)
sklearn.feature_extraction._hash._iteritems(d)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/_dict_vectorizer.py----------------------------------------
A:sklearn.feature_extraction._dict_vectorizer.vocab[feature_name]->len(feature_names)
A:sklearn.feature_extraction._dict_vectorizer.indices->numpy.frombuffer(indices, dtype=np.intc)
A:sklearn.feature_extraction._dict_vectorizer.result_matrix->result_matrix.toarray().toarray()
A:sklearn.feature_extraction._dict_vectorizer.map_index->numpy.empty(len(feature_names), dtype=np.int32)
A:sklearn.feature_extraction._dict_vectorizer.X->check_array(X, accept_sparse=['csr', 'csc'])
A:sklearn.feature_extraction._dict_vectorizer.new_vocab[names[i]]->len(new_vocab)
sklearn.feature_extraction.DictVectorizer(self,*,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction.DictVectorizer._add_iterable_element(self,f,v,feature_names,vocab,*,fitting=True,transforming=False,indices=None,values=None)
sklearn.feature_extraction.DictVectorizer._more_tags(self)
sklearn.feature_extraction.DictVectorizer._transform(self,X,fitting)
sklearn.feature_extraction.DictVectorizer.fit(self,X,y=None)
sklearn.feature_extraction.DictVectorizer.fit_transform(self,X,y=None)
sklearn.feature_extraction.DictVectorizer.get_feature_names_out(self,input_features=None)
sklearn.feature_extraction.DictVectorizer.inverse_transform(self,X,dict_type=dict)
sklearn.feature_extraction.DictVectorizer.restrict(self,support,indices=False)
sklearn.feature_extraction.DictVectorizer.transform(self,X)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer(self,*,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.__init__(self,*,dtype=np.float64,separator='=',sparse=True,sort=True)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer._add_iterable_element(self,f,v,feature_names,vocab,*,fitting=True,transforming=False,indices=None,values=None)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer._more_tags(self)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer._transform(self,X,fitting)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit(self,X,y=None)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.fit_transform(self,X,y=None)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.get_feature_names_out(self,input_features=None)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.inverse_transform(self,X,dict_type=dict)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.restrict(self,support,indices=False)
sklearn.feature_extraction._dict_vectorizer.DictVectorizer.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/_stop_words.py----------------------------------------
A:sklearn.feature_extraction._stop_words.ENGLISH_STOP_WORDS->frozenset(['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves'])


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/tests/test_text.py----------------------------------------
A:sklearn.feature_extraction.tests.test_text.wa->CountVectorizer(ngram_range=(1, 2), encoding='ascii').build_analyzer()
A:sklearn.feature_extraction.tests.test_text.text->StringIO('A test with a file-like object!')
A:sklearn.feature_extraction.tests.test_text.text_bytes->StringIO('A test with a file-like object!').encode('utf-8')
A:sklearn.feature_extraction.tests.test_text.ca->CountVectorizer(analyzer='char', ngram_range=(3, 6), encoding='ascii').build_analyzer()
A:sklearn.feature_extraction.tests.test_text.cnga->CountVectorizer(analyzer='word', strip_accents='unicode', ngram_range=(3, 6)).build_analyzer()
A:sklearn.feature_extraction.tests.test_text.cnga_file->CountVectorizer(input='file', analyzer='word', ngram_range=(3, 6)).build_analyzer()
A:sklearn.feature_extraction.tests.test_text.file->StringIO(text)
A:sklearn.feature_extraction.tests.test_text.terms->numpy.sort(np.unique(analyze(doc)))
A:sklearn.feature_extraction.tests.test_text.v->TfidfVectorizer(binary=True, use_idf=False, norm=None)
A:sklearn.feature_extraction.tests.test_text.vect->Vectorizer()
A:sklearn.feature_extraction.tests.test_text.X->csr_container((5, 5), dtype=np.int64)
A:sklearn.feature_extraction.tests.test_text.inv->Vectorizer().inverse_transform(X)
A:sklearn.feature_extraction.tests.test_text.pipe->Pipeline([('count', CountVectorizer(vocabulary=what_we_like)), ('tfidf', TfidfTransformer())])
A:sklearn.feature_extraction.tests.test_text.cv->CountVectorizer(vocabulary=vocab_dict)
A:sklearn.feature_extraction.tests.test_text.X1->CountVectorizer(vocabulary=vocab_dict).fit_transform(ALL_FOOD_DOCS[:5])
A:sklearn.feature_extraction.tests.test_text.X2->TfidfVectorizer(binary=True, use_idf=False, norm=None).transform(['hello world', 'hello hello']).toarray()
A:sklearn.feature_extraction.tests.test_text.vectorizer->Vectorizer()
A:sklearn.feature_extraction.tests.test_text.feature_names_out->TfidfTransformer(sublinear_tf=True, use_idf=False, norm=None).get_feature_names_out(feature_names_in)
A:sklearn.feature_extraction.tests.test_text.tr->TfidfTransformer(sublinear_tf=True, use_idf=False, norm=None)
A:sklearn.feature_extraction.tests.test_text.tfidf->TfidfTransformer(norm='l1').fit(counts_train).transform(counts_train).toarray()
A:sklearn.feature_extraction.tests.test_text.train_data->iter(ALL_FOOD_DOCS[:-1])
A:sklearn.feature_extraction.tests.test_text.v1->CountVectorizer(max_df=0.5)
A:sklearn.feature_extraction.tests.test_text.counts_train->counts_train.tocsr().tocsr()
A:sklearn.feature_extraction.tests.test_text.v2->CountVectorizer(vocabulary=v1.vocabulary_)
A:sklearn.feature_extraction.tests.test_text.counts_test->counts_test.tocsr().tocsr()
A:sklearn.feature_extraction.tests.test_text.t1->TfidfTransformer(norm='l1')
A:sklearn.feature_extraction.tests.test_text.tfidf_test->TfidfTransformer(norm='l1').transform(counts_test).toarray()
A:sklearn.feature_extraction.tests.test_text.t2->TfidfTransformer(norm='l1', use_idf=False)
A:sklearn.feature_extraction.tests.test_text.tf->TfidfTransformer(norm='l1', use_idf=False).fit(counts_train).transform(counts_train).toarray()
A:sklearn.feature_extraction.tests.test_text.t3->TfidfTransformer(use_idf=True)
A:sklearn.feature_extraction.tests.test_text.tv->TfidfVectorizer(norm=norm, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)
A:sklearn.feature_extraction.tests.test_text.tfidf2->TfidfVectorizer(norm=norm, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf).fit_transform(train_data).toarray()
A:sklearn.feature_extraction.tests.test_text.tfidf_test2->TfidfVectorizer(norm=norm, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf).transform(test_data).toarray()
A:sklearn.feature_extraction.tests.test_text.v3->CountVectorizer(vocabulary=None)
A:sklearn.feature_extraction.tests.test_text.processor->CountVectorizer(vocabulary=None).build_preprocessor()
A:sklearn.feature_extraction.tests.test_text.expected->function(text)
A:sklearn.feature_extraction.tests.test_text.result->roundtripped_function(text)
A:sklearn.feature_extraction.tests.test_text.feature_names->CountVectorizer(vocabulary=vocab_dict).get_feature_names_out()
A:sklearn.feature_extraction.tests.test_text.cv_1->CountVectorizer(max_features=1)
A:sklearn.feature_extraction.tests.test_text.cv_3->CountVectorizer(max_features=3)
A:sklearn.feature_extraction.tests.test_text.cv_None->CountVectorizer(max_features=None)
A:sklearn.feature_extraction.tests.test_text.counts_1->CountVectorizer(max_features=1).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.counts_3->CountVectorizer(max_features=3).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.counts_None->CountVectorizer(max_features=None).fit_transform(JUNK_FOOD_DOCS).sum(axis=0)
A:sklearn.feature_extraction.tests.test_text.features_1->CountVectorizer(max_features=1).get_feature_names_out()
A:sklearn.feature_extraction.tests.test_text.features_3->CountVectorizer(max_features=3).get_feature_names_out()
A:sklearn.feature_extraction.tests.test_text.features_None->CountVectorizer(max_features=None).get_feature_names_out()
A:sklearn.feature_extraction.tests.test_text.X_sparse->Vectorizer().fit_transform(test_data)
A:sklearn.feature_extraction.tests.test_text.transformed_data->Vectorizer().fit_transform(data)
A:sklearn.feature_extraction.tests.test_text.inversed_data->Vectorizer().inverse_transform(transformed_data)
A:sklearn.feature_extraction.tests.test_text.analyze->Vectorizer().build_analyzer()
A:sklearn.feature_extraction.tests.test_text.inversed_terms->numpy.sort(np.unique(inversed_terms))
A:sklearn.feature_extraction.tests.test_text.transformed_data2->Vectorizer().fit_transform(data).toarray()
A:sklearn.feature_extraction.tests.test_text.inversed_data2->Vectorizer().inverse_transform(transformed_data2)
A:sklearn.feature_extraction.tests.test_text.transformed_data3->Vectorizer().fit_transform(data).tocsc()
A:sklearn.feature_extraction.tests.test_text.inversed_data3->Vectorizer().inverse_transform(transformed_data3)
A:sklearn.feature_extraction.tests.test_text.(train_data, test_data, target_train, target_test)->train_test_split(data, target, test_size=0.1, random_state=0)
A:sklearn.feature_extraction.tests.test_text.pipeline->Pipeline([('vect', TfidfVectorizer()), ('svc', LinearSVC(dual='auto'))])
A:sklearn.feature_extraction.tests.test_text.grid_search->GridSearchCV(pipeline, parameters, n_jobs=1)
A:sklearn.feature_extraction.tests.test_text.pred->GridSearchCV(pipeline, parameters, n_jobs=1).fit(train_data, target_train).predict(test_data)
A:sklearn.feature_extraction.tests.test_text.cv_scores->cross_val_score(pipeline, data, target, cv=3)
A:sklearn.feature_extraction.tests.test_text.X_counted->Vectorizer().fit_transform([document])
A:sklearn.feature_extraction.tests.test_text.X_hashed->Vectorizer().transform([document])
A:sklearn.feature_extraction.tests.test_text.X_1->Vectorizer().fit_transform(ALL_FOOD_DOCS)
A:sklearn.feature_extraction.tests.test_text.X_2->Vectorizer().transform(ALL_FOOD_DOCS)
A:sklearn.feature_extraction.tests.test_text.s->pickle.dumps(orig)
A:sklearn.feature_extraction.tests.test_text.copy->TfidfVectorizer(vocabulary=vect.vocabulary_, use_idf=True)
A:sklearn.feature_extraction.tests.test_text.vec->CountVectorizer(max_features=1)
A:sklearn.feature_extraction.tests.test_text.function->factory(vec)
A:sklearn.feature_extraction.tests.test_text.roundtripped_function->pickle.loads(pickle.dumps(function))
A:sklearn.feature_extraction.tests.test_text.rng->numpy.random.RandomState(0)
A:sklearn.feature_extraction.tests.test_text.vocab_words->numpy.array(['beer', 'burger', 'celeri', 'coke', 'pizza', 'salad', 'sparkling', 'tomato', 'water'])
A:sklearn.feature_extraction.tests.test_text.vocab_set->set(rng.choice(vocab_words, size=5, replace=False))
A:sklearn.feature_extraction.tests.test_text.unpickled_cv->pickle.loads(pickle.dumps(cv))
A:sklearn.feature_extraction.tests.test_text.vocab_dict->dict()
A:sklearn.feature_extraction.tests.test_text.words->numpy.random.RandomState(0).choice(vocab_words, size=5, replace=False)
A:sklearn.feature_extraction.tests.test_text.vect_transform->Vectorizer().transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.stop_None_transform->Vectorizer().transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.stop_del_transform->Vectorizer().transform(JUNK_FOOD_DOCS).toarray()
A:sklearn.feature_extraction.tests.test_text.orig->TfidfVectorizer(use_idf=True)
A:sklearn.feature_extraction.tests.test_text.expected_idf_len->len(vect.idf_)
A:sklearn.feature_extraction.tests.test_text.hv->HashingVectorizer()
A:sklearn.feature_extraction.tests.test_text.vect_vocab->TfidfVectorizer(vocabulary=['the'])
A:sklearn.feature_extraction.tests.test_text.vect_vocab_clone->clone(vect_vocab)
A:sklearn.feature_extraction.tests.test_text.X_trans->TfidfTransformer().fit_transform(X)
A:sklearn.feature_extraction.tests.test_text.X_csc->csc_container(X)
A:sklearn.feature_extraction.tests.test_text.X_csr->csr_container(X)
A:sklearn.feature_extraction.tests.test_text.X_trans_csc->TfidfTransformer().fit_transform(X_csc)
A:sklearn.feature_extraction.tests.test_text.X_trans_csr->TfidfTransformer().fit_transform(X_csr)
A:sklearn.feature_extraction.tests.test_text.X_idf->Vectorizer().fit_transform(X)
A:sklearn.feature_extraction.tests.test_text.message->re.escape(f'Invalid value for ngram_range={invalid_range} lower boundary larger than the upper boundary.')
A:sklearn.feature_extraction.tests.test_text.stop_words->estimator.get_stop_words()
A:sklearn.feature_extraction.tests.test_text.tokenize->estimator.build_tokenizer()
A:sklearn.feature_extraction.tests.test_text.preprocess->estimator.build_preprocessor()
A:sklearn.feature_extraction.tests.test_text.X.indices->csr_container((5, 5), dtype=np.int64).indices.astype(INDICES_DTYPE)
A:sklearn.feature_extraction.tests.test_text.X.indptr->csr_container((5, 5), dtype=np.int64).indptr.astype(INDICES_DTYPE)
A:sklearn.feature_extraction.tests.test_text.Xs->CountVectorizer()._sort_features(X, vocabulary)
A:sklearn.feature_extraction.tests.test_text.f->tmpdir.join('file.txt')
A:sklearn.feature_extraction.tests.test_text.hashing->HashingVectorizer(n_features=1000000, ngram_range=(2, 3))
A:sklearn.feature_extraction.tests.test_text.est->Estimator()
sklearn.feature_extraction.tests.test_text._check_stop_words_consistency(estimator)
sklearn.feature_extraction.tests.test_text.lazy_analyze(s)
sklearn.feature_extraction.tests.test_text.split_tokenize(s)
sklearn.feature_extraction.tests.test_text.strip_eacute(s)
sklearn.feature_extraction.tests.test_text.test_callable_analyzer_change_behavior(Estimator,analyzer,input_type)
sklearn.feature_extraction.tests.test_text.test_callable_analyzer_error(Estimator,input_type,err_type,err_msg)
sklearn.feature_extraction.tests.test_text.test_callable_analyzer_reraise_error(tmpdir,Estimator)
sklearn.feature_extraction.tests.test_text.test_char_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.test_char_wb_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.test_count_binary_occurrences()
sklearn.feature_extraction.tests.test_text.test_count_vectorizer_max_features()
sklearn.feature_extraction.tests.test_text.test_count_vectorizer_pipeline_grid_selection()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_token_pattern()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_token_pattern_with_several_group()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_gap_index()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_pipeline()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_custom_vocabulary_repeated_indices()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_empty_vocabulary()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_sort_features_64bit_sparse_indices(csr_container)
sklearn.feature_extraction.tests.test_text.test_countvectorizer_stop_words()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_uppercase_in_vocab()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_vocab_dicts_when_pickling()
sklearn.feature_extraction.tests.test_text.test_countvectorizer_vocab_sets_when_pickling()
sklearn.feature_extraction.tests.test_text.test_feature_names()
sklearn.feature_extraction.tests.test_text.test_fit_countvectorizer_twice()
sklearn.feature_extraction.tests.test_text.test_hashed_binary_occurrences()
sklearn.feature_extraction.tests.test_text.test_hashing_vectorizer()
sklearn.feature_extraction.tests.test_text.test_hashingvectorizer_nan_in_docs()
sklearn.feature_extraction.tests.test_text.test_n_features_in(Vectorizer,X)
sklearn.feature_extraction.tests.test_text.test_non_unique_vocab()
sklearn.feature_extraction.tests.test_text.test_nonnegative_hashing_vectorizer_result_indices()
sklearn.feature_extraction.tests.test_text.test_pickling_built_processors(factory)
sklearn.feature_extraction.tests.test_text.test_pickling_transformer()
sklearn.feature_extraction.tests.test_text.test_pickling_vectorizer()
sklearn.feature_extraction.tests.test_text.test_stop_word_validation_custom_preprocessor(Estimator)
sklearn.feature_extraction.tests.test_text.test_stop_words_removal()
sklearn.feature_extraction.tests.test_text.test_strip_accents()
sklearn.feature_extraction.tests.test_text.test_sublinear_tf()
sklearn.feature_extraction.tests.test_text.test_tf_idf_smoothing()
sklearn.feature_extraction.tests.test_text.test_tf_transformer_feature_names_out()
sklearn.feature_extraction.tests.test_text.test_tfidf_no_smoothing()
sklearn.feature_extraction.tests.test_text.test_tfidf_transformer_sparse(csc_container,csr_container)
sklearn.feature_extraction.tests.test_text.test_tfidf_transformer_type(X_dtype)
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_setter()
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_setters()
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_type(vectorizer_dtype,output_dtype,warning_expected)
sklearn.feature_extraction.tests.test_text.test_tfidf_vectorizer_with_fixed_vocabulary()
sklearn.feature_extraction.tests.test_text.test_tfidfvectorizer_binary()
sklearn.feature_extraction.tests.test_text.test_tfidfvectorizer_export_idf()
sklearn.feature_extraction.tests.test_text.test_tfidfvectorizer_invalid_idf_attr()
sklearn.feature_extraction.tests.test_text.test_tie_breaking_sample_order_invariance()
sklearn.feature_extraction.tests.test_text.test_to_ascii()
sklearn.feature_extraction.tests.test_text.test_transformer_idf_setter()
sklearn.feature_extraction.tests.test_text.test_unicode_decode_error()
sklearn.feature_extraction.tests.test_text.test_unused_parameters_warn(Vectorizer,stop_words,tokenizer,preprocessor,ngram_range,token_pattern,analyzer,unused_name,ovrd_name,ovrd_msg)
sklearn.feature_extraction.tests.test_text.test_vectorizer()
sklearn.feature_extraction.tests.test_text.test_vectorizer_inverse_transform(Vectorizer)
sklearn.feature_extraction.tests.test_text.test_vectorizer_max_df()
sklearn.feature_extraction.tests.test_text.test_vectorizer_max_features(Vectorizer)
sklearn.feature_extraction.tests.test_text.test_vectorizer_min_df()
sklearn.feature_extraction.tests.test_text.test_vectorizer_pipeline_cross_validation()
sklearn.feature_extraction.tests.test_text.test_vectorizer_pipeline_grid_selection()
sklearn.feature_extraction.tests.test_text.test_vectorizer_stop_words_inconsistent()
sklearn.feature_extraction.tests.test_text.test_vectorizer_string_object_as_input(Vectorizer)
sklearn.feature_extraction.tests.test_text.test_vectorizer_unicode()
sklearn.feature_extraction.tests.test_text.test_vectorizer_vocab_clone()
sklearn.feature_extraction.tests.test_text.test_vectorizers_do_not_have_set_output(Estimator)
sklearn.feature_extraction.tests.test_text.test_vectorizers_invalid_ngram_range(vec)
sklearn.feature_extraction.tests.test_text.test_word_analyzer_unigrams(Vectorizer)
sklearn.feature_extraction.tests.test_text.test_word_analyzer_unigrams_and_bigrams()
sklearn.feature_extraction.tests.test_text.test_word_ngram_analyzer()
sklearn.feature_extraction.tests.test_text.uppercase(s)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/tests/test_feature_hasher.py----------------------------------------
A:sklearn.feature_extraction.tests.test_feature_hasher.feature_hasher->FeatureHasher(n_features=n_features, input_type='string')
A:sklearn.feature_extraction.tests.test_feature_hasher.X1->FeatureHasher(n_features=16).transform(raw_X)
A:sklearn.feature_extraction.tests.test_feature_hasher.X2->FeatureHasher(n_features=16, input_type='pair').transform(gen)
A:sklearn.feature_extraction.tests.test_feature_hasher.X->FeatureHasher().transform([{'foo': 0}])
A:sklearn.feature_extraction.tests.test_feature_hasher.(indices, indptr, _)->_hashing_transform(raw_X_, 2 ** 7, str, False)
A:sklearn.feature_extraction.tests.test_feature_hasher.(indices_0, indptr_0, _)->_hashing_transform(raw_X_, 2 ** 7, str, False, seed=0)
A:sklearn.feature_extraction.tests.test_feature_hasher.(indices_1, _, _)->_hashing_transform(raw_X_, 2 ** 7, str, False, seed=1)
A:sklearn.feature_extraction.tests.test_feature_hasher.(x1, x2)->FeatureHasher(n_features=n_features, input_type='string').transform(raw_X).toarray()
A:sklearn.feature_extraction.tests.test_feature_hasher.x1_nz->numpy.abs(x1[x1 != 0])
A:sklearn.feature_extraction.tests.test_feature_hasher.x2_nz->numpy.abs(x2[x2 != 0])
A:sklearn.feature_extraction.tests.test_feature_hasher.Xt->FeatureHasher(alternate_sign=False, n_features=1, input_type='string').fit_transform(X)
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_dicts()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_pairs()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_pairs_with_string_values()
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_single_string(raw_X)
sklearn.feature_extraction.tests.test_feature_hasher.test_feature_hasher_strings()
sklearn.feature_extraction.tests.test_feature_hasher.test_hash_collisions()
sklearn.feature_extraction.tests.test_feature_hasher.test_hash_empty_input()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_alternate_sign()
sklearn.feature_extraction.tests.test_feature_hasher.test_hasher_zeros()
sklearn.feature_extraction.tests.test_feature_hasher.test_hashing_transform_seed()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/tests/test_image.py----------------------------------------
A:sklearn.feature_extraction.tests.test_image.grad_x->img_to_graph(x, mask=mask).todense()
A:sklearn.feature_extraction.tests.test_image.grad_y->img_to_graph(y)
A:sklearn.feature_extraction.tests.test_image.mask->numpy.ones((size, size))
A:sklearn.feature_extraction.tests.test_image.x->numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
A:sklearn.feature_extraction.tests.test_image.desired->numpy.array([[1, 0, 0], [0, 1, 1], [0, 1, 1]])
A:sklearn.feature_extraction.tests.test_image.A->grid_to_graph(n_x=size, n_y=size, n_z=size, mask=mask, dtype=np.float64)
A:sklearn.feature_extraction.tests.test_image.graph->grid_to_graph(*face.shape, mask=mask, dtype=None)
A:sklearn.feature_extraction.tests.test_image.face->face.astype(np.float32).astype(np.float32)
A:sklearn.feature_extraction.tests.test_image.face_color->numpy.zeros(face.shape + (3,))
A:sklearn.feature_extraction.tests.test_image.images->numpy.zeros((3,) + face.shape)
A:sklearn.feature_extraction.tests.test_image.patches->_extract_patches(face, patch_shape=p)
A:sklearn.feature_extraction.tests.test_image.expected_n_patches->int(0.5 * (i_h - p_h + 1) * (i_w - p_w + 1))
A:sklearn.feature_extraction.tests.test_image.face_reconstructed->reconstruct_from_patches_2d(patches, face.shape)
A:sklearn.feature_extraction.tests.test_image.extr->PatchExtractor(patch_size=(p_h, p_w), random_state=0)
A:sklearn.feature_extraction.tests.test_image.faces->_make_images(orange_face)
A:sklearn.feature_extraction.tests.test_image.image->numpy.arange(np.prod(image_shape)).reshape(image_shape)
A:sklearn.feature_extraction.tests.test_image.ndim->len(image_shape)
A:sklearn.feature_extraction.tests.test_image.last_patch_slices->tuple((slice(i, i + j, None) for (i, j) in zip(last_patch, patch_size)))
A:sklearn.feature_extraction.tests.test_image.extractor->PatchExtractor(patch_size=(8, 8, 8))
sklearn.feature_extraction.tests.test_image._make_images(face)
sklearn.feature_extraction.tests.test_image.downsampled_face(raccoon_face_fxt)
sklearn.feature_extraction.tests.test_image.downsampled_face_collection(downsampled_face)
sklearn.feature_extraction.tests.test_image.orange_face(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_connect_regions(raccoon_face_fxt)
sklearn.feature_extraction.tests.test_image.test_connect_regions_with_grid(raccoon_face_fxt)
sklearn.feature_extraction.tests.test_image.test_extract_patch_same_size_image(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_all(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_all_color(orange_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_all_rect(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_less_than_max_patches(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_max_patches(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_square(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_extract_patches_strided()
sklearn.feature_extraction.tests.test_image.test_grid_to_graph()
sklearn.feature_extraction.tests.test_image.test_img_to_graph()
sklearn.feature_extraction.tests.test_image.test_img_to_graph_sparse()
sklearn.feature_extraction.tests.test_image.test_patch_extractor_all_patches(downsampled_face_collection)
sklearn.feature_extraction.tests.test_image.test_patch_extractor_color(orange_face)
sklearn.feature_extraction.tests.test_image.test_patch_extractor_fit(downsampled_face_collection)
sklearn.feature_extraction.tests.test_image.test_patch_extractor_max_patches(downsampled_face_collection)
sklearn.feature_extraction.tests.test_image.test_patch_extractor_max_patches_default(downsampled_face_collection)
sklearn.feature_extraction.tests.test_image.test_patch_extractor_wrong_input(orange_face)
sklearn.feature_extraction.tests.test_image.test_reconstruct_patches_perfect(downsampled_face)
sklearn.feature_extraction.tests.test_image.test_reconstruct_patches_perfect_color(orange_face)
sklearn.feature_extraction.tests.test_image.test_width_patch()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/feature_extraction/tests/test_dict_vectorizer.py----------------------------------------
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v->DictVectorizer(sparse=sparse).fit(D)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.X->X.toarray().toarray()
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d1->dict([('useless%d' % i, 10) for i in range(20)], useful1=1, useful2=20)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d2->dict([('useless%d' % i, 10) for i in range(20)], useful1=20, useful2=1)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sel->SelectKBest(chi2, k=2).fit(X, [0, 1])
A:sklearn.feature_extraction.tests.test_dict_vectorizer.D_out->DictVectorizer(sparse=sparse).fit(D).inverse_transform(X)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.names->DictVectorizer(sparse=sparse).fit(D).get_feature_names_out()
A:sklearn.feature_extraction.tests.test_dict_vectorizer.rng->Random(global_random_seed)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d_sorted->dict(items)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.d_shuffled->dict(items)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v_1->DictVectorizer().fit([d_sorted])
A:sklearn.feature_extraction.tests.test_dict_vectorizer.v_2->DictVectorizer().fit([d_shuffled])
A:sklearn.feature_extraction.tests.test_dict_vectorizer.dv->DictVectorizer(sparse=False)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.dense_vectorizer->DictVectorizer(sparse=False)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sparse_vectorizer->DictVectorizer(sparse=True)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.dense_vector_fit->DictVectorizer(sparse=False).fit_transform(movie_entry_fit)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sparse_vector_fit->DictVectorizer(sparse=True).fit_transform(movie_entry_fit)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.dense_vector_transform->DictVectorizer(sparse=False).transform(movie_entry_transform)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sparse_vector_transform->DictVectorizer(sparse=True).transform(movie_entry_transform)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.dense_inverse_transform->DictVectorizer(sparse=False).inverse_transform(dense_vector_transform)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.sparse_inverse_transform->DictVectorizer(sparse=True).inverse_transform(sparse_vector_transform)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.vectorizer->DictVectorizer(sparse=True)
A:sklearn.feature_extraction.tests.test_dict_vectorizer.feature_names->DictVectorizer(sparse=False).get_feature_names_out()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_deterministic_vocabulary(global_random_seed)
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dict_vectorizer_get_feature_names_out()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dict_vectorizer_not_fitted_error(method,input)
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dict_vectorizer_unsupported_value_type()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dictvectorizer(sparse,dtype,sort,iterable)
sklearn.feature_extraction.tests.test_dict_vectorizer.test_dictvectorizer_dense_sparse_equivalence()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_feature_selection()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_iterable_not_string_error()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_iterable_value()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_mapping_error()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_n_features_in()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_one_of_k()
sklearn.feature_extraction.tests.test_dict_vectorizer.test_unseen_or_no_features()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/__init__.py----------------------------------------
sklearn.model_selection.__init__.__getattr__(name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/_search_successive_halving.py----------------------------------------
A:sklearn.model_selection._search_successive_halving.train_idx->resample(train_idx, replace=False, random_state=self.random_state, n_samples=int(self.fraction * len(train_idx)))
A:sklearn.model_selection._search_successive_halving.test_idx->resample(test_idx, replace=False, random_state=self.random_state, n_samples=int(self.fraction * len(test_idx)))
A:sklearn.model_selection._search_successive_halving.iter_indices->numpy.flatnonzero(iteration == itr)
A:sklearn.model_selection._search_successive_halving.sorted_indices->numpy.roll(np.argsort(scores), np.count_nonzero(np.isnan(scores)))
A:sklearn.model_selection._search_successive_halving.n_splits->self._checked_cv_orig.get_n_splits(X, y, **split_params)
A:sklearn.model_selection._search_successive_halving.y->self._validate_data(X='no_validation', y=y)
A:sklearn.model_selection._search_successive_halving.self.max_resources_->_num_samples(X)
A:sklearn.model_selection._search_successive_halving.last_iter->numpy.max(results['iter'])
A:sklearn.model_selection._search_successive_halving.last_iter_indices->numpy.flatnonzero(results['iter'] == last_iter)
A:sklearn.model_selection._search_successive_halving.best_idx->numpy.nanargmax(test_scores)
A:sklearn.model_selection._search_successive_halving.self._checked_cv_orig->check_cv(self.cv, y, classifier=is_classifier(self.estimator))
A:sklearn.model_selection._search_successive_halving.routed_params->self._get_routed_params_for_fit(params)
A:sklearn.model_selection._search_successive_halving.self._n_samples_orig->_num_samples(X)
A:sklearn.model_selection._search_successive_halving.candidate_params->_top_k(results, n_candidates_to_keep, itr)
A:sklearn.model_selection._search_successive_halving.self.min_resources_->max(self.min_resources_, self.max_resources_ // self.factor ** last_iteration)
A:sklearn.model_selection._search_successive_halving.n_iterations->min(n_possible_iterations, n_required_iterations)
A:sklearn.model_selection._search_successive_halving.power->max(0, itr - n_required_iterations + n_possible_iterations)
A:sklearn.model_selection._search_successive_halving.n_resources->min(n_resources, self.max_resources_)
A:sklearn.model_selection._search_successive_halving.n_candidates->len(candidate_params)
A:sklearn.model_selection._search_successive_halving.cv->_SubsampleMetaSplitter(base_cv=self._checked_cv_orig, fraction=n_resources / self._n_samples_orig, subsample_test=True, random_state=self.random_state)
A:sklearn.model_selection._search_successive_halving.results->evaluate_candidates(candidate_params, cv, more_results=more_results)
A:sklearn.model_selection._search_successive_halving.n_candidates_to_keep->ceil(n_candidates / self.factor)
A:sklearn.model_selection._search_successive_halving.self.n_remaining_candidates_->len(candidate_params)
A:sklearn.model_selection._search_successive_halving.tags->deepcopy(super()._more_tags())
sklearn.model_selection.HalvingGridSearchCV(self,estimator,param_grid,*,factor=3,resource='n_samples',max_resources='auto',min_resources='exhaust',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection.HalvingGridSearchCV._generate_candidate_params(self)
sklearn.model_selection.HalvingRandomSearchCV(self,estimator,param_distributions,*,n_candidates='exhaust',factor=3,resource='n_samples',max_resources='auto',min_resources='smallest',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection.HalvingRandomSearchCV._generate_candidate_params(self)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving(self,estimator,*,scoring=None,n_jobs=None,refit=True,cv=5,verbose=0,random_state=None,error_score=np.nan,return_train_score=True,max_resources='auto',min_resources='exhaust',resource='n_samples',factor=3,aggressive_elimination=False)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving.__init__(self,estimator,*,scoring=None,n_jobs=None,refit=True,cv=5,verbose=0,random_state=None,error_score=np.nan,return_train_score=True,max_resources='auto',min_resources='exhaust',resource='n_samples',factor=3,aggressive_elimination=False)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving._check_input_parameters(self,X,y,split_params)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving._generate_candidate_params(self)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving._more_tags(self)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving._run_search(self,evaluate_candidates)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving._select_best_index(refit,refit_metric,results)
sklearn.model_selection._search_successive_halving.BaseSuccessiveHalving.fit(self,X,y=None,**params)
sklearn.model_selection._search_successive_halving.HalvingGridSearchCV(self,estimator,param_grid,*,factor=3,resource='n_samples',max_resources='auto',min_resources='exhaust',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection._search_successive_halving.HalvingGridSearchCV.__init__(self,estimator,param_grid,*,factor=3,resource='n_samples',max_resources='auto',min_resources='exhaust',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection._search_successive_halving.HalvingGridSearchCV._generate_candidate_params(self)
sklearn.model_selection._search_successive_halving.HalvingRandomSearchCV(self,estimator,param_distributions,*,n_candidates='exhaust',factor=3,resource='n_samples',max_resources='auto',min_resources='smallest',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection._search_successive_halving.HalvingRandomSearchCV.__init__(self,estimator,param_distributions,*,n_candidates='exhaust',factor=3,resource='n_samples',max_resources='auto',min_resources='smallest',aggressive_elimination=False,cv=5,scoring=None,refit=True,error_score=np.nan,return_train_score=True,random_state=None,n_jobs=None,verbose=0)
sklearn.model_selection._search_successive_halving.HalvingRandomSearchCV._generate_candidate_params(self)
sklearn.model_selection._search_successive_halving._SubsampleMetaSplitter(self,*,base_cv,fraction,subsample_test,random_state)
sklearn.model_selection._search_successive_halving._SubsampleMetaSplitter.__init__(self,*,base_cv,fraction,subsample_test,random_state)
sklearn.model_selection._search_successive_halving._SubsampleMetaSplitter.split(self,X,y,**kwargs)
sklearn.model_selection._search_successive_halving._top_k(results,k,itr)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/_search.py----------------------------------------
A:sklearn.model_selection._search.items->sorted(dist.items())
A:sklearn.model_selection._search.(keys, values)->zip(*items)
A:sklearn.model_selection._search.params->_check_method_params(X, params=params)
A:sklearn.model_selection._search.product->partial(reduce, operator.mul)
A:sklearn.model_selection._search.(keys, values_lists)->zip(*sorted(sub_grid.items())[::-1])
A:sklearn.model_selection._search.total->numpy.prod(sizes)
A:sklearn.model_selection._search.(ind, offset)->divmod(ind, n)
A:sklearn.model_selection._search.rng->check_random_state(self.random_state)
A:sklearn.model_selection._search.param_grid->ParameterGrid(self.param_distributions)
A:sklearn.model_selection._search.grid_size->len(ParameterGrid(self.param_distributions))
A:sklearn.model_selection._search.dist->check_random_state(self.random_state).choice(self.param_distributions)
A:sklearn.model_selection._search.params[k]->v.rvs(random_state=rng)
A:sklearn.model_selection._search.score_params->dict()
A:sklearn.model_selection._search.score->self.scorer_(self.best_estimator_, X, y, **score_params)
A:sklearn.model_selection._search.best_index->results[f'rank_test_{refit_metric}'].argmin()
A:sklearn.model_selection._search.scorers->_MultimetricScorer(scorers=scorers, raise_exc=self.error_score == 'raise')
A:sklearn.model_selection._search.routed_params->self._get_routed_params_for_fit(params)
A:sklearn.model_selection._search.groups->_check_method_params(X, params=params).pop('groups', None)
A:sklearn.model_selection._search.(scorers, refit_metric)->self._get_scorers(convert_multimetric=False)
A:sklearn.model_selection._search.(X, y)->indexable(X, y)
A:sklearn.model_selection._search.cv_orig->check_cv(self.cv, y, classifier=is_classifier(estimator))
A:sklearn.model_selection._search.n_splits->check_cv(self.cv, y, classifier=is_classifier(estimator)).get_n_splits(X, y, **routed_params.splitter.split)
A:sklearn.model_selection._search.base_estimator->clone(self.estimator)
A:sklearn.model_selection._search.parallel->Parallel(n_jobs=self.n_jobs, pre_dispatch=self.pre_dispatch)
A:sklearn.model_selection._search.fit_and_score_kwargs->dict(scorer=scorers, fit_params=routed_params.estimator.fit, score_params=routed_params.scorer.score, return_train_score=self.return_train_score, return_n_test_samples=True, return_times=True, return_parameters=False, error_score=self.error_score, verbose=self.verbose)
A:sklearn.model_selection._search.all_more_results->defaultdict(list)
A:sklearn.model_selection._search.candidate_params->list(candidate_params)
A:sklearn.model_selection._search.n_candidates->len(candidate_params)
A:sklearn.model_selection._search.out->_aggregate_score_dicts(out)
A:sklearn.model_selection._search.results->dict(more_results or {})
A:sklearn.model_selection._search.self.multimetric_->isinstance(first_test_score, dict)
A:sklearn.model_selection._search.self.best_index_->self._select_best_index(self.refit, refit_metric, results)
A:sklearn.model_selection._search.self.best_estimator_->clone(base_estimator).set_params(**clone(self.best_params_, safe=False))
A:sklearn.model_selection._search.refit_start_time->time.time()
A:sklearn.model_selection._search.refit_end_time->time.time()
A:sklearn.model_selection._search.results[key]->numpy.asarray(val)
A:sklearn.model_selection._search.array->numpy.array(array, dtype=np.float64).reshape(n_candidates, n_splits)
A:sklearn.model_selection._search.array_means->numpy.nan_to_num(array_means, nan=min_array_means)
A:sklearn.model_selection._search.array_stds->numpy.sqrt(np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights))
A:sklearn.model_selection._search.rank_result->rankdata(-array_means, method='min').astype(np.int32, copy=False)
A:sklearn.model_selection._search.param_results->defaultdict(partial(MaskedArray, np.empty(n_candidates), mask=True, dtype=object))
A:sklearn.model_selection._search.test_scores_dict->_normalize_score_results(out['test_scores'])
A:sklearn.model_selection._search.train_scores_dict->_normalize_score_results(out['train_scores'])
A:sklearn.model_selection._search.router->MetadataRouter(owner=self.__class__.__name__)
A:sklearn.model_selection._search.(scorer, _)->self._get_scorers(convert_multimetric=True)
sklearn.model_selection.GridSearchCV(self,estimator,param_grid,*,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score=np.nan,return_train_score=False)
sklearn.model_selection.GridSearchCV._run_search(self,evaluate_candidates)
sklearn.model_selection.ParameterGrid(self,param_grid)
sklearn.model_selection.ParameterGrid.__getitem__(self,ind)
sklearn.model_selection.ParameterGrid.__iter__(self)
sklearn.model_selection.ParameterGrid.__len__(self)
sklearn.model_selection.ParameterSampler(self,param_distributions,n_iter,*,random_state=None)
sklearn.model_selection.ParameterSampler.__iter__(self)
sklearn.model_selection.ParameterSampler.__len__(self)
sklearn.model_selection.ParameterSampler._is_all_lists(self)
sklearn.model_selection.RandomizedSearchCV(self,estimator,param_distributions,*,n_iter=10,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score=np.nan,return_train_score=False)
sklearn.model_selection.RandomizedSearchCV._run_search(self,evaluate_candidates)
sklearn.model_selection._search.BaseSearchCV(self,estimator,*,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score=np.nan,return_train_score=True)
sklearn.model_selection._search.BaseSearchCV.__init__(self,estimator,*,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score=np.nan,return_train_score=True)
sklearn.model_selection._search.BaseSearchCV._check_refit_for_multimetric(self,scores)
sklearn.model_selection._search.BaseSearchCV._estimator_type(self)
sklearn.model_selection._search.BaseSearchCV._format_results(self,candidate_params,n_splits,out,more_results=None)
sklearn.model_selection._search.BaseSearchCV._get_routed_params_for_fit(self,params)
sklearn.model_selection._search.BaseSearchCV._get_scorers(self,convert_multimetric)
sklearn.model_selection._search.BaseSearchCV._more_tags(self)
sklearn.model_selection._search.BaseSearchCV._run_search(self,evaluate_candidates)
sklearn.model_selection._search.BaseSearchCV._select_best_index(refit,refit_metric,results)
sklearn.model_selection._search.BaseSearchCV.classes_(self)
sklearn.model_selection._search.BaseSearchCV.decision_function(self,X)
sklearn.model_selection._search.BaseSearchCV.fit(self,X,y=None,**params)
sklearn.model_selection._search.BaseSearchCV.get_metadata_routing(self)
sklearn.model_selection._search.BaseSearchCV.inverse_transform(self,Xt)
sklearn.model_selection._search.BaseSearchCV.n_features_in_(self)
sklearn.model_selection._search.BaseSearchCV.predict(self,X)
sklearn.model_selection._search.BaseSearchCV.predict_log_proba(self,X)
sklearn.model_selection._search.BaseSearchCV.predict_proba(self,X)
sklearn.model_selection._search.BaseSearchCV.score(self,X,y=None,**params)
sklearn.model_selection._search.BaseSearchCV.score_samples(self,X)
sklearn.model_selection._search.BaseSearchCV.transform(self,X)
sklearn.model_selection._search.GridSearchCV(self,estimator,param_grid,*,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score=np.nan,return_train_score=False)
sklearn.model_selection._search.GridSearchCV.__init__(self,estimator,param_grid,*,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',error_score=np.nan,return_train_score=False)
sklearn.model_selection._search.GridSearchCV._run_search(self,evaluate_candidates)
sklearn.model_selection._search.ParameterGrid(self,param_grid)
sklearn.model_selection._search.ParameterGrid.__getitem__(self,ind)
sklearn.model_selection._search.ParameterGrid.__init__(self,param_grid)
sklearn.model_selection._search.ParameterGrid.__iter__(self)
sklearn.model_selection._search.ParameterGrid.__len__(self)
sklearn.model_selection._search.ParameterSampler(self,param_distributions,n_iter,*,random_state=None)
sklearn.model_selection._search.ParameterSampler.__init__(self,param_distributions,n_iter,*,random_state=None)
sklearn.model_selection._search.ParameterSampler.__iter__(self)
sklearn.model_selection._search.ParameterSampler.__len__(self)
sklearn.model_selection._search.ParameterSampler._is_all_lists(self)
sklearn.model_selection._search.RandomizedSearchCV(self,estimator,param_distributions,*,n_iter=10,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score=np.nan,return_train_score=False)
sklearn.model_selection._search.RandomizedSearchCV.__init__(self,estimator,param_distributions,*,n_iter=10,scoring=None,n_jobs=None,refit=True,cv=None,verbose=0,pre_dispatch='2*n_jobs',random_state=None,error_score=np.nan,return_train_score=False)
sklearn.model_selection._search.RandomizedSearchCV._run_search(self,evaluate_candidates)
sklearn.model_selection._search._check_refit(search_cv,attr)
sklearn.model_selection._search._estimator_has(attr)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/_split.py----------------------------------------
A:sklearn.model_selection._split.(X, y, groups)->indexable(X, y, groups)
A:sklearn.model_selection._split.indices->numpy.arange(n_samples)
A:sklearn.model_selection._split.test_mask->numpy.zeros(len(self.test_fold), dtype=bool)
A:sklearn.model_selection._split.n_samples->_num_samples(arrays[0])
A:sklearn.model_selection._split.n_splits->int(n_splits)
A:sklearn.model_selection._split.fold_sizes->numpy.full(n_splits, n_samples // n_splits, dtype=int)
A:sklearn.model_selection._split.groups->check_array(groups, input_name='groups', ensure_2d=False, dtype=None)
A:sklearn.model_selection._split.(unique_groups, groups)->numpy.unique(groups, return_inverse=True)
A:sklearn.model_selection._split.n_groups->len(unique_groups)
A:sklearn.model_selection._split.n_samples_per_group->numpy.bincount(groups)
A:sklearn.model_selection._split.n_samples_per_fold->numpy.zeros(self.n_splits)
A:sklearn.model_selection._split.group_to_fold->numpy.zeros(len(unique_groups))
A:sklearn.model_selection._split.lightest_fold->numpy.argmin(n_samples_per_fold)
A:sklearn.model_selection._split.rng->check_random_state(self.random_state)
A:sklearn.model_selection._split.y->check_array(y, input_name='y', ensure_2d=False, dtype=None)
A:sklearn.model_selection._split.type_of_target_y->type_of_target(y)
A:sklearn.model_selection._split.(_, y_idx, y_inv)->numpy.unique(y, return_index=True, return_inverse=True)
A:sklearn.model_selection._split.(_, class_perm)->numpy.unique(y_idx, return_inverse=True)
A:sklearn.model_selection._split.n_classes->len(y_cnt)
A:sklearn.model_selection._split.y_counts->numpy.bincount(y_encoded)
A:sklearn.model_selection._split.min_groups->numpy.min(y_counts)
A:sklearn.model_selection._split.y_order->numpy.sort(y_encoded)
A:sklearn.model_selection._split.allocation->numpy.asarray([np.bincount(y_order[i::self.n_splits], minlength=n_classes) for i in range(self.n_splits)])
A:sklearn.model_selection._split.test_folds->self._make_test_folds(X, y)
A:sklearn.model_selection._split.folds_for_class->numpy.arange(self.n_splits).repeat(allocation[:, k])
A:sklearn.model_selection._split.(_, y_inv, y_cnt)->numpy.unique(y, return_inverse=True, return_counts=True)
A:sklearn.model_selection._split.n_smallest_class->numpy.min(y_cnt)
A:sklearn.model_selection._split.(_, groups_inv, groups_cnt)->numpy.unique(groups, return_inverse=True, return_counts=True)
A:sklearn.model_selection._split.y_counts_per_group->numpy.zeros((len(groups_cnt), n_classes))
A:sklearn.model_selection._split.y_counts_per_fold->numpy.zeros((self.n_splits, n_classes))
A:sklearn.model_selection._split.groups_per_fold->defaultdict(set)
A:sklearn.model_selection._split.sorted_groups_idx->numpy.argsort(-np.std(y_counts_per_group, axis=1), kind='mergesort')
A:sklearn.model_selection._split.best_fold->self._find_best_fold(y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt, group_y_counts=group_y_counts)
A:sklearn.model_selection._split.std_per_class->numpy.std(y_counts_per_fold / y_cnt.reshape(1, -1), axis=0)
A:sklearn.model_selection._split.fold_eval->numpy.mean(std_per_class)
A:sklearn.model_selection._split.samples_in_fold->numpy.sum(y_counts_per_fold[i])
A:sklearn.model_selection._split.test_starts->range(n_samples - n_splits * test_size, n_samples, test_size)
A:sklearn.model_selection._split.unique_groups->numpy.unique(groups)
A:sklearn.model_selection._split.combi->combinations(range(len(unique_groups)), self.n_groups)
A:sklearn.model_selection._split.test_index->numpy.zeros(_num_samples(X), dtype=bool)
A:sklearn.model_selection._split.cv->CVClass(test_size=n_test, train_size=n_train, random_state=random_state)
A:sklearn.model_selection._split.(n_train, n_test)->_validate_shuffle_split(n_samples, test_size, train_size, default_test_size=0.25)
A:sklearn.model_selection._split.permutation->check_random_state(self.random_state).permutation(class_counts[i])
A:sklearn.model_selection._split.(classes, group_indices)->numpy.unique(groups, return_inverse=True)
A:sklearn.model_selection._split.train->numpy.arange(n_train)
A:sklearn.model_selection._split.test->numpy.arange(n_train, n_train + n_test)
A:sklearn.model_selection._split.(classes, y_indices)->numpy.unique(y, return_inverse=True)
A:sklearn.model_selection._split.class_counts->numpy.bincount(y_indices)
A:sklearn.model_selection._split.class_indices->numpy.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
A:sklearn.model_selection._split.n_i->_approximate_mode(class_counts, n_train, rng)
A:sklearn.model_selection._split.t_i->_approximate_mode(class_counts_remaining, n_test, rng)
A:sklearn.model_selection._split.perm_indices_class_i->class_indices[i].take(permutation, mode='clip')
A:sklearn.model_selection._split.n_test->float(test_size)
A:sklearn.model_selection._split.n_train->float(train_size)
A:sklearn.model_selection._split.self.test_fold->column_or_1d(self.test_fold)
A:sklearn.model_selection._split.self.unique_folds->numpy.unique(self.test_fold)
A:sklearn.model_selection._split.ind->numpy.arange(len(self.test_fold))
A:sklearn.model_selection._split.self.cv->list(cv)
A:sklearn.model_selection._split.n_arrays->len(arrays)
A:sklearn.model_selection._split.arrays->indexable(*arrays)
A:sklearn.model_selection._split.(train, test)->next(cv.split(X=arrays[0], y=stratify))
A:sklearn.model_selection._split.options->numpy.get_printoptions()
A:sklearn.model_selection._split.params_list->list()
A:sklearn.model_selection._split.this_line_length->len(line_sep)
A:sklearn.model_selection._split.lines->'\n'.join((l.rstrip(' ') for l in lines.split('\n')))
A:sklearn.model_selection._split.init->getattr(cls.__init__, 'deprecated_original', cls.__init__)
A:sklearn.model_selection._split.init_signature->signature(init)
A:sklearn.model_selection._split.args->sorted([p.name for p in init_signature.parameters.values() if p.name != 'self' and p.kind != p.VAR_KEYWORD])
A:sklearn.model_selection._split.params->dict()
A:sklearn.model_selection._split.value->self.cvargs.get(key, None)
A:sklearn.model_selection._split.shuffle->getattr(cv, 'shuffle', True)
A:sklearn.model_selection._split.random_state->getattr(cv, 'random_state', 0)
sklearn.model_selection.BaseCrossValidator(_MetadataRequester,metaclass=ABCMeta)
sklearn.model_selection.BaseCrossValidator.__repr__(self)
sklearn.model_selection.BaseCrossValidator._iter_test_indices(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator._iter_test_masks(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseCrossValidator.split(self,X,y=None,groups=None)
sklearn.model_selection.BaseShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection.BaseShuffleSplit.__repr__(self)
sklearn.model_selection.BaseShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection.BaseShuffleSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.BaseShuffleSplit.split(self,X,y=None,groups=None)
sklearn.model_selection.GroupKFold(self,n_splits=5)
sklearn.model_selection.GroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection.GroupKFold.split(self,X,y=None,groups=None)
sklearn.model_selection.GroupShuffleSplit(self,n_splits=5,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection.GroupShuffleSplit._iter_indices(self,X,y,groups)
sklearn.model_selection.GroupShuffleSplit.split(self,X,y=None,groups=None)
sklearn.model_selection.KFold(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection.KFold._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeaveOneGroupOut(GroupsConsumerMixin,BaseCrossValidator)
sklearn.model_selection.LeaveOneGroupOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection.LeaveOneGroupOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.LeaveOneGroupOut.split(self,X,y=None,groups=None)
sklearn.model_selection.LeaveOneOut(BaseCrossValidator)
sklearn.model_selection.LeaveOneOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeaveOneOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection.LeavePGroupsOut(self,n_groups)
sklearn.model_selection.LeavePGroupsOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection.LeavePGroupsOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.LeavePGroupsOut.split(self,X,y=None,groups=None)
sklearn.model_selection.LeavePOut(self,p)
sklearn.model_selection.LeavePOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection.LeavePOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection.PredefinedSplit(self,test_fold)
sklearn.model_selection.PredefinedSplit._iter_test_masks(self)
sklearn.model_selection.PredefinedSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.PredefinedSplit.split(self,X=None,y=None,groups=None)
sklearn.model_selection.RepeatedKFold(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection.RepeatedStratifiedKFold(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection.ShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection.ShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection.StratifiedGroupKFold(self,n_splits=5,shuffle=False,random_state=None)
sklearn.model_selection.StratifiedGroupKFold._find_best_fold(self,y_counts_per_fold,y_cnt,group_y_counts)
sklearn.model_selection.StratifiedGroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection.StratifiedKFold(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection.StratifiedKFold._iter_test_masks(self,X,y=None,groups=None)
sklearn.model_selection.StratifiedKFold._make_test_folds(self,X,y=None)
sklearn.model_selection.StratifiedKFold.split(self,X,y,groups=None)
sklearn.model_selection.StratifiedShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection.StratifiedShuffleSplit._iter_indices(self,X,y,groups=None)
sklearn.model_selection.StratifiedShuffleSplit.split(self,X,y,groups=None)
sklearn.model_selection.TimeSeriesSplit(self,n_splits=5,*,max_train_size=None,test_size=None,gap=0)
sklearn.model_selection.TimeSeriesSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator(_MetadataRequester,metaclass=ABCMeta)
sklearn.model_selection._split.BaseCrossValidator.__repr__(self)
sklearn.model_selection._split.BaseCrossValidator._iter_test_indices(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator._iter_test_masks(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseCrossValidator.split(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.BaseShuffleSplit.__init__(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.BaseShuffleSplit.__repr__(self)
sklearn.model_selection._split.BaseShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.BaseShuffleSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split.GroupKFold(self,n_splits=5)
sklearn.model_selection._split.GroupKFold.__init__(self,n_splits=5)
sklearn.model_selection._split.GroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection._split.GroupKFold.split(self,X,y=None,groups=None)
sklearn.model_selection._split.GroupShuffleSplit(self,n_splits=5,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.GroupShuffleSplit.__init__(self,n_splits=5,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.GroupShuffleSplit._iter_indices(self,X,y,groups)
sklearn.model_selection._split.GroupShuffleSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split.GroupsConsumerMixin(_MetadataRequester)
sklearn.model_selection._split.KFold(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection._split.KFold.__init__(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection._split.KFold._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeaveOneGroupOut(GroupsConsumerMixin,BaseCrossValidator)
sklearn.model_selection._split.LeaveOneGroupOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection._split.LeaveOneGroupOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.LeaveOneGroupOut.split(self,X,y=None,groups=None)
sklearn.model_selection._split.LeaveOneOut(BaseCrossValidator)
sklearn.model_selection._split.LeaveOneOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeaveOneOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection._split.LeavePGroupsOut(self,n_groups)
sklearn.model_selection._split.LeavePGroupsOut.__init__(self,n_groups)
sklearn.model_selection._split.LeavePGroupsOut._iter_test_masks(self,X,y,groups)
sklearn.model_selection._split.LeavePGroupsOut.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.LeavePGroupsOut.split(self,X,y=None,groups=None)
sklearn.model_selection._split.LeavePOut(self,p)
sklearn.model_selection._split.LeavePOut.__init__(self,p)
sklearn.model_selection._split.LeavePOut._iter_test_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.LeavePOut.get_n_splits(self,X,y=None,groups=None)
sklearn.model_selection._split.PredefinedSplit(self,test_fold)
sklearn.model_selection._split.PredefinedSplit.__init__(self,test_fold)
sklearn.model_selection._split.PredefinedSplit._iter_test_masks(self)
sklearn.model_selection._split.PredefinedSplit.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split.PredefinedSplit.split(self,X=None,y=None,groups=None)
sklearn.model_selection._split.RepeatedKFold(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedKFold.__init__(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedStratifiedKFold(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.RepeatedStratifiedKFold.__init__(self,*,n_splits=5,n_repeats=10,random_state=None)
sklearn.model_selection._split.ShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.ShuffleSplit.__init__(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.ShuffleSplit._iter_indices(self,X,y=None,groups=None)
sklearn.model_selection._split.StratifiedGroupKFold(self,n_splits=5,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedGroupKFold.__init__(self,n_splits=5,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedGroupKFold._find_best_fold(self,y_counts_per_fold,y_cnt,group_y_counts)
sklearn.model_selection._split.StratifiedGroupKFold._iter_test_indices(self,X,y,groups)
sklearn.model_selection._split.StratifiedKFold(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedKFold.__init__(self,n_splits=5,*,shuffle=False,random_state=None)
sklearn.model_selection._split.StratifiedKFold._iter_test_masks(self,X,y=None,groups=None)
sklearn.model_selection._split.StratifiedKFold._make_test_folds(self,X,y=None)
sklearn.model_selection._split.StratifiedKFold.split(self,X,y,groups=None)
sklearn.model_selection._split.StratifiedShuffleSplit(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.StratifiedShuffleSplit.__init__(self,n_splits=10,*,test_size=None,train_size=None,random_state=None)
sklearn.model_selection._split.StratifiedShuffleSplit._iter_indices(self,X,y,groups=None)
sklearn.model_selection._split.StratifiedShuffleSplit.split(self,X,y,groups=None)
sklearn.model_selection._split.TimeSeriesSplit(self,n_splits=5,*,max_train_size=None,test_size=None,gap=0)
sklearn.model_selection._split.TimeSeriesSplit.__init__(self,n_splits=5,*,max_train_size=None,test_size=None,gap=0)
sklearn.model_selection._split.TimeSeriesSplit.split(self,X,y=None,groups=None)
sklearn.model_selection._split._BaseKFold(self,n_splits,*,shuffle,random_state)
sklearn.model_selection._split._BaseKFold.__init__(self,n_splits,*,shuffle,random_state)
sklearn.model_selection._split._BaseKFold.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._BaseKFold.split(self,X,y=None,groups=None)
sklearn.model_selection._split._CVIterableWrapper(self,cv)
sklearn.model_selection._split._CVIterableWrapper.__init__(self,cv)
sklearn.model_selection._split._CVIterableWrapper.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._CVIterableWrapper.split(self,X=None,y=None,groups=None)
sklearn.model_selection._split._RepeatedSplits(self,cv,*,n_repeats=10,random_state=None,**cvargs)
sklearn.model_selection._split._RepeatedSplits.__init__(self,cv,*,n_repeats=10,random_state=None,**cvargs)
sklearn.model_selection._split._RepeatedSplits.__repr__(self)
sklearn.model_selection._split._RepeatedSplits.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection._split._RepeatedSplits.split(self,X,y=None,groups=None)
sklearn.model_selection._split._build_repr(self)
sklearn.model_selection._split._pprint(params,offset=0,printer=repr)
sklearn.model_selection._split._validate_shuffle_split(n_samples,test_size,train_size,default_test_size=None)
sklearn.model_selection._split._yields_constant_splits(cv)
sklearn.model_selection._split.check_cv(cv=5,y=None,*,classifier=False)
sklearn.model_selection._split.train_test_split(*arrays,test_size=None,train_size=None,random_state=None,shuffle=True,stratify=None)
sklearn.model_selection.check_cv(cv=5,y=None,*,classifier=False)
sklearn.model_selection.train_test_split(*arrays,test_size=None,train_size=None,random_state=None,shuffle=True,stratify=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/_validation.py----------------------------------------
A:sklearn.model_selection._validation.params->_check_params_groups_deprecation(fit_params, params, groups)
A:sklearn.model_selection._validation.(X, y)->indexable(X, y)
A:sklearn.model_selection._validation.cv->check_cv(cv, y, classifier=is_classifier(estimator))
A:sklearn.model_selection._validation.scorers->_check_multimetric_scoring(estimator, scoring)
A:sklearn.model_selection._validation._scorer->_MultimetricScorer(scorers=scorers, raise_exc=error_score == 'raise')
A:sklearn.model_selection._validation.router->MetadataRouter(owner='cross_validate').add(splitter=cv, method_mapping=MethodMapping().add(caller='fit', callee='split')).add(estimator=estimator, method_mapping=MethodMapping().add(caller='fit', callee='fit'))
A:sklearn.model_selection._validation.routed_params->Bunch()
A:sklearn.model_selection._validation.routed_params.splitter->Bunch(split={'groups': groups})
A:sklearn.model_selection._validation.routed_params.estimator->Bunch(fit=params)
A:sklearn.model_selection._validation.routed_params.scorer->Bunch(score={})
A:sklearn.model_selection._validation.indices->numpy.arange(len(groups))
A:sklearn.model_selection._validation.parallel->Parallel(n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose)
A:sklearn.model_selection._validation.results->_aggregate_score_dicts(results)
A:sklearn.model_selection._validation.(ret['indices']['train'], ret['indices']['test'])->zip(*indices)
A:sklearn.model_selection._validation.test_scores_dict->_normalize_score_results(results['test_scores'])
A:sklearn.model_selection._validation.train_scores_dict->_normalize_score_results(results['train_scores'])
A:sklearn.model_selection._validation.results[i]['test_scores']->formatted_error.copy()
A:sklearn.model_selection._validation.results[i]['train_scores']->formatted_error.copy()
A:sklearn.model_selection._validation.num_failed_fits->len(fit_errors)
A:sklearn.model_selection._validation.num_fits->len(results)
A:sklearn.model_selection._validation.fit_errors_counter->Counter(fit_errors)
A:sklearn.model_selection._validation.fit_errors_summary->'\n'.join((f'{delimiter}{n} fits failed with the following error:\n{error}' for (error, n) in fit_errors_counter.items()))
A:sklearn.model_selection._validation.scorer->check_scoring(estimator, scoring=scoring)
A:sklearn.model_selection._validation.cv_results->cross_validate(estimator=estimator, X=X, y=y, groups=groups, scoring={'score': scorer}, cv=cv, n_jobs=n_jobs, verbose=verbose, fit_params=fit_params, params=params, pre_dispatch=pre_dispatch, error_score=error_score)
A:sklearn.model_selection._validation.sorted_keys->sorted(parameters)
A:sklearn.model_selection._validation.params_msg->', '.join((f'{k}={parameters[k]}' for k in sorted_keys))
A:sklearn.model_selection._validation.fit_params->_check_method_params(X, params=fit_params, indices=train)
A:sklearn.model_selection._validation.score_params_train->_check_method_params(X, params=score_params, indices=train)
A:sklearn.model_selection._validation.score_params_test->_check_method_params(X, params=score_params, indices=test)
A:sklearn.model_selection._validation.estimator->estimator.set_params(**clone(parameters, safe=False)).set_params(**clone(parameters, safe=False))
A:sklearn.model_selection._validation.start_time->time.time()
A:sklearn.model_selection._validation.(X_train, y_train)->_safe_split(estimator, X, y, train_subset)
A:sklearn.model_selection._validation.(X_test, y_test)->_safe_split(estimator, X, y, test, train_subset)
A:sklearn.model_selection._validation.train_scores->_score(estimator, X_train, y_train, scorer, score_params_train, error_score)
A:sklearn.model_selection._validation.result['fit_error']->format_exc()
A:sklearn.model_selection._validation.test_scores->_score(estimator, X_test, y_test, scorer, score_params_test, error_score)
A:sklearn.model_selection._validation.result['n_test_samples']->_num_samples(X_test)
A:sklearn.model_selection._validation.scores->scores.item().item()
A:sklearn.model_selection._validation.score->_permutation_test_score(clone(estimator), X, y, groups, cv, scorer, fit_params=fit_params)
A:sklearn.model_selection._validation.splits->list(cv.split(X, y, **routed_params.splitter.split))
A:sklearn.model_selection._validation.test_indices->numpy.concatenate([test for (_, test) in splits])
A:sklearn.model_selection._validation.y->LabelEncoder().fit_transform(y)
A:sklearn.model_selection._validation.le->LabelEncoder()
A:sklearn.model_selection._validation.y_enc->numpy.zeros_like(y, dtype=int)
A:sklearn.model_selection._validation.y_enc[:, i_label]->LabelEncoder().fit_transform(y[:, i_label])
A:sklearn.model_selection._validation.predictions->_enforce_prediction_order(estimator.classes_, predictions, n_classes, method)
A:sklearn.model_selection._validation.inv_test_indices->numpy.empty(len(test_indices), dtype=int)
A:sklearn.model_selection._validation.inv_test_indices[test_indices]->numpy.arange(len(test_indices))
A:sklearn.model_selection._validation.label_preds->numpy.concatenate([p[i_label] for p in predictions])
A:sklearn.model_selection._validation.(X_test, _)->_safe_split(estimator, X, y, test, train)
A:sklearn.model_selection._validation.func->getattr(estimator, method)
A:sklearn.model_selection._validation.predictions_for_all_classes->numpy.full((_num_samples(predictions), n_classes), default_values[method], dtype=predictions.dtype)
A:sklearn.model_selection._validation.hit->numpy.zeros(n_samples, dtype=bool)
A:sklearn.model_selection._validation.(X, y, groups)->indexable(X, y, groups)
A:sklearn.model_selection._validation.random_state->check_random_state(random_state)
A:sklearn.model_selection._validation.permutation_scores->numpy.array(permutation_scores)
A:sklearn.model_selection._validation.indices[this_mask]->check_random_state(random_state).permutation(indices[this_mask])
A:sklearn.model_selection._validation.cv_iter->list(cv.split(X, y, groups))
A:sklearn.model_selection._validation.n_max_training_samples->len(cv_iter[0][0])
A:sklearn.model_selection._validation.train_sizes_abs->numpy.unique(train_sizes_abs)
A:sklearn.model_selection._validation.rng->check_random_state(random_state)
A:sklearn.model_selection._validation.out->numpy.asarray(out).transpose((2, 1, 0))
A:sklearn.model_selection._validation.n_min_required_samples->numpy.min(train_sizes_abs)
A:sklearn.model_selection._validation.n_max_required_samples->numpy.max(train_sizes_abs)
A:sklearn.model_selection._validation.partitions->zip(train_sizes, np.split(train, train_sizes)[:-1])
A:sklearn.model_selection._validation.partial_fit_func->partial(estimator.partial_fit, classes=classes, **fit_params)
A:sklearn.model_selection._validation.(X_partial_train, y_partial_train)->_safe_split(estimator, X, y, partial_train)
A:sklearn.model_selection._validation.start_fit->time.time()
A:sklearn.model_selection._validation.start_score->time.time()
A:sklearn.model_selection._validation.n_params->len(param_range)
sklearn.model_selection._validation._aggregate_score_dicts(scores)
sklearn.model_selection._validation._check_is_permutation(indices,n_samples)
sklearn.model_selection._validation._check_params_groups_deprecation(fit_params,params,groups)
sklearn.model_selection._validation._enforce_prediction_order(classes,predictions,n_classes,method)
sklearn.model_selection._validation._fit_and_predict(estimator,X,y,train,test,fit_params,method)
sklearn.model_selection._validation._fit_and_score(estimator,X,y,*,scorer,train,test,verbose,parameters,fit_params,score_params,return_train_score=False,return_parameters=False,return_n_test_samples=False,return_times=False,return_estimator=False,split_progress=None,candidate_progress=None,error_score=np.nan)
sklearn.model_selection._validation._incremental_fit_estimator(estimator,X,y,classes,train,test,train_sizes,scorer,return_times,error_score,fit_params)
sklearn.model_selection._validation._insert_error_scores(results,error_score)
sklearn.model_selection._validation._normalize_score_results(scores,scaler_score_key='score')
sklearn.model_selection._validation._permutation_test_score(estimator,X,y,groups,cv,scorer,fit_params)
sklearn.model_selection._validation._score(estimator,X_test,y_test,scorer,score_params,error_score='raise')
sklearn.model_selection._validation._shuffle(y,groups,random_state)
sklearn.model_selection._validation._translate_train_sizes(train_sizes,n_max_training_samples)
sklearn.model_selection._validation._warn_or_raise_about_fit_failures(results,error_score)
sklearn.model_selection._validation.cross_val_predict(estimator,X,y=None,*,groups=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',method='predict')
sklearn.model_selection._validation.cross_val_score(estimator,X,y=None,*,groups=None,scoring=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',error_score=np.nan)
sklearn.model_selection._validation.cross_validate(estimator,X,y=None,*,groups=None,scoring=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',return_train_score=False,return_estimator=False,return_indices=False,error_score=np.nan)
sklearn.model_selection._validation.learning_curve(estimator,X,y,*,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=None,pre_dispatch='all',verbose=0,shuffle=False,random_state=None,error_score=np.nan,return_times=False,fit_params=None)
sklearn.model_selection._validation.permutation_test_score(estimator,X,y,*,groups=None,cv=None,n_permutations=100,n_jobs=None,random_state=0,verbose=0,scoring=None,fit_params=None)
sklearn.model_selection._validation.validation_curve(estimator,X,y,*,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=None,pre_dispatch='all',verbose=0,error_score=np.nan,fit_params=None)
sklearn.model_selection.cross_val_predict(estimator,X,y=None,*,groups=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',method='predict')
sklearn.model_selection.cross_val_score(estimator,X,y=None,*,groups=None,scoring=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',error_score=np.nan)
sklearn.model_selection.cross_validate(estimator,X,y=None,*,groups=None,scoring=None,cv=None,n_jobs=None,verbose=0,fit_params=None,params=None,pre_dispatch='2*n_jobs',return_train_score=False,return_estimator=False,return_indices=False,error_score=np.nan)
sklearn.model_selection.learning_curve(estimator,X,y,*,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=None,pre_dispatch='all',verbose=0,shuffle=False,random_state=None,error_score=np.nan,return_times=False,fit_params=None)
sklearn.model_selection.permutation_test_score(estimator,X,y,*,groups=None,cv=None,n_permutations=100,n_jobs=None,random_state=0,verbose=0,scoring=None,fit_params=None)
sklearn.model_selection.validation_curve(estimator,X,y,*,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=None,pre_dispatch='all',verbose=0,error_score=np.nan,fit_params=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/_plot.py----------------------------------------
A:sklearn.model_selection._plot.(_, ax)->matplotlib.pyplot.subplots()
A:sklearn.model_selection._plot.score_name->_validate_score_name(score_name, scoring, negate_score)
A:sklearn.model_selection._plot.(train_sizes, train_scores, test_scores)->learning_curve(estimator, X, y, groups=groups, train_sizes=train_sizes, cv=cv, scoring=scoring, exploit_incremental_learning=exploit_incremental_learning, n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose, shuffle=shuffle, random_state=random_state, error_score=error_score, return_times=False, fit_params=fit_params)
A:sklearn.model_selection._plot.viz->cls(param_name=param_name, param_range=np.array(param_range, copy=False), train_scores=train_scores, test_scores=test_scores, score_name=score_name)
A:sklearn.model_selection._plot.(train_scores, test_scores)->validation_curve(estimator, X, y, param_name=param_name, param_range=param_range, groups=groups, cv=cv, scoring=scoring, n_jobs=n_jobs, pre_dispatch=pre_dispatch, verbose=verbose, error_score=error_score, fit_params=fit_params)
sklearn.model_selection.LearningCurveDisplay(self,*,train_sizes,train_scores,test_scores,score_name=None)
sklearn.model_selection.LearningCurveDisplay.from_estimator(cls,estimator,X,y,*,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=None,pre_dispatch='all',verbose=0,shuffle=False,random_state=None,error_score=np.nan,fit_params=None,ax=None,negate_score=False,score_name=None,score_type='both',log_scale='deprecated',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection.LearningCurveDisplay.plot(self,ax=None,*,negate_score=False,score_name=None,score_type='both',log_scale='deprecated',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection.ValidationCurveDisplay(self,*,param_name,param_range,train_scores,test_scores,score_name=None)
sklearn.model_selection.ValidationCurveDisplay.from_estimator(cls,estimator,X,y,*,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=None,pre_dispatch='all',verbose=0,error_score=np.nan,fit_params=None,ax=None,negate_score=False,score_name=None,score_type='both',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection.ValidationCurveDisplay.plot(self,ax=None,*,negate_score=False,score_name=None,score_type='both',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection._plot.LearningCurveDisplay(self,*,train_sizes,train_scores,test_scores,score_name=None)
sklearn.model_selection._plot.LearningCurveDisplay.__init__(self,*,train_sizes,train_scores,test_scores,score_name=None)
sklearn.model_selection._plot.LearningCurveDisplay.from_estimator(cls,estimator,X,y,*,groups=None,train_sizes=np.linspace(0.1,1.0,5),cv=None,scoring=None,exploit_incremental_learning=False,n_jobs=None,pre_dispatch='all',verbose=0,shuffle=False,random_state=None,error_score=np.nan,fit_params=None,ax=None,negate_score=False,score_name=None,score_type='both',log_scale='deprecated',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection._plot.LearningCurveDisplay.plot(self,ax=None,*,negate_score=False,score_name=None,score_type='both',log_scale='deprecated',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection._plot.ValidationCurveDisplay(self,*,param_name,param_range,train_scores,test_scores,score_name=None)
sklearn.model_selection._plot.ValidationCurveDisplay.__init__(self,*,param_name,param_range,train_scores,test_scores,score_name=None)
sklearn.model_selection._plot.ValidationCurveDisplay.from_estimator(cls,estimator,X,y,*,param_name,param_range,groups=None,cv=None,scoring=None,n_jobs=None,pre_dispatch='all',verbose=0,error_score=np.nan,fit_params=None,ax=None,negate_score=False,score_name=None,score_type='both',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection._plot.ValidationCurveDisplay.plot(self,ax=None,*,negate_score=False,score_name=None,score_type='both',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)
sklearn.model_selection._plot._BaseCurveDisplay
sklearn.model_selection._plot._BaseCurveDisplay._plot_curve(self,x_data,*,ax=None,negate_score=False,score_name=None,score_type='test',log_scale='deprecated',std_display_style='fill_between',line_kw=None,fill_between_kw=None,errorbar_kw=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/test_successive_halving.py----------------------------------------
A:sklearn.model_selection.tests.test_successive_halving.params->super().get_params(deep=deep)
A:sklearn.model_selection.tests.test_successive_halving.(X, y)->make_classification(n_samples=150, n_features=4, random_state=42)
A:sklearn.model_selection.tests.test_successive_halving.search->HalvingRandomSearchCV(SVC(), cv=3, param_distributions=params, return_train_score=True, random_state=0)
A:sklearn.model_selection.tests.test_successive_halving.unique_nan_ranks->numpy.unique(ranks[np.isnan(scores)])
A:sklearn.model_selection.tests.test_successive_halving.base_estimator->FastClassifier()
A:sklearn.model_selection.tests.test_successive_halving.sh->Est(base_estimator, param_grid, factor=2, cv=n_splits, return_train_score=False, refit=False)
A:sklearn.model_selection.tests.test_successive_halving.cv->_SubsampleMetaSplitter(base_cv=KFold(5), fraction=0.5, subsample_test=subsample_test, random_state=None)
A:sklearn.model_selection.tests.test_successive_halving.folds_a->list(cv.split(X, y, groups=None))
A:sklearn.model_selection.tests.test_successive_halving.folds_b->list(cv.split(X, y, groups=None))
A:sklearn.model_selection.tests.test_successive_halving.got->_top_k(results, k=k, itr=itr)
A:sklearn.model_selection.tests.test_successive_halving.pd->pytest.importorskip('pandas')
A:sklearn.model_selection.tests.test_successive_halving.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_successive_halving.cv_results_df->pytest.importorskip('pandas').DataFrame(sh.cv_results_)
A:sklearn.model_selection.tests.test_successive_halving.cv_results_df['params_str']->cv_results_df['params'].apply(str)
A:sklearn.model_selection.tests.test_successive_halving.table->pytest.importorskip('pandas').DataFrame(sh.cv_results_).pivot(index='params_str', columns='iter', values='mean_test_score')
A:sklearn.model_selection.tests.test_successive_halving.nan_mask->pytest.importorskip('pandas').isna(table)
A:sklearn.model_selection.tests.test_successive_halving.discarded_max_score->table[it].where(discarded_now_mask).max()
A:sklearn.model_selection.tests.test_successive_halving.kept_min_score->table[it].where(kept_mask).min()
A:sklearn.model_selection.tests.test_successive_halving.last_iter->cv_results_df['iter'].max()
A:sklearn.model_selection.tests.test_successive_halving.idx_best_last_iter->cv_results_df[cv_results_df['iter'] == last_iter]['mean_test_score'].idxmax()
A:sklearn.model_selection.tests.test_successive_halving.idx_best_all_iters->cv_results_df['mean_test_score'].idxmax()
A:sklearn.model_selection.tests.test_successive_halving.(uniques, counts)->numpy.unique(passed_n_samples, return_counts=True)
A:sklearn.model_selection.tests.test_successive_halving.groups->numpy.random.RandomState(0).randint(0, 3, 50)
A:sklearn.model_selection.tests.test_successive_halving.clf->LinearSVC(dual='auto', random_state=0)
A:sklearn.model_selection.tests.test_successive_halving.gs->Est(clf, grid, cv=cv)
A:sklearn.model_selection.tests.test_successive_halving.X->numpy.empty(0).reshape(0, 3)
A:sklearn.model_selection.tests.test_successive_halving.best_index->SearchCV._select_best_index(None, None, results)
A:sklearn.model_selection.tests.test_successive_halving.n_candidates->sum(search.n_candidates_)
sklearn.model_selection.tests.test_successive_halving.FastClassifier(self,strategy='stratified',random_state=None,constant=None,**kwargs)
sklearn.model_selection.tests.test_successive_halving.FastClassifier.__init__(self,strategy='stratified',random_state=None,constant=None,**kwargs)
sklearn.model_selection.tests.test_successive_halving.FastClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_successive_halving.SometimesFailClassifier(self,strategy='stratified',random_state=None,constant=None,n_estimators=10,fail_fit=False,fail_predict=False,a=0)
sklearn.model_selection.tests.test_successive_halving.SometimesFailClassifier.__init__(self,strategy='stratified',random_state=None,constant=None,n_estimators=10,fail_fit=False,fail_predict=False,a=0)
sklearn.model_selection.tests.test_successive_halving.SometimesFailClassifier.fit(self,X,y)
sklearn.model_selection.tests.test_successive_halving.SometimesFailClassifier.predict(self,X)
sklearn.model_selection.tests.test_successive_halving.test_aggressive_elimination(Est,aggressive_elimination,max_resources,expected_n_iterations,expected_n_required_iterations,expected_n_possible_iterations,expected_n_remaining_candidates,expected_n_candidates,expected_n_resources)
sklearn.model_selection.tests.test_successive_halving.test_base_estimator_inputs(Est)
sklearn.model_selection.tests.test_successive_halving.test_cv_results(Est)
sklearn.model_selection.tests.test_successive_halving.test_groups_support(Est)
sklearn.model_selection.tests.test_successive_halving.test_halving_random_search_list_of_dicts()
sklearn.model_selection.tests.test_successive_halving.test_input_errors(Est,params,expected_error_message)
sklearn.model_selection.tests.test_successive_halving.test_input_errors_randomized(params,expected_error_message)
sklearn.model_selection.tests.test_successive_halving.test_min_max_resources(Est,min_resources,max_resources,expected_n_iterations,expected_n_possible_iterations,expected_n_resources)
sklearn.model_selection.tests.test_successive_halving.test_min_resources_null(SearchCV)
sklearn.model_selection.tests.test_successive_halving.test_n_iterations(Est,max_resources,n_iterations,n_possible_iterations)
sklearn.model_selection.tests.test_successive_halving.test_nan_handling(HalvingSearch,fail_at)
sklearn.model_selection.tests.test_successive_halving.test_random_search(max_resources,n_candidates,expected_n_candidates)
sklearn.model_selection.tests.test_successive_halving.test_random_search_discrete_distributions(param_distributions,expected_n_candidates)
sklearn.model_selection.tests.test_successive_halving.test_resource_parameter(Est)
sklearn.model_selection.tests.test_successive_halving.test_select_best_index(SearchCV)
sklearn.model_selection.tests.test_successive_halving.test_subsample_splitter_determinism(subsample_test)
sklearn.model_selection.tests.test_successive_halving.test_subsample_splitter_shapes(fraction,subsample_test,expected_train_size,expected_test_size)
sklearn.model_selection.tests.test_successive_halving.test_top_k(k,itr,expected)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/test_validation.py----------------------------------------
A:sklearn.model_selection.tests.test_validation.X->scale(X)
A:sklearn.model_selection.tests.test_validation.T->T.reshape(len(T), -1).reshape(len(T), -1)
A:sklearn.model_selection.tests.test_validation.y->numpy.ones(9)
A:sklearn.model_selection.tests.test_validation.y2->numpy.array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3])
A:sklearn.model_selection.tests.test_validation.P->numpy.eye(5)
A:sklearn.model_selection.tests.test_validation.clf->SVC(kernel='linear', random_state=0)
A:sklearn.model_selection.tests.test_validation.X_sparse->csr_container(X)
A:sklearn.model_selection.tests.test_validation.scores->cross_val_score(clf, X, y, cv=3, scoring=failing_scorer, error_score=error_score)
A:sklearn.model_selection.tests.test_validation.multioutput_y->numpy.column_stack([y, y[::-1]])
A:sklearn.model_selection.tests.test_validation.(X, y)->load_iris(return_X_y=True)
A:sklearn.model_selection.tests.test_validation.grid->GridSearchCV(clf, param_grid={'C': [1, 10]})
A:sklearn.model_selection.tests.test_validation.estimator->ConsumingClassifier(registry=estimator_registry).set_fit_request(sample_weight='fit_sample_weight', metadata='fit_metadata')
A:sklearn.model_selection.tests.test_validation.multiclass_scorer->make_scorer(precision_recall_fscore_support)
A:sklearn.model_selection.tests.test_validation.pipeline->Pipeline([('imputer', SimpleImputer()), ('classifier', MockClassifier())])
A:sklearn.model_selection.tests.test_validation.results->cross_validate(clf, X, y, cv=3, scoring=scoring, return_train_score=return_train_score, error_score=error_score)
A:sklearn.model_selection.tests.test_validation.cv->KFold(n_splits=3, shuffle=True, random_state=global_random_seed)
A:sklearn.model_selection.tests.test_validation.(X_reg, y_reg)->make_regression(n_samples=30, random_state=0)
A:sklearn.model_selection.tests.test_validation.reg->Ridge()
A:sklearn.model_selection.tests.test_validation.(X_clf, y_clf)->make_classification(n_samples=30, random_state=0)
A:sklearn.model_selection.tests.test_validation.X_reg->csr_container(X_reg)
A:sklearn.model_selection.tests.test_validation.X_clf->csr_container(X_clf)
A:sklearn.model_selection.tests.test_validation.mse_scorer->check_scoring(est, scoring='neg_mean_squared_error')
A:sklearn.model_selection.tests.test_validation.r2_scorer->check_scoring(est, scoring='r2')
A:sklearn.model_selection.tests.test_validation.est->LinearSVC(dual='auto', random_state=42)
A:sklearn.model_selection.tests.test_validation.train_mse_scores->numpy.array(train_mse_scores)
A:sklearn.model_selection.tests.test_validation.test_mse_scores->numpy.array(test_mse_scores)
A:sklearn.model_selection.tests.test_validation.train_r2_scores->numpy.array(train_r2_scores)
A:sklearn.model_selection.tests.test_validation.test_r2_scores->numpy.array(test_r2_scores)
A:sklearn.model_selection.tests.test_validation.fitted_estimators->numpy.array(fitted_estimators)
A:sklearn.model_selection.tests.test_validation.mse_scores_dict->cross_validate(clf, X, y, scoring='neg_mean_squared_error', return_estimator=True, cv=cv)
A:sklearn.model_selection.tests.test_validation.r2_scores_dict->cross_validate(clf, X, y, scoring=['r2'], return_train_score=False, cv=cv)
A:sklearn.model_selection.tests.test_validation.est_coef->est_coef.toarray().toarray()
A:sklearn.model_selection.tests.test_validation.fitted_est_coef->fitted_est_coef.toarray().toarray()
A:sklearn.model_selection.tests.test_validation.y_pred->SVC(kernel='linear', random_state=0).predict(X)
A:sklearn.model_selection.tests.test_validation.keys_with_train->keys_sans_train.union({'train_r2', 'train_neg_mean_squared_error'})
A:sklearn.model_selection.tests.test_validation.cv_results->cross_validate(estimator, X, y, cv=cv, n_jobs=2, return_indices=True)
A:sklearn.model_selection.tests.test_validation.svm->SVC(kernel='linear')
A:sklearn.model_selection.tests.test_validation.iris->load_iris()
A:sklearn.model_selection.tests.test_validation.kfold->KFold(5)
A:sklearn.model_selection.tests.test_validation.scores_indices->cross_val_score(svm, X, y, cv=kfold)
A:sklearn.model_selection.tests.test_validation.mask_train->numpy.zeros(len(y), dtype=bool)
A:sklearn.model_selection.tests.test_validation.mask_test->numpy.zeros(len(y), dtype=bool)
A:sklearn.model_selection.tests.test_validation.scores_masks->cross_val_score(svm, X, y, cv=cv_masks)
A:sklearn.model_selection.tests.test_validation.linear_kernel->numpy.dot(X, X.T)
A:sklearn.model_selection.tests.test_validation.score_precomputed->cross_val_score(svm, linear_kernel, y)
A:sklearn.model_selection.tests.test_validation.score_linear->cross_val_score(svm, X, y)
A:sklearn.model_selection.tests.test_validation.score_callable->cross_val_score(svm, X, y)
A:sklearn.model_selection.tests.test_validation.n_classes->len(np.unique(y))
A:sklearn.model_selection.tests.test_validation.W_sparse->coo_container((np.array([1]), (np.array([1]), np.array([0]))), shape=(10, 1))
A:sklearn.model_selection.tests.test_validation.P_sparse->coo_container(np.eye(5))
A:sklearn.model_selection.tests.test_validation.DUMMY_OBJ->object()
A:sklearn.model_selection.tests.test_validation.scoring->make_scorer(explained_variance_score)
A:sklearn.model_selection.tests.test_validation.score->numpy.memmap(tf.name, shape=(), mode='r', dtype=np.float64)
A:sklearn.model_selection.tests.test_validation.zo_scores->cross_val_score(clf, iris.data, iris.target, scoring='accuracy')
A:sklearn.model_selection.tests.test_validation.f1_scores->cross_val_score(clf, iris.data, iris.target, scoring='f1_weighted')
A:sklearn.model_selection.tests.test_validation.r2_scores->cross_val_score(reg, X, y, scoring='r2')
A:sklearn.model_selection.tests.test_validation.neg_mse_scores->cross_val_score(reg, X, y, scoring='neg_mean_squared_error')
A:sklearn.model_selection.tests.test_validation.expected_neg_mse->numpy.array([-763.07, -553.16, -274.38, -273.26, -1681.99])
A:sklearn.model_selection.tests.test_validation.ev_scores->cross_val_score(reg, X, y, scoring=scoring)
A:sklearn.model_selection.tests.test_validation.(score, scores, pvalue)->permutation_test_score(svm, X, y, n_permutations=30, cv=cv, scoring='accuracy')
A:sklearn.model_selection.tests.test_validation.(score_group, _, pvalue_group)->permutation_test_score(svm_sparse, X_sparse, y, n_permutations=30, cv=cv_sparse, scoring='accuracy', groups=np.ones(y.size), random_state=0)
A:sklearn.model_selection.tests.test_validation.svm_sparse->SVC(kernel='linear')
A:sklearn.model_selection.tests.test_validation.cv_sparse->StratifiedKFold(2)
A:sklearn.model_selection.tests.test_validation.scorer->ConsumingScorer(registry=scorer_registry).set_score_request(sample_weight='score_weights', metadata='score_metadata')
A:sklearn.model_selection.tests.test_validation.(score, _, pvalue)->permutation_test_score(svm, X, y, n_permutations=100, scoring=scorer, cv=cv, random_state=0)
A:sklearn.model_selection.tests.test_validation.p->numpy.arange(100)
A:sklearn.model_selection.tests.test_validation.scoring_micro->make_scorer(precision_score, average='micro')
A:sklearn.model_selection.tests.test_validation.scoring_macro->make_scorer(precision_score, average='macro')
A:sklearn.model_selection.tests.test_validation.scoring_samples->make_scorer(precision_score, average='samples')
A:sklearn.model_selection.tests.test_validation.score_micro->cross_val_score(clf, X, y, scoring=scoring_micro)
A:sklearn.model_selection.tests.test_validation.score_macro->cross_val_score(clf, X, y, scoring=scoring_macro)
A:sklearn.model_selection.tests.test_validation.score_samples->cross_val_score(clf, X, y, scoring=scoring_samples)
A:sklearn.model_selection.tests.test_validation.preds2->numpy.zeros_like(y)
A:sklearn.model_selection.tests.test_validation.preds2[test]->LinearSVC(dual='auto', random_state=42).predict(X[test])
A:sklearn.model_selection.tests.test_validation.preds->cross_val_predict(classif, X, y, cv=10)
A:sklearn.model_selection.tests.test_validation.Xsp->coo_container(Xsp)
A:sklearn.model_selection.tests.test_validation.ind->numpy.argsort(y)
A:sklearn.model_selection.tests.test_validation.predictions->cross_val_predict(est, X, y, method=method, cv=kfold3)
A:sklearn.model_selection.tests.test_validation.(train, test)->next(ShuffleSplit().split(X))
A:sklearn.model_selection.tests.test_validation.yhat_proba->cross_val_predict(clf, X, y, cv=cv, method='predict_proba')
A:sklearn.model_selection.tests.test_validation.mock_classifier->MockClassifier()
A:sklearn.model_selection.tests.test_validation.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_validation.y_hat->cross_val_predict(mock_classifier, X, y=None, cv=5, method='predict')
A:sklearn.model_selection.tests.test_validation.y_hat_proba->cross_val_predict(mock_classifier, X, y=None, cv=5, method='predict_proba')
A:sklearn.model_selection.tests.test_validation.a->cross_val_score(clf, X, y, params=fit_params, cv=3)
A:sklearn.model_selection.tests.test_validation.(train_sizes, train_scores, test_scores, fit_times, score_times)->learning_curve(estimator, X, y, cv=KFold(n_splits=n_splits), train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train, return_times=True)
A:sklearn.model_selection.tests.test_validation.(train_sizes2, train_scores2, test_scores2)->learning_curve(estimator, X, y, cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples), train_sizes=np.linspace(0.1, 1.0, 10), shuffle=shuffle_train)
A:sklearn.model_selection.tests.test_validation.(X, _)->make_classification(n_samples=30, n_features=1, n_informative=1, n_redundant=0, n_classes=2, n_clusters_per_class=1, random_state=0)
A:sklearn.model_selection.tests.test_validation.(train_sizes, train_scores, test_scores)->learning_curve(estimator, X, y, cv=cv, train_sizes=np.linspace(0.1, 1.0, 10))
A:sklearn.model_selection.tests.test_validation.sys.stdout->StringIO()
A:sklearn.model_selection.tests.test_validation.out->sys.stdout.getvalue()
A:sklearn.model_selection.tests.test_validation.train_sizes->numpy.linspace(0.2, 1.0, 5)
A:sklearn.model_selection.tests.test_validation.(train_sizes_inc, train_scores_inc, test_scores_inc)->learning_curve(estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3), groups=groups, shuffle=True, random_state=2, exploit_incremental_learning=True)
A:sklearn.model_selection.tests.test_validation.(train_sizes_batch, train_scores_batch, test_scores_batch)->learning_curve(estimator, X, y, cv=cv, n_jobs=1, train_sizes=np.linspace(0.3, 1.0, 3), groups=groups, shuffle=True, random_state=2)
A:sklearn.model_selection.tests.test_validation.(train_sizes, _, _)->learning_curve(estimator, X, y, cv=3, train_sizes=np.linspace(0.33, 1.0, 3))
A:sklearn.model_selection.tests.test_validation.groups->numpy.array([1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4])
A:sklearn.model_selection.tests.test_validation.param_range->numpy.linspace(1, 0, 10)
A:sklearn.model_selection.tests.test_validation.(train_scores, test_scores)->validation_curve(MockEstimatorWithParameter(), X, y, param_name='param', param_range=param_range, cv=2)
A:sklearn.model_selection.tests.test_validation.(_, _)->validation_curve(MockEstimatorWithSingleFitCallAllowed(), X, y, param_name='param', param_range=param_range, cv=2)
A:sklearn.model_selection.tests.test_validation.scores1->validation_curve(SVC(kernel='linear', random_state=0), X, y, param_name='C', param_range=[0.1, 0.1, 0.2, 0.2], cv=OneTimeSplitter(n_splits=n_splits, n_samples=n_samples))
A:sklearn.model_selection.tests.test_validation.scores2->validation_curve(SVC(kernel='linear', random_state=0), X, y, param_name='C', param_range=[0.1, 0.1, 0.2, 0.2], cv=KFold(n_splits=n_splits, shuffle=True))
A:sklearn.model_selection.tests.test_validation.scores3->validation_curve(SVC(kernel='linear', random_state=0), X, y, param_name='C', param_range=[0.1, 0.1, 0.2, 0.2], cv=KFold(n_splits=n_splits))
A:sklearn.model_selection.tests.test_validation.y_sparse->csr_container(y)
A:sklearn.model_selection.tests.test_validation.classif->OneVsRestClassifier(SVC(kernel='linear'))
A:sklearn.model_selection.tests.test_validation.preds_sparse->preds_sparse.toarray().toarray()
A:sklearn.model_selection.tests.test_validation.expected_predictions->get_expected_predictions(X, y, kfold3, classes, est, method)
A:sklearn.model_selection.tests.test_validation.expected_predictions[test]->getattr(est, method)(X[test])
A:sklearn.model_selection.tests.test_validation.(_, y_enc)->numpy.unique(y, return_inverse=True)
A:sklearn.model_selection.tests.test_validation.fold_preds->getattr(est, method)(X[test])
A:sklearn.model_selection.tests.test_validation.i_cols_fit->numpy.unique(y_enc[train])
A:sklearn.model_selection.tests.test_validation.n_classes_in_label->len(set(y[:, i_col]))
A:sklearn.model_selection.tests.test_validation.y_enc->numpy.concatenate(y_enc_cols, axis=1)
A:sklearn.model_selection.tests.test_validation.fold_cols->numpy.unique(y_enc[train][:, i_col])
A:sklearn.model_selection.tests.test_validation.idx->numpy.ix_(test, fold_cols)
A:sklearn.model_selection.tests.test_validation.cv_predict_output->cross_val_predict(est, X, tg, method=method, cv=cv)
A:sklearn.model_selection.tests.test_validation.probs->self.predict_proba(X)
A:sklearn.model_selection.tests.test_validation.func->getattr(est, method)
A:sklearn.model_selection.tests.test_validation.expected_predictions_->func(X[test])
A:sklearn.model_selection.tests.test_validation.exp_pred_test->numpy.full((len(test), classes), np.finfo(expected_predictions.dtype).min)
A:sklearn.model_selection.tests.test_validation.kfold3->KFold(n_splits=3)
A:sklearn.model_selection.tests.test_validation.kfold4->KFold(n_splits=4)
A:sklearn.model_selection.tests.test_validation.le->LabelEncoder()
A:sklearn.model_selection.tests.test_validation.tf->tempfile.NamedTemporaryFile(mode='wb', delete=False)
A:sklearn.model_selection.tests.test_validation.failing_clf->FailingClassifier(FailingClassifier.FAILING_PARAMETER)
A:sklearn.model_selection.tests.test_validation.fit_and_score_args->dict(estimator=clf, X=X, y=y, scorer=scorer, train=train, test=test, verbose=verbose, parameters=None, fit_params=None, score_params=None, return_train_score=train_score, split_progress=split_prg, candidate_progress=cdt_prg)
A:sklearn.model_selection.tests.test_validation.result->_fit_and_score(**fit_and_score_args)
A:sklearn.model_selection.tests.test_validation.num_values_too_high->(X > self.max_x_value).sum()
A:sklearn.model_selection.tests.test_validation.warning_message->re.compile(f"2 fits failed.+total of 3.+The score on these train-test partitions for these parameters will be set to {cross_validate_kwargs['error_score']}.+{individual_fit_error_message}", flags=re.DOTALL)
A:sklearn.model_selection.tests.test_validation.error_message->re.compile(f'All the 7 fits failed.+your model is misconfigured.+{individual_fit_error_message}', flags=re.DOTALL)
A:sklearn.model_selection.tests.test_validation.failing_scorer->partial(_failing_scorer, error_msg=error_msg)
A:sklearn.model_selection.tests.test_validation.non_failing_scorer->make_scorer(mean_squared_error)
A:sklearn.model_selection.tests.test_validation.(out, _)->capsys.readouterr()
A:sklearn.model_selection.tests.test_validation.outlines->sys.stdout.getvalue().split('\n')
A:sklearn.model_selection.tests.test_validation.cm->confusion_matrix(y, y_pred)
A:sklearn.model_selection.tests.test_validation.sorted_idx->numpy.argsort(y)
A:sklearn.model_selection.tests.test_validation.svc->SVC()
A:sklearn.model_selection.tests.test_validation.(_, train_score, test_score, *_)->learning_curve(svc, X, y, cv=5, error_score=np.nan)
A:sklearn.model_selection.tests.test_validation.err_msg->re.escape("['metadata'] are passed to cross validation")
A:sklearn.model_selection.tests.test_validation.scorer_registry->_Registry()
A:sklearn.model_selection.tests.test_validation.splitter_registry->_Registry()
A:sklearn.model_selection.tests.test_validation.splitter->ConsumingSplitter(registry=splitter_registry).set_split_request(groups='split_groups', metadata='split_metadata')
A:sklearn.model_selection.tests.test_validation.estimator_registry->_Registry()
A:sklearn.model_selection.tests.test_validation.n_samples->_num_samples(X)
A:sklearn.model_selection.tests.test_validation.score_weights->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_validation.score_metadata->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_validation.split_groups->numpy.random.RandomState(0).randint(0, 3, n_samples)
A:sklearn.model_selection.tests.test_validation.split_metadata->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_validation.fit_sample_weight->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_validation.fit_metadata->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_validation.params->dict(split_groups=split_groups, split_metadata=split_metadata, fit_sample_weight=fit_sample_weight, fit_metadata=fit_metadata)
sklearn.model_selection.tests.test_validation.DataDependentFailingClassifier(self,max_x_value=None)
sklearn.model_selection.tests.test_validation.DataDependentFailingClassifier.__init__(self,max_x_value=None)
sklearn.model_selection.tests.test_validation.DataDependentFailingClassifier.fit(self,X,y=None)
sklearn.model_selection.tests.test_validation.DataDependentFailingClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_validation.MockClassifier(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_validation.MockClassifier.__init__(self,a=0,allow_nd=False)
sklearn.model_selection.tests.test_validation.MockClassifier.fit(self,X,Y=None,sample_weight=None,class_prior=None,sparse_sample_weight=None,sparse_param=None,dummy_int=None,dummy_str=None,dummy_obj=None,callback=None)
sklearn.model_selection.tests.test_validation.MockClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_validation.MockClassifier.predict(self,T)
sklearn.model_selection.tests.test_validation.MockClassifier.predict_proba(self,T)
sklearn.model_selection.tests.test_validation.MockClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter(self,param=0.5)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.__init__(self,param=0.5)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.fit(self,X_subset,y_subset)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.predict(self,X)
sklearn.model_selection.tests.test_validation.MockEstimatorWithParameter.score(self,X=None,y=None)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed(MockEstimatorWithParameter)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed.fit(self,X_subset,y_subset)
sklearn.model_selection.tests.test_validation.MockEstimatorWithSingleFitCallAllowed.predict(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.__init__(self,n_max_train_sizes)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.fit(self,X_subset,y_subset=None)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.predict(self,X)
sklearn.model_selection.tests.test_validation.MockImprovingEstimator.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator(self,n_max_train_sizes,expected_fit_params=None)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator.__init__(self,n_max_train_sizes,expected_fit_params=None)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator._is_training_data(self,X)
sklearn.model_selection.tests.test_validation.MockIncrementalImprovingEstimator.partial_fit(self,X,y=None,**params)
sklearn.model_selection.tests.test_validation.RFWithDecisionFunction(RandomForestClassifier)
sklearn.model_selection.tests.test_validation.RFWithDecisionFunction.decision_function(self,X)
sklearn.model_selection.tests.test_validation._failing_scorer(estimator,X,y,error_msg)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_binary(est,X,y,method)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_multiclass(est,X,y,method)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_multilabel(est,X,y,method)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_with_method_binary(est)
sklearn.model_selection.tests.test_validation.check_cross_val_predict_with_method_multiclass(est)
sklearn.model_selection.tests.test_validation.check_cross_validate_multi_metric(clf,X,y,scores,cv)
sklearn.model_selection.tests.test_validation.check_cross_validate_single_metric(clf,X,y,scores,cv)
sklearn.model_selection.tests.test_validation.get_expected_predictions(X,y,cv,classes,est,method)
sklearn.model_selection.tests.test_validation.test_callable_multimetric_confusion_matrix_cross_validate()
sklearn.model_selection.tests.test_validation.test_check_is_permutation()
sklearn.model_selection.tests.test_validation.test_cross_val_predict(coo_container)
sklearn.model_selection.tests.test_validation.test_cross_val_predict_class_subset()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_decision_function_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_input_types(coo_container)
sklearn.model_selection.tests.test_validation.test_cross_val_predict_method_checking()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_pandas()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_predict_log_proba_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_predict_proba_shape()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_sparse_prediction(csr_container)
sklearn.model_selection.tests.test_validation.test_cross_val_predict_unbalanced()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method_multilabel_ovr()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method_multilabel_rf()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method_multilabel_rf_rare_class()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_with_method_rare_class()
sklearn.model_selection.tests.test_validation.test_cross_val_predict_y_none()
sklearn.model_selection.tests.test_validation.test_cross_val_score(coo_container)
sklearn.model_selection.tests.test_validation.test_cross_val_score_allow_nans()
sklearn.model_selection.tests.test_validation.test_cross_val_score_failing_scorer(error_score)
sklearn.model_selection.tests.test_validation.test_cross_val_score_fit_params(coo_container)
sklearn.model_selection.tests.test_validation.test_cross_val_score_mask()
sklearn.model_selection.tests.test_validation.test_cross_val_score_multilabel()
sklearn.model_selection.tests.test_validation.test_cross_val_score_pandas()
sklearn.model_selection.tests.test_validation.test_cross_val_score_precomputed()
sklearn.model_selection.tests.test_validation.test_cross_val_score_predict_groups()
sklearn.model_selection.tests.test_validation.test_cross_val_score_score_func()
sklearn.model_selection.tests.test_validation.test_cross_val_score_sparse_fit_params(coo_container)
sklearn.model_selection.tests.test_validation.test_cross_val_score_with_score_func_classification()
sklearn.model_selection.tests.test_validation.test_cross_val_score_with_score_func_regression()
sklearn.model_selection.tests.test_validation.test_cross_validate(use_sparse:bool,csr_container)
sklearn.model_selection.tests.test_validation.test_cross_validate_all_failing_fits_error(error_score)
sklearn.model_selection.tests.test_validation.test_cross_validate_failing_scorer(error_score,return_train_score,with_multimetric)
sklearn.model_selection.tests.test_validation.test_cross_validate_fit_param_deprecation()
sklearn.model_selection.tests.test_validation.test_cross_validate_invalid_scoring_param()
sklearn.model_selection.tests.test_validation.test_cross_validate_many_jobs()
sklearn.model_selection.tests.test_validation.test_cross_validate_nested_estimator()
sklearn.model_selection.tests.test_validation.test_cross_validate_return_indices(global_random_seed)
sklearn.model_selection.tests.test_validation.test_cross_validate_routing(cv_method)
sklearn.model_selection.tests.test_validation.test_cross_validate_some_failing_fits_warning(error_score)
sklearn.model_selection.tests.test_validation.test_fit_and_score_failing()
sklearn.model_selection.tests.test_validation.test_fit_and_score_verbosity(capsys,train_score,scorer,verbose,split_prg,cdt_prg,expected)
sklearn.model_selection.tests.test_validation.test_fit_and_score_working()
sklearn.model_selection.tests.test_validation.test_gridsearchcv_cross_val_predict_with_method()
sklearn.model_selection.tests.test_validation.test_groups_with_routing_validation(cv_method)
sklearn.model_selection.tests.test_validation.test_learning_curve()
sklearn.model_selection.tests.test_validation.test_learning_curve_batch_and_incremental_learning_are_equal()
sklearn.model_selection.tests.test_validation.test_learning_curve_fit_params()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning_fit_params()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning_not_possible()
sklearn.model_selection.tests.test_validation.test_learning_curve_incremental_learning_unsupervised()
sklearn.model_selection.tests.test_validation.test_learning_curve_n_sample_range_out_of_bounds()
sklearn.model_selection.tests.test_validation.test_learning_curve_partial_fit_regressors()
sklearn.model_selection.tests.test_validation.test_learning_curve_remove_duplicate_sample_sizes()
sklearn.model_selection.tests.test_validation.test_learning_curve_some_failing_fits_warning(global_random_seed)
sklearn.model_selection.tests.test_validation.test_learning_curve_unsupervised()
sklearn.model_selection.tests.test_validation.test_learning_curve_verbose()
sklearn.model_selection.tests.test_validation.test_learning_curve_with_boolean_indices()
sklearn.model_selection.tests.test_validation.test_learning_curve_with_shuffle()
sklearn.model_selection.tests.test_validation.test_passed_unrequested_metadata(cv_method)
sklearn.model_selection.tests.test_validation.test_permutation_score(coo_container)
sklearn.model_selection.tests.test_validation.test_permutation_test_score_allow_nans()
sklearn.model_selection.tests.test_validation.test_permutation_test_score_fit_params()
sklearn.model_selection.tests.test_validation.test_permutation_test_score_pandas()
sklearn.model_selection.tests.test_validation.test_score()
sklearn.model_selection.tests.test_validation.test_score_memmap()
sklearn.model_selection.tests.test_validation.test_validation_curve()
sklearn.model_selection.tests.test_validation.test_validation_curve_clone_estimator()
sklearn.model_selection.tests.test_validation.test_validation_curve_cv_splits_consistency()
sklearn.model_selection.tests.test_validation.test_validation_curve_fit_params()
sklearn.model_selection.tests.test_validation.three_params_scorer(i,j,k)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/test_split.py----------------------------------------
A:sklearn.model_selection.tests.test_split.X->numpy.zeros((10, 1))
A:sklearn.model_selection.tests.test_split.digits->load_digits()
A:sklearn.model_selection.tests.test_split.X_1d->numpy.array([1, 2, 3, 4])
A:sklearn.model_selection.tests.test_split.y->numpy.repeat([0, 1], X.shape[0] / 2)
A:sklearn.model_selection.tests.test_split.groups->numpy.random.RandomState(0).randint(0, 5, 15)
A:sklearn.model_selection.tests.test_split.loo->LeaveOneOut()
A:sklearn.model_selection.tests.test_split.lpo->LeavePOut(p)
A:sklearn.model_selection.tests.test_split.kf->kfold(3, shuffle=True, random_state=np.random.RandomState(0))
A:sklearn.model_selection.tests.test_split.skf->LeavePOut(p=2).split(X[:i], y[:i], groups[:i])
A:sklearn.model_selection.tests.test_split.lolo->LeaveOneGroupOut().split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lopo->LeavePGroupsOut(p)
A:sklearn.model_selection.tests.test_split.ss->ShuffleSplit(random_state=21)
A:sklearn.model_selection.tests.test_split.ps->PredefinedSplit(folds)
A:sklearn.model_selection.tests.test_split.sgkf->StratifiedGroupKFold(n_splits=n_splits)
A:sklearn.model_selection.tests.test_split.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_split.y_2d->numpy.repeat([0, 1], X.shape[0] / 2).reshape(-1, 1)
A:sklearn.model_selection.tests.test_split.y_multilabel->numpy.array([[0, 0, 0, 0], [0, 1, 1, 0], [0, 0, 0, 1], [1, 1, 0, 1], [0, 0, 1, 0]])
A:sklearn.model_selection.tests.test_split.msg->re.escape(f'The groups parameter contains fewer than (or equal to) n_groups (3) numbers of unique groups ({groups}). LeavePGroupsOut expects that at least n_groups + 1 (4) unique groups be present')
A:sklearn.model_selection.tests.test_split.n_samples->len(groups)
A:sklearn.model_selection.tests.test_split.collected_test_samples->set()
A:sklearn.model_selection.tests.test_split.X1->numpy.ones(18)
A:sklearn.model_selection.tests.test_split.X2->numpy.ones(16)
A:sklearn.model_selection.tests.test_split.skf_3->StratifiedKFold(3)
A:sklearn.model_selection.tests.test_split.sgkf_3->StratifiedGroupKFold(3)
A:sklearn.model_selection.tests.test_split.naive_groups->numpy.arange(len(y))
A:sklearn.model_selection.tests.test_split.splits->TimeSeriesSplit(n_splits=4, gap=2).split(X)
A:sklearn.model_selection.tests.test_split.(train, test)->next(splits)
A:sklearn.model_selection.tests.test_split.splits_base->get_splits(y)
A:sklearn.model_selection.tests.test_split.y_perm->numpy.take(perm, y)
A:sklearn.model_selection.tests.test_split.splits_perm->get_splits(y_perm)
A:sklearn.model_selection.tests.test_split.cv->LeavePOut(p=2)
A:sklearn.model_selection.tests.test_split.kf2->StratifiedKFold(5, shuffle=True, random_state=1)
A:sklearn.model_selection.tests.test_split.kf3->KFold(3, shuffle=True, random_state=1)
A:sklearn.model_selection.tests.test_split.all_folds->numpy.zeros(300)
A:sklearn.model_selection.tests.test_split.groups_1->numpy.arange(len(y))
A:sklearn.model_selection.tests.test_split.groups_2->numpy.arange(len(y2))
A:sklearn.model_selection.tests.test_split.X_40->numpy.ones(40)
A:sklearn.model_selection.tests.test_split.kf0->StratifiedKFold(5, shuffle=True, random_state=0)
A:sklearn.model_selection.tests.test_split.kf1->StratifiedKFold(5, shuffle=True, random_state=0)
A:sklearn.model_selection.tests.test_split.test_set1->sorted([tuple(s[1]) for s in kf1.split(X, y)])
A:sklearn.model_selection.tests.test_split.test_set2->sorted([tuple(s[1]) for s in kf2.split(X, y)])
A:sklearn.model_selection.tests.test_split.model->SVC(C=10, gamma=0.005)
A:sklearn.model_selection.tests.test_split.mean_score->cross_val_score(model, X, y, cv=cv).mean()
A:sklearn.model_selection.tests.test_split.expected->numpy.asarray([[0.833, 0.166], [0.666, 0.333], [0.5, 0.5]])
A:sklearn.model_selection.tests.test_split.gkf->GroupKFold(n_splits=n_splits)
A:sklearn.model_selection.tests.test_split.g->numpy.random.RandomState(0).choice(n_groups, n_points)
A:sklearn.model_selection.tests.test_split.sgkf_folds->StratifiedGroupKFold(n_splits=n_splits).split(X, y, groups=g)
A:sklearn.model_selection.tests.test_split.gkf_folds->GroupKFold(n_splits=n_splits).split(X, y, groups=g)
A:sklearn.model_selection.tests.test_split.ss1->ShuffleSplit(test_size=0.2, random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss2->ShuffleSplit(test_size=2, random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss3->ShuffleSplit(test_size=np.int32(2), random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.ss4->ShuffleSplit(test_size=int(2), random_state=0).split(X)
A:sklearn.model_selection.tests.test_split.(X_train, X_test)->train_test_split(X_df)
A:sklearn.model_selection.tests.test_split.sss->StratifiedShuffleSplit(test_size=2, random_state=42)
A:sklearn.model_selection.tests.test_split.test_size->numpy.ceil(0.33 * len(y))
A:sklearn.model_selection.tests.test_split.bf->scipy.stats.binom(n_splits, p)
A:sklearn.model_selection.tests.test_split.prob->scipy.stats.binom(n_splits, p).pmf(count)
A:sklearn.model_selection.tests.test_split.(n_train, n_test)->_validate_shuffle_split(n_samples, test_size=1.0 / n_folds, train_size=1.0 - 1.0 / n_folds)
A:sklearn.model_selection.tests.test_split.group_counts->numpy.unique(groups)
A:sklearn.model_selection.tests.test_split.expected_ratio->numpy.mean(y[:, 4])
A:sklearn.model_selection.tests.test_split.folds->numpy.zeros(n_samples)
A:sklearn.model_selection.tests.test_split.(ps_train, ps_test)->zip(*ps.split())
A:sklearn.model_selection.tests.test_split.Xy->numpy.ones(len(groups))
A:sklearn.model_selection.tests.test_split.slo->GroupShuffleSplit(n_splits, test_size=test_size, random_state=0)
A:sklearn.model_selection.tests.test_split.l_unique->numpy.unique(groups_i)
A:sklearn.model_selection.tests.test_split.l->numpy.asarray(groups_i)
A:sklearn.model_selection.tests.test_split.l_train_unique->numpy.unique(l[train])
A:sklearn.model_selection.tests.test_split.l_test_unique->numpy.unique(l[test])
A:sklearn.model_selection.tests.test_split.logo->LeaveOneGroupOut()
A:sklearn.model_selection.tests.test_split.lpgo_1->LeavePGroupsOut(n_groups=1)
A:sklearn.model_selection.tests.test_split.lpgo_2->LeavePGroupsOut(n_groups=2)
A:sklearn.model_selection.tests.test_split.n_groups->len(np.unique(groups))
A:sklearn.model_selection.tests.test_split.groups_arr->numpy.asarray(groups_i)
A:sklearn.model_selection.tests.test_split.groups_changing->numpy.array(groups, copy=True)
A:sklearn.model_selection.tests.test_split.lolo_changing->LeaveOneGroupOut().split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lplo->LeavePGroupsOut(n_groups=2).split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.lplo_changing->LeavePGroupsOut(n_groups=2).split(X, groups=groups)
A:sklearn.model_selection.tests.test_split.Xygroups->numpy.arange(3)
A:sklearn.model_selection.tests.test_split.repeated_cv->RepeatedCV(n_splits=n_splits, n_repeats=n_repeats)
A:sklearn.model_selection.tests.test_split.repeated_cv_repr->'{}(n_repeats=6, n_splits=2, random_state=None)'.format(repeated_cv.__class__.__name__)
A:sklearn.model_selection.tests.test_split.rkf->RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats)
A:sklearn.model_selection.tests.test_split.rskf->RepeatedStratifiedKFold(n_splits=2, n_repeats=2, random_state=random_state)
A:sklearn.model_selection.tests.test_split.xp->_array_api_for_tests(array_namespace, device)
A:sklearn.model_selection.tests.test_split.X_np->numpy.zeros((10, 1)).astype(dtype_name)
A:sklearn.model_selection.tests.test_split.X_xp->_array_api_for_tests(array_namespace, device).asarray(X_np, device=device)
A:sklearn.model_selection.tests.test_split.y_np->numpy.repeat([0, 1], X.shape[0] / 2).astype(dtype_name)
A:sklearn.model_selection.tests.test_split.y_xp->_array_api_for_tests(array_namespace, device).asarray(y_np, device=device)
A:sklearn.model_selection.tests.test_split.(X_train_np, X_test_np, y_train_np, y_test_np)->train_test_split(X_np, y, random_state=0, shuffle=shuffle, stratify=stratify)
A:sklearn.model_selection.tests.test_split.stratify_xp->_array_api_for_tests(array_namespace, device).asarray(stratify)
A:sklearn.model_selection.tests.test_split.(X_train_xp, X_test_xp, y_train_xp, y_test_xp)->train_test_split(X_xp, y_xp, shuffle=shuffle, stratify=stratify_xp, random_state=0)
A:sklearn.model_selection.tests.test_split.X_s->sparse_container(X)
A:sklearn.model_selection.tests.test_split.split->train_test_split(X, y, stratify=y, train_size=0.25)
A:sklearn.model_selection.tests.test_split.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.model_selection.tests.test_split.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.model_selection.tests.test_split.X_df->MockDataFrame(X)
A:sklearn.model_selection.tests.test_split.(X_train_arr, X_test_arr)->train_test_split(X_df)
A:sklearn.model_selection.tests.test_split.y2->numpy.hstack((np.ones(4), np.zeros(3)))
A:sklearn.model_selection.tests.test_split.y3->numpy.hstack((np.ones(4), np.zeros(3))).tolist()
A:sklearn.model_selection.tests.test_split.(X_train1, X_test1, y_train1, y_test1)->train_test_split(X, y1, stratify=y1 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.(X_train2, X_test2, y_train2, y_test2)->train_test_split(X, y2, stratify=y2 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.(X_train3, X_test3, y_train3, y_test3)->train_test_split(X, y3, stratify=y3 if stratify else None, random_state=0)
A:sklearn.model_selection.tests.test_split.y_binary->numpy.array([0, 1, 0, 1, 0, 0, 1, 1, 1])
A:sklearn.model_selection.tests.test_split.y_multiclass->numpy.array([0, 1, 0, 1, 2, 1, 2, 0, 2])
A:sklearn.model_selection.tests.test_split.y_multiclass_2d->numpy.array([0, 1, 0, 1, 2, 1, 2, 0, 2]).reshape(-1, 1)
A:sklearn.model_selection.tests.test_split.y_multioutput->numpy.array([[1, 2], [0, 3], [0, 0], [3, 1], [2, 0]])
A:sklearn.model_selection.tests.test_split.kf_iter->KFold().split(X, y)
A:sklearn.model_selection.tests.test_split.kf_iter_wrapped->check_cv(kf_iter)
A:sklearn.model_selection.tests.test_split.kf_randomized_iter->KFold(shuffle=True, random_state=0).split(X, y)
A:sklearn.model_selection.tests.test_split.kf_randomized_iter_wrapped->check_cv(kf_randomized_iter)
A:sklearn.model_selection.tests.test_split.lkf->kfold(n_splits=n_splits)
A:sklearn.model_selection.tests.test_split.cv_iter->list(lkf.split(X, y, groups.tolist()))
A:sklearn.model_selection.tests.test_split.tscv->TimeSeriesSplit(2)
A:sklearn.model_selection.tests.test_split.n_splits_actual->len(list(splits))
A:sklearn.model_selection.tests.test_split.suffix_start->max(len(train) - max_train_size, 0)
A:sklearn.model_selection.tests.test_split.check_splits->TimeSeriesSplit(n_splits=3, max_train_size=5).split(X)
A:sklearn.model_selection.tests.test_split.(X, y)->make_classification(n_samples=15, n_classes=2, random_state=0)
A:sklearn.model_selection.tests.test_split.gs->GridSearchCV(DummyClassifier(), param_grid={'strategy': ['stratified', 'most_frequent']}, cv=inner_cv, error_score='raise')
A:sklearn.model_selection.tests.test_split.metadata->LeavePOut(p=2).get_metadata_routing()
sklearn.model_selection.tests.test_split._check_time_series_max_train_size(splits,check_splits,max_train_size)
sklearn.model_selection.tests.test_split.check_cv_coverage(cv,X,y,groups,expected_n_splits)
sklearn.model_selection.tests.test_split.check_valid_split(train,test,n_samples=None)
sklearn.model_selection.tests.test_split.test_2d_y()
sklearn.model_selection.tests.test_split.test_array_api_train_test_split(shuffle,stratify,array_namespace,device,dtype_name)
sklearn.model_selection.tests.test_split.test_build_repr()
sklearn.model_selection.tests.test_split.test_check_cv()
sklearn.model_selection.tests.test_split.test_cross_validator_with_default_params()
sklearn.model_selection.tests.test_split.test_cv_iterable_wrapper()
sklearn.model_selection.tests.test_split.test_get_n_splits_for_repeated_kfold()
sklearn.model_selection.tests.test_split.test_get_n_splits_for_repeated_stratified_kfold()
sklearn.model_selection.tests.test_split.test_group_kfold(kfold)
sklearn.model_selection.tests.test_split.test_group_shuffle_split()
sklearn.model_selection.tests.test_split.test_group_shuffle_split_default_test_size(train_size,exp_train,exp_test)
sklearn.model_selection.tests.test_split.test_kfold_balance()
sklearn.model_selection.tests.test_split.test_kfold_can_detect_dependent_samples_on_digits()
sklearn.model_selection.tests.test_split.test_kfold_indices()
sklearn.model_selection.tests.test_split.test_kfold_no_shuffle()
sklearn.model_selection.tests.test_split.test_kfold_valueerrors()
sklearn.model_selection.tests.test_split.test_leave_group_out_changing_groups()
sklearn.model_selection.tests.test_split.test_leave_group_out_order_dependence()
sklearn.model_selection.tests.test_split.test_leave_one_out_empty_trainset()
sklearn.model_selection.tests.test_split.test_leave_one_p_group_out()
sklearn.model_selection.tests.test_split.test_leave_one_p_group_out_error_on_fewer_number_of_groups()
sklearn.model_selection.tests.test_split.test_leave_p_out_empty_trainset()
sklearn.model_selection.tests.test_split.test_nested_cv()
sklearn.model_selection.tests.test_split.test_predefinedsplit_with_kfold_split()
sklearn.model_selection.tests.test_split.test_random_state_shuffle_false(Klass)
sklearn.model_selection.tests.test_split.test_repeated_cv_repr(RepeatedCV)
sklearn.model_selection.tests.test_split.test_repeated_cv_value_errors()
sklearn.model_selection.tests.test_split.test_repeated_kfold_determinstic_split()
sklearn.model_selection.tests.test_split.test_repeated_stratified_kfold_determinstic_split()
sklearn.model_selection.tests.test_split.test_shuffle_kfold()
sklearn.model_selection.tests.test_split.test_shuffle_kfold_stratifiedkfold_reproducibility(kfold)
sklearn.model_selection.tests.test_split.test_shuffle_split()
sklearn.model_selection.tests.test_split.test_shuffle_split_default_test_size(split_class,train_size,exp_train,exp_test)
sklearn.model_selection.tests.test_split.test_shuffle_split_empty_trainset(CVSplitter)
sklearn.model_selection.tests.test_split.test_shuffle_stratifiedkfold()
sklearn.model_selection.tests.test_split.test_shufflesplit_errors(test_size,train_size)
sklearn.model_selection.tests.test_split.test_shufflesplit_reproducible()
sklearn.model_selection.tests.test_split.test_splitter_get_metadata_routing(cv)
sklearn.model_selection.tests.test_split.test_splitter_set_split_request(cv)
sklearn.model_selection.tests.test_split.test_stratified_group_kfold_against_group_kfold(cls_distr,n_groups)
sklearn.model_selection.tests.test_split.test_stratified_group_kfold_approximate()
sklearn.model_selection.tests.test_split.test_stratified_group_kfold_homogeneous_groups(y,groups,expected)
sklearn.model_selection.tests.test_split.test_stratified_group_kfold_trivial()
sklearn.model_selection.tests.test_split.test_stratified_kfold_label_invariance(k,shuffle,kfold)
sklearn.model_selection.tests.test_split.test_stratified_kfold_no_shuffle()
sklearn.model_selection.tests.test_split.test_stratified_kfold_ratios(k,shuffle,kfold)
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_even()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_init()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_iter()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_multilabel()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_multilabel_many_labels()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_overlap_train_test_bug()
sklearn.model_selection.tests.test_split.test_stratified_shuffle_split_respects_test_size()
sklearn.model_selection.tests.test_split.test_stratifiedkfold_balance(kfold)
sklearn.model_selection.tests.test_split.test_stratifiedshufflesplit_list_input()
sklearn.model_selection.tests.test_split.test_time_series_cv()
sklearn.model_selection.tests.test_split.test_time_series_gap()
sklearn.model_selection.tests.test_split.test_time_series_max_train_size()
sklearn.model_selection.tests.test_split.test_time_series_test_size()
sklearn.model_selection.tests.test_split.test_train_test_split(coo_container)
sklearn.model_selection.tests.test_split.test_train_test_split_32bit_overflow()
sklearn.model_selection.tests.test_split.test_train_test_split_allow_nans()
sklearn.model_selection.tests.test_split.test_train_test_split_default_test_size(train_size,exp_train,exp_test)
sklearn.model_selection.tests.test_split.test_train_test_split_empty_trainset()
sklearn.model_selection.tests.test_split.test_train_test_split_errors()
sklearn.model_selection.tests.test_split.test_train_test_split_list_input()
sklearn.model_selection.tests.test_split.test_train_test_split_mock_pandas()
sklearn.model_selection.tests.test_split.test_train_test_split_pandas()
sklearn.model_selection.tests.test_split.test_train_test_split_sparse(sparse_container)
sklearn.model_selection.tests.test_split.test_yields_constant_splits(cv,expected)
sklearn.model_selection.tests.testcheck_cv_coverage(cv,X,y,groups,expected_n_splits)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/test_search.py----------------------------------------
A:sklearn.model_selection.tests.test_search.self.classes_->numpy.unique(Y)
A:sklearn.model_selection.tests.test_search.X->numpy.arange(6).reshape(6, -1)
A:sklearn.model_selection.tests.test_search.y->numpy.array([0] * 5 + [1] * 5)
A:sklearn.model_selection.tests.test_search.grid1->ParameterGrid(params1)
A:sklearn.model_selection.tests.test_search.grid2->ParameterGrid(params2)
A:sklearn.model_selection.tests.test_search.points->set((tuple(chain(*sorted(p.items()))) for p in grid2))
A:sklearn.model_selection.tests.test_search.empty->ParameterGrid({})
A:sklearn.model_selection.tests.test_search.has_empty->ParameterGrid([{'C': [1, 10]}, {}, {'C': [0.5]}])
A:sklearn.model_selection.tests.test_search.clf->LinearSVC(dual='auto', random_state=0)
A:sklearn.model_selection.tests.test_search.grid_search->GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=3, verbose=3)
A:sklearn.model_selection.tests.test_search.sys.stdout->StringIO()
A:sklearn.model_selection.tests.test_search.pipe->Pipeline([('trs', MinimalTransformer()), ('clf', None)])
A:sklearn.model_selection.tests.test_search.searcher->SearchCV(clf, {'foo_param': [1, 2, 3]}, cv=2, error_score='raise')
A:sklearn.model_selection.tests.test_search.(X, y)->make_classification(random_state=42)
A:sklearn.model_selection.tests.test_search.clf_no_score->LinearSVCNoScore(dual='auto', random_state=0)
A:sklearn.model_selection.tests.test_search.grid_search_no_score->GridSearchCV(clf_no_score, {'C': Cs})
A:sklearn.model_selection.tests.test_search.search_no_scoring->GridSearchCV(clf, grid, scoring=None).fit(X, y)
A:sklearn.model_selection.tests.test_search.search_accuracy->GridSearchCV(clf, grid, scoring='accuracy').fit(X, y)
A:sklearn.model_selection.tests.test_search.search_no_score_method_auc->GridSearchCV(LinearSVCNoScore(dual='auto'), grid, scoring='roc_auc').fit(X, y)
A:sklearn.model_selection.tests.test_search.search_auc->GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y)
A:sklearn.model_selection.tests.test_search.score_no_scoring->GridSearchCV(clf, grid, scoring=None).fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_accuracy->GridSearchCV(clf, grid, scoring='accuracy').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_no_score_auc->GridSearchCV(LinearSVCNoScore(dual='auto'), grid, scoring='roc_auc').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.score_auc->GridSearchCV(clf, grid, scoring='roc_auc').fit(X, y).score(X, y)
A:sklearn.model_selection.tests.test_search.rng->numpy.random.RandomState(0)
A:sklearn.model_selection.tests.test_search.groups->numpy.random.RandomState(0).randint(0, 3, 15)
A:sklearn.model_selection.tests.test_search.gs->SearchCV(est, cv=2, **param_grid_search).fit(X, y)
A:sklearn.model_selection.tests.test_search.random_search->RandomizedSearchCV(est, est_parameters, cv=cv, n_iter=3)
A:sklearn.model_selection.tests.test_search.(X_, y_)->make_classification(n_samples=200, n_features=100, random_state=0)
A:sklearn.model_selection.tests.test_search.cv->GridSearchCV(clf, grid_params, cv=n_splits)
A:sklearn.model_selection.tests.test_search.error_msg->re.escape("Parameter grid for parameter 'C' needs to be a list or a numpy array, but got '1,2,3' (of type str) instead. Single values need to be wrapped in a list with one element.")
A:sklearn.model_selection.tests.test_search.search->SearchCV(model, params, error_score='raise')
A:sklearn.model_selection.tests.test_search.y_pred->SearchCV(model, params, error_score='raise').predict(X)
A:sklearn.model_selection.tests.test_search.X_->csr_container(X_)
A:sklearn.model_selection.tests.test_search.y_pred2->GridSearchCV(clf, grid_params, cv=n_splits).predict(X_[180:])
A:sklearn.model_selection.tests.test_search.F1Loss->make_scorer(f1_loss, greater_is_better=False)
A:sklearn.model_selection.tests.test_search.y_pred3->GridSearchCV(clf, grid_params, cv=n_splits).predict(X_[180:])
A:sklearn.model_selection.tests.test_search.K_train->numpy.zeros((10, 20))
A:sklearn.model_selection.tests.test_search.K_test->numpy.dot(X_[180:], X_[:180].T)
A:sklearn.model_selection.tests.test_search.y_train->numpy.ones((10,))
A:sklearn.model_selection.tests.test_search.X_4d->numpy.arange(10 * 5 * 3 * 2).reshape(10, 5, 3, 2)
A:sklearn.model_selection.tests.test_search.y_3d->numpy.arange(10 * 7 * 11).reshape(10, 7, 11)
A:sklearn.model_selection.tests.test_search.km->KMeans(random_state=0, init='random', n_init=1)
A:sklearn.model_selection.tests.test_search.(X, _)->make_blobs(cluster_std=0.1, random_state=1, centers=[[0, 1], [1, 0], [0, 0]])
A:sklearn.model_selection.tests.test_search.sampler->ParameterSampler(params_distribution, n_iter=7)
A:sklearn.model_selection.tests.test_search.mask->numpy.ones(X.shape[0], dtype=bool)
A:sklearn.model_selection.tests.test_search.test_cv_scores->numpy.array([search.cv_results_['split%d_test_score' % s][0] for s in range(search.n_splits_)])
A:sklearn.model_selection.tests.test_search.train_cv_scores->numpy.array([search.cv_results_['split%d_train_score' % s][0] for s in range(search.n_splits_)])
A:sklearn.model_selection.tests.test_search.params->dict(C=np.logspace(-4, 1, 3), gamma=np.logspace(-5, 0, 3, base=0.1))
A:sklearn.model_selection.tests.test_search.n_outliers->int(outliers_fraction * n_samples)
A:sklearn.model_selection.tests.test_search.y_true->numpy.array([1] * n_samples)
A:sklearn.model_selection.tests.test_search.svc->LinearSVC(dual='auto', random_state=0)
A:sklearn.model_selection.tests.test_search.rs->RandomizedSearchCV(gbdt, param_grid, n_iter=1)
A:sklearn.model_selection.tests.test_search.result_keys->list(cv_results.keys())
A:sklearn.model_selection.tests.test_search.cv_scores->numpy.array([grid_search.cv_results_['split%d_test_score' % s][candidate_i] for s in range(n_splits)])
A:sklearn.model_selection.tests.test_search.correct_score->LinearSVC(dual='auto').score(X[test], y[test])
A:sklearn.model_selection.tests.test_search.dec->LinearSVC(dual='auto', random_state=0).decision_function(X[test])
A:sklearn.model_selection.tests.test_search.grid_search_pickled->pickle.loads(pickle.dumps(grid_search))
A:sklearn.model_selection.tests.test_search.random_search_pickled->pickle.loads(pickle.dumps(random_search))
A:sklearn.model_selection.tests.test_search.p->Pipeline([('imputer', SimpleImputer(strategy='mean', missing_values=np.nan)), ('classifier', MockClassifier())])
A:sklearn.model_selection.tests.test_search.warning_message->re.compile('5 fits failed.+total of 15.+The score on these train-test partitions for these parameters will be set to 0\\.1', flags=re.DOTALL)
A:sklearn.model_selection.tests.test_search.n_candidates->len(gs.cv_results_['params'])
A:sklearn.model_selection.tests.test_search.samples->list(sampler)
A:sklearn.model_selection.tests.test_search.gs2->GridSearchCV(LinearSVC(dual='auto', random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits), return_train_score=True)
A:sklearn.model_selection.tests.test_search.gs3->GridSearchCV(LinearSVC(dual='auto', random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X, y), return_train_score=True)
A:sklearn.model_selection.tests.test_search.gs4->GridSearchCV(LinearSVC(dual='auto', random_state=0), param_grid={'C': [0.1, 0.2, 0.3]}, cv=KFold(n_splits=n_splits, shuffle=True, random_state=0), return_train_score=True)
A:sklearn.model_selection.tests.test_search.X_round_trip->GridSearchCV(clf, {'foo_param': [1, 2, 3]}, cv=3, verbose=3).inverse_transform(grid_search.transform(X))
A:sklearn.model_selection.tests.test_search.results[k]->numpy.asanyarray(results[k])
A:sklearn.model_selection.tests.test_search.results->evaluate([{'min_samples_split': 5}, {'min_samples_split': 10}])
A:sklearn.model_selection.tests.test_search.mycv->CustomSearchCV(clf, return_train_score=True).fit(X, y)
A:sklearn.model_selection.tests.test_search.gscv->fit_grid([{'max_depth': [1, 2]}, {'min_samples_split': [5, 10]}])
A:sklearn.model_selection.tests.test_search.ridge->RandomizedSearchCV(Ridge(), {'alpha': [0.001, 0.01, 0.1]}, cv=cv, n_jobs=4)
A:sklearn.model_selection.tests.test_search.grid->SearchCV(DecisionTreeClassifier(), scoring=FailingScorer(), cv=3, return_train_score=return_train_score, **specialized_params)
A:sklearn.model_selection.tests.test_search.last_rank->SearchCV(DecisionTreeClassifier(), scoring=FailingScorer(), cv=3, return_train_score=return_train_score, **specialized_params).cv_results_['rank_test_score'].max()
A:sklearn.model_selection.tests.test_search.non_finite_mask->numpy.isnan(grid.cv_results_['mean_test_score'])
A:sklearn.model_selection.tests.test_search.cm->confusion_matrix(y, y_pred)
A:sklearn.model_selection.tests.test_search.est->LinearSVC(dual='auto')
A:sklearn.model_selection.tests.test_search.search_callable->GridSearchCV(est, {'C': [0.1, 1]}, scoring=custom_scorer, refit=True)
A:sklearn.model_selection.tests.test_search.search_str->GridSearchCV(est, {'C': [0.1, 1]}, scoring='recall', refit='recall')
A:sklearn.model_selection.tests.test_search.search_list_str->GridSearchCV(est, {'C': [0.1, 1]}, scoring=['recall'], refit='recall')
A:sklearn.model_selection.tests.test_search.error_message->re.compile(f'All the 15 fits failed.+your model is misconfigured.+{individual_fit_error_message}', flags=re.DOTALL)
A:sklearn.model_selection.tests.test_search.gbdt->HistGradientBoostingClassifier()
A:sklearn.model_selection.tests.test_search.preds_original->GridSearchCV(clf, grid_params, cv=n_splits).predict(X)
A:sklearn.model_selection.tests.test_search.X_precomputed->euclidean_distances(X)
A:sklearn.model_selection.tests.test_search.preds_precomputed->GridSearchCV(clf, grid_params, cv=n_splits).predict(X_precomputed)
A:sklearn.model_selection.tests.test_search.model->Pipeline([('transformer', MinimalTransformer()), ('predictor', Predictor())])
A:sklearn.model_selection.tests.test_search.(X_train, X_valid, y_train, y_valid)->train_test_split(*make_classification(random_state=42), random_state=42)
A:sklearn.model_selection.tests.test_search.match->re.findall('score=[\\d\\.]+', captured)
A:sklearn.model_selection.tests.test_search.n_samples->_num_samples(X)
A:sklearn.model_selection.tests.test_search.score_weights->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_search.score_metadata->numpy.random.RandomState(0).rand(n_samples)
A:sklearn.model_selection.tests.test_search.scorer_registry->_Registry()
A:sklearn.model_selection.tests.test_search.scorer->ConsumingScorer(registry=scorer_registry).set_score_request(sample_weight='score_weights', metadata='score_metadata')
A:sklearn.model_selection.tests.test_search.scoring->dict(my_scorer=scorer, accuracy='accuracy')
sklearn.model_selection.tests.test_search.BrokenClassifier(self,parameter=None)
sklearn.model_selection.tests.test_search.BrokenClassifier.__init__(self,parameter=None)
sklearn.model_selection.tests.test_search.BrokenClassifier.fit(self,X,y)
sklearn.model_selection.tests.test_search.BrokenClassifier.predict(self,X)
sklearn.model_selection.tests.test_search.FailingClassifier(self,parameter=None)
sklearn.model_selection.tests.test_search.FailingClassifier.__init__(self,parameter=None)
sklearn.model_selection.tests.test_search.FailingClassifier.fit(self,X,y=None)
sklearn.model_selection.tests.test_search.FailingClassifier.predict(self,X)
sklearn.model_selection.tests.test_search.FailingClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_search.LinearSVCNoScore(LinearSVC)
sklearn.model_selection.tests.test_search.LinearSVCNoScore.score(self)
sklearn.model_selection.tests.test_search.MockClassifier(self,foo_param=0)
sklearn.model_selection.tests.test_search.MockClassifier.__init__(self,foo_param=0)
sklearn.model_selection.tests.test_search.MockClassifier.fit(self,X,Y)
sklearn.model_selection.tests.test_search.MockClassifier.get_params(self,deep=False)
sklearn.model_selection.tests.test_search.MockClassifier.inverse_transform(self,X)
sklearn.model_selection.tests.test_search.MockClassifier.predict(self,T)
sklearn.model_selection.tests.test_search.MockClassifier.score(self,X=None,Y=None)
sklearn.model_selection.tests.test_search.MockClassifier.set_params(self,**params)
sklearn.model_selection.tests.test_search.MockClassifier.transform(self,X)
sklearn.model_selection.tests.test_search.assert_grid_iter_equals_getitem(grid)
sklearn.model_selection.tests.test_search.check_cv_results_array_types(search,param_keys,score_keys)
sklearn.model_selection.tests.test_search.check_cv_results_keys(cv_results,param_keys,score_keys,n_cand,extra_keys=())
sklearn.model_selection.tests.test_search.compare_cv_results_multimetric_with_single(search_multi,search_acc,search_rec)
sklearn.model_selection.tests.test_search.compare_refit_methods_when_refit_with_acc(search_multi,search_acc,refit)
sklearn.model_selection.tests.test_search.test_SearchCV_with_fit_params(SearchCV)
sklearn.model_selection.tests.test_search.test_X_as_list()
sklearn.model_selection.tests.test_search.test__custom_fit_no_run_search()
sklearn.model_selection.tests.test_search.test_callable_multimetric_clf_all_fits_fail()
sklearn.model_selection.tests.test_search.test_callable_multimetric_confusion_matrix()
sklearn.model_selection.tests.test_search.test_callable_multimetric_error_failing_clf()
sklearn.model_selection.tests.test_search.test_callable_multimetric_error_on_invalid_key()
sklearn.model_selection.tests.test_search.test_callable_multimetric_same_as_list_of_strings()
sklearn.model_selection.tests.test_search.test_callable_single_metric_same_as_single_string()
sklearn.model_selection.tests.test_search.test_classes__property()
sklearn.model_selection.tests.test_search.test_custom_run_search()
sklearn.model_selection.tests.test_search.test_empty_cv_iterator_error()
sklearn.model_selection.tests.test_search.test_grid_search()
sklearn.model_selection.tests.test_search.test_grid_search_allows_nans()
sklearn.model_selection.tests.test_search.test_grid_search_bad_param_grid()
sklearn.model_selection.tests.test_search.test_grid_search_classifier_all_fits_fail()
sklearn.model_selection.tests.test_search.test_grid_search_correct_score_results()
sklearn.model_selection.tests.test_search.test_grid_search_cv_results()
sklearn.model_selection.tests.test_search.test_grid_search_cv_results_multimetric()
sklearn.model_selection.tests.test_search.test_grid_search_cv_splits_consistency()
sklearn.model_selection.tests.test_search.test_grid_search_error()
sklearn.model_selection.tests.test_search.test_grid_search_failing_classifier()
sklearn.model_selection.tests.test_search.test_grid_search_failing_classifier_raise()
sklearn.model_selection.tests.test_search.test_grid_search_groups()
sklearn.model_selection.tests.test_search.test_grid_search_no_score()
sklearn.model_selection.tests.test_search.test_grid_search_one_grid_point()
sklearn.model_selection.tests.test_search.test_grid_search_pipeline_steps()
sklearn.model_selection.tests.test_search.test_grid_search_precomputed_kernel()
sklearn.model_selection.tests.test_search.test_grid_search_precomputed_kernel_error_nonsquare()
sklearn.model_selection.tests.test_search.test_grid_search_score_method()
sklearn.model_selection.tests.test_search.test_grid_search_sparse(csr_container)
sklearn.model_selection.tests.test_search.test_grid_search_sparse_scoring(csr_container)
sklearn.model_selection.tests.test_search.test_grid_search_when_param_grid_includes_range()
sklearn.model_selection.tests.test_search.test_grid_search_with_multioutput_data()
sklearn.model_selection.tests.test_search.test_gridsearch_nd()
sklearn.model_selection.tests.test_search.test_gridsearch_no_predict()
sklearn.model_selection.tests.test_search.test_multi_metric_search_forwards_metadata(SearchCV,param_search)
sklearn.model_selection.tests.test_search.test_n_features_in()
sklearn.model_selection.tests.test_search.test_no_refit()
sklearn.model_selection.tests.test_search.test_pandas_input()
sklearn.model_selection.tests.test_search.test_param_sampler()
sklearn.model_selection.tests.test_search.test_parameter_grid()
sklearn.model_selection.tests.test_search.test_parameters_sampler_replacement()
sklearn.model_selection.tests.test_search.test_pickle()
sklearn.model_selection.tests.test_search.test_predict_proba_disabled()
sklearn.model_selection.tests.test_search.test_random_search_bad_cv()
sklearn.model_selection.tests.test_search.test_random_search_cv_results()
sklearn.model_selection.tests.test_search.test_random_search_cv_results_multimetric()
sklearn.model_selection.tests.test_search.test_refit()
sklearn.model_selection.tests.test_search.test_refit_callable()
sklearn.model_selection.tests.test_search.test_refit_callable_invalid_type()
sklearn.model_selection.tests.test_search.test_refit_callable_multi_metric()
sklearn.model_selection.tests.test_search.test_refit_callable_out_bound(out_bound_value,search_cv)
sklearn.model_selection.tests.test_search.test_scalar_fit_param(SearchCV,param_search)
sklearn.model_selection.tests.test_search.test_scalar_fit_param_compat(SearchCV,param_search)
sklearn.model_selection.tests.test_search.test_score_rejects_params_with_no_routing_enabled(SearchCV,param_search)
sklearn.model_selection.tests.test_search.test_search_cv__pairwise_property_delegated_to_base_estimator()
sklearn.model_selection.tests.test_search.test_search_cv_pairwise_property_delegated_to_base_estimator(pairwise)
sklearn.model_selection.tests.test_search.test_search_cv_pairwise_property_equivalence_of_precomputed()
sklearn.model_selection.tests.test_search.test_search_cv_results_none_param()
sklearn.model_selection.tests.test_search.test_search_cv_results_rank_tie_breaking()
sklearn.model_selection.tests.test_search.test_search_cv_score_samples_error(search_cv)
sklearn.model_selection.tests.test_search.test_search_cv_score_samples_method(search_cv)
sklearn.model_selection.tests.test_search.test_search_cv_timing()
sklearn.model_selection.tests.test_search.test_search_cv_using_minimal_compatible_estimator(SearchCV,Predictor)
sklearn.model_selection.tests.test_search.test_search_cv_verbose_3(capsys,return_train_score)
sklearn.model_selection.tests.test_search.test_search_default_iid(SearchCV,specialized_params)
sklearn.model_selection.tests.test_search.test_search_estimator_param(SearchCV,param_search)
sklearn.model_selection.tests.test_search.test_search_train_scores_set_to_false()
sklearn.model_selection.tests.test_search.test_searchcv_raise_warning_with_non_finite_score(SearchCV,specialized_params,return_train_score)
sklearn.model_selection.tests.test_search.test_stochastic_gradient_loss_param()
sklearn.model_selection.tests.test_search.test_transform_inverse_transform_round_trip()
sklearn.model_selection.tests.test_search.test_trivial_cv_results_attr()
sklearn.model_selection.tests.test_search.test_unsupervised_grid_search()
sklearn.model_selection.tests.test_search.test_validate_parameter_input(klass,input,error_type,error_message)
sklearn.model_selection.tests.test_search.test_y_as_list()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/test_plot.py----------------------------------------
A:sklearn.model_selection.tests.test_plot.estimator->DecisionTreeClassifier(random_state=0)
A:sklearn.model_selection.tests.test_plot.display->SubclassOfDisplay.from_estimator(estimator, X, y, **params)
A:sklearn.model_selection.tests.test_plot.(_, legend_labels)->SubclassOfDisplay.from_estimator(estimator, X, y, **params).ax_.get_legend_handles_labels()
A:sklearn.model_selection.tests.test_plot.(train_sizes_abs, train_scores, test_scores)->learning_curve(estimator, X, y, train_sizes=train_sizes)
A:sklearn.model_selection.tests.test_plot.(train_scores, test_scores)->validation_curve(estimator, X, y, param_name=param_name, param_range=param_range)
A:sklearn.model_selection.tests.test_plot.(_, legend_label)->SubclassOfDisplay.from_estimator(estimator, X, y, **params).ax_.get_legend_handles_labels()
A:sklearn.model_selection.tests.test_plot.(x_data, y_data)->SubclassOfDisplay.from_estimator(estimator, X, y, **params).errorbar_[0].lines[0].get_data()
A:sklearn.model_selection.tests.test_plot.(x_data_train, y_data_train)->SubclassOfDisplay.from_estimator(estimator, X, y, **params).errorbar_[0].lines[0].get_data()
A:sklearn.model_selection.tests.test_plot.(x_data_test, y_data_test)->SubclassOfDisplay.from_estimator(estimator, X, y, **params).errorbar_[1].lines[0].get_data()
sklearn.model_selection.tests.test_plot.data()
sklearn.model_selection.tests.test_plot.test_curve_display_negate_score(pyplot,data,CurveDisplay,specific_params)
sklearn.model_selection.tests.test_plot.test_curve_display_parameters_validation(pyplot,data,params,err_type,err_msg,CurveDisplay,specific_params)
sklearn.model_selection.tests.test_plot.test_curve_display_plot_kwargs(pyplot,data,CurveDisplay,specific_params)
sklearn.model_selection.tests.test_plot.test_curve_display_score_name(pyplot,data,score_name,ylabel,CurveDisplay,specific_params)
sklearn.model_selection.tests.test_plot.test_curve_display_std_display_style(pyplot,data,CurveDisplay,specific_params)
sklearn.model_selection.tests.test_plot.test_curve_display_xscale_auto(pyplot,data,CurveDisplay,specific_params,expected_xscale)
sklearn.model_selection.tests.test_plot.test_learning_curve_display_default_usage(pyplot,data)
sklearn.model_selection.tests.test_plot.test_learning_curve_display_deprecate_log_scale(data,pyplot)
sklearn.model_selection.tests.test_plot.test_learning_curve_display_score_type(pyplot,data,std_display_style)
sklearn.model_selection.tests.test_plot.test_subclassing_displays(pyplot,data,Display,params)
sklearn.model_selection.tests.test_plot.test_validation_curve_display_default_usage(pyplot,data)
sklearn.model_selection.tests.test_plot.test_validation_curve_display_score_type(pyplot,data,std_display_style)
sklearn.model_selection.tests.test_plot.test_validation_curve_xscale_from_param_range_provided_as_a_list(pyplot,data,param_range,xscale)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/model_selection/tests/common.py----------------------------------------
A:sklearn.model_selection.tests.common.self.indices->iter(KFold(n_splits=n_splits).split(np.ones(n_samples)))
sklearn.model_selection.tests.common.OneTimeSplitter(self,n_splits=4,n_samples=99)
sklearn.model_selection.tests.common.OneTimeSplitter.__init__(self,n_splits=4,n_samples=99)
sklearn.model_selection.tests.common.OneTimeSplitter.get_n_splits(self,X=None,y=None,groups=None)
sklearn.model_selection.tests.common.OneTimeSplitter.split(self,X=None,y=None,groups=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_forest.py----------------------------------------
A:sklearn.ensemble._forest.random_instance->check_random_state(random_state)
A:sklearn.ensemble._forest.sample_indices->_generate_sample_indices(random_state, n_samples, n_samples_bootstrap)
A:sklearn.ensemble._forest.sample_counts->numpy.bincount(indices, minlength=n_samples)
A:sklearn.ensemble._forest.indices_range->numpy.arange(n_samples)
A:sklearn.ensemble._forest.curr_sample_weight->_check_sample_weight(sample_weight, X).copy()
A:sklearn.ensemble._forest.indices->_generate_sample_indices(tree.random_state, n_samples, n_samples_bootstrap)
A:sklearn.ensemble._forest.X->self._validate_X_predict(X)
A:sklearn.ensemble._forest.results->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer='threads')((delayed(tree.apply)(X, check_input=False) for tree in self.estimators_))
A:sklearn.ensemble._forest.indicators->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer='threads')((delayed(tree.decision_path)(X, check_input=False) for tree in self.estimators_))
A:sklearn.ensemble._forest.n_nodes_ptr->numpy.array(n_nodes).cumsum()
A:sklearn.ensemble._forest.(X, y)->self._validate_data(X, y, multi_output=True, accept_sparse='csc', dtype=DTYPE, force_all_finite=False)
A:sklearn.ensemble._forest.estimator->type(self.estimator)(criterion=self.criterion)
A:sklearn.ensemble._forest.missing_values_in_feature_mask->type(self.estimator)(criterion=self.criterion)._compute_missing_values_in_feature_mask(X, estimator_name=self.__class__.__name__)
A:sklearn.ensemble._forest.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.ensemble._forest.y->check_random_state(self.random_state).uniform(size=_num_samples(X))
A:sklearn.ensemble._forest.(y, expanded_class_weight)->self._validate_y_class_weight(y)
A:sklearn.ensemble._forest.n_samples_bootstrap->_get_n_samples_bootstrap(n_samples, self.max_samples)
A:sklearn.ensemble._forest.random_state->check_random_state(self.random_state)
A:sklearn.ensemble._forest.trees->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer='threads')((delayed(_parallel_build_trees)(t, self.bootstrap, X, y, sample_weight, i, len(trees), verbose=self.verbose, class_weight=self.class_weight, n_samples_bootstrap=n_samples_bootstrap, missing_values_in_feature_mask=missing_values_in_feature_mask) for (i, t) in enumerate(trees)))
A:sklearn.ensemble._forest.y_type->type_of_target(y)
A:sklearn.ensemble._forest.oob_pred->numpy.zeros(shape=oob_pred_shape, dtype=np.float64)
A:sklearn.ensemble._forest.n_oob_pred->numpy.zeros((n_samples, n_outputs), dtype=np.int64)
A:sklearn.ensemble._forest.unsampled_indices->_generate_unsampled_indices(estimator.random_state, n_samples, n_samples_bootstrap)
A:sklearn.ensemble._forest.y_pred->tree.predict(X, check_input=False)
A:sklearn.ensemble._forest.all_importances->numpy.mean(all_importances, axis=0, dtype=np.float64)
A:sklearn.ensemble._forest.prediction->predict(X, check_input=False)
A:sklearn.ensemble._forest.self.oob_decision_function_->self.oob_decision_function_.squeeze(axis=-1)
A:sklearn.ensemble._forest.self.oob_score_->scoring_function(y, self.oob_prediction_)
A:sklearn.ensemble._forest.y_original->numpy.copy(y)
A:sklearn.ensemble._forest.y_store_unique_indices->numpy.zeros(y.shape, dtype=int)
A:sklearn.ensemble._forest.(classes_k, y_store_unique_indices[:, k])->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.ensemble._forest.expanded_class_weight->compute_sample_weight(class_weight, y_original)
A:sklearn.ensemble._forest.proba->self.predict_proba(X)
A:sklearn.ensemble._forest.predictions->numpy.empty((n_samples, self.n_outputs_), dtype=class_type)
A:sklearn.ensemble._forest.predictions[:, k]->self.classes_[k].take(np.argmax(proba[k], axis=1), axis=0)
A:sklearn.ensemble._forest.(n_jobs, _, _)->_partition_estimators(self.n_estimators, self.n_jobs)
A:sklearn.ensemble._forest.lock->threading.Lock()
A:sklearn.ensemble._forest.proba[k]->numpy.log(proba[k])
A:sklearn.ensemble._forest.y_hat->numpy.zeros(X.shape[0], dtype=np.float64)
A:sklearn.ensemble._forest.self.oob_prediction_->self.oob_prediction_.squeeze(axis=-1)
A:sklearn.ensemble._forest.grid->numpy.asarray(grid, dtype=DTYPE, order='C')
A:sklearn.ensemble._forest.averaged_predictions->numpy.zeros(shape=grid.shape[0], dtype=np.float64, order='C')
A:sklearn.ensemble._forest.rnd->check_random_state(self.random_state)
A:sklearn.ensemble._forest.self.one_hot_encoder_->OneHotEncoder(sparse_output=self.sparse_output)
A:sklearn.ensemble._forest.output->self.one_hot_encoder_.fit_transform(self.apply(X))
sklearn.ensemble.ExtraTreesClassifier(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble.ExtraTreesRegressor(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble.RandomForestClassifier(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble.RandomForestRegressor(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble.RandomTreesEmbedding(self,n_estimators=100,*,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,sparse_output=True,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.RandomTreesEmbedding._set_oob_score_and_attributes(self,X,y,scoring_function=None)
sklearn.ensemble.RandomTreesEmbedding.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.RandomTreesEmbedding.fit_transform(self,X,y=None,sample_weight=None)
sklearn.ensemble.RandomTreesEmbedding.get_feature_names_out(self,input_features=None)
sklearn.ensemble.RandomTreesEmbedding.transform(self,X)
sklearn.ensemble._forest.BaseForest(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,max_samples=None)
sklearn.ensemble._forest.BaseForest.__init__(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,max_samples=None)
sklearn.ensemble._forest.BaseForest._compute_oob_predictions(self,X,y)
sklearn.ensemble._forest.BaseForest._get_estimators_indices(self)
sklearn.ensemble._forest.BaseForest._more_tags(self)
sklearn.ensemble._forest.BaseForest._set_oob_score_and_attributes(self,X,y,scoring_function=None)
sklearn.ensemble._forest.BaseForest._validate_X_predict(self,X)
sklearn.ensemble._forest.BaseForest._validate_y_class_weight(self,y)
sklearn.ensemble._forest.BaseForest.apply(self,X)
sklearn.ensemble._forest.BaseForest.decision_path(self,X)
sklearn.ensemble._forest.BaseForest.estimators_samples_(self)
sklearn.ensemble._forest.BaseForest.feature_importances_(self)
sklearn.ensemble._forest.BaseForest.fit(self,X,y,sample_weight=None)
sklearn.ensemble._forest.ExtraTreesClassifier(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.ExtraTreesClassifier.__init__(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.ExtraTreesRegressor(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.ExtraTreesRegressor.__init__(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.ForestClassifier(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,max_samples=None)
sklearn.ensemble._forest.ForestClassifier.__init__(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,max_samples=None)
sklearn.ensemble._forest.ForestClassifier._get_oob_predictions(tree,X)
sklearn.ensemble._forest.ForestClassifier._more_tags(self)
sklearn.ensemble._forest.ForestClassifier._set_oob_score_and_attributes(self,X,y,scoring_function=None)
sklearn.ensemble._forest.ForestClassifier._validate_y_class_weight(self,y)
sklearn.ensemble._forest.ForestClassifier.predict(self,X)
sklearn.ensemble._forest.ForestClassifier.predict_log_proba(self,X)
sklearn.ensemble._forest.ForestClassifier.predict_proba(self,X)
sklearn.ensemble._forest.ForestRegressor(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,max_samples=None)
sklearn.ensemble._forest.ForestRegressor.__init__(self,estimator,n_estimators=100,*,estimator_params=tuple(),bootstrap=False,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,max_samples=None)
sklearn.ensemble._forest.ForestRegressor._compute_partial_dependence_recursion(self,grid,target_features)
sklearn.ensemble._forest.ForestRegressor._get_oob_predictions(tree,X)
sklearn.ensemble._forest.ForestRegressor._more_tags(self)
sklearn.ensemble._forest.ForestRegressor._set_oob_score_and_attributes(self,X,y,scoring_function=None)
sklearn.ensemble._forest.ForestRegressor.predict(self,X)
sklearn.ensemble._forest.RandomForestClassifier(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.RandomForestClassifier.__init__(self,n_estimators=100,*,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features='sqrt',max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,class_weight=None,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.RandomForestRegressor(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.RandomForestRegressor.__init__(self,n_estimators=100,*,criterion='squared_error',max_depth=None,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_features=1.0,max_leaf_nodes=None,min_impurity_decrease=0.0,bootstrap=True,oob_score=False,n_jobs=None,random_state=None,verbose=0,warm_start=False,ccp_alpha=0.0,max_samples=None,monotonic_cst=None)
sklearn.ensemble._forest.RandomTreesEmbedding(self,n_estimators=100,*,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,sparse_output=True,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble._forest.RandomTreesEmbedding.__init__(self,n_estimators=100,*,max_depth=5,min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_leaf_nodes=None,min_impurity_decrease=0.0,sparse_output=True,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble._forest.RandomTreesEmbedding._set_oob_score_and_attributes(self,X,y,scoring_function=None)
sklearn.ensemble._forest.RandomTreesEmbedding.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble._forest.RandomTreesEmbedding.fit_transform(self,X,y=None,sample_weight=None)
sklearn.ensemble._forest.RandomTreesEmbedding.get_feature_names_out(self,input_features=None)
sklearn.ensemble._forest.RandomTreesEmbedding.transform(self,X)
sklearn.ensemble._forest._accumulate_prediction(predict,X,out,lock)
sklearn.ensemble._forest._generate_sample_indices(random_state,n_samples,n_samples_bootstrap)
sklearn.ensemble._forest._generate_unsampled_indices(random_state,n_samples,n_samples_bootstrap)
sklearn.ensemble._forest._get_n_samples_bootstrap(n_samples,max_samples)
sklearn.ensemble._forest._parallel_build_trees(tree,bootstrap,X,y,sample_weight,tree_idx,n_trees,verbose=0,class_weight=None,n_samples_bootstrap=None,missing_values_in_feature_mask=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_iforest.py----------------------------------------
A:sklearn.ensemble._iforest.X->self._validate_data(X, accept_sparse='csr', dtype=tree_dtype, reset=False)
A:sklearn.ensemble._iforest.rnd->check_random_state(self.random_state)
A:sklearn.ensemble._iforest.y->check_random_state(self.random_state).uniform(size=X.shape[0])
A:sklearn.ensemble._iforest.max_samples->int(self.max_samples * X.shape[0])
A:sklearn.ensemble._iforest.max_depth->int(np.ceil(np.log2(max(max_samples, 2))))
A:sklearn.ensemble._iforest.(self._average_path_length_per_tree, self._decision_path_lengths)->zip(*[(_average_path_length(tree.tree_.n_node_samples), tree.tree_.compute_node_depths()) for tree in self.estimators_])
A:sklearn.ensemble._iforest.self.offset_->numpy.percentile(self._score_samples(X), 100.0 * self.contamination)
A:sklearn.ensemble._iforest.decision_func->self.decision_function(X)
A:sklearn.ensemble._iforest.is_inlier->numpy.ones_like(decision_func, dtype=int)
A:sklearn.ensemble._iforest.n_samples->_num_samples(X)
A:sklearn.ensemble._iforest.chunk_n_rows->get_chunk_n_rows(row_bytes=16 * self._max_features, max_n_rows=n_samples)
A:sklearn.ensemble._iforest.slices->gen_batches(n_samples, chunk_n_rows)
A:sklearn.ensemble._iforest.scores->numpy.zeros(n_samples, order='f')
A:sklearn.ensemble._iforest.scores[sl]->self._compute_score_samples(X[sl], subsample_features)
A:sklearn.ensemble._iforest.depths->numpy.zeros(n_samples, order='f')
A:sklearn.ensemble._iforest.average_path_length_max_samples->_average_path_length([self._max_samples])
A:sklearn.ensemble._iforest.leaves_index->tree.apply(X_subset, check_input=False)
A:sklearn.ensemble._iforest.n_samples_leaf->n_samples_leaf.reshape((1, -1)).reshape((1, -1))
A:sklearn.ensemble._iforest.average_path_length->numpy.zeros(n_samples_leaf.shape)
sklearn.ensemble.IsolationForest(self,*,n_estimators=100,max_samples='auto',contamination='auto',max_features=1.0,bootstrap=False,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble.IsolationForest._compute_chunked_score_samples(self,X)
sklearn.ensemble.IsolationForest._compute_score_samples(self,X,subsample_features)
sklearn.ensemble.IsolationForest._more_tags(self)
sklearn.ensemble.IsolationForest._parallel_args(self)
sklearn.ensemble.IsolationForest._score_samples(self,X)
sklearn.ensemble.IsolationForest._set_oob_score(self,X,y)
sklearn.ensemble.IsolationForest.decision_function(self,X)
sklearn.ensemble.IsolationForest.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble.IsolationForest.predict(self,X)
sklearn.ensemble.IsolationForest.score_samples(self,X)
sklearn.ensemble._iforest.IsolationForest(self,*,n_estimators=100,max_samples='auto',contamination='auto',max_features=1.0,bootstrap=False,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble._iforest.IsolationForest.__init__(self,*,n_estimators=100,max_samples='auto',contamination='auto',max_features=1.0,bootstrap=False,n_jobs=None,random_state=None,verbose=0,warm_start=False)
sklearn.ensemble._iforest.IsolationForest._compute_chunked_score_samples(self,X)
sklearn.ensemble._iforest.IsolationForest._compute_score_samples(self,X,subsample_features)
sklearn.ensemble._iforest.IsolationForest._more_tags(self)
sklearn.ensemble._iforest.IsolationForest._parallel_args(self)
sklearn.ensemble._iforest.IsolationForest._score_samples(self,X)
sklearn.ensemble._iforest.IsolationForest._set_oob_score(self,X,y)
sklearn.ensemble._iforest.IsolationForest.decision_function(self,X)
sklearn.ensemble._iforest.IsolationForest.fit(self,X,y=None,sample_weight=None)
sklearn.ensemble._iforest.IsolationForest.predict(self,X)
sklearn.ensemble._iforest.IsolationForest.score_samples(self,X)
sklearn.ensemble._iforest._average_path_length(n_samples_leaf)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_weight_boosting.py----------------------------------------
A:sklearn.ensemble._weight_boosting.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], ensure_2d=True, allow_nd=True, dtype=None, y_numeric=is_regressor(self))
A:sklearn.ensemble._weight_boosting.sample_weight->numpy.exp(np.log(sample_weight) + estimator_weight * incorrect * (sample_weight > 0))
A:sklearn.ensemble._weight_boosting.self.estimator_weights_->numpy.zeros(self.n_estimators, dtype=np.float64)
A:sklearn.ensemble._weight_boosting.self.estimator_errors_->numpy.ones(self.n_estimators, dtype=np.float64)
A:sklearn.ensemble._weight_boosting.random_state->check_random_state(self.random_state)
A:sklearn.ensemble._weight_boosting.(sample_weight, estimator_weight, estimator_error)->self._boost(iboost, X, y, sample_weight, random_state)
A:sklearn.ensemble._weight_boosting.sample_weight_sum->numpy.sum(sample_weight)
A:sklearn.ensemble._weight_boosting.X->self._check_X(X)
A:sklearn.ensemble._weight_boosting.norm->self.estimator_weights_.sum()
A:sklearn.ensemble._weight_boosting.proba->self._make_estimator(random_state=random_state).predict_proba(X)
A:sklearn.ensemble._weight_boosting.log_proba->numpy.log(proba)
A:sklearn.ensemble._weight_boosting.estimator->self._make_estimator(random_state=random_state)
A:sklearn.ensemble._weight_boosting.y_predict_proba->self._make_estimator(random_state=random_state).predict_proba(X)
A:sklearn.ensemble._weight_boosting.self.classes_->getattr(estimator, 'classes_', None)
A:sklearn.ensemble._weight_boosting.self.n_classes_->len(self.classes_)
A:sklearn.ensemble._weight_boosting.y_predict->self._make_estimator(random_state=random_state).predict(X)
A:sklearn.ensemble._weight_boosting.estimator_error->(masked_sample_weight * masked_error_vector).sum()
A:sklearn.ensemble._weight_boosting.y_codes->numpy.array([-1.0 / (n_classes - 1), 1.0])
A:sklearn.ensemble._weight_boosting.y_coding->numpy.array([-1.0 / (n_classes - 1), 1.0]).take(classes == y[:, np.newaxis])
A:sklearn.ensemble._weight_boosting.pred->sum((np.where((estimator.predict(X) == classes).T, w, -1 / (n_classes - 1) * w) for (estimator, w) in zip(self.estimators_, self.estimator_weights_)))
A:sklearn.ensemble._weight_boosting.current_pred->numpy.where((estimator.predict(X) == classes).T, weight, -1 / (n_classes - 1) * weight)
A:sklearn.ensemble._weight_boosting.tmp_pred->numpy.copy(pred)
A:sklearn.ensemble._weight_boosting.decision->self.decision_function(X)
A:sklearn.ensemble._weight_boosting.bootstrap_idx->check_random_state(self.random_state).choice(np.arange(_num_samples(X)), size=_num_samples(X), replace=True, p=sample_weight)
A:sklearn.ensemble._weight_boosting.X_->_safe_indexing(X, bootstrap_idx)
A:sklearn.ensemble._weight_boosting.y_->_safe_indexing(y, bootstrap_idx)
A:sklearn.ensemble._weight_boosting.error_vect->numpy.abs(y_predict - y)
A:sklearn.ensemble._weight_boosting.error_max->masked_error_vector.max()
A:sklearn.ensemble._weight_boosting.sorted_idx->numpy.argsort(predictions, axis=1)
A:sklearn.ensemble._weight_boosting.weight_cdf->stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
A:sklearn.ensemble._weight_boosting.median_idx->median_or_above.argmax(axis=1)
sklearn.ensemble.AdaBoostClassifier(self,estimator=None,*,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble.AdaBoostClassifier._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._boost_discrete(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._boost_real(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostClassifier._compute_proba_from_decision(decision,n_classes)
sklearn.ensemble.AdaBoostClassifier._validate_estimator(self)
sklearn.ensemble.AdaBoostClassifier.decision_function(self,X)
sklearn.ensemble.AdaBoostClassifier.predict(self,X)
sklearn.ensemble.AdaBoostClassifier.predict_log_proba(self,X)
sklearn.ensemble.AdaBoostClassifier.predict_proba(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_decision_function(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_predict(self,X)
sklearn.ensemble.AdaBoostClassifier.staged_predict_proba(self,X)
sklearn.ensemble.AdaBoostRegressor(self,estimator=None,*,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble.AdaBoostRegressor._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble.AdaBoostRegressor._get_median_predict(self,X,limit)
sklearn.ensemble.AdaBoostRegressor._validate_estimator(self)
sklearn.ensemble.AdaBoostRegressor.predict(self,X)
sklearn.ensemble.AdaBoostRegressor.staged_predict(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier(self,estimator=None,*,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.__init__(self,estimator=None,*,n_estimators=50,learning_rate=1.0,algorithm='SAMME.R',random_state=None)
sklearn.ensemble._weight_boosting.AdaBoostClassifier._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble._weight_boosting.AdaBoostClassifier._boost_discrete(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble._weight_boosting.AdaBoostClassifier._boost_real(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble._weight_boosting.AdaBoostClassifier._compute_proba_from_decision(decision,n_classes)
sklearn.ensemble._weight_boosting.AdaBoostClassifier._validate_estimator(self)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.decision_function(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_log_proba(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.predict_proba(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_decision_function(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict(self,X)
sklearn.ensemble._weight_boosting.AdaBoostClassifier.staged_predict_proba(self,X)
sklearn.ensemble._weight_boosting.AdaBoostRegressor(self,estimator=None,*,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble._weight_boosting.AdaBoostRegressor.__init__(self,estimator=None,*,n_estimators=50,learning_rate=1.0,loss='linear',random_state=None)
sklearn.ensemble._weight_boosting.AdaBoostRegressor._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble._weight_boosting.AdaBoostRegressor._get_median_predict(self,X,limit)
sklearn.ensemble._weight_boosting.AdaBoostRegressor._validate_estimator(self)
sklearn.ensemble._weight_boosting.AdaBoostRegressor.predict(self,X)
sklearn.ensemble._weight_boosting.AdaBoostRegressor.staged_predict(self,X)
sklearn.ensemble._weight_boosting.BaseWeightBoosting(self,estimator=None,*,n_estimators=50,estimator_params=tuple(),learning_rate=1.0,random_state=None)
sklearn.ensemble._weight_boosting.BaseWeightBoosting.__init__(self,estimator=None,*,n_estimators=50,estimator_params=tuple(),learning_rate=1.0,random_state=None)
sklearn.ensemble._weight_boosting.BaseWeightBoosting._boost(self,iboost,X,y,sample_weight,random_state)
sklearn.ensemble._weight_boosting.BaseWeightBoosting._check_X(self,X)
sklearn.ensemble._weight_boosting.BaseWeightBoosting.feature_importances_(self)
sklearn.ensemble._weight_boosting.BaseWeightBoosting.fit(self,X,y,sample_weight=None)
sklearn.ensemble._weight_boosting.BaseWeightBoosting.staged_score(self,X,y,sample_weight=None)
sklearn.ensemble._weight_boosting._samme_proba(estimator,n_classes,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_stacking.py----------------------------------------
A:sklearn.ensemble._stacking.self.final_estimator_->clone(default)
A:sklearn.ensemble._stacking.(names, all_estimators)->self._validate_estimators()
A:sklearn.ensemble._stacking.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_single_estimator)(clone(est), X, y, sample_weight) for est in all_estimators if est != 'drop'))
A:sklearn.ensemble._stacking.self.named_estimators_->Bunch()
A:sklearn.ensemble._stacking.cv->check_cv(self.cv, y=y, classifier=is_classifier(self))
A:sklearn.ensemble._stacking.cv.random_state->numpy.random.RandomState()
A:sklearn.ensemble._stacking.predictions->Parallel(n_jobs=self.n_jobs)((delayed(cross_val_predict)(clone(est), X, y, cv=deepcopy(cv), method=meth, n_jobs=self.n_jobs, params=fit_params, verbose=self.verbose) for (est, meth) in zip(all_estimators, self.stack_method_) if est != 'drop'))
A:sklearn.ensemble._stacking.X_meta->self._concatenate_predictions(X, predictions)
A:sklearn.ensemble._stacking.input_features->_check_feature_names_in(self, input_features, generate_names=self.passthrough)
A:sklearn.ensemble._stacking.class_name->self.__class__.__name__.lower()
A:sklearn.ensemble._stacking.(names, estimators)->zip(*self.estimators)
A:sklearn.ensemble._stacking.parallel->_VisualBlock('parallel', estimators, names=names, dash_wrapped=False)
A:sklearn.ensemble._stacking.final_block->_VisualBlock('parallel', [final_estimator], names=['final_estimator'], dash_wrapped=False)
A:sklearn.ensemble._stacking.has_estimator->any((est != 'drop' for est in estimators))
A:sklearn.ensemble._stacking.self._label_encoder->LabelEncoder().fit(y)
A:sklearn.ensemble._stacking.y_encoded->self._label_encoder.transform(y)
A:sklearn.ensemble._stacking.y_pred->self.final_estimator_.predict_proba(self.transform(X))
A:sklearn.ensemble._stacking.final_estimator->RidgeCV()
A:sklearn.ensemble._stacking.y->column_or_1d(y, warn=True)
sklearn.ensemble.StackingClassifier(self,estimators,final_estimator=None,*,cv=None,stack_method='auto',n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble.StackingClassifier._sk_visual_block_(self)
sklearn.ensemble.StackingClassifier._validate_estimators(self)
sklearn.ensemble.StackingClassifier._validate_final_estimator(self)
sklearn.ensemble.StackingClassifier.decision_function(self,X)
sklearn.ensemble.StackingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.StackingClassifier.predict(self,X,**predict_params)
sklearn.ensemble.StackingClassifier.predict_proba(self,X)
sklearn.ensemble.StackingClassifier.transform(self,X)
sklearn.ensemble.StackingRegressor(self,estimators,final_estimator=None,*,cv=None,n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble.StackingRegressor._sk_visual_block_(self)
sklearn.ensemble.StackingRegressor._validate_final_estimator(self)
sklearn.ensemble.StackingRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble.StackingRegressor.fit_transform(self,X,y,sample_weight=None)
sklearn.ensemble.StackingRegressor.transform(self,X)
sklearn.ensemble._stacking.StackingClassifier(self,estimators,final_estimator=None,*,cv=None,stack_method='auto',n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble._stacking.StackingClassifier.__init__(self,estimators,final_estimator=None,*,cv=None,stack_method='auto',n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble._stacking.StackingClassifier._sk_visual_block_(self)
sklearn.ensemble._stacking.StackingClassifier._validate_estimators(self)
sklearn.ensemble._stacking.StackingClassifier._validate_final_estimator(self)
sklearn.ensemble._stacking.StackingClassifier.decision_function(self,X)
sklearn.ensemble._stacking.StackingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble._stacking.StackingClassifier.predict(self,X,**predict_params)
sklearn.ensemble._stacking.StackingClassifier.predict_proba(self,X)
sklearn.ensemble._stacking.StackingClassifier.transform(self,X)
sklearn.ensemble._stacking.StackingRegressor(self,estimators,final_estimator=None,*,cv=None,n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble._stacking.StackingRegressor.__init__(self,estimators,final_estimator=None,*,cv=None,n_jobs=None,passthrough=False,verbose=0)
sklearn.ensemble._stacking.StackingRegressor._sk_visual_block_(self)
sklearn.ensemble._stacking.StackingRegressor._validate_final_estimator(self)
sklearn.ensemble._stacking.StackingRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble._stacking.StackingRegressor.fit_transform(self,X,y,sample_weight=None)
sklearn.ensemble._stacking.StackingRegressor.transform(self,X)
sklearn.ensemble._stacking._BaseStacking(self,estimators,final_estimator=None,*,cv=None,stack_method='auto',n_jobs=None,verbose=0,passthrough=False)
sklearn.ensemble._stacking._BaseStacking.__init__(self,estimators,final_estimator=None,*,cv=None,stack_method='auto',n_jobs=None,verbose=0,passthrough=False)
sklearn.ensemble._stacking._BaseStacking._clone_final_estimator(self,default)
sklearn.ensemble._stacking._BaseStacking._concatenate_predictions(self,X,predictions)
sklearn.ensemble._stacking._BaseStacking._method_name(name,estimator,method)
sklearn.ensemble._stacking._BaseStacking._sk_visual_block_with_final_estimator(self,final_estimator)
sklearn.ensemble._stacking._BaseStacking._transform(self,X)
sklearn.ensemble._stacking._BaseStacking.fit(self,X,y,sample_weight=None)
sklearn.ensemble._stacking._BaseStacking.get_feature_names_out(self,input_features=None)
sklearn.ensemble._stacking._BaseStacking.n_features_in_(self)
sklearn.ensemble._stacking._BaseStacking.predict(self,X,**predict_params)
sklearn.ensemble._stacking._estimator_has(attr)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_base.py----------------------------------------
A:sklearn.ensemble._base.random_state->check_random_state(random_state)
A:sklearn.ensemble._base.to_set[key]->check_random_state(random_state).randint(np.iinfo(np.int32).max)
A:sklearn.ensemble._base.estimator->clone(self.estimator_)
A:sklearn.ensemble._base.n_jobs->min(effective_n_jobs(n_jobs), n_estimators)
A:sklearn.ensemble._base.n_estimators_per_job->numpy.full(n_jobs, n_estimators // n_jobs, dtype=int)
A:sklearn.ensemble._base.starts->numpy.cumsum(n_estimators_per_job)
A:sklearn.ensemble._base.(names, estimators)->zip(*self.estimators)
A:sklearn.ensemble._base.has_estimator->any((est != 'drop' for est in estimators))
A:sklearn.ensemble._base.allow_nan->all((_safe_tags(est[1])['allow_nan'] if est[1] != 'drop' else True for est in self.estimators))
sklearn.ensemble.BaseEnsemble(self,estimator=None,*,n_estimators=10,estimator_params=tuple())
sklearn.ensemble.BaseEnsemble.__getitem__(self,index)
sklearn.ensemble.BaseEnsemble.__iter__(self)
sklearn.ensemble.BaseEnsemble.__len__(self)
sklearn.ensemble.BaseEnsemble._make_estimator(self,append=True,random_state=None)
sklearn.ensemble.BaseEnsemble._validate_estimator(self,default=None)
sklearn.ensemble._base.BaseEnsemble(self,estimator=None,*,n_estimators=10,estimator_params=tuple())
sklearn.ensemble._base.BaseEnsemble.__getitem__(self,index)
sklearn.ensemble._base.BaseEnsemble.__init__(self,estimator=None,*,n_estimators=10,estimator_params=tuple())
sklearn.ensemble._base.BaseEnsemble.__iter__(self)
sklearn.ensemble._base.BaseEnsemble.__len__(self)
sklearn.ensemble._base.BaseEnsemble._make_estimator(self,append=True,random_state=None)
sklearn.ensemble._base.BaseEnsemble._validate_estimator(self,default=None)
sklearn.ensemble._base._BaseHeterogeneousEnsemble(self,estimators)
sklearn.ensemble._base._BaseHeterogeneousEnsemble.__init__(self,estimators)
sklearn.ensemble._base._BaseHeterogeneousEnsemble._more_tags(self)
sklearn.ensemble._base._BaseHeterogeneousEnsemble._validate_estimators(self)
sklearn.ensemble._base._BaseHeterogeneousEnsemble.get_params(self,deep=True)
sklearn.ensemble._base._BaseHeterogeneousEnsemble.named_estimators(self)
sklearn.ensemble._base._BaseHeterogeneousEnsemble.set_params(self,**params)
sklearn.ensemble._base._fit_single_estimator(estimator,X,y,sample_weight=None,message_clsname=None,message=None)
sklearn.ensemble._base._partition_estimators(n_estimators,n_jobs)
sklearn.ensemble._base._set_random_states(estimator,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_gb.py----------------------------------------
A:sklearn.ensemble._gb._LOSSES->_loss.loss._LOSSES.copy().copy()
A:sklearn.ensemble._gb.predictions->estimator.predict(X).astype(np.float64)
A:sklearn.ensemble._gb.terminal_regions->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, ccp_alpha=self.ccp_alpha).apply(X)
A:sklearn.ensemble._gb.masked_terminal_regions->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, ccp_alpha=self.ccp_alpha).apply(X).copy()
A:sklearn.ensemble._gb.neg_g->neg_gradient.take(indices, axis=0)
A:sklearn.ensemble._gb.numerator->numpy.average(neg_g, weights=sw)
A:sklearn.ensemble._gb.denominator->numpy.average(hessian, weights=sw)
A:sklearn.ensemble._gb.hessian->neg_gradient.take(indices, axis=0).copy()
A:sklearn.ensemble._gb.y_->y.astype(DOUBLE, copy=False).take(indices, axis=0)
A:sklearn.ensemble._gb.update->compute_update(y_, indices, neg_gradient, raw_prediction, k)
A:sklearn.ensemble._gb.abserr->numpy.abs(y_true - raw_prediction.squeeze())
A:sklearn.ensemble._gb.delta->_weighted_percentile(abserr, sample_weight, 100 * loss.quantile)
A:sklearn.ensemble._gb.loss.closs.delta->float(delta)
A:sklearn.ensemble._gb.self.verbose_fmt->' '.join(verbose_fmt)
A:sklearn.ensemble._gb.self.start_time->time()
A:sklearn.ensemble._gb.remaining_time->'{0:.2f}s'.format(remaining_time)
A:sklearn.ensemble._gb.neg_g_view->neg_gradient.reshape((-1, 1))
A:sklearn.ensemble._gb.y->y.astype(DOUBLE, copy=False).astype(DOUBLE, copy=False)
A:sklearn.ensemble._gb.tree->DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf, min_weight_fraction_leaf=self.min_weight_fraction_leaf, min_impurity_decrease=self.min_impurity_decrease, max_features=self.max_features, max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, ccp_alpha=self.ccp_alpha)
A:sklearn.ensemble._gb.max_features->max(1, int(self.max_features * self.n_features_in_))
A:sklearn.ensemble._gb.self.init_->DummyRegressor(strategy='mean')
A:sklearn.ensemble._gb.self.estimators_->numpy.resize(self.estimators_, (total_n_estimators, self.n_trees_per_iteration_))
A:sklearn.ensemble._gb.self.train_score_->numpy.resize(self.train_score_, total_n_estimators)
A:sklearn.ensemble._gb.self.oob_improvement_->numpy.zeros((total_n_estimators,), dtype=np.float64)
A:sklearn.ensemble._gb.self.oob_scores_->numpy.zeros((total_n_estimators,), dtype=np.float64)
A:sklearn.ensemble._gb.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE, multi_output=True)
A:sklearn.ensemble._gb.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.ensemble._gb.self._loss->self._get_loss(sample_weight=sample_weight)
A:sklearn.ensemble._gb.(X_train, X_val, y_train, y_val, sample_weight_train, sample_weight_val)->train_test_split(X, y, sample_weight, random_state=self.random_state, test_size=self.validation_fraction, stratify=stratify)
A:sklearn.ensemble._gb.raw_predictions->self.decision_function(X)
A:sklearn.ensemble._gb.msg->'The initial estimator {} does not support sample weights.'.format(self.init_.__class__.__name__)
A:sklearn.ensemble._gb.self._rng->check_random_state(self.random_state)
A:sklearn.ensemble._gb.X_train->check_array(X_train, dtype=DTYPE, order='C', accept_sparse='csr', force_all_finite=False)
A:sklearn.ensemble._gb.n_stages->self._fit_stages(X_train, y_train, raw_predictions, sample_weight_train, self._rng, X_val, y_val, sample_weight_val, begin_at_stage, monitor)
A:sklearn.ensemble._gb.sample_mask->_random_sample_mask(n_samples, n_inbag, random_state)
A:sklearn.ensemble._gb.n_inbag->max(1, int(self.subsample * n_samples))
A:sklearn.ensemble._gb.verbose_reporter->VerboseReporter(verbose=self.verbose)
A:sklearn.ensemble._gb.loss_history->numpy.full(self.n_iter_no_change, np.inf)
A:sklearn.ensemble._gb.y_val_pred_iter->self._staged_raw_predict(X_val, check_input=False)
A:sklearn.ensemble._gb.early_stopping->monitor(i, self, locals())
A:sklearn.ensemble._gb.X->self._validate_data(X, dtype=DTYPE, order='C', accept_sparse='csr', reset=False)
A:sklearn.ensemble._gb.avg_feature_importances->numpy.mean(relevant_feature_importances, axis=0, dtype=np.float64)
A:sklearn.ensemble._gb.grid->numpy.asarray(grid, dtype=DTYPE, order='C')
A:sklearn.ensemble._gb.averaged_predictions->numpy.zeros((n_trees_per_stage, grid.shape[0]), dtype=np.float64, order='C')
A:sklearn.ensemble._gb.leaves->leaves.reshape(X.shape[0], self.estimators_.shape[0]).reshape(X.shape[0], self.estimators_.shape[0])
A:sklearn.ensemble._gb.leaves[:, i, j]->estimator.apply(X, check_input=False)
A:sklearn.ensemble._gb.label_encoder->LabelEncoder()
A:sklearn.ensemble._gb.encoded_y_int->LabelEncoder().fit_transform(y)
A:sklearn.ensemble._gb.encoded_y->LabelEncoder().fit_transform(y).astype(float, copy=False)
A:sklearn.ensemble._gb.n_trim_classes->numpy.count_nonzero(np.bincount(encoded_y_int, sample_weight))
A:sklearn.ensemble._gb.encoded_classes->numpy.argmax(raw_predictions, axis=1)
A:sklearn.ensemble._gb.proba->self.predict_proba(X)
sklearn.ensemble.GradientBoostingClassifier(self,*,loss='log_loss',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble.GradientBoostingClassifier._encode_y(self,y,sample_weight)
sklearn.ensemble.GradientBoostingClassifier._get_loss(self,sample_weight)
sklearn.ensemble.GradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict_log_proba(self,X)
sklearn.ensemble.GradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble.GradientBoostingRegressor(self,*,loss='squared_error',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble.GradientBoostingRegressor._encode_y(self,y=None,sample_weight=None)
sklearn.ensemble.GradientBoostingRegressor._get_loss(self,sample_weight)
sklearn.ensemble.GradientBoostingRegressor.apply(self,X)
sklearn.ensemble.GradientBoostingRegressor.predict(self,X)
sklearn.ensemble.GradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble._gb.BaseGradientBoosting(self,*,loss,learning_rate,n_estimators,criterion,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_depth,min_impurity_decrease,init,subsample,max_features,ccp_alpha,random_state,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001)
sklearn.ensemble._gb.BaseGradientBoosting.__init__(self,*,loss,learning_rate,n_estimators,criterion,min_samples_split,min_samples_leaf,min_weight_fraction_leaf,max_depth,min_impurity_decrease,init,subsample,max_features,ccp_alpha,random_state,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001)
sklearn.ensemble._gb.BaseGradientBoosting._check_initialized(self)
sklearn.ensemble._gb.BaseGradientBoosting._clear_state(self)
sklearn.ensemble._gb.BaseGradientBoosting._compute_partial_dependence_recursion(self,grid,target_features)
sklearn.ensemble._gb.BaseGradientBoosting._encode_y(self,y=None,sample_weight=None)
sklearn.ensemble._gb.BaseGradientBoosting._fit_stage(self,i,X,y,raw_predictions,sample_weight,sample_mask,random_state,X_csc=None,X_csr=None)
sklearn.ensemble._gb.BaseGradientBoosting._fit_stages(self,X,y,raw_predictions,sample_weight,random_state,X_val,y_val,sample_weight_val,begin_at_stage=0,monitor=None)
sklearn.ensemble._gb.BaseGradientBoosting._get_loss(self,sample_weight)
sklearn.ensemble._gb.BaseGradientBoosting._init_state(self)
sklearn.ensemble._gb.BaseGradientBoosting._is_fitted(self)
sklearn.ensemble._gb.BaseGradientBoosting._make_estimator(self,append=True)
sklearn.ensemble._gb.BaseGradientBoosting._raw_predict(self,X)
sklearn.ensemble._gb.BaseGradientBoosting._raw_predict_init(self,X)
sklearn.ensemble._gb.BaseGradientBoosting._resize_state(self)
sklearn.ensemble._gb.BaseGradientBoosting._set_max_features(self)
sklearn.ensemble._gb.BaseGradientBoosting._staged_raw_predict(self,X,check_input=True)
sklearn.ensemble._gb.BaseGradientBoosting.apply(self,X)
sklearn.ensemble._gb.BaseGradientBoosting.feature_importances_(self)
sklearn.ensemble._gb.BaseGradientBoosting.fit(self,X,y,sample_weight=None,monitor=None)
sklearn.ensemble._gb.GradientBoostingClassifier(self,*,loss='log_loss',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble._gb.GradientBoostingClassifier.__init__(self,*,loss='log_loss',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble._gb.GradientBoostingClassifier._encode_y(self,y,sample_weight)
sklearn.ensemble._gb.GradientBoostingClassifier._get_loss(self,sample_weight)
sklearn.ensemble._gb.GradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.predict(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.predict_log_proba(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble._gb.GradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble._gb.GradientBoostingRegressor(self,*,loss='squared_error',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble._gb.GradientBoostingRegressor.__init__(self,*,loss='squared_error',learning_rate=0.1,n_estimators=100,subsample=1.0,criterion='friedman_mse',min_samples_split=2,min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,min_impurity_decrease=0.0,init=None,random_state=None,max_features=None,alpha=0.9,verbose=0,max_leaf_nodes=None,warm_start=False,validation_fraction=0.1,n_iter_no_change=None,tol=0.0001,ccp_alpha=0.0)
sklearn.ensemble._gb.GradientBoostingRegressor._encode_y(self,y=None,sample_weight=None)
sklearn.ensemble._gb.GradientBoostingRegressor._get_loss(self,sample_weight)
sklearn.ensemble._gb.GradientBoostingRegressor.apply(self,X)
sklearn.ensemble._gb.GradientBoostingRegressor.predict(self,X)
sklearn.ensemble._gb.GradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble._gb.VerboseReporter(self,verbose)
sklearn.ensemble._gb.VerboseReporter.__init__(self,verbose)
sklearn.ensemble._gb.VerboseReporter.init(self,est,begin_at_stage=0)
sklearn.ensemble._gb.VerboseReporter.update(self,j,est)
sklearn.ensemble._gb._init_raw_predictions(X,estimator,loss,use_predict_proba)
sklearn.ensemble._gb._safe_divide(numerator,denominator)
sklearn.ensemble._gb._update_terminal_regions(loss,tree,X,y,neg_gradient,raw_prediction,sample_weight,sample_mask,learning_rate=0.1,k=0)
sklearn.ensemble._gb.set_huber_delta(loss,y_true,raw_prediction,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_voting.py----------------------------------------
A:sklearn.ensemble._voting.(names, clfs)->self._validate_estimators()
A:sklearn.ensemble._voting.self.estimators_->Parallel(n_jobs=self.n_jobs)((delayed(_fit_single_estimator)(clone(clf), X, y, sample_weight=sample_weight, message_clsname='Voting', message=self._log_message(names[idx], idx + 1, len(clfs))) for (idx, clf) in enumerate(clfs) if clf != 'drop'))
A:sklearn.ensemble._voting.self.named_estimators_->Bunch()
A:sklearn.ensemble._voting.est_iter->iter(self.estimators_)
A:sklearn.ensemble._voting.(names, estimators)->zip(*self.estimators)
A:sklearn.ensemble._voting.self.le_->LabelEncoder().fit(y)
A:sklearn.ensemble._voting.transformed_y->self.le_.transform(y)
A:sklearn.ensemble._voting.maj->self.le_.inverse_transform(maj)
A:sklearn.ensemble._voting.predictions->self._predict(X)
A:sklearn.ensemble._voting.avg->numpy.average(self._collect_probas(X), axis=0, weights=self._weights_not_none)
A:sklearn.ensemble._voting.probas->self._collect_probas(X)
A:sklearn.ensemble._voting.class_name->self.__class__.__name__.lower()
A:sklearn.ensemble._voting.n_classes->len(self.classes_)
A:sklearn.ensemble._voting.y->column_or_1d(y, warn=True)
sklearn.ensemble.VotingClassifier(self,estimators,*,voting='hard',weights=None,n_jobs=None,flatten_transform=True,verbose=False)
sklearn.ensemble.VotingClassifier._check_voting(self)
sklearn.ensemble.VotingClassifier._collect_probas(self,X)
sklearn.ensemble.VotingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble.VotingClassifier.get_feature_names_out(self,input_features=None)
sklearn.ensemble.VotingClassifier.predict(self,X)
sklearn.ensemble.VotingClassifier.predict_proba(self,X)
sklearn.ensemble.VotingClassifier.transform(self,X)
sklearn.ensemble.VotingRegressor(self,estimators,*,weights=None,n_jobs=None,verbose=False)
sklearn.ensemble.VotingRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble.VotingRegressor.get_feature_names_out(self,input_features=None)
sklearn.ensemble.VotingRegressor.predict(self,X)
sklearn.ensemble.VotingRegressor.transform(self,X)
sklearn.ensemble._voting.VotingClassifier(self,estimators,*,voting='hard',weights=None,n_jobs=None,flatten_transform=True,verbose=False)
sklearn.ensemble._voting.VotingClassifier.__init__(self,estimators,*,voting='hard',weights=None,n_jobs=None,flatten_transform=True,verbose=False)
sklearn.ensemble._voting.VotingClassifier._check_voting(self)
sklearn.ensemble._voting.VotingClassifier._collect_probas(self,X)
sklearn.ensemble._voting.VotingClassifier.fit(self,X,y,sample_weight=None)
sklearn.ensemble._voting.VotingClassifier.get_feature_names_out(self,input_features=None)
sklearn.ensemble._voting.VotingClassifier.predict(self,X)
sklearn.ensemble._voting.VotingClassifier.predict_proba(self,X)
sklearn.ensemble._voting.VotingClassifier.transform(self,X)
sklearn.ensemble._voting.VotingRegressor(self,estimators,*,weights=None,n_jobs=None,verbose=False)
sklearn.ensemble._voting.VotingRegressor.__init__(self,estimators,*,weights=None,n_jobs=None,verbose=False)
sklearn.ensemble._voting.VotingRegressor.fit(self,X,y,sample_weight=None)
sklearn.ensemble._voting.VotingRegressor.get_feature_names_out(self,input_features=None)
sklearn.ensemble._voting.VotingRegressor.predict(self,X)
sklearn.ensemble._voting.VotingRegressor.transform(self,X)
sklearn.ensemble._voting._BaseVoting(TransformerMixin,_BaseHeterogeneousEnsemble)
sklearn.ensemble._voting._BaseVoting._log_message(self,name,idx,total)
sklearn.ensemble._voting._BaseVoting._predict(self,X)
sklearn.ensemble._voting._BaseVoting._sk_visual_block_(self)
sklearn.ensemble._voting._BaseVoting._weights_not_none(self)
sklearn.ensemble._voting._BaseVoting.fit(self,X,y,sample_weight=None)
sklearn.ensemble._voting._BaseVoting.fit_transform(self,X,y=None,**fit_params)
sklearn.ensemble._voting._BaseVoting.n_features_in_(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_bagging.py----------------------------------------
A:sklearn.ensemble._bagging.indices->sample_without_replacement(n_population, n_samples, random_state=random_state)
A:sklearn.ensemble._bagging.random_state->check_random_state(self.random_state)
A:sklearn.ensemble._bagging.feature_indices->_generate_indices(random_state, bootstrap_features, n_features, max_features)
A:sklearn.ensemble._bagging.sample_indices->_generate_indices(random_state, bootstrap_samples, n_samples, max_samples)
A:sklearn.ensemble._bagging.support_sample_weight->has_fit_parameter(ensemble.estimator_, 'sample_weight')
A:sklearn.ensemble._bagging.has_check_input->has_fit_parameter(ensemble.estimator_, 'check_input')
A:sklearn.ensemble._bagging.estimator->DecisionTreeRegressor()
A:sklearn.ensemble._bagging.estimator_fit->partial(estimator.fit, check_input=check_input)
A:sklearn.ensemble._bagging.(features, indices)->_generate_bagging_indices(random_state, bootstrap_features, bootstrap, n_features, n_samples, max_features, max_samples)
A:sklearn.ensemble._bagging.curr_sample_weight->_check_sample_weight(sample_weight, X, dtype=None).copy()
A:sklearn.ensemble._bagging.sample_counts->numpy.bincount(indices, minlength=n_samples)
A:sklearn.ensemble._bagging.proba->numpy.zeros((n_samples, n_classes))
A:sklearn.ensemble._bagging.proba_estimator->DecisionTreeRegressor().predict_proba(X[:, features])
A:sklearn.ensemble._bagging.predictions->numpy.zeros((n_samples,))
A:sklearn.ensemble._bagging.log_proba->numpy.log(self.predict_proba(X))
A:sklearn.ensemble._bagging.all_classes->numpy.arange(n_classes, dtype=int)
A:sklearn.ensemble._bagging.log_proba_estimator->DecisionTreeRegressor().predict_log_proba(X[:, features])
A:sklearn.ensemble._bagging.log_proba[:, estimator.classes_]->numpy.logaddexp(log_proba[:, estimator.classes_], log_proba_estimator[:, range(len(estimator.classes_))])
A:sklearn.ensemble._bagging.missing->numpy.setdiff1d(all_classes, estimator.classes_)
A:sklearn.ensemble._bagging.log_proba[:, missing]->numpy.logaddexp(log_proba[:, missing], -np.inf)
A:sklearn.ensemble._bagging.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'], dtype=None, force_all_finite=False, multi_output=True)
A:sklearn.ensemble._bagging.sample_weight->_check_sample_weight(sample_weight, X, dtype=None)
A:sklearn.ensemble._bagging.y->column_or_1d(y, warn=True)
A:sklearn.ensemble._bagging.max_samples->int(max_samples * X.shape[0])
A:sklearn.ensemble._bagging.max_features->max(1, int(max_features))
A:sklearn.ensemble._bagging.(n_jobs, n_estimators, starts)->_partition_estimators(n_more_estimators, self.n_jobs)
A:sklearn.ensemble._bagging.total_n_estimators->sum(n_estimators)
A:sklearn.ensemble._bagging.seeds->check_random_state(self.random_state).randint(MAX_INT, size=n_more_estimators)
A:sklearn.ensemble._bagging.all_results->Parallel(n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args())((delayed(_parallel_build_estimators)(n_estimators[i], self, X, y, sample_weight, seeds[starts[i]:starts[i + 1]], total_n_estimators, verbose=self.verbose, check_input=check_input) for i in range(n_jobs)))
A:sklearn.ensemble._bagging.(feature_indices, sample_indices)->_generate_bagging_indices(seed, self.bootstrap_features, self.bootstrap, self.n_features_in_, self._n_samples, self._max_features, self._max_samples)
A:sklearn.ensemble._bagging.p->DecisionTreeRegressor().predict(X[mask, :][:, features])
A:sklearn.ensemble._bagging.oob_score->accuracy_score(y, np.argmax(predictions, axis=1))
A:sklearn.ensemble._bagging.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.ensemble._bagging.self.n_classes_->len(self.classes_)
A:sklearn.ensemble._bagging.predicted_probabilitiy->self.predict_proba(X)
A:sklearn.ensemble._bagging.X->self._validate_data(X, accept_sparse=['csr', 'csc'], dtype=None, force_all_finite=False, reset=False)
A:sklearn.ensemble._bagging.(n_jobs, _, starts)->_partition_estimators(self.n_estimators, self.n_jobs)
A:sklearn.ensemble._bagging.all_proba->Parallel(n_jobs=n_jobs, verbose=self.verbose, **self._parallel_args())((delayed(_parallel_predict_proba)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X, self.n_classes_) for i in range(n_jobs)))
A:sklearn.ensemble._bagging.all_log_proba->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_log_proba)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X, self.n_classes_) for i in range(n_jobs)))
A:sklearn.ensemble._bagging.all_decisions->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_decision_function)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs)))
A:sklearn.ensemble._bagging.all_y_hat->Parallel(n_jobs=n_jobs, verbose=self.verbose)((delayed(_parallel_predict_regression)(self.estimators_[starts[i]:starts[i + 1]], self.estimators_features_[starts[i]:starts[i + 1]], X) for i in range(n_jobs)))
A:sklearn.ensemble._bagging.n_predictions->numpy.zeros((n_samples,))
A:sklearn.ensemble._bagging.self.oob_score_->r2_score(y, predictions)
sklearn.ensemble.BaggingClassifier(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble.BaggingClassifier._more_tags(self)
sklearn.ensemble.BaggingClassifier._set_oob_score(self,X,y)
sklearn.ensemble.BaggingClassifier._validate_estimator(self)
sklearn.ensemble.BaggingClassifier._validate_y(self,y)
sklearn.ensemble.BaggingClassifier.decision_function(self,X)
sklearn.ensemble.BaggingClassifier.predict(self,X)
sklearn.ensemble.BaggingClassifier.predict_log_proba(self,X)
sklearn.ensemble.BaggingClassifier.predict_proba(self,X)
sklearn.ensemble.BaggingRegressor(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble.BaggingRegressor._more_tags(self)
sklearn.ensemble.BaggingRegressor._set_oob_score(self,X,y)
sklearn.ensemble.BaggingRegressor._validate_estimator(self)
sklearn.ensemble.BaggingRegressor.predict(self,X)
sklearn.ensemble._bagging.BaggingClassifier(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaggingClassifier.__init__(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaggingClassifier._more_tags(self)
sklearn.ensemble._bagging.BaggingClassifier._set_oob_score(self,X,y)
sklearn.ensemble._bagging.BaggingClassifier._validate_estimator(self)
sklearn.ensemble._bagging.BaggingClassifier._validate_y(self,y)
sklearn.ensemble._bagging.BaggingClassifier.decision_function(self,X)
sklearn.ensemble._bagging.BaggingClassifier.predict(self,X)
sklearn.ensemble._bagging.BaggingClassifier.predict_log_proba(self,X)
sklearn.ensemble._bagging.BaggingClassifier.predict_proba(self,X)
sklearn.ensemble._bagging.BaggingRegressor(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaggingRegressor.__init__(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaggingRegressor._more_tags(self)
sklearn.ensemble._bagging.BaggingRegressor._set_oob_score(self,X,y)
sklearn.ensemble._bagging.BaggingRegressor._validate_estimator(self)
sklearn.ensemble._bagging.BaggingRegressor.predict(self,X)
sklearn.ensemble._bagging.BaseBagging(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaseBagging.__init__(self,estimator=None,n_estimators=10,*,max_samples=1.0,max_features=1.0,bootstrap=True,bootstrap_features=False,oob_score=False,warm_start=False,n_jobs=None,random_state=None,verbose=0)
sklearn.ensemble._bagging.BaseBagging._fit(self,X,y,max_samples=None,max_depth=None,sample_weight=None,check_input=True)
sklearn.ensemble._bagging.BaseBagging._get_estimators_indices(self)
sklearn.ensemble._bagging.BaseBagging._parallel_args(self)
sklearn.ensemble._bagging.BaseBagging._set_oob_score(self,X,y)
sklearn.ensemble._bagging.BaseBagging._validate_y(self,y)
sklearn.ensemble._bagging.BaseBagging.estimators_samples_(self)
sklearn.ensemble._bagging.BaseBagging.fit(self,X,y,sample_weight=None)
sklearn.ensemble._bagging._estimator_has(attr)
sklearn.ensemble._bagging._generate_bagging_indices(random_state,bootstrap_features,bootstrap_samples,n_features,n_samples,max_features,max_samples)
sklearn.ensemble._bagging._generate_indices(random_state,bootstrap,n_population,n_samples)
sklearn.ensemble._bagging._parallel_build_estimators(n_estimators,ensemble,X,y,sample_weight,seeds,total_n_estimators,verbose,check_input)
sklearn.ensemble._bagging._parallel_decision_function(estimators,estimators_features,X)
sklearn.ensemble._bagging._parallel_predict_log_proba(estimators,estimators_features,X,n_classes)
sklearn.ensemble._bagging._parallel_predict_proba(estimators,estimators_features,X,n_classes)
sklearn.ensemble._bagging._parallel_predict_regression(estimators,estimators_features,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/binning.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.binning.missing_mask->numpy.isnan(col_data)
A:sklearn.ensemble._hist_gradient_boosting.binning.col_data->numpy.ascontiguousarray(col_data, dtype=X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.binning.distinct_values->numpy.unique(col_data)
A:sklearn.ensemble._hist_gradient_boosting.binning.percentiles->numpy.linspace(0, 100, num=max_bins + 1)
A:sklearn.ensemble._hist_gradient_boosting.binning.midpoints->percentile(col_data, percentiles, method='midpoint').astype(X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.binning.X->check_array(X, dtype=[X_DTYPE], force_all_finite=False)
A:sklearn.ensemble._hist_gradient_boosting.binning.rng->check_random_state(self.random_state)
A:sklearn.ensemble._hist_gradient_boosting.binning.subset->check_random_state(self.random_state).choice(X.shape[0], self.subsample, replace=False)
A:sklearn.ensemble._hist_gradient_boosting.binning.self.is_categorical_->numpy.asarray(self.is_categorical, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.binning.thresholds->_find_binning_thresholds(X[:, f_idx], max_bins)
A:sklearn.ensemble._hist_gradient_boosting.binning.self.n_bins_non_missing_->numpy.array(n_bins_non_missing, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.binning.n_threads->_openmp_effective_n_threads(self.n_threads)
A:sklearn.ensemble._hist_gradient_boosting.binning.binned->numpy.zeros_like(X, dtype=X_BINNED_DTYPE, order='F')
A:sklearn.ensemble._hist_gradient_boosting.binning.categorical_features_indices->numpy.flatnonzero(self.is_categorical_)
A:sklearn.ensemble._hist_gradient_boosting.binning.f_idx_map->numpy.zeros(n_features, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.binning.f_idx_map[categorical_features_indices]->numpy.arange(n_categorical_features, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.binning.known_cat_bitsets->numpy.zeros((n_categorical_features, 8), dtype=X_BITSET_INNER_DTYPE)
sklearn.ensemble._hist_gradient_boosting.binning._BinMapper(self,n_bins=256,subsample=int(200000.0),is_categorical=None,known_categories=None,random_state=None,n_threads=None)
sklearn.ensemble._hist_gradient_boosting.binning._BinMapper.__init__(self,n_bins=256,subsample=int(200000.0),is_categorical=None,known_categories=None,random_state=None,n_threads=None)
sklearn.ensemble._hist_gradient_boosting.binning._BinMapper.fit(self,X,y=None)
sklearn.ensemble._hist_gradient_boosting.binning._BinMapper.make_known_categories_bitsets(self)
sklearn.ensemble._hist_gradient_boosting.binning._BinMapper.transform(self,X)
sklearn.ensemble._hist_gradient_boosting.binning._find_binning_thresholds(col_data,max_bins)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.grower.n_threads->_openmp_effective_n_threads(n_threads)
A:sklearn.ensemble._hist_gradient_boosting.grower.n_bins_non_missing->numpy.asarray(n_bins_non_missing, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.grower.has_missing_values->numpy.asarray(has_missing_values, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.grower.monotonic_cst->numpy.asarray(monotonic_cst, dtype=np.int8)
A:sklearn.ensemble._hist_gradient_boosting.grower.self.with_monotonic_cst->numpy.any(monotonic_cst != MonotonicConstraint.NO_CST)
A:sklearn.ensemble._hist_gradient_boosting.grower.is_categorical->numpy.asarray(is_categorical, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.grower.self.histogram_builder->HistogramBuilder(X_binned, n_bins, gradients, hessians, hessians_are_constant, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.grower.self.splitter->Splitter(X_binned=X_binned, n_bins_non_missing=n_bins_non_missing, missing_values_bin_idx=missing_values_bin_idx, has_missing_values=has_missing_values, is_categorical=is_categorical, monotonic_cst=monotonic_cst, l2_regularization=l2_regularization, min_hessian_to_split=min_hessian_to_split, min_samples_leaf=min_samples_leaf, min_gain_to_split=min_gain_to_split, hessians_are_constant=hessians_are_constant, feature_fraction_per_split=feature_fraction_per_split, rng=rng, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.grower.sum_gradients->sum_parallel(gradients, self.n_threads)
A:sklearn.ensemble._hist_gradient_boosting.grower.sum_hessians->sum_parallel(hessians, self.n_threads)
A:sklearn.ensemble._hist_gradient_boosting.grower.self.root->TreeNode(depth=depth, sample_indices=self.splitter.partition, sum_gradients=sum_gradients, sum_hessians=sum_hessians, value=0)
A:sklearn.ensemble._hist_gradient_boosting.grower.self.root.interaction_cst_indices->range(len(self.interaction_cst))
A:sklearn.ensemble._hist_gradient_boosting.grower.allowed_features->set()
A:sklearn.ensemble._hist_gradient_boosting.grower.self.root.allowed_features->numpy.fromiter(allowed_features, dtype=np.uint32, count=len(allowed_features))
A:sklearn.ensemble._hist_gradient_boosting.grower.tic->time()
A:sklearn.ensemble._hist_gradient_boosting.grower.self.root.histograms->self.histogram_builder.compute_histograms_brute(self.root.sample_indices, self.root.allowed_features)
A:sklearn.ensemble._hist_gradient_boosting.grower.node.split_info->self.splitter.find_node_split(n_samples=node.n_samples, histograms=node.histograms, sum_gradients=node.sum_gradients, sum_hessians=node.sum_hessians, value=node.value, lower_bound=node.children_lower_bound, upper_bound=node.children_upper_bound, allowed_features=node.allowed_features)
A:sklearn.ensemble._hist_gradient_boosting.grower.node->self.splittable_nodes.pop()
A:sklearn.ensemble._hist_gradient_boosting.grower.(sample_indices_left, sample_indices_right, right_child_pos)->self.splitter.split_indices(node.split_info, node.sample_indices)
A:sklearn.ensemble._hist_gradient_boosting.grower.left_child_node->TreeNode(depth, sample_indices_left, node.split_info.sum_gradient_left, node.split_info.sum_hessian_left, value=node.split_info.value_left)
A:sklearn.ensemble._hist_gradient_boosting.grower.right_child_node->TreeNode(depth, sample_indices_right, node.split_info.sum_gradient_right, node.split_info.sum_hessian_right, value=node.split_info.value_right)
A:sklearn.ensemble._hist_gradient_boosting.grower.(left_child_node.allowed_features, left_child_node.interaction_cst_indices)->self._compute_interactions(node)
A:sklearn.ensemble._hist_gradient_boosting.grower.smallest_child.histograms->self.histogram_builder.compute_histograms_brute(smallest_child.sample_indices, smallest_child.allowed_features)
A:sklearn.ensemble._hist_gradient_boosting.grower.largest_child.histograms->self.histogram_builder.compute_histograms_subtraction(node.histograms, smallest_child.histograms, smallest_child.allowed_features)
A:sklearn.ensemble._hist_gradient_boosting.grower.predictor_nodes->numpy.zeros(self.n_nodes, dtype=PREDICTOR_RECORD_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.grower.binned_left_cat_bitsets->numpy.zeros((self.n_categorical_splits, 8), dtype=X_BITSET_INNER_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.grower.raw_left_cat_bitsets->numpy.zeros((self.n_categorical_splits, 8), dtype=X_BITSET_INNER_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.grower.(next_free_node_idx, next_free_bitset_idx)->_fill_predictor_arrays(predictor_nodes, binned_left_cat_bitsets, raw_left_cat_bitsets, grower_node.left_child, binning_thresholds=binning_thresholds, n_bins_non_missing=n_bins_non_missing, next_free_node_idx=next_free_node_idx, next_free_bitset_idx=next_free_bitset_idx)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower(self,X_binned,gradients,hessians,max_leaf_nodes=None,max_depth=None,min_samples_leaf=20,min_gain_to_split=0.0,min_hessian_to_split=0.001,n_bins=256,n_bins_non_missing=None,has_missing_values=False,is_categorical=None,monotonic_cst=None,interaction_cst=None,l2_regularization=0.0,feature_fraction_per_split=1.0,rng=np.random.default_rng(),shrinkage=1.0,n_threads=None)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower.__init__(self,X_binned,gradients,hessians,max_leaf_nodes=None,max_depth=None,min_samples_leaf=20,min_gain_to_split=0.0,min_hessian_to_split=0.001,n_bins=256,n_bins_non_missing=None,has_missing_values=False,is_categorical=None,monotonic_cst=None,interaction_cst=None,l2_regularization=0.0,feature_fraction_per_split=1.0,rng=np.random.default_rng(),shrinkage=1.0,n_threads=None)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._apply_shrinkage(self)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._compute_best_split_and_push(self,node)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._compute_interactions(self,node)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._finalize_leaf(self,node)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._finalize_splittable_nodes(self)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._intilialize_root(self,gradients,hessians,hessians_are_constant)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower._validate_parameters(self,X_binned,min_gain_to_split,min_hessian_to_split)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower.grow(self)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower.make_predictor(self,binning_thresholds)
sklearn.ensemble._hist_gradient_boosting.grower.TreeGrower.split_next(self)
sklearn.ensemble._hist_gradient_boosting.grower.TreeNode(self,depth,sample_indices,sum_gradients,sum_hessians,value=None)
sklearn.ensemble._hist_gradient_boosting.grower.TreeNode.__init__(self,depth,sample_indices,sum_gradients,sum_hessians,value=None)
sklearn.ensemble._hist_gradient_boosting.grower.TreeNode.__lt__(self,other_node)
sklearn.ensemble._hist_gradient_boosting.grower.TreeNode.set_children_bounds(self,lower,upper)
sklearn.ensemble._hist_gradient_boosting.grower._fill_predictor_arrays(predictor_nodes,binned_left_cat_bitsets,raw_left_cat_bitsets,grower_node,binning_thresholds,n_bins_non_missing,next_free_node_idx=0,next_free_bitset_idx=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting._LOSSES->_loss.loss._LOSSES.copy().copy()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.update->loss.fit_intercept_only(y_true=y_true[indices] - raw_prediction[indices], sample_weight=sw)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.check_X_kwargs->dict(dtype=[X_DTYPE], force_all_finite=False)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self.is_categorical_->self._check_categorical_features(X)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X->self._preprocess_X(X, reset=False)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.ordinal_encoder->OrdinalEncoder(categories='auto', handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan, dtype=X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.check_X->partial(check_array, **check_X_kwargs)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.numerical_preprocessor->FunctionTransformer(check_X)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._preprocessor->ColumnTransformer([('encoder', ordinal_encoder, self.is_categorical_), ('numerical', numerical_preprocessor, ~self.is_categorical_)])
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.known_categories->self._check_categories()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.categorical_remapped->numpy.zeros(n_features, dtype=bool)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.feature_name->repr(encoder.feature_names_in_[feature_idx])
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.known_categories[feature_idx]->numpy.arange(len(categories), dtype=X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.categorical_columns_mask->numpy.asarray(X.dtypes == 'category')
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X_has_categorical_columns->numpy.asarray(X.dtypes == 'category').any()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.categorical_features->numpy.asarray(categorical_features)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.types->set((type(f) for f in categorical_features))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.feature_names_in_->getattr(X, 'columns', None)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.is_categorical->numpy.zeros(n_features, dtype=bool)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.feature_names->list(feature_names_in_)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.interaction_cst->self._check_interaction_cst(self._n_features)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.fit_start_time->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(X, known_categories)->self._preprocess_X(X, reset=True)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.y->y.astype(Y_DTYPE, copy=False).astype(Y_DTYPE, copy=False)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.sample_weight->self._finalize_sample_weight(sample_weight, y)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.rng->check_random_state(self.random_state)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._random_seed->check_random_state(self.random_state).randint(np.iinfo(np.uint32).max, dtype='u8')
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.feature_subsample_seed->check_random_state(self.random_state).randint(np.iinfo(np.uint32).max, dtype='u8')
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._feature_subsample_rng->numpy.random.default_rng(feature_subsample_seed)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.monotonic_cst->_check_monotonic_cst(self, self.monotonic_cst)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._loss->self._get_loss(sample_weight=sample_weight)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(X_train, X_val, y_train, y_val)->train_test_split(X, y, test_size=self.validation_fraction, stratify=stratify, random_state=self._random_seed)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(X_train, X_val, y_train, y_val, sample_weight_train, sample_weight_val)->train_test_split(X, y, sample_weight, test_size=self.validation_fraction, stratify=stratify, random_state=self._random_seed)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._bin_mapper->_BinMapper(n_bins=n_bins, is_categorical=self._is_categorical_remapped, known_categories=known_categories, random_state=self._random_seed, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X_binned_train->self._bin_data(X_train, is_training_data=True)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X_binned_val->self._bin_data(X_val, is_training_data=False)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.has_missing_values->(X_binned_train == self._bin_mapper.missing_values_bin_idx_).any(axis=0).astype(np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._baseline_prediction->self._loss.fit_intercept_only(y_true=y_train, sample_weight=sample_weight_train).reshape((1, -1))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.raw_predictions->self._raw_predict(X)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.raw_predictions_val->self._raw_predict(X_binned_val, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self._scorer->check_scoring(self, self.scoring)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(X_binned_small_train, y_small_train, sample_weight_small_train, indices_small_train)->self._get_small_trainset(X_binned_train, y_train, sample_weight_train, self._random_seed)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self.train_score_->numpy.asarray(self.train_score_)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.self.validation_score_->numpy.asarray(self.validation_score_)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(gradient, hessian)->self._loss.init_gradient_and_hessian(n_samples=n_samples, dtype=G_H_DTYPE, order='F')
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.iteration_start_time->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.g_view->gradient.reshape((-1, 1))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.h_view->hessian.reshape((-1, 1))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.grower->TreeGrower(X_binned=X_binned_train, gradients=g_view[:, k], hessians=h_view[:, k], n_bins=n_bins, n_bins_non_missing=self._bin_mapper.n_bins_non_missing_, has_missing_values=has_missing_values, is_categorical=self._is_categorical_remapped, monotonic_cst=monotonic_cst, interaction_cst=interaction_cst, max_leaf_nodes=self.max_leaf_nodes, max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, l2_regularization=self.l2_regularization, feature_fraction_per_split=self.max_features, rng=self._feature_subsample_rng, shrinkage=self.learning_rate, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.predictor->TreeGrower(X_binned=X_binned_train, gradients=g_view[:, k], hessians=h_view[:, k], n_bins=n_bins, n_bins_non_missing=self._bin_mapper.n_bins_non_missing_, has_missing_values=has_missing_values, is_categorical=self._is_categorical_remapped, monotonic_cst=monotonic_cst, interaction_cst=interaction_cst, max_leaf_nodes=self.max_leaf_nodes, max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, l2_regularization=self.l2_regularization, feature_fraction_per_split=self.max_features, rng=self._feature_subsample_rng, shrinkage=self.learning_rate, n_threads=n_threads).make_predictor(binning_thresholds=self._bin_mapper.bin_thresholds_)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.tic_pred->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.toc_pred->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.should_early_stop->self._check_early_stopping_scorer(X_binned_small_train, y_small_train, sample_weight_small_train, X_binned_val, y_val, sample_weight_val, raw_predictions_small_train=raw_predictions_small_train, raw_predictions_val=raw_predictions_val)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.n_total_leaves->sum((predictor.get_n_leaf_nodes() for predictors_at_ith_iteration in self._predictors for predictor in predictors_at_ith_iteration))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.n_predictors->sum((len(predictors_at_ith_iteration) for predictors_at_ith_iteration in self._predictors))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.indices->resample(indices, n_samples=subsample_size, replace=False, random_state=seed, stratify=stratify)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X_binned_small_train->numpy.ascontiguousarray(X_binned_small_train)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.patcher_raw_predict->_patch_raw_predict(self, raw_predictions)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.tic->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.X_binned->numpy.ascontiguousarray(X_binned)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.toc->time()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.n_trees->len(predictors_of_ith_iteration)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.max_depth->max((predictor.get_max_depth() for predictor in predictors_of_ith_iteration))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.n_leaves->sum((predictor.get_n_leaf_nodes() for predictor in predictors_of_ith_iteration))
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.is_binned->getattr(self, '_in_fit', False)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.(known_cat_bitsets, f_idx_map)->self._bin_mapper.make_known_categories_bitsets()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.predict->partial(predictor.predict, known_cat_bitsets=known_cat_bitsets, f_idx_map=f_idx_map, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.grid->numpy.asarray(grid, dtype=X_DTYPE, order='C')
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.averaged_predictions->numpy.zeros((self.n_trees_per_iteration_, grid.shape[0]), dtype=Y_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.expanded_class_weight->compute_sample_weight(self.class_weight, y)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.encoded_classes->numpy.argmax(proba, axis=1)
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.decision->decision.ravel().ravel()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.staged_decision->staged_decision.ravel().ravel()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.label_encoder->LabelEncoder()
A:sklearn.ensemble._hist_gradient_boosting.gradient_boosting.encoded_y->encoded_y.astype(Y_DTYPE, copy=False).astype(Y_DTYPE, copy=False)
sklearn.ensemble.HistGradientBoostingClassifier(self,loss='log_loss',*,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None,class_weight=None)
sklearn.ensemble.HistGradientBoostingClassifier._encode_y(self,y)
sklearn.ensemble.HistGradientBoostingClassifier._finalize_sample_weight(self,sample_weight,y)
sklearn.ensemble.HistGradientBoostingClassifier._get_loss(self,sample_weight)
sklearn.ensemble.HistGradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble.HistGradientBoostingClassifier.predict(self,X)
sklearn.ensemble.HistGradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble.HistGradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble.HistGradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble.HistGradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble.HistGradientBoostingRegressor(self,loss='squared_error',*,quantile=None,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None)
sklearn.ensemble.HistGradientBoostingRegressor._encode_y(self,y)
sklearn.ensemble.HistGradientBoostingRegressor._get_loss(self,sample_weight)
sklearn.ensemble.HistGradientBoostingRegressor.predict(self,X)
sklearn.ensemble.HistGradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting(self,loss,*,learning_rate,max_iter,max_leaf_nodes,max_depth,min_samples_leaf,l2_regularization,max_features,max_bins,categorical_features,monotonic_cst,interaction_cst,warm_start,early_stopping,scoring,validation_fraction,n_iter_no_change,tol,verbose,random_state)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting.__init__(self,loss,*,learning_rate,max_iter,max_leaf_nodes,max_depth,min_samples_leaf,l2_regularization,max_features,max_bins,categorical_features,monotonic_cst,interaction_cst,warm_start,early_stopping,scoring,validation_fraction,n_iter_no_change,tol,verbose,random_state)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._bin_data(self,X,is_training_data)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._check_categorical_features(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._check_categories(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._check_early_stopping_loss(self,raw_predictions,y_train,sample_weight_train,raw_predictions_val,y_val,sample_weight_val,n_threads=1)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._check_early_stopping_scorer(self,X_binned_small_train,y_small_train,sample_weight_small_train,X_binned_val,y_val,sample_weight_val,raw_predictions_small_train=None,raw_predictions_val=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._check_interaction_cst(self,n_features)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._clear_state(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._compute_partial_dependence_recursion(self,grid,target_features)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._encode_y(self,y=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._finalize_sample_weight(self,sample_weight,y)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._get_loss(self,sample_weight)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._get_small_trainset(self,X_binned_train,y_train,sample_weight_train,seed)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._is_fitted(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._more_tags(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._predict_iterations(self,X,predictors,raw_predictions,is_binned,n_threads)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._preprocess_X(self,X,*,reset)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._print_iteration_stats(self,iteration_start_time)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._raw_predict(self,X,n_threads=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._score_with_raw_predictions(self,X,y,sample_weight,raw_predictions=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._should_stop(self,scores)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._staged_raw_predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting._validate_parameters(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting.fit(self,X,y,sample_weight=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting.n_iter_(self)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier(self,loss='log_loss',*,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None,class_weight=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.__init__(self,loss='log_loss',*,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None,class_weight=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier._encode_y(self,y)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier._finalize_sample_weight(self,sample_weight,y)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier._get_loss(self,sample_weight)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.decision_function(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.predict_proba(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_decision_function(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier.staged_predict_proba(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor(self,loss='squared_error',*,quantile=None,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.__init__(self,loss='squared_error',*,quantile=None,learning_rate=0.1,max_iter=100,max_leaf_nodes=31,max_depth=None,min_samples_leaf=20,l2_regularization=0.0,max_features=1.0,max_bins=255,categorical_features='warn',monotonic_cst=None,interaction_cst=None,warm_start=False,early_stopping='auto',scoring='loss',validation_fraction=0.1,n_iter_no_change=10,tol=1e-07,verbose=0,random_state=None)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor._encode_y(self,y)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor._get_loss(self,sample_weight)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor.staged_predict(self,X)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting._patch_raw_predict(estimator,raw_predictions)
sklearn.ensemble._hist_gradient_boosting.gradient_boosting._update_leaves_values(loss,grower,y_true,raw_prediction,sample_weight)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/predictor.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.predictor.out->numpy.empty(X.shape[0], dtype=Y_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.predictor.self.nodes->self.nodes.astype(PREDICTOR_RECORD_DTYPE, casting='same_kind')
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor(self,nodes,binned_left_cat_bitsets,raw_left_cat_bitsets)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.__init__(self,nodes,binned_left_cat_bitsets,raw_left_cat_bitsets)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.__setstate__(self,state)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.compute_partial_dependence(self,grid,target_features,out)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.get_max_depth(self)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.get_n_leaf_nodes(self)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.predict(self,X,known_cat_bitsets,f_idx_map,n_threads)
sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor.predict_binned(self,X,missing_values_bin_idx,n_threads)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.DATA->numpy.random.RandomState(42).normal(loc=[0, 10], scale=[1, 0.01], size=(int(1000000.0), 2)).astype(X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.data->numpy.random.RandomState(42).normal(size=30000).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.bin_thresholds->_find_binning_thresholds(data, max_bins=255)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.err_msg->'n_bins={} should be no smaller than 3 and no larger than 256'.format(n_bins)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper->_BinMapper(n_bins=n_bins)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.binned->_BinMapper(n_bins=n_bins).fit_transform(data)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.is_categorical->numpy.zeros(2, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.min_indices->numpy.random.RandomState(42).normal(loc=[0, 10], scale=[1, 0.01], size=(int(1000000.0), 2)).astype(X_DTYPE).argmin(axis=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.max_indices->numpy.random.RandomState(42).normal(loc=[0, 10], scale=[1, 0.01], size=(int(1000000.0), 2)).astype(X_DTYPE).argmax(axis=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.tol->int(0.05 * expected_count_per_bin)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.count->(binned[:, feature_idx] == bin_idx).sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.rng->numpy.random.RandomState(42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.distinct_values->numpy.random.RandomState(42).normal(size=n_distinct)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.repeated_indices->numpy.random.RandomState(42).randint(low=0, high=n_distinct, size=1000)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_1->_BinMapper(n_bins=n_distinct + 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.binned_1->_BinMapper(n_bins=n_distinct + 1).fit_transform(data)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_2->_BinMapper(n_bins=min(256, n_distinct * 3) + 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.binned_2->_BinMapper(n_bins=min(256, n_distinct * 3) + 1).fit_transform(data)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_small->_BinMapper(n_bins=max_bins_small + 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_large->_BinMapper(n_bins=max_bins_small + 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.binned_small->_BinMapper(n_bins=max_bins_small + 1).fit_transform(data)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.binned_large->_BinMapper(n_bins=max_bins_small + 1).fit_transform(binned_small)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.X->numpy.array([[1, 2, 3]], dtype=X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_no_subsample->_BinMapper(subsample=None, random_state=0).fit(DATA)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.mapper_subsample->_BinMapper(subsample=256, random_state=0).fit(DATA)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.X_trans->_BinMapper(n_bins=n_bins).transform(X)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.bin_mapper->_BinMapper(is_categorical=is_categorical, known_categories=known_categories)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.expected_binned_X->numpy.array([0, 1, 2, 3]).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.X1->numpy.arange(10, 20).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.X2->numpy.arange(10, 15).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.(known_cat_bitsets, f_idx_map)->_BinMapper(is_categorical=is_categorical, known_categories=known_categories).make_known_categories_bitsets()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.expected_f_idx_map->numpy.array([0, 0, 1], dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_binning.expected_cat_bitset->numpy.zeros((2, 8), dtype=np.uint32)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_idempotence(max_bins_small,max_bins_large)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_identity_repeated_values(max_bins,n_distinct,multiplier)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_identity_small(max_bins,scale,offset)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_n_features_transform()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_random_data(max_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_repeated_values_invariance(n_distinct)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_bin_mapper_small_random_data(n_samples,max_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_categorical_feature(n_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_categorical_feature_negative_missing()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_categorical_parameters(is_categorical,known_categories,match)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_categorical_with_numerical_features(n_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_find_binning_thresholds_low_n_bins()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_find_binning_thresholds_random_data()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_find_binning_thresholds_regular_data()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_find_binning_thresholds_small_regular_data()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_infinite_values()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_invalid_n_bins(n_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_make_known_categories_bitsets()
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_map_to_bins(max_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_missing_values_support(n_bins,n_bins_non_missing,X_trans_expected)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_n_bins_non_missing(n_bins,diff)
sklearn.ensemble._hist_gradient_boosting.tests.test_binning.test_subsample()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.values->get_leaves_values()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.X_binned->numpy.arange(n_samples).reshape(-1, 1).astype(X_BINNED_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.gradients->numpy.random.RandomState(global_random_seed).normal(size=n_samples).astype(G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.hessians->numpy.ones(shape=1, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.grower->TreeGrower(X_binned, gradients, hessians, monotonic_cst=[monotonic_cst], shrinkage=0.1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.predictor->TreeGrower(X_binned, gradients, hessians, monotonic_cst=[monotonic_cst], shrinkage=0.1).make_predictor(binning_thresholds=np.zeros((X_binned.shape[1], X_binned.max() + 1)))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.f_0->numpy.random.RandomState(global_random_seed).rand(n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.f_1->numpy.random.RandomState(global_random_seed).rand(n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.X->pytest.importorskip('pandas').DataFrame({'a': [0, 1, 2], 'b': [0, 1, 2]})
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.noise->numpy.random.RandomState(global_random_seed).normal(loc=0.0, scale=0.01, size=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.gbdt->HistGradientBoostingRegressor(monotonic_cst=monotonic_cst)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.linspace->numpy.linspace(0, 1, 100)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.sin->numpy.sin(linspace)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.constant->numpy.full_like(linspace, fill_value=0.5)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.pred->HistGradientBoostingRegressor(monotonic_cst=monotonic_cst).predict(X)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.expected_msg->re.escape("monotonic_cst['a'] must be either -1, 0 or 1. Got '+'.")
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.pd->pytest.importorskip('pandas')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.y->numpy.array([0, 1, 0])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.sample_indices->numpy.arange(n_samples, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.all_hessians->numpy.ones(n_samples, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.all_gradients->numpy.array([1, 1, 100, 1, 1], dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.sum_gradients->numpy.array([1, 1, 100, 1, 1], dtype=G_H_DTYPE).sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.sum_hessians->numpy.ones(n_samples, dtype=G_H_DTYPE).sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.builder->HistogramBuilder(X_binned, n_bins, all_gradients, all_hessians, hessians_are_constant, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.n_bins_non_missing->numpy.array([n_bins - 1] * X_binned.shape[1], dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.has_missing_values->numpy.array([False] * X_binned.shape[1], dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.monotonic_cst->numpy.array([MonotonicConstraint.NO_CST] * X_binned.shape[1], dtype=np.int8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.is_categorical->numpy.zeros_like(monotonic_cst, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.splitter->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.histograms->HistogramBuilder(X_binned, n_bins, all_gradients, all_hessians, hessians_are_constant, n_threads).compute_histograms_brute(sample_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.value->compute_node_value(sum_gradients, sum_hessians, current_lower_bound, current_upper_bound, l2_regularization)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.split_info->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).find_node_split(n_samples, histograms, sum_gradients, sum_hessians, value, lower_bound=children_lower_bound, upper_bound=children_upper_bound)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.assert_children_values_bounded(grower,monotonic_cst)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.assert_children_values_monotonic(predictor,monotonic_cst)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.assert_leaves_values_monotonic(predictor,monotonic_cst)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.is_decreasing(a)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.is_increasing(a)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.test_bounded_value_min_gain_to_split()
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.test_input_error()
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.test_input_error_related_to_feature_names()
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.test_nodes_values(monotonic_cst,seed)
sklearn.ensemble._hist_gradient_boosting.tests.test_monotonic_contraints.test_predictions(global_random_seed,use_feature_names)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.rng->numpy.random.default_rng(42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.X_binned->numpy.asfortranarray(X_binned, dtype=X_BINNED_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.sample_indices->numpy.arange(n_samples, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.ordered_hessians->numpy.ones_like(binned_feature, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.sum_hessians->numpy.ones(1, dtype=G_H_DTYPE).sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.ordered_gradients->numpy.full_like(binned_feature, sign, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.sum_gradients->rng.uniform(low=0.5, high=1, size=n_samples).astype(G_H_DTYPE).sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.builder->HistogramBuilder(X_binned, n_bins, all_gradients, all_hessians, hessians_are_constant, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.n_bins_non_missing->numpy.array([n_bins] * X_binned.shape[1], dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.has_missing_values->numpy.array([False] * X_binned.shape[1], dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.monotonic_cst->numpy.array([MonotonicConstraint.NO_CST] * X_binned.shape[1], dtype=np.int8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.is_categorical->numpy.zeros_like(monotonic_cst, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.splitter->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.histograms->HistogramBuilder(X_binned, n_bins, all_gradients, all_hessians, hessians_are_constant, n_threads).compute_histograms_brute(sample_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.value->compute_node_value(sum_gradients, sum_hessians, -np.inf, np.inf, l2_regularization)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.split_info->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).find_node_split(n_samples, histograms, sum_gradients, sum_hessians, value)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.all_gradients->numpy.random.default_rng(42).uniform(low=0.5, high=1, size=n_samples).astype(G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.all_hessians->numpy.ones(1, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.hists_parent->numpy.asarray(hists_parent, dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.value_parent->compute_node_value(sum_gradients, sum_hessians, -np.inf, np.inf, l2_regularization)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.si_parent->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).find_node_split(n_samples, hists_parent, sum_gradients, sum_hessians, value_parent)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.(sample_indices_left, sample_indices_right, _)->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).split_indices(si_parent, sample_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.hists_left->numpy.asarray(hists_left, dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.value_left->compute_node_value(si_parent.sum_gradient_left, si_parent.sum_hessian_left, -np.inf, np.inf, l2_regularization)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.hists_right->numpy.asarray(hists_right, dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.value_right->compute_node_value(si_parent.sum_gradient_right, si_parent.sum_hessian_right, -np.inf, np.inf, l2_regularization)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.si_left->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).find_node_split(n_samples, hists_left, si_parent.sum_gradient_left, si_parent.sum_hessian_left, value_left)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.si_right->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).find_node_split(n_samples, hists_right, si_parent.sum_gradient_right, si_parent.sum_hessian_right, value_right)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.expected_gradient->all_gradients[indices].sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.expected_hessian->all_hessians[indices].sum()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.gradients->hists['sum_gradients'].sum(axis=1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.hessians->hists['sum_hessians'].sum(axis=1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.si_root->Splitter(feature_fraction_per_split=1.0, **params).find_node_split(n_samples, histograms, sum_gradients, sum_hessians, value, allowed_features=allowed_features)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.(samples_left, samples_right, position_right)->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).split_indices(si_root, splitter.partition)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.n_samples->len(X_binned)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.(samples_left, samples_right, _)->Splitter(X_binned, n_bins_non_missing, missing_values_bin_idx, has_missing_values, is_categorical, monotonic_cst, l2_regularization, min_hessian_to_split, min_samples_leaf, min_gain_to_split, hessians_are_constant).split_indices(split_info, splitter.partition)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.missing_samples_indices->numpy.flatnonzero(np.array(X_binned) == missing_values_bin_idx)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.non_missing_samples_indices->numpy.flatnonzero(np.array(X_binned) != missing_values_bin_idx)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.expected_bitset->numpy.zeros(8, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.left_mask->numpy.isin(X_binned.ravel(), expected_categories_left)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.allowed_features->numpy.array(list(set(range(n_features)) - forbidden_features), dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.params->dict(X_binned=X_binned, n_bins_non_missing=n_bins_non_missing, missing_values_bin_idx=missing_values_bin_idx, has_missing_values=has_missing_values, is_categorical=is_categorical, monotonic_cst=monotonic_cst, l2_regularization=l2_regularization, min_hessian_to_split=min_hessian_to_split, min_samples_leaf=min_samples_leaf, min_gain_to_split=min_gain_to_split, hessians_are_constant=hessians_are_constant, rng=rng)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.splitter_subsample->Splitter(feature_fraction_per_split=0.25, **params)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.splitter_all_features->Splitter(feature_fraction_per_split=1.0, **params)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting._assert_categories_equals_bitset(categories,bitset)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_gradient_and_hessian_sanity(constant_hessian)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_histogram_split(n_bins)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_min_gain_to_split()
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_split_feature_fraction_per_split(forbidden_features)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_split_indices()
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_split_interaction_constraints()
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_splitting_categorical_cat_smooth(X_binned,has_missing_values,n_bins_non_missing)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_splitting_categorical_sanity(X_binned,all_gradients,expected_categories_left,n_bins_non_missing,missing_values_bin_idx,has_missing_values,expected_missing_go_to_left)
sklearn.ensemble._hist_gradient_boosting.tests.test_splitting.test_splitting_missing_values(X_binned,all_gradients,has_missing_values,n_bins_non_missing,expected_split_on_nan,expected_bin_idx,expected_go_to_left)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_classification, y_classification)->make_classification(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_regression, y_regression)->make_regression(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_multi_classification, y_multi_classification)->make_classification(n_classes=3, n_informative=3, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.rng->numpy.random.RandomState(42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_dumb->numpy.random.RandomState(42).randn(n_samples, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_dumb->(X_dumb[:, 0] > 0).astype('int64')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X, y)->make_classification(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gb->Est(categorical_features=[True], max_bins=2)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbdt->HistGradientBoostingRegressor()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X->pytest.importorskip('pandas').DataFrame({'a': pd.Series([1, 2, 3], dtype='category'), 'b': [4, 5, 6]})
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y->rng.randint(low=0, high=2, size=n_samples).copy()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.sample_weight->numpy.ones(shape=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.coef->numpy.array([0.5, -2])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=0.5, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbdt_gamma->HistGradientBoostingRegressor(loss='gamma', random_state=123)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbdt_mse->HistGradientBoostingRegressor(loss='squared_error', random_state=123)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.dummy->DummyRegressor(strategy='mean').fit(X_train, y_train)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss_gbdt_gamma->mean_gamma_deviance(y, gbdt_gamma.predict(X))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss_gbdt_mse->mean_gamma_deviance(y, np.maximum(np.min(y_train), gbdt_mse.predict(X)))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss_dummy->mean_gamma_deviance(y, dummy.predict(X))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.model->HistGradientBoostingRegressor(loss='quantile', quantile=quantile, max_iter=25, random_state=0, max_leaf_nodes=10).fit(X, y)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.pinball_loss->PinballLoss(quantile=quantile)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss_true_quantile->pinball_loss(y, X @ coef + intercept)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss_pred_quantile->pinball_loss(y, model.predict(X))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbdt_pois->HistGradientBoostingRegressor(loss='poisson', random_state=rng)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbdt_ls->HistGradientBoostingRegressor(loss='squared_error', random_state=rng)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.metric_pois->mean_poisson_deviance(y, gbdt_pois.predict(X))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.metric_ls->mean_poisson_deviance(y, np.clip(gbdt_ls.predict(X), 1e-15, None))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.metric_dummy->mean_poisson_deviance(y, dummy.predict(X))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.mapper_whole_data->_BinMapper(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.mask->numpy.random.RandomState(42).binomial(1, 0.01, size=X.shape).astype(bool)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_small, y_small, *_)->Est(categorical_features=[True], max_bins=2)._get_small_trainset(X, y, seed=42, sample_weight_train=None)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(unique, counts)->numpy.unique(y_small, return_counts=True)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.mm->MinMaxScaler().fit(X)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.nan_mask->numpy.isnan(X[:, feature_idx])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_max->numpy.percentile(y, 70)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbm1->HistGradientBoostingRegressor(max_iter=100, max_leaf_nodes=5, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbm2->make_pipeline(MinMaxImputer(), clone(gbm1))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_isnan->numpy.isnan(X.ravel())
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.stump_clf->HistGradientBoostingClassifier(min_samples_leaf=1, max_iter=1, learning_rate=1, max_depth=2)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.gbrt->HistGradientBoostingClassifier(n_iter_no_change=10, scoring=scoring)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.est->HistGradientBoostingRegressor(random_state=0, scoring='neg_mean_absolute_error', early_stopping=True)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.est_sw->clone(est).fit(X, y, sample_weight=sample_weight)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.est_dup->clone(est).fit(X_dup, y_dup)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.bin_mapper->_BinMapper()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_binned->_BinMapper().fit_transform(X)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.loss->Loss(sample_weight=sample_weight)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(gradients, hessians)->Loss(sample_weight=sample_weight).init_gradient_and_hessian(n_samples=n_samples, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.raw_predictions->numpy.random.RandomState(42).normal(size=(n_samples, 1))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.sum_sw->numpy.zeros(shape=(n_features, bin_mapper.n_bins))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.grower->TreeGrower(X_binned, gradients[:, 0], hessians[:, 0], n_bins=bin_mapper.n_bins)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.histograms->TreeGrower(X_binned, gradients[:, 0], hessians[:, 0], n_bins=bin_mapper.n_bins).histogram_builder.compute_histograms_brute(grower.root.sample_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.mock_scorer->Mock(side_effect=get_scorer('neg_median_absolute_error'))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist->HistGradientBoostingClassifier(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.mock_raw_predict->Mock(side_effect=hist._raw_predict)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.staged_method->getattr(gb, 'staged_' + method_name)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.staged_predictions->list(staged_method(X_test))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.aux->HistGradientBoosting(max_iter=n_iter)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.pred_aux->getattr(aux, method_name)(X_test)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f1->numpy.random.RandomState(42).rand(n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f2->numpy.random.RandomState(42).randint(6, size=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_test->numpy.zeros((10, X.shape[1]), dtype=float)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf_cat->HistGradientBoostingClassifier(max_iter=1, max_depth=1, categorical_features=native_cat_spec)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf_no_cat->HistGradientBoostingClassifier(max_iter=1, max_depth=4, categorical_features=None)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.ct->make_column_transformer((OneHotEncoder(sparse_output=False), [1]), remainder='passthrough')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_ohe->make_column_transformer((OneHotEncoder(sparse_output=False), [1]), remainder='passthrough').fit_transform(X)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X[:, 0]->numpy.random.RandomState(42).randint(0, 10, size=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X[:, 1]->numpy.random.RandomState(42).randint(0, 10, size=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.pd->pytest.importorskip('pandas')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.expected_msg->re.escape('categorical_features should be passed as an array of integers or as a boolean mask when the model is fitted on data without feature names.')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.categorical_features->numpy.asarray(categorical_features)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.est_no_interactions->HistGradientBoostingRegressor(interaction_cst=[{0}, {1}], random_state=42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_df->_convert_container(f_cat[:, None], dataframe_lib, ['f_cat'], categorical_feature_names=['f_cat'])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf->HistGradientBoostingClassifier(random_state=0, max_depth=3)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf_class_weighted->clone(clf).set_params(class_weight=class_weight)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_imb->numpy.concatenate((X[~y_is_1], X[y_is_1][:10]))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.y_imb->numpy.concatenate((y[~y_is_1], y[y_is_1][:10]))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf_balanced->clone(clf).set_params(class_weight='balanced')
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.clf_sample_weight->clone(clf).set_params(class_weight=None)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_test_neg->numpy.asarray([[1, -2], [3, -4]])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.X_test_nan->numpy.asarray([[1, np.nan], [3, np.nan]])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f_num->numpy.random.RandomState(42).rand(n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f_cat->numpy.random.RandomState(42).randint(0, high=100, size=100).astype(str)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(X_train, X_test, X_train_df, X_test_df, y_train, y_test)->train_test_split(X, X_df, y, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist_kwargs->dict(max_iter=10, max_bins=max_bins, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist_np->HistGradientBoosting(categorical_features=[False, True], **hist_kwargs)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist_pd->HistGradientBoosting(categorical_features='from_dtype', **hist_kwargs)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.score_np->HistGradientBoosting(categorical_features=[False, True], **hist_kwargs).score(X_test, y_test)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.score_pd->HistGradientBoosting(categorical_features='from_dtype', **hist_kwargs).score(X_test_df, y_test)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f_ints->numpy.random.RandomState(42).randint(low=0, high=2, size=n_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.flipped->numpy.random.RandomState(42).choice([True, False], size=n_samples, p=[0.1, 0.9])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.df_a_b->_convert_container(f_cat_a_b[:, None], dataframe_lib, ['f_cat'], categorical_feature_names=['f_cat'])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.df_b_a->_convert_container(f_cat_b_a[:, None], dataframe_lib, ['f_cat'], categorical_feature_names=['f_cat'])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist_a_b->HistGradientBoostingClassifier(categorical_features='from_dtype', random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.hist_b_a->HistGradientBoostingClassifier(categorical_features='from_dtype', random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.new_dtype->numpy.dtype({'names': list(new_dtype_dict.keys()), 'formats': list(new_dtype_dict.values())})
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.(cls, args, state)->predictor.__reduce__()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.new_state->state.copy()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.new_state['nodes']->get_different_bitness_node_ndarray(new_state['nodes'])
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.score->HistGradientBoostingClassifier(random_state=0, max_depth=3).score(X, y)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.f->io.BytesIO()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.p->NumpyPickler(f)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.p.dispatch_table->copyreg.dispatch_table.copy()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.new_clf->joblib.load(joblib_dump_with_different_bitness())
A:sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.new_score->joblib.load(joblib_dump_with_different_bitness()).score(X, y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting._make_dumb_dataset(n_samples)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.get_different_bitness_node_ndarray(node_ndarray)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.reduce_predictor_with_different_bitness(predictor)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_absolute_error()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_absolute_error_sample_weight()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_binning_train_validation_are_separated()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_bad_encoding_errors(Est,use_pandas,feature_name)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_different_order_same_model(dataframe_lib)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_encoding_strategies()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_features_warn()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_errors(Est,categorical_features,monotonic_cst,expected_msg)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_errors_with_feature_names(Est)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_categorical_spec_no_categories(Est,categorical_features,as_array)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_check_interaction_cst(interaction_cst,n_features,result)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_class_weights()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_consistent_lengths()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_custom_loss(Est,loss,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_dataframe_categorical_errors(dataframe_lib,HistGradientBoosting)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_dataframe_categorical_results_same_as_ndarray(dataframe_lib,HistGradientBoosting)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_different_bitness_joblib_pickle()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_different_bitness_pickle()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_classification(data,scoring,validation_fraction,early_stopping,n_iter_no_change,tol)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_default(GradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_on_test_set_with_warm_start()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_regression(scoring,validation_fraction,early_stopping,n_iter_no_change,tol)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_early_stopping_with_sample_weights(monkeypatch)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_gamma()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_gamma_y_positive(y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_infinite_values()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_infinite_values_missing_values()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_init_parameters_validation(GradientBoosting,X,y,params,err_msg)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_interaction_cst_numerically()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_max_depth_max_leaf_nodes()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_minmax_imputation()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_resilience(problem,missing_proportion,expected_min_score_classification,expected_min_score_regression)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_missing_values_trivial()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_no_user_warning_with_scoring()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_poisson()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_poisson_y_positive(y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_quantile_asymmetric_error(quantile)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_raw_predict_is_called_with_custom_scorer()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_sample_weight_effect(problem,duplication)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_should_stop(scores,n_iter_no_change,tol,stopping)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_single_node_trees(Est)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_small_trainset()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_staged_predict(HistGradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_string_target_early_stopping(scoring)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_sum_hessians_are_sample_weight(Loss)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_uint8_predict(Est)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_unknown_categories_nan(insert_missing,Est,bool_categorical_parameter,missing_value)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_unknown_category_that_are_negative()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_division_hessians(data)
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_sample_weights_classification()
sklearn.ensemble._hist_gradient_boosting.tests.test_gradient_boosting.test_zero_sample_weights_regression()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.rng->numpy.random.RandomState(seed)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.X_binned->numpy.asfortranarray(X_binned)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.target->numpy.array([true_decision_function(x) for x in X_binned], dtype=Y_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.all_gradients->numpy.array([10, 1] * 11 + [1], dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.all_hessians->numpy.ones(1, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.(X_binned, all_gradients, all_hessians)->_make_training_data()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.grower->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.(left_node, right_node)->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).split_next()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.(right_left_node, right_right_node)->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).split_next()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.predictor->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories)))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.input_data->numpy.array([[0, 0], [42, 99], [128, 254], [129, 0], [129, 85], [254, 85], [129, 86], [129, 254], [242, 100]], dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.predictions->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict(X, known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.X->numpy.array([0, 1, np.inf, np.nan, np.nan]).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.y_scale->y.std()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.mapper->_BinMapper(n_bins=n_bins)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.depth->max((leaf.depth for leaf in grower.finalized_leaves))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.X_binned_float->numpy.asfortranarray(X_binned).astype(np.float32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.X_binned_C_array->numpy.ascontiguousarray(X_binned)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.gradients->numpy.random.RandomState(seed).normal(size=n_samples).astype(G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.hessians->numpy.ones(shape=1, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.all_nans->numpy.full(shape=(n_samples, 1), fill_value=np.nan)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.known_cat_bitsets->numpy.zeros((1, 8), dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.f_idx_map->numpy.array([0], dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.y_pred->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict(all_nans, known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.bin_mapper->_BinMapper()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.(known_cat_bitsets, f_idx_map)->_BinMapper().make_known_categories_bitsets()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.predictions_binned->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict_binned(X_binned, missing_values_bin_idx=bin_mapper.missing_values_bin_idx_, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.is_categorical->numpy.ones(1, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.prediction_binned->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict_binned(np.asarray([[6]]).astype(X_BINNED_DTYPE), missing_values_bin_idx=6, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.prediction->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict(np.array([[np.nan]]), known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.X_ohe->numpy.asfortranarray(X_ohe).astype(np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.preds->TreeGrower(X_binned, gradients, hessians, n_bins=n_bins, min_samples_leaf=1, interaction_cst=interaction_cst, n_threads=n_threads).make_predictor(binning_thresholds=np.zeros((1, n_unique_categories))).predict_binned(X_binned, missing_values_bin_idx=255, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.grower_ohe->TreeGrower(X_ohe, gradients, hessians, **grower_params)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.predictor_ohe->TreeGrower(X_ohe, gradients, hessians, **grower_params).make_predictor(binning_thresholds=np.zeros((X_ohe.shape[1], n_unique_categories)))
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.preds_ohe->TreeGrower(X_ohe, gradients, hessians, **grower_params).make_predictor(binning_thresholds=np.zeros((X_ohe.shape[1], n_unique_categories))).predict_binned(X_ohe, missing_values_bin_idx=255, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.parent_interaction_cst_indices->set(node.interaction_cst_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.right_interactions_cst_indices->set(node.right_child.interaction_cst_indices)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_grower.left_interactions_cst_indices->set(node.left_child.interaction_cst_indices)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower._check_children_consistency(parent,left,right)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower._make_training_data(n_bins=256,constant_hessian=True)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.assert_is_stump(grower)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_grow_tree(n_bins,constant_hessian,stopping_param,shrinkage)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_grow_tree_categories()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_grower_interaction_constraints()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_init_parameters_validation()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_input_validation()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_max_depth(max_depth)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_min_samples_leaf(n_samples,min_samples_leaf,n_bins,constant_hessian,noise)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_min_samples_leaf_root(n_samples,min_samples_leaf)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_missing_value_predict_only()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_ohe_equivalence(min_samples_leaf,n_unique_categories,target)
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_predictor_from_grower()
sklearn.ensemble._hist_gradient_boosting.tests.test_grower.test_split_on_nan_with_infinite_values()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.bitset->numpy.zeros(n_32bits_ints, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.binned_bitset->numpy.zeros(2, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.raw_bitset->numpy.zeros(2, dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.raw_categories->numpy.asarray(raw_categories, dtype=X_DTYPE)
sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.test_raw_bitset_from_binned_bitset(raw_categories,binned_cat_to_insert,expected_raw_bitset)
sklearn.ensemble._hist_gradient_boosting.tests.test_bitset.test_set_get_bitset(values_to_insert,expected_bitset)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.rng->numpy.random.RandomState(seed=seed)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.(X, y)->make_classification(n_samples=n_samples, n_classes=n_classes, n_features=5, n_informative=5, n_redundant=0, n_clusters_per_class=1, random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.X->_BinMapper(n_bins=max_bins + 1).fit_transform(X).astype(np.float32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=rng)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.est_sklearn->HistGradientBoostingClassifier(loss='log_loss', max_iter=max_iter, max_bins=max_bins, learning_rate=lr, early_stopping=False, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.est_lightgbm->get_equivalent_estimator(est_sklearn, lib='lightgbm', n_classes=n_classes)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.pred_lightgbm->get_equivalent_estimator(est_sklearn, lib='lightgbm', n_classes=n_classes).predict(X_test)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.pred_sklearn->HistGradientBoostingClassifier(loss='log_loss', max_iter=max_iter, max_bins=max_bins, learning_rate=lr, early_stopping=False, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes).predict(X_test)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.acc_lightgbm->accuracy_score(y_test, pred_lightgbm)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.acc_sklearn->accuracy_score(y_test, pred_sklearn)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.proba_lightgbm->get_equivalent_estimator(est_sklearn, lib='lightgbm', n_classes=n_classes).predict_proba(X_train)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.proba_sklearn->HistGradientBoostingClassifier(loss='log_loss', max_iter=max_iter, max_bins=max_bins, learning_rate=lr, early_stopping=False, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes).predict_proba(X_train)
sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.test_same_predictions_classification(seed,min_samples_leaf,n_samples,max_leaf_nodes)
sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.test_same_predictions_multiclass_classification(seed,min_samples_leaf,n_samples,max_leaf_nodes)
sklearn.ensemble._hist_gradient_boosting.tests.test_compare_lightgbm.test_same_predictions_regression(seed,loss,min_samples_leaf,n_samples,max_leaf_nodes)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.n_threads->_openmp_effective_n_threads()
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.(X, y)->make_regression(n_samples=500, n_features=10, n_informative=5, random_state=42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.mapper->_BinMapper(n_bins=n_bins, random_state=42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.X_train_binned->_BinMapper(n_bins=n_bins, random_state=42).fit_transform(X_train)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.hessians->numpy.ones(1, dtype=G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.grower->TreeGrower(X_train_binned, gradients, hessians, min_samples_leaf=min_samples_leaf, max_leaf_nodes=max_leaf_nodes, n_bins=n_bins, n_bins_non_missing=mapper.n_bins_non_missing_)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.predictor->TreePredictor(nodes, binned_cat_bitsets, raw_categorical_bitsets)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.known_cat_bitsets->numpy.zeros((1, 8), dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.f_idx_map->numpy.array([0], dtype=np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.y_pred_train->TreePredictor(nodes, binned_cat_bitsets, raw_categorical_bitsets).predict(X_train, known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.y_pred_test->TreePredictor(nodes, binned_cat_bitsets, raw_categorical_bitsets).predict(X_test, known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.X->numpy.array([-np.inf, 10, 20, np.inf]).reshape(-1, 1)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.nodes->numpy.zeros(3, dtype=PREDICTOR_RECORD_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.binned_cat_bitsets->numpy.zeros((1, 8), dtype=X_BITSET_INNER_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.raw_categorical_bitsets->numpy.zeros((1, 8), dtype=X_BITSET_INNER_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.known_cat_bitset->numpy.zeros((0, 8), dtype=X_BITSET_INNER_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.predictions->TreePredictor(nodes, binned_cat_bitsets, raw_categorical_bitsets).predict(np.array([[np.nan, 17]], dtype=X_DTYPE).T, known_cat_bitsets, f_idx_map, n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.categories->numpy.array([2, 5, 6, 8, 10, 15], dtype=X_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.bins_go_left->numpy.array(bins_go_left, dtype=X_BINNED_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.prediction_binned->TreePredictor(nodes, binned_cat_bitsets, raw_categorical_bitsets).predict_binned(X_binned, missing_values_bin_idx=6, n_threads=n_threads)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.known_cat_bitsets[0, 0]->numpy.sum(2 ** categories, dtype=np.uint32)
sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.test_categorical_predictor(bins_go_left,expected_predictions)
sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.test_infinite_values_and_thresholds(num_threshold,expected_predictions)
sklearn.ensemble._hist_gradient_boosting.tests.test_predictor.test_regression_dataset(n_bins)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.binned_feature->numpy.random.RandomState(42).randint(0, n_bins - 1, size=n_samples, dtype=np.uint8)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.ordered_gradients->numpy.random.RandomState(42).randn(n_samples).astype(G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.ordered_hessians->numpy.random.RandomState(42).lognormal(size=n_samples).astype(G_H_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.sample_indices->numpy.arange(n_samples).astype(np.uint32)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist->numpy.zeros((1, 3), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.rng->numpy.random.RandomState(42)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_gc->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_ghc->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.permutation->numpy.random.RandomState(42).permutation(n_sub_samples)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_gc_perm->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_ghc_perm->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_gc_root->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_ghc_root->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_naive->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_parent->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.mask->numpy.random.RandomState(42).randint(0, 2, n_samples).astype(bool)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_left->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_right->numpy.zeros((1, n_bins), dtype=HISTOGRAM_DTYPE)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_left_sub->numpy.copy(hist_parent)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.hist_right_sub->numpy.copy(hist_parent)
sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.test_build_histogram(build_func)
sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.test_hist_subtraction(constant_hessian)
sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.test_histogram_sample_order_independence()
sklearn.ensemble._hist_gradient_boosting.tests.test_histogram.test_unrolled_equivalent_to_naive(constant_hessian)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py----------------------------------------
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.(X_classification, y_classification)->make_classification(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.(X_regression, y_regression)->make_regression(random_state=0)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.estimator->GradientBoosting(max_iter=10, early_stopping=False, warm_start=True)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.gb_warm_start->GradientBoosting(n_iter_no_change=100, max_iter=50, random_state=rng, warm_start=True)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.gb_no_warm_start->GradientBoosting(n_iter_no_change=100, max_iter=75, random_state=rng, warm_start=False)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.gb->GradientBoosting(n_iter_no_change=n_iter_no_change, max_iter=10000, early_stopping=True, random_state=42, warm_start=True, tol=0.001, scoring=scoring)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.gb_1->GradientBoosting(early_stopping=True, max_iter=2, random_state=random_state)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.gb_2->GradientBoosting(early_stopping=True, max_iter=2, random_state=random_state, warm_start=True)
A:sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.random_state->_get_rng(rng_type)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start._assert_predictor_equal(gb_1,gb_2,X)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_max_iter_with_warm_start_validation(GradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_random_seeds_warm_start(GradientBoosting,X,y,rng_type)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_warm_start_clear(GradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_warm_start_early_stopping(GradientBoosting,X,y,scoring)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_warm_start_equal_n_estimators(GradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_warm_start_max_depth(GradientBoosting,X,y)
sklearn.ensemble._hist_gradient_boosting.tests.test_warm_start.test_warm_start_yields_identical_results(GradientBoosting,X,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_base.py----------------------------------------
A:sklearn.ensemble.tests.test_base.ensemble->BaggingClassifier(estimator=Perceptron(random_state=None), n_estimators=3)
A:sklearn.ensemble.tests.test_base.iris->load_iris()
A:sklearn.ensemble.tests.test_base.random_state->numpy.random.RandomState(3)
A:sklearn.ensemble.tests.test_base.np_int_ensemble->BaggingClassifier(estimator=Perceptron(), n_estimators=np.int32(3))
A:sklearn.ensemble.tests.test_base.clf1->Perceptron(random_state=None)
A:sklearn.ensemble.tests.test_base.clf2->Perceptron(random_state=None)
A:sklearn.ensemble.tests.test_base.est1->Pipeline(make_steps())
A:sklearn.ensemble.tests.test_base.params->sklearn.pipeline.Pipeline.get_params(self, *args, **kwargs).items()
A:sklearn.ensemble.tests.test_base.est2->cls(make_steps())
sklearn.ensemble.tests.test_base.test_base()
sklearn.ensemble.tests.test_base.test_set_random_states()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_forest.py----------------------------------------
A:sklearn.ensemble.tests.test_forest.(X_large, y_large)->sklearn.datasets.make_classification(n_samples=500, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, shuffle=False, random_state=0)
A:sklearn.ensemble.tests.test_forest.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_forest.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_forest.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.ensemble.tests.test_forest.(X_reg, y_reg)->sklearn.datasets.make_regression(n_samples=500, n_features=10, random_state=1)
A:sklearn.ensemble.tests.test_forest.(hastie_X, hastie_y)->sklearn.datasets.make_hastie_10_2(n_samples=20, random_state=1)
A:sklearn.ensemble.tests.test_forest.hastie_X->hastie_X.astype(np.float32).astype(np.float32)
A:sklearn.ensemble.tests.test_forest.clf->RandomForestClassifier(n_jobs=2, random_state=rng)
A:sklearn.ensemble.tests.test_forest.leaf_indices->RandomForestClassifier(n_jobs=2, random_state=rng).apply(X)
A:sklearn.ensemble.tests.test_forest.score->ForestEstimator(random_state=0).score(X, y)
A:sklearn.ensemble.tests.test_forest.reg->ExtraTreesRegressor(max_features=1, random_state=1).fit(X, y)
A:sklearn.ensemble.tests.test_forest.X->numpy.array([[0, 1, 2], [np.nan, 0, 2.0]])
A:sklearn.ensemble.tests.test_forest.y->numpy.random.RandomState(0).randint(0, high=2, size=n_samples)
A:sklearn.ensemble.tests.test_forest.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.ensemble.tests.test_forest.forest_poi->RandomForestRegressor(criterion='poisson', min_samples_leaf=10, max_features='sqrt', random_state=rng)
A:sklearn.ensemble.tests.test_forest.forest_mse->RandomForestRegressor(criterion='squared_error', min_samples_leaf=10, max_features='sqrt', random_state=rng)
A:sklearn.ensemble.tests.test_forest.dummy->DummyRegressor(strategy='mean').fit(X_train, y_train)
A:sklearn.ensemble.tests.test_forest.metric_poi->mean_poisson_deviance(y, forest_poi.predict(X))
A:sklearn.ensemble.tests.test_forest.metric_mse->mean_poisson_deviance(y, np.clip(forest_mse.predict(X), 1e-06, None))
A:sklearn.ensemble.tests.test_forest.metric_dummy->mean_poisson_deviance(y, dummy.predict(X))
A:sklearn.ensemble.tests.test_forest.r->FOREST_REGRESSORS[name](random_state=0)
A:sklearn.ensemble.tests.test_forest.est->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap)
A:sklearn.ensemble.tests.test_forest.n_important->numpy.sum(importances > 0.1)
A:sklearn.ensemble.tests.test_forest.sample_weight->numpy.ones(iris.target.shape)
A:sklearn.ensemble.tests.test_forest.n_samples->len(samples)
A:sklearn.ensemble.tests.test_forest.features->list(range(n_features))
A:sklearn.ensemble.tests.test_forest.mask_b->numpy.ones(n_samples, dtype=bool)
A:sklearn.ensemble.tests.test_forest.n_samples_b->len(X_)
A:sklearn.ensemble.tests.test_forest.data->numpy.array([[0, 0, 1, 0, 0, 1, 0, 1], [1, 0, 1, 1, 1, 0, 1, 2], [1, 0, 1, 1, 0, 1, 1, 3], [0, 1, 1, 1, 0, 1, 0, 4], [1, 1, 0, 1, 0, 1, 1, 5], [1, 1, 0, 1, 1, 1, 1, 6], [1, 0, 1, 0, 0, 1, 0, 7], [1, 1, 1, 1, 1, 1, 1, 8], [1, 1, 1, 1, 0, 1, 1, 9], [1, 1, 1, 0, 1, 1, 1, 0]])
A:sklearn.ensemble.tests.test_forest.true_importances->numpy.zeros(n_features)
A:sklearn.ensemble.tests.test_forest.true_importances[i]->mdi_importance(i, X, y)
A:sklearn.ensemble.tests.test_forest.err_msg->"This {} instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.".format(name)
A:sklearn.ensemble.tests.test_forest.classifier->RandomForestClassifier(random_state=0, bootstrap=False)
A:sklearn.ensemble.tests.test_forest.test_score->ForestRegressor(n_estimators=50, bootstrap=True, oob_score=oob_score, random_state=0).score(X_test, y_test)
A:sklearn.ensemble.tests.test_forest.regressor->ForestRegressor(n_estimators=50, bootstrap=True, oob_score=oob_score, random_state=0)
A:sklearn.ensemble.tests.test_forest.estimator->clone(estimator)
A:sklearn.ensemble.tests.test_forest.y_type->type_of_target(y)
A:sklearn.ensemble.tests.test_forest.n_samples_bootstrap->_get_n_samples_bootstrap(len(X), estimator.max_samples)
A:sklearn.ensemble.tests.test_forest.oob_pred->numpy.zeros([n_samples_test, 2])
A:sklearn.ensemble.tests.test_forest.oob_pred_sample->numpy.zeros(2)
A:sklearn.ensemble.tests.test_forest.oob_unsampled_indices->_generate_unsampled_indices(tree.random_state, len(X), n_samples_bootstrap)
A:sklearn.ensemble.tests.test_forest.forest->RandomForestRegressor(criterion='absolute_error')
A:sklearn.ensemble.tests.test_forest.y1->RandomForestRegressor(criterion='absolute_error').predict(X)
A:sklearn.ensemble.tests.test_forest.y2->RandomForestRegressor(criterion='absolute_error').predict(X)
A:sklearn.ensemble.tests.test_forest.obj->ForestEstimator(random_state=0)
A:sklearn.ensemble.tests.test_forest.pickle_object->pickle.dumps(obj)
A:sklearn.ensemble.tests.test_forest.obj2->pickle.loads(pickle_object)
A:sklearn.ensemble.tests.test_forest.score2->pickle.loads(pickle_object).score(X, y)
A:sklearn.ensemble.tests.test_forest.y_pred->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_forest.proba->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).predict_proba(X_test)
A:sklearn.ensemble.tests.test_forest.log_proba->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).predict_log_proba(X_test)
A:sklearn.ensemble.tests.test_forest.hasher->RandomTreesEmbedding(n_estimators=2, max_depth=2, sparse_output=False, random_state=0).fit(X)
A:sklearn.ensemble.tests.test_forest.(X, y)->make_data(n_samples=n_samples, n_features=n_features, random_state=rng)
A:sklearn.ensemble.tests.test_forest.X_transformed->RandomTreesEmbedding(n_estimators=2, max_depth=2, sparse_output=False, random_state=0).fit(X).fit_transform(X)
A:sklearn.ensemble.tests.test_forest.hasher_dense->RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0)
A:sklearn.ensemble.tests.test_forest.hasher_sparse->RandomTreesEmbedding(n_estimators=10, sparse_output=True, random_state=0)
A:sklearn.ensemble.tests.test_forest.X_transformed_dense->RandomTreesEmbedding(n_estimators=10, sparse_output=False, random_state=0).fit_transform(X)
A:sklearn.ensemble.tests.test_forest.X_transformed_sparse->RandomTreesEmbedding(n_estimators=2, max_depth=2, sparse_output=False, random_state=0).fit(X).fit_transform(csc_container(X))
A:sklearn.ensemble.tests.test_forest.svd->TruncatedSVD(n_components=2)
A:sklearn.ensemble.tests.test_forest.X_reduced->TruncatedSVD(n_components=2).fit_transform(X_transformed)
A:sklearn.ensemble.tests.test_forest.linear_clf->LinearSVC()
A:sklearn.ensemble.tests.test_forest.X_train->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.ensemble.tests.test_forest.y_train->numpy.random.RandomState(0).randint(0, 2, n_samples)
A:sklearn.ensemble.tests.test_forest.X_test->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.ensemble.tests.test_forest.uniques->defaultdict(int)
A:sklearn.ensemble.tests.test_forest.tree->''.join(('%d,%d/' % (f, int(t)) if f >= 0 else '-' for (f, t) in zip(tree.tree_.feature, tree.tree_.threshold)))
A:sklearn.ensemble.tests.test_forest.X[:, 0]->numpy.random.randint(0, 2, 1000)
A:sklearn.ensemble.tests.test_forest.X[:, 1]->numpy.random.randint(0, 3, 1000)
A:sklearn.ensemble.tests.test_forest.out->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).estimators_[0].tree_.apply(X)
A:sklearn.ensemble.tests.test_forest.node_counts->numpy.bincount(out)
A:sklearn.ensemble.tests.test_forest.weights->numpy.random.RandomState(0).rand(X.shape[0])
A:sklearn.ensemble.tests.test_forest.total_weight->numpy.sum(weights)
A:sklearn.ensemble.tests.test_forest.node_weights->numpy.bincount(out, weights=weights)
A:sklearn.ensemble.tests.test_forest.dense->ForestEstimator(random_state=0, max_depth=2).fit(X, y)
A:sklearn.ensemble.tests.test_forest.sparse->ForestEstimator(random_state=0, max_depth=2).fit(sparse_container(X), y)
A:sklearn.ensemble.tests.test_forest.X_2d->sklearn.datasets.load_iris().data[:, 0].reshape((-1, 1))
A:sklearn.ensemble.tests.test_forest.clf1->ForestClassifier(random_state=0)
A:sklearn.ensemble.tests.test_forest.clf2->ForestClassifier(class_weight=class_weight, random_state=0)
A:sklearn.ensemble.tests.test_forest.clf3->ForestClassifier(class_weight=[{0: 2.0, 1: 2.0, 2: 1.0}, {0: 2.0, 1: 1.0, 2: 2.0}, {0: 1.0, 1: 2.0, 2: 2.0}], random_state=0)
A:sklearn.ensemble.tests.test_forest.clf4->ForestClassifier(class_weight='balanced', random_state=0)
A:sklearn.ensemble.tests.test_forest.est_ws->ForestEstimator(n_estimators=n_estimators, random_state=42, warm_start=True)
A:sklearn.ensemble.tests.test_forest.est_no_ws->ForestEstimator(n_estimators=10, random_state=42, warm_start=False)
A:sklearn.ensemble.tests.test_forest.est_2->ForestEstimator(n_estimators=5, max_depth=3, warm_start=False, random_state=1, bootstrap=True, oob_score=False)
A:sklearn.ensemble.tests.test_forest.est_3->ForestEstimator(n_estimators=15, max_depth=3, warm_start=True, random_state=1, bootstrap=True, oob_score=False)
A:sklearn.ensemble.tests.test_forest.result->RandomForestClassifier(random_state=0, bootstrap=False).fit(X, y).predict(X)
A:sklearn.ensemble.tests.test_forest.(indicator, n_nodes_ptr)->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).decision_path(X)
A:sklearn.ensemble.tests.test_forest.leaves->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).apply(X)
A:sklearn.ensemble.tests.test_forest.gbr->RandomForestRegressor(n_estimators=10).fit(X, y)
A:sklearn.ensemble.tests.test_forest.ms_1_model->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=1.0, random_state=0)
A:sklearn.ensemble.tests.test_forest.ms_1_predict->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=1.0, random_state=0).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_forest.ms_None_model->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=None, random_state=0)
A:sklearn.ensemble.tests.test_forest.ms_None_predict->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=None, random_state=0).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_forest.ms_1_ms->mean_squared_error(ms_1_predict, y_test)
A:sklearn.ensemble.tests.test_forest.ms_None_ms->mean_squared_error(ms_None_predict, y_test)
A:sklearn.ensemble.tests.test_forest.(X_train, X_test, y_train, _)->train_test_split(X_large, y_large, random_state=0, stratify=y_large)
A:sklearn.ensemble.tests.test_forest.ms_1_proba->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=1.0, random_state=0).fit(X_train, y_train).predict_proba(X_test)
A:sklearn.ensemble.tests.test_forest.ms_None_proba->FOREST_CLASSIFIERS[name](bootstrap=True, max_samples=None, random_state=0).fit(X_train, y_train).predict_proba(X_test)
A:sklearn.ensemble.tests.test_forest.est1->ForestClass(n_estimators=1, random_state=rng, max_samples=None)
A:sklearn.ensemble.tests.test_forest.est2->ForestClass(n_estimators=1, random_state=rng, max_samples=2)
A:sklearn.ensemble.tests.test_forest.mse_criterion->MSE(n_outputs, n_samples)
A:sklearn.ensemble.tests.test_forest.random_state->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_forest.names->RandomTreesEmbedding(n_estimators=2, max_depth=2, sparse_output=False, random_state=0).fit(X).get_feature_names_out()
A:sklearn.ensemble.tests.test_forest.estimators_samples->ForestClass(n_estimators=10, max_samples=max_samples, max_features=0.5, random_state=seed, bootstrap=bootstrap).estimators_samples_.copy()
A:sklearn.ensemble.tests.test_forest.X_missing->numpy.array([[0, 1, 2], [np.nan, 0, 2.0]]).copy()
A:sklearn.ensemble.tests.test_forest.(X_missing_train, X_missing_test, y_train, y_test)->train_test_split(X_missing, y, random_state=0)
A:sklearn.ensemble.tests.test_forest.forest_with_missing->Forest(random_state=rng, n_estimators=50)
A:sklearn.ensemble.tests.test_forest.score_with_missing->Forest(random_state=rng, n_estimators=50).score(X_missing_test, y_test)
A:sklearn.ensemble.tests.test_forest.score_without_missing->RandomForestRegressor(criterion='absolute_error').score(X_test, y_test)
A:sklearn.ensemble.tests.test_forest.X_non_predictive->numpy.random.RandomState(0).standard_normal(size=(n_samples, 10))
A:sklearn.ensemble.tests.test_forest.X_random_mask->numpy.random.RandomState(0).choice([False, True], size=n_samples, p=[0.95, 0.05])
A:sklearn.ensemble.tests.test_forest.y_mask->numpy.random.RandomState(0).randint(0, high=2, size=n_samples).astype(bool)
A:sklearn.ensemble.tests.test_forest.predictive_feature->numpy.random.RandomState(0).standard_normal(size=n_samples)
A:sklearn.ensemble.tests.test_forest.X_predictive->numpy.random.RandomState(0).standard_normal(size=(n_samples, 10)).copy()
A:sklearn.ensemble.tests.test_forest.(X_predictive_train, X_predictive_test, X_non_predictive_train, X_non_predictive_test, y_train, y_test)->train_test_split(X_predictive, X_non_predictive, y, random_state=0)
A:sklearn.ensemble.tests.test_forest.forest_predictive->Forest(random_state=0).fit(X_predictive_train, y_train)
A:sklearn.ensemble.tests.test_forest.forest_non_predictive->Forest(random_state=0).fit(X_non_predictive_train, y_train)
A:sklearn.ensemble.tests.test_forest.predictive_test_score->Forest(random_state=0).fit(X_predictive_train, y_train).score(X_predictive_test, y_test)
sklearn.ensemble.tests.test_forest.MyBackend(self,*args,**kwargs)
sklearn.ensemble.tests.test_forest.MyBackend.__init__(self,*args,**kwargs)
sklearn.ensemble.tests.test_forest.MyBackend.start_call(self)
sklearn.ensemble.tests.test_forest.test_1d_input(name)
sklearn.ensemble.tests.test_forest.test_backend_respected()
sklearn.ensemble.tests.test_forest.test_balance_property_random_forest(criterion)
sklearn.ensemble.tests.test_forest.test_class_weight_balanced_and_bootstrap_multi_output(name)
sklearn.ensemble.tests.test_forest.test_class_weight_errors(name)
sklearn.ensemble.tests.test_forest.test_class_weights(name)
sklearn.ensemble.tests.test_forest.test_classes_shape(name)
sklearn.ensemble.tests.test_forest.test_classification_toy(name)
sklearn.ensemble.tests.test_forest.test_classifier_error_oob_score_multiclass_multioutput(ForestClassifier)
sklearn.ensemble.tests.test_forest.test_decision_path(name)
sklearn.ensemble.tests.test_forest.test_distribution()
sklearn.ensemble.tests.test_forest.test_dtype_convert(n_classes=15)
sklearn.ensemble.tests.test_forest.test_estimators_samples(ForestClass,bootstrap,seed)
sklearn.ensemble.tests.test_forest.test_forest_classifier_oob(ForestClassifier,X,y,X_type,lower_bound_accuracy,oob_score)
sklearn.ensemble.tests.test_forest.test_forest_degenerate_feature_importances()
sklearn.ensemble.tests.test_forest.test_forest_feature_importances_sum()
sklearn.ensemble.tests.test_forest.test_forest_multioutput_integral_regression_target(ForestRegressor)
sklearn.ensemble.tests.test_forest.test_forest_oob_score_requires_bootstrap(ForestEstimator)
sklearn.ensemble.tests.test_forest.test_forest_oob_warning(ForestEstimator)
sklearn.ensemble.tests.test_forest.test_forest_regressor_oob(ForestRegressor,X,y,X_type,lower_bound_r2,oob_score)
sklearn.ensemble.tests.test_forest.test_forest_y_sparse(csr_container)
sklearn.ensemble.tests.test_forest.test_gridsearch(name)
sklearn.ensemble.tests.test_forest.test_importances(dtype,name,criterion)
sklearn.ensemble.tests.test_forest.test_importances_asymptotic()
sklearn.ensemble.tests.test_forest.test_iris_criterion(name,criterion)
sklearn.ensemble.tests.test_forest.test_large_max_samples_exception(name)
sklearn.ensemble.tests.test_forest.test_little_tree_with_small_max_samples(ForestClass)
sklearn.ensemble.tests.test_forest.test_max_leaf_nodes_max_depth(name)
sklearn.ensemble.tests.test_forest.test_max_samples_bootstrap(name)
sklearn.ensemble.tests.test_forest.test_max_samples_boundary_classifiers(name)
sklearn.ensemble.tests.test_forest.test_max_samples_boundary_regressors(name)
sklearn.ensemble.tests.test_forest.test_memory_layout(name,dtype)
sklearn.ensemble.tests.test_forest.test_min_impurity_decrease()
sklearn.ensemble.tests.test_forest.test_min_samples_leaf(name)
sklearn.ensemble.tests.test_forest.test_min_samples_split(name)
sklearn.ensemble.tests.test_forest.test_min_weight_fraction_leaf(name)
sklearn.ensemble.tests.test_forest.test_missing_value_is_predictive(Forest)
sklearn.ensemble.tests.test_forest.test_missing_values_is_resilient(make_data,Forest)
sklearn.ensemble.tests.test_forest.test_mse_criterion_object_segfault_smoke_test(Forest)
sklearn.ensemble.tests.test_forest.test_multioutput(name)
sklearn.ensemble.tests.test_forest.test_multioutput_string(name)
sklearn.ensemble.tests.test_forest.test_non_supported_criterion_raises_error_with_missing_values()
sklearn.ensemble.tests.test_forest.test_oob_not_computed_twice(name)
sklearn.ensemble.tests.test_forest.test_parallel(name)
sklearn.ensemble.tests.test_forest.test_parallel_train()
sklearn.ensemble.tests.test_forest.test_pickle(name)
sklearn.ensemble.tests.test_forest.test_poisson_vs_mse()
sklearn.ensemble.tests.test_forest.test_poisson_y_positive_check()
sklearn.ensemble.tests.test_forest.test_probability(name)
sklearn.ensemble.tests.test_forest.test_random_hasher()
sklearn.ensemble.tests.test_forest.test_random_hasher_sparse_data(csc_container)
sklearn.ensemble.tests.test_forest.test_random_trees_dense_equal()
sklearn.ensemble.tests.test_forest.test_random_trees_dense_type()
sklearn.ensemble.tests.test_forest.test_random_trees_embedding_feature_names_out()
sklearn.ensemble.tests.test_forest.test_random_trees_embedding_raise_error_oob(oob_score)
sklearn.ensemble.tests.test_forest.test_read_only_buffer(csr_container,monkeypatch)
sklearn.ensemble.tests.test_forest.test_regression_criterion(name,criterion)
sklearn.ensemble.tests.test_forest.test_regressor_attributes(name)
sklearn.ensemble.tests.test_forest.test_round_samples_to_one_when_samples_too_low(class_weight)
sklearn.ensemble.tests.test_forest.test_sparse_input(name,sparse_container)
sklearn.ensemble.tests.test_forest.test_unfitted_feature_importances(name)
sklearn.ensemble.tests.test_forest.test_warm_start(name)
sklearn.ensemble.tests.test_forest.test_warm_start_clear(name)
sklearn.ensemble.tests.test_forest.test_warm_start_equal_n_estimators(name)
sklearn.ensemble.tests.test_forest.test_warm_start_oob(name)
sklearn.ensemble.tests.test_forest.test_warm_start_smaller_n_estimators(name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_gradient_boosting.py----------------------------------------
A:sklearn.ensemble.tests.test_gradient_boosting.(X_reg, y_reg)->make_regression(n_samples=100, n_features=4, n_informative=8, noise=10, random_state=7)
A:sklearn.ensemble.tests.test_gradient_boosting.y_reg->scale(y_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.rng->numpy.random.RandomState(42)
A:sklearn.ensemble.tests.test_gradient_boosting.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_gradient_boosting.perm->numpy.random.RandomState(42).permutation(iris.target.size)
A:sklearn.ensemble.tests.test_gradient_boosting.clf->GradientBoostingClassifier(**params)
A:sklearn.ensemble.tests.test_gradient_boosting.leaves->GradientBoostingClassifier(**params).apply(iris.data)
A:sklearn.ensemble.tests.test_gradient_boosting.(X, y)->sklearn.datasets.make_hastie_10_2(n_samples=100, random_state=20)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt_10_stumps->GradientBoostingClassifier(n_estimators=10, **common_params)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt_50_stumps->GradientBoostingClassifier(n_estimators=50, **common_params)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt_stumps->GradientBoostingClassifier(max_depth=1, **common_params)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt_10_nodes->GradientBoostingClassifier(max_leaf_nodes=10, **common_params)
A:sklearn.ensemble.tests.test_gradient_boosting.ones->numpy.ones(len(y_reg))
A:sklearn.ensemble.tests.test_gradient_boosting.reg->GradientBoostingRegressor(loss='huber', learning_rate=0.1, max_leaf_nodes=6, n_estimators=100, random_state=global_random_seed)
A:sklearn.ensemble.tests.test_gradient_boosting.y_pred->GBEstimator(min_impurity_decrease=0.1).predict(X_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.mse->mean_squared_error(y_test, clf.predict(X_test))
A:sklearn.ensemble.tests.test_gradient_boosting.sample_weight->numpy.random.RandomState(global_random_seed).rand(100)
A:sklearn.ensemble.tests.test_gradient_boosting.score->GradientBoosting(n_estimators=n_estimators, max_depth=1, subsample=0.5, warm_start=True, random_state=1).score(iris.data, iris.target)
A:sklearn.ensemble.tests.test_gradient_boosting.random_state->check_random_state(global_random_seed)
A:sklearn.ensemble.tests.test_gradient_boosting.gbdt->GradientBoosting()
A:sklearn.ensemble.tests.test_gradient_boosting.y_proba->GradientBoostingClassifier(**params).predict_proba(T)
A:sklearn.ensemble.tests.test_gradient_boosting.(x, y)->sklearn.datasets.make_hastie_10_2(n_samples=100, random_state=1)
A:sklearn.ensemble.tests.test_gradient_boosting.x_sparse_csc->csc_container(x)
A:sklearn.ensemble.tests.test_gradient_boosting.x_fortran->numpy.asfortranarray(x)
A:sklearn.ensemble.tests.test_gradient_boosting.gbrt->GradientBoostingRegressor(n_estimators=1, max_features=0.01 / X.shape[1])
A:sklearn.ensemble.tests.test_gradient_boosting.log_loss->GradientBoostingRegressor(n_estimators=1, max_features=0.01 / X.shape[1])._loss(y_test, gbrt.decision_function(X_test))
A:sklearn.ensemble.tests.test_gradient_boosting.california->fetch_california_housing_fxt()
A:sklearn.ensemble.tests.test_gradient_boosting.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=42)
A:sklearn.ensemble.tests.test_gradient_boosting.X->numpy.zeros((10, 10))
A:sklearn.ensemble.tests.test_gradient_boosting.estimator->GradientBoosting(n_estimators=n_estimators, max_depth=1, subsample=0.5, warm_start=True, random_state=1)
A:sklearn.ensemble.tests.test_gradient_boosting.staged_func->getattr(estimator, 'staged_' + func, None)
A:sklearn.ensemble.tests.test_gradient_boosting.staged_result->list(staged_func(X))
A:sklearn.ensemble.tests.test_gradient_boosting.serialized_clf->pickle.dumps(clf, protocol=pickle.HIGHEST_PROTOCOL)
A:sklearn.ensemble.tests.test_gradient_boosting.clf_quantile->GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=global_random_seed)
A:sklearn.ensemble.tests.test_gradient_boosting.y_quantile->GradientBoostingRegressor(n_estimators=100, loss='quantile', max_depth=4, alpha=0.5, random_state=global_random_seed).predict(X_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.clf_ae->GradientBoostingRegressor(n_estimators=100, loss='absolute_error', max_depth=4, random_state=global_random_seed)
A:sklearn.ensemble.tests.test_gradient_boosting.y_ae->GradientBoostingRegressor(n_estimators=100, loss='absolute_error', max_depth=4, random_state=global_random_seed).predict(X_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.symbol_y->tosequence(map(str, y))
A:sklearn.ensemble.tests.test_gradient_boosting.float_y->numpy.asarray(y, dtype=np.float32)
A:sklearn.ensemble.tests.test_gradient_boosting.y_->numpy.asfortranarray(y_)
A:sklearn.ensemble.tests.test_gradient_boosting.X_->numpy.ascontiguousarray(X)
A:sklearn.ensemble.tests.test_gradient_boosting.sys.stdout->StringIO()
A:sklearn.ensemble.tests.test_gradient_boosting.header->verbose_output.readline().rstrip()
A:sklearn.ensemble.tests.test_gradient_boosting.n_lines->sum((1 for l in verbose_output.readlines()))
A:sklearn.ensemble.tests.test_gradient_boosting.est->GBEstimator(min_impurity_decrease=0.1)
A:sklearn.ensemble.tests.test_gradient_boosting.est_ws->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.est_2->Cls(n_estimators=100, max_depth=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.est2->clone(est)
A:sklearn.ensemble.tests.test_gradient_boosting.est_dense->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.y_pred_dense->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True).predict(X)
A:sklearn.ensemble.tests.test_gradient_boosting.X_sparse->sparse_container(X)
A:sklearn.ensemble.tests.test_gradient_boosting.est_sparse->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.y_pred_sparse->Cls(n_estimators=100, max_depth=1, subsample=0.5, random_state=1, warm_start=True).predict(X)
A:sklearn.ensemble.tests.test_gradient_boosting.est_c->Cls(n_estimators=1, random_state=global_random_seed, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.est_fortran->Cls(n_estimators=1, random_state=global_random_seed, warm_start=True)
A:sklearn.ensemble.tests.test_gradient_boosting.X_fortran->numpy.asfortranarray(X)
A:sklearn.ensemble.tests.test_gradient_boosting.baseline->DummyRegressor(strategy='mean').fit(X_reg, y_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.mse_baseline->mean_squared_error(baseline.predict(X_reg), y_reg)
A:sklearn.ensemble.tests.test_gradient_boosting.mse_gbdt->mean_squared_error(y_reg, y_pred)
A:sklearn.ensemble.tests.test_gradient_boosting.y->numpy.arange(n_samples)
A:sklearn.ensemble.tests.test_gradient_boosting.gb->GradientBoostingClassifier(n_iter_no_change=5, random_state=0, validation_fraction=0.4)
A:sklearn.ensemble.tests.test_gradient_boosting.(y, X)->sklearn.datasets.make_multilabel_classification(random_state=0, n_samples=50, n_features=1, n_classes=20)
A:sklearn.ensemble.tests.test_gradient_boosting.dense->EstimatorClass(n_estimators=10, random_state=0, max_depth=2, min_impurity_decrease=1e-07).fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.sparse->EstimatorClass(n_estimators=10, random_state=0, max_depth=2, min_impurity_decrease=1e-07).fit(X_sparse, y)
A:sklearn.ensemble.tests.test_gradient_boosting.gb_large_tol->GradientBoostingEstimator(n_estimators=n_estimators, n_iter_no_change=10, learning_rate=0.1, max_depth=3, random_state=42, tol=0.1)
A:sklearn.ensemble.tests.test_gradient_boosting.gb_small_tol->GradientBoostingEstimator(n_estimators=n_estimators, n_iter_no_change=10, learning_rate=0.1, max_depth=3, random_state=42, tol=0.001)
A:sklearn.ensemble.tests.test_gradient_boosting.gbc->GradientBoostingClassifier(n_iter_no_change=5)
A:sklearn.ensemble.tests.test_gradient_boosting.gbr->GradientBoostingRegressor().fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.gbc2->clone(gbc).set_params(validation_fraction=0.3)
A:sklearn.ensemble.tests.test_gradient_boosting.gbc3->clone(gbc).set_params(n_iter_no_change=20)
A:sklearn.ensemble.tests.test_gradient_boosting.gbr2->clone(gbr).set_params(validation_fraction=0.3)
A:sklearn.ensemble.tests.test_gradient_boosting.gbr3->clone(gbr).set_params(n_iter_no_change=20)
A:sklearn.ensemble.tests.test_gradient_boosting.init_est->NoSampleWeightWrapper(init_estimator())
A:sklearn.ensemble.tests.test_gradient_boosting.init->NuSVR(gamma='auto', nu=invalid_nu)
A:sklearn.ensemble.tests.test_gradient_boosting.x1->numpy.minimum(y, n_samples / 2)
A:sklearn.ensemble.tests.test_gradient_boosting.x2->numpy.minimum(-y, -n_samples / 2)
A:sklearn.ensemble.tests.test_gradient_boosting.gbt_absolute_error->GradientBoostingRegressor(loss='absolute_error').fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.gbt_huber->GradientBoostingRegressor(loss='huber').fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.gbt_squared_error->GradientBoostingRegressor().fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.gbt_huber_predictions->GradientBoostingRegressor(loss='huber').fit(X, y).predict(X)
A:sklearn.ensemble.tests.test_gradient_boosting.gbt->GradientBoostingClassifier(loss='log_loss', n_estimators=100).fit(X, y)
A:sklearn.ensemble.tests.test_gradient_boosting.pred_result->numpy.array([[0.999999727, 1.11956255e-07, 8.04921671e-08, 8.04921668e-08], [1.11956254e-07, 0.999999727, 8.04921671e-08, 8.04921668e-08], [1.19417637e-07, 1.19417637e-07, 0.999999675, 8.60526098e-08], [1.19417637e-07, 1.19417637e-07, 8.60526088e-08, 0.999999675], [0.999999727, 1.11956255e-07, 8.04921671e-08, 8.04921668e-08], [1.11956254e-07, 0.999999727, 8.04921671e-08, 8.04921668e-08], [1.19417637e-07, 1.19417637e-07, 0.999999675, 8.60526098e-08], [1.19417637e-07, 1.19417637e-07, 8.60526088e-08, 0.999999675], [0.999999727, 1.11956255e-07, 8.04921671e-08, 8.04921668e-08], [1.11956254e-07, 0.999999727, 8.04921671e-08, 8.04921668e-08]])
A:sklearn.ensemble.tests.test_gradient_boosting.train_score->numpy.array([1.1330015e-06, 9.75183397e-07, 8.39348103e-07, 7.22433588e-07, 6.21804338e-07, 5.35191943e-07, 4.60643966e-07, 3.9647993e-07, 3.41253434e-07, 2.9371955e-07])
A:sklearn.ensemble.tests.test_gradient_boosting.sample_weights->numpy.tile([1, 10], n_samples // 2)
sklearn.ensemble.tests.test_gradient_boosting._make_multiclass()
sklearn.ensemble.tests.test_gradient_boosting.early_stopping_monitor(i,est,locals)
sklearn.ensemble.tests.test_gradient_boosting.test_binomial_error_exact_backward_compat()
sklearn.ensemble.tests.test_gradient_boosting.test_check_inputs_predict_stages(csc_container)
sklearn.ensemble.tests.test_gradient_boosting.test_classification_synthetic(loss,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_classification_toy(loss,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_complete_classification()
sklearn.ensemble.tests.test_gradient_boosting.test_complete_regression()
sklearn.ensemble.tests.test_gradient_boosting.test_degenerate_targets()
sklearn.ensemble.tests.test_gradient_boosting.test_early_stopping_n_classes()
sklearn.ensemble.tests.test_gradient_boosting.test_early_stopping_stratified()
sklearn.ensemble.tests.test_gradient_boosting.test_exponential_n_classes_gt_2()
sklearn.ensemble.tests.test_gradient_boosting.test_feature_importance_regression(fetch_california_housing_fxt,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_feature_importances(GradientBoosting,X,y)
sklearn.ensemble.tests.test_gradient_boosting.test_float_class_labels()
sklearn.ensemble.tests.test_gradient_boosting.test_gb_denominator_zero(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_gbr_degenerate_feature_importances()
sklearn.ensemble.tests.test_gradient_boosting.test_gradient_boosting_early_stopping(GradientBoostingEstimator)
sklearn.ensemble.tests.test_gradient_boosting.test_gradient_boosting_validation_fraction()
sklearn.ensemble.tests.test_gradient_boosting.test_gradient_boosting_with_init(gb,dataset_maker,init_estimator,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_gradient_boosting_with_init_pipeline()
sklearn.ensemble.tests.test_gradient_boosting.test_gradient_boosting_without_early_stopping()
sklearn.ensemble.tests.test_gradient_boosting.test_huber_exact_backward_compat()
sklearn.ensemble.tests.test_gradient_boosting.test_huber_vs_mean_and_median()
sklearn.ensemble.tests.test_gradient_boosting.test_iris(subsample,sample_weight,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_max_feature_regression(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_max_features()
sklearn.ensemble.tests.test_gradient_boosting.test_max_leaf_nodes_max_depth(GBEstimator)
sklearn.ensemble.tests.test_gradient_boosting.test_mem_layout()
sklearn.ensemble.tests.test_gradient_boosting.test_min_impurity_decrease(GBEstimator)
sklearn.ensemble.tests.test_gradient_boosting.test_monitor_early_stopping(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_more_verbose_output()
sklearn.ensemble.tests.test_gradient_boosting.test_multinomial_error_exact_backward_compat()
sklearn.ensemble.tests.test_gradient_boosting.test_non_uniform_weights_toy_edge_case_clf()
sklearn.ensemble.tests.test_gradient_boosting.test_non_uniform_weights_toy_edge_case_reg(loss,value)
sklearn.ensemble.tests.test_gradient_boosting.test_oob_attributes_error(GradientBoostingEstimator,oob_attribute)
sklearn.ensemble.tests.test_gradient_boosting.test_oob_improvement(GradientBoostingEstimator)
sklearn.ensemble.tests.test_gradient_boosting.test_oob_multilcass_iris()
sklearn.ensemble.tests.test_gradient_boosting.test_oob_scores(GradientBoostingEstimator)
sklearn.ensemble.tests.test_gradient_boosting.test_probability_log(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_quantile_loss(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_raise_if_init_has_no_predict_proba()
sklearn.ensemble.tests.test_gradient_boosting.test_regression_dataset(loss,subsample,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_regression_synthetic(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_safe_divide()
sklearn.ensemble.tests.test_gradient_boosting.test_serialization()
sklearn.ensemble.tests.test_gradient_boosting.test_shape_y()
sklearn.ensemble.tests.test_gradient_boosting.test_single_class_with_sample_weight()
sklearn.ensemble.tests.test_gradient_boosting.test_sparse_input(EstimatorClass,sparse_container)
sklearn.ensemble.tests.test_gradient_boosting.test_squared_error_exact_backward_compat()
sklearn.ensemble.tests.test_gradient_boosting.test_staged_functions_defensive(Estimator,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_staged_predict()
sklearn.ensemble.tests.test_gradient_boosting.test_staged_predict_proba()
sklearn.ensemble.tests.test_gradient_boosting.test_symbol_labels()
sklearn.ensemble.tests.test_gradient_boosting.test_verbose_output()
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start(Cls,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_clear(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_equal_n_estimators(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_fortran(Cls,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_max_depth(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_n_estimators(Cls,global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_oob(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_oob_switch(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_smaller_n_estimators(Cls)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_sparse(Cls,sparse_container)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_state_oob_scores(GradientBoosting)
sklearn.ensemble.tests.test_gradient_boosting.test_warm_start_wo_nestimators_change()
sklearn.ensemble.tests.test_gradient_boosting.test_zero_estimator_clf(global_random_seed)
sklearn.ensemble.tests.test_gradient_boosting.test_zero_estimator_reg(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_voting.py----------------------------------------
A:sklearn.ensemble.tests.test_voting.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_voting.X_scaled->StandardScaler().fit_transform(X)
A:sklearn.ensemble.tests.test_voting.(X_r, y_r)->sklearn.datasets.load_diabetes(return_X_y=True)
A:sklearn.ensemble.tests.test_voting.ensemble->VotingClassifier(**params)
A:sklearn.ensemble.tests.test_voting.eclf->VotingClassifier(estimators=[('mock', clf)], voting='soft')
A:sklearn.ensemble.tests.test_voting.ereg->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=[1, 2, 10])
A:sklearn.ensemble.tests.test_voting.clf1->LogisticRegression(random_state=global_random_seed)
A:sklearn.ensemble.tests.test_voting.clf2->RandomForestClassifier(n_estimators=10, random_state=global_random_seed)
A:sklearn.ensemble.tests.test_voting.clf3->GaussianNB()
A:sklearn.ensemble.tests.test_voting.scores->cross_val_score(eclf, X_scaled, y, scoring='accuracy')
A:sklearn.ensemble.tests.test_voting.reg1->DummyRegressor(strategy='mean')
A:sklearn.ensemble.tests.test_voting.reg2->DummyRegressor(strategy='median')
A:sklearn.ensemble.tests.test_voting.reg3->DummyRegressor(strategy='quantile', quantile=0.2)
A:sklearn.ensemble.tests.test_voting.(X_r_train, X_r_test, y_r_train, y_r_test)->train_test_split(X_r, y_r, test_size=0.25)
A:sklearn.ensemble.tests.test_voting.reg1_pred->DummyRegressor(strategy='mean').fit(X_r_train, y_r_train).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.reg2_pred->DummyRegressor(strategy='median').fit(X_r_train, y_r_train).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.reg3_pred->DummyRegressor(strategy='quantile', quantile=0.2).fit(X_r_train, y_r_train).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.ereg_pred->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=[1, 2, 10]).fit(X_r_train, y_r_train).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.avg->numpy.average(np.asarray([reg1_pred, reg2_pred, reg3_pred]), axis=0, weights=[1, 2, 10])
A:sklearn.ensemble.tests.test_voting.ereg_weights_none->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=None)
A:sklearn.ensemble.tests.test_voting.ereg_weights_equal->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=[1, 1, 1])
A:sklearn.ensemble.tests.test_voting.ereg_none_pred->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=None).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.ereg_equal_pred->VotingRegressor([('mean', reg1), ('median', reg2), ('quantile', reg3)], weights=[1, 1, 1]).predict(X_r_test)
A:sklearn.ensemble.tests.test_voting.X->numpy.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])
A:sklearn.ensemble.tests.test_voting.y->numpy.array([1, 1, 2, 2])
A:sklearn.ensemble.tests.test_voting.clf1_res->numpy.array([[0.59790391, 0.40209609], [0.57622162, 0.42377838], [0.50728456, 0.49271544], [0.40241774, 0.59758226]])
A:sklearn.ensemble.tests.test_voting.clf2_res->numpy.array([[0.8, 0.2], [0.8, 0.2], [0.2, 0.8], [0.3, 0.7]])
A:sklearn.ensemble.tests.test_voting.clf3_res->numpy.array([[0.9985082, 0.0014918], [0.99845843, 0.00154157], [0.0, 1.0], [0.0, 1.0]])
A:sklearn.ensemble.tests.test_voting.eclf_res->VotingClassifier(estimators=[('mock', clf)], voting='soft').fit(X, y).predict_proba(X)
A:sklearn.ensemble.tests.test_voting.(X, y)->make_multilabel_classification(n_classes=2, n_labels=1, allow_unlabeled=False, random_state=123)
A:sklearn.ensemble.tests.test_voting.clf->MockClassifier()
A:sklearn.ensemble.tests.test_voting.grid->GridSearchCV(estimator=eclf, param_grid=params, cv=2)
A:sklearn.ensemble.tests.test_voting.eclf1->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft').fit(X, y)
A:sklearn.ensemble.tests.test_voting.eclf2->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', flatten_transform=True).fit(X, y)
A:sklearn.ensemble.tests.test_voting.sample_weight->numpy.random.RandomState(global_random_seed).uniform(size=(len(y),))
A:sklearn.ensemble.tests.test_voting.eclf3->VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', flatten_transform=False).fit(X, y)
A:sklearn.ensemble.tests.test_voting.clf4->KNeighborsClassifier()
A:sklearn.ensemble.tests.test_voting.X1->numpy.array([[1], [2]])
A:sklearn.ensemble.tests.test_voting.y1->numpy.array([1, 2])
A:sklearn.ensemble.tests.test_voting.voter->clone(voter)
A:sklearn.ensemble.tests.test_voting.y_pred->clone(voter).predict(X_scaled)
A:sklearn.ensemble.tests.test_voting.voting->VotingClassifier(estimators=[('lr', LogisticRegression(random_state=0)), ('tree', DecisionTreeClassifier(random_state=0))], voting='soft', flatten_transform=False)
A:sklearn.ensemble.tests.test_voting.names_out->VotingClassifier(estimators=[('lr', LogisticRegression(random_state=0)), ('tree', DecisionTreeClassifier(random_state=0))], voting='soft', flatten_transform=False).get_feature_names_out()
A:sklearn.ensemble.tests.test_voting.X_trans->VotingClassifier(estimators=[('lr', LogisticRegression(random_state=0)), ('tree', DecisionTreeClassifier(random_state=0))], voting='soft', flatten_transform=False).transform(X)
sklearn.ensemble.tests.test_voting.test_estimator_weights_format(global_random_seed)
sklearn.ensemble.tests.test_voting.test_get_features_names_out_classifier(kwargs,expected_names)
sklearn.ensemble.tests.test_voting.test_get_features_names_out_classifier_error()
sklearn.ensemble.tests.test_voting.test_get_features_names_out_regressor()
sklearn.ensemble.tests.test_voting.test_gridsearch()
sklearn.ensemble.tests.test_voting.test_majority_label_iris(global_random_seed)
sklearn.ensemble.tests.test_voting.test_multilabel()
sklearn.ensemble.tests.test_voting.test_n_features_in(est)
sklearn.ensemble.tests.test_voting.test_none_estimator_with_weights(X,y,voter)
sklearn.ensemble.tests.test_voting.test_notfitted()
sklearn.ensemble.tests.test_voting.test_parallel_fit(global_random_seed)
sklearn.ensemble.tests.test_voting.test_predict_on_toy_problem(global_random_seed)
sklearn.ensemble.tests.test_voting.test_predict_proba_on_toy_problem()
sklearn.ensemble.tests.test_voting.test_predictproba_hardvoting()
sklearn.ensemble.tests.test_voting.test_sample_weight(global_random_seed)
sklearn.ensemble.tests.test_voting.test_sample_weight_kwargs()
sklearn.ensemble.tests.test_voting.test_set_estimator_drop()
sklearn.ensemble.tests.test_voting.test_tie_situation()
sklearn.ensemble.tests.test_voting.test_transform(global_random_seed)
sklearn.ensemble.tests.test_voting.test_voting_classifier_estimator_init(params,err_msg)
sklearn.ensemble.tests.test_voting.test_voting_classifier_set_params(global_random_seed)
sklearn.ensemble.tests.test_voting.test_voting_verbose(estimator,capsys)
sklearn.ensemble.tests.test_voting.test_weights_iris(global_random_seed)
sklearn.ensemble.tests.test_voting.test_weights_regressor()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_iforest.py----------------------------------------
A:sklearn.ensemble.tests.test_iforest.iris->load_iris()
A:sklearn.ensemble.tests.test_iforest.diabetes->load_diabetes()
A:sklearn.ensemble.tests.test_iforest.X_train->numpy.array([[0, 1], [1, 2]])
A:sklearn.ensemble.tests.test_iforest.X_test->numpy.vstack((X[1000:], X_outliers))
A:sklearn.ensemble.tests.test_iforest.grid->ParameterGrid({'max_samples': [0.5, 1.0], 'bootstrap': [True, False]})
A:sklearn.ensemble.tests.test_iforest.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_iforest.(X_train, X_test)->train_test_split(diabetes.data, random_state=rng)
A:sklearn.ensemble.tests.test_iforest.X_train_sparse->sparse_container(X_train)
A:sklearn.ensemble.tests.test_iforest.X_test_sparse->sparse_container(X_test)
A:sklearn.ensemble.tests.test_iforest.sparse_classifier->IsolationForest(n_estimators=10, random_state=global_random_seed, **params).fit(X_train_sparse)
A:sklearn.ensemble.tests.test_iforest.sparse_results->IsolationForest(n_estimators=10, random_state=global_random_seed, **params).fit(X_train_sparse).predict(X_test_sparse)
A:sklearn.ensemble.tests.test_iforest.dense_classifier->IsolationForest(n_estimators=10, random_state=global_random_seed, **params).fit(X_train)
A:sklearn.ensemble.tests.test_iforest.dense_results->IsolationForest(n_estimators=10, random_state=global_random_seed, **params).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.clf->IsolationForest(n_estimators=10, max_samples=20, random_state=rng, warm_start=True)
A:sklearn.ensemble.tests.test_iforest.ensemble->IsolationForest(n_jobs=1, random_state=global_random_seed).fit(X_train)
A:sklearn.ensemble.tests.test_iforest.y1->IsolationForest(n_jobs=1, random_state=global_random_seed).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.y2->IsolationForest(n_jobs=1, random_state=global_random_seed).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.y3->IsolationForest(n_jobs=1, random_state=global_random_seed).fit(X_train).predict(X_test)
A:sklearn.ensemble.tests.test_iforest.X->sparse_container(X)
A:sklearn.ensemble.tests.test_iforest.X_outliers->numpy.random.RandomState(0).uniform(low=-1, high=1, size=(200, 2))
A:sklearn.ensemble.tests.test_iforest.y_test->numpy.array([0] * 200 + [1] * 200)
A:sklearn.ensemble.tests.test_iforest.pred->IsolationForest(n_estimators=10, max_samples=20, random_state=rng, warm_start=True).predict(X)
A:sklearn.ensemble.tests.test_iforest.(X_train, X_test, y_train, y_test)->train_test_split(diabetes.data[:50], diabetes.target[:50], random_state=rng)
A:sklearn.ensemble.tests.test_iforest.avg_path_length->_average_path_length(np.arange(5))
A:sklearn.ensemble.tests.test_iforest.clf1->IsolationForest(contamination=0.1).fit(X_train)
A:sklearn.ensemble.tests.test_iforest.clf2->IsolationForest().fit(X_train)
A:sklearn.ensemble.tests.test_iforest.iforest->IsolationForest(n_estimators=5, contamination=contamination, random_state=0).fit(X)
A:sklearn.ensemble.tests.test_iforest.(X, _)->make_classification(n_samples=50, n_features=4, random_state=0)
A:sklearn.ensemble.tests.test_iforest.pd->pytest.importorskip('pandas')
A:sklearn.ensemble.tests.test_iforest.model->IsolationForest(random_state=0, contamination=0.05)
A:sklearn.ensemble.tests.test_iforest.X_decision->IsolationForest(n_estimators=5, contamination=contamination, random_state=0).fit(X).decision_function(X)
sklearn.ensemble.tests.test_iforest.test_iforest(global_random_seed)
sklearn.ensemble.tests.test_iforest.test_iforest_average_path_length()
sklearn.ensemble.tests.test_iforest.test_iforest_chunks_works1(mocked_get_chunk,contamination,n_predict_calls,global_random_seed)
sklearn.ensemble.tests.test_iforest.test_iforest_chunks_works2(mocked_get_chunk,contamination,n_predict_calls,global_random_seed)
sklearn.ensemble.tests.test_iforest.test_iforest_error()
sklearn.ensemble.tests.test_iforest.test_iforest_parallel_regression(global_random_seed)
sklearn.ensemble.tests.test_iforest.test_iforest_performance(global_random_seed)
sklearn.ensemble.tests.test_iforest.test_iforest_preserve_feature_names()
sklearn.ensemble.tests.test_iforest.test_iforest_sparse(global_random_seed,sparse_container)
sklearn.ensemble.tests.test_iforest.test_iforest_sparse_input_float_contamination(sparse_container)
sklearn.ensemble.tests.test_iforest.test_iforest_subsampled_features()
sklearn.ensemble.tests.test_iforest.test_iforest_warm_start()
sklearn.ensemble.tests.test_iforest.test_iforest_with_n_jobs_does_not_segfault(csc_container)
sklearn.ensemble.tests.test_iforest.test_iforest_with_uniform_data()
sklearn.ensemble.tests.test_iforest.test_iforest_works(contamination,global_random_seed)
sklearn.ensemble.tests.test_iforest.test_max_samples_attribute()
sklearn.ensemble.tests.test_iforest.test_max_samples_consistency()
sklearn.ensemble.tests.test_iforest.test_recalculate_max_depth()
sklearn.ensemble.tests.test_iforest.test_score_samples()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_stacking.py----------------------------------------
A:sklearn.ensemble.tests.test_stacking.diabetes->load_diabetes()
A:sklearn.ensemble.tests.test_stacking.iris->load_iris()
A:sklearn.ensemble.tests.test_stacking.(X_multilabel, y_multilabel)->make_multilabel_classification(n_classes=3, random_state=42)
A:sklearn.ensemble.tests.test_stacking.(X_binary, y_binary)->make_classification(n_classes=2, random_state=42)
A:sklearn.ensemble.tests.test_stacking.(X_train, X_test, y_train, y_test)->train_test_split(scale(X_iris), y_iris, stratify=y_iris, random_state=42)
A:sklearn.ensemble.tests.test_stacking.clf->StackingClassifier(estimators=[('ridge', Ridge())])
A:sklearn.ensemble.tests.test_stacking.X_trans->StackingClassifier(estimators=[('ridge', Ridge())]).transform(X_test)
A:sklearn.ensemble.tests.test_stacking.(X, y)->make_dataset(random_state=0, n_samples=100)
A:sklearn.ensemble.tests.test_stacking.(X_train, X_test, y_train, _)->train_test_split(sparse_container(scale(X_iris)), y_iris, random_state=42)
A:sklearn.ensemble.tests.test_stacking.rf->RandomForestClassifier(n_estimators=10, random_state=42)
A:sklearn.ensemble.tests.test_stacking.clf_drop->StackingClassifier(estimators=estimators, final_estimator=rf, cv=5)
A:sklearn.ensemble.tests.test_stacking.reg->StackingRegressor(**params, cv=3)
A:sklearn.ensemble.tests.test_stacking.reg_drop->StackingRegressor(estimators=estimators, final_estimator=rf, cv=5)
A:sklearn.ensemble.tests.test_stacking.result->StackingRegressor(**params, cv=3).predict(X_test, **predict_params)
A:sklearn.ensemble.tests.test_stacking.X_meta->StackingClassifier(estimators=[('ridge', Ridge())]).transform(X_)
A:sklearn.ensemble.tests.test_stacking.self.reg->DummyRegressor()
A:sklearn.ensemble.tests.test_stacking.self.clf->DummyClassifier(strategy='stratified')
A:sklearn.ensemble.tests.test_stacking.estimator_full->clone(estimator)
A:sklearn.ensemble.tests.test_stacking.estimator_drop->clone(estimator)
A:sklearn.ensemble.tests.test_stacking.total_sample_weight->numpy.array([0.1] * n_half_samples + [0.9] * (len(y) - n_half_samples))
A:sklearn.ensemble.tests.test_stacking.(X_train, X_test, y_train, _, sample_weight_train, _)->train_test_split(X, y, total_sample_weight, random_state=42)
A:sklearn.ensemble.tests.test_stacking.y_pred_no_weight->StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier(), stack_method='decision_function').fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_stacking.y_pred_unit_weight->StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier(), stack_method='decision_function').fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_stacking.y_pred_biased->StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier(), stack_method='decision_function').fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_stacking.stacker->StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier(), stack_method='decision_function').fit(X_train, y_train)
A:sklearn.ensemble.tests.test_stacking.stacker_cv_3->clone(stacker)
A:sklearn.ensemble.tests.test_stacking.stacker_cv_5->clone(stacker)
A:sklearn.ensemble.tests.test_stacking.(X_train1, X_train2, y_train1, y_train2)->train_test_split(X, y, random_state=42, test_size=0.5)
A:sklearn.ensemble.tests.test_stacking.estimator.fit->Mock(name='fit')
A:sklearn.ensemble.tests.test_stacking.stack_func->getattr(estimator, stack_method)
A:sklearn.ensemble.tests.test_stacking.predict_method_mocked->Mock(side_effect=stack_func)
A:sklearn.ensemble.tests.test_stacking.stack_func_mock->getattr(estimator, stack_method)
A:sklearn.ensemble.tests.test_stacking.y_pred->StackingClassifier(estimators=[('ridge', Ridge())]).predict(X_test)
A:sklearn.ensemble.tests.test_stacking.y_train_before_fit->y_train.copy()
A:sklearn.ensemble.tests.test_stacking.final_estimator->KNeighborsClassifier()
A:sklearn.ensemble.tests.test_stacking.expected_names->numpy.concatenate((expected_names, feature_names))
A:sklearn.ensemble.tests.test_stacking.names_out->StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier(), stack_method='decision_function').fit(X_train, y_train).get_feature_names_out(feature_names)
sklearn.ensemble.tests.test_stacking.NoWeightClassifier(ClassifierMixin,BaseEstimator)
sklearn.ensemble.tests.test_stacking.NoWeightClassifier.fit(self,X,y)
sklearn.ensemble.tests.test_stacking.NoWeightRegressor(RegressorMixin,BaseEstimator)
sklearn.ensemble.tests.test_stacking.NoWeightRegressor.fit(self,X,y)
sklearn.ensemble.tests.test_stacking.NoWeightRegressor.predict(self,X)
sklearn.ensemble.tests.test_stacking.test_get_feature_names_out(stacker,feature_names,X,y,expected_names,passthrough)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_base_regressor()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_drop_binary_prob()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_drop_column_binary_classification()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_drop_estimator()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_error(y,params,type_err,msg_err)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_iris(cv,final_estimator,passthrough)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_multilabel_auto_predict(stack_method,passthrough)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_multilabel_decision_function()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_multilabel_predict_proba(estimator)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_sample_weight_fit_param()
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_sparse_passthrough(sparse_container)
sklearn.ensemble.tests.test_stacking.test_stacking_classifier_stratify_default()
sklearn.ensemble.tests.test_stacking.test_stacking_cv_influence(stacker,X,y)
sklearn.ensemble.tests.test_stacking.test_stacking_prefit(Stacker,Estimator,stack_method,final_estimator,X,y)
sklearn.ensemble.tests.test_stacking.test_stacking_prefit_error(stacker,X,y)
sklearn.ensemble.tests.test_stacking.test_stacking_randomness(estimator,X,y)
sklearn.ensemble.tests.test_stacking.test_stacking_regressor_diabetes(cv,final_estimator,predict_params,passthrough)
sklearn.ensemble.tests.test_stacking.test_stacking_regressor_drop_estimator()
sklearn.ensemble.tests.test_stacking.test_stacking_regressor_error(y,params,type_err,msg_err)
sklearn.ensemble.tests.test_stacking.test_stacking_regressor_sparse_passthrough(sparse_container)
sklearn.ensemble.tests.test_stacking.test_stacking_with_sample_weight(stacker,X,y)
sklearn.ensemble.tests.test_stacking.test_stacking_without_n_features_in(make_dataset,Stacking,Estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_common.py----------------------------------------
A:sklearn.ensemble.tests.test_common.(X, y)->make_regression(n_samples=10)
A:sklearn.ensemble.tests.test_common.(X_r, y_r)->load_diabetes(return_X_y=True)
A:sklearn.ensemble.tests.test_common.estimator_new_params->clone(estimator)
A:sklearn.ensemble.tests.test_common.estimator_dropped->clone(estimator)
A:sklearn.ensemble.tests.test_common.ensemble->Ensemble(estimators=[('pipe1', pipe), ('pipe2', pipe)])
A:sklearn.ensemble.tests.test_common.err_msg->'should be a {}'.format(ensemble_type)
A:sklearn.ensemble.tests.test_common.X->X.copy().copy()
A:sklearn.ensemble.tests.test_common.mask->numpy.random.choice([1, 0], X.shape, p=[0.1, 0.9]).astype(bool)
A:sklearn.ensemble.tests.test_common.pipe->make_pipeline(SimpleImputer(), Estimator())
sklearn.ensemble.tests.test_common.test_ensemble_heterogeneous_estimators_all_dropped(X,y,estimator)
sklearn.ensemble.tests.test_common.test_ensemble_heterogeneous_estimators_behavior(X,y,estimator)
sklearn.ensemble.tests.test_common.test_ensemble_heterogeneous_estimators_name_validation(X,y,Ensemble)
sklearn.ensemble.tests.test_common.test_ensemble_heterogeneous_estimators_type(Ensemble)
sklearn.ensemble.tests.test_common.test_heterogeneous_ensemble_support_missing_values(Ensemble,Estimator,X,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_bagging.py----------------------------------------
A:sklearn.ensemble.tests.test_bagging.rng->numpy.random.RandomState(0)
A:sklearn.ensemble.tests.test_bagging.iris->load_iris()
A:sklearn.ensemble.tests.test_bagging.perm->numpy.random.RandomState(0).permutation(diabetes.target.size)
A:sklearn.ensemble.tests.test_bagging.diabetes->load_diabetes()
A:sklearn.ensemble.tests.test_bagging.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=43)
A:sklearn.ensemble.tests.test_bagging.grid->ParameterGrid({'max_samples': [0.5, 1.0], 'max_features': [0.5, 1.0], 'bootstrap': [True, False], 'bootstrap_features': [True, False]})
A:sklearn.ensemble.tests.test_bagging.self.data_type_->type(X)
A:sklearn.ensemble.tests.test_bagging.X_train_sparse->sparse_container(X_train)
A:sklearn.ensemble.tests.test_bagging.X_test_sparse->sparse_container(X_test)
A:sklearn.ensemble.tests.test_bagging.sparse_classifier->BaggingRegressor(estimator=CustomSVR(), random_state=1, **params).fit(X_train_sparse, y_train)
A:sklearn.ensemble.tests.test_bagging.sparse_results->BaggingRegressor(estimator=CustomSVR(), random_state=1, **params).fit(X_train_sparse, y_train).predict(X_test_sparse)
A:sklearn.ensemble.tests.test_bagging.dense_classifier->BaggingClassifier(estimator=CustomSVC(kernel='linear', decision_function_shape='ovr'), random_state=1, **params).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.dense_results->BaggingRegressor(estimator=CustomSVR(), random_state=1, **params).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.sparse_type->type(X_train_sparse)
A:sklearn.ensemble.tests.test_bagging.self.training_hash_->joblib.hash(X)
A:sklearn.ensemble.tests.test_bagging.estimator->BaggingClassifier(DummyZeroEstimator())
A:sklearn.ensemble.tests.test_bagging.ensemble->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.clf->BaggingRegressor(estimator=MyEstimator(), n_estimators=1, random_state=0)
A:sklearn.ensemble.tests.test_bagging.test_score->BaggingRegressor(estimator=MyEstimator(), n_estimators=1, random_state=0).score(X_test, y_test)
A:sklearn.ensemble.tests.test_bagging.regr->BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=1, bootstrap=True, oob_score=True, random_state=rng)
A:sklearn.ensemble.tests.test_bagging.clf1->BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=1, bootstrap=False, bootstrap_features=False, random_state=rng).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.clf2->KNeighborsRegressor().fit(X_train, y_train)
A:sklearn.ensemble.tests.test_bagging.base->DecisionTreeClassifier()
A:sklearn.ensemble.tests.test_bagging.y1->BaggingClassifier(n_estimators=5, warm_start=True, random_state=3141).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.y2->BaggingRegressor(estimator=MyEstimator(), n_estimators=1, random_state=0).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.y3->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.decisions1->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.decisions2->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.decisions3->BaggingRegressor(SVR(), n_jobs=3, random_state=0).fit(X_train, y_train).decision_function(X_test)
A:sklearn.ensemble.tests.test_bagging.self.classes_->numpy.unique(y)
A:sklearn.ensemble.tests.test_bagging.(X, y)->make_hastie_10_2(n_samples=2 * max_samples, random_state=1)
A:sklearn.ensemble.tests.test_bagging.clf_ws->BaggingClassifier(n_estimators=5, warm_start=True, random_state=3141)
A:sklearn.ensemble.tests.test_bagging.clf_no_ws->BaggingClassifier(n_estimators=10, random_state=random_state, warm_start=False)
A:sklearn.ensemble.tests.test_bagging.y_pred->BaggingRegressor(estimator=MyEstimator(), n_estimators=1, random_state=0).predict(X_test)
A:sklearn.ensemble.tests.test_bagging.bagging->BaggingClassifier(LogisticRegression(), max_features=0.3, random_state=1)
A:sklearn.ensemble.tests.test_bagging.base_pipeline->make_pipeline(SparseRandomProjection(n_components=2), LogisticRegression())
A:sklearn.ensemble.tests.test_bagging.pipeline_estimator_coef->BaggingRegressor(estimator=MyEstimator(), n_estimators=1, random_state=0).estimators_[0].steps[-1][1].coef_.copy()
A:sklearn.ensemble.tests.test_bagging.X->numpy.random.RandomState(0).randn(13, 4)
A:sklearn.ensemble.tests.test_bagging.regressor->DecisionTreeRegressor()
A:sklearn.ensemble.tests.test_bagging.pipeline->make_pipeline(classifier)
A:sklearn.ensemble.tests.test_bagging.bagging_regressor->BaggingRegressor(pipeline)
A:sklearn.ensemble.tests.test_bagging.y_hat->BaggingClassifier(pipeline).predict(X)
A:sklearn.ensemble.tests.test_bagging.y->numpy.arange(13)
A:sklearn.ensemble.tests.test_bagging.classifier->DecisionTreeClassifier()
A:sklearn.ensemble.tests.test_bagging.bagging_classifier->BaggingClassifier(pipeline)
sklearn.ensemble.tests.test_bagging.DummySizeEstimator(BaseEstimator)
sklearn.ensemble.tests.test_bagging.DummySizeEstimator.fit(self,X,y)
sklearn.ensemble.tests.test_bagging.DummySizeEstimator.predict(self,X)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator(BaseEstimator)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator.fit(self,X,y)
sklearn.ensemble.tests.test_bagging.DummyZeroEstimator.predict(self,X)
sklearn.ensemble.tests.test_bagging.replace(X)
sklearn.ensemble.tests.test_bagging.test_bagging_allow_nan_tag(bagging,expected_allow_nan)
sklearn.ensemble.tests.test_bagging.test_bagging_classifier_with_missing_inputs()
sklearn.ensemble.tests.test_bagging.test_bagging_get_estimators_indices()
sklearn.ensemble.tests.test_bagging.test_bagging_regressor_with_missing_inputs()
sklearn.ensemble.tests.test_bagging.test_bagging_sample_weight_unsupported_but_passed()
sklearn.ensemble.tests.test_bagging.test_bagging_small_max_features()
sklearn.ensemble.tests.test_bagging.test_bagging_with_pipeline()
sklearn.ensemble.tests.test_bagging.test_bootstrap_features()
sklearn.ensemble.tests.test_bagging.test_bootstrap_samples()
sklearn.ensemble.tests.test_bagging.test_classification()
sklearn.ensemble.tests.test_bagging.test_error()
sklearn.ensemble.tests.test_bagging.test_estimator()
sklearn.ensemble.tests.test_bagging.test_estimators_samples()
sklearn.ensemble.tests.test_bagging.test_estimators_samples_deterministic()
sklearn.ensemble.tests.test_bagging.test_gridsearch()
sklearn.ensemble.tests.test_bagging.test_max_samples_consistency()
sklearn.ensemble.tests.test_bagging.test_oob_score_classification()
sklearn.ensemble.tests.test_bagging.test_oob_score_consistency()
sklearn.ensemble.tests.test_bagging.test_oob_score_regression()
sklearn.ensemble.tests.test_bagging.test_oob_score_removed_on_warm_start()
sklearn.ensemble.tests.test_bagging.test_parallel_classification()
sklearn.ensemble.tests.test_bagging.test_parallel_regression()
sklearn.ensemble.tests.test_bagging.test_probability()
sklearn.ensemble.tests.test_bagging.test_regression()
sklearn.ensemble.tests.test_bagging.test_set_oob_score_label_encoding()
sklearn.ensemble.tests.test_bagging.test_single_estimator()
sklearn.ensemble.tests.test_bagging.test_sparse_classification(sparse_container,params,method)
sklearn.ensemble.tests.test_bagging.test_sparse_regression(sparse_container)
sklearn.ensemble.tests.test_bagging.test_warm_start(random_state=42)
sklearn.ensemble.tests.test_bagging.test_warm_start_equal_n_estimators()
sklearn.ensemble.tests.test_bagging.test_warm_start_equivalence()
sklearn.ensemble.tests.test_bagging.test_warm_start_smaller_n_estimators()
sklearn.ensemble.tests.test_bagging.test_warm_start_with_oob_score_fails()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/ensemble/tests/test_weight_boosting.py----------------------------------------
A:sklearn.ensemble.tests.test_weight_boosting.rng->numpy.random.RandomState(42)
A:sklearn.ensemble.tests.test_weight_boosting.iris->sklearn.datasets.load_iris()
A:sklearn.ensemble.tests.test_weight_boosting.perm->numpy.random.RandomState(42).permutation(iris.target.size)
A:sklearn.ensemble.tests.test_weight_boosting.(iris.data, iris.target)->shuffle(iris.data, iris.target, random_state=rng)
A:sklearn.ensemble.tests.test_weight_boosting.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.ensemble.tests.test_weight_boosting.(diabetes.data, diabetes.target)->shuffle(diabetes.data, diabetes.target, random_state=rng)
A:sklearn.ensemble.tests.test_weight_boosting.probs->numpy.array([[1, 1e-06, 0], [0.19, 0.6, 0.2], [-999, 0.51, 0.5], [1e-06, 1, 1e-09]])
A:sklearn.ensemble.tests.test_weight_boosting.mock->MockEstimator()
A:sklearn.ensemble.tests.test_weight_boosting.samme_proba->_samme_proba(mock, 3, np.ones_like(probs))
A:sklearn.ensemble.tests.test_weight_boosting.y_t->numpy.ones(len(X))
A:sklearn.ensemble.tests.test_weight_boosting.clf->AdaBoostClassifier(n_estimators=1, random_state=global_random_seed, algorithm=algorithm).fit(X, y)
A:sklearn.ensemble.tests.test_weight_boosting.classes->numpy.unique(iris.target)
A:sklearn.ensemble.tests.test_weight_boosting.proba->AdaBoostClassifier(n_estimators=1, random_state=global_random_seed, algorithm=algorithm).fit(X, y).predict_proba(iris.data)
A:sklearn.ensemble.tests.test_weight_boosting.score->AdaBoostRegressor(random_state=0).score(diabetes.data, diabetes.target)
A:sklearn.ensemble.tests.test_weight_boosting.reg->AdaBoostRegressor(loss=loss, random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.iris_weights->numpy.random.RandomState(42).randint(10, size=iris.target.shape)
A:sklearn.ensemble.tests.test_weight_boosting.diabetes_weights->numpy.random.RandomState(42).randint(10, size=diabetes.target.shape)
A:sklearn.ensemble.tests.test_weight_boosting.predictions->AdaBoostClassifier(n_estimators=1, random_state=global_random_seed, algorithm=algorithm).fit(X, y).predict(diabetes.data)
A:sklearn.ensemble.tests.test_weight_boosting.boost->AdaBoostRegressor(DummyRegressor())
A:sklearn.ensemble.tests.test_weight_boosting.obj->AdaBoostRegressor(random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.s->pickle.dumps(obj)
A:sklearn.ensemble.tests.test_weight_boosting.obj2->pickle.loads(s)
A:sklearn.ensemble.tests.test_weight_boosting.score2->pickle.loads(s).score(diabetes.data, diabetes.target)
A:sklearn.ensemble.tests.test_weight_boosting.(X, y)->sklearn.datasets.make_classification(n_classes=n_classes, n_clusters_per_class=1, random_state=global_random_seed)
A:sklearn.ensemble.tests.test_weight_boosting.msg->re.escape('sample_weight.shape == (1,), expected (6,)')
A:sklearn.ensemble.tests.test_weight_boosting.self.data_type_->type(X)
A:sklearn.ensemble.tests.test_weight_boosting.y->numpy.random.RandomState(42).choice([0, 1], size=1000)
A:sklearn.ensemble.tests.test_weight_boosting.(X_train, X_test, y_train, y_test)->train_test_split(*datasets.load_digits(return_X_y=True), random_state=42)
A:sklearn.ensemble.tests.test_weight_boosting.X_train_sparse->sparse_container(X_train)
A:sklearn.ensemble.tests.test_weight_boosting.X_test_sparse->sparse_container(X_test)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_classifier->AdaBoostClassifier(estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train_sparse, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.dense_classifier->AdaBoostClassifier(estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_clf_results->AdaBoostClassifier(estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train_sparse, y_train).staged_score(X_test_sparse, y_test)
A:sklearn.ensemble.tests.test_weight_boosting.dense_clf_results->AdaBoostClassifier(estimator=CustomSVC(probability=True), random_state=1, algorithm='SAMME').fit(X_train, y_train).staged_score(X_test, y_test)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_regressor->AdaBoostRegressor(estimator=CustomSVR(), random_state=1).fit(X_train_sparse, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.dense_regressor->AdaBoostRegressor(estimator=CustomSVR(), random_state=1).fit(X_train, y_train)
A:sklearn.ensemble.tests.test_weight_boosting.sparse_regr_results->AdaBoostRegressor(estimator=CustomSVR(), random_state=1).fit(X_train_sparse, y_train).staged_predict(X_test_sparse)
A:sklearn.ensemble.tests.test_weight_boosting.dense_regr_results->AdaBoostRegressor(estimator=CustomSVR(), random_state=1).fit(X_train, y_train).staged_predict(X_test)
A:sklearn.ensemble.tests.test_weight_boosting.X->numpy.random.RandomState(42).normal(size=(1000, 10))
A:sklearn.ensemble.tests.test_weight_boosting.yc->numpy.random.RandomState(42).choice([0, 1], 51)
A:sklearn.ensemble.tests.test_weight_boosting.yr->numpy.random.RandomState(42).randn(51)
A:sklearn.ensemble.tests.test_weight_boosting.estimator->NoSampleWeightWrapper(DummyClassifier())
A:sklearn.ensemble.tests.test_weight_boosting.err_msg->"{} doesn't support sample_weight".format(estimator.__class__.__name__)
A:sklearn.ensemble.tests.test_weight_boosting.regr_no_outlier->AdaBoostRegressor(estimator=LinearRegression(), n_estimators=1, random_state=0)
A:sklearn.ensemble.tests.test_weight_boosting.regr_with_weight->clone(regr_no_outlier)
A:sklearn.ensemble.tests.test_weight_boosting.regr_with_outlier->clone(regr_no_outlier)
A:sklearn.ensemble.tests.test_weight_boosting.sample_weight->numpy.ones_like(y)
A:sklearn.ensemble.tests.test_weight_boosting.score_with_outlier->clone(regr_no_outlier).score(X[:-1], y[:-1])
A:sklearn.ensemble.tests.test_weight_boosting.score_no_outlier->AdaBoostRegressor(estimator=LinearRegression(), n_estimators=1, random_state=0).score(X[:-1], y[:-1])
A:sklearn.ensemble.tests.test_weight_boosting.score_with_weight->clone(regr_no_outlier).score(X[:-1], y[:-1])
A:sklearn.ensemble.tests.test_weight_boosting.model->AdaBoostClassifier(algorithm=algorithm, random_state=42)
A:sklearn.ensemble.tests.test_weight_boosting.tree->DecisionTreeClassifier(max_depth=10, random_state=12)
A:sklearn.ensemble.tests.test_weight_boosting.ada_model->AdaBoostClassifier(estimator=tree, n_estimators=20, algorithm='SAMME', random_state=12)
A:sklearn.ensemble.tests.test_weight_boosting.y_score->AdaBoostClassifier(n_estimators=1, random_state=global_random_seed, algorithm=algorithm).fit(X, y).decision_function(X)
A:sklearn.ensemble.tests.test_weight_boosting.adaboost_clf->AdaBoostClassifier(n_estimators=1)
sklearn.ensemble.tests.test_weight_boosting.test_adaboost_classifier_sample_weight_error()
sklearn.ensemble.tests.test_weight_boosting.test_adaboost_consistent_predict(algorithm)
sklearn.ensemble.tests.test_weight_boosting.test_adaboost_decision_function(algorithm,global_random_seed)
sklearn.ensemble.tests.test_weight_boosting.test_adaboost_negative_weight_error(model,X,y)
sklearn.ensemble.tests.test_weight_boosting.test_adaboost_numerically_stable_feature_importance_with_small_weights()
sklearn.ensemble.tests.test_weight_boosting.test_adaboostclassifier_without_sample_weight(algorithm)
sklearn.ensemble.tests.test_weight_boosting.test_adaboostregressor_sample_weight()
sklearn.ensemble.tests.test_weight_boosting.test_classification_toy(algorithm)
sklearn.ensemble.tests.test_weight_boosting.test_deprecated_samme_r_algorithm()
sklearn.ensemble.tests.test_weight_boosting.test_diabetes(loss)
sklearn.ensemble.tests.test_weight_boosting.test_estimator()
sklearn.ensemble.tests.test_weight_boosting.test_gridsearch()
sklearn.ensemble.tests.test_weight_boosting.test_importances()
sklearn.ensemble.tests.test_weight_boosting.test_iris()
sklearn.ensemble.tests.test_weight_boosting.test_multidimensional_X()
sklearn.ensemble.tests.test_weight_boosting.test_oneclass_adaboost_proba()
sklearn.ensemble.tests.test_weight_boosting.test_pickle()
sklearn.ensemble.tests.test_weight_boosting.test_regression_toy()
sklearn.ensemble.tests.test_weight_boosting.test_samme_proba()
sklearn.ensemble.tests.test_weight_boosting.test_sample_weight_adaboost_regressor()
sklearn.ensemble.tests.test_weight_boosting.test_sample_weights_infinite()
sklearn.ensemble.tests.test_weight_boosting.test_sparse_classification(sparse_container,expected_internal_type)
sklearn.ensemble.tests.test_weight_boosting.test_sparse_regression(sparse_container,expected_internal_type)
sklearn.ensemble.tests.test_weight_boosting.test_staged_predict(algorithm)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_ransac.py----------------------------------------
A:sklearn.linear_model._ransac._EPSILON->numpy.spacing(1)
A:sklearn.linear_model._ransac.nom->max(_EPSILON, 1 - probability)
A:sklearn.linear_model._ransac.denom->max(_EPSILON, 1 - inlier_ratio ** min_samples)
A:sklearn.linear_model._ransac.check_X_params->dict(accept_sparse='csr', force_all_finite=False)
A:sklearn.linear_model._ransac.check_y_params->dict(ensure_2d=False)
A:sklearn.linear_model._ransac.(X, y)->self._validate_data(X, y, validate_separately=(check_X_params, check_y_params))
A:sklearn.linear_model._ransac.estimator->LinearRegression()
A:sklearn.linear_model._ransac.min_samples->numpy.ceil(self.min_samples * X.shape[0])
A:sklearn.linear_model._ransac.residual_threshold->numpy.median(np.abs(y - np.median(y)))
A:sklearn.linear_model._ransac.random_state->check_random_state(self.random_state)
A:sklearn.linear_model._ransac.estimator_fit_has_sample_weight->has_fit_parameter(estimator, 'sample_weight')
A:sklearn.linear_model._ransac.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.linear_model._ransac.sample_idxs->numpy.arange(n_samples)
A:sklearn.linear_model._ransac.subset_idxs->sample_without_replacement(n_samples, min_samples, random_state=random_state)
A:sklearn.linear_model._ransac.y_pred->LinearRegression().predict(X)
A:sklearn.linear_model._ransac.residuals_subset->loss_function(y, y_pred)
A:sklearn.linear_model._ransac.n_inliers_subset->numpy.sum(inlier_mask_subset)
A:sklearn.linear_model._ransac.score_subset->LinearRegression().score(X_inlier_subset, y_inlier_subset)
A:sklearn.linear_model._ransac.max_trials->min(max_trials, _dynamic_max_trials(n_inliers_best, n_samples, min_samples, self.stop_probability))
A:sklearn.linear_model._ransac.X->self._validate_data(X, force_all_finite=False, accept_sparse=True, reset=False)
sklearn.linear_model.RANSACRegressor(self,estimator=None,*,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,loss='absolute_error',random_state=None)
sklearn.linear_model.RANSACRegressor._more_tags(self)
sklearn.linear_model.RANSACRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RANSACRegressor.predict(self,X)
sklearn.linear_model.RANSACRegressor.score(self,X,y)
sklearn.linear_model._ransac.RANSACRegressor(self,estimator=None,*,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,loss='absolute_error',random_state=None)
sklearn.linear_model._ransac.RANSACRegressor.__init__(self,estimator=None,*,min_samples=None,residual_threshold=None,is_data_valid=None,is_model_valid=None,max_trials=100,max_skips=np.inf,stop_n_inliers=np.inf,stop_score=np.inf,stop_probability=0.99,loss='absolute_error',random_state=None)
sklearn.linear_model._ransac.RANSACRegressor._more_tags(self)
sklearn.linear_model._ransac.RANSACRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ransac.RANSACRegressor.predict(self,X)
sklearn.linear_model._ransac.RANSACRegressor.score(self,X,y)
sklearn.linear_model._ransac._dynamic_max_trials(n_inliers,n_samples,min_samples,probability)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_bayes.py----------------------------------------
A:sklearn.linear_model._bayes.max_iter->_deprecate_n_iter(self.n_iter, self.max_iter)
A:sklearn.linear_model._bayes.(X, y)->self._validate_data(X, y, dtype=[np.float64, np.float32], y_numeric=True, ensure_min_samples=2)
A:sklearn.linear_model._bayes.sample_weight->_check_sample_weight(sample_weight, X, dtype=dtype)
A:sklearn.linear_model._bayes.(X, y, X_offset_, y_offset_, X_scale_)->_preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
A:sklearn.linear_model._bayes.(X, y, _)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model._bayes.alpha_->numpy.asarray(1.0 / (np.var(y) + eps), dtype=dtype)
A:sklearn.linear_model._bayes.lambda_->numpy.ones(n_features, dtype=dtype)
A:sklearn.linear_model._bayes.self.scores_->list()
A:sklearn.linear_model._bayes.XT_y->numpy.dot(X.T, y)
A:sklearn.linear_model._bayes.(U, S, Vh)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.linear_model._bayes.(coef_, rmse_)->self._update_coef_(X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_)
A:sklearn.linear_model._bayes.s->(lambda_1 * np.log(lambda_) - lambda_2 * lambda_).sum()
A:sklearn.linear_model._bayes.gamma_->numpy.sum(alpha_ * eigen_vals_ / (lambda_ + alpha_ * eigen_vals_))
A:sklearn.linear_model._bayes.coef_old_->numpy.copy(coef_)
A:sklearn.linear_model._bayes.(self.coef_, rmse_)->self._update_coef_(X, y, n_samples, n_features, XT_y, U, Vh, eigen_vals_, alpha_, lambda_)
A:sklearn.linear_model._bayes.scaled_sigma_->numpy.dot(Vh.T, Vh / (eigen_vals_ + lambda_ / alpha_)[:, np.newaxis])
A:sklearn.linear_model._bayes.y_mean->self._decision_function(X)
A:sklearn.linear_model._bayes.sigmas_squared_data->(np.dot(X, self.sigma_) * X).sum(axis=1)
A:sklearn.linear_model._bayes.y_std->numpy.sqrt(sigmas_squared_data + 1.0 / self.alpha_)
A:sklearn.linear_model._bayes.coef_->update_coeff(X, y, coef_, alpha_, keep_lambda, sigma_)
A:sklearn.linear_model._bayes.rmse_->numpy.sum((y - np.dot(X, coef_)) ** 2)
A:sklearn.linear_model._bayes.logdet_sigma->numpy.full(n_features, lambda_, dtype=np.array(lambda_).dtype)
A:sklearn.linear_model._bayes.keep_lambda->numpy.ones(n_features, dtype=bool)
A:sklearn.linear_model._bayes.sigma_->pinvh(sigma_inv)
A:sklearn.linear_model._bayes.gram->numpy.dot(X_keep.T, X_keep)
A:sklearn.linear_model._bayes.eye->numpy.eye(gram.shape[0], dtype=X.dtype)
sklearn.linear_model.ARDRegression(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model.ARDRegression._update_sigma(self,X,alpha_,lambda_,keep_lambda)
sklearn.linear_model.ARDRegression._update_sigma_woodbury(self,X,alpha_,lambda_,keep_lambda)
sklearn.linear_model.ARDRegression.fit(self,X,y)
sklearn.linear_model.ARDRegression.predict(self,X,return_std=False)
sklearn.linear_model.BayesianRidge(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,alpha_init=None,lambda_init=None,compute_score=False,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model.BayesianRidge._log_marginal_likelihood(self,n_samples,n_features,eigen_vals,alpha_,lambda_,coef,rmse)
sklearn.linear_model.BayesianRidge._update_coef_(self,X,y,n_samples,n_features,XT_y,U,Vh,eigen_vals_,alpha_,lambda_)
sklearn.linear_model.BayesianRidge.fit(self,X,y,sample_weight=None)
sklearn.linear_model.BayesianRidge.predict(self,X,return_std=False)
sklearn.linear_model._bayes.ARDRegression(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model._bayes.ARDRegression.__init__(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,compute_score=False,threshold_lambda=10000.0,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model._bayes.ARDRegression._update_sigma(self,X,alpha_,lambda_,keep_lambda)
sklearn.linear_model._bayes.ARDRegression._update_sigma_woodbury(self,X,alpha_,lambda_,keep_lambda)
sklearn.linear_model._bayes.ARDRegression.fit(self,X,y)
sklearn.linear_model._bayes.ARDRegression.predict(self,X,return_std=False)
sklearn.linear_model._bayes.BayesianRidge(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,alpha_init=None,lambda_init=None,compute_score=False,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model._bayes.BayesianRidge.__init__(self,*,max_iter=None,tol=0.001,alpha_1=1e-06,alpha_2=1e-06,lambda_1=1e-06,lambda_2=1e-06,alpha_init=None,lambda_init=None,compute_score=False,fit_intercept=True,copy_X=True,verbose=False,n_iter='deprecated')
sklearn.linear_model._bayes.BayesianRidge._log_marginal_likelihood(self,n_samples,n_features,eigen_vals,alpha_,lambda_,coef,rmse)
sklearn.linear_model._bayes.BayesianRidge._update_coef_(self,X,y,n_samples,n_features,XT_y,U,Vh,eigen_vals_,alpha_,lambda_)
sklearn.linear_model._bayes.BayesianRidge.fit(self,X,y,sample_weight=None)
sklearn.linear_model._bayes.BayesianRidge.predict(self,X,return_std=False)
sklearn.linear_model._bayes._deprecate_n_iter(n_iter,max_iter)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py----------------------------------------
A:sklearn.linear_model._stochastic_gradient.self.estimator->clone(estimator)
A:sklearn.linear_model._stochastic_gradient.est.coef_->est._standard_coef.ravel().reshape(1, -1)
A:sklearn.linear_model._stochastic_gradient.est.intercept_->numpy.atleast_1d(intercept)
A:sklearn.linear_model._stochastic_gradient.penalty->str(penalty).lower()
A:sklearn.linear_model._stochastic_gradient.coef_init->coef_init.ravel().ravel()
A:sklearn.linear_model._stochastic_gradient.self.coef_->est._standard_coef.ravel().reshape(1, -1)
A:sklearn.linear_model._stochastic_gradient.intercept_init->numpy.asarray(intercept_init, dtype=input_dtype)
A:sklearn.linear_model._stochastic_gradient.self.intercept_->numpy.atleast_1d(intercept)
A:sklearn.linear_model._stochastic_gradient.self.offset_->numpy.zeros(1, dtype=input_dtype, order='C')
A:sklearn.linear_model._stochastic_gradient.self._average_coef->numpy.zeros(n_features, dtype=X.dtype, order='C')
A:sklearn.linear_model._stochastic_gradient.self._average_intercept->numpy.zeros(1, dtype=X.dtype, order='C')
A:sklearn.linear_model._stochastic_gradient.validation_mask->self._make_validation_split(y, sample_mask=sample_weight > 0)
A:sklearn.linear_model._stochastic_gradient.cv->splitter_type(test_size=self.validation_fraction, random_state=self.random_state)
A:sklearn.linear_model._stochastic_gradient.(idx_train, idx_val)->next(cv.split(np.zeros(shape=(y.shape[0], 1)), y))
A:sklearn.linear_model._stochastic_gradient.y_i->numpy.ones(y.shape, dtype=input_dtye, order='C')
A:sklearn.linear_model._stochastic_gradient.coef->est._standard_coef.ravel()
A:sklearn.linear_model._stochastic_gradient.average_coef->est._average_coef.ravel()
A:sklearn.linear_model._stochastic_gradient.(y_i, coef, intercept, average_coef, average_intercept)->_prepare_fit_binary(est, y, i, input_dtye=X.dtype)
A:sklearn.linear_model._stochastic_gradient.random_state->check_random_state(self.random_state)
A:sklearn.linear_model._stochastic_gradient.(dataset, intercept_decay)->make_dataset(X, y, sample_weight, random_state=random_state)
A:sklearn.linear_model._stochastic_gradient.penalty_type->self._get_penalty_type(self.penalty)
A:sklearn.linear_model._stochastic_gradient.learning_rate_type->self._get_learning_rate_type(learning_rate)
A:sklearn.linear_model._stochastic_gradient.classes->numpy.unique(y)
A:sklearn.linear_model._stochastic_gradient.validation_score_cb->self._make_validation_score_cb(validation_mask, X, y, sample_weight)
A:sklearn.linear_model._stochastic_gradient.seed->check_random_state(self.random_state).randint(0, np.iinfo(np.int32).max)
A:sklearn.linear_model._stochastic_gradient._plain_sgd->_get_plain_sgd_function(input_dtype=coef.dtype)
A:sklearn.linear_model._stochastic_gradient.(coef, intercept, average_coef, average_intercept, n_iter_)->_plain_sgd(coef, intercept, average_coef, average_intercept, est._loss_function_, penalty_type, alpha, C, est.l1_ratio, dataset, validation_mask, est.early_stopping, validation_score_cb, int(est.n_iter_no_change), max_iter, tol, int(est.fit_intercept), int(est.verbose), int(est.shuffle), seed, pos_weight, neg_weight, learning_rate_type, est.eta0, est.power_t, 0, est.t_, intercept_decay, est.average)
A:sklearn.linear_model._stochastic_gradient.(X, y)->self._validate_data(X, y, accept_sparse='csr', copy=False, order='C', dtype=[np.float64, np.float32], accept_large_sparse=False, reset=first_call)
A:sklearn.linear_model._stochastic_gradient.self._expanded_class_weight->compute_class_weight(self.class_weight, classes=self.classes_, y=y)
A:sklearn.linear_model._stochastic_gradient.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.linear_model._stochastic_gradient.self._loss_function_->self._get_loss_function(loss)
A:sklearn.linear_model._stochastic_gradient.y->(self.decision_function(X) >= 0).astype(np.int32)
A:sklearn.linear_model._stochastic_gradient.(coef, intercept, n_iter_)->fit_binary(self, 1, X, y, alpha, C, learning_rate, max_iter, self._expanded_class_weight[1], self._expanded_class_weight[0], sample_weight, random_state=self.random_state)
A:sklearn.linear_model._stochastic_gradient.self._standard_intercept->numpy.atleast_1d(intercept)
A:sklearn.linear_model._stochastic_gradient.seeds->check_random_state(self.random_state).randint(MAX_INT, size=len(self.classes_))
A:sklearn.linear_model._stochastic_gradient.result->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, require='sharedmem')((delayed(fit_binary)(self, i, X, y, alpha, C, learning_rate, max_iter, self._expanded_class_weight[i], 1.0, sample_weight, validation_mask=validation_mask, random_state=seed) for (i, seed) in enumerate(seeds)))
A:sklearn.linear_model._stochastic_gradient.n_iter_->max(n_iter_, n_iter_i)
A:sklearn.linear_model._stochastic_gradient.scores->self.decision_function(X)
A:sklearn.linear_model._stochastic_gradient.prob2->numpy.ones((scores.shape[0], 2))
A:sklearn.linear_model._stochastic_gradient.prob_sum->prob.sum(axis=1)
A:sklearn.linear_model._stochastic_gradient.prob_sum[all_zero]->len(self.classes_)
A:sklearn.linear_model._stochastic_gradient.X->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.linear_model._stochastic_gradient.loss_function->self._get_loss_function(loss)
A:sklearn.linear_model._stochastic_gradient.(coef, intercept, average_coef, average_intercept, self.n_iter_)->_plain_sgd(coef, intercept[0], average_coef, average_intercept[0], self._loss_function_, penalty_type, alpha, C, self.l1_ratio, dataset, validation_mask, self.early_stopping, validation_score_cb, int(self.n_iter_no_change), max_iter, tol, int(self.fit_intercept), int(self.verbose), int(self.shuffle), seed, neg_weight, pos_weight, learning_rate_type, self.eta0, self.power_t, one_class, self.t_, offset_decay, self.average)
A:sklearn.linear_model._stochastic_gradient.(dataset, offset_decay)->make_dataset(X, y, sample_weight)
sklearn.linear_model.SGDClassifier(self,loss='hinge',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=None,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False,average=False)
sklearn.linear_model.SGDClassifier._check_proba(self)
sklearn.linear_model.SGDClassifier._more_tags(self)
sklearn.linear_model.SGDClassifier.predict_log_proba(self,X)
sklearn.linear_model.SGDClassifier.predict_proba(self,X)
sklearn.linear_model.SGDOneClassSVM(self,nu=0.5,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,warm_start=False,average=False)
sklearn.linear_model.SGDOneClassSVM._fit(self,X,alpha,C,loss,learning_rate,coef_init=None,offset_init=None,sample_weight=None)
sklearn.linear_model.SGDOneClassSVM._fit_one_class(self,X,alpha,C,sample_weight,learning_rate,max_iter)
sklearn.linear_model.SGDOneClassSVM._more_tags(self)
sklearn.linear_model.SGDOneClassSVM._partial_fit(self,X,alpha,C,loss,learning_rate,max_iter,sample_weight,coef_init,offset_init)
sklearn.linear_model.SGDOneClassSVM.decision_function(self,X)
sklearn.linear_model.SGDOneClassSVM.fit(self,X,y=None,coef_init=None,offset_init=None,sample_weight=None)
sklearn.linear_model.SGDOneClassSVM.partial_fit(self,X,y=None,sample_weight=None)
sklearn.linear_model.SGDOneClassSVM.predict(self,X)
sklearn.linear_model.SGDOneClassSVM.score_samples(self,X)
sklearn.linear_model.SGDRegressor(self,loss='squared_error',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model.SGDRegressor._more_tags(self)
sklearn.linear_model._stochastic_gradient.BaseSGD(self,loss,*,penalty='l2',alpha=0.0001,C=1.0,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=0.1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGD.__init__(self,loss,*,penalty='l2',alpha=0.0001,C=1.0,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=0.1,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGD._allocate_parameter_mem(self,n_classes,n_features,input_dtype,coef_init=None,intercept_init=None,one_class=0)
sklearn.linear_model._stochastic_gradient.BaseSGD._get_learning_rate_type(self,learning_rate)
sklearn.linear_model._stochastic_gradient.BaseSGD._get_loss_function(self,loss)
sklearn.linear_model._stochastic_gradient.BaseSGD._get_penalty_type(self,penalty)
sklearn.linear_model._stochastic_gradient.BaseSGD._make_validation_score_cb(self,validation_mask,X,y,sample_weight,classes=None)
sklearn.linear_model._stochastic_gradient.BaseSGD._make_validation_split(self,y,sample_mask)
sklearn.linear_model._stochastic_gradient.BaseSGD._more_validate_params(self,for_partial_fit=False)
sklearn.linear_model._stochastic_gradient.BaseSGD.fit(self,X,y)
sklearn.linear_model._stochastic_gradient.BaseSGD.loss_function_(self)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier(self,loss='hinge',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=None,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.__init__(self,loss='hinge',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=None,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier._fit(self,X,y,alpha,C,loss,learning_rate,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier._fit_binary(self,X,y,alpha,C,sample_weight,learning_rate,max_iter)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier._fit_multiclass(self,X,y,alpha,C,learning_rate,sample_weight,max_iter)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier._partial_fit(self,X,y,alpha,C,loss,learning_rate,max_iter,classes,sample_weight,coef_init,intercept_init)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit(self,X,y,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor(self,loss='squared_error',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor.__init__(self,loss='squared_error',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor._decision_function(self,X)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor._fit(self,X,y,alpha,C,loss,learning_rate,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor._fit_regressor(self,X,y,alpha,C,loss,learning_rate,sample_weight,max_iter)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor._partial_fit(self,X,y,alpha,C,loss,learning_rate,max_iter,sample_weight,coef_init,intercept_init)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor.fit(self,X,y,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor.partial_fit(self,X,y,sample_weight=None)
sklearn.linear_model._stochastic_gradient.BaseSGDRegressor.predict(self,X)
sklearn.linear_model._stochastic_gradient.SGDClassifier(self,loss='hinge',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=None,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDClassifier.__init__(self,loss='hinge',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,n_jobs=None,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDClassifier._check_proba(self)
sklearn.linear_model._stochastic_gradient.SGDClassifier._more_tags(self)
sklearn.linear_model._stochastic_gradient.SGDClassifier.predict_log_proba(self,X)
sklearn.linear_model._stochastic_gradient.SGDClassifier.predict_proba(self,X)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM(self,nu=0.5,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.__init__(self,nu=0.5,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,random_state=None,learning_rate='optimal',eta0=0.0,power_t=0.5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM._fit(self,X,alpha,C,loss,learning_rate,coef_init=None,offset_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM._fit_one_class(self,X,alpha,C,sample_weight,learning_rate,max_iter)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM._more_tags(self)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM._partial_fit(self,X,alpha,C,loss,learning_rate,max_iter,sample_weight,coef_init,offset_init)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.decision_function(self,X)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.fit(self,X,y=None,coef_init=None,offset_init=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.partial_fit(self,X,y=None,sample_weight=None)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.predict(self,X)
sklearn.linear_model._stochastic_gradient.SGDOneClassSVM.score_samples(self,X)
sklearn.linear_model._stochastic_gradient.SGDRegressor(self,loss='squared_error',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDRegressor.__init__(self,loss='squared_error',*,penalty='l2',alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,epsilon=DEFAULT_EPSILON,random_state=None,learning_rate='invscaling',eta0=0.01,power_t=0.25,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,warm_start=False,average=False)
sklearn.linear_model._stochastic_gradient.SGDRegressor._more_tags(self)
sklearn.linear_model._stochastic_gradient._ValidationScoreCallback(self,estimator,X_val,y_val,sample_weight_val,classes=None)
sklearn.linear_model._stochastic_gradient._ValidationScoreCallback.__init__(self,estimator,X_val,y_val,sample_weight_val,classes=None)
sklearn.linear_model._stochastic_gradient._get_plain_sgd_function(input_dtype)
sklearn.linear_model._stochastic_gradient._prepare_fit_binary(est,y,i,input_dtye)
sklearn.linear_model._stochastic_gradient.fit_binary(est,i,X,y,alpha,C,learning_rate,max_iter,pos_weight,neg_weight,sample_weight,validation_mask=None,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_omp.py----------------------------------------
A:sklearn.linear_model._omp.X->as_float_array(X, copy=False, force_all_finite=False)
A:sklearn.linear_model._omp.(nrm2, swap)->scipy.linalg.get_blas_funcs(('nrm2', 'swap'), (Gram,))
A:sklearn.linear_model._omp.(potrs,)->get_lapack_funcs(('potrs',), (Gram,))
A:sklearn.linear_model._omp.alpha->numpy.dot(X.T, y)
A:sklearn.linear_model._omp.gamma->numpy.empty(0)
A:sklearn.linear_model._omp.indices->numpy.arange(len(Gram))
A:sklearn.linear_model._omp.L->numpy.empty((max_features, max_features), dtype=Gram.dtype)
A:sklearn.linear_model._omp.coefs->orthogonal_mp(X_train, y_train, n_nonzero_coefs=max_iter, tol=None, precompute=False, copy_X=False, return_path=True)
A:sklearn.linear_model._omp.lam->numpy.argmax(np.abs(alpha))
A:sklearn.linear_model._omp.L[n_active, :n_active]->numpy.dot(X[:, :n_active].T, X[:, lam])
A:sklearn.linear_model._omp.L[n_active, n_active]->sqrt(Lkk)
A:sklearn.linear_model._omp.L[0, 0]->sqrt(Gram[lam, lam])
A:sklearn.linear_model._omp.(X.T[n_active], X.T[lam])->swap(X.T[n_active], X.T[lam])
A:sklearn.linear_model._omp.(gamma, _)->potrs(L[:n_active, :n_active], Xy[:n_active], lower=True, overwrite_b=False)
A:sklearn.linear_model._omp.Xy->Xy.copy().copy()
A:sklearn.linear_model._omp.(Gram[n_active], Gram[lam])->swap(Gram[n_active], Gram[lam])
A:sklearn.linear_model._omp.(Gram.T[n_active], Gram.T[lam])->swap(Gram.T[n_active], Gram.T[lam])
A:sklearn.linear_model._omp.beta->numpy.dot(Gram[:, :n_active], gamma)
A:sklearn.linear_model._omp.delta->numpy.inner(gamma, beta[:n_active])
A:sklearn.linear_model._omp.y->check_array(y)
A:sklearn.linear_model._omp.n_nonzero_coefs->int(0.1 * len(Gram))
A:sklearn.linear_model._omp.G->numpy.asfortranarray(G)
A:sklearn.linear_model._omp.norms_squared->numpy.sum(y ** 2, axis=0)
A:sklearn.linear_model._omp.coef->numpy.zeros((len(Gram), Xy.shape[1]), dtype=Gram.dtype)
A:sklearn.linear_model._omp.out->_gram_omp(Gram, Xy[:, k], n_nonzero_coefs, norms_squared[k] if tol is not None else None, tol, copy_Gram=copy_Gram, copy_Xy=False, return_path=return_path)
A:sklearn.linear_model._omp.Gram->check_array(Gram, order='F', copy=copy_Gram)
A:sklearn.linear_model._omp.(X, y)->self._validate_data(X, y, y_numeric=True, ensure_min_features=2)
A:sklearn.linear_model._omp.(X, y, X_offset, y_offset, X_scale, Gram, Xy)->_pre_fit(X, y, None, self.precompute, self.fit_intercept, copy=True)
A:sklearn.linear_model._omp.self.n_nonzero_coefs_->max(int(0.1 * n_features), 1)
A:sklearn.linear_model._omp.(coef_, self.n_iter_)->orthogonal_mp_gram(Gram, Xy=Xy, n_nonzero_coefs=self.n_nonzero_coefs_, tol=self.tol, norms_squared=norms_sq, copy_Gram=True, copy_Xy=True, return_n_iter=True)
A:sklearn.linear_model._omp.X_train->X_train.copy().copy()
A:sklearn.linear_model._omp.y_train->as_float_array(y_train, copy=False)
A:sklearn.linear_model._omp.X_test->X_test.copy().copy()
A:sklearn.linear_model._omp.y_test->as_float_array(y_test, copy=False)
A:sklearn.linear_model._omp.X_mean->X_train.copy().copy().mean(axis=0)
A:sklearn.linear_model._omp.y_mean->as_float_array(y_train, copy=False).mean(axis=0)
A:sklearn.linear_model._omp.cv->check_cv(self.cv, classifier=False)
A:sklearn.linear_model._omp.routed_params->Bunch()
A:sklearn.linear_model._omp.routed_params.splitter->Bunch(split={})
A:sklearn.linear_model._omp.cv_paths->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_omp_path_residues)(X[train], y[train], X[test], y[test], self.copy, self.fit_intercept, max_iter) for (train, test) in cv.split(X, **routed_params.splitter.split)))
A:sklearn.linear_model._omp.min_early_stop->min((fold.shape[0] for fold in cv_paths))
A:sklearn.linear_model._omp.mse_folds->numpy.array([(fold[:min_early_stop] ** 2).mean(axis=1) for fold in cv_paths])
A:sklearn.linear_model._omp.omp->OrthogonalMatchingPursuit(n_nonzero_coefs=best_n_nonzero_coefs, fit_intercept=self.fit_intercept).fit(X, y)
A:sklearn.linear_model._omp.router->MetadataRouter(owner=self.__class__.__name__).add(splitter=self.cv, method_mapping=MethodMapping().add(callee='split', caller='fit'))
sklearn.linear_model.OrthogonalMatchingPursuit(self,*,n_nonzero_coefs=None,tol=None,fit_intercept=True,precompute='auto')
sklearn.linear_model.OrthogonalMatchingPursuit.fit(self,X,y)
sklearn.linear_model.OrthogonalMatchingPursuitCV(self,*,copy=True,fit_intercept=True,max_iter=None,cv=None,n_jobs=None,verbose=False)
sklearn.linear_model.OrthogonalMatchingPursuitCV.fit(self,X,y,**fit_params)
sklearn.linear_model.OrthogonalMatchingPursuitCV.get_metadata_routing(self)
sklearn.linear_model._omp.OrthogonalMatchingPursuit(self,*,n_nonzero_coefs=None,tol=None,fit_intercept=True,precompute='auto')
sklearn.linear_model._omp.OrthogonalMatchingPursuit.__init__(self,*,n_nonzero_coefs=None,tol=None,fit_intercept=True,precompute='auto')
sklearn.linear_model._omp.OrthogonalMatchingPursuit.fit(self,X,y)
sklearn.linear_model._omp.OrthogonalMatchingPursuitCV(self,*,copy=True,fit_intercept=True,max_iter=None,cv=None,n_jobs=None,verbose=False)
sklearn.linear_model._omp.OrthogonalMatchingPursuitCV.__init__(self,*,copy=True,fit_intercept=True,max_iter=None,cv=None,n_jobs=None,verbose=False)
sklearn.linear_model._omp.OrthogonalMatchingPursuitCV.fit(self,X,y,**fit_params)
sklearn.linear_model._omp.OrthogonalMatchingPursuitCV.get_metadata_routing(self)
sklearn.linear_model._omp._cholesky_omp(X,y,n_nonzero_coefs,tol=None,copy_X=True,return_path=False)
sklearn.linear_model._omp._gram_omp(Gram,Xy,n_nonzero_coefs,tol_0=None,tol=None,copy_Gram=True,copy_Xy=True,return_path=False)
sklearn.linear_model._omp._omp_path_residues(X_train,y_train,X_test,y_test,copy=True,fit_intercept=True,max_iter=100)
sklearn.linear_model._omp.orthogonal_mp(X,y,*,n_nonzero_coefs=None,tol=None,precompute=False,copy_X=True,return_path=False,return_n_iter=False)
sklearn.linear_model._omp.orthogonal_mp_gram(Gram,Xy,*,n_nonzero_coefs=None,tol=None,norms_squared=None,copy_Gram=True,copy_Xy=True,return_path=False,return_n_iter=False)
sklearn.linear_model.orthogonal_mp(X,y,*,n_nonzero_coefs=None,tol=None,precompute=False,copy_X=True,return_path=False,return_n_iter=False)
sklearn.linear_model.orthogonal_mp_gram(Gram,Xy,*,n_nonzero_coefs=None,tol=None,norms_squared=None,copy_Gram=True,copy_Xy=True,return_path=False,return_n_iter=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_ridge.py----------------------------------------
A:sklearn.linear_model._ridge.X1->_get_rescaled_operator(X, X_offset_scale, sample_weight_sqrt)
A:sklearn.linear_model._ridge.sample_weight_sqrt->numpy.ones(X.shape[0], dtype=X.dtype)
A:sklearn.linear_model._ridge.coefs->numpy.empty((y.shape[1], n_features), dtype=X.dtype)
A:sklearn.linear_model._ridge.mv->create_mv(alpha[i])
A:sklearn.linear_model._ridge.C->scipy.sparse.linalg.LinearOperator((n_features, n_features), matvec=mv, dtype=X.dtype)
A:sklearn.linear_model._ridge.(coef, info)->_sparse_linalg_cg(C, y_column, rtol=tol)
A:sklearn.linear_model._ridge.coefs[i]->_get_rescaled_operator(X, X_offset_scale, sample_weight_sqrt).rmatvec(coef)
A:sklearn.linear_model._ridge.y_column->_get_rescaled_operator(X, X_offset_scale, sample_weight_sqrt).rmatvec(y_column)
A:sklearn.linear_model._ridge.(coefs[i], info)->_sparse_linalg_cg(C, y_column, maxiter=max_iter, rtol=tol)
A:sklearn.linear_model._ridge.n_iter->numpy.empty(y.shape[1], dtype=np.int32)
A:sklearn.linear_model._ridge.sqrt_alpha->numpy.sqrt(alpha)
A:sklearn.linear_model._ridge.info->scipy.sparse.linalg.lsqr(X1, y_column, damp=sqrt_alpha[i], atol=tol, btol=tol, iter_lim=max_iter)
A:sklearn.linear_model._ridge.A->(V * w).dot(V.T)
A:sklearn.linear_model._ridge.Xy->safe_sparse_dot(X.T, y, dense_output=True)
A:sklearn.linear_model._ridge.one_alpha->(alpha == alpha[0]).all()
A:sklearn.linear_model._ridge.coef[:]->scipy.linalg.solve(A, target, assume_a='pos', overwrite_a=False).ravel()
A:sklearn.linear_model._ridge.K->safe_sparse_dot(X, X.T, dense_output=True)
A:sklearn.linear_model._ridge.alpha->check_scalar_alpha(alpha, f'alphas[{index}]')
A:sklearn.linear_model._ridge.sw->numpy.sqrt(np.atleast_1d(sample_weight))
A:sklearn.linear_model._ridge.dual_coef->_solve_cholesky_kernel(K, y, alpha)
A:sklearn.linear_model._ridge.dual_coefs->numpy.empty([n_targets, n_samples], K.dtype)
A:sklearn.linear_model._ridge.dual_coef[:]->scipy.linalg.solve(K, target, assume_a='pos', overwrite_a=False).ravel()
A:sklearn.linear_model._ridge.(U, s, Vt)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.linear_model._ridge.UTy->numpy.dot(U.T, y)
A:sklearn.linear_model._ridge.d->numpy.zeros((s.size, alpha.size), dtype=X.dtype)
A:sklearn.linear_model._ridge.x0->numpy.zeros((n_features,))
A:sklearn.linear_model._ridge.result->scipy.optimize.minimize(func, x0, **config)
A:sklearn.linear_model._ridge._accept_sparse->_get_valid_accept_sparse(sparse.issparse(X), self.solver)
A:sklearn.linear_model._ridge.X->numpy.hstack((X, intercept_column))
A:sklearn.linear_model._ridge.y->column_or_1d(y, warn=True)
A:sklearn.linear_model._ridge.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.linear_model._ridge.(X, y, sample_weight_sqrt)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model._ridge.coef->coef.ravel().ravel()
A:sklearn.linear_model._ridge.(coef, n_iter)->_solve_lsqr(X, y, alpha=alpha, fit_intercept=fit_intercept, max_iter=max_iter, tol=tol, X_offset=X_offset, X_scale=X_scale, sample_weight_sqrt=sample_weight_sqrt if has_sw else None)
A:sklearn.linear_model._ridge.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model._ridge.intercept->numpy.zeros((y.shape[1],), dtype=X.dtype)
A:sklearn.linear_model._ridge.(coef_, n_iter_, _)->sag_solver(X, target.ravel(), sample_weight, 'squared', alpha_i, 0, max_iter, tol, verbose, random_state, False, max_squared_sum, init, is_saga=solver == 'saga')
A:sklearn.linear_model._ridge.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X, sample_weight=sample_weight)
A:sklearn.linear_model._ridge.(self.coef_, self.n_iter_, self.intercept_)->_ridge_regression(X, y, alpha=self.alpha, sample_weight=sample_weight, max_iter=self.max_iter, tol=self.tol, solver='sag', positive=self.positive, random_state=self.random_state, return_n_iter=True, return_intercept=True, check_input=False)
A:sklearn.linear_model._ridge.(self.coef_, self.n_iter_)->_ridge_regression(X, y, alpha=self.alpha, sample_weight=sample_weight, max_iter=self.max_iter, tol=self.tol, solver=solver, positive=self.positive, random_state=self.random_state, return_n_iter=True, return_intercept=False, check_input=False, fit_intercept=self.fit_intercept, **params)
A:sklearn.linear_model._ridge.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float64], multi_output=True, y_numeric=True)
A:sklearn.linear_model._ridge.accept_sparse->_get_valid_accept_sparse(sparse.issparse(X), solver)
A:sklearn.linear_model._ridge.self._label_binarizer->LabelBinarizer(pos_label=1, neg_label=-1)
A:sklearn.linear_model._ridge.Y->self._label_binarizer.fit_transform(y)
A:sklearn.linear_model._ridge.(X, y, sample_weight, Y)->self._prepare_data(X, y, sample_weight, solver='eigen')
A:sklearn.linear_model._ridge.abs_cosine->numpy.abs(query.dot(vectors))
A:sklearn.linear_model._ridge.index->numpy.argmax(abs_cosine)
A:sklearn.linear_model._ridge.v->v.ravel().ravel()
A:sklearn.linear_model._ridge.res->numpy.empty((n_features, v.shape[1]), dtype=self.X.dtype)
A:sklearn.linear_model._ridge.res[-1]->numpy.dot(self.sqrt_sw, v)
A:sklearn.linear_model._ridge.X_mean->numpy.zeros(X.shape[1], dtype=X.dtype)
A:sklearn.linear_model._ridge.sample_weight_matrix->scipy.sparse.dia_matrix((sqrt_sw, 0), shape=(n_samples, n_samples))
A:sklearn.linear_model._ridge.X_weighted->scipy.sparse.dia_matrix((sqrt_sw, 0), shape=(n_samples, n_samples)).dot(X)
A:sklearn.linear_model._ridge.(X_mean, _)->mean_variance_axis(X_weighted, axis=0)
A:sklearn.linear_model._ridge.weight_sum->numpy.ones(n_samples, dtype=X.dtype).dot(sqrt_sw)
A:sklearn.linear_model._ridge.diag->numpy.empty(X.shape[0], dtype=X.dtype)
A:sklearn.linear_model._ridge.batch->slice(start, min(X.shape[0], start + batch_size), 1)
A:sklearn.linear_model._ridge.X_batch->X[batch].toarray()
A:sklearn.linear_model._ridge.diag[batch]->(X_batch.dot(A) * X_batch).sum(axis=1)
A:sklearn.linear_model._ridge.(K, X_mean)->self._compute_gram(X, sqrt_sw)
A:sklearn.linear_model._ridge.(eigvals, Q)->scipy.linalg.eigh(K)
A:sklearn.linear_model._ridge.QT_y->numpy.dot(Q.T, y)
A:sklearn.linear_model._ridge.intercept_dim->_find_smallest_angle(normalized_sw, U)
A:sklearn.linear_model._ridge.c->numpy.dot(Q, self._diag_dot(w, QT_y))
A:sklearn.linear_model._ridge.G_inverse_diag->self._decomp_diag(w, Q)
A:sklearn.linear_model._ridge.cov->numpy.empty((n_features + 1, n_features + 1), dtype=X.dtype)
A:sklearn.linear_model._ridge.(cov[:-1, :-1], X_mean)->self._compute_covariance(X, sqrt_sw)
A:sklearn.linear_model._ridge.cov[-1, -1]->numpy.ones(n_samples, dtype=X.dtype).dot(sqrt_sw)
A:sklearn.linear_model._ridge.nullspace_dim->max(0, n_features - n_samples)
A:sklearn.linear_model._ridge.(eigvals, V)->scipy.linalg.eigh(cov)
A:sklearn.linear_model._ridge.AXy->(V * w).dot(V.T).dot(X_op.T.dot(y))
A:sklearn.linear_model._ridge.y_hat->_X_CenterStackOp(X, X_mean, sqrt_sw).dot(AXy)
A:sklearn.linear_model._ridge.hat_diag->self._sparse_multidot_diag(X, A, X_mean, sqrt_sw)
A:sklearn.linear_model._ridge.intercept_sv->numpy.zeros(V.shape[0])
A:sklearn.linear_model._ridge.X_op->_X_CenterStackOp(X, X_mean, sqrt_sw)
A:sklearn.linear_model._ridge.(U, singvals, _)->scipy.linalg.svd(X, full_matrices=0)
A:sklearn.linear_model._ridge.UT_y->numpy.dot(U.T, y)
A:sklearn.linear_model._ridge.self.alphas->numpy.asarray(self.alphas)
A:sklearn.linear_model._ridge.gcv_mode->_check_gcv_mode(X, self.gcv_mode)
A:sklearn.linear_model._ridge.(X, y, sqrt_sw)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model._ridge.sqrt_sw->numpy.ones(n_samples, dtype=X.dtype)
A:sklearn.linear_model._ridge.(X_mean, *decomposition)->decompose(X, y, sqrt_sw)
A:sklearn.linear_model._ridge.scorer->check_scoring(self, scoring=self.scoring, allow_none=True)
A:sklearn.linear_model._ridge.self.cv_values_->self.cv_values_.reshape(cv_values_shape)
A:sklearn.linear_model._ridge.(G_inverse_diag, c)->solve(float(alpha), y, sqrt_sw, X_mean, *decomposition)
A:sklearn.linear_model._ridge.self.cv_values_[:, i]->predictions.ravel()
A:sklearn.linear_model._ridge.identity_estimator->_IdentityRegressor()
A:sklearn.linear_model._ridge.alpha_score->scorer(identity_estimator, predictions.ravel(), y.ravel())
A:sklearn.linear_model._ridge.best_score->numpy.atleast_1d(alpha_score)
A:sklearn.linear_model._ridge.best_alpha->numpy.full(n_y, alpha)
A:sklearn.linear_model._ridge.self.coef_->safe_sparse_dot(self.dual_coef_.T, X)
A:sklearn.linear_model._ridge.check_scalar_alpha->partial(check_scalar, target_type=numbers.Real, min_val=0.0, include_boundaries='neither')
A:sklearn.linear_model._ridge.self.alphas[0]->check_scalar_alpha(self.alphas[0], 'alphas')
A:sklearn.linear_model._ridge.alphas->numpy.asarray(self.alphas)
A:sklearn.linear_model._ridge.estimator->_RidgeGCV(alphas, fit_intercept=self.fit_intercept, scoring=self.scoring, gcv_mode=self.gcv_mode, store_cv_values=self.store_cv_values, is_clf=is_classifier(self), alpha_per_target=self.alpha_per_target)
A:sklearn.linear_model._ridge.gs->GridSearchCV(model(fit_intercept=self.fit_intercept, solver=solver), parameters, cv=cv, scoring=self.scoring)
sklearn.linear_model.Ridge(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,solver='auto',positive=False,random_state=None)
sklearn.linear_model.Ridge.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RidgeCV(_RoutingNotSupportedMixin,MultiOutputMixin,RegressorMixin,_BaseRidgeCV)
sklearn.linear_model.RidgeCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RidgeClassifier(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,class_weight=None,solver='auto',positive=False,random_state=None)
sklearn.linear_model.RidgeClassifier.fit(self,X,y,sample_weight=None)
sklearn.linear_model.RidgeClassifierCV(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,cv=None,class_weight=None,store_cv_values=False)
sklearn.linear_model.RidgeClassifierCV._more_tags(self)
sklearn.linear_model.RidgeClassifierCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge.Ridge(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge.Ridge.__init__(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge.Ridge.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge.RidgeCV(_RoutingNotSupportedMixin,MultiOutputMixin,RegressorMixin,_BaseRidgeCV)
sklearn.linear_model._ridge.RidgeCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge.RidgeClassifier(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,class_weight=None,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge.RidgeClassifier.__init__(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,class_weight=None,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge.RidgeClassifier.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge.RidgeClassifierCV(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,cv=None,class_weight=None,store_cv_values=False)
sklearn.linear_model._ridge.RidgeClassifierCV.__init__(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,cv=None,class_weight=None,store_cv_values=False)
sklearn.linear_model._ridge.RidgeClassifierCV._more_tags(self)
sklearn.linear_model._ridge.RidgeClassifierCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge._BaseRidge(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge._BaseRidge.__init__(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=None,tol=0.0001,solver='auto',positive=False,random_state=None)
sklearn.linear_model._ridge._BaseRidge.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge._BaseRidgeCV(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,cv=None,gcv_mode=None,store_cv_values=False,alpha_per_target=False)
sklearn.linear_model._ridge._BaseRidgeCV.__init__(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,cv=None,gcv_mode=None,store_cv_values=False,alpha_per_target=False)
sklearn.linear_model._ridge._BaseRidgeCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge._IdentityClassifier(self,classes)
sklearn.linear_model._ridge._IdentityClassifier.__init__(self,classes)
sklearn.linear_model._ridge._IdentityClassifier.decision_function(self,y_predict)
sklearn.linear_model._ridge._IdentityRegressor
sklearn.linear_model._ridge._IdentityRegressor.decision_function(self,y_predict)
sklearn.linear_model._ridge._IdentityRegressor.predict(self,y_predict)
sklearn.linear_model._ridge._RidgeClassifierMixin(LinearClassifierMixin)
sklearn.linear_model._ridge._RidgeClassifierMixin._more_tags(self)
sklearn.linear_model._ridge._RidgeClassifierMixin._prepare_data(self,X,y,sample_weight,solver)
sklearn.linear_model._ridge._RidgeClassifierMixin.classes_(self)
sklearn.linear_model._ridge._RidgeClassifierMixin.predict(self,X)
sklearn.linear_model._ridge._RidgeGCV(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,copy_X=True,gcv_mode=None,store_cv_values=False,is_clf=False,alpha_per_target=False)
sklearn.linear_model._ridge._RidgeGCV.__init__(self,alphas=(0.1,1.0,10.0),*,fit_intercept=True,scoring=None,copy_X=True,gcv_mode=None,store_cv_values=False,is_clf=False,alpha_per_target=False)
sklearn.linear_model._ridge._RidgeGCV._compute_covariance(self,X,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV._compute_gram(self,X,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV._decomp_diag(v_prime,Q)
sklearn.linear_model._ridge._RidgeGCV._diag_dot(D,B)
sklearn.linear_model._ridge._RidgeGCV._eigen_decompose_covariance(self,X,y,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV._eigen_decompose_gram(self,X,y,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV._solve_eigen_covariance(self,alpha,y,sqrt_sw,X_mean,eigvals,V,X)
sklearn.linear_model._ridge._RidgeGCV._solve_eigen_covariance_intercept(self,alpha,y,sqrt_sw,X_mean,eigvals,V,X)
sklearn.linear_model._ridge._RidgeGCV._solve_eigen_covariance_no_intercept(self,alpha,y,sqrt_sw,X_mean,eigvals,V,X)
sklearn.linear_model._ridge._RidgeGCV._solve_eigen_gram(self,alpha,y,sqrt_sw,X_mean,eigvals,Q,QT_y)
sklearn.linear_model._ridge._RidgeGCV._solve_svd_design_matrix(self,alpha,y,sqrt_sw,X_mean,singvals_sq,U,UT_y)
sklearn.linear_model._ridge._RidgeGCV._sparse_multidot_diag(self,X,A,X_mean,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV._svd_decompose_design_matrix(self,X,y,sqrt_sw)
sklearn.linear_model._ridge._RidgeGCV.fit(self,X,y,sample_weight=None)
sklearn.linear_model._ridge._XT_CenterStackOp(self,X,X_mean,sqrt_sw)
sklearn.linear_model._ridge._XT_CenterStackOp.__init__(self,X,X_mean,sqrt_sw)
sklearn.linear_model._ridge._XT_CenterStackOp._matmat(self,v)
sklearn.linear_model._ridge._XT_CenterStackOp._matvec(self,v)
sklearn.linear_model._ridge._X_CenterStackOp(self,X,X_mean,sqrt_sw)
sklearn.linear_model._ridge._X_CenterStackOp.__init__(self,X,X_mean,sqrt_sw)
sklearn.linear_model._ridge._X_CenterStackOp._matmat(self,v)
sklearn.linear_model._ridge._X_CenterStackOp._matvec(self,v)
sklearn.linear_model._ridge._X_CenterStackOp._transpose(self)
sklearn.linear_model._ridge._check_gcv_mode(X,gcv_mode)
sklearn.linear_model._ridge._find_smallest_angle(query,vectors)
sklearn.linear_model._ridge._get_rescaled_operator(X,X_offset,sample_weight_sqrt)
sklearn.linear_model._ridge._get_valid_accept_sparse(is_X_sparse,solver)
sklearn.linear_model._ridge._ridge_regression(X,y,alpha,sample_weight=None,solver='auto',max_iter=None,tol=0.0001,verbose=0,positive=False,random_state=None,return_n_iter=False,return_intercept=False,X_scale=None,X_offset=None,check_input=True,fit_intercept=False)
sklearn.linear_model._ridge._solve_cholesky(X,y,alpha)
sklearn.linear_model._ridge._solve_cholesky_kernel(K,y,alpha,sample_weight=None,copy=False)
sklearn.linear_model._ridge._solve_lbfgs(X,y,alpha,positive=True,max_iter=None,tol=0.0001,X_offset=None,X_scale=None,sample_weight_sqrt=None)
sklearn.linear_model._ridge._solve_lsqr(X,y,*,alpha,fit_intercept=True,max_iter=None,tol=0.0001,X_offset=None,X_scale=None,sample_weight_sqrt=None)
sklearn.linear_model._ridge._solve_sparse_cg(X,y,alpha,max_iter=None,tol=0.0001,verbose=0,X_offset=None,X_scale=None,sample_weight_sqrt=None)
sklearn.linear_model._ridge._solve_svd(X,y,alpha)
sklearn.linear_model._ridge.ridge_regression(X,y,alpha,*,sample_weight=None,solver='auto',max_iter=None,tol=0.0001,verbose=0,positive=False,random_state=None,return_n_iter=False,return_intercept=False,check_input=True)
sklearn.linear_model.ridge_regression(X,y,alpha,*,sample_weight=None,solver='auto',max_iter=None,tol=0.0001,verbose=0,positive=False,random_state=None,return_n_iter=False,return_intercept=False,check_input=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_passive_aggressive.py----------------------------------------
sklearn.linear_model.PassiveAggressiveClassifier(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='hinge',n_jobs=None,random_state=None,warm_start=False,class_weight=None,average=False)
sklearn.linear_model.PassiveAggressiveClassifier.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.PassiveAggressiveClassifier.partial_fit(self,X,y,classes=None)
sklearn.linear_model.PassiveAggressiveRegressor(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False)
sklearn.linear_model.PassiveAggressiveRegressor.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model.PassiveAggressiveRegressor.partial_fit(self,X,y)
sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='hinge',n_jobs=None,random_state=None,warm_start=False,class_weight=None,average=False)
sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier.__init__(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='hinge',n_jobs=None,random_state=None,warm_start=False,class_weight=None,average=False)
sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier.partial_fit(self,X,y,classes=None)
sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False)
sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor.__init__(self,*,C=1.0,fit_intercept=True,max_iter=1000,tol=0.001,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,shuffle=True,verbose=0,loss='epsilon_insensitive',epsilon=DEFAULT_EPSILON,random_state=None,warm_start=False,average=False)
sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor.fit(self,X,y,coef_init=None,intercept_init=None)
sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor.partial_fit(self,X,y)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py----------------------------------------
A:sklearn.linear_model._logistic.Cs->numpy.logspace(-4, 4, Cs)
A:sklearn.linear_model._logistic.solver->_check_solver(self.solver, self.penalty, self.dual)
A:sklearn.linear_model._logistic.X->check_array(X, accept_sparse='csr', dtype=np.float64, accept_large_sparse=solver not in ['liblinear', 'sag', 'saga'])
A:sklearn.linear_model._logistic.y->LabelEncoder().fit(y).transform(y)
A:sklearn.linear_model._logistic.classes->numpy.unique(y)
A:sklearn.linear_model._logistic.random_state->check_random_state(random_state)
A:sklearn.linear_model._logistic.multi_class->_check_multi_class(self.multi_class, solver, len(classes))
A:sklearn.linear_model._logistic.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.linear_model._logistic.le->LabelEncoder()
A:sklearn.linear_model._logistic.class_weight_->compute_class_weight(class_weight, classes=mask_classes, y=y_bin)
A:sklearn.linear_model._logistic.w0->coef_.ravel()
A:sklearn.linear_model._logistic.y_bin->numpy.ones(y.shape, dtype=X.dtype)
A:sklearn.linear_model._logistic.mask_classes->numpy.array([-1, 1])
A:sklearn.linear_model._logistic.Y_multi->numpy.hstack([1 - Y_multi, Y_multi])
A:sklearn.linear_model._logistic.lbin->LabelBinarizer()
A:sklearn.linear_model._logistic.loss->LinearModelLoss(base_loss=HalfBinomialLoss(), fit_intercept=fit_intercept)
A:sklearn.linear_model._logistic.coefs->list()
A:sklearn.linear_model._logistic.n_iter->numpy.zeros(len(Cs), dtype=np.int32)
A:sklearn.linear_model._logistic.opt_res->scipy.optimize.minimize(func, w0, method='L-BFGS-B', jac=True, args=(X, target, sample_weight, l2_reg_strength, n_threads), options={'maxiter': max_iter, 'maxls': 50, 'iprint': iprint, 'gtol': tol, 'ftol': 64 * np.finfo(float).eps})
A:sklearn.linear_model._logistic.n_iter_i->n_iter_i.item().item()
A:sklearn.linear_model._logistic.(w0, n_iter_i)->_newton_cg(hess, func, grad, w0, args=args, maxiter=max_iter, tol=tol)
A:sklearn.linear_model._logistic.sol->NewtonCholeskySolver(coef=w0, linear_loss=loss, l2_reg_strength=l2_reg_strength, tol=tol, max_iter=max_iter, n_threads=n_threads, verbose=verbose)
A:sklearn.linear_model._logistic.(coef_, intercept_, n_iter_i)->_fit_liblinear(X, target, C, fit_intercept, intercept_scaling, None, penalty, dual, verbose, max_iter, tol, random_state, sample_weight=sample_weight)
A:sklearn.linear_model._logistic.target->target.astype(X.dtype, copy=False).astype(X.dtype, copy=False)
A:sklearn.linear_model._logistic.(w0, n_iter_i, warm_start_sag)->sag_solver(X, target, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, False, max_squared_sum, warm_start_sag, is_saga=solver == 'saga')
A:sklearn.linear_model._logistic.n_classes->len(encoded_labels)
A:sklearn.linear_model._logistic.multi_w0->numpy.reshape(w0, (n_classes, -1), order='F')
A:sklearn.linear_model._logistic.(coefs, Cs, n_iter)->_logistic_regression_path(X_train, y_train, Cs=Cs, l1_ratio=l1_ratio, fit_intercept=fit_intercept, solver=solver, max_iter=max_iter, class_weight=class_weight, pos_class=pos_class, multi_class=multi_class, tol=tol, verbose=verbose, dual=dual, penalty=penalty, intercept_scaling=intercept_scaling, random_state=random_state, check_input=False, max_squared_sum=max_squared_sum, sample_weight=sample_weight)
A:sklearn.linear_model._logistic.log_reg->LogisticRegression(solver=solver, multi_class=multi_class)
A:sklearn.linear_model._logistic.log_reg.classes_->numpy.unique(y_train)
A:sklearn.linear_model._logistic.y_test->numpy.ones(y_test.shape, dtype=np.float64)
A:sklearn.linear_model._logistic.scores->numpy.reshape(scores, (n_classes, len(folds), -1))
A:sklearn.linear_model._logistic.scoring->self._get_scorer()
A:sklearn.linear_model._logistic.score_params->_check_method_params(X=X, params=score_params, indices=test)
A:sklearn.linear_model._logistic.(X, y)->self._validate_data(X, y, accept_sparse='csr', dtype=np.float64, order='C', accept_large_sparse=solver not in ['liblinear', 'sag', 'saga'])
A:sklearn.linear_model._logistic.self.classes_->numpy.unique(y)
A:sklearn.linear_model._logistic.(self.coef_, self.intercept_, self.n_iter_)->_fit_liblinear(X, y, self.C, self.fit_intercept, self.intercept_scaling, self.class_weight, self.penalty, self.dual, self.verbose, self.max_iter, self.tol, self.random_state, sample_weight=sample_weight)
A:sklearn.linear_model._logistic.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model._logistic.warm_start_coef->numpy.append(warm_start_coef, self.intercept_[:, np.newaxis], axis=1)
A:sklearn.linear_model._logistic.path_func->delayed(_log_reg_scoring_path)
A:sklearn.linear_model._logistic.fold_coefs_->Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)((path_func(X, y, train, test, pos_class=label, Cs=self.Cs, fit_intercept=self.fit_intercept, penalty=self.penalty, dual=self.dual, solver=solver, tol=self.tol, max_iter=self.max_iter, verbose=self.verbose, class_weight=class_weight, scoring=self.scoring, multi_class=multi_class, intercept_scaling=self.intercept_scaling, random_state=self.random_state, max_squared_sum=max_squared_sum, sample_weight=sample_weight, l1_ratio=l1_ratio, score_params=routed_params.scorer.score) for label in iter_encoded_labels for (train, test) in folds for l1_ratio in l1_ratios_))
A:sklearn.linear_model._logistic.(fold_coefs_, _, n_iter_)->zip(*fold_coefs_)
A:sklearn.linear_model._logistic.self.coef_->numpy.empty((n_classes, X.shape[1]))
A:sklearn.linear_model._logistic.self.intercept_->numpy.zeros(n_classes)
A:sklearn.linear_model._logistic.decision->self.decision_function(X)
A:sklearn.linear_model._logistic.label_encoder->LabelEncoder().fit(y)
A:sklearn.linear_model._logistic.encoded_labels->LabelEncoder().fit(y).transform(label_encoder.classes_)
A:sklearn.linear_model._logistic.routed_params->Bunch()
A:sklearn.linear_model._logistic.routed_params.splitter->Bunch(split={})
A:sklearn.linear_model._logistic.routed_params.scorer->Bunch(score={})
A:sklearn.linear_model._logistic.cv->check_cv(self.cv, y, classifier=True)
A:sklearn.linear_model._logistic.folds->list(cv.split(X, y, **routed_params.splitter.split))
A:sklearn.linear_model._logistic.class_weight->dict(enumerate(class_weight))
A:sklearn.linear_model._logistic.(coefs_paths, Cs, scores, n_iter_)->zip(*fold_coefs_)
A:sklearn.linear_model._logistic.coefs_paths->numpy.reshape(coefs_paths, (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_), -1))
A:sklearn.linear_model._logistic.self.n_iter_->numpy.transpose(self.n_iter_, (0, 1, 3, 2))
A:sklearn.linear_model._logistic.self.scores_->dict(zip(classes, scores))
A:sklearn.linear_model._logistic.self.coefs_paths_->dict(zip(classes, coefs_paths))
A:sklearn.linear_model._logistic.self.C_->numpy.asarray(self.C_)
A:sklearn.linear_model._logistic.self.l1_ratio_->numpy.asarray(self.l1_ratio_)
A:sklearn.linear_model._logistic.best_index->numpy.reshape(scores, (n_classes, len(folds), -1)).sum(axis=0).argmax()
A:sklearn.linear_model._logistic.coef_init->numpy.mean(coefs_paths[:, best_index, :], axis=0)
A:sklearn.linear_model._logistic.(w, _, _)->_logistic_regression_path(X, y, pos_class=encoded_label, Cs=[C_], solver=solver, fit_intercept=self.fit_intercept, coef=coef_init, max_iter=self.max_iter, tol=self.tol, penalty=self.penalty, class_weight=class_weight, multi_class=multi_class, verbose=max(0, self.verbose - 1), random_state=self.random_state, check_input=False, max_squared_sum=max_squared_sum, sample_weight=sample_weight, l1_ratio=l1_ratio_)
A:sklearn.linear_model._logistic.best_indices->numpy.argmax(scores, axis=1)
A:sklearn.linear_model._logistic.w->numpy.mean([coefs_paths[:, i, best_indices[i], :] for i in range(len(folds))], axis=0)
A:sklearn.linear_model._logistic.self.l1_ratios_->numpy.asarray(l1_ratios_)
A:sklearn.linear_model._logistic.self.coefs_paths_[cls]->numpy.transpose(self.coefs_paths_[cls], (0, 2, 1, 3))
A:sklearn.linear_model._logistic.self.scores_[cls]->numpy.transpose(self.scores_[cls], (0, 2, 1))
A:sklearn.linear_model._logistic.router->MetadataRouter(owner=self.__class__.__name__).add_self_request(self).add(splitter=self.cv, method_mapping=MethodMapping().add(callee='split', caller='fit')).add(scorer=self._get_scorer(), method_mapping=MethodMapping().add(callee='score', caller='score').add(callee='score', caller='fit'))
sklearn.linear_model.LogisticRegression(self,penalty='l2',*,dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='lbfgs',max_iter=100,multi_class='auto',verbose=0,warm_start=False,n_jobs=None,l1_ratio=None)
sklearn.linear_model.LogisticRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model.LogisticRegression.predict_log_proba(self,X)
sklearn.linear_model.LogisticRegression.predict_proba(self,X)
sklearn.linear_model.LogisticRegressionCV(self,*,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=None,verbose=0,refit=True,intercept_scaling=1.0,multi_class='auto',random_state=None,l1_ratios=None)
sklearn.linear_model.LogisticRegressionCV._get_scorer(self)
sklearn.linear_model.LogisticRegressionCV._more_tags(self)
sklearn.linear_model.LogisticRegressionCV.fit(self,X,y,sample_weight=None,**params)
sklearn.linear_model.LogisticRegressionCV.get_metadata_routing(self)
sklearn.linear_model.LogisticRegressionCV.score(self,X,y,sample_weight=None,**score_params)
sklearn.linear_model._logistic.LogisticRegression(self,penalty='l2',*,dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='lbfgs',max_iter=100,multi_class='auto',verbose=0,warm_start=False,n_jobs=None,l1_ratio=None)
sklearn.linear_model._logistic.LogisticRegression.__init__(self,penalty='l2',*,dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='lbfgs',max_iter=100,multi_class='auto',verbose=0,warm_start=False,n_jobs=None,l1_ratio=None)
sklearn.linear_model._logistic.LogisticRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model._logistic.LogisticRegression.predict_log_proba(self,X)
sklearn.linear_model._logistic.LogisticRegression.predict_proba(self,X)
sklearn.linear_model._logistic.LogisticRegressionCV(self,*,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=None,verbose=0,refit=True,intercept_scaling=1.0,multi_class='auto',random_state=None,l1_ratios=None)
sklearn.linear_model._logistic.LogisticRegressionCV.__init__(self,*,Cs=10,fit_intercept=True,cv=None,dual=False,penalty='l2',scoring=None,solver='lbfgs',tol=0.0001,max_iter=100,class_weight=None,n_jobs=None,verbose=0,refit=True,intercept_scaling=1.0,multi_class='auto',random_state=None,l1_ratios=None)
sklearn.linear_model._logistic.LogisticRegressionCV._get_scorer(self)
sklearn.linear_model._logistic.LogisticRegressionCV._more_tags(self)
sklearn.linear_model._logistic.LogisticRegressionCV.fit(self,X,y,sample_weight=None,**params)
sklearn.linear_model._logistic.LogisticRegressionCV.get_metadata_routing(self)
sklearn.linear_model._logistic.LogisticRegressionCV.score(self,X,y,sample_weight=None,**score_params)
sklearn.linear_model._logistic._check_multi_class(multi_class,solver,n_classes)
sklearn.linear_model._logistic._check_solver(solver,penalty,dual)
sklearn.linear_model._logistic._log_reg_scoring_path(X,y,train,test,*,pos_class,Cs,scoring,fit_intercept,max_iter,tol,class_weight,verbose,solver,penalty,dual,intercept_scaling,multi_class,random_state,max_squared_sum,sample_weight,l1_ratio,score_params)
sklearn.linear_model._logistic._logistic_regression_path(X,y,pos_class=None,Cs=10,fit_intercept=True,max_iter=100,tol=0.0001,verbose=0,solver='lbfgs',coef=None,class_weight=None,dual=False,penalty='l2',intercept_scaling=1.0,multi_class='auto',random_state=None,check_input=True,max_squared_sum=None,sample_weight=None,l1_ratio=None,n_threads=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_quantile.py----------------------------------------
A:sklearn.linear_model._quantile.(X, y)->self._validate_data(X, y, accept_sparse=['csc', 'csr', 'coo'], y_numeric=True, multi_output=False)
A:sklearn.linear_model._quantile.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.linear_model._quantile.n_indices->len(indices)
A:sklearn.linear_model._quantile.X->_safe_indexing(X, indices)
A:sklearn.linear_model._quantile.y->_safe_indexing(y, indices)
A:sklearn.linear_model._quantile.c->numpy.concatenate([np.full(2 * n_params, fill_value=alpha), sample_weight * self.quantile, sample_weight * (1 - self.quantile)])
A:sklearn.linear_model._quantile.eye->numpy.eye(n_indices)
A:sklearn.linear_model._quantile.ones->numpy.ones((n_indices, 1))
A:sklearn.linear_model._quantile.A_eq->numpy.concatenate([X, -X, eye, -eye], axis=1)
A:sklearn.linear_model._quantile.result->linprog(c=c, A_eq=A_eq, b_eq=b_eq, method=solver, options=solver_options)
sklearn.linear_model.QuantileRegressor(self,*,quantile=0.5,alpha=1.0,fit_intercept=True,solver='highs',solver_options=None)
sklearn.linear_model.QuantileRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._quantile.QuantileRegressor(self,*,quantile=0.5,alpha=1.0,fit_intercept=True,solver='highs',solver_options=None)
sklearn.linear_model._quantile.QuantileRegressor.__init__(self,*,quantile=0.5,alpha=1.0,fit_intercept=True,solver='highs',solver_options=None)
sklearn.linear_model._quantile.QuantileRegressor.fit(self,X,y,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_perceptron.py----------------------------------------
sklearn.linear_model.Perceptron(self,*,penalty=None,alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,eta0=1.0,n_jobs=None,random_state=0,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False)
sklearn.linear_model._perceptron.Perceptron(self,*,penalty=None,alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,eta0=1.0,n_jobs=None,random_state=0,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False)
sklearn.linear_model._perceptron.Perceptron.__init__(self,*,penalty=None,alpha=0.0001,l1_ratio=0.15,fit_intercept=True,max_iter=1000,tol=0.001,shuffle=True,verbose=0,eta0=1.0,n_jobs=None,random_state=0,early_stopping=False,validation_fraction=0.1,n_iter_no_change=5,class_weight=None,warm_start=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_huber.py----------------------------------------
A:sklearn.linear_model._huber.n_samples->numpy.sum(sample_weight)
A:sklearn.linear_model._huber.abs_linear_loss->numpy.abs(linear_loss)
A:sklearn.linear_model._huber.num_outliers->numpy.count_nonzero(outliers_mask)
A:sklearn.linear_model._huber.n_sw_outliers->numpy.sum(outliers_sw)
A:sklearn.linear_model._huber.weighted_loss->numpy.dot(weighted_non_outliers.T, non_outliers)
A:sklearn.linear_model._huber.grad->numpy.zeros(n_features + 1)
A:sklearn.linear_model._huber.signed_outliers->numpy.ones_like(outliers)
A:sklearn.linear_model._huber.X_outliers->axis0_safe_slice(X, outliers_mask, num_outliers)
A:sklearn.linear_model._huber.(X, y)->self._validate_data(X, y, copy=False, accept_sparse=['csr'], y_numeric=True, dtype=[np.float64, np.float32])
A:sklearn.linear_model._huber.sample_weight->_check_sample_weight(sample_weight, X)
A:sklearn.linear_model._huber.parameters->numpy.zeros(X.shape[1] + 1)
A:sklearn.linear_model._huber.bounds->numpy.tile([-np.inf, np.inf], (parameters.shape[0], 1))
A:sklearn.linear_model._huber.opt_res->scipy.optimize.minimize(_huber_loss_and_gradient, parameters, method='L-BFGS-B', jac=True, args=(X, y, self.epsilon, self.alpha, sample_weight), options={'maxiter': self.max_iter, 'gtol': self.tol, 'iprint': -1}, bounds=bounds)
A:sklearn.linear_model._huber.self.n_iter_->_check_optimize_result('lbfgs', opt_res, self.max_iter)
A:sklearn.linear_model._huber.residual->numpy.abs(y - safe_sparse_dot(X, self.coef_) - self.intercept_)
sklearn.linear_model.HuberRegressor(self,*,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model.HuberRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._huber.HuberRegressor(self,*,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model._huber.HuberRegressor.__init__(self,*,epsilon=1.35,max_iter=100,alpha=0.0001,warm_start=False,fit_intercept=True,tol=1e-05)
sklearn.linear_model._huber.HuberRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._huber._huber_loss_and_gradient(w,X,y,epsilon,alpha,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_base.py----------------------------------------
A:sklearn.linear_model._base.rng->check_random_state(random_state)
A:sklearn.linear_model._base.seed->check_random_state(random_state).randint(1, np.iinfo(np.int32).max)
A:sklearn.linear_model._base.dataset->ArrayData(X, y, sample_weight, seed=seed)
A:sklearn.linear_model._base.X->self._validate_data(X, accept_sparse='csr', reset=False)
A:sklearn.linear_model._base.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype, only_non_negative=True)
A:sklearn.linear_model._base.y->safe_sparse_dot(sw_matrix, y)
A:sklearn.linear_model._base.(X_offset, X_var)->mean_variance_axis(X, axis=0, weights=sample_weight)
A:sklearn.linear_model._base.X_offset->numpy.zeros(X.shape[1], dtype=X.dtype)
A:sklearn.linear_model._base.y_offset->numpy.zeros(y.shape[1], dtype=X.dtype)
A:sklearn.linear_model._base.X_scale->numpy.ones(X.shape[1], dtype=X.dtype)
A:sklearn.linear_model._base.sample_weight_sqrt->numpy.sqrt(sample_weight)
A:sklearn.linear_model._base.sw_matrix->scipy.sparse.dia_matrix((sample_weight_sqrt, 0), shape=(n_samples, n_samples))
A:sklearn.linear_model._base.self.coef_->numpy.ravel(self.coef_)
A:sklearn.linear_model._base.(xp, _)->get_namespace(X)
A:sklearn.linear_model._base.scores->self.decision_function(X)
A:sklearn.linear_model._base.indices->xp.argmax(scores, axis=1)
A:sklearn.linear_model._base.prob->self.decision_function(X)
A:sklearn.linear_model._base.(X, y)->self._validate_data(X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True)
A:sklearn.linear_model._base.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, fit_intercept=fit_intercept, copy=copy, check_input=check_input, sample_weight=sample_weight)
A:sklearn.linear_model._base.(X, y, sample_weight_sqrt)->_rescale_data(X, y, sample_weight, inplace=copy_X_in_preprocess_data)
A:sklearn.linear_model._base.outs->Parallel(n_jobs=n_jobs_)((delayed(lsqr)(X_centered, y[:, j].ravel()) for j in range(y.shape[1])))
A:sklearn.linear_model._base.X_centered->scipy.sparse.linalg.LinearOperator(shape=X.shape, matvec=matvec, rmatvec=rmatvec)
A:sklearn.linear_model._base.(self.coef_, _, self.rank_, self.singular_)->scipy.linalg.lstsq(X, y)
A:sklearn.linear_model._base.f2->min(f1 + 1, n_features - 1)
A:sklearn.linear_model._base.expected->numpy.dot(v1, v2)
A:sklearn.linear_model._base.rtol->max(rtols)
A:sklearn.linear_model._base.(X, y, _)->_rescale_data(X, y, sample_weight=sample_weight)
A:sklearn.linear_model._base.precompute->numpy.empty(shape=(n_features, n_features), dtype=X.dtype, order='C')
A:sklearn.linear_model._base.common_dtype->numpy.result_type(X.dtype, y.dtype)
A:sklearn.linear_model._base.Xy->numpy.empty(shape=(n_features, n_targets), dtype=common_dtype, order='F')
sklearn.linear_model.LinearRegression(self,*,fit_intercept=True,copy_X=True,n_jobs=None,positive=False)
sklearn.linear_model.LinearRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model._base.LinearClassifierMixin(ClassifierMixin)
sklearn.linear_model._base.LinearClassifierMixin._predict_proba_lr(self,X)
sklearn.linear_model._base.LinearClassifierMixin.decision_function(self,X)
sklearn.linear_model._base.LinearClassifierMixin.predict(self,X)
sklearn.linear_model._base.LinearModel(BaseEstimator,metaclass=ABCMeta)
sklearn.linear_model._base.LinearModel._decision_function(self,X)
sklearn.linear_model._base.LinearModel._more_tags(self)
sklearn.linear_model._base.LinearModel._set_intercept(self,X_offset,y_offset,X_scale)
sklearn.linear_model._base.LinearModel.fit(self,X,y)
sklearn.linear_model._base.LinearModel.predict(self,X)
sklearn.linear_model._base.LinearRegression(self,*,fit_intercept=True,copy_X=True,n_jobs=None,positive=False)
sklearn.linear_model._base.LinearRegression.__init__(self,*,fit_intercept=True,copy_X=True,n_jobs=None,positive=False)
sklearn.linear_model._base.LinearRegression.fit(self,X,y,sample_weight=None)
sklearn.linear_model._base.SparseCoefMixin
sklearn.linear_model._base.SparseCoefMixin.densify(self)
sklearn.linear_model._base.SparseCoefMixin.sparsify(self)
sklearn.linear_model._base._check_precomputed_gram_matrix(X,precompute,X_offset,X_scale,rtol=None,atol=1e-05)
sklearn.linear_model._base._pre_fit(X,y,Xy,precompute,fit_intercept,copy,check_input=True,sample_weight=None)
sklearn.linear_model._base._preprocess_data(X,y,*,fit_intercept,copy=True,copy_y=True,sample_weight=None,check_input=True)
sklearn.linear_model._base._rescale_data(X,y,sample_weight,inplace=False)
sklearn.linear_model._base.make_dataset(X,y,sample_weight,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_theil_sen.py----------------------------------------
A:sklearn.linear_model._theil_sen.diff_norm->numpy.sqrt(np.sum(diff ** 2, axis=1))
A:sklearn.linear_model._theil_sen.is_x_old_in_X->int(mask.sum() < X.shape[0])
A:sklearn.linear_model._theil_sen.quotient_norm->scipy.linalg.norm(np.sum(diff / diff_norm, axis=0))
A:sklearn.linear_model._theil_sen.spatial_median_old->numpy.mean(X, axis=0)
A:sklearn.linear_model._theil_sen.spatial_median->_modified_weiszfeld_step(X, spatial_median_old)
A:sklearn.linear_model._theil_sen.fit_intercept->int(fit_intercept)
A:sklearn.linear_model._theil_sen.weights->numpy.vstack(weights)
A:sklearn.linear_model._theil_sen.X_subpopulation->numpy.ones((n_subsamples, n_features))
A:sklearn.linear_model._theil_sen.y_subpopulation->numpy.zeros(max(n_subsamples, n_features))
A:sklearn.linear_model._theil_sen.(lstsq,)->get_lapack_funcs(('gelss',), (X_subpopulation, y_subpopulation))
A:sklearn.linear_model._theil_sen.n_subsamples->min(n_dim, n_samples)
A:sklearn.linear_model._theil_sen.all_combinations->max(1, np.rint(binom(n_samples, n_subsamples)))
A:sklearn.linear_model._theil_sen.n_subpopulation->int(min(self.max_subpopulation, all_combinations))
A:sklearn.linear_model._theil_sen.random_state->check_random_state(self.random_state)
A:sklearn.linear_model._theil_sen.(X, y)->self._validate_data(X, y, y_numeric=True)
A:sklearn.linear_model._theil_sen.(n_subsamples, self.n_subpopulation_)->self._check_subparams(n_samples, n_features)
A:sklearn.linear_model._theil_sen.self.breakdown_->_breakdown_point(n_samples, n_subsamples)
A:sklearn.linear_model._theil_sen.tol_outliers->int(self.breakdown_ * n_samples)
A:sklearn.linear_model._theil_sen.indices->list(combinations(range(n_samples), n_subsamples))
A:sklearn.linear_model._theil_sen.n_jobs->effective_n_jobs(self.n_jobs)
A:sklearn.linear_model._theil_sen.index_list->numpy.array_split(indices, n_jobs)
A:sklearn.linear_model._theil_sen.(self.n_iter_, coefs)->_spatial_median(weights, max_iter=self.max_iter, tol=self.tol)
sklearn.linear_model.TheilSenRegressor(self,*,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=None,verbose=False)
sklearn.linear_model.TheilSenRegressor._check_subparams(self,n_samples,n_features)
sklearn.linear_model.TheilSenRegressor.fit(self,X,y)
sklearn.linear_model._theil_sen.TheilSenRegressor(self,*,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=None,verbose=False)
sklearn.linear_model._theil_sen.TheilSenRegressor.__init__(self,*,fit_intercept=True,copy_X=True,max_subpopulation=10000.0,n_subsamples=None,max_iter=300,tol=0.001,random_state=None,n_jobs=None,verbose=False)
sklearn.linear_model._theil_sen.TheilSenRegressor._check_subparams(self,n_samples,n_features)
sklearn.linear_model._theil_sen.TheilSenRegressor.fit(self,X,y)
sklearn.linear_model._theil_sen._breakdown_point(n_samples,n_subsamples)
sklearn.linear_model._theil_sen._lstsq(X,y,indices,fit_intercept)
sklearn.linear_model._theil_sen._modified_weiszfeld_step(X,x_old)
sklearn.linear_model._theil_sen._spatial_median(X,max_iter=300,tol=0.001)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_sag.py----------------------------------------
A:sklearn.linear_model._sag.mun->min(2 * n_samples * alpha_scaled, L)
A:sklearn.linear_model._sag.X->check_array(X, dtype=_dtype, accept_sparse='csr', order='C')
A:sklearn.linear_model._sag.y->check_array(y, dtype=_dtype, ensure_2d=False, order='C')
A:sklearn.linear_model._sag.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.linear_model._sag.coef_init->numpy.vstack((coef_init, intercept_init))
A:sklearn.linear_model._sag.intercept_init->numpy.zeros(n_classes, dtype=X.dtype)
A:sklearn.linear_model._sag.intercept_sum_gradient->numpy.zeros(n_classes, dtype=X.dtype)
A:sklearn.linear_model._sag.gradient_memory_init->numpy.zeros((n_samples, n_classes), dtype=X.dtype, order='C')
A:sklearn.linear_model._sag.sum_gradient_init->numpy.zeros((n_features, n_classes), dtype=X.dtype, order='C')
A:sklearn.linear_model._sag.seen_init->numpy.zeros(n_samples, dtype=np.int32, order='C')
A:sklearn.linear_model._sag.(dataset, intercept_decay)->make_dataset(X, y, sample_weight, random_state)
A:sklearn.linear_model._sag.max_squared_sum->row_norms(X, squared=True).max()
A:sklearn.linear_model._sag.step_size->get_auto_step_size(max_squared_sum, alpha_scaled, loss, fit_intercept, n_samples=n_samples, is_saga=is_saga)
A:sklearn.linear_model._sag.(num_seen, n_iter_)->sag(dataset, coef_init, intercept_init, n_samples, n_features, n_classes, tol, max_iter, loss, step_size, alpha_scaled, beta_scaled, sum_gradient_init, gradient_memory_init, seen_init, num_seen_init, fit_intercept, intercept_sum_gradient, intercept_decay, is_saga, verbose)
sklearn.linear_model._sag.get_auto_step_size(max_squared_sum,alpha_scaled,loss,fit_intercept,n_samples=None,is_saga=False)
sklearn.linear_model._sag.sag_solver(X,y,sample_weight=None,loss='log',alpha=1.0,beta=0.0,max_iter=1000,tol=0.001,verbose=0,random_state=None,check_input=True,max_squared_sum=None,warm_start_mem=None,is_saga=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py----------------------------------------
A:sklearn.linear_model._coordinate_descent.sparse_X->scipy.sparse.issparse(X)
A:sklearn.linear_model._coordinate_descent.sparse_y->scipy.sparse.issparse(y)
A:sklearn.linear_model._coordinate_descent.X->check_array(X, accept_sparse='csc', dtype=[np.float64, np.float32], order='F', copy=copy_X)
A:sklearn.linear_model._coordinate_descent.y->y.astype(X.dtype).astype(X.dtype)
A:sklearn.linear_model._coordinate_descent.n_samples->len(y)
A:sklearn.linear_model._coordinate_descent.X_sparse->scipy.sparse.issparse(X)
A:sklearn.linear_model._coordinate_descent.(X, y, _, _, _)->_preprocess_data(X, y, fit_intercept=fit_intercept, copy=False)
A:sklearn.linear_model._coordinate_descent.Xy->check_array(Xy, dtype=X.dtype.type, order='C', copy=False, ensure_2d=False)
A:sklearn.linear_model._coordinate_descent.(_, _, X_offset, _, X_scale)->_preprocess_data(X, y, fit_intercept=fit_intercept)
A:sklearn.linear_model._coordinate_descent.alphas->numpy.tile(np.sort(alphas)[::-1], (n_l1_ratio, 1))
A:sklearn.linear_model._coordinate_descent.X_offset_param->params.pop('X_offset', None)
A:sklearn.linear_model._coordinate_descent.X_scale_param->params.pop('X_scale', None)
A:sklearn.linear_model._coordinate_descent.sample_weight->_check_sample_weight(sample_weight, X, dtype=X.dtype)
A:sklearn.linear_model._coordinate_descent.tol->params.pop('tol', 0.0001)
A:sklearn.linear_model._coordinate_descent.max_iter->params.pop('max_iter', 1000)
A:sklearn.linear_model._coordinate_descent.random_state->params.pop('random_state', None)
A:sklearn.linear_model._coordinate_descent.selection->params.pop('selection', 'cyclic')
A:sklearn.linear_model._coordinate_descent.X_sparse_scaling->numpy.zeros(n_features, dtype=X.dtype)
A:sklearn.linear_model._coordinate_descent.(X, y, _, _, _, precompute, Xy)->_pre_fit(X, y, Xy, precompute, fit_intercept=False, copy=False, check_input=check_input)
A:sklearn.linear_model._coordinate_descent.n_alphas->len(alphas[0])
A:sklearn.linear_model._coordinate_descent.dual_gaps->numpy.empty(n_alphas)
A:sklearn.linear_model._coordinate_descent.rng->check_random_state(random_state)
A:sklearn.linear_model._coordinate_descent.coefs->numpy.empty((n_targets, n_features, n_alphas), dtype=X.dtype)
A:sklearn.linear_model._coordinate_descent.coef_->numpy.zeros((n_targets, n_features), dtype=X.dtype, order='F')
A:sklearn.linear_model._coordinate_descent.model->self._get_estimator()
A:sklearn.linear_model._coordinate_descent.precompute->getattr(self, 'precompute', None)
A:sklearn.linear_model._coordinate_descent.path->staticmethod(lasso_path)
A:sklearn.linear_model._coordinate_descent.(X, y)->self._validate_data(X, y, validate_separately=(check_X_params, check_y_params))
A:sklearn.linear_model._coordinate_descent.(X, y, X_offset, y_offset, X_scale, precompute, Xy)->_pre_fit(X, y, None, self.precompute, fit_intercept=self.fit_intercept, copy=should_copy, check_input=check_input, sample_weight=sample_weight)
A:sklearn.linear_model._coordinate_descent.dual_gaps_->numpy.zeros(n_targets, dtype=X.dtype)
A:sklearn.linear_model._coordinate_descent.(_, this_coef, this_dual_gap, this_iter)->self.path(X, y[:, k], l1_ratio=self.l1_ratio, eps=None, n_alphas=None, alphas=[alpha], precompute=precompute, Xy=this_Xy, copy_X=True, coef_init=coef_[k], verbose=False, return_n_iter=True, positive=self.positive, check_input=False, tol=self.tol, X_offset=X_offset, X_scale=X_scale, max_iter=self.max_iter, random_state=self.random_state, selection=self.selection, sample_weight=sample_weight)
A:sklearn.linear_model._coordinate_descent.(X_train, y_train, X_offset, y_offset, X_scale, precompute, Xy)->_pre_fit(X_train, y_train, None, precompute, fit_intercept=fit_intercept, copy=False, sample_weight=sw_train)
A:sklearn.linear_model._coordinate_descent.path_params->self.get_params()
A:sklearn.linear_model._coordinate_descent.X_train->check_array(X_train, accept_sparse='csc', dtype=dtype, order=X_order)
A:sklearn.linear_model._coordinate_descent.(alphas, coefs, _)->path(X_train, y_train, **path_params)
A:sklearn.linear_model._coordinate_descent.y_offset->numpy.atleast_1d(y_offset)
A:sklearn.linear_model._coordinate_descent.X_test_coefs->safe_sparse_dot(X_test, coefs)
A:sklearn.linear_model._coordinate_descent.this_mse->numpy.average(residues ** 2, weights=sw_test, axis=0)
A:sklearn.linear_model._coordinate_descent.check_y_params->dict(ensure_2d=False, order='F')
A:sklearn.linear_model._coordinate_descent.check_X_params->dict(dtype=[np.float64, np.float32], order='F', copy=self.copy_X and self.fit_intercept)
A:sklearn.linear_model._coordinate_descent.l1_ratios->numpy.atleast_1d(path_params['l1_ratio'])
A:sklearn.linear_model._coordinate_descent.n_l1_ratio->len(l1_ratios)
A:sklearn.linear_model._coordinate_descent.check_scalar_alpha->partial(check_scalar, target_type=Real, min_val=0.0, include_boundaries='left')
A:sklearn.linear_model._coordinate_descent.cv->check_cv(self.cv)
A:sklearn.linear_model._coordinate_descent.splitter_supports_sample_weight->get_routing_for_object(cv).consumes(method='split', params=['sample_weight'])
A:sklearn.linear_model._coordinate_descent.routed_params->Bunch()
A:sklearn.linear_model._coordinate_descent.routed_params.splitter->Bunch(split=Bunch())
A:sklearn.linear_model._coordinate_descent.folds->list(cv.split(X, y, **routed_params.splitter.split))
A:sklearn.linear_model._coordinate_descent.mse_paths->numpy.reshape(mse_paths, (n_l1_ratio, len(folds), -1))
A:sklearn.linear_model._coordinate_descent.mean_mse->numpy.mean(mse_paths, axis=1)
A:sklearn.linear_model._coordinate_descent.self.mse_path_->numpy.squeeze(np.moveaxis(mse_paths, 2, 1))
A:sklearn.linear_model._coordinate_descent.i_best_alpha->numpy.argmin(mse_alphas)
A:sklearn.linear_model._coordinate_descent.self.alphas_->numpy.asarray(alphas[0])
A:sklearn.linear_model._coordinate_descent.router->MetadataRouter(owner=self.__class__.__name__).add_self_request(self).add(splitter=check_cv(self.cv), method_mapping=MethodMapping().add(callee='split', caller='fit'))
A:sklearn.linear_model._coordinate_descent.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=False)
A:sklearn.linear_model._coordinate_descent.self.coef_->numpy.asfortranarray(self.coef_)
A:sklearn.linear_model._coordinate_descent.(self.coef_, self.dual_gap_, self.eps_, self.n_iter_)->cd_fast.enet_coordinate_descent_multi_task(self.coef_, l1_reg, l2_reg, X, y, self.max_iter, self.tol, check_random_state(self.random_state), random)
sklearn.linear_model.ElasticNet(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.ElasticNet._decision_function(self,X)
sklearn.linear_model.ElasticNet.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.linear_model.ElasticNet.sparse_coef_(self)
sklearn.linear_model.ElasticNetCV(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.ElasticNetCV._get_estimator(self)
sklearn.linear_model.ElasticNetCV._is_multitask(self)
sklearn.linear_model.ElasticNetCV._more_tags(self)
sklearn.linear_model.Lasso(self,alpha=1.0,*,fit_intercept=True,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.LassoCV(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model.LassoCV._get_estimator(self)
sklearn.linear_model.LassoCV._is_multitask(self)
sklearn.linear_model.LassoCV._more_tags(self)
sklearn.linear_model.MultiTaskElasticNet(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskElasticNet._more_tags(self)
sklearn.linear_model.MultiTaskElasticNet.fit(self,X,y)
sklearn.linear_model.MultiTaskElasticNetCV(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskElasticNetCV._get_estimator(self)
sklearn.linear_model.MultiTaskElasticNetCV._is_multitask(self)
sklearn.linear_model.MultiTaskElasticNetCV._more_tags(self)
sklearn.linear_model.MultiTaskElasticNetCV.fit(self,X,y,**params)
sklearn.linear_model.MultiTaskLasso(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskLassoCV(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model.MultiTaskLassoCV._get_estimator(self)
sklearn.linear_model.MultiTaskLassoCV._is_multitask(self)
sklearn.linear_model.MultiTaskLassoCV._more_tags(self)
sklearn.linear_model.MultiTaskLassoCV.fit(self,X,y,**params)
sklearn.linear_model._coordinate_descent.ElasticNet(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.ElasticNet.__init__(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,precompute=False,max_iter=1000,copy_X=True,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.ElasticNet._decision_function(self,X)
sklearn.linear_model._coordinate_descent.ElasticNet.fit(self,X,y,sample_weight=None,check_input=True)
sklearn.linear_model._coordinate_descent.ElasticNet.sparse_coef_(self)
sklearn.linear_model._coordinate_descent.ElasticNetCV(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.ElasticNetCV.__init__(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.ElasticNetCV._get_estimator(self)
sklearn.linear_model._coordinate_descent.ElasticNetCV._is_multitask(self)
sklearn.linear_model._coordinate_descent.ElasticNetCV._more_tags(self)
sklearn.linear_model._coordinate_descent.Lasso(self,alpha=1.0,*,fit_intercept=True,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.Lasso.__init__(self,alpha=1.0,*,fit_intercept=True,precompute=False,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.LassoCV(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.LassoCV.__init__(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.LassoCV._get_estimator(self)
sklearn.linear_model._coordinate_descent.LassoCV._is_multitask(self)
sklearn.linear_model._coordinate_descent.LassoCV._more_tags(self)
sklearn.linear_model._coordinate_descent.LinearModelCV(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.LinearModelCV.__init__(self,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,precompute='auto',max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,positive=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.LinearModelCV._get_estimator(self)
sklearn.linear_model._coordinate_descent.LinearModelCV._is_multitask(self)
sklearn.linear_model._coordinate_descent.LinearModelCV._more_tags(self)
sklearn.linear_model._coordinate_descent.LinearModelCV.fit(self,X,y,sample_weight=None,**params)
sklearn.linear_model._coordinate_descent.LinearModelCV.get_metadata_routing(self)
sklearn.linear_model._coordinate_descent.LinearModelCV.path(X,y,**kwargs)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNet(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskElasticNet.__init__(self,alpha=1.0,*,l1_ratio=0.5,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskElasticNet._more_tags(self)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNet.fit(self,X,y)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV.__init__(self,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,cv=None,copy_X=True,verbose=0,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV._get_estimator(self)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV._is_multitask(self)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV._more_tags(self)
sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV.fit(self,X,y,**params)
sklearn.linear_model._coordinate_descent.MultiTaskLasso(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskLasso.__init__(self,alpha=1.0,*,fit_intercept=True,copy_X=True,max_iter=1000,tol=0.0001,warm_start=False,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV.__init__(self,*,eps=0.001,n_alphas=100,alphas=None,fit_intercept=True,max_iter=1000,tol=0.0001,copy_X=True,cv=None,verbose=False,n_jobs=None,random_state=None,selection='cyclic')
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV._get_estimator(self)
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV._is_multitask(self)
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV._more_tags(self)
sklearn.linear_model._coordinate_descent.MultiTaskLassoCV.fit(self,X,y,**params)
sklearn.linear_model._coordinate_descent._alpha_grid(X,y,Xy=None,l1_ratio=1.0,fit_intercept=True,eps=0.001,n_alphas=100,copy_X=True)
sklearn.linear_model._coordinate_descent._path_residuals(X,y,sample_weight,train,test,fit_intercept,path,path_params,alphas=None,l1_ratio=1,X_order=None,dtype=None)
sklearn.linear_model._coordinate_descent._set_order(X,y,order='C')
sklearn.linear_model._coordinate_descent.enet_path(X,y,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,check_input=True,**params)
sklearn.linear_model._coordinate_descent.lasso_path(X,y,*,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,**params)
sklearn.linear_model.enet_path(X,y,*,l1_ratio=0.5,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,check_input=True,**params)
sklearn.linear_model.lasso_path(X,y,*,eps=0.001,n_alphas=100,alphas=None,precompute='auto',Xy=None,copy_X=True,coef_init=None,verbose=False,return_n_iter=False,positive=False,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py----------------------------------------
A:sklearn.linear_model._linear_loss.coef->numpy.zeros_like(X, shape=n_dof, dtype=dtype)
A:sklearn.linear_model._linear_loss.weights->numpy.zeros_like(X, shape=n_dof, dtype=dtype).reshape((self.base_loss.n_classes, -1), order='F')
A:sklearn.linear_model._linear_loss.(weights, intercept)->self.weight_intercept(coef)
A:sklearn.linear_model._linear_loss.(weights, intercept, raw_prediction)->self.weight_intercept_raw(coef, X)
A:sklearn.linear_model._linear_loss.loss->numpy.average(loss, weights=sample_weight)
A:sklearn.linear_model._linear_loss.(loss, grad_pointwise)->self.base_loss.loss_gradient(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads)
A:sklearn.linear_model._linear_loss.grad->numpy.empty((n_classes, n_dof), dtype=weights.dtype, order='F')
A:sklearn.linear_model._linear_loss.grad[-1]->self.base_loss.gradient(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads).sum()
A:sklearn.linear_model._linear_loss.grad[:, -1]->self.base_loss.gradient(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads).sum(axis=0)
A:sklearn.linear_model._linear_loss.grad_pointwise->self.base_loss.gradient(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads)
A:sklearn.linear_model._linear_loss.(grad_pointwise, hess_pointwise)->self.base_loss.gradient_hessian(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads)
A:sklearn.linear_model._linear_loss.hess_pointwise->numpy.abs(hess_pointwise)
A:sklearn.linear_model._linear_loss.hess->numpy.empty(shape=(n_dof, n_dof), dtype=weights.dtype)
A:sklearn.linear_model._linear_loss.hess[:n_features, :n_features]->numpy.dot(X.T, WX)
A:sklearn.linear_model._linear_loss.hess[-1, -1]->numpy.abs(hess_pointwise).sum()
A:sklearn.linear_model._linear_loss.hessian_sum->numpy.abs(hess_pointwise).sum()
A:sklearn.linear_model._linear_loss.hX_sum->numpy.atleast_1d(hX_sum)
A:sklearn.linear_model._linear_loss.ret->numpy.empty_like(s)
A:sklearn.linear_model._linear_loss.ret[:n_features]->numpy.linalg.multi_dot([X.T, hX, s[:n_features]])
A:sklearn.linear_model._linear_loss.(grad_pointwise, proba)->self.base_loss.gradient_proba(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=n_threads)
A:sklearn.linear_model._linear_loss.s->s.reshape((n_classes, -1), order='F').reshape((n_classes, -1), order='F')
A:sklearn.linear_model._linear_loss.hess_prod->numpy.empty((n_classes, n_dof), dtype=weights.dtype, order='F')
sklearn.linear_model._linear_loss.LinearModelLoss(self,base_loss,fit_intercept)
sklearn.linear_model._linear_loss.LinearModelLoss.__init__(self,base_loss,fit_intercept)
sklearn.linear_model._linear_loss.LinearModelLoss.gradient(self,coef,X,y,sample_weight=None,l2_reg_strength=0.0,n_threads=1,raw_prediction=None)
sklearn.linear_model._linear_loss.LinearModelLoss.gradient_hessian(self,coef,X,y,sample_weight=None,l2_reg_strength=0.0,n_threads=1,gradient_out=None,hessian_out=None,raw_prediction=None)
sklearn.linear_model._linear_loss.LinearModelLoss.gradient_hessian_product(self,coef,X,y,sample_weight=None,l2_reg_strength=0.0,n_threads=1)
sklearn.linear_model._linear_loss.LinearModelLoss.init_zero_coef(self,X,dtype=None)
sklearn.linear_model._linear_loss.LinearModelLoss.l2_penalty(self,weights,l2_reg_strength)
sklearn.linear_model._linear_loss.LinearModelLoss.loss(self,coef,X,y,sample_weight=None,l2_reg_strength=0.0,n_threads=1,raw_prediction=None)
sklearn.linear_model._linear_loss.LinearModelLoss.loss_gradient(self,coef,X,y,sample_weight=None,l2_reg_strength=0.0,n_threads=1,raw_prediction=None)
sklearn.linear_model._linear_loss.LinearModelLoss.weight_intercept(self,coef)
sklearn.linear_model._linear_loss.LinearModelLoss.weight_intercept_raw(self,coef,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_least_angle.py----------------------------------------
A:sklearn.linear_model._least_angle.Cov->Xy.copy()
A:sklearn.linear_model._least_angle.Gram->self._get_gram(self.precompute, X, y)
A:sklearn.linear_model._least_angle.X->as_float_array(X, copy=self.copy_X)
A:sklearn.linear_model._least_angle.max_features->min(max_iter, n_features)
A:sklearn.linear_model._least_angle.dtypes->set((a.dtype for a in (X, y, Xy, Gram) if a is not None))
A:sklearn.linear_model._least_angle.return_dtype->next(iter(dtypes))
A:sklearn.linear_model._least_angle.coefs->numpy.resize(coefs, (n_iter + add_features, n_features))
A:sklearn.linear_model._least_angle.alphas->numpy.resize(alphas, n_iter + add_features)
A:sklearn.linear_model._least_angle.sign_active->numpy.append(sign_active, 0.0)
A:sklearn.linear_model._least_angle.L->numpy.empty((max_features, max_features), dtype=Gram.dtype)
A:sklearn.linear_model._least_angle.(swap, nrm2)->scipy.linalg.get_blas_funcs(('swap', 'nrm2'), (Cov,))
A:sklearn.linear_model._least_angle.(solve_cholesky,)->get_lapack_funcs(('potrs',), (L,))
A:sklearn.linear_model._least_angle.Gram_copy->self._get_gram(self.precompute, X, y).copy()
A:sklearn.linear_model._least_angle.Cov_copy->Xy.copy().copy()
A:sklearn.linear_model._least_angle.C_idx->numpy.argmax(np.abs(Cov))
A:sklearn.linear_model._least_angle.C->numpy.fabs(C_)
A:sklearn.linear_model._least_angle.sign_active[n_active]->numpy.sign(C_)
A:sklearn.linear_model._least_angle.(Cov[C_idx], Cov[0])->swap(Cov[C_idx], Cov[0])
A:sklearn.linear_model._least_angle.(X.T[n], X.T[m])->swap(X.T[n], X.T[m])
A:sklearn.linear_model._least_angle.L[n_active, :n_active]->numpy.dot(X.T[n_active], X.T[:n_active].T)
A:sklearn.linear_model._least_angle.(Gram[m], Gram[n])->swap(Gram[m], Gram[n])
A:sklearn.linear_model._least_angle.(Gram[:, m], Gram[:, n])->swap(Gram[:, m], Gram[:, n])
A:sklearn.linear_model._least_angle.v->numpy.dot(L[n_active, :n_active], L[n_active, :n_active])
A:sklearn.linear_model._least_angle.diag->max(np.sqrt(np.abs(c - v)), eps)
A:sklearn.linear_model._least_angle.(least_squares, _)->solve_cholesky(L_, sign_active[:n_active], lower=True)
A:sklearn.linear_model._least_angle.L_->L[:n_active, :n_active].copy()
A:sklearn.linear_model._least_angle.tmp->max(np.sum(least_squares * sign_active[:n_active]), eps)
A:sklearn.linear_model._least_angle.eq_dir->numpy.dot(X.T[:n_active].T, least_squares)
A:sklearn.linear_model._least_angle.corr_eq_dir->numpy.dot(Gram[:n_active, n_active:].T, least_squares)
A:sklearn.linear_model._least_angle.g1->utils.arrayfuncs.min_pos((C - Cov) / (AA - corr_eq_dir + tiny32))
A:sklearn.linear_model._least_angle.gamma_->min(g1, g2, C / AA)
A:sklearn.linear_model._least_angle.g2->utils.arrayfuncs.min_pos((C + Cov) / (AA + corr_eq_dir + tiny32))
A:sklearn.linear_model._least_angle.z_pos->utils.arrayfuncs.min_pos(z)
A:sklearn.linear_model._least_angle.coef->numpy.zeros_like(coef)
A:sklearn.linear_model._least_angle.(X.T[i], X.T[i + 1])->swap(X.T[i], X.T[i + 1])
A:sklearn.linear_model._least_angle.temp->numpy.dot(X.T[n_active], residual)
A:sklearn.linear_model._least_angle.(Gram[i], Gram[i + 1])->swap(Gram[i], Gram[i + 1])
A:sklearn.linear_model._least_angle.(Gram[:, i], Gram[:, i + 1])->swap(Gram[:, i], Gram[:, i + 1])
A:sklearn.linear_model._least_angle.precompute->numpy.dot(X.T, X)
A:sklearn.linear_model._least_angle.(X, y, X_offset, y_offset, X_scale)->_preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
A:sklearn.linear_model._least_angle.self.coef_->numpy.empty((n_targets, n_features), dtype=X.dtype)
A:sklearn.linear_model._least_angle.(alphas, active, coef_path, n_iter_)->lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model._least_angle.(alphas, _, self.coef_[k], n_iter_)->lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=False, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model._least_angle.(X, y)->self._validate_data(X, y, y_numeric=True)
A:sklearn.linear_model._least_angle.alpha->getattr(self, 'alpha', 0.0)
A:sklearn.linear_model._least_angle.rng->check_random_state(self.random_state)
A:sklearn.linear_model._least_angle.noise->check_random_state(self.random_state).uniform(high=self.jitter, size=len(y))
A:sklearn.linear_model._least_angle.X_train->_check_copy_and_writeable(X_train, copy)
A:sklearn.linear_model._least_angle.y_train->as_float_array(y_train, copy=False)
A:sklearn.linear_model._least_angle.X_test->_check_copy_and_writeable(X_test, copy)
A:sklearn.linear_model._least_angle.y_test->as_float_array(y_test, copy=False)
A:sklearn.linear_model._least_angle.X_mean->_check_copy_and_writeable(X_train, copy).mean(axis=0)
A:sklearn.linear_model._least_angle.y_mean->as_float_array(y_train, copy=False).mean(axis=0)
A:sklearn.linear_model._least_angle.(alphas, active, coefs)->lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
A:sklearn.linear_model._least_angle.y->as_float_array(y, copy=self.copy_X)
A:sklearn.linear_model._least_angle.cv->check_cv(self.cv, classifier=False)
A:sklearn.linear_model._least_angle.routed_params->Bunch(splitter=Bunch(split={}))
A:sklearn.linear_model._least_angle.cv_paths->Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for (train, test) in cv.split(X, y, **routed_params.splitter.split)))
A:sklearn.linear_model._least_angle.all_alphas->numpy.unique(all_alphas)
A:sklearn.linear_model._least_angle.stride->int(max(1, int(len(all_alphas) / float(self.max_n_alphas))))
A:sklearn.linear_model._least_angle.mse_path->numpy.empty((len(all_alphas), len(cv_paths)))
A:sklearn.linear_model._least_angle.this_residues->scipy.interpolate.interp1d(alphas, residues, axis=0)(all_alphas)
A:sklearn.linear_model._least_angle.mse_path[:, index]->numpy.mean(this_residues, axis=-1)
A:sklearn.linear_model._least_angle.mask->numpy.all(np.isfinite(mse_path), axis=-1)
A:sklearn.linear_model._least_angle.i_best_alpha->numpy.argmin(mse_path.mean(axis=-1))
A:sklearn.linear_model._least_angle.router->MetadataRouter(owner=self.__class__.__name__).add(splitter=check_cv(self.cv), method_mapping=MethodMapping().add(callee='split', caller='fit'))
A:sklearn.linear_model._least_angle.(X, y, Xmean, ymean, Xstd)->_preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
A:sklearn.linear_model._least_angle.(alphas_, _, coef_path_, self.n_iter_)->lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
A:sklearn.linear_model._least_angle.criterion_factor->log(n_samples)
A:sklearn.linear_model._least_angle.residuals_sum_squares->numpy.sum(residuals ** 2, axis=0)
A:sklearn.linear_model._least_angle.degrees_of_freedom->numpy.zeros(coef_path_.shape[1], dtype=int)
A:sklearn.linear_model._least_angle.degrees_of_freedom[k]->numpy.sum(mask)
A:sklearn.linear_model._least_angle.self.noise_variance_->self._estimate_noise_variance(X, y, positive=self.positive)
A:sklearn.linear_model._least_angle.n_best->numpy.argmin(self.criterion_)
A:sklearn.linear_model._least_angle.ols_model->LinearRegression(positive=positive, fit_intercept=False)
A:sklearn.linear_model._least_angle.y_pred->LinearRegression(positive=positive, fit_intercept=False).fit(X, y).predict(X)
sklearn.linear_model.Lars(self,*,fit_intercept=True,verbose=False,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,jitter=None,random_state=None)
sklearn.linear_model.Lars._fit(self,X,y,max_iter,alpha,fit_path,Xy=None)
sklearn.linear_model.Lars._get_gram(precompute,X,y)
sklearn.linear_model.Lars.fit(self,X,y,Xy=None)
sklearn.linear_model.LarsCV(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True)
sklearn.linear_model.LarsCV._more_tags(self)
sklearn.linear_model.LarsCV.fit(self,X,y,**params)
sklearn.linear_model.LarsCV.get_metadata_routing(self)
sklearn.linear_model.LassoLars(self,alpha=1.0,*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,positive=False,jitter=None,random_state=None)
sklearn.linear_model.LassoLarsCV(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True,positive=False)
sklearn.linear_model.LassoLarsIC(self,criterion='aic',*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,positive=False,noise_variance=None)
sklearn.linear_model.LassoLarsIC._estimate_noise_variance(self,X,y,positive)
sklearn.linear_model.LassoLarsIC._more_tags(self)
sklearn.linear_model.LassoLarsIC.fit(self,X,y,copy_X=None)
sklearn.linear_model._least_angle.Lars(self,*,fit_intercept=True,verbose=False,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,jitter=None,random_state=None)
sklearn.linear_model._least_angle.Lars.__init__(self,*,fit_intercept=True,verbose=False,precompute='auto',n_nonzero_coefs=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,jitter=None,random_state=None)
sklearn.linear_model._least_angle.Lars._fit(self,X,y,max_iter,alpha,fit_path,Xy=None)
sklearn.linear_model._least_angle.Lars._get_gram(precompute,X,y)
sklearn.linear_model._least_angle.Lars.fit(self,X,y,Xy=None)
sklearn.linear_model._least_angle.LarsCV(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True)
sklearn.linear_model._least_angle.LarsCV.__init__(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True)
sklearn.linear_model._least_angle.LarsCV._more_tags(self)
sklearn.linear_model._least_angle.LarsCV.fit(self,X,y,**params)
sklearn.linear_model._least_angle.LarsCV.get_metadata_routing(self)
sklearn.linear_model._least_angle.LassoLars(self,alpha=1.0,*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,positive=False,jitter=None,random_state=None)
sklearn.linear_model._least_angle.LassoLars.__init__(self,alpha=1.0,*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,fit_path=True,positive=False,jitter=None,random_state=None)
sklearn.linear_model._least_angle.LassoLarsCV(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True,positive=False)
sklearn.linear_model._least_angle.LassoLarsCV.__init__(self,*,fit_intercept=True,verbose=False,max_iter=500,precompute='auto',cv=None,max_n_alphas=1000,n_jobs=None,eps=np.finfo(float).eps,copy_X=True,positive=False)
sklearn.linear_model._least_angle.LassoLarsIC(self,criterion='aic',*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,positive=False,noise_variance=None)
sklearn.linear_model._least_angle.LassoLarsIC.__init__(self,criterion='aic',*,fit_intercept=True,verbose=False,precompute='auto',max_iter=500,eps=np.finfo(float).eps,copy_X=True,positive=False,noise_variance=None)
sklearn.linear_model._least_angle.LassoLarsIC._estimate_noise_variance(self,X,y,positive)
sklearn.linear_model._least_angle.LassoLarsIC._more_tags(self)
sklearn.linear_model._least_angle.LassoLarsIC.fit(self,X,y,copy_X=None)
sklearn.linear_model._least_angle._check_copy_and_writeable(array,copy=False)
sklearn.linear_model._least_angle._lars_path_residues(X_train,y_train,X_test,y_test,Gram=None,copy=True,method='lar',verbose=False,fit_intercept=True,max_iter=500,eps=np.finfo(float).eps,positive=False)
sklearn.linear_model._least_angle._lars_path_solver(X,y,Xy=None,Gram=None,n_samples=None,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)
sklearn.linear_model._least_angle.lars_path(X,y,Xy=None,*,Gram=None,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)
sklearn.linear_model._least_angle.lars_path_gram(Xy,Gram,*,n_samples,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)
sklearn.linear_model.lars_path(X,y,Xy=None,*,Gram=None,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)
sklearn.linear_model.lars_path_gram(Xy,Gram,*,n_samples,max_iter=500,alpha_min=0,method='lar',copy_X=True,eps=np.finfo(float).eps,copy_Gram=True,verbose=0,return_path=True,return_n_iter=False,positive=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_glm/glm.py----------------------------------------
A:sklearn.linear_model._glm.glm.(X, y)->self._validate_data(X, y, accept_sparse=['csc', 'csr'], dtype=[np.float64, np.float32], y_numeric=True, multi_output=False)
A:sklearn.linear_model._glm.glm.loss_dtype->min(max(y.dtype, X.dtype), np.float64)
A:sklearn.linear_model._glm.glm.y->check_array(y, dtype=raw_prediction.dtype, order='C', ensure_2d=False)
A:sklearn.linear_model._glm.glm.sample_weight->_check_sample_weight(sample_weight, X, dtype=y.dtype)
A:sklearn.linear_model._glm.glm.self._base_loss->self._get_loss()
A:sklearn.linear_model._glm.glm.linear_loss->LinearModelLoss(base_loss=self._base_loss, fit_intercept=self.fit_intercept)
A:sklearn.linear_model._glm.glm.coef->self.solver(coef=coef, linear_loss=linear_loss, l2_reg_strength=l2_reg_strength, tol=self.tol, max_iter=self.max_iter, n_threads=n_threads).solve(X, y, sample_weight)
A:sklearn.linear_model._glm.glm.coef[-1]->LinearModelLoss(base_loss=self._base_loss, fit_intercept=self.fit_intercept).base_loss.link.link(np.average(y, weights=sample_weight))
A:sklearn.linear_model._glm.glm.n_threads->_openmp_effective_n_threads()
A:sklearn.linear_model._glm.glm.opt_res->scipy.optimize.minimize(func, coef, method='L-BFGS-B', jac=True, options={'maxiter': self.max_iter, 'maxls': 50, 'iprint': self.verbose - 1, 'gtol': self.tol, 'ftol': 64 * np.finfo(float).eps}, args=(X, y, sample_weight, l2_reg_strength, n_threads))
A:sklearn.linear_model._glm.glm.self.n_iter_->_check_optimize_result('lbfgs', opt_res)
A:sklearn.linear_model._glm.glm.sol->self.solver(coef=coef, linear_loss=linear_loss, l2_reg_strength=l2_reg_strength, tol=self.tol, max_iter=self.max_iter, n_threads=n_threads)
A:sklearn.linear_model._glm.glm.X->self._validate_data(X, accept_sparse=['csr', 'csc', 'coo'], dtype=[np.float64, np.float32], ensure_2d=True, allow_nd=False, reset=False)
A:sklearn.linear_model._glm.glm.raw_prediction->self._linear_predictor(X)
A:sklearn.linear_model._glm.glm.y_pred->self._base_loss.link.inverse(raw_prediction)
A:sklearn.linear_model._glm.glm.constant->numpy.average(base_loss.constant_to_optimal_zero(y_true=y, sample_weight=None), weights=sample_weight)
A:sklearn.linear_model._glm.glm.deviance->base_loss(y_true=y, raw_prediction=raw_prediction, sample_weight=sample_weight, n_threads=1)
A:sklearn.linear_model._glm.glm.y_mean->self._get_loss().link.link(np.average(y, weights=sample_weight))
A:sklearn.linear_model._glm.glm.deviance_null->base_loss(y_true=y, raw_prediction=np.tile(y_mean, y.shape[0]), sample_weight=sample_weight, n_threads=1)
A:sklearn.linear_model._glm.glm.base_loss->self._get_loss()
sklearn.linear_model.GammaRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model.GammaRegressor._get_loss(self)
sklearn.linear_model.PoissonRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model.PoissonRegressor._get_loss(self)
sklearn.linear_model.TweedieRegressor(self,*,power=0.0,alpha=1.0,fit_intercept=True,link='auto',solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model.TweedieRegressor._get_loss(self)
sklearn.linear_model._glm._GeneralizedLinearRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm._GeneralizedLinearRegressor._get_loss(self)
sklearn.linear_model._glm._GeneralizedLinearRegressor._linear_predictor(self,X)
sklearn.linear_model._glm._GeneralizedLinearRegressor._more_tags(self)
sklearn.linear_model._glm._GeneralizedLinearRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._glm._GeneralizedLinearRegressor.predict(self,X)
sklearn.linear_model._glm._GeneralizedLinearRegressor.score(self,X,y,sample_weight=None)
sklearn.linear_model._glm.glm.GammaRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.GammaRegressor.__init__(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.GammaRegressor._get_loss(self)
sklearn.linear_model._glm.glm.PoissonRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.PoissonRegressor.__init__(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.PoissonRegressor._get_loss(self)
sklearn.linear_model._glm.glm.TweedieRegressor(self,*,power=0.0,alpha=1.0,fit_intercept=True,link='auto',solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.TweedieRegressor.__init__(self,*,power=0.0,alpha=1.0,fit_intercept=True,link='auto',solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm.TweedieRegressor._get_loss(self)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor.__init__(self,*,alpha=1.0,fit_intercept=True,solver='lbfgs',max_iter=100,tol=0.0001,warm_start=False,verbose=0)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor._get_loss(self)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor._linear_predictor(self,X)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor._more_tags(self)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor.fit(self,X,y,sample_weight=None)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor.predict(self,X)
sklearn.linear_model._glm.glm._GeneralizedLinearRegressor.score(self,X,y,sample_weight=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_glm/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_glm/_newton_solver.py----------------------------------------
A:sklearn.linear_model._glm._newton_solver.(_, _, self.raw_prediction)->self.linear_loss.weight_intercept_raw(self.coef, X)
A:sklearn.linear_model._glm._newton_solver.self.loss_value->self.linear_loss.loss(coef=self.coef, X=X, y=y, sample_weight=sample_weight, l2_reg_strength=self.l2_reg_strength, n_threads=self.n_threads, raw_prediction=self.raw_prediction)
A:sklearn.linear_model._glm._newton_solver.opt_res->scipy.optimize.minimize(self.linear_loss.loss_gradient, self.coef, method='L-BFGS-B', jac=True, options={'maxiter': self.max_iter, 'maxls': 50, 'iprint': self.verbose - 1, 'gtol': self.tol, 'ftol': 64 * np.finfo(np.float64).eps}, args=(X, y, sample_weight, self.l2_reg_strength, self.n_threads))
A:sklearn.linear_model._glm._newton_solver.self.n_iter_->_check_optimize_result('lbfgs', opt_res)
A:sklearn.linear_model._glm._newton_solver.(_, _, raw_prediction_newton)->self.linear_loss.weight_intercept_raw(self.coef_newton, X)
A:sklearn.linear_model._glm._newton_solver.(self.loss_value, self.gradient)->self.linear_loss.loss_gradient(coef=self.coef, X=X, y=y, sample_weight=sample_weight, l2_reg_strength=self.l2_reg_strength, n_threads=self.n_threads, raw_prediction=raw)
A:sklearn.linear_model._glm._newton_solver.tiny_loss->numpy.abs(self.loss_value_old * eps)
A:sklearn.linear_model._glm._newton_solver.sum_abs_grad_old->scipy.linalg.norm(self.gradient_old, ord=1)
A:sklearn.linear_model._glm._newton_solver.sum_abs_grad->scipy.linalg.norm(self.gradient, ord=1)
A:sklearn.linear_model._glm._newton_solver.check->numpy.max(np.abs(self.gradient))
A:sklearn.linear_model._glm._newton_solver.loss_value->self.linear_loss.loss(coef=self.coef, X=X, y=y, sample_weight=sample_weight, l2_reg_strength=self.l2_reg_strength, n_threads=self.n_threads)
A:sklearn.linear_model._glm._newton_solver.self.gradient->numpy.empty_like(self.coef)
A:sklearn.linear_model._glm._newton_solver.self.hessian->numpy.empty_like(self.coef, shape=(n_dof, n_dof))
A:sklearn.linear_model._glm._newton_solver.(_, _, self.hessian_warning)->self.linear_loss.gradient_hessian(coef=self.coef, X=X, y=y, sample_weight=sample_weight, l2_reg_strength=self.l2_reg_strength, n_threads=self.n_threads, gradient_out=self.gradient, hessian_out=self.hessian, raw_prediction=self.raw_prediction)
A:sklearn.linear_model._glm._newton_solver.self.coef_newton->scipy.linalg.solve(self.hessian, -self.gradient, check_finite=False, assume_a='sym')
sklearn.linear_model._glm._newton_solver.NewtonCholeskySolver(NewtonSolver)
sklearn.linear_model._glm._newton_solver.NewtonCholeskySolver.inner_solve(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonCholeskySolver.setup(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonCholeskySolver.update_gradient_hessian(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver(self,*,coef,linear_loss=LinearModelLoss(base_loss=HalfSquaredError(),fit_intercept=True),l2_reg_strength=0.0,tol=0.0001,max_iter=100,n_threads=1,verbose=0)
sklearn.linear_model._glm._newton_solver.NewtonSolver.__init__(self,*,coef,linear_loss=LinearModelLoss(base_loss=HalfSquaredError(),fit_intercept=True),l2_reg_strength=0.0,tol=0.0001,max_iter=100,n_threads=1,verbose=0)
sklearn.linear_model._glm._newton_solver.NewtonSolver.check_convergence(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.fallback_lbfgs_solve(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.finalize(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.inner_solve(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.line_search(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.setup(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.solve(self,X,y,sample_weight)
sklearn.linear_model._glm._newton_solver.NewtonSolver.update_gradient_hessian(self,X,y,sample_weight)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_glm/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/_glm/tests/test_glm.py----------------------------------------
A:sklearn.linear_model._glm.tests.test_glm.res_NM->minimize(fun, x, method='Nelder-Mead', options={'xatol': tol_NM, 'fatol': tol_NM})
A:sklearn.linear_model._glm.tests.test_glm.res->TweedieRegressor(power=power, link=link).fit(X, y).fit(X, y)
A:sklearn.linear_model._glm.tests.test_glm.(X, y)->make_regression(n_samples=n_samples + test_size, n_features=n_features, n_informative=n_features - 2, noise=0.5, random_state=42)
A:sklearn.linear_model._glm.tests.test_glm.k->min(n_samples, n_features)
A:sklearn.linear_model._glm.tests.test_glm.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.linear_model._glm.tests.test_glm.X->numpy.array([[1.0, 0], [0, 1]], dtype=float)
A:sklearn.linear_model._glm.tests.test_glm.(U, s, Vt)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.linear_model._glm.tests.test_glm.coef_unpenalized->numpy.random.RandomState(global_random_seed).uniform(low=1, high=3, size=n_features)
A:sklearn.linear_model._glm.tests.test_glm.raw_prediction->numpy.random.RandomState(global_random_seed).uniform(low=-3, high=3, size=n_samples)
A:sklearn.linear_model._glm.tests.test_glm.linear_loss->LinearModelLoss(base_loss=HalfTweedieLoss(power=3), fit_intercept=False)
A:sklearn.linear_model._glm.tests.test_glm.sw->numpy.full_like(y, fill_value=1 / n_samples)
A:sklearn.linear_model._glm.tests.test_glm.y->numpy.array([1, 2], dtype=float)
A:sklearn.linear_model._glm.tests.test_glm.fun->partial(linear_loss.loss, X=X[:, :-1], y=y, sample_weight=sw, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model._glm.tests.test_glm.grad->partial(linear_loss.gradient, X=X[:, :-1], y=y, sample_weight=sw, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model._glm.tests.test_glm.coef_penalized_with_intercept->_special_minimize(fun, grad, coef_unpenalized, tol_NM=1e-06, tol=1e-14)
A:sklearn.linear_model._glm.tests.test_glm.coef_penalized_without_intercept->_special_minimize(fun, grad, coef_unpenalized[:-1], tol_NM=1e-06, tol=1e-14)
A:sklearn.linear_model._glm.tests.test_glm.params->dict(alpha=alpha, fit_intercept=fit_intercept, solver=solver, tol=1e-12, max_iter=1000)
A:sklearn.linear_model._glm.tests.test_glm.model->clone(model).set_params(**params)
A:sklearn.linear_model._glm.tests.test_glm.norm_solution->numpy.linalg.norm(np.r_[intercept, coef])
A:sklearn.linear_model._glm.tests.test_glm.norm_model->numpy.linalg.norm(np.r_[model.intercept_, model.coef_])
A:sklearn.linear_model._glm.tests.test_glm.glm->TweedieRegressor(power=power, link=link).fit(X, y)
A:sklearn.linear_model._glm.tests.test_glm.glm_params->dict(alpha=alpha, fit_intercept=fit_intercept)
A:sklearn.linear_model._glm.tests.test_glm.coef->TweedieRegressor(power=power, link=link).fit(X, y).coef_.copy()
A:sklearn.linear_model._glm.tests.test_glm.sample_weight->numpy.ones(y.shape)
A:sklearn.linear_model._glm.tests.test_glm.coef1->TweedieRegressor(power=power, link=link).fit(X, y).coef_.copy()
A:sklearn.linear_model._glm.tests.test_glm.X2->numpy.concatenate([X, X[:n_samples // 2]], axis=0)
A:sklearn.linear_model._glm.tests.test_glm.y2->numpy.concatenate([y, y[:n_samples // 2]])
A:sklearn.linear_model._glm.tests.test_glm.sample_weight_1->numpy.ones(len(y))
A:sklearn.linear_model._glm.tests.test_glm.glm1->PoissonRegressor(warm_start=False, max_iter=1000, alpha=alpha, **params)
A:sklearn.linear_model._glm.tests.test_glm.glm2->PoissonRegressor(warm_start=True, max_iter=1, alpha=alpha, **params)
A:sklearn.linear_model._glm.tests.test_glm.objective_glm1->LinearModelLoss(base_loss=HalfTweedieLoss(power=3), fit_intercept=False).loss(coef=np.r_[glm1.coef_, glm1.intercept_] if fit_intercept else glm1.coef_, X=X, y=y, sample_weight=sw, l2_reg_strength=alpha)
A:sklearn.linear_model._glm.tests.test_glm.objective_glm2->LinearModelLoss(base_loss=HalfTweedieLoss(power=3), fit_intercept=False).loss(coef=np.r_[glm2.coef_, glm2.intercept_] if fit_intercept else glm2.coef_, X=X, y=y, sample_weight=sw, l2_reg_strength=alpha)
A:sklearn.linear_model._glm.tests.test_glm.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=test_size, random_state=0)
A:sklearn.linear_model._glm.tests.test_glm.sw_train->numpy.random.RandomState(0).rand(len(y_train))
A:sklearn.linear_model._glm.tests.test_glm.ridge->Ridge(alpha=alpha_ridge, random_state=42, fit_intercept=fit_intercept, **ridge_params)
A:sklearn.linear_model._glm.tests.test_glm.est->_GeneralizedLinearRegressor(max_iter=1, tol=1e-20)
A:sklearn.linear_model._glm.tests.test_glm.X_orig->numpy.random.RandomState(global_random_seed).normal(size=(20, 3))
A:sklearn.linear_model._glm.tests.test_glm.X_collinear->numpy.hstack([X_orig] * 10)
A:sklearn.linear_model._glm.tests.test_glm.baseline_pred->numpy.full_like(y, y.mean())
A:sklearn.linear_model._glm.tests.test_glm.constant_model_deviance->mean_poisson_deviance(y, baseline_pred)
A:sklearn.linear_model._glm.tests.test_glm.reg->PoissonRegressor(solver=newton_solver, alpha=1e-10).fit(X_collinear, y)
A:sklearn.linear_model._glm.tests.test_glm.original_newton_deviance->mean_poisson_deviance(y, reg.predict(X_orig))
A:sklearn.linear_model._glm.tests.test_glm.collinear_lbfgs_deviance->mean_poisson_deviance(y, reg.predict(X_collinear))
A:sklearn.linear_model._glm.tests.test_glm.collinear_newton_deviance->mean_poisson_deviance(y, reg.predict(X_collinear))
A:sklearn.linear_model._glm.tests.test_glm.penalized_collinear_newton_deviance->mean_poisson_deviance(y, reg.predict(X_collinear))
A:sklearn.linear_model._glm.tests.test_glm.sol->NewtonCholeskySolver(coef=linear_loss.init_zero_coef(X) + 1, linear_loss=linear_loss, l2_reg_strength=0, verbose=verbose)
A:sklearn.linear_model._glm.tests.test_glm.captured->capsys.readouterr()
A:sklearn.linear_model._glm.tests.test_glm.sol.coef_newton->numpy.array([1e-06, 0])
sklearn.linear_model._glm.tests.test_glm.BinomialRegressor(_GeneralizedLinearRegressor)
sklearn.linear_model._glm.tests.test_glm.BinomialRegressor._get_loss(self)
sklearn.linear_model._glm.tests.test_glm._special_minimize(fun,grad,x,tol_NM,tol)
sklearn.linear_model._glm.tests.test_glm.glm_dataset(global_random_seed,request)
sklearn.linear_model._glm.tests.test_glm.regression_data()
sklearn.linear_model._glm.tests.test_glm.test_convergence_warning(regression_data)
sklearn.linear_model._glm.tests.test_glm.test_glm_identity_regression(fit_intercept)
sklearn.linear_model._glm.tests.test_glm.test_glm_log_regression(solver,fit_intercept,estimator)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression_hstacked_X(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression_unpenalized(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression_unpenalized_hstacked_X(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression_unpenalized_vstacked_X(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_regression_vstacked_X(solver,fit_intercept,glm_dataset)
sklearn.linear_model._glm.tests.test_glm.test_glm_sample_weight_consistency(fit_intercept,alpha,GLMEstimator)
sklearn.linear_model._glm.tests.test_glm.test_glm_wrong_y_range(glm)
sklearn.linear_model._glm.tests.test_glm.test_linalg_warning_with_newton_solver(global_random_seed)
sklearn.linear_model._glm.tests.test_glm.test_newton_solver_verbosity(capsys,verbose)
sklearn.linear_model._glm.tests.test_glm.test_normal_ridge_comparison(n_samples,n_features,fit_intercept,sample_weight,request)
sklearn.linear_model._glm.tests.test_glm.test_poisson_glmnet(solver)
sklearn.linear_model._glm.tests.test_glm.test_sample_weights_validation()
sklearn.linear_model._glm.tests.test_glm.test_tags(estimator,value)
sklearn.linear_model._glm.tests.test_glm.test_tweedie_link_argument(name,link_class)
sklearn.linear_model._glm.tests.test_glm.test_tweedie_link_auto(power,expected_link_class)
sklearn.linear_model._glm.tests.test_glm.test_tweedie_score(regression_data,power,link)
sklearn.linear_model._glm.tests.test_glm.test_warm_start(solver,fit_intercept,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_ridge.py----------------------------------------
A:sklearn.linear_model.tests.test_ridge.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_ridge.ind->numpy.arange(X_diabetes.shape[0])
A:sklearn.linear_model.tests.test_ridge.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_ridge.iris->sklearn.datasets.load_iris()
A:sklearn.linear_model.tests.test_ridge.k->min(n_samples, n_features)
A:sklearn.linear_model.tests.test_ridge.X->sparse_container(X)
A:sklearn.linear_model.tests.test_ridge.(U, s, Vt)->scipy.linalg.svd(X)
A:sklearn.linear_model.tests.test_ridge.coef_ols->numpy.random.RandomState(42).uniform(low=-10, high=10, size=n_features)
A:sklearn.linear_model.tests.test_ridge.y->numpy.random.RandomState(42).rand(n_samples)
A:sklearn.linear_model.tests.test_ridge.coef_ridge->scipy.linalg.solve(X.T @ X + d, X.T @ y)
A:sklearn.linear_model.tests.test_ridge.params->dict(fit_intercept=fit_intercept, alpha=1.0, solver=solver, positive=solver == 'lbfgs', random_state=global_random_seed, tol=1e-12)
A:sklearn.linear_model.tests.test_ridge.model->Ridge(alpha=0.01, solver='lbfgs', fit_intercept=False, tol=1e-12, positive=True, max_iter=1)
A:sklearn.linear_model.tests.test_ridge.sw->numpy.random.RandomState(42).chisquare(1, shape[0])
A:sklearn.linear_model.tests.test_ridge.coef->Ridge(**params).fit(X, y, sample_weight=None).coef_.copy()
A:sklearn.linear_model.tests.test_ridge.K->numpy.dot(X_diabetes, X_diabetes.T)
A:sklearn.linear_model.tests.test_ridge.dual_coef->_solve_cholesky_kernel(K, y, alpha=[0.01])
A:sklearn.linear_model.tests.test_ridge.ridge->RidgeCV(alphas=(1, 10, 100))
A:sklearn.linear_model.tests.test_ridge.ols->LinearRegression(fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.penalties->numpy.arange(n_targets)
A:sklearn.linear_model.tests.test_ridge.coef_cholesky->_solve_svd(X, y, alpha)
A:sklearn.linear_model.tests.test_ridge.X_m->numpy.random.RandomState(42).randn(8)
A:sklearn.linear_model.tests.test_ridge.sqrt_sw->numpy.sqrt(sw)
A:sklearn.linear_model.tests.test_ridge.Y->numpy.concatenate([y, y], axis=1)
A:sklearn.linear_model.tests.test_ridge.A->numpy.random.RandomState(42).randn(9, *n_col)
A:sklearn.linear_model.tests.test_ridge.operator->_X_CenterStackOp(csr_container(X), X_m, sqrt_sw)
A:sklearn.linear_model.tests.test_ridge.reference_operator->numpy.hstack([X - sqrt_sw[:, None] * X_m, sqrt_sw[:, None]])
A:sklearn.linear_model.tests.test_ridge.X_mean->numpy.average(X, axis=0, weights=sw)
A:sklearn.linear_model.tests.test_ridge.true_gram->X_centered.dot(X_centered.T)
A:sklearn.linear_model.tests.test_ridge.X_sparse->sparse_container(X)
A:sklearn.linear_model.tests.test_ridge.gcv->_RidgeGCV(fit_intercept=True)
A:sklearn.linear_model.tests.test_ridge.(computed_gram, computed_mean)->_RidgeGCV(fit_intercept=True)._compute_gram(X_sparse, sqrt_sw)
A:sklearn.linear_model.tests.test_ridge.true_covariance->X_centered.T.dot(X_centered)
A:sklearn.linear_model.tests.test_ridge.(computed_cov, computed_mean)->_RidgeGCV(fit_intercept=True)._compute_covariance(X_sparse, sqrt_sw)
A:sklearn.linear_model.tests.test_ridge.(X, y, c)->make_regression(n_samples=n_samples, n_features=n_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, shuffle=shuffle, coef=True, random_state=random_state)
A:sklearn.linear_model.tests.test_ridge.c->numpy.asarray([c])
A:sklearn.linear_model.tests.test_ridge.removed_X->sparse_container(X).copy()
A:sklearn.linear_model.tests.test_ridge.(X, y)->make_regression(n_samples=300, n_features=300, random_state=42)
A:sklearn.linear_model.tests.test_ridge.svd_ridge->Ridge(solver='svd', alpha=alpha).fit(X, y)
A:sklearn.linear_model.tests.test_ridge.loo_ridge->RidgeCV(cv=n_samples, fit_intercept=True, alphas=alphas, scoring=scoring)
A:sklearn.linear_model.tests.test_ridge.gcv_ridge->RidgeCV(alphas=alphas, store_cv_values=True, gcv_mode=gcv_mode, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_ridge.X_gcv->X_container(X)
A:sklearn.linear_model.tests.test_ridge.sample_weight->numpy.random.RandomState(42).uniform(low=0.01, high=2, size=X.shape[0])
A:sklearn.linear_model.tests.test_ridge.indices->numpy.repeat(np.arange(X.shape[0]), sample_weight)
A:sklearn.linear_model.tests.test_ridge.cv->KFold(5)
A:sklearn.linear_model.tests.test_ridge.splits->KFold(5).split(X_tiled, y_tiled, groups=indices)
A:sklearn.linear_model.tests.test_ridge.kfold->RidgeCV(alphas=alphas, cv=splits, scoring='neg_mean_squared_error', fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_ridge.ridge_reg->Ridge(alpha=kfold.alpha_, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_ridge.predictions->cross_val_predict(ridge_reg, X_tiled, y_tiled, cv=splits)
A:sklearn.linear_model.tests.test_ridge.kfold_errors->numpy.asarray(kfold_errors)
A:sklearn.linear_model.tests.test_ridge.(X, _)->make_regression(n_samples=5, n_features=2)
A:sklearn.linear_model.tests.test_ridge.ridge_gcv->_RidgeGCV(fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_ridge.scoring->make_scorer(func)
A:sklearn.linear_model.tests.test_ridge.ridge_gcv2->RidgeCV(fit_intercept=False, scoring=scoring)
A:sklearn.linear_model.tests.test_ridge.ridge_gcv3->RidgeCV(fit_intercept=False, scoring=scoring)
A:sklearn.linear_model.tests.test_ridge.scorer->get_scorer('neg_mean_squared_error')
A:sklearn.linear_model.tests.test_ridge.ridge_gcv4->RidgeCV(fit_intercept=False, scoring=scorer)
A:sklearn.linear_model.tests.test_ridge.Y_pred->Classifier(**params).fit(X, Y).predict(X)
A:sklearn.linear_model.tests.test_ridge.y_pred->Ridge(**params).fit(X, y, sample_weight=None).predict(X)
A:sklearn.linear_model.tests.test_ridge.ridge_cv->RidgeCV(alphas=alphas, cv=6, alpha_per_target=True)
A:sklearn.linear_model.tests.test_ridge.reg->Ridge(**params).fit(X, y, sample_weight=None)
A:sklearn.linear_model.tests.test_ridge.clf->Classifier(**params).fit(X, Y)
A:sklearn.linear_model.tests.test_ridge.alphas->numpy.logspace(-2, 2, num=5)
A:sklearn.linear_model.tests.test_ridge.score->RidgeCV(alphas=(1, 10, 100)).score(X, y_diabetes)
A:sklearn.linear_model.tests.test_ridge.ridge2->Ridge(tol=0.001, fit_intercept=False)
A:sklearn.linear_model.tests.test_ridge.score2->Ridge(tol=0.001, fit_intercept=False).score(X, y_diabetes)
A:sklearn.linear_model.tests.test_ridge.ret_dense->test_func(None)
A:sklearn.linear_model.tests.test_ridge.ret_sparse->test_func(csr_container)
A:sklearn.linear_model.tests.test_ridge.rega->RidgeClassifier(class_weight='balanced')
A:sklearn.linear_model.tests.test_ridge.reg1->Ridge(**params).fit(X, y, sample_weight=sample_weight_1)
A:sklearn.linear_model.tests.test_ridge.reg2->Ridge(**params).fit(X2, y2, sample_weight=sample_weight_2)
A:sklearn.linear_model.tests.test_ridge.x->numpy.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])
A:sklearn.linear_model.tests.test_ridge.n_alphas->len(alphas)
A:sklearn.linear_model.tests.test_ridge.r->RidgeClassifierCV(alphas=alphas, cv=None, store_cv_values=True, scoring=scoring_)
A:sklearn.linear_model.tests.test_ridge.ridge_est->Estimator(alphas=alphas)
A:sklearn.linear_model.tests.test_ridge.ridgecv->RidgeCV(alphas=alphas, cv=cv)
A:sklearn.linear_model.tests.test_ridge.gs->GridSearchCV(Ridge(), parameters, cv=cv)
A:sklearn.linear_model.tests.test_ridge.sparse_ridge->Ridge(**params)
A:sklearn.linear_model.tests.test_ridge.dense_ridge->Ridge(**params)
A:sklearn.linear_model.tests.test_ridge.X_csr->csr_container(X)
A:sklearn.linear_model.tests.test_ridge.err_msg->"solver='{}' does not support".format(solver)
A:sklearn.linear_model.tests.test_ridge.X_testing->container(X)
A:sklearn.linear_model.tests.test_ridge.out->ridge_regression(X_testing, y, alpha=alpha, solver=solver, sample_weight=sample_weight, positive=positive, return_intercept=return_intercept, tol=tol)
A:sklearn.linear_model.tests.test_ridge.X_64->numpy.random.RandomState(42).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_ridge.y_64->numpy.random.RandomState(42).randn(n_samples, n_target)
A:sklearn.linear_model.tests.test_ridge.X_32->numpy.random.RandomState(42).randn(n_samples, n_features).astype(np.float32)
A:sklearn.linear_model.tests.test_ridge.y_32->numpy.random.RandomState(42).randn(n_samples, n_target).astype(np.float32)
A:sklearn.linear_model.tests.test_ridge.ridge_32->Ridge(alpha=alpha, solver='cholesky')
A:sklearn.linear_model.tests.test_ridge.ridge_64->Ridge(alpha=alpha, solver='cholesky')
A:sklearn.linear_model.tests.test_ridge.alpha->numpy.asarray([alpha])
A:sklearn.linear_model.tests.test_ridge.random_state->numpy.random.RandomState(seed)
A:sklearn.linear_model.tests.test_ridge.results->dict()
A:sklearn.linear_model.tests.test_ridge.results[current_dtype]->ridge_regression(X.astype(current_dtype), y.astype(current_dtype), alpha=alpha, solver=solver, random_state=random_state, sample_weight=None, positive=positive, max_iter=500, tol=1e-10, return_n_iter=False, return_intercept=False)
A:sklearn.linear_model.tests.test_ridge.(_, _)->ridge_regression(X, y, alpha, positive=True, solver=solver, return_intercept=False)
A:sklearn.linear_model.tests.test_ridge.model_positive->Ridge(alpha=alpha, positive=True).fit(X, y)
A:sklearn.linear_model.tests.test_ridge.loss->ridge_loss(model)
A:sklearn.linear_model.tests.test_ridge.loss_positive->ridge_loss(model_positive)
A:sklearn.linear_model.tests.test_ridge.loss_perturbed->ridge_loss(model_positive, random_state=random_state)
A:sklearn.linear_model.tests.test_ridge.coef_lbfgs->_solve_lbfgs(X, y, alpha, **config)
A:sklearn.linear_model.tests.test_ridge.X2->sparse_container(X2)
A:sklearn.linear_model.tests.test_ridge.y2->numpy.concatenate([y, y[:n_samples // 2]])
A:sklearn.linear_model.tests.test_ridge.sample_weight_1->numpy.random.RandomState(42).uniform(low=0.01, high=2, size=X.shape[0]).copy()
A:sklearn.linear_model.tests.test_ridge.sample_weight_2->numpy.concatenate([sample_weight, sample_weight[:n_samples // 2]], axis=0)
sklearn.linear_model.tests.test_ridge._accuracy_callable(y_test,y_pred)
sklearn.linear_model.tests.test_ridge._make_sparse_offset_regression(n_samples=100,n_features=100,proportion_nonzero=0.5,n_informative=10,n_targets=1,bias=13.0,X_offset=30.0,noise=30.0,shuffle=True,coef=False,positive=False,random_state=None)
sklearn.linear_model.tests.test_ridge._mean_squared_error_callable(y_test,y_pred)
sklearn.linear_model.tests.test_ridge._test_multi_ridge_diabetes(sparse_container)
sklearn.linear_model.tests.test_ridge._test_ridge_classifiers(sparse_container)
sklearn.linear_model.tests.test_ridge._test_ridge_cv(sparse_container)
sklearn.linear_model.tests.test_ridge._test_ridge_diabetes(sparse_container)
sklearn.linear_model.tests.test_ridge._test_ridge_loo(sparse_container)
sklearn.linear_model.tests.test_ridge._test_tolerance(sparse_container)
sklearn.linear_model.tests.test_ridge.ols_ridge_dataset(global_random_seed,request)
sklearn.linear_model.tests.test_ridge.test_X_CenterStackOp(n_col,csr_container)
sklearn.linear_model.tests.test_ridge.test_check_gcv_mode_choice(sparse_container,mode,mode_n_greater_than_p,mode_p_greater_than_n)
sklearn.linear_model.tests.test_ridge.test_class_weight_vs_sample_weight(reg)
sklearn.linear_model.tests.test_ridge.test_class_weights()
sklearn.linear_model.tests.test_ridge.test_class_weights_cv()
sklearn.linear_model.tests.test_ridge.test_compute_covariance(shape,uniform_weights,csr_container)
sklearn.linear_model.tests.test_ridge.test_compute_gram(shape,uniform_weights,csr_container)
sklearn.linear_model.tests.test_ridge.test_dense_sparse(test_func,csr_container)
sklearn.linear_model.tests.test_ridge.test_dtype_match(solver)
sklearn.linear_model.tests.test_ridge.test_dtype_match_cholesky()
sklearn.linear_model.tests.test_ridge.test_lbfgs_solver_consistency(alpha)
sklearn.linear_model.tests.test_ridge.test_lbfgs_solver_error()
sklearn.linear_model.tests.test_ridge.test_n_iter()
sklearn.linear_model.tests.test_ridge.test_positive_ridge_loss(alpha)
sklearn.linear_model.tests.test_ridge.test_primal_dual_relationship()
sklearn.linear_model.tests.test_ridge.test_raises_value_error_if_sample_weights_greater_than_1d()
sklearn.linear_model.tests.test_ridge.test_ridge_best_score(ridge,make_dataset,cv)
sklearn.linear_model.tests.test_ridge.test_ridge_classifier_cv_store_cv_values(scoring)
sklearn.linear_model.tests.test_ridge.test_ridge_classifier_with_scoring(sparse_container,scoring,cv)
sklearn.linear_model.tests.test_ridge.test_ridge_cv_individual_penalties()
sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse(solver,with_sample_weight,global_random_seed,csr_container)
sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse_error(solver,csr_container)
sklearn.linear_model.tests.test_ridge.test_ridge_fit_intercept_sparse_sag(with_sample_weight,global_random_seed,csr_container)
sklearn.linear_model.tests.test_ridge.test_ridge_gcv_cv_values_not_stored(ridge,make_dataset)
sklearn.linear_model.tests.test_ridge.test_ridge_gcv_sample_weights(gcv_mode,X_container,fit_intercept,n_features,y_shape,noise)
sklearn.linear_model.tests.test_ridge.test_ridge_gcv_vs_ridge_loo_cv(gcv_mode,X_container,X_shape,y_shape,fit_intercept,noise)
sklearn.linear_model.tests.test_ridge.test_ridge_ground_truth_positive_test(fit_intercept,alpha)
sklearn.linear_model.tests.test_ridge.test_ridge_individual_penalties()
sklearn.linear_model.tests.test_ridge.test_ridge_intercept()
sklearn.linear_model.tests.test_ridge.test_ridge_loo_cv_asym_scoring()
sklearn.linear_model.tests.test_ridge.test_ridge_positive_error_test(solver)
sklearn.linear_model.tests.test_ridge.test_ridge_positive_regression_test(solver,fit_intercept,alpha)
sklearn.linear_model.tests.test_ridge.test_ridge_regression(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_check_arguments_validity(return_intercept,sample_weight,container,solver)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_convergence_fail()
sklearn.linear_model.tests.test_ridge.test_ridge_regression_custom_scoring(sparse_container,cv)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_dtype_stability(solver,seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_hstacked_X(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_sample_weights(solver,fit_intercept,sparse_container,alpha,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized_hstacked_X(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_unpenalized_vstacked_X(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_regression_vstacked_X(solver,fit_intercept,ols_ridge_dataset,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_sag_with_X_fortran()
sklearn.linear_model.tests.test_ridge.test_ridge_sample_weight_consistency(fit_intercept,sparse_container,data,solver,global_random_seed)
sklearn.linear_model.tests.test_ridge.test_ridge_shapes_type()
sklearn.linear_model.tests.test_ridge.test_ridge_vs_lstsq()
sklearn.linear_model.tests.test_ridge.test_ridgeclassifier_multilabel(Classifier,params)
sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_conversion(Estimator)
sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_scalar(Estimator)
sklearn.linear_model.tests.test_ridge.test_ridgecv_alphas_validation(Estimator,params,err_type,err_msg)
sklearn.linear_model.tests.test_ridge.test_ridgecv_int_alphas()
sklearn.linear_model.tests.test_ridge.test_ridgecv_sample_weight()
sklearn.linear_model.tests.test_ridge.test_ridgecv_store_cv_values(scoring)
sklearn.linear_model.tests.test_ridge.test_solver_consistency(solver,proportion_nonzero,n_samples,dtype,sparse_container,seed)
sklearn.linear_model.tests.test_ridge.test_sparse_cg_max_iter()
sklearn.linear_model.tests.test_ridge.test_sparse_design_with_sample_weights(n_samples,n_features,sparse_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_base.py----------------------------------------
A:sklearn.linear_model.tests.test_base.reg->reg.fit(X, y, sample_weight=sample_weight).fit(X, y, sample_weight=sample_weight)
A:sklearn.linear_model.tests.test_base.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.linear_model.tests.test_base.X->sparse_container(X)
A:sklearn.linear_model.tests.test_base.y->numpy.random.RandomState(global_random_seed).rand(n_samples)
A:sklearn.linear_model.tests.test_base.W->numpy.diag(sample_weight)
A:sklearn.linear_model.tests.test_base.coefs2->scipy.linalg.solve(Xw, yw)
A:sklearn.linear_model.tests.test_base.X2->numpy.concatenate([X, X[:n_samples // 2]], axis=0)
A:sklearn.linear_model.tests.test_base.X3->numpy.array([[0.27677969, 0.70693172, 0.01628859], [0.08385139, 0.20692515, 0.70922346]])
A:sklearn.linear_model.tests.test_base.lr2_without_intercept->LinearRegression(fit_intercept=False).fit(X2, y)
A:sklearn.linear_model.tests.test_base.lr2_with_intercept->LinearRegression().fit(X2, y)
A:sklearn.linear_model.tests.test_base.lr3_without_intercept->LinearRegression(fit_intercept=False).fit(X3, y)
A:sklearn.linear_model.tests.test_base.lr3_with_intercept->LinearRegression().fit(X3, y)
A:sklearn.linear_model.tests.test_base.beta->numpy.random.RandomState(global_random_seed).rand(n)
A:sklearn.linear_model.tests.test_base.ols->LinearRegression(positive=True)
A:sklearn.linear_model.tests.test_base.Xcsr->csr_container(X)
A:sklearn.linear_model.tests.test_base.params->dict(fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_base.clf_dense->LinearRegression(**params)
A:sklearn.linear_model.tests.test_base.clf_sparse->LinearRegression(**params)
A:sklearn.linear_model.tests.test_base.(X, y)->make_regression()
A:sklearn.linear_model.tests.test_base.Y_pred->LinearRegression(positive=True).predict(X)
A:sklearn.linear_model.tests.test_base.y_pred->LinearRegression(positive=True).predict(X)
A:sklearn.linear_model.tests.test_base.regn->LinearRegression(positive=False)
A:sklearn.linear_model.tests.test_base.original_X_data->numpy.random.RandomState(global_random_seed).randn(10, 12)
A:sklearn.linear_model.tests.test_base.original_y_data->numpy.random.RandomState(global_random_seed).randn(10, 2)
A:sklearn.linear_model.tests.test_base.orginal_sw_data->numpy.random.RandomState(global_random_seed).rand(10)
A:sklearn.linear_model.tests.test_base.sample_weight->numpy.random.RandomState(global_random_seed).uniform(low=0.01, high=2, size=X.shape[0])
A:sklearn.linear_model.tests.test_base.pd->pytest.importorskip('pandas')
A:sklearn.linear_model.tests.test_base.df->pytest.importorskip('pandas').DataFrame({'0': np.random.randn(10)})
A:sklearn.linear_model.tests.test_base.arr->pytest.importorskip('pandas').arrays.SparseArray(arr, fill_value=0)
A:sklearn.linear_model.tests.test_base.df['0']->pytest.importorskip('pandas').arrays.SparseArray(df['0'], fill_value=0)
A:sklearn.linear_model.tests.test_base.expected_X_mean->numpy.average(X, axis=0, weights=sample_weight)
A:sklearn.linear_model.tests.test_base.expected_y_mean->numpy.average(y, axis=0, weights=sample_weight)
A:sklearn.linear_model.tests.test_base.(Xt, yt, X_mean, y_mean, X_scale)->_preprocess_data(X, y, fit_intercept=True)
A:sklearn.linear_model.tests.test_base.(_, yt, _, y_mean, _)->_preprocess_data(X, y, fit_intercept=True)
A:sklearn.linear_model.tests.test_base.X_sample_weight_avg->numpy.average(X, weights=sample_weight, axis=0)
A:sklearn.linear_model.tests.test_base.X_sample_weight_var->numpy.average((X - X_sample_weight_avg) ** 2, weights=sample_weight, axis=0)
A:sklearn.linear_model.tests.test_base.XA->sparse_container(X).toarray()
A:sklearn.linear_model.tests.test_base.csr->csr_container(X)
A:sklearn.linear_model.tests.test_base.(csr_, y, _, _, _)->_preprocess_data(csr, y, fit_intercept=True)
A:sklearn.linear_model.tests.test_base.(X_, y_, _, _, _)->_preprocess_data(X, y, fit_intercept=True, copy=to_copy, check_input=False)
A:sklearn.linear_model.tests.test_base.X_32->load_iris().data.astype(np.float32)
A:sklearn.linear_model.tests.test_base.y_32->load_iris().target.astype(np.float32)
A:sklearn.linear_model.tests.test_base.X_64->load_iris().data.astype(np.float64)
A:sklearn.linear_model.tests.test_base.y_64->load_iris().target.astype(np.float64)
A:sklearn.linear_model.tests.test_base.(Xt_32, yt_32, X_mean_32, y_mean_32, X_scale_32)->_preprocess_data(X_32, y_32, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_base.(Xt_64, yt_64, X_mean_64, y_mean_64, X_scale_64)->_preprocess_data(X_64, y_64, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_base.(Xt_3264, yt_3264, X_mean_3264, y_mean_3264, X_scale_3264)->_preprocess_data(X_32, y_64, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_base.(Xt_6432, yt_6432, X_mean_6432, y_mean_6432, X_scale_6432)->_preprocess_data(X_64, y_32, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_base.expected_sqrt_sw->numpy.sqrt(sample_weight)
A:sklearn.linear_model.tests.test_base.(rescaled_X, rescaled_y, sqrt_sw)->_rescale_data(X, y, sample_weight)
A:sklearn.linear_model.tests.test_base.rescaled_X->rescaled_X.toarray().toarray()
A:sklearn.linear_model.tests.test_base.rescaled_y->rescaled_y.ravel().ravel()
A:sklearn.linear_model.tests.test_base.iris->load_iris()
A:sklearn.linear_model.tests.test_base.X_csr_32->csr_container(X_32)
A:sklearn.linear_model.tests.test_base.sample_weight_32->numpy.arange(y_32.size, dtype=np.float32)
A:sklearn.linear_model.tests.test_base.X_csr_64->csr_container(X_64)
A:sklearn.linear_model.tests.test_base.sample_weight_64->numpy.arange(y_64.size, dtype=np.float64)
A:sklearn.linear_model.tests.test_base.(dataset_32, _)->make_dataset(X_32, y_32, sample_weight_32)
A:sklearn.linear_model.tests.test_base.(dataset_64, _)->make_dataset(X_64, y_64, sample_weight_64)
A:sklearn.linear_model.tests.test_base.(xi_32, yi_32, _, _)->dataset_32._next_py()
A:sklearn.linear_model.tests.test_base.(xi_64, yi_64, _, _)->dataset_64._next_py()
A:sklearn.linear_model.tests.test_base.(datasetcsr_32, _)->make_dataset(X_csr_32, y_32, sample_weight_32)
A:sklearn.linear_model.tests.test_base.(datasetcsr_64, _)->make_dataset(X_csr_64, y_64, sample_weight_64)
A:sklearn.linear_model.tests.test_base.(xicsr_32, yicsr_32, _, _)->datasetcsr_32._next_py()
A:sklearn.linear_model.tests.test_base.(xicsr_64, yicsr_64, _, _)->datasetcsr_64._next_py()
A:sklearn.linear_model.tests.test_base.coef->reg.fit(X, y, sample_weight=sample_weight).fit(X, y, sample_weight=sample_weight).coef_.copy()
A:sklearn.linear_model.tests.test_base.sample_weight_0->numpy.random.RandomState(global_random_seed).uniform(low=0.01, high=2, size=X.shape[0]).copy()
A:sklearn.linear_model.tests.test_base.coef_0->reg.fit(X, y, sample_weight=sample_weight).fit(X, y, sample_weight=sample_weight).coef_.copy()
A:sklearn.linear_model.tests.test_base.y2->numpy.concatenate([y, y[:n_samples // 2]])
A:sklearn.linear_model.tests.test_base.sample_weight_1->numpy.random.RandomState(global_random_seed).uniform(low=0.01, high=2, size=X.shape[0]).copy()
A:sklearn.linear_model.tests.test_base.sample_weight_2->numpy.concatenate([sample_weight, sample_weight[:n_samples // 2]], axis=0)
A:sklearn.linear_model.tests.test_base.reg1->LinearRegression(**params).fit(X, y, sample_weight=sample_weight_1)
A:sklearn.linear_model.tests.test_base.reg2->LinearRegression(**params).fit(X2, y2, sample_weight=sample_weight_2)
sklearn.linear_model.tests.test_base.test_csr_preprocess_data(csr_container)
sklearn.linear_model.tests.test_base.test_dtype_preprocess_data(global_random_seed)
sklearn.linear_model.tests.test_base.test_fit_intercept()
sklearn.linear_model.tests.test_base.test_fused_types_make_dataset(csr_container)
sklearn.linear_model.tests.test_base.test_inplace_data_preprocessing(sparse_container,use_sw,global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression()
sklearn.linear_model.tests.test_base.test_linear_regression_multiple_outcome()
sklearn.linear_model.tests.test_base.test_linear_regression_pd_sparse_dataframe_warning()
sklearn.linear_model.tests.test_base.test_linear_regression_positive()
sklearn.linear_model.tests.test_base.test_linear_regression_positive_multiple_outcome(global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_positive_vs_nonpositive(global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_positive_vs_nonpositive_when_positive(global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_sample_weight_consistency(sparse_container,fit_intercept,global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_sample_weights(sparse_container,fit_intercept,global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_sparse(global_random_seed)
sklearn.linear_model.tests.test_base.test_linear_regression_sparse_equal_dense(fit_intercept,csr_container)
sklearn.linear_model.tests.test_base.test_linear_regression_sparse_multiple_outcome(global_random_seed,coo_container)
sklearn.linear_model.tests.test_base.test_preprocess_copy_data_no_checks(sparse_container,to_copy)
sklearn.linear_model.tests.test_base.test_preprocess_data(global_random_seed)
sklearn.linear_model.tests.test_base.test_preprocess_data_multioutput(global_random_seed,sparse_container)
sklearn.linear_model.tests.test_base.test_preprocess_data_weighted(sparse_container,global_random_seed)
sklearn.linear_model.tests.test_base.test_raises_value_error_if_positive_and_sparse()
sklearn.linear_model.tests.test_base.test_raises_value_error_if_sample_weights_greater_than_1d(n_samples,n_features)
sklearn.linear_model.tests.test_base.test_rescale_data(n_targets,sparse_container,global_random_seed)
sklearn.linear_model.tests.test_base.test_sparse_preprocess_data_offsets(global_random_seed,lil_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_logistic.py----------------------------------------
A:sklearn.linear_model.tests.test_logistic.pytestmark->pytest.mark.filterwarnings('error::sklearn.exceptions.ConvergenceWarning:sklearn.*')
A:sklearn.linear_model.tests.test_logistic.LogisticRegression->partial(LogisticRegressionDefault, random_state=0)
A:sklearn.linear_model.tests.test_logistic.LogisticRegressionCV->partial(LogisticRegressionCVDefault, random_state=0)
A:sklearn.linear_model.tests.test_logistic.iris->load_iris()
A:sklearn.linear_model.tests.test_logistic.n_samples->len(y)
A:sklearn.linear_model.tests.test_logistic.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_logistic.predicted->LogisticRegression(solver=solver, max_iter=0).fit(X, y).fit(X, y).predict(X)
A:sklearn.linear_model.tests.test_logistic.probabilities->LogisticRegression(solver=solver, max_iter=0).fit(X, y).predict_proba(iris.data)
A:sklearn.linear_model.tests.test_logistic.mock_scorer->MockScorer()
A:sklearn.linear_model.tests.test_logistic.lr->LogisticRegression(penalty='elasticnet', solver='saga', C=C, l1_ratio=l1_ratio, random_state=0, max_iter=250, tol=0.001)
A:sklearn.linear_model.tests.test_logistic.(X, y)->make_classification(n_samples=10, random_state=0)
A:sklearn.linear_model.tests.test_logistic.custom_score->LogisticRegression(penalty='elasticnet', solver='saga', C=C, l1_ratio=l1_ratio, random_state=0, max_iter=250, tol=0.001).score(X, lr.predict(X))
A:sklearn.linear_model.tests.test_logistic.pred->LogisticRegression(solver=solver, max_iter=0).fit(X, y).predict(iris.data)
A:sklearn.linear_model.tests.test_logistic.model->LR(penalty='elasticnet', solver='saga')
A:sklearn.linear_model.tests.test_logistic.target->(iris.target > 0).astype(np.intp)
A:sklearn.linear_model.tests.test_logistic.clf->LogisticRegression(solver=solver, max_iter=0).fit(X, y)
A:sklearn.linear_model.tests.test_logistic.mlr->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.decision->LogisticRegression(solver=solver, max_iter=0).fit(X, y).decision_function(X)
A:sklearn.linear_model.tests.test_logistic.proba->LogisticRegression(solver=solver, max_iter=0).fit(X, y).predict_proba(X)
A:sklearn.linear_model.tests.test_logistic.X->load_iris().data.copy()
A:sklearn.linear_model.tests.test_logistic.pred_d_d->LogisticRegression(solver=solver, max_iter=0).fit(X, y).decision_function(X)
A:sklearn.linear_model.tests.test_logistic.pred_s_d->LogisticRegression(solver=solver, max_iter=0).fit(X, y).decision_function(X)
A:sklearn.linear_model.tests.test_logistic.sp_data->coo_container(X)
A:sklearn.linear_model.tests.test_logistic.pred_s_s->LogisticRegression(solver=solver, max_iter=0).fit(X, y).decision_function(sp_data)
A:sklearn.linear_model.tests.test_logistic.pred_d_s->LogisticRegression(solver=solver, max_iter=0).fit(X, y).decision_function(sp_data)
A:sklearn.linear_model.tests.test_logistic.rng->numpy.random.RandomState(10)
A:sklearn.linear_model.tests.test_logistic.X_->numpy.random.RandomState(10).random_sample((5, 10))
A:sklearn.linear_model.tests.test_logistic.y_->numpy.ones(X_.shape[0])
A:sklearn.linear_model.tests.test_logistic.Xnan->numpy.array(X, dtype=np.float64)
A:sklearn.linear_model.tests.test_logistic.logistic->LogisticRegression(random_state=0)
A:sklearn.linear_model.tests.test_logistic.Cs->numpy.logspace(-4, 4, 3)
A:sklearn.linear_model.tests.test_logistic.(coefs, Cs, _)->f(_logistic_regression_path)(X, y, Cs=Cs, tol=1e-06, solver=solver, intercept_scaling=10000.0, random_state=0, multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.lr_coef->numpy.concatenate([lr.coef_.ravel(), lr.intercept_])
A:sklearn.linear_model.tests.test_logistic.lr1->LogisticRegression(random_state=0, dual=True, tol=0.001, solver='liblinear', multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.lr2->LogisticRegression(random_state=0, dual=True, tol=0.001, solver='liblinear', multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.lr3->LogisticRegression(random_state=8, dual=True, tol=0.001, solver='liblinear', multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.X_ref->numpy.random.RandomState(10).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_logistic.y->load_iris().target.copy()
A:sklearn.linear_model.tests.test_logistic.lr_cv->LogisticRegressionCV()
A:sklearn.linear_model.tests.test_logistic.coefs_paths->numpy.asarray(list(lrcv.coefs_paths_.values()))
A:sklearn.linear_model.tests.test_logistic.scores->numpy.asarray(list(lrcv.scores_.values()))
A:sklearn.linear_model.tests.test_logistic.params->dict(Cs=1, fit_intercept=False, multi_class='ovr', class_weight=class_weight, tol=1e-08)
A:sklearn.linear_model.tests.test_logistic.scorer->get_scorer(scoring + averaging)
A:sklearn.linear_model.tests.test_logistic.(X_ref, y)->make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=3, random_state=0)
A:sklearn.linear_model.tests.test_logistic.y_str->LabelEncoder().fit(['bar', 'baz', 'foo']).inverse_transform(y)
A:sklearn.linear_model.tests.test_logistic.lr_str->LogisticRegression(multi_class='multinomial')
A:sklearn.linear_model.tests.test_logistic.lr_cv_str->LogisticRegression(class_weight={'bar': 1, 'baz': 2, 'foo': 0}, multi_class='multinomial').fit(X_ref, y_str)
A:sklearn.linear_model.tests.test_logistic.csr->csr_container(X)
A:sklearn.linear_model.tests.test_logistic.clfs->LogisticRegressionCV()
A:sklearn.linear_model.tests.test_logistic.cv->StratifiedKFold(n_splits=5)
A:sklearn.linear_model.tests.test_logistic.precomputed_folds->list(cv.split(train, target))
A:sklearn.linear_model.tests.test_logistic.clf1->LogisticRegression(solver=solver, multi_class='ovr', class_weight='balanced')
A:sklearn.linear_model.tests.test_logistic.target_copy->(iris.target > 0).astype(np.intp).copy()
A:sklearn.linear_model.tests.test_logistic.clf_multi->LogisticRegression(multi_class='multinomial', solver='lbfgs')
A:sklearn.linear_model.tests.test_logistic.train->scale(train)
A:sklearn.linear_model.tests.test_logistic.multi_score->LogisticRegression(multi_class='multinomial', solver='lbfgs').score(train, target)
A:sklearn.linear_model.tests.test_logistic.ovr_score->LogisticRegression(solver=solver, max_iter=0).fit(X, y).score(train, target)
A:sklearn.linear_model.tests.test_logistic.n_classes->len(weight)
A:sklearn.linear_model.tests.test_logistic.clf_lbfgs->LogisticRegressionCV(solver='lbfgs', **params)
A:sklearn.linear_model.tests.test_logistic.clf_sw_none->LR(solver=solver, **kw)
A:sklearn.linear_model.tests.test_logistic.clf_sw_ones->LR(solver=solver, **kw)
A:sklearn.linear_model.tests.test_logistic.clf_sw_lbfgs->LR(**kw, tol=1e-05)
A:sklearn.linear_model.tests.test_logistic.clf_sw->LogisticRegression(solver='liblinear', fit_intercept=False, penalty='l2', dual=True, random_state=42, multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.clf_cw_12->LR(solver=solver, class_weight={0: 1, 1: 2}, **kw)
A:sklearn.linear_model.tests.test_logistic.clf_sw_12->LR(solver=solver, **kw)
A:sklearn.linear_model.tests.test_logistic.clf_cw->LogisticRegression(solver='liblinear', fit_intercept=False, class_weight={0: 1, 1: 2}, penalty='l2', dual=True, random_state=42, multi_class='ovr')
A:sklearn.linear_model.tests.test_logistic.class_weight->compute_class_weight('balanced', classes=classes, y=y)
A:sklearn.linear_model.tests.test_logistic.class_weight_dict->_compute_class_weight_dictionary(y)
A:sklearn.linear_model.tests.test_logistic.X_iris->scale(iris.data)
A:sklearn.linear_model.tests.test_logistic.clf2->LogisticRegression(solver=solver, multi_class='ovr', class_weight=class_weight_dict)
A:sklearn.linear_model.tests.test_logistic.ref_i->LogisticRegression(solver=solver, multi_class='multinomial', tol=1e-06)
A:sklearn.linear_model.tests.test_logistic.ref_w->LogisticRegression(solver=solver, multi_class='multinomial', fit_intercept=False, tol=1e-06)
A:sklearn.linear_model.tests.test_logistic.clf_i->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, max_iter=2000, tol=1e-07)
A:sklearn.linear_model.tests.test_logistic.clf_w->LogisticRegression(solver=solver, multi_class='multinomial', random_state=42, max_iter=2000, tol=1e-07, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.clf_path->LogisticRegressionCV(solver=solver, max_iter=2000, tol=1e-06, multi_class='multinomial', Cs=[1.0])
A:sklearn.linear_model.tests.test_logistic.X_noise->numpy.random.RandomState(10).normal(scale=0.1, size=(n_samples, 3))
A:sklearn.linear_model.tests.test_logistic.X_constant->numpy.zeros(shape=(n_samples, 2))
A:sklearn.linear_model.tests.test_logistic.lr_liblinear->LogisticRegression(penalty='l1', C=1.0, solver='liblinear', fit_intercept=False, multi_class='ovr', tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.lr_saga->LogisticRegression(penalty='l1', C=1.0, solver='saga', fit_intercept=False, multi_class='ovr', max_iter=1000, tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.lr_saga_dense->LogisticRegression(penalty='l1', C=1.0, solver='saga', fit_intercept=False, multi_class='ovr', max_iter=1000, tol=1e-10)
A:sklearn.linear_model.tests.test_logistic.common_params->dict(solver='saga', penalty=penalty, random_state=random_seed, max_iter=1000, tol=1e-12)
A:sklearn.linear_model.tests.test_logistic.clf_multi_loss->log_loss(y, clf_multi.predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.clf_ovr->LogisticRegression(multi_class='ovr', solver='lbfgs')
A:sklearn.linear_model.tests.test_logistic.clf_ovr_loss->log_loss(y, clf_ovr.predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.clf_wrong_loss->log_loss(y, clf_multi._predict_proba_lr(X))
A:sklearn.linear_model.tests.test_logistic.y_bin->load_iris().target.copy().copy()
A:sklearn.linear_model.tests.test_logistic.clf_cv->LogisticRegressionCV(tol=0.01, solver=solver, Cs=n_Cs, cv=n_cv_fold, random_state=42)
A:sklearn.linear_model.tests.test_logistic.cum_diff->numpy.sum(np.abs(coef_1 - clf.coef_))
A:sklearn.linear_model.tests.test_logistic.(X_sparse, y_sparse)->make_classification(n_samples=50, n_features=20, random_state=0)
A:sklearn.linear_model.tests.test_logistic.X_sparse->csr_container(X_sparse)
A:sklearn.linear_model.tests.test_logistic.saga->LogisticRegression(C=1.0 / (n_samples * alpha), solver='saga', multi_class='ovr', max_iter=200, fit_intercept=False, penalty=penalty, random_state=0, tol=1e-06)
A:sklearn.linear_model.tests.test_logistic.liblinear->LogisticRegression(C=1.0 / (n_samples * alpha), solver='liblinear', multi_class='ovr', max_iter=200, fit_intercept=False, penalty=penalty, random_state=0, tol=1e-06)
A:sklearn.linear_model.tests.test_logistic.X_32->numpy.array(X).astype(np.float32)
A:sklearn.linear_model.tests.test_logistic.y_32->numpy.array(Y1).astype(np.float32)
A:sklearn.linear_model.tests.test_logistic.X_64->numpy.array(X).astype(np.float64)
A:sklearn.linear_model.tests.test_logistic.y_64->numpy.array(Y1).astype(np.float64)
A:sklearn.linear_model.tests.test_logistic.X_sparse_32->csr_container(X, dtype=np.float32)
A:sklearn.linear_model.tests.test_logistic.X_sparse_64->csr_container(X, dtype=np.float64)
A:sklearn.linear_model.tests.test_logistic.lr_templ->LogisticRegression(solver=solver, multi_class=multi_class, random_state=42, tol=solver_tol, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_logistic.lr_32->clone(lr_templ)
A:sklearn.linear_model.tests.test_logistic.lr_32_sparse->clone(lr_templ)
A:sklearn.linear_model.tests.test_logistic.lr_64->clone(lr_templ)
A:sklearn.linear_model.tests.test_logistic.lr_64_sparse->clone(lr_templ)
A:sklearn.linear_model.tests.test_logistic.lr_no_ws->LogisticRegression(multi_class='multinomial', solver='sag', warm_start=False, random_state=0)
A:sklearn.linear_model.tests.test_logistic.lr_ws->LogisticRegression(multi_class='multinomial', solver='sag', warm_start=True, random_state=0)
A:sklearn.linear_model.tests.test_logistic.lr_no_ws_loss->log_loss(y, lr_no_ws.fit(X, y).predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.lr_ws_loss->log_loss(y, lr_ws.predict_proba(X))
A:sklearn.linear_model.tests.test_logistic.coeffs->list()
A:sklearn.linear_model.tests.test_logistic.lr_enet->LogisticRegression(penalty='elasticnet', solver='saga', random_state=0, C=C, l1_ratio=l1_ratio, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.lr_expected->LogisticRegression(penalty=penalty, C=C, solver='saga', random_state=0, tol=0.01)
A:sklearn.linear_model.tests.test_logistic.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.linear_model.tests.test_logistic.enet_clf->LogisticRegression(penalty='elasticnet', C=C, solver='saga', random_state=0, tol=0.01)
A:sklearn.linear_model.tests.test_logistic.gs->GridSearchCV(lr, param_grid, cv=cv)
A:sklearn.linear_model.tests.test_logistic.l1_clf->LogisticRegression(penalty='l1', C=C, solver='saga', random_state=0, tol=0.01)
A:sklearn.linear_model.tests.test_logistic.l2_clf->LogisticRegression(penalty='l2', C=C, solver='saga', random_state=0, tol=0.01)
A:sklearn.linear_model.tests.test_logistic.lr_l2->LogisticRegression(penalty='l2', solver='saga', random_state=0, C=C, fit_intercept=False)
A:sklearn.linear_model.tests.test_logistic.coef->LogisticRegression(penalty='elasticnet', solver='saga', C=C, l1_ratio=l1_ratio, random_state=0, max_iter=250, tol=0.001).coef_.ravel()
A:sklearn.linear_model.tests.test_logistic.l1_ratios->numpy.linspace(0, 1, 2)
A:sklearn.linear_model.tests.test_logistic.lrcv->LogisticRegressionCV(penalty='elasticnet', solver='saga', l1_ratios=l1_ratios, Cs=Cs, cv=cv, random_state=0, max_iter=250, tol=0.001)
A:sklearn.linear_model.tests.test_logistic.sgd->SGDClassifier(penalty='elasticnet', random_state=1, fit_intercept=False, tol=None, max_iter=2000, l1_ratio=l1_ratio, alpha=1.0 / C / n_samples, loss='log_loss')
A:sklearn.linear_model.tests.test_logistic.log->LogisticRegression(penalty='elasticnet', random_state=1, fit_intercept=False, tol=1e-05, max_iter=1000, l1_ratio=l1_ratio, C=C, solver='saga')
A:sklearn.linear_model.tests.test_logistic.(coefs, _, _)->_logistic_regression_path(X, y, penalty='l1', Cs=Cs, solver='saga', random_state=0, multi_class='multinomial')
A:sklearn.linear_model.tests.test_logistic.scaled_data->scale(iris.data)
A:sklearn.linear_model.tests.test_logistic.est_auto_bin->fit(X, y_bin, multi_class='auto', solver=solver)
A:sklearn.linear_model.tests.test_logistic.est_ovr_bin->fit(X, y_bin, multi_class='ovr', solver=solver)
A:sklearn.linear_model.tests.test_logistic.est_auto_multi->fit(X, y_multi, multi_class='auto', solver=solver)
A:sklearn.linear_model.tests.test_logistic.est_ovr_multi->fit(X, y_multi, multi_class='ovr', solver=solver)
A:sklearn.linear_model.tests.test_logistic.est_multi_multi->fit(X, y_multi, multi_class='multinomial', solver=solver)
A:sklearn.linear_model.tests.test_logistic.lr_none->LogisticRegression(penalty=None, solver=solver, random_state=0)
A:sklearn.linear_model.tests.test_logistic.lr_l2_C_inf->LogisticRegression(penalty='l2', C=np.inf, solver=solver, random_state=0)
A:sklearn.linear_model.tests.test_logistic.pred_none->LogisticRegression(penalty=None, solver=solver, random_state=0).fit(X, y).predict(X)
A:sklearn.linear_model.tests.test_logistic.pred_l2_C_inf->LogisticRegression(penalty='l2', C=np.inf, solver=solver, random_state=0).fit(X, y).predict(X)
A:sklearn.linear_model.tests.test_logistic.X2->numpy.vstack([X, X])
A:sklearn.linear_model.tests.test_logistic.y2->numpy.hstack([y, 3 - y])
A:sklearn.linear_model.tests.test_logistic.sample_weight->numpy.ones(len(y))
A:sklearn.linear_model.tests.test_logistic.(X2, y2, sample_weight)->shuffle(X2, y2, sample_weight, random_state=0)
A:sklearn.linear_model.tests.test_logistic.base_clf->LogisticRegression(solver='liblinear', random_state=42)
A:sklearn.linear_model.tests.test_logistic.clf_no_weight->clone(base_clf).fit(X, y)
A:sklearn.linear_model.tests.test_logistic.clf_with_weight->clone(base_clf).fit(X2, y2, sample_weight=sample_weight)
A:sklearn.linear_model.tests.test_logistic.X_clf_no_weight->getattr(clf_no_weight, method)(X)
A:sklearn.linear_model.tests.test_logistic.X_clf_with_weight->getattr(clf_with_weight, method)(X)
A:sklearn.linear_model.tests.test_logistic.avg_scores_lrcv->LogisticRegressionCV(penalty='elasticnet', solver='saga', l1_ratios=l1_ratios, Cs=Cs, cv=cv, random_state=0, max_iter=250, tol=0.001).scores_[1].mean(axis=0)
A:sklearn.linear_model.tests.test_logistic.avg_score_lr->cross_val_score(lr, X, y, cv=cv).mean()
A:sklearn.linear_model.tests.test_logistic.X_scaled->scale(iris.data)
A:sklearn.linear_model.tests.test_logistic.n_features->len(X)
A:sklearn.linear_model.tests.test_logistic.W->numpy.ones(n_features)
A:sklearn.linear_model.tests.test_logistic.expected->numpy.ones(n_features).copy()
A:sklearn.linear_model.tests.test_logistic.X_prep->StandardScaler().fit_transform(X)
A:sklearn.linear_model.tests.test_logistic.(X_t, y_t)->make_classification(n_samples=10, random_state=rng)
A:sklearn.linear_model.tests.test_logistic.scorer1->get_scorer('accuracy')
A:sklearn.linear_model.tests.test_logistic.lr_cv1->LogisticRegressionCV(scoring=scorer1)
A:sklearn.linear_model.tests.test_logistic.scorer2->get_scorer('accuracy')
A:sklearn.linear_model.tests.test_logistic.lr_cv2->LogisticRegressionCV(scoring=scorer2)
A:sklearn.linear_model.tests.test_logistic.score_1->LogisticRegressionCV(scoring=scorer1).score(X_t, y_t, **kwargs)
A:sklearn.linear_model.tests.test_logistic.score_2->LogisticRegressionCV(scoring=scorer2).score(X_t, y_t, **kwargs)
sklearn.linear_model.tests.test_logistic._compute_class_weight_dictionary(y)
sklearn.linear_model.tests.test_logistic.check_predictions(clf,X,y)
sklearn.linear_model.tests.test_logistic.test_LogisticRegressionCV_GridSearchCV_elastic_net(multi_class)
sklearn.linear_model.tests.test_logistic.test_LogisticRegressionCV_GridSearchCV_elastic_net_ovr()
sklearn.linear_model.tests.test_logistic.test_LogisticRegressionCV_elasticnet_attribute_shapes()
sklearn.linear_model.tests.test_logistic.test_LogisticRegressionCV_no_refit(penalty,multi_class)
sklearn.linear_model.tests.test_logistic.test_LogisticRegression_elastic_net_objective(C,l1_ratio)
sklearn.linear_model.tests.test_logistic.test_check_solver_option(LR)
sklearn.linear_model.tests.test_logistic.test_consistency_path()
sklearn.linear_model.tests.test_logistic.test_dtype_match(solver,multi_class,fit_intercept,csr_container)
sklearn.linear_model.tests.test_logistic.test_elastic_net_coeffs()
sklearn.linear_model.tests.test_logistic.test_elastic_net_l1_l2_equivalence(C,penalty,l1_ratio)
sklearn.linear_model.tests.test_logistic.test_elastic_net_versus_sgd(C,l1_ratio)
sklearn.linear_model.tests.test_logistic.test_elastic_net_vs_l1_l2(C)
sklearn.linear_model.tests.test_logistic.test_elasticnet_l1_ratio_err_helpful(LR)
sklearn.linear_model.tests.test_logistic.test_inconsistent_input()
sklearn.linear_model.tests.test_logistic.test_l1_ratio_non_elasticnet()
sklearn.linear_model.tests.test_logistic.test_large_sparse_matrix(solver,global_random_seed,csr_container)
sklearn.linear_model.tests.test_logistic.test_liblinear_decision_function_zero()
sklearn.linear_model.tests.test_logistic.test_liblinear_dual_random_state()
sklearn.linear_model.tests.test_logistic.test_liblinear_logregcv_sparse(csr_container)
sklearn.linear_model.tests.test_logistic.test_liblinear_not_stuck()
sklearn.linear_model.tests.test_logistic.test_logistic_cv()
sklearn.linear_model.tests.test_logistic.test_logistic_cv_mock_scorer()
sklearn.linear_model.tests.test_logistic.test_logistic_cv_multinomial_score(scoring,multiclass_agg_list)
sklearn.linear_model.tests.test_logistic.test_logistic_cv_sparse(csr_container)
sklearn.linear_model.tests.test_logistic.test_logistic_regression_class_weights()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_cv_refit(random_seed,penalty)
sklearn.linear_model.tests.test_logistic.test_logistic_regression_multi_class_auto(est,solver)
sklearn.linear_model.tests.test_logistic.test_logistic_regression_multinomial()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_path_coefs_multinomial()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_path_convergence_fail()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_sample_weights()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_solvers()
sklearn.linear_model.tests.test_logistic.test_logistic_regression_solvers_multiclass()
sklearn.linear_model.tests.test_logistic.test_logistic_regressioncv_class_weights(weight,class_weight,global_random_seed)
sklearn.linear_model.tests.test_logistic.test_logisticregression_liblinear_sample_weight(params)
sklearn.linear_model.tests.test_logistic.test_logreg_intercept_scaling_zero()
sklearn.linear_model.tests.test_logistic.test_logreg_l1()
sklearn.linear_model.tests.test_logistic.test_logreg_l1_sparse_data(csr_container)
sklearn.linear_model.tests.test_logistic.test_logreg_predict_proba_multinomial()
sklearn.linear_model.tests.test_logistic.test_lr_cv_scores_differ_when_sample_weight_is_requested()
sklearn.linear_model.tests.test_logistic.test_lr_cv_scores_without_enabling_metadata_routing()
sklearn.linear_model.tests.test_logistic.test_lr_liblinear_warning()
sklearn.linear_model.tests.test_logistic.test_max_iter(max_iter,multi_class,solver,message)
sklearn.linear_model.tests.test_logistic.test_multinomial_binary(solver)
sklearn.linear_model.tests.test_logistic.test_multinomial_binary_probabilities(global_random_seed)
sklearn.linear_model.tests.test_logistic.test_multinomial_identifiability_on_iris(fit_intercept)
sklearn.linear_model.tests.test_logistic.test_multinomial_logistic_regression_string_inputs()
sklearn.linear_model.tests.test_logistic.test_n_iter(solver)
sklearn.linear_model.tests.test_logistic.test_nan()
sklearn.linear_model.tests.test_logistic.test_ovr_multinomial_iris()
sklearn.linear_model.tests.test_logistic.test_passing_params_without_enabling_metadata_routing()
sklearn.linear_model.tests.test_logistic.test_penalty_none(solver)
sklearn.linear_model.tests.test_logistic.test_predict_2_classes(csr_container)
sklearn.linear_model.tests.test_logistic.test_predict_3_classes(csr_container)
sklearn.linear_model.tests.test_logistic.test_predict_iris(clf)
sklearn.linear_model.tests.test_logistic.test_saga_sparse(csr_container)
sklearn.linear_model.tests.test_logistic.test_saga_vs_liblinear(csr_container)
sklearn.linear_model.tests.test_logistic.test_sample_weight_not_modified(multi_class,class_weight)
sklearn.linear_model.tests.test_logistic.test_scores_attribute_layout_elasticnet()
sklearn.linear_model.tests.test_logistic.test_single_feature_newton_cg()
sklearn.linear_model.tests.test_logistic.test_sparsify(coo_container)
sklearn.linear_model.tests.test_logistic.test_warm_start(solver,warm_start,fit_intercept,multi_class)
sklearn.linear_model.tests.test_logistic.test_warm_start_converge_LR()
sklearn.linear_model.tests.test_logistic.test_write_parameters()
sklearn.linear_model.tests.test_logistic.test_zero_max_iter(solver)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_theil_sen.py----------------------------------------
A:sklearn.linear_model.tests.test_theil_sen.random_state->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_theil_sen.x->numpy.random.RandomState(0).normal(size=n_samples)
A:sklearn.linear_model.tests.test_theil_sen.X->numpy.random.RandomState(0).normal(size=(n_samples, n_features))
A:sklearn.linear_model.tests.test_theil_sen.w->numpy.array([5.0, 10.0, 42.0, 7.0])
A:sklearn.linear_model.tests.test_theil_sen.ix->numpy.random.RandomState(0).randint(0, n_samples, size=n_outliers)
A:sklearn.linear_model.tests.test_theil_sen.new_y->_modified_weiszfeld_step(X, y)
A:sklearn.linear_model.tests.test_theil_sen.y->numpy.random.RandomState(0).normal(size=n_samples)
A:sklearn.linear_model.tests.test_theil_sen.(_, median)->_spatial_median(X, max_iter=100, tol=1e-06)
A:sklearn.linear_model.tests.test_theil_sen.true_median->numpy.median(X.ravel())
A:sklearn.linear_model.tests.test_theil_sen.dists->numpy.array([norm(x - y) for x in X])
A:sklearn.linear_model.tests.test_theil_sen.fermat_weber->fmin_bfgs(cost_func, median, disp=False)
A:sklearn.linear_model.tests.test_theil_sen.(X, y, w, c)->gen_toy_problem_2d()
A:sklearn.linear_model.tests.test_theil_sen.lstq->LinearRegression(fit_intercept=False).fit(X, y)
A:sklearn.linear_model.tests.test_theil_sen.theil_sen->TheilSenRegressor(fit_intercept=True, random_state=0).fit(X, y)
A:sklearn.linear_model.tests.test_theil_sen.bp->_breakdown_point(10000000000.0, 2)
A:sklearn.linear_model.tests.test_theil_sen.y_pred->TheilSenRegressor(fit_intercept=True, random_state=0).fit(X, y).predict(X)
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_1d(intercept=True)
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_2d()
sklearn.linear_model.tests.test_theil_sen.gen_toy_problem_4d()
sklearn.linear_model.tests.test_theil_sen.no_stdout_stderr()
sklearn.linear_model.tests.test_theil_sen.test_calc_breakdown_point()
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_invalid_input(param,ExceptionCls,match)
sklearn.linear_model.tests.test_theil_sen.test_checksubparams_n_subsamples_if_less_samples_than_features()
sklearn.linear_model.tests.test_theil_sen.test_less_samples_than_features()
sklearn.linear_model.tests.test_theil_sen.test_modweiszfeld_step_1d()
sklearn.linear_model.tests.test_theil_sen.test_modweiszfeld_step_2d()
sklearn.linear_model.tests.test_theil_sen.test_spatial_median_1d()
sklearn.linear_model.tests.test_theil_sen.test_spatial_median_2d()
sklearn.linear_model.tests.test_theil_sen.test_subpopulation()
sklearn.linear_model.tests.test_theil_sen.test_subsamples()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_1d()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_1d_no_intercept()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_2d()
sklearn.linear_model.tests.test_theil_sen.test_theil_sen_parallel()
sklearn.linear_model.tests.test_theil_sen.test_verbosity()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_huber.py----------------------------------------
A:sklearn.linear_model.tests.test_huber.rng->numpy.random.RandomState(1)
A:sklearn.linear_model.tests.test_huber.(X, y)->make_regression(n_samples=200, n_features=2, noise=4.0, random_state=0)
A:sklearn.linear_model.tests.test_huber.num_noise->int(0.1 * n_samples)
A:sklearn.linear_model.tests.test_huber.random_samples->numpy.random.RandomState(1).randint(0, n_samples, num_noise)
A:sklearn.linear_model.tests.test_huber.lr->LinearRegression()
A:sklearn.linear_model.tests.test_huber.huber->HuberRegressor(alpha=0.01)
A:sklearn.linear_model.tests.test_huber.sample_weight->numpy.ones(X.shape[0])
A:sklearn.linear_model.tests.test_huber.w->numpy.random.RandomState(1).randn(n_features)
A:sklearn.linear_model.tests.test_huber.w[-1]->numpy.abs(w[-1])
A:sklearn.linear_model.tests.test_huber.grad_same->scipy.optimize.check_grad(loss_func, grad_func, w, X, y, 0.01, 0.1, sample_weight)
A:sklearn.linear_model.tests.test_huber.scale->max(np.mean(np.abs(huber.coef_)), np.mean(np.abs(huber.intercept_)))
A:sklearn.linear_model.tests.test_huber.X_new->numpy.vstack((X, np.vstack((X[1], X[1], X[3]))))
A:sklearn.linear_model.tests.test_huber.y_new->numpy.concatenate((y, [y[1]], [y[1]], [y[3]]))
A:sklearn.linear_model.tests.test_huber.X_csr->csr_container(X)
A:sklearn.linear_model.tests.test_huber.huber_sparse->HuberRegressor(alpha=0.1)
A:sklearn.linear_model.tests.test_huber.sgdreg->SGDRegressor(alpha=0.0, loss='huber', shuffle=True, random_state=0, max_iter=10000, fit_intercept=False, epsilon=1.35, tol=None)
A:sklearn.linear_model.tests.test_huber.huber_warm->HuberRegressor(alpha=1.0, max_iter=10000, warm_start=True, tol=0.1)
A:sklearn.linear_model.tests.test_huber.huber_warm_coef->HuberRegressor(alpha=1.0, max_iter=10000, warm_start=True, tol=0.1).coef_.copy()
A:sklearn.linear_model.tests.test_huber.huber_score->HuberRegressor(alpha=0.01).score(X[mask], y[mask])
A:sklearn.linear_model.tests.test_huber.huber_outlier_score->HuberRegressor(alpha=0.01).score(X[~mask], y[~mask])
A:sklearn.linear_model.tests.test_huber.ridge->Ridge(alpha=0.01)
A:sklearn.linear_model.tests.test_huber.ridge_score->Ridge(alpha=0.01).score(X[mask], y[mask])
A:sklearn.linear_model.tests.test_huber.ridge_outlier_score->Ridge(alpha=0.01).score(X[~mask], y[~mask])
sklearn.linear_model.tests.test_huber.make_regression_with_outliers(n_samples=50,n_features=20)
sklearn.linear_model.tests.test_huber.test_huber_and_sgd_same_results()
sklearn.linear_model.tests.test_huber.test_huber_better_r2_score()
sklearn.linear_model.tests.test_huber.test_huber_bool()
sklearn.linear_model.tests.test_huber.test_huber_equals_lr_for_high_epsilon()
sklearn.linear_model.tests.test_huber.test_huber_gradient()
sklearn.linear_model.tests.test_huber.test_huber_max_iter()
sklearn.linear_model.tests.test_huber.test_huber_sample_weights(csr_container)
sklearn.linear_model.tests.test_huber.test_huber_scaling_invariant()
sklearn.linear_model.tests.test_huber.test_huber_sparse(csr_container)
sklearn.linear_model.tests.test_huber.test_huber_warm_start()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_bayes.py----------------------------------------
A:sklearn.linear_model.tests.test_bayes.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_bayes.clf->ARDRegression(compute_score=True)
A:sklearn.linear_model.tests.test_bayes.M_inv_dot_y->numpy.linalg.solve(M, y)
A:sklearn.linear_model.tests.test_bayes.X->numpy.array([[1, 1], [3, 4], [5, 7], [4, 1], [2, 6], [3, 10], [3, 2]])
A:sklearn.linear_model.tests.test_bayes.br_model->BayesianRidge(compute_score=True).fit(X, y, sample_weight=w)
A:sklearn.linear_model.tests.test_bayes.rr_model->Ridge(alpha=br_model.lambda_ / br_model.alpha_).fit(X, y, sample_weight=w)
A:sklearn.linear_model.tests.test_bayes.Y->numpy.array([1, 2, 3])
A:sklearn.linear_model.tests.test_bayes.y->f_noise(X, noise_mult)
A:sklearn.linear_model.tests.test_bayes.reg->ARDRegression()
A:sklearn.linear_model.tests.test_bayes.r2->ARDRegression().fit(X, y).score(X, y)
A:sklearn.linear_model.tests.test_bayes.random_state->check_random_state(42)
A:sklearn.linear_model.tests.test_bayes.constant_value->check_random_state(42).rand()
A:sklearn.linear_model.tests.test_bayes.expected->numpy.full(n_samples, constant_value, dtype=np.array(constant_value).dtype)
A:sklearn.linear_model.tests.test_bayes.y_pred->ARDRegression(compute_score=True).fit(X, y).predict(X)
A:sklearn.linear_model.tests.test_bayes.(_, y_std)->ARDRegression(compute_score=True).fit(X, y).predict(X, return_std=True)
A:sklearn.linear_model.tests.test_bayes.regressor->ARDRegression()
A:sklearn.linear_model.tests.test_bayes.abs_coef_error->numpy.abs(1 - regressor.coef_[1])
A:sklearn.linear_model.tests.test_bayes.w->numpy.array([1.0, 0.0, 1.0, -1.0, 0.0])
A:sklearn.linear_model.tests.test_bayes.X_test->numpy.random.random((n_test, d))
A:sklearn.linear_model.tests.test_bayes.m1->BayesianRidge()
A:sklearn.linear_model.tests.test_bayes.(y_mean1, y_std1)->BayesianRidge().predict(X_test, return_std=True)
A:sklearn.linear_model.tests.test_bayes.m2->ARDRegression()
A:sklearn.linear_model.tests.test_bayes.(y_mean2, y_std2)->ARDRegression().predict(X_test, return_std=True)
A:sklearn.linear_model.tests.test_bayes.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.linear_model.tests.test_bayes.lmbda->numpy.arange(1, n_features + 1)
A:sklearn.linear_model.tests.test_bayes.keep_lambda->numpy.array([True] * n_features)
A:sklearn.linear_model.tests.test_bayes.sigma->ARDRegression()._update_sigma(X, alpha, lmbda, keep_lambda)
A:sklearn.linear_model.tests.test_bayes.sigma_woodbury->ARDRegression()._update_sigma_woodbury(X, alpha, lmbda, keep_lambda)
A:sklearn.linear_model.tests.test_bayes.model->Estimator(n_iter=5, max_iter=5)
A:sklearn.linear_model.tests.test_bayes.(y_mean, y_std)->Estimator(n_iter=5, max_iter=5).predict(X, return_std=True)
sklearn.linear_model.tests.test_bayes.test_ard_accuracy_on_easy_problem(global_random_seed,n_samples,n_features)
sklearn.linear_model.tests.test_bayes.test_bayesian_initial_params()
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_ard_max_iter_and_n_iter_both_set(Estimator)
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_ard_n_iter_deprecated(Estimator)
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_parameter()
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_score_values()
sklearn.linear_model.tests.test_bayes.test_bayesian_ridge_scores()
sklearn.linear_model.tests.test_bayes.test_bayesian_sample_weights()
sklearn.linear_model.tests.test_bayes.test_dtype_correctness(Estimator)
sklearn.linear_model.tests.test_bayes.test_dtype_match(dtype,Estimator)
sklearn.linear_model.tests.test_bayes.test_prediction_bayesian_ridge_ard_with_constant_input()
sklearn.linear_model.tests.test_bayes.test_return_std()
sklearn.linear_model.tests.test_bayes.test_std_bayesian_ridge_ard_with_constant_input()
sklearn.linear_model.tests.test_bayes.test_toy_ard_object()
sklearn.linear_model.tests.test_bayes.test_toy_bayesian_ridge_object()
sklearn.linear_model.tests.test_bayes.test_update_of_sigma_in_ard()
sklearn.linear_model.tests.test_bayes.test_update_sigma(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_linear_loss.py----------------------------------------
A:sklearn.linear_model.tests.test_linear_loss.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_linear_loss.X->X_container(X)
A:sklearn.linear_model.tests.test_linear_loss.coef->coef.ravel(order='F').ravel(order='F')
A:sklearn.linear_model.tests.test_linear_loss.coef.flat[:]->numpy.random.RandomState(42).uniform(low=coef_bound[0], high=coef_bound[1], size=n_dof)
A:sklearn.linear_model.tests.test_linear_loss.proba->linear_model_loss.base_loss.link.inverse(raw_prediction)
A:sklearn.linear_model.tests.test_linear_loss.s->numpy.random.RandomState(42).randn(*coef.shape)
A:sklearn.linear_model.tests.test_linear_loss.k->(s < r).sum(axis=1)
A:sklearn.linear_model.tests.test_linear_loss.y->linear_model_loss.base_loss.link.inverse(raw_prediction + rng.uniform(low=-1, high=1, size=n_samples))
A:sklearn.linear_model.tests.test_linear_loss.loss->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_linear_loss.(X, y, coef)->random_X_y_coef(linear_model_loss=loss, n_samples=n_samples, n_features=n_features, seed=42)
A:sklearn.linear_model.tests.test_linear_loss.sample_weight->numpy.linspace(1, y.shape[0], num=y.shape[0])
A:sklearn.linear_model.tests.test_linear_loss.l1->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.g1->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient(coef, X, y)
A:sklearn.linear_model.tests.test_linear_loss.(l2, g2)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss_gradient(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(g3, h3)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(g4, h4, _)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.l1_sp->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.g1_sp->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(l2_sp, g2_sp)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss_gradient(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(g3_sp, h3_sp)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(g4_sp, h4_sp, _)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.loss_inter->LinearModelLoss(base_loss=base_loss(), fit_intercept=True)
A:sklearn.linear_model.tests.test_linear_loss.(l, g)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss_gradient(coef, X, y)
A:sklearn.linear_model.tests.test_linear_loss.(_, hessp)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(l_inter, g_inter)->LinearModelLoss(base_loss=base_loss(), fit_intercept=True).loss_gradient(coef, X_inter, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.(_, hessp_inter)->LinearModelLoss(base_loss=base_loss(), fit_intercept=True).gradient_hessian_product(coef, X_inter, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.h->hessp(s)
A:sklearn.linear_model.tests.test_linear_loss.h_inter->hessp_inter(s)
A:sklearn.linear_model.tests.test_linear_loss.(g, hessp)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength)
A:sklearn.linear_model.tests.test_linear_loss.approx_g1->scipy.optimize.approx_fprime(coef, lambda coef: loss.loss(coef - eps, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength), 2 * eps)
A:sklearn.linear_model.tests.test_linear_loss.approx_g2->scipy.optimize.approx_fprime(coef, lambda coef: loss.loss(coef - 2 * eps, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength), 4 * eps)
A:sklearn.linear_model.tests.test_linear_loss.vector->numpy.zeros_like(g)
A:sklearn.linear_model.tests.test_linear_loss.hess_col->hessp(vector)
A:sklearn.linear_model.tests.test_linear_loss.d_x->numpy.linspace(-eps, eps, 30)
A:sklearn.linear_model.tests.test_linear_loss.d_grad->numpy.array([loss.gradient(coef + t * vector, X, y, sample_weight=sample_weight, l2_reg_strength=l2_reg_strength) for t in d_x])
A:sklearn.linear_model.tests.test_linear_loss.approx_hess_col->scipy.linalg.lstsq(d_x[:, np.newaxis], d_grad)[0].ravel()
A:sklearn.linear_model.tests.test_linear_loss.(g2, hessp)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef, X, y)
A:sklearn.linear_model.tests.test_linear_loss.coef_r->coef.ravel(order='F').ravel(order='F').ravel(order='F')
A:sklearn.linear_model.tests.test_linear_loss.s_r->numpy.random.RandomState(42).randn(*coef.shape).ravel(order='F')
A:sklearn.linear_model.tests.test_linear_loss.(l_r, g_r)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).loss_gradient(coef_r, X, y)
A:sklearn.linear_model.tests.test_linear_loss.g1_r->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient(coef_r, X, y)
A:sklearn.linear_model.tests.test_linear_loss.(g2_r, hessp_r)->LinearModelLoss(base_loss=HalfMultinomialLoss(), fit_intercept=fit_intercept).gradient_hessian_product(coef_r, X, y)
A:sklearn.linear_model.tests.test_linear_loss.h_r->hessp_r(s_r)
sklearn.linear_model.tests.test_linear_loss.random_X_y_coef(linear_model_loss,n_samples,n_features,coef_bound=(-2,2),seed=42)
sklearn.linear_model.tests.test_linear_loss.test_gradients_hessians_numerically(base_loss,fit_intercept,sample_weight,l2_reg_strength)
sklearn.linear_model.tests.test_linear_loss.test_init_zero_coef(base_loss,fit_intercept,n_features,dtype)
sklearn.linear_model.tests.test_linear_loss.test_loss_grad_hess_are_the_same(base_loss,fit_intercept,sample_weight,l2_reg_strength,csr_container)
sklearn.linear_model.tests.test_linear_loss.test_loss_gradients_hessp_intercept(base_loss,sample_weight,l2_reg_strength,X_container)
sklearn.linear_model.tests.test_linear_loss.test_multinomial_coef_shape(fit_intercept)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_omp.py----------------------------------------
A:sklearn.linear_model.tests.test_omp.(y, X, gamma)->make_sparse_coded_signal(n_samples=n_targets, n_components=n_features, n_features=n_samples, n_nonzero_coefs=n_nonzero_coefs, random_state=0)
A:sklearn.linear_model.tests.test_omp.gamma->numpy.zeros(n_features)
A:sklearn.linear_model.tests.test_omp.gamma_gram->orthogonal_mp_gram(G_readonly, Xy_readonly[:, 0], n_nonzero_coefs=5, copy_Gram=False, copy_Xy=False)
A:sklearn.linear_model.tests.test_omp.(idx,)->gamma[:, 0].nonzero()
A:sklearn.linear_model.tests.test_omp.gamma_rec->orthogonal_mp(X, y[:, 0], n_nonzero_coefs=5)
A:sklearn.linear_model.tests.test_omp.G_readonly->G.copy()
A:sklearn.linear_model.tests.test_omp.Xy_readonly->Xy.copy()
A:sklearn.linear_model.tests.test_omp.omp->OrthogonalMatchingPursuit(n_nonzero_coefs=n_features)
A:sklearn.linear_model.tests.test_omp.coef_normalized->OrthogonalMatchingPursuit(n_nonzero_coefs=n_features).coef_[0].copy()
A:sklearn.linear_model.tests.test_omp.newX->rng.randn(n_samples, n_features).copy()
A:sklearn.linear_model.tests.test_omp.newy->numpy.dot(newX, gamma)
A:sklearn.linear_model.tests.test_omp.new_y->numpy.dot(X, gamma)
A:sklearn.linear_model.tests.test_omp.new_Xy->numpy.dot(X.T, new_y)
A:sklearn.linear_model.tests.test_omp.gamma_hat->orthogonal_mp(X, new_y, n_nonzero_coefs=2)
A:sklearn.linear_model.tests.test_omp.gamma_hat_gram->orthogonal_mp_gram(G, new_Xy, n_nonzero_coefs=2)
A:sklearn.linear_model.tests.test_omp.y_empty->numpy.zeros_like(y)
A:sklearn.linear_model.tests.test_omp.Xy_empty->numpy.dot(X.T, y_empty)
A:sklearn.linear_model.tests.test_omp.gamma_empty->ignore_warnings(orthogonal_mp)(X, y_empty, n_nonzero_coefs=1)
A:sklearn.linear_model.tests.test_omp.gamma_empty_gram->ignore_warnings(orthogonal_mp)(G, Xy_empty, n_nonzero_coefs=1)
A:sklearn.linear_model.tests.test_omp.path->orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=True, precompute=True)
A:sklearn.linear_model.tests.test_omp.last->orthogonal_mp(X, y, n_nonzero_coefs=5, return_path=False, precompute=True)
A:sklearn.linear_model.tests.test_omp.ompcv->OrthogonalMatchingPursuitCV(fit_intercept=False, max_iter=10)
A:sklearn.linear_model.tests.test_omp.rng->check_random_state(0)
A:sklearn.linear_model.tests.test_omp.X->check_random_state(0).randn(n_samples, n_features)
A:sklearn.linear_model.tests.test_omp.Y->check_random_state(0).randn(n_samples, n_targets)
A:sklearn.linear_model.tests.test_omp.lstsq->LinearRegression()
A:sklearn.linear_model.tests.test_omp.coef->orthogonal_mp_gram(G.astype(data_type), Xy.astype(data_type), n_nonzero_coefs=5)
A:sklearn.linear_model.tests.test_omp.coef_32->orthogonal_mp_gram(G.astype(np.float32), Xy.astype(np.float32), n_nonzero_coefs=5)
A:sklearn.linear_model.tests.test_omp.coef_64->orthogonal_mp_gram(G.astype(np.float32), Xy.astype(np.float64), n_nonzero_coefs=5)
sklearn.linear_model.tests.test_omp.test_bad_input(positional_params,keyword_params)
sklearn.linear_model.tests.test_omp.test_correct_shapes()
sklearn.linear_model.tests.test_omp.test_correct_shapes_gram()
sklearn.linear_model.tests.test_omp.test_estimator()
sklearn.linear_model.tests.test_omp.test_identical_regressors()
sklearn.linear_model.tests.test_omp.test_n_nonzero_coefs()
sklearn.linear_model.tests.test_omp.test_no_atoms()
sklearn.linear_model.tests.test_omp.test_omp_cv()
sklearn.linear_model.tests.test_omp.test_omp_gram_dtype_match(data_type)
sklearn.linear_model.tests.test_omp.test_omp_gram_numerical_consistency()
sklearn.linear_model.tests.test_omp.test_omp_path()
sklearn.linear_model.tests.test_omp.test_omp_reaches_least_squares()
sklearn.linear_model.tests.test_omp.test_omp_return_path_prop_with_gram()
sklearn.linear_model.tests.test_omp.test_orthogonal_mp_gram_readonly()
sklearn.linear_model.tests.test_omp.test_perfect_signal_recovery()
sklearn.linear_model.tests.test_omp.test_swapped_regressors()
sklearn.linear_model.tests.test_omp.test_tol()
sklearn.linear_model.tests.test_omp.test_unreachable_accuracy()
sklearn.linear_model.tests.test_omp.test_with_without_gram()
sklearn.linear_model.tests.test_omp.test_with_without_gram_tol()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_sparse_coordinate_descent.py----------------------------------------
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clf->ElasticNet(alpha=0.1, copy_X=copy_X, random_state=rng)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.X->scipy.sparse.random(100, 20, format='csc', random_state=rng)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.T->lil_container((3, 1))
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.pred->ElasticNet(alpha=0.1, copy_X=copy_X, random_state=rng).predict(T)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.sw->numpy.abs(np.random.RandomState(42).normal(scale=10, size=y.shape))
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.random_state->numpy.random.RandomState(seed)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.w->numpy.abs(w)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.rnd->numpy.random.RandomState(seed).uniform(size=(n_samples, n_features))
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.y->numpy.random.RandomState(0).rand(100)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.(X, y)->make_sparse_data(csc_container, n_samples=40, n_features=10)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.s_clf->Lasso(alpha=0.1, fit_intercept=False, max_iter=max_iter, tol=1e-07)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.d_clf->Lasso(alpha=0.1, fit_intercept=False, max_iter=max_iter, tol=1e-07)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.estimator->ElasticNet(alpha=0.01, precompute=False)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.Xs->csc_container(X)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.reg_dense->Model(**params).fit(X, y, sample_weight=sw)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.reg_sparse->Model(**params).fit(Xs, y, sample_weight=sw)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clfs->LassoCV(max_iter=100, cv=4)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.clfd->LassoCV(max_iter=100, cv=4)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.l->ElasticNet()
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.sample->numpy.array([1, 2, 3, 4, 5]).reshape(1, -1)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.predict_dense->ElasticNet().predict(sample)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.l_sp->ElasticNet()
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.X_sp->coo_container(X)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.sample_sparse->coo_container(sample)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.predict_sparse->ElasticNet().predict(sample_sparse)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_sparse_coordinate_descent.X.data->create_memmap_backed_data(X.data)
sklearn.linear_model.tests.test_sparse_coordinate_descent.make_sparse_data(sparse_container,n_samples=100,n_features=100,n_informative=10,seed=42,positive=False,n_targets=1)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_multitarget(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_toy_explicit_sparse_input(lil_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_enet_toy_list_input(with_sample_weight,csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_lasso_zero(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_path_parameters(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_same_multiple_output_sparse_dense(coo_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_same_output_sparse_dense_lasso_and_enet_cv(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_coef()
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_dense_equality(Model,fit_intercept,n_samples,n_features,with_sample_weight,csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_enet_coordinate_descent(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_enet_not_as_toy_dataset(csc_container,alpha,fit_intercept,positive)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_lasso_not_as_toy_dataset(csc_container)
sklearn.linear_model.tests.test_sparse_coordinate_descent.test_sparse_read_only_buffer(copy_X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_sag.py----------------------------------------
A:sklearn.linear_model.tests.test_sag.iris->load_iris()
A:sklearn.linear_model.tests.test_sag.w->numpy.random.RandomState(42).normal(size=n_features)
A:sklearn.linear_model.tests.test_sag.pred->numpy.dot(myX, w)
A:sklearn.linear_model.tests.test_sag.p->loss(pred, myy)
A:sklearn.linear_model.tests.test_sag.weights->numpy.array([[0.1, 0.2, 0.3], [1.1, 1.2, -1.3]])
A:sklearn.linear_model.tests.test_sag.sum_gradient->numpy.zeros(n_features)
A:sklearn.linear_model.tests.test_sag.gradient_memory->numpy.zeros(n_samples)
A:sklearn.linear_model.tests.test_sag.intercept_gradient_memory->numpy.zeros(n_samples)
A:sklearn.linear_model.tests.test_sag.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_sag.seen->set()
A:sklearn.linear_model.tests.test_sag.idx->int(rng.rand() * n_samples)
A:sklearn.linear_model.tests.test_sag.gradient->dloss(p, y[idx])
A:sklearn.linear_model.tests.test_sag.last_updated->numpy.zeros(n_features, dtype=int)
A:sklearn.linear_model.tests.test_sag.c_sum->numpy.zeros(n_iter * n_samples)
A:sklearn.linear_model.tests.test_sag.(X, y)->make_classification(random_state=rng)
A:sklearn.linear_model.tests.test_sag.step_size->get_step_size(X, alpha, fit_intercept, classification=True)
A:sklearn.linear_model.tests.test_sag.clf->LogisticRegression(solver=solver, random_state=rng, warm_start=True)
A:sklearn.linear_model.tests.test_sag.(weights, intercept)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=log_dloss, fit_intercept=fit_intercept, saga=solver == 'saga')
A:sklearn.linear_model.tests.test_sag.(weights2, intercept2)->sag(X, y, step_size, alpha, n_iter=n_iter, dloss=squared_dloss, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.intercept->numpy.array([1.0, 0, -0.2])
A:sklearn.linear_model.tests.test_sag.weights2->numpy.atleast_2d(weights2)
A:sklearn.linear_model.tests.test_sag.intercept2->numpy.array(intercept2)
A:sklearn.linear_model.tests.test_sag.X->numpy.array([[1.1, 2.2], [2.2, -4.4], [3.3, -2.2], [1.1, 1.1]])
A:sklearn.linear_model.tests.test_sag.true_w->numpy.random.RandomState(42).normal(size=n_features)
A:sklearn.linear_model.tests.test_sag.y->numpy.array([0, 1, 2, 0], dtype=np.float64)
A:sklearn.linear_model.tests.test_sag.(weights1, intercept1)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=squared_dloss, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.clf1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.clf2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha)
A:sklearn.linear_model.tests.test_sag.clf3->Ridge(fit_intercept=fit_intercept, tol=1e-05, solver='lsqr', alpha=alpha, max_iter=n_iter, random_state=42)
A:sklearn.linear_model.tests.test_sag.pobj1->get_pobj(clf1.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.pobj2->get_pobj(clf2.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.pobj3->get_pobj(clf3.coef_, alpha, X, y, squared_loss)
A:sklearn.linear_model.tests.test_sag.(spweights1, spintercept1)->sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight)
A:sklearn.linear_model.tests.test_sag.(spweights2, spintercept2)->sag_sparse(X, y_encoded, step_size, alpha, n_iter=max_iter, dloss=log_dloss, sample_weight=sample_weight, sparse=True)
A:sklearn.linear_model.tests.test_sag.max_squared_sum_->row_norms(X, squared=True).max()
A:sklearn.linear_model.tests.test_sag.mun_sqr->min(2 * n_samples * alpha, L_sqr)
A:sklearn.linear_model.tests.test_sag.mun_log->min(2 * n_samples * alpha, L_log)
A:sklearn.linear_model.tests.test_sag.step_size_sqr_->get_auto_step_size(max_squared_sum_, alpha, 'squared', fit_intercept, n_samples=n_samples, is_saga=saga)
A:sklearn.linear_model.tests.test_sag.step_size_log_->get_auto_step_size(max_squared_sum_, alpha, 'log', fit_intercept, n_samples=n_samples, is_saga=saga)
A:sklearn.linear_model.tests.test_sag.score1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept).score(X, y)
A:sklearn.linear_model.tests.test_sag.score2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha).score(X, y)
A:sklearn.linear_model.tests.test_sag.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_sag.y_tmp->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_sag.(spweights, spintercept)->sag_sparse(X, y, step_size, alpha, n_iter=n_iter, dloss=log_dloss, sample_weight=sample_weight, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_sag.y_encoded->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_sag.coef1->numpy.vstack(coef1)
A:sklearn.linear_model.tests.test_sag.intercept1->numpy.array(intercept1)
A:sklearn.linear_model.tests.test_sag.coef2->numpy.vstack(coef2)
A:sklearn.linear_model.tests.test_sag.pred1->LogisticRegression(solver='sag', C=1.0 / alpha, fit_intercept=fit_intercept).predict(X)
A:sklearn.linear_model.tests.test_sag.pred2->Ridge(fit_intercept=fit_intercept, solver='sag', alpha=alpha).predict(X)
A:sklearn.linear_model.tests.test_sag.le->LabelEncoder()
A:sklearn.linear_model.tests.test_sag.class_weight_->compute_class_weight(class_weight, classes=np.unique(y), y=y)
A:sklearn.linear_model.tests.test_sag.msg->re.escape('Current sag implementation does not handle the case step_size * alpha_scaled == 1')
A:sklearn.linear_model.tests.test_sag.n_classes->len(np.unique(y))
A:sklearn.linear_model.tests.test_sag.sample_weights->numpy.array([0.8, 1, 1, 0.8])
A:sklearn.linear_model.tests.test_sag.(dataset, _)->make_dataset(X, y, sample_weights, random_state=42)
A:sklearn.linear_model.tests.test_sag.(loss_1, grad_1)->_multinomial_grad_loss_all_samples(dataset, weights, intercept, n_samples, n_features, n_classes)
A:sklearn.linear_model.tests.test_sag.loss->LinearModelLoss(base_loss=HalfMultinomialLoss(n_classes=n_classes), fit_intercept=True)
A:sklearn.linear_model.tests.test_sag.(loss_2, grad_2)->LinearModelLoss(base_loss=HalfMultinomialLoss(n_classes=n_classes), fit_intercept=True).loss_gradient(weights_intercept, X, y, l2_reg_strength=0.0, sample_weight=sample_weights)
A:sklearn.linear_model.tests.test_sag.lbin->LabelBinarizer()
A:sklearn.linear_model.tests.test_sag.Y_bin->LabelBinarizer().fit_transform(y)
A:sklearn.linear_model.tests.test_sag.logsumexp_prediction->logsumexp(prediction, axis=1)
A:sklearn.linear_model.tests.test_sag.grad_1->numpy.dot(X.T, diff)
A:sklearn.linear_model.tests.test_sag.grad_gt->numpy.array([[-0.557487, -1.619151, +2.176638], [-0.903942, +5.258745, -4.354803]])
sklearn.linear_model.tests.test_sag.get_pobj(w,alpha,myX,myy,loss)
sklearn.linear_model.tests.test_sag.get_step_size(X,alpha,fit_intercept,classification=True)
sklearn.linear_model.tests.test_sag.log_dloss(p,y)
sklearn.linear_model.tests.test_sag.log_loss(p,y)
sklearn.linear_model.tests.test_sag.sag(X,y,step_size,alpha,n_iter=1,dloss=None,sparse=False,sample_weight=None,fit_intercept=True,saga=False)
sklearn.linear_model.tests.test_sag.sag_sparse(X,y,step_size,alpha,n_iter=1,dloss=None,sample_weight=None,sparse=False,fit_intercept=True,saga=False,random_state=0)
sklearn.linear_model.tests.test_sag.squared_dloss(p,y)
sklearn.linear_model.tests.test_sag.squared_loss(p,y)
sklearn.linear_model.tests.test_sag.test_binary_classifier_class_weight(csr_container)
sklearn.linear_model.tests.test_sag.test_classifier_matching()
sklearn.linear_model.tests.test_sag.test_classifier_results(csr_container)
sklearn.linear_model.tests.test_sag.test_classifier_single_class()
sklearn.linear_model.tests.test_sag.test_get_auto_step_size()
sklearn.linear_model.tests.test_sag.test_multiclass_classifier_class_weight(csr_container)
sklearn.linear_model.tests.test_sag.test_multinomial_loss()
sklearn.linear_model.tests.test_sag.test_multinomial_loss_ground_truth()
sklearn.linear_model.tests.test_sag.test_regressor_matching()
sklearn.linear_model.tests.test_sag.test_sag_classifier_computed_correctly(csr_container)
sklearn.linear_model.tests.test_sag.test_sag_classifier_raises_error(solver)
sklearn.linear_model.tests.test_sag.test_sag_multiclass_computed_correctly(csr_container)
sklearn.linear_model.tests.test_sag.test_sag_pobj_matches_logistic_regression(csr_container)
sklearn.linear_model.tests.test_sag.test_sag_pobj_matches_ridge_regression(csr_container)
sklearn.linear_model.tests.test_sag.test_sag_regressor(seed,csr_container)
sklearn.linear_model.tests.test_sag.test_sag_regressor_computed_correctly(csr_container)
sklearn.linear_model.tests.test_sag.test_step_size_alpha_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_common.py----------------------------------------
A:sklearn.linear_model.tests.test_common.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.linear_model.tests.test_common.X->make_low_rank_matrix(n_samples=n_train, n_features=n_features, random_state=rng)
A:sklearn.linear_model.tests.test_common.expectation->numpy.exp(X @ coef + 0.5)
A:sklearn.linear_model.tests.test_common.y->(y > expectation + 1).astype(np.float64)
A:sklearn.linear_model.tests.test_common.sw->numpy.random.RandomState(global_random_seed).uniform(low=1, high=10, size=y.shape[0])
sklearn.linear_model.tests.test_common.test_balance_property(model,with_sample_weight,global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_coordinate_descent.py----------------------------------------
A:sklearn.linear_model.tests.test_coordinate_descent.X->numpy.asfortranarray(rng.uniform(size=(100, 10)))
A:sklearn.linear_model.tests.test_coordinate_descent.y->numpy.random.RandomState(0).rand(100)
A:sklearn.linear_model.tests.test_coordinate_descent.(X2, y2)->_set_order(X, y, order=order)
A:sklearn.linear_model.tests.test_coordinate_descent.clf->ElasticNet(alpha=0.1, copy_X=True, random_state=rng)
A:sklearn.linear_model.tests.test_coordinate_descent.pred->ElasticNet(alpha=0.1, copy_X=True, random_state=rng).predict(T)
A:sklearn.linear_model.tests.test_coordinate_descent.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_coordinate_descent.(X, y, _, _)->build_dataset()
A:sklearn.linear_model.tests.test_coordinate_descent.n_samples->len(y)
A:sklearn.linear_model.tests.test_coordinate_descent.random_state->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_coordinate_descent.w->numpy.random.RandomState(0).randn(n_features)
A:sklearn.linear_model.tests.test_coordinate_descent.X_test->numpy.random.RandomState(0).random_sample(size=(10, 3))
A:sklearn.linear_model.tests.test_coordinate_descent.y_test->numpy.dot(X_test, w)
A:sklearn.linear_model.tests.test_coordinate_descent.(X, y, X_test, y_test)->build_dataset()
A:sklearn.linear_model.tests.test_coordinate_descent.lars->LassoLarsCV(max_iter=30, cv=3).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.mse_lars->scipy.interpolate.interp1d(lars.cv_alphas_, lars.mse_path_.T)
A:sklearn.linear_model.tests.test_coordinate_descent.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_coordinate_descent.pipe->make_pipeline(StandardScaler(), LassoCV(cv=ShuffleSplit(random_state=0)))
A:sklearn.linear_model.tests.test_coordinate_descent.clf_unconstrained->LassoCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_constrained->LassoCV(n_alphas=3, eps=0.1, max_iter=max_iter, positive=True, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.lassocv->LassoCV(alphas=alphas)
A:sklearn.linear_model.tests.test_coordinate_descent.model_dense->make_pipeline(StandardScaler(with_mean=False), LinearModel(**params))
A:sklearn.linear_model.tests.test_coordinate_descent.model_sparse->make_pipeline(StandardScaler(with_mean=False), LinearModel(**params))
A:sklearn.linear_model.tests.test_coordinate_descent.X_sparse->csr_container(X)
A:sklearn.linear_model.tests.test_coordinate_descent.y_pred_dense->make_pipeline(StandardScaler(with_mean=False), LinearModel(**params)).predict(X)
A:sklearn.linear_model.tests.test_coordinate_descent.y_pred_sparse->make_pipeline(StandardScaler(with_mean=False), LinearModel(**params)).predict(X_sparse)
A:sklearn.linear_model.tests.test_coordinate_descent.(alphas_lars, _, coef_path_lars)->lars_path(X, y, method='lasso')
A:sklearn.linear_model.tests.test_coordinate_descent.coef_path_cont_lars->scipy.interpolate.interp1d(alphas_lars[::-1], coef_path_lars[:, ::-1])
A:sklearn.linear_model.tests.test_coordinate_descent.(alphas_lasso2, coef_path_lasso2, _)->lasso_path(X, y, alphas=alphas)
A:sklearn.linear_model.tests.test_coordinate_descent.coef_path_cont_lasso->scipy.interpolate.interp1d(alphas_lasso2[::-1], coef_path_lasso2[:, ::-1])
A:sklearn.linear_model.tests.test_coordinate_descent.clf1->ElasticNet(alpha=0.01, precompute=gram)
A:sklearn.linear_model.tests.test_coordinate_descent.clf2->MultiTaskLasso(alpha=0.1, max_iter=10)
A:sklearn.linear_model.tests.test_coordinate_descent.lasso->LassoCV(n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.enet->ElasticNet(alpha=alpha_enet, l1_ratio=0, **common_params).fit(X, y, sample_weight=sw)
A:sklearn.linear_model.tests.test_coordinate_descent.enetcv_unconstrained->ElasticNetCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.enetcv_constrained->ElasticNetCV(n_alphas=3, eps=0.1, max_iter=max_iter, cv=2, positive=True, n_jobs=1)
A:sklearn.linear_model.tests.test_coordinate_descent.m_enet->MultiTaskElasticNetCV(n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.m_lasso->MultiTaskLassoCV(n_alphas=3)
A:sklearn.linear_model.tests.test_coordinate_descent.X_train->numpy.random.RandomState(0).random_sample(size=(10, 3))
A:sklearn.linear_model.tests.test_coordinate_descent.y1->numpy.empty(10)
A:sklearn.linear_model.tests.test_coordinate_descent.y2->numpy.concatenate([y, y[:n_samples // 2]])
A:sklearn.linear_model.tests.test_coordinate_descent.Y->numpy.array([-1, 0, 1])
A:sklearn.linear_model.tests.test_coordinate_descent.T->numpy.array([[2], [3], [4]])
A:sklearn.linear_model.tests.test_coordinate_descent.estimator->MultiTaskEstimatorCV(cv=splitter)
A:sklearn.linear_model.tests.test_coordinate_descent.garbage->numpy.random.RandomState(0).standard_normal(X.shape)
A:sklearn.linear_model.tests.test_coordinate_descent.precompute->numpy.dot(garbage.T, garbage)
A:sklearn.linear_model.tests.test_coordinate_descent.sample_weight->numpy.ones(X.shape[0])
A:sklearn.linear_model.tests.test_coordinate_descent.gram->numpy.dot(X_c.T, X_c)
A:sklearn.linear_model.tests.test_coordinate_descent.model->ElasticNet(alpha=0.001, tol=0.001).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.(X, y)->make_regression(random_state=42, n_targets=2)
A:sklearn.linear_model.tests.test_coordinate_descent.low_reg_model->ElasticNet(alpha=final_alpha).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.high_reg_model->ElasticNet(alpha=final_alpha * 10).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.warm_low_reg_model->deepcopy(high_reg_model)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_cyclic->MultiTaskElasticNet(selection='cyclic', tol=1e-08)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_random->MultiTaskElasticNet(selection='random', tol=1e-08, random_state=42)
A:sklearn.linear_model.tests.test_coordinate_descent.new_y->numpy.hstack((y[:, np.newaxis], y[:, np.newaxis]))
A:sklearn.linear_model.tests.test_coordinate_descent.(X, Y, _, _)->build_dataset(n_samples=50, n_features=50, n_targets=2)
A:sklearn.linear_model.tests.test_coordinate_descent.csr->csr_container(X)
A:sklearn.linear_model.tests.test_coordinate_descent.(_, coefs, _)->path(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.(_, sparse_coefs, _)->path(csr, y)
A:sklearn.linear_model.tests.test_coordinate_descent.original_X->numpy.asfortranarray(rng.uniform(size=(100, 10))).copy()
A:sklearn.linear_model.tests.test_coordinate_descent.Gram->numpy.asfortranarray(rng.uniform(size=(100, 10))).T.dot(X)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_float->model(fit_intercept=False)
A:sklearn.linear_model.tests.test_coordinate_descent.clf_precompute->ElasticNet(alpha=0.5, max_iter=100, precompute=Gram, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_coordinate_descent.multi_y->numpy.hstack((y[:, np.newaxis], y[:, np.newaxis]))
A:sklearn.linear_model.tests.test_coordinate_descent.clf_multioutput->MultiTaskElasticNet(alpha=0.5, max_iter=100, fit_intercept=fit_intercept)
A:sklearn.linear_model.tests.test_coordinate_descent.est->MultiTaskLassoCV(n_alphas=5, fit_intercept=True).fit(X, y)
A:sklearn.linear_model.tests.test_coordinate_descent.est_desired->MultiTaskElasticNetCV(l1_ratio=1e-05, **estkwds)
A:sklearn.linear_model.tests.test_coordinate_descent.est_no_intercept->Lasso(fit_intercept=False)
A:sklearn.linear_model.tests.test_coordinate_descent.params->dict(tol=1e-12)
A:sklearn.linear_model.tests.test_coordinate_descent.reg->ElasticNet()
A:sklearn.linear_model.tests.test_coordinate_descent.coef->ElasticNet().coef_.copy()
A:sklearn.linear_model.tests.test_coordinate_descent.sample_weight_0->numpy.ones(X.shape[0]).copy()
A:sklearn.linear_model.tests.test_coordinate_descent.coef_0->ElasticNet().coef_.copy()
A:sklearn.linear_model.tests.test_coordinate_descent.X2->numpy.concatenate([X, X[:n_samples // 2]], axis=0)
A:sklearn.linear_model.tests.test_coordinate_descent.sample_weight_1->numpy.ones(X.shape[0]).copy()
A:sklearn.linear_model.tests.test_coordinate_descent.sample_weight_2->numpy.concatenate([sample_weight, sample_weight[:n_samples // 2]], axis=0)
A:sklearn.linear_model.tests.test_coordinate_descent.reg1->ElasticNet(**params).fit(X, y, sample_weight=sample_weight_1)
A:sklearn.linear_model.tests.test_coordinate_descent.reg2->ElasticNet(**params).fit(X2, y2, sample_weight=sample_weight_2)
A:sklearn.linear_model.tests.test_coordinate_descent.beta->numpy.random.RandomState(0).rand(n_features)
A:sklearn.linear_model.tests.test_coordinate_descent.sw->numpy.random.RandomState(0).uniform(low=0.01, high=2, size=X.shape[0])
A:sklearn.linear_model.tests.test_coordinate_descent.alphas->numpy.logspace(np.log10(1e-05), np.log10(1), num=10)
A:sklearn.linear_model.tests.test_coordinate_descent.splits_sw->list(LeaveOneGroupOut().split(X, groups=groups_sw))
A:sklearn.linear_model.tests.test_coordinate_descent.reg_sw->ElasticNetCV(alphas=alphas, cv=splits_sw, fit_intercept=fit_intercept, **params)
A:sklearn.linear_model.tests.test_coordinate_descent.splits->list(LeaveOneGroupOut().split(X, groups=groups))
A:sklearn.linear_model.tests.test_coordinate_descent.gs->GridSearchCV(estimator=ElasticNet(), param_grid=param, cv=cv, scoring='neg_mean_squared_error').fit(X, y, sample_weight=sample_weight)
A:sklearn.linear_model.tests.test_coordinate_descent.common_params->dict(tol=1e-12)
A:sklearn.linear_model.tests.test_coordinate_descent.ridge->Ridge(alpha=alpha, **common_params).fit(X, y, sample_weight=sw)
A:sklearn.linear_model.tests.test_coordinate_descent.sw_with_null->numpy.random.RandomState(0).uniform(low=0.01, high=2, size=X.shape[0]).copy()
A:sklearn.linear_model.tests.test_coordinate_descent.reg_trimmed->clone(estimator).set_params(**params).fit(X_trimmed, y_trimmed, sample_weight=sw_trimmed)
A:sklearn.linear_model.tests.test_coordinate_descent.reg_null_weighted->clone(estimator).set_params(**params).fit(X, y, sample_weight=sw_with_null)
A:sklearn.linear_model.tests.test_coordinate_descent.X_dup->numpy.concatenate([X, X], axis=0)
A:sklearn.linear_model.tests.test_coordinate_descent.y_dup->numpy.concatenate([y, y], axis=0)
A:sklearn.linear_model.tests.test_coordinate_descent.sw_dup->numpy.concatenate([sw, sw], axis=0)
A:sklearn.linear_model.tests.test_coordinate_descent.reg_2sw->clone(estimator).set_params(**params).fit(X, y, sample_weight=2 * sw)
A:sklearn.linear_model.tests.test_coordinate_descent.reg_dup->clone(estimator).set_params(**params).fit(X_dup, y_dup, sample_weight=sw_dup)
A:sklearn.linear_model.tests.test_coordinate_descent.groups->numpy.array([0, 1] * (len(y) // 2))
A:sklearn.linear_model.tests.test_coordinate_descent.train_indices->list(range(0, split_index))
A:sklearn.linear_model.tests.test_coordinate_descent.test_indices->list(range(split_index, len(X)))
A:sklearn.linear_model.tests.test_coordinate_descent.splitter->CVSplitterSampleWeight().set_split_request(groups=True, sample_weight=True)
sklearn.linear_model.tests.test_coordinate_descent._scale_alpha_inplace(estimator,n_samples)
sklearn.linear_model.tests.test_coordinate_descent.build_dataset(n_samples=50,n_features=200,n_informative_features=10,n_targets=1)
sklearn.linear_model.tests.test_coordinate_descent.test_1d_multioutput_enet_and_multitask_enet_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_1d_multioutput_lasso_and_multitask_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_check_input_false()
sklearn.linear_model.tests.test_coordinate_descent.test_coef_shape_not_zero()
sklearn.linear_model.tests.test_coordinate_descent.test_convergence_warnings()
sklearn.linear_model.tests.test_coordinate_descent.test_cv_estimators_reject_params_with_no_routing_enabled(EstimatorCV)
sklearn.linear_model.tests.test_coordinate_descent.test_elasticnet_precompute_gram()
sklearn.linear_model.tests.test_coordinate_descent.test_elasticnet_precompute_gram_weighted_samples()
sklearn.linear_model.tests.test_coordinate_descent.test_elasticnet_precompute_incorrect_gram()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_coordinate_descent(klass,n_classes,kwargs)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_copy_X_False_check_input_False()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_copy_X_True(check_input)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_cv_grid_search(sample_weight)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_cv_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_cv_sample_weight_consistency(fit_intercept,l1_ratio,precompute,sparse_container)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_cv_sample_weight_correctness(fit_intercept,sparse_container)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_float_precision()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_l1_ratio()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_multitarget()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_nonfinite_params()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_path()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_path_positive()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_enet_ridge_consistency(ridge_alpha)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_sample_weight_consistency(fit_intercept,alpha,precompute,sparse_container,global_random_seed)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_sample_weight_does_not_overwrite_sample_weight(check_input)
sklearn.linear_model.tests.test_coordinate_descent.test_enet_toy()
sklearn.linear_model.tests.test_coordinate_descent.test_lassoCV_does_not_set_precompute(monkeypatch,precompute,inner_precompute)
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_alpha_warning()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_cv_with_some_model_selection()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_dual_gap()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_non_float_y(model)
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_path_return_models_vs_new_return_gives_same_coefficients()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_positive_constraint()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_readonly_data()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_toy()
sklearn.linear_model.tests.test_coordinate_descent.test_lasso_zero()
sklearn.linear_model.tests.test_coordinate_descent.test_lassocv_alphas_validation(alphas,err_type,err_msg)
sklearn.linear_model.tests.test_coordinate_descent.test_linear_models_cv_fit_with_loky(estimator)
sklearn.linear_model.tests.test_coordinate_descent.test_model_pipeline_same_dense_and_sparse(LinearModel,params,csr_container)
sklearn.linear_model.tests.test_coordinate_descent.test_multi_task_lasso_and_enet()
sklearn.linear_model.tests.test_coordinate_descent.test_multi_task_lasso_cv_dtype()
sklearn.linear_model.tests.test_coordinate_descent.test_multi_task_lasso_readonly_data()
sklearn.linear_model.tests.test_coordinate_descent.test_multioutput_enetcv_error()
sklearn.linear_model.tests.test_coordinate_descent.test_multitask_cv_estimators_with_sample_weight(MultiTaskEstimatorCV)
sklearn.linear_model.tests.test_coordinate_descent.test_multitask_enet_and_lasso_cv()
sklearn.linear_model.tests.test_coordinate_descent.test_overrided_gram_matrix()
sklearn.linear_model.tests.test_coordinate_descent.test_path_parameters()
sklearn.linear_model.tests.test_coordinate_descent.test_path_unknown_parameter(path_func)
sklearn.linear_model.tests.test_coordinate_descent.test_random_descent(csr_container)
sklearn.linear_model.tests.test_coordinate_descent.test_read_only_buffer()
sklearn.linear_model.tests.test_coordinate_descent.test_sample_weight_invariance(estimator)
sklearn.linear_model.tests.test_coordinate_descent.test_set_order_dense(order,input_order)
sklearn.linear_model.tests.test_coordinate_descent.test_set_order_sparse(order,input_order,coo_container)
sklearn.linear_model.tests.test_coordinate_descent.test_sparse_dense_descent_paths(csr_container)
sklearn.linear_model.tests.test_coordinate_descent.test_sparse_input_convergence_warning(csr_container)
sklearn.linear_model.tests.test_coordinate_descent.test_sparse_input_dtype_enet_and_lassocv(csr_container)
sklearn.linear_model.tests.test_coordinate_descent.test_uniform_targets()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start_convergence()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start_convergence_with_regularizer_decrement()
sklearn.linear_model.tests.test_coordinate_descent.test_warm_start_multitask_lasso()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_sgd.py----------------------------------------
A:sklearn.linear_model.tests.test_sgd.X->numpy.array([[1, 2], [3, 4]])
A:sklearn.linear_model.tests.test_sgd.T->numpy.array([[-1, -1], [2, 2], [3, 2]])
A:sklearn.linear_model.tests.test_sgd.X2->numpy.array([[-1, 1], [-0.75, 0.5], [-1.5, 1.5], [1, 1], [0.75, 0.5], [1.5, 1.5], [-1, -1], [0, -0.5], [1, -1]])
A:sklearn.linear_model.tests.test_sgd.T2->numpy.array([[-1.5, 0.5], [1, 2], [0, -2]])
A:sklearn.linear_model.tests.test_sgd.X3->numpy.array([[1, 1, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 1], [0, 0, 0, 0, 1, 1], [0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0]])
A:sklearn.linear_model.tests.test_sgd.Y3->numpy.array([1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.linear_model.tests.test_sgd.X4->numpy.array([[1, 0.9, 0.8, 0, 0, 0], [1, 0.84, 0.98, 0, 0, 0], [1, 0.96, 0.88, 0, 0, 0], [1, 0.91, 0.99, 0, 0, 0], [0, 0, 0, 0.89, 0.91, 1], [0, 0, 0, 0.79, 0.84, 1], [0, 0, 0, 0.91, 0.95, 1], [0, 0, 0, 0.93, 1, 1]])
A:sklearn.linear_model.tests.test_sgd.Y4->numpy.array([1, 1, 1, 1, 2, 2, 2, 2])
A:sklearn.linear_model.tests.test_sgd.iris->sklearn.datasets.load_iris()
A:sklearn.linear_model.tests.test_sgd.X5->numpy.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])
A:sklearn.linear_model.tests.test_sgd.weights->numpy.zeros(X.shape[1])
A:sklearn.linear_model.tests.test_sgd.average_weights->average_weights.reshape(1, -1).reshape(1, -1)
A:sklearn.linear_model.tests.test_sgd.p->numpy.dot(entry, coef)
A:sklearn.linear_model.tests.test_sgd.clf->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0)
A:sklearn.linear_model.tests.test_sgd.clf2->klass(average=0, learning_rate='constant', eta0=eta0, nu=nu, max_iter=1, shuffle=False)
A:sklearn.linear_model.tests.test_sgd.clf3->klass(nu=0.5, eta0=0.01, shuffle=False, warm_start=True, learning_rate=lr)
A:sklearn.linear_model.tests.test_sgd.clf1->klass(average=7, learning_rate='constant', eta0=eta0, nu=nu, max_iter=2, shuffle=False)
A:sklearn.linear_model.tests.test_sgd.Y_encode->numpy.array(Y)
A:sklearn.linear_model.tests.test_sgd.(average_weights, average_intercept)->asgd(klass, X3, Y3, eta, alpha)
A:sklearn.linear_model.tests.test_sgd.cv->ShuffleSplit(test_size=validation_fraction, random_state=seed)
A:sklearn.linear_model.tests.test_sgd.(idx_train, idx_val)->next(cv.split(X, Y))
A:sklearn.linear_model.tests.test_sgd.idx_train->numpy.sort(idx_train)
A:sklearn.linear_model.tests.test_sgd.sgd_estimator->klass()
A:sklearn.linear_model.tests.test_sgd.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_sgd.w->numpy.random.RandomState(0).normal(size=n_features)
A:sklearn.linear_model.tests.test_sgd.y->numpy.array([1, 0])
A:sklearn.linear_model.tests.test_sgd.pred->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).predict(X_test)
A:sklearn.linear_model.tests.test_sgd.np_Y2->numpy.array(Y2)
A:sklearn.linear_model.tests.test_sgd.classes->numpy.unique(Y_)
A:sklearn.linear_model.tests.test_sgd.y_i->numpy.ones(np_Y2.shape[0])
A:sklearn.linear_model.tests.test_sgd.(average_coef, average_intercept)->asgd(klass, X2, y_i, eta, alpha)
A:sklearn.linear_model.tests.test_sgd.message->'probability estimates are not available for loss={!r}'.format(loss)
A:sklearn.linear_model.tests.test_sgd.d->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).decision_function([x])
A:sklearn.linear_model.tests.test_sgd.lp->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).predict_log_proba([[-1, -1]])
A:sklearn.linear_model.tests.test_sgd.x->numpy.array([[1, 2], [3, 4]]).mean(axis=0)
A:sklearn.linear_model.tests.test_sgd.n->len(X4)
A:sklearn.linear_model.tests.test_sgd.idx->numpy.arange(X.shape[0])
A:sklearn.linear_model.tests.test_sgd.clf_weighted->klass(alpha=0.1, max_iter=1000, class_weight={0: 0.5, 1: 0.5})
A:sklearn.linear_model.tests.test_sgd.sample_weights->numpy.random.RandomState(0).random_sample(Y4.shape[0])
A:sklearn.linear_model.tests.test_sgd.multiplied_together->numpy.copy(sample_weights)
A:sklearn.linear_model.tests.test_sgd.f1->sklearn.metrics.f1_score(y, clf_balanced.predict(X), average='weighted')
A:sklearn.linear_model.tests.test_sgd.clf_balanced->klass(alpha=0.0001, max_iter=1000, class_weight='balanced', shuffle=False).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.X_imbalanced->numpy.vstack([X] + [X_0] * 10)
A:sklearn.linear_model.tests.test_sgd.y_imbalanced->numpy.concatenate([y] + [y_0] * 10)
A:sklearn.linear_model.tests.test_sgd.y_pred->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).predict(T)
A:sklearn.linear_model.tests.test_sgd.id1->id(clf.coef_.data)
A:sklearn.linear_model.tests.test_sgd.id2->id(clf.coef_.data)
A:sklearn.linear_model.tests.test_sgd.y_pred2->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).predict(T)
A:sklearn.linear_model.tests.test_sgd.random_state->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_sgd.score->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).score(X, y)
A:sklearn.linear_model.tests.test_sgd.ground_truth_coef->numpy.random.RandomState(0).randn(n_features)
A:sklearn.linear_model.tests.test_sgd.cd->sklearn.linear_model.ElasticNet(alpha=alpha, l1_ratio=l1_ratio, fit_intercept=False)
A:sklearn.linear_model.tests.test_sgd.sgd->klass(penalty='elasticnet', max_iter=50, alpha=alpha, l1_ratio=l1_ratio, fit_intercept=False)
A:sklearn.linear_model.tests.test_sgd.coef->numpy.zeros(X.shape[1])
A:sklearn.linear_model.tests.test_sgd.average_coef->numpy.zeros(X.shape[1])
A:sklearn.linear_model.tests.test_sgd.y_scores->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).decision_function(T)
A:sklearn.linear_model.tests.test_sgd.y_scores2->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).decision_function(T)
A:sklearn.linear_model.tests.test_sgd.(average_coef, average_offset)->asgd_oneclass(klass, X3, eta, nu)
A:sklearn.linear_model.tests.test_sgd.X_train->numpy.array([[-2, -1], [-1, -1], [1, 1]])
A:sklearn.linear_model.tests.test_sgd.X_test->numpy.array([[0.5, -2], [2, 2]])
A:sklearn.linear_model.tests.test_sgd.scores->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).score_samples(X_test)
A:sklearn.linear_model.tests.test_sgd.y_pred_ocsvm->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).predict(X_test)
A:sklearn.linear_model.tests.test_sgd.dec_ocsvm->sklearn.linear_model.SGDClassifier(early_stopping=True, validation_fraction=validation_fraction, random_state=0).decision_function(X_test).reshape(1, -1)
A:sklearn.linear_model.tests.test_sgd.transform->Nystroem(gamma=gamma, random_state=random_state)
A:sklearn.linear_model.tests.test_sgd.clf_sgd->SGDOneClassSVM(nu=nu, shuffle=True, fit_intercept=True, max_iter=max_iter, random_state=random_state, tol=None)
A:sklearn.linear_model.tests.test_sgd.pipe_sgd->make_pipeline(transform, clf_sgd)
A:sklearn.linear_model.tests.test_sgd.y_pred_sgdocsvm->make_pipeline(transform, clf_sgd).predict(X_test)
A:sklearn.linear_model.tests.test_sgd.dec_sgdocsvm->make_pipeline(transform, clf_sgd).decision_function(X_test).reshape(1, -1)
A:sklearn.linear_model.tests.test_sgd.(X, y)->sklearn.datasets.make_classification(random_state=global_random_seed)
A:sklearn.linear_model.tests.test_sgd.est_en->SGDClassifier(alpha=0.001, penalty='elasticnet', tol=None, max_iter=6, l1_ratio=1e-10, random_state=42).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.est_l1->SGDClassifier(alpha=0.001, penalty='l1', max_iter=6, random_state=42, tol=None).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.est_l2->SGDClassifier(alpha=0.001, penalty='l2', max_iter=6, random_state=42, tol=None).fit(X, y)
A:sklearn.linear_model.tests.test_sgd.X_scaled->MinMaxScaler().fit_transform(X)
A:sklearn.linear_model.tests.test_sgd.ground_truth->numpy.random.RandomState(0).normal(size=n_features)
A:sklearn.linear_model.tests.test_sgd.model->SGDClassifier(alpha=100000.0, learning_rate='constant', eta0=0.1, penalty=penalty, shuffle=False, tol=None, max_iter=6)
A:sklearn.linear_model.tests.test_sgd.model_0->SGDClassifier(tol=None, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_1->SGDClassifier(tol=0, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_2->SGDClassifier(tol=0.1, random_state=0, max_iter=max_iter)
A:sklearn.linear_model.tests.test_sgd.model_3->SGDClassifier(max_iter=3, tol=0.001, random_state=0)
A:sklearn.linear_model.tests.test_sgd.loss->sklearn.linear_model._sgd_fast.SquaredEpsilonInsensitive(0.1)
A:sklearn.linear_model.tests.test_sgd.search->RandomizedSearchCV(clf, param_grid, n_iter=5, n_jobs=2, random_state=0)
A:sklearn.linear_model.tests.test_sgd.clf_sequential->SGDClassifier(max_iter=1000, n_jobs=1, random_state=42)
A:sklearn.linear_model.tests.test_sgd.clf_parallel->SGDClassifier(max_iter=1000, n_jobs=4, random_state=42)
A:sklearn.linear_model.tests.test_sgd.est->Estimator().fit(X, y)
A:sklearn.linear_model.tests.test_sgd.mock->Mock(side_effect=_stochastic_gradient._ValidationScoreCallback)
A:sklearn.linear_model.tests.test_sgd.sample_weight->numpy.zeros_like(Y)
A:sklearn.linear_model.tests.test_sgd._X->numpy.array([[1, 2], [3, 4]]).astype(data_type)
A:sklearn.linear_model.tests.test_sgd._Y->numpy.array(Y, dtype=data_type)
A:sklearn.linear_model.tests.test_sgd.sgd_model->SGDEstimator()
A:sklearn.linear_model.tests.test_sgd.X_64->numpy.array([[1, 2], [3, 4]]).astype(dtype=np.float64)
A:sklearn.linear_model.tests.test_sgd.Y_64->numpy.array(Y, dtype=np.float64)
A:sklearn.linear_model.tests.test_sgd.X_32->numpy.array([[1, 2], [3, 4]]).astype(dtype=np.float32)
A:sklearn.linear_model.tests.test_sgd.Y_32->numpy.array(Y, dtype=np.float32)
A:sklearn.linear_model.tests.test_sgd.sgd_64->SGDEstimator(max_iter=20)
A:sklearn.linear_model.tests.test_sgd.sgd_32->SGDEstimator(max_iter=20)
sklearn.linear_model.tests.test_sgd.SGDClassifier(**kwargs)
sklearn.linear_model.tests.test_sgd.SGDOneClassSVM(**kwargs)
sklearn.linear_model.tests.test_sgd.SGDRegressor(**kwargs)
sklearn.linear_model.tests.test_sgd.SparseSGDClassifier(**kwargs)
sklearn.linear_model.tests.test_sgd.SparseSGDOneClassSVM(**kwargs)
sklearn.linear_model.tests.test_sgd.SparseSGDRegressor(**kwargs)
sklearn.linear_model.tests.test_sgd._SparseSGDClassifier(linear_model.SGDClassifier)
sklearn.linear_model.tests.test_sgd._SparseSGDClassifier.decision_function(self,X)
sklearn.linear_model.tests.test_sgd._SparseSGDClassifier.fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDClassifier.partial_fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDClassifier.predict_proba(self,X)
sklearn.linear_model.tests.test_sgd._SparseSGDOneClassSVM(linear_model.SGDOneClassSVM)
sklearn.linear_model.tests.test_sgd._SparseSGDOneClassSVM.decision_function(self,X,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDOneClassSVM.fit(self,X,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDOneClassSVM.partial_fit(self,X,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDRegressor(linear_model.SGDRegressor)
sklearn.linear_model.tests.test_sgd._SparseSGDRegressor.decision_function(self,X,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDRegressor.fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd._SparseSGDRegressor.partial_fit(self,X,y,*args,**kw)
sklearn.linear_model.tests.test_sgd._test_loss_common(loss_function,cases)
sklearn.linear_model.tests.test_sgd._test_warm_start(klass,X,Y,lr)
sklearn.linear_model.tests.test_sgd._test_warm_start_oneclass(klass,X,lr)
sklearn.linear_model.tests.test_sgd._update_kwargs(kwargs)
sklearn.linear_model.tests.test_sgd.asgd(klass,X,y,eta,alpha,weight_init=None,intercept_init=0.0)
sklearn.linear_model.tests.test_sgd.asgd_oneclass(klass,X,eta,nu,coef_init=None,offset_init=0.0)
sklearn.linear_model.tests.test_sgd.test_SGDClassifier_fit_for_all_backends(backend)
sklearn.linear_model.tests.test_sgd.test_adaptive_longer_than_constant(klass)
sklearn.linear_model.tests.test_sgd.test_average_binary_computed_correctly(klass)
sklearn.linear_model.tests.test_sgd.test_average_sparse(klass)
sklearn.linear_model.tests.test_sgd.test_average_sparse_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_balanced_weight(klass)
sklearn.linear_model.tests.test_sgd.test_class_weights(klass)
sklearn.linear_model.tests.test_sgd.test_clone(klass)
sklearn.linear_model.tests.test_sgd.test_clone_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_early_stopping(klass)
sklearn.linear_model.tests.test_sgd.test_elasticnet_convergence(klass)
sklearn.linear_model.tests.test_sgd.test_equal_class_weight(klass)
sklearn.linear_model.tests.test_sgd.test_fit_then_partial_fit(klass)
sklearn.linear_model.tests.test_sgd.test_gradient_squared_hinge()
sklearn.linear_model.tests.test_sgd.test_input_format(klass)
sklearn.linear_model.tests.test_sgd.test_l1_ratio()
sklearn.linear_model.tests.test_sgd.test_large_regularization(penalty)
sklearn.linear_model.tests.test_sgd.test_late_onset_averaging_not_reached(klass)
sklearn.linear_model.tests.test_sgd.test_late_onset_averaging_reached(klass)
sklearn.linear_model.tests.test_sgd.test_late_onset_averaging_reached_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_loss_attribute_deprecation(Estimator)
sklearn.linear_model.tests.test_sgd.test_loss_epsilon_insensitive()
sklearn.linear_model.tests.test_sgd.test_loss_function_epsilon(klass)
sklearn.linear_model.tests.test_sgd.test_loss_hinge()
sklearn.linear_model.tests.test_sgd.test_loss_huber()
sklearn.linear_model.tests.test_sgd.test_loss_log()
sklearn.linear_model.tests.test_sgd.test_loss_modified_huber()
sklearn.linear_model.tests.test_sgd.test_loss_squared_epsilon_insensitive()
sklearn.linear_model.tests.test_sgd.test_loss_squared_loss()
sklearn.linear_model.tests.test_sgd.test_multi_core_gridsearch_and_early_stopping()
sklearn.linear_model.tests.test_sgd.test_multi_thread_multi_class_and_early_stopping()
sklearn.linear_model.tests.test_sgd.test_multiple_fit(klass)
sklearn.linear_model.tests.test_sgd.test_n_iter_no_change(klass)
sklearn.linear_model.tests.test_sgd.test_not_enough_sample_for_early_stopping(klass)
sklearn.linear_model.tests.test_sgd.test_numerical_stability_large_gradient()
sklearn.linear_model.tests.test_sgd.test_ocsvm_vs_sgdocsvm()
sklearn.linear_model.tests.test_sgd.test_partial_fit(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_binary(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_equal_fit(klass,lr)
sklearn.linear_model.tests.test_sgd.test_partial_fit_equal_fit_classif(klass,lr)
sklearn.linear_model.tests.test_sgd.test_partial_fit_equal_fit_oneclass(klass,lr)
sklearn.linear_model.tests.test_sgd.test_partial_fit_exception(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_multiclass(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_multiclass_average(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_partial_fit_weight_class_balanced(klass)
sklearn.linear_model.tests.test_sgd.test_plain_has_no_average_attr(klass)
sklearn.linear_model.tests.test_sgd.test_provide_coef(klass)
sklearn.linear_model.tests.test_sgd.test_regression_losses(klass)
sklearn.linear_model.tests.test_sgd.test_sample_weights(klass)
sklearn.linear_model.tests.test_sgd.test_set_coef_multiclass(klass)
sklearn.linear_model.tests.test_sgd.test_set_intercept_offset(klass,fit_params)
sklearn.linear_model.tests.test_sgd.test_set_intercept_offset_binary(klass,fit_params)
sklearn.linear_model.tests.test_sgd.test_set_intercept_to_intercept(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_at_least_two_labels(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_averaged_computed_correctly(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_averaged_computed_correctly_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_averaged_partial_fit(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_averaged_partial_fit_oneclass(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_clf(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_dtype_match(SGDEstimator,data_type)
sklearn.linear_model.tests.test_sgd.test_sgd_early_stopping_with_partial_fit(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_epsilon_insensitive(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_error_on_zero_validation_weight()
sklearn.linear_model.tests.test_sgd.test_sgd_huber_fit(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_l1(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_least_squares_fit(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_multiclass(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_multiclass_average(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_multiclass_njobs(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_multiclass_with_init_coef(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_numerical_consistency(SGDEstimator)
sklearn.linear_model.tests.test_sgd.test_sgd_oneclass()
sklearn.linear_model.tests.test_sgd.test_sgd_predict_proba_method_access(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_proba(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_random_state(Estimator,global_random_seed)
sklearn.linear_model.tests.test_sgd.test_sgd_reg(klass)
sklearn.linear_model.tests.test_sgd.test_sgd_verbose(Estimator)
sklearn.linear_model.tests.test_sgd.test_tol_parameter()
sklearn.linear_model.tests.test_sgd.test_underflow_or_overlow()
sklearn.linear_model.tests.test_sgd.test_validation_mask_correctly_subsets(monkeypatch)
sklearn.linear_model.tests.test_sgd.test_validation_set_not_used_for_training(klass)
sklearn.linear_model.tests.test_sgd.test_warm_start(klass,lr)
sklearn.linear_model.tests.test_sgd.test_warm_start_multiclass(klass)
sklearn.linear_model.tests.test_sgd.test_warm_start_oneclass(klass,lr)
sklearn.linear_model.tests.test_sgd.test_weights_multiplied(klass)
sklearn.linear_model.tests.test_sgd.test_wrong_class_weight_label(klass)
sklearn.linear_model.tests.test_sgd.test_wrong_sample_weights(klass)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_passive_aggressive.py----------------------------------------
A:sklearn.linear_model.tests.test_passive_aggressive.iris->load_iris()
A:sklearn.linear_model.tests.test_passive_aggressive.random_state->check_random_state(12)
A:sklearn.linear_model.tests.test_passive_aggressive.indices->numpy.arange(iris.data.shape[0])
A:sklearn.linear_model.tests.test_passive_aggressive.self.w->numpy.zeros(n_features, dtype=np.float64)
A:sklearn.linear_model.tests.test_passive_aggressive.p->self.project(X[i])
A:sklearn.linear_model.tests.test_passive_aggressive.loss->max(np.abs(p - y[i]) - self.epsilon, 0)
A:sklearn.linear_model.tests.test_passive_aggressive.sqnorm->numpy.dot(X[i], X[i])
A:sklearn.linear_model.tests.test_passive_aggressive.step->min(self.C, loss / sqnorm)
A:sklearn.linear_model.tests.test_passive_aggressive.clf->PassiveAggressiveClassifier(class_weight={0: 0.5}, max_iter=100)
A:sklearn.linear_model.tests.test_passive_aggressive.score->PassiveAggressiveClassifier(class_weight={0: 0.5}, max_iter=100).score(data, y)
A:sklearn.linear_model.tests.test_passive_aggressive.classes->numpy.unique(y)
A:sklearn.linear_model.tests.test_passive_aggressive.y_bin->y.copy()
A:sklearn.linear_model.tests.test_passive_aggressive.clf1->MyPassiveAggressive(loss=loss, n_iter=2)
A:sklearn.linear_model.tests.test_passive_aggressive.clf2->PassiveAggressiveClassifier(loss=loss, max_iter=2, shuffle=False, tol=None)
A:sklearn.linear_model.tests.test_passive_aggressive.X2->numpy.array([[-1.0, -1.0], [-1.0, 0], [-0.8, -1.0], [1.0, 1.0], [1.0, 0.0]])
A:sklearn.linear_model.tests.test_passive_aggressive.clf_balanced->PassiveAggressiveClassifier(C=0.1, tol=None, class_weight='balanced')
A:sklearn.linear_model.tests.test_passive_aggressive.clf_weighted->PassiveAggressiveClassifier(C=0.1, tol=None, class_weight={0: 0.5, 1: 0.5})
A:sklearn.linear_model.tests.test_passive_aggressive.reg->PassiveAggressiveRegressor(max_iter=100)
A:sklearn.linear_model.tests.test_passive_aggressive.pred->PassiveAggressiveRegressor(max_iter=100).predict(data)
A:sklearn.linear_model.tests.test_passive_aggressive.reg1->MyPassiveAggressive(loss=loss, n_iter=2)
A:sklearn.linear_model.tests.test_passive_aggressive.reg2->PassiveAggressiveRegressor(tol=None, loss=loss, max_iter=2, shuffle=False)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive(self,C=1.0,epsilon=0.01,loss='hinge',fit_intercept=True,n_iter=1,random_state=None)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.__init__(self,C=1.0,epsilon=0.01,loss='hinge',fit_intercept=True,n_iter=1,random_state=None)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.fit(self,X,y)
sklearn.linear_model.tests.test_passive_aggressive.MyPassiveAggressive.project(self,X)
sklearn.linear_model.tests.test_passive_aggressive.test_class_weights()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_accuracy(csr_container,fit_intercept,average)
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_correctness(loss,csr_container)
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_partial_fit(csr_container,average)
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_refit()
sklearn.linear_model.tests.test_passive_aggressive.test_classifier_undefined_methods(response_method)
sklearn.linear_model.tests.test_passive_aggressive.test_equal_class_weight()
sklearn.linear_model.tests.test_passive_aggressive.test_partial_fit_weight_class_balanced()
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_correctness(loss,csr_container)
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_mse(csr_container,fit_intercept,average)
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_partial_fit(csr_container,average)
sklearn.linear_model.tests.test_passive_aggressive.test_regressor_undefined_methods()
sklearn.linear_model.tests.test_passive_aggressive.test_wrong_class_weight_label()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_quantile.py----------------------------------------
A:sklearn.linear_model.tests.test_quantile.(X, y)->make_regression(n_samples=100, n_features=20, random_state=1, noise=1.0)
A:sklearn.linear_model.tests.test_quantile.X_sparse->sparse_container(X)
A:sklearn.linear_model.tests.test_quantile.model->QuantileRegressor(quantile=quantile, alpha=0, solver=default_solver).fit(X, y)
A:sklearn.linear_model.tests.test_quantile.huber->HuberRegressor(epsilon=1 + 0.0001, alpha=alpha, fit_intercept=fit_intercept).fit(X, y)
A:sklearn.linear_model.tests.test_quantile.quant->QuantileRegressor(quantile=0.5, alpha=1e-08, solver=default_solver)
A:sklearn.linear_model.tests.test_quantile.weight->numpy.ones(n)
A:sklearn.linear_model.tests.test_quantile.fraction_below->numpy.mean(y < quant.predict(X))
A:sklearn.linear_model.tests.test_quantile.weighted_fraction_below->numpy.average(y < quant.predict(X), weights=weight)
A:sklearn.linear_model.tests.test_quantile.rng->numpy.random.RandomState(42)
A:sklearn.linear_model.tests.test_quantile.X->numpy.linspace(0, 10, num=10).reshape(-1, 1)
A:sklearn.linear_model.tests.test_quantile.coef->numpy.array([0.5, -2])
A:sklearn.linear_model.tests.test_quantile.y->numpy.linspace(0, 10, num=10)
A:sklearn.linear_model.tests.test_quantile.loss->mean_pinball_loss(y, X @ coef[1:] + coef[0], alpha=quantile)
A:sklearn.linear_model.tests.test_quantile.L1->numpy.sum(np.abs(coef[1:]))
A:sklearn.linear_model.tests.test_quantile.res->minimize(fun=func, x0=[1, 0, -1], method='Nelder-Mead', tol=1e-12, options={'maxiter': 2000})
A:sklearn.linear_model.tests.test_quantile.params->dict(alpha=0, solver=default_solver)
A:sklearn.linear_model.tests.test_quantile.model1->QuantileRegressor(quantile=quantile, **params).fit(X, y)
A:sklearn.linear_model.tests.test_quantile.model2->QuantileRegressor(quantile=quantile, **params)
A:sklearn.linear_model.tests.test_quantile.A->numpy.random.RandomState(42).randn(n_features, n_features)
A:sklearn.linear_model.tests.test_quantile.reg->QuantileRegressor(alpha=0, solver='interior-point', solver_options={'maxiter': 1})
A:sklearn.linear_model.tests.test_quantile.quant_dense->QuantileRegressor(alpha=alpha, fit_intercept=fit_intercept, solver=default_solver).fit(X, y)
A:sklearn.linear_model.tests.test_quantile.quant_sparse->QuantileRegressor(alpha=alpha, fit_intercept=fit_intercept, solver=solver).fit(X_sparse, y)
sklearn.linear_model.tests.test_quantile.X_y_data()
sklearn.linear_model.tests.test_quantile.default_solver()
sklearn.linear_model.tests.test_quantile.test_asymmetric_error(quantile,default_solver)
sklearn.linear_model.tests.test_quantile.test_equivariance(quantile,default_solver)
sklearn.linear_model.tests.test_quantile.test_error_interior_point_future(X_y_data,monkeypatch)
sklearn.linear_model.tests.test_quantile.test_incompatible_solver_for_sparse_input(X_y_data,solver,csc_container)
sklearn.linear_model.tests.test_quantile.test_linprog_failure()
sklearn.linear_model.tests.test_quantile.test_quantile_equals_huber_for_low_epsilon(fit_intercept,default_solver)
sklearn.linear_model.tests.test_quantile.test_quantile_estimates_calibration(q,default_solver)
sklearn.linear_model.tests.test_quantile.test_quantile_sample_weight(default_solver)
sklearn.linear_model.tests.test_quantile.test_quantile_toy_example(quantile,alpha,intercept,coef,default_solver)
sklearn.linear_model.tests.test_quantile.test_sparse_input(sparse_container,solver,fit_intercept,default_solver)
sklearn.linear_model.tests.test_quantile.test_too_new_solver_methods_raise_error(X_y_data,solver)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_perceptron.py----------------------------------------
A:sklearn.linear_model.tests.test_perceptron.iris->load_iris()
A:sklearn.linear_model.tests.test_perceptron.random_state->check_random_state(12)
A:sklearn.linear_model.tests.test_perceptron.indices->numpy.arange(iris.data.shape[0])
A:sklearn.linear_model.tests.test_perceptron.self.w->numpy.zeros(n_features, dtype=np.float64)
A:sklearn.linear_model.tests.test_perceptron.X->numpy.atleast_2d(X)
A:sklearn.linear_model.tests.test_perceptron.data->container(X)
A:sklearn.linear_model.tests.test_perceptron.clf->Perceptron(max_iter=100)
A:sklearn.linear_model.tests.test_perceptron.score->Perceptron(max_iter=100).score(data, y)
A:sklearn.linear_model.tests.test_perceptron.y_bin->y.copy()
A:sklearn.linear_model.tests.test_perceptron.clf1->Perceptron(l1_ratio=0, penalty='elasticnet')
A:sklearn.linear_model.tests.test_perceptron.clf2->Perceptron(l1_ratio=0.15, penalty='elasticnet')
A:sklearn.linear_model.tests.test_perceptron.clf_l1->Perceptron(penalty='l1').fit(X, y)
A:sklearn.linear_model.tests.test_perceptron.clf_elasticnet->Perceptron(l1_ratio=0, penalty='elasticnet').fit(X, y)
A:sklearn.linear_model.tests.test_perceptron.clf_l2->Perceptron(penalty='l2').fit(X, y)
sklearn.linear_model.tests.test_perceptron.MyPerceptron(self,n_iter=1)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.__init__(self,n_iter=1)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.fit(self,X,y)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.predict(self,X)
sklearn.linear_model.tests.test_perceptron.MyPerceptron.project(self,X)
sklearn.linear_model.tests.test_perceptron.test_perceptron_accuracy(container)
sklearn.linear_model.tests.test_perceptron.test_perceptron_correctness()
sklearn.linear_model.tests.test_perceptron.test_perceptron_l1_ratio()
sklearn.linear_model.tests.test_perceptron.test_undefined_methods()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_least_angle.py----------------------------------------
A:sklearn.linear_model.tests.test_least_angle.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.linear_model.tests.test_least_angle.G->numpy.dot(X.T, X)
A:sklearn.linear_model.tests.test_least_angle.Xy->numpy.dot(X.T, y)
A:sklearn.linear_model.tests.test_least_angle.sys.stdout->StringIO()
A:sklearn.linear_model.tests.test_least_angle.(_, _, coef_path_)->sklearn.linear_model.lars_path(X, y, Gram='auto', copy_X=False, copy_Gram=False, alpha_min=0.0, method='lasso', verbose=0, max_iter=500)
A:sklearn.linear_model.tests.test_least_angle.cov->numpy.dot(X.T, res)
A:sklearn.linear_model.tests.test_least_angle.C->numpy.max(abs(cov))
A:sklearn.linear_model.tests.test_least_angle.ocur->len(cov[C - eps < abs(cov)])
A:sklearn.linear_model.tests.test_least_angle.output->sklearn.linear_model.lars_path(X, y, method=method)
A:sklearn.linear_model.tests.test_least_angle.output_pre->sklearn.linear_model.lars_path(X, y, Gram=G, Xy=Xy, method=method)
A:sklearn.linear_model.tests.test_least_angle.clf->sklearn.linear_model.Lars(fit_intercept=False).fit(H, np.arange(n))
A:sklearn.linear_model.tests.test_least_angle.X->numpy.random.RandomState(0).rand(20, 6).astype(dtype)
A:sklearn.linear_model.tests.test_least_angle.y->numpy.random.RandomState(0).rand(20).astype(dtype)
A:sklearn.linear_model.tests.test_least_angle.rng->numpy.random.RandomState(0)
A:sklearn.linear_model.tests.test_least_angle.(alphas_, _, coef_path_)->sklearn.linear_model.lars_path(X, y, method='lasso', Xy=Xy, Gram=G, alpha_min=0.9)
A:sklearn.linear_model.tests.test_least_angle.(alpha_, _, coef)->sklearn.linear_model.lars_path(X, y, method='lasso', Gram=G, Xy=Xy, alpha_min=0.9, return_path=False)
A:sklearn.linear_model.tests.test_least_angle.X1->numpy.array([[1, 1.0], [1.0, 1.0]])
A:sklearn.linear_model.tests.test_least_angle.y1->numpy.array([1, 1])
A:sklearn.linear_model.tests.test_least_angle.(_, _, coef_path)->sklearn.linear_model.lars_path(X1, y1)
A:sklearn.linear_model.tests.test_least_angle.lars->sklearn.linear_model.Lars(n_nonzero_coefs=6, verbose=verbose)
A:sklearn.linear_model.tests.test_least_angle.coord_descent->sklearn.linear_model.Lasso(alpha=alpha, tol=0.0001)
A:sklearn.linear_model.tests.test_least_angle.(alphas, _, lasso_path)->sklearn.linear_model.lars_path(X, y, method='lasso', positive=True)
A:sklearn.linear_model.tests.test_least_angle.lasso_cd->sklearn.linear_model.Lasso(fit_intercept=False, tol=1e-08, positive=True)
A:sklearn.linear_model.tests.test_least_angle.error->scipy.linalg.norm(c - lasso_cd.coef_)
A:sklearn.linear_model.tests.test_least_angle.clf1->sklearn.linear_model.LassoLars(fit_intercept=False, alpha=alpha, positive=True).fit(X, y)
A:sklearn.linear_model.tests.test_least_angle.clf2->sklearn.linear_model.Lasso(fit_intercept=False, alpha=alpha, tol=1e-08, positive=True).fit(X, y)
A:sklearn.linear_model.tests.test_least_angle.err->scipy.linalg.norm(clf1.coef_ - clf2.coef_)
A:sklearn.linear_model.tests.test_least_angle.lasso->sklearn.linear_model.LassoLars()
A:sklearn.linear_model.tests.test_least_angle.lasso2->sklearn.linear_model.LassoLars(alpha=lasso.alphas_[2])
A:sklearn.linear_model.tests.test_least_angle.w->numpy.zeros((m, 1))
A:sklearn.linear_model.tests.test_least_angle.i->numpy.arange(0, m)
A:sklearn.linear_model.tests.test_least_angle.(lars_alphas, _, lars_coef)->sklearn.linear_model.lars_path(X, y, method='lasso')
A:sklearn.linear_model.tests.test_least_angle.(_, lasso_coef2, _)->sklearn.linear_model.lasso_path(X, y, alphas=lars_alphas, tol=1e-06)
A:sklearn.linear_model.tests.test_least_angle.lars_obj->objective_function(lars_coef_)
A:sklearn.linear_model.tests.test_least_angle.cd_obj->objective_function(cd_coef_)
A:sklearn.linear_model.tests.test_least_angle.Y_pred->getattr(linear_model, estname)(positive=True, **params).predict(X)
A:sklearn.linear_model.tests.test_least_angle.y_pred->getattr(linear_model, estname)(positive=True, **params).predict(X)
A:sklearn.linear_model.tests.test_least_angle.lars_cv->sklearn.linear_model.LassoLarsCV(max_iter=5, cv=5)
A:sklearn.linear_model.tests.test_least_angle.x->numpy.array([[0.47299829, 0, 0, 0, 0], [0.08239882, 0.85784863, 0, 0, 0], [0.30114139, -0.07501577, 0.80895216, 0, 0], [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0], [-0.69363927, 0.06754067, 0.18064514, -0.0803561, 0.40427291]])
A:sklearn.linear_model.tests.test_least_angle.lars_bic->sklearn.linear_model.LassoLarsIC('bic')
A:sklearn.linear_model.tests.test_least_angle.lars_aic->sklearn.linear_model.LassoLarsIC('aic')
A:sklearn.linear_model.tests.test_least_angle.splitted_data->train_test_split(X, y, random_state=42)
A:sklearn.linear_model.tests.test_least_angle.(_, _, coefs)->sklearn.linear_model.lars_path(X, y, return_path=True, method=method, positive=True)
A:sklearn.linear_model.tests.test_least_angle.params->default_parameter.copy()
A:sklearn.linear_model.tests.test_least_angle.estimator->getattr(linear_model, estname)(positive=True, **params)
A:sklearn.linear_model.tests.test_least_angle.r->numpy.array([[0, 0, 0, 0, 0, -79.81036280949903, -83.52878873278283, -83.77765373919071, -83.78415693288893, -84.03339059175666], [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0, 0.025219751009936], [0, -3.577397088285891, -4.702795355871871, -7.016748621359461, -7.614898471899412, -0.336938391359179, 0, 0, 0.001213370600853, 0.048162321585148], [0, 0, 0, 2.231558436628169, 2.723267514525966, 2.811549786389614, 2.813766976061531, 2.817462468949557, 2.817368178703816, 2.816221090636795], [0, 0, -1.218422599914637, -3.457726183014808, -4.02130452206071, -45.827461592423745, -47.776608869312305, -47.9115616107464, -47.914845922736234, -48.03956233426572]])
A:sklearn.linear_model.tests.test_least_angle.model_lasso_lars->sklearn.linear_model.LassoLars(alpha=0, fit_intercept=False)
A:sklearn.linear_model.tests.test_least_angle.lasso_lars->LassoLarsIC(precompute=False)
A:sklearn.linear_model.tests.test_least_angle.X_copy->numpy.random.RandomState(0).rand(20, 6).astype(dtype).copy()
A:sklearn.linear_model.tests.test_least_angle.est_jitter->clone(est).set_params(jitter=1e-07, random_state=0)
A:sklearn.linear_model.tests.test_least_angle.X_before->numpy.random.RandomState(0).rand(20, 6).astype(dtype).copy()
A:sklearn.linear_model.tests.test_least_angle.model->make_pipeline(StandardScaler(), LassoLarsIC(fit_intercept=fit_intercept))
A:sklearn.linear_model.tests.test_least_angle.X_64->numpy.random.RandomState(0).rand(10, 6)
A:sklearn.linear_model.tests.test_least_angle.y_64->numpy.random.RandomState(0).rand(10)
A:sklearn.linear_model.tests.test_least_angle.model_64->LARS(**args).fit(X_64, y_64)
A:sklearn.linear_model.tests.test_least_angle.model_32->LARS(**args).fit(X_64.astype(np.float32), y_64.astype(np.float32))
A:sklearn.linear_model.tests.test_least_angle.best_alpha_selected->numpy.argmin(model[-1].criterion_)
A:sklearn.linear_model.tests.test_least_angle.(X, y)->sklearn.datasets.make_regression(n_samples=10, n_features=11 - fit_intercept, random_state=rng)
sklearn.linear_model.tests.test_least_angle._assert_same_lars_path_result(output1,output2)
sklearn.linear_model.tests.test_least_angle.test_X_none_gram_not_none()
sklearn.linear_model.tests.test_least_angle.test_all_precomputed()
sklearn.linear_model.tests.test_least_angle.test_collinearity()
sklearn.linear_model.tests.test_least_angle.test_copy_X_with_auto_gram()
sklearn.linear_model.tests.test_least_angle.test_estimatorclasses_positive_constraint()
sklearn.linear_model.tests.test_least_angle.test_lars_add_features()
sklearn.linear_model.tests.test_least_angle.test_lars_cv()
sklearn.linear_model.tests.test_least_angle.test_lars_cv_max_iter(recwarn)
sklearn.linear_model.tests.test_least_angle.test_lars_dtype_match(LARS,has_coef_path,args,dtype)
sklearn.linear_model.tests.test_least_angle.test_lars_lstsq()
sklearn.linear_model.tests.test_least_angle.test_lars_n_nonzero_coefs(verbose=False)
sklearn.linear_model.tests.test_least_angle.test_lars_numeric_consistency(LARS,has_coef_path,args)
sklearn.linear_model.tests.test_least_angle.test_lars_path_gram_equivalent(method,return_path)
sklearn.linear_model.tests.test_least_angle.test_lars_path_positive_constraint()
sklearn.linear_model.tests.test_least_angle.test_lars_path_readonly_data()
sklearn.linear_model.tests.test_least_angle.test_lars_precompute(classifier)
sklearn.linear_model.tests.test_least_angle.test_lars_with_jitter(est)
sklearn.linear_model.tests.test_least_angle.test_lasso_gives_lstsq_solution()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_copyX_behaviour(copy_X)
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_fit_copyX_behaviour(copy_X)
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_ic()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_path_length()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_R_implementation()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_early_stopping()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_ill_conditioned()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_ill_conditioned2()
sklearn.linear_model.tests.test_least_angle.test_lasso_lars_vs_lasso_cd_positive()
sklearn.linear_model.tests.test_least_angle.test_lassolarsic_alpha_selection(criterion)
sklearn.linear_model.tests.test_least_angle.test_lassolarsic_noise_variance(fit_intercept)
sklearn.linear_model.tests.test_least_angle.test_multitarget()
sklearn.linear_model.tests.test_least_angle.test_no_path()
sklearn.linear_model.tests.test_least_angle.test_no_path_all_precomputed()
sklearn.linear_model.tests.test_least_angle.test_no_path_precomputed()
sklearn.linear_model.tests.test_least_angle.test_rank_deficient_design()
sklearn.linear_model.tests.test_least_angle.test_simple()
sklearn.linear_model.tests.test_least_angle.test_simple_precomputed()
sklearn.linear_model.tests.test_least_angle.test_singular_matrix()
sklearn.linear_model.tests.test_least_angle.test_x_none_gram_none_raises_value_error()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/linear_model/tests/test_ransac.py----------------------------------------
A:sklearn.linear_model.tests.test_ransac.X->check_random_state(42).rand(10, 2)
A:sklearn.linear_model.tests.test_ransac.data->numpy.column_stack([X, y])
A:sklearn.linear_model.tests.test_ransac.rng->check_random_state(42)
A:sklearn.linear_model.tests.test_ransac.outliers->numpy.unique(rng.randint(len(X), size=200))
A:sklearn.linear_model.tests.test_ransac.estimator->LinearRegression()
A:sklearn.linear_model.tests.test_ransac.ransac_estimator->RANSACRegressor(estimator, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ref_inlier_mask->numpy.ones_like(ransac_estimator.inlier_mask_).astype(np.bool_)
A:sklearn.linear_model.tests.test_ransac.y->numpy.zeros((100,))
A:sklearn.linear_model.tests.test_ransac.max_trials->_dynamic_max_trials(len(X) - len(outliers), X.shape[0], 2, 1 - 1e-09)
A:sklearn.linear_model.tests.test_ransac.X_sparse->sparse_container(X)
A:sklearn.linear_model.tests.test_ransac.ransac_none_estimator->RANSACRegressor(None, min_samples=2, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator1->RANSACRegressor(estimator, min_samples=2, residual_threshold=5, random_state=0, loss=loss_multi1)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator2->RANSACRegressor(estimator, min_samples=2, residual_threshold=5, random_state=0, loss=loss_multi2)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator5->RANSACRegressor(estimator, min_samples=2, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator6->RANSACRegressor(estimator, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator7->RANSACRegressor(estimator, min_samples=X.shape[0] + 1, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator8->RANSACRegressor(Ridge(), min_samples=None, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.yyy->numpy.column_stack([y, y, y])
A:sklearn.linear_model.tests.test_ransac.ransac_estimator0->RANSACRegressor(estimator, min_samples=2, residual_threshold=5, random_state=0)
A:sklearn.linear_model.tests.test_ransac.ransac_estimator3->RANSACRegressor(estimator, min_samples=2, residual_threshold=5, random_state=0, loss='squared_error')
A:sklearn.linear_model.tests.test_ransac.weights->numpy.ones(n_samples)
A:sklearn.linear_model.tests.test_ransac.random_state->check_random_state(0)
A:sklearn.linear_model.tests.test_ransac.X_->numpy.append(X_, outlier_X, axis=0)
A:sklearn.linear_model.tests.test_ransac.y_->numpy.append(y_, outlier_y)
A:sklearn.linear_model.tests.test_ransac.sample_weight->check_random_state(42).randint(1, 4, size=y.shape[0])
A:sklearn.linear_model.tests.test_ransac.outlier_X->check_random_state(0).randint(0, 1000, [1, 1])
A:sklearn.linear_model.tests.test_ransac.outlier_weight->check_random_state(0).randint(0, 10, 1)
A:sklearn.linear_model.tests.test_ransac.outlier_y->check_random_state(0).randint(-1000, 0, 1)
A:sklearn.linear_model.tests.test_ransac.X_flat->numpy.append(np.repeat(X_, sample_weight, axis=0), np.repeat(outlier_X, outlier_weight, axis=0), axis=0)
A:sklearn.linear_model.tests.test_ransac.y_flat->numpy.ndarray.flatten(np.append(np.repeat(y_, sample_weight, axis=0), np.repeat(outlier_y, outlier_weight, axis=0), axis=0))
A:sklearn.linear_model.tests.test_ransac.(X, y)->make_regression(n_samples=1000, random_state=10)
A:sklearn.linear_model.tests.test_ransac.ransac->RANSACRegressor(estimator=LinearRegression(), random_state=0)
A:sklearn.linear_model.tests.test_ransac.final_model->LinearRegression()
sklearn.linear_model.tests.test_ransac.test_perfect_horizontal_line()
sklearn.linear_model.tests.test_ransac.test_ransac_default_residual_threshold()
sklearn.linear_model.tests.test_ransac.test_ransac_dynamic_max_trials()
sklearn.linear_model.tests.test_ransac.test_ransac_exceed_max_skips()
sklearn.linear_model.tests.test_ransac.test_ransac_final_model_fit_sample_weight()
sklearn.linear_model.tests.test_ransac.test_ransac_fit_sample_weight()
sklearn.linear_model.tests.test_ransac.test_ransac_inliers_outliers()
sklearn.linear_model.tests.test_ransac.test_ransac_is_data_valid()
sklearn.linear_model.tests.test_ransac.test_ransac_is_model_valid()
sklearn.linear_model.tests.test_ransac.test_ransac_max_trials()
sklearn.linear_model.tests.test_ransac.test_ransac_min_n_samples()
sklearn.linear_model.tests.test_ransac.test_ransac_multi_dimensional_targets()
sklearn.linear_model.tests.test_ransac.test_ransac_no_valid_data()
sklearn.linear_model.tests.test_ransac.test_ransac_no_valid_model()
sklearn.linear_model.tests.test_ransac.test_ransac_none_estimator()
sklearn.linear_model.tests.test_ransac.test_ransac_predict()
sklearn.linear_model.tests.test_ransac.test_ransac_residual_loss()
sklearn.linear_model.tests.test_ransac.test_ransac_score()
sklearn.linear_model.tests.test_ransac.test_ransac_sparse(sparse_container)
sklearn.linear_model.tests.test_ransac.test_ransac_stop_n_inliers()
sklearn.linear_model.tests.test_ransac.test_ransac_stop_score()
sklearn.linear_model.tests.test_ransac.test_ransac_warn_exceed_max_skips()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_estimator_html_repr.py----------------------------------------
A:sklearn.utils._estimator_html_repr._CONTAINER_ID_COUNTER->_IDCounter('sk-container-id')
A:sklearn.utils._estimator_html_repr._ESTIMATOR_ID_COUNTER->_IDCounter('sk-estimator-id')
A:sklearn.utils._estimator_html_repr._CSS_STYLE->_get_css_style()
A:sklearn.utils._estimator_html_repr.name->html.escape(name)
A:sklearn.utils._estimator_html_repr.name_details->html.escape(str(name_details))
A:sklearn.utils._estimator_html_repr.est_id->_IDCounter('sk-estimator-id').get_id()
A:sklearn.utils._estimator_html_repr.est_block->_get_visual_block(estimator)
A:sklearn.utils._estimator_html_repr.doc_link->estimator._get_doc_link()
A:sklearn.utils._estimator_html_repr.est_infos->zip(est_block.estimators, est_block.names, est_block.name_details)
A:sklearn.utils._estimator_html_repr.serial_block->_VisualBlock('serial', [est], dash_wrapped=False)
A:sklearn.utils._estimator_html_repr.container_id->_IDCounter('sk-container-id').get_id()
A:sklearn.utils._estimator_html_repr.style_template->Template(_CSS_STYLE)
A:sklearn.utils._estimator_html_repr.style_with_id->Template(_CSS_STYLE).substitute(id=container_id)
A:sklearn.utils._estimator_html_repr.estimator_str->str(estimator)
A:sklearn.utils._estimator_html_repr.html_output->out.getvalue()
A:sklearn.utils._estimator_html_repr.sklearn_version->parse_version(__version__)
A:sklearn.utils._estimator_html_repr.estimator_module->'.'.join(itertools.takewhile(lambda part: not part.startswith('_'), self.__class__.__module__.split('.')))
sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin._doc_link_template(self)
sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin._doc_link_template(self,value)
sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin._get_doc_link(self)
sklearn.utils._estimator_html_repr._IDCounter(self,prefix)
sklearn.utils._estimator_html_repr._IDCounter.__init__(self,prefix)
sklearn.utils._estimator_html_repr._IDCounter.get_id(self)
sklearn.utils._estimator_html_repr._VisualBlock(self,kind,estimators,*,names=None,name_details=None,dash_wrapped=True)
sklearn.utils._estimator_html_repr._VisualBlock.__init__(self,kind,estimators,*,names=None,name_details=None,dash_wrapped=True)
sklearn.utils._estimator_html_repr._VisualBlock._sk_visual_block_(self)
sklearn.utils._estimator_html_repr._get_css_style()
sklearn.utils._estimator_html_repr._get_visual_block(estimator)
sklearn.utils._estimator_html_repr._write_estimator_html(out,estimator,estimator_label,estimator_label_details,is_fitted_css_class,is_fitted_icon='',first_call=False)
sklearn.utils._estimator_html_repr._write_label_html(out,name,name_details,outer_class='sk-label-container',inner_class='sk-label',checked=False,doc_link='',is_fitted_css_class='',is_fitted_icon='')
sklearn.utils._estimator_html_repr.estimator_html_repr(estimator)
sklearn.utils.estimator_html_repr(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/validation.py----------------------------------------
A:sklearn.utils.validation.sig->signature(f)
A:sklearn.utils.validation.args_msg->', '.join(args_msg)
A:sklearn.utils.validation.(xp, _)->get_namespace(X)
A:sklearn.utils.validation.X->X.tocsr().tocsr()
A:sklearn.utils.validation.first_pass_isfinite->xp.isfinite(xp.sum(X))
A:sklearn.utils.validation.out->cy_isfinite(X.reshape(-1), allow_nan=allow_nan)
A:sklearn.utils.validation.has_inf->xp.any(xp.isinf(X))
A:sklearn.utils.validation.type_->type(X)
A:sklearn.utils.validation.x->x.tocsr().tocsr()
A:sklearn.utils.validation.memory->joblib.Memory(location=memory, verbose=0)
A:sklearn.utils.validation.uniques->numpy.unique(lengths)
A:sklearn.utils.validation.sparse_container->sparse_container.copy().copy()
A:sklearn.utils.validation.(xp, is_array_api_compliant)->get_namespace(array)
A:sklearn.utils.validation.dtype_orig->numpy.result_type(*dtypes_orig)
A:sklearn.utils.validation.dtypes_orig->list(array.dtypes)
A:sklearn.utils.validation.pandas_requires_conversion->_pandas_dtype_needs_early_conversion(array.dtype)
A:sklearn.utils.validation.type_if_series->type(array)
A:sklearn.utils.validation.array->getattr(0.5 * (array + array.T), conversion)()
A:sklearn.utils.validation.dtype->numpy.dtype(dtype)
A:sklearn.utils.validation.estimator_name->estimator.__class__.__name__.lower()
A:sklearn.utils.validation.unique_dtypes->set([dt.subtype.name for dt in array_orig.dtypes])
A:sklearn.utils.validation.n_samples->_num_samples(X)
A:sklearn.utils.validation.y->y.tocsr().tocsr()
A:sklearn.utils.validation.diff->diff.tocsr().tocsr()
A:sklearn.utils.validation.symmetric->numpy.allclose(array, array.T, atol=tol)
A:sklearn.utils.validation.X_min->xp.min(X)
A:sklearn.utils.validation.types_str->', '.join((type_name(t) for t in target_type))
A:sklearn.utils.validation.target_type_str->type_name(target_type)
A:sklearn.utils.validation.lambdas->numpy.real(lambdas)
A:sklearn.utils.validation.max_imag_abs->numpy.abs(np.imag(lambdas)).max()
A:sklearn.utils.validation.max_real_abs->numpy.abs(np.real(lambdas)).max()
A:sklearn.utils.validation.max_eig->numpy.real(lambdas).max()
A:sklearn.utils.validation.min_eig->numpy.real(lambdas).min()
A:sklearn.utils.validation.sample_weight->check_array(sample_weight, accept_sparse=False, ensure_2d=False, dtype=dtype, order='C', copy=copy, input_name='sample_weight')
A:sklearn.utils.validation.prediction_method->reduce(lambda x, y: x or y, prediction_method)
A:sklearn.utils.validation.method_params_validated[param_key]->_safe_indexing(method_params_validated[param_key], indices)
A:sklearn.utils.validation.feature_names->numpy.asarray(list(df_protocol.column_names()), dtype=object)
A:sklearn.utils.validation.df_protocol->X.tocsr().tocsr().__dataframe__()
A:sklearn.utils.validation.types->sorted((t.__qualname__ for t in set((type(v) for v in feature_names))))
A:sklearn.utils.validation.feature_names_in_->getattr(estimator, 'feature_names_in_', None)
A:sklearn.utils.validation.n_features_in_->getattr(estimator, 'n_features_in_', None)
A:sklearn.utils.validation.input_features->numpy.asarray(input_features, dtype=object)
A:sklearn.utils.validation.monotonic_cst->numpy.asarray(monotonic_cst, dtype=np.int8)
A:sklearn.utils.validation.unexpected_feature_names->list(set(original_monotonic_cst) - set(estimator.feature_names_in_))
A:sklearn.utils.validation.n_unexpeced->len(unexpected_feature_names)
A:sklearn.utils.validation.unexpected_cst->numpy.setdiff1d(monotonic_cst, [-1, 0, 1])
A:sklearn.utils.validation.classes->numpy.unique(y_true)
A:sklearn.utils.validation.classes_repr->', '.join([repr(c) for c in classes.tolist()])
sklearn.utils._is_arraylike_not_scalar(array)
sklearn.utils._is_pandas_df(X)
sklearn.utils._is_polars_df(X)
sklearn.utils._use_interchange_protocol(X)
sklearn.utils.as_float_array(X,*,copy=True,force_all_finite=True)
sklearn.utils.assert_all_finite(X,*,allow_nan=False,estimator_name=None,input_name='')
sklearn.utils.check_X_y(X,y,accept_sparse=False,*,accept_large_sparse=True,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,multi_output=False,ensure_min_samples=1,ensure_min_features=1,y_numeric=False,estimator=None)
sklearn.utils.check_array(array,accept_sparse=False,*,accept_large_sparse=True,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,ensure_min_samples=1,ensure_min_features=1,estimator=None,input_name='')
sklearn.utils.check_consistent_length(*arrays)
sklearn.utils.check_random_state(seed)
sklearn.utils.check_scalar(x,name,target_type,*,min_val=None,max_val=None,include_boundaries='both')
sklearn.utils.check_symmetric(array,*,tol=1e-10,raise_warning=True,raise_exception=False)
sklearn.utils.column_or_1d(y,*,dtype=None,warn=False)
sklearn.utils.indexable(*iterables)
sklearn.utils.validation._allclose_dense_sparse(x,y,rtol=1e-07,atol=1e-09)
sklearn.utils.validation._assert_all_finite(X,allow_nan=False,msg_dtype=None,estimator_name=None,input_name='')
sklearn.utils.validation._assert_all_finite_element_wise(X,*,xp,allow_nan,msg_dtype=None,estimator_name=None,input_name='')
sklearn.utils.validation._check_estimator_name(estimator)
sklearn.utils.validation._check_feature_names_in(estimator,input_features=None,*,generate_names=True)
sklearn.utils.validation._check_large_sparse(X,accept_large_sparse=False)
sklearn.utils.validation._check_method_params(X,params,indices=None)
sklearn.utils.validation._check_monotonic_cst(estimator,monotonic_cst=None)
sklearn.utils.validation._check_pos_label_consistency(pos_label,y_true)
sklearn.utils.validation._check_psd_eigenvalues(lambdas,enable_warnings=False)
sklearn.utils.validation._check_response_method(estimator,response_method)
sklearn.utils.validation._check_sample_weight(sample_weight,X,dtype=None,copy=False,only_non_negative=False)
sklearn.utils.validation._check_y(y,multi_output=False,y_numeric=False,estimator=None)
sklearn.utils.validation._deprecate_positional_args(func=None,*,version='1.3')
sklearn.utils.validation._ensure_no_complex_data(array)
sklearn.utils.validation._ensure_sparse_format(sparse_container,accept_sparse,dtype,copy,force_all_finite,accept_large_sparse,estimator_name=None,input_name='')
sklearn.utils.validation._generate_get_feature_names_out(estimator,n_features_out,input_features=None)
sklearn.utils.validation._get_feature_names(X)
sklearn.utils.validation._is_arraylike(x)
sklearn.utils.validation._is_arraylike_not_scalar(array)
sklearn.utils.validation._is_extension_array_dtype(array)
sklearn.utils.validation._is_fitted(estimator,attributes=None,all_or_any=all)
sklearn.utils.validation._is_pandas_df(X)
sklearn.utils.validation._is_polars_df(X)
sklearn.utils.validation._make_indexable(iterable)
sklearn.utils.validation._num_features(X)
sklearn.utils.validation._num_samples(x)
sklearn.utils.validation._pandas_dtype_needs_early_conversion(pd_dtype)
sklearn.utils.validation._use_interchange_protocol(X)
sklearn.utils.validation.as_float_array(X,*,copy=True,force_all_finite=True)
sklearn.utils.validation.assert_all_finite(X,*,allow_nan=False,estimator_name=None,input_name='')
sklearn.utils.validation.check_X_y(X,y,accept_sparse=False,*,accept_large_sparse=True,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,multi_output=False,ensure_min_samples=1,ensure_min_features=1,y_numeric=False,estimator=None)
sklearn.utils.validation.check_array(array,accept_sparse=False,*,accept_large_sparse=True,dtype='numeric',order=None,copy=False,force_all_finite=True,ensure_2d=True,allow_nd=False,ensure_min_samples=1,ensure_min_features=1,estimator=None,input_name='')
sklearn.utils.validation.check_consistent_length(*arrays)
sklearn.utils.validation.check_is_fitted(estimator,attributes=None,*,msg=None,all_or_any=all)
sklearn.utils.validation.check_memory(memory)
sklearn.utils.validation.check_non_negative(X,whom)
sklearn.utils.validation.check_random_state(seed)
sklearn.utils.validation.check_scalar(x,name,target_type,*,min_val=None,max_val=None,include_boundaries='both')
sklearn.utils.validation.check_symmetric(array,*,tol=1e-10,raise_warning=True,raise_exception=False)
sklearn.utils.validation.column_or_1d(y,*,dtype=None,warn=False)
sklearn.utils.validation.has_fit_parameter(estimator,parameter)
sklearn.utils.validation.indexable(*iterables)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/__init__.py----------------------------------------
A:sklearn.utils.__init__.modules_info->threadpool_info()
A:sklearn.utils.__init__.open_blas_used->any((info['internal_api'] == 'openblas' for info in modules_info))
A:sklearn.utils.__init__.openblas_arm64_stable_version->parse_version('0.3.16')
A:sklearn.utils.__init__.openblas_version->info.get('version')
A:sklearn.utils.__init__.openblas_architecture->info.get('architecture')
A:sklearn.utils.__init__.mask->numpy.zeros(mask_length, dtype=bool)
A:sklearn.utils.__init__.ind->numpy.arange(mask.shape[0])
A:sklearn.utils.__init__.key->key.tolist().tolist()
A:sklearn.utils.__init__.key_start_type->_determine_key_type(key.start)
A:sklearn.utils.__init__.key_stop_type->_determine_key_type(key.stop)
A:sklearn.utils.__init__.unique_key->set(key)
A:sklearn.utils.__init__.indices_dtype->_determine_key_type(indices)
A:sklearn.utils.__init__.idx->_safe_indexing(np.arange(n_columns), key)
A:sklearn.utils.__init__.key_dtype->_determine_key_type(key)
A:sklearn.utils.__init__.start->timeit.default_timer()
A:sklearn.utils.__init__.columns->list(key)
A:sklearn.utils.__init__.col_idx->all_columns.get_loc(col)
A:sklearn.utils.__init__.n_columns->X_interchange.num_columns()
A:sklearn.utils.__init__.column_names->list(X_interchange.column_names())
A:sklearn.utils.__init__.random_state->check_random_state(random_state)
A:sklearn.utils.__init__.indices->check_random_state(random_state).permutation(indices)
A:sklearn.utils.__init__.y->numpy.array([' '.join(row.astype('str')) for row in y])
A:sklearn.utils.__init__.(classes, y_indices)->numpy.unique(y, return_inverse=True)
A:sklearn.utils.__init__.class_counts->numpy.bincount(y_indices)
A:sklearn.utils.__init__.class_indices->numpy.split(np.argsort(y_indices, kind='mergesort'), np.cumsum(class_counts)[:-1])
A:sklearn.utils.__init__.n_i->_approximate_mode(class_counts, max_n_samples, random_state)
A:sklearn.utils.__init__.indices_i->check_random_state(random_state).choice(class_indices[i], n_i[i], replace=replace)
A:sklearn.utils.__init__.X->X.copy().copy()
A:sklearn.utils.__init__.chunk->list(islice(gen, chunksize))
A:sklearn.utils.__init__.end->min(n_samples, end)
A:sklearn.utils.__init__.out->numpy.empty(len(sequence), dtype=object)
A:sklearn.utils.__init__.chunk_n_rows->min(chunk_n_rows, max_n_rows)
A:sklearn.utils.__init__.rng->check_random_state(rng)
A:sklearn.utils.__init__.floored->numpy.floor(continuous)
A:sklearn.utils.__init__.need_to_add->int(n_draws - floored.sum())
A:sklearn.utils.__init__.(inds,)->numpy.where(remainder == value)
A:sklearn.utils.__init__.add_now->min(len(inds), need_to_add)
A:sklearn.utils.__init__.inds->check_random_state(rng).choice(inds, size=add_now, replace=False)
sklearn.utils.__init__._approximate_mode(class_counts,n_draws,rng)
sklearn.utils.__init__._array_indexing(array,key,key_dtype,axis)
sklearn.utils.__init__._chunk_generator(gen,chunksize)
sklearn.utils.__init__._determine_key_type(key,accept_slice=True)
sklearn.utils.__init__._get_column_indices(X,key)
sklearn.utils.__init__._get_column_indices_for_bool_or_int(key,n_columns)
sklearn.utils.__init__._get_column_indices_interchange(X_interchange,key,key_dtype)
sklearn.utils.__init__._in_unstable_openblas_configuration()
sklearn.utils.__init__._is_pandas_na(x)
sklearn.utils.__init__._list_indexing(X,key,key_dtype)
sklearn.utils.__init__._message_with_time(source,message,time)
sklearn.utils.__init__._pandas_indexing(X,key,key_dtype,axis)
sklearn.utils.__init__._polars_indexing(X,key,key_dtype,axis)
sklearn.utils.__init__._print_elapsed_time(source,message=None)
sklearn.utils.__init__._safe_assign(X,values,*,row_indexer=None,column_indexer=None)
sklearn.utils.__init__._safe_indexing(X,indices,*,axis=0)
sklearn.utils.__init__._to_object_array(sequence)
sklearn.utils.__init__.axis0_safe_slice(X,mask,len_mask)
sklearn.utils.__init__.check_matplotlib_support(caller_name)
sklearn.utils.__init__.check_pandas_support(caller_name)
sklearn.utils.__init__.gen_batches(n,batch_size,*,min_batch_size=0)
sklearn.utils.__init__.gen_even_slices(n,n_packs,*,n_samples=None)
sklearn.utils.__init__.get_chunk_n_rows(row_bytes,*,max_n_rows=None,working_memory=None)
sklearn.utils.__init__.indices_to_mask(indices,mask_length)
sklearn.utils.__init__.is_scalar_nan(x)
sklearn.utils.__init__.resample(*arrays,replace=True,n_samples=None,random_state=None,stratify=None)
sklearn.utils.__init__.safe_mask(X,mask)
sklearn.utils.__init__.safe_sqr(X,*,copy=True)
sklearn.utils.__init__.shuffle(*arrays,random_state=None,n_samples=None)
sklearn.utils.__init__.tosequence(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/fixes.py----------------------------------------
A:sklearn.utils.fixes.np_version->parse_version(np.__version__)
A:sklearn.utils.fixes.np_base_version->parse_version(np_version.base_version)
A:sklearn.utils.fixes.sp_version->parse_version(scipy.__version__)
A:sklearn.utils.fixes.sp_base_version->parse_version(sp_version.base_version)
A:sklearn.utils.fixes.sklearn._sklearn_threadpool_controller->threadpoolctl.ThreadpoolController()
A:sklearn.utils.fixes.controller->_get_threadpool_controller()
A:sklearn.utils.fixes.mode->numpy.ravel(mode)
A:sklearn.utils.fixes.kwargs['tol']->kwargs.pop('rtol')
A:sklearn.utils.fixes.the_min->the_min.toarray().ravel().toarray().ravel()
A:sklearn.utils.fixes.the_max->the_max.toarray().ravel().toarray().ravel()
A:sklearn.utils.fixes.major_index->numpy.compress(mask, major_index)
A:sklearn.utils.fixes.X->type(X)((X.data, X.indices, X.indptr), shape=X.shape)
A:sklearn.utils.fixes.value->numpy.compress(mask, value)
A:sklearn.utils.fixes.(major_index, value)->_minor_reduce(mat, min_or_max)
A:sklearn.utils.fixes.value[not_full]->min_or_max(value[not_full], 0)
A:sklearn.utils.fixes.res->scipy.sparse.coo_matrix((value, (major_index, np.zeros(len(value)))), dtype=X.dtype, shape=(M, 1))
A:sklearn.utils.fixes.zero->type(X)((X.data, X.indices, X.indptr), shape=X.shape).dtype.type(0)
A:sklearn.utils.fixes.m->min_or_max(zero, m)
A:sklearn.utils.fixes.frame->frame.fillna(value=np.nan).infer_objects(copy=False).fillna(value=np.nan).infer_objects(copy=False)
A:sklearn.utils.fixes.index_dtype->_smallest_admissible_index_dtype(maxval=max(sparse_container.shape))
A:sklearn.utils.fixes.sparse_container.indices->sparse_container.indices.astype(index_dtype, copy=False)
A:sklearn.utils.fixes.sparse_container.indptr->sparse_container.indptr.astype(index_dtype, copy=False)
A:sklearn.utils.fixes.sparse_container.row->sparse_container.row.astype(index_dtype, copy=False)
A:sklearn.utils.fixes.sparse_container.col->sparse_container.col.astype(index_dtype, copy=False)
A:sklearn.utils.fixes.int32min->numpy.int32(np.iinfo(np.int32).min)
A:sklearn.utils.fixes.int32max->numpy.int32(np.iinfo(np.int32).max)
A:sklearn.utils.fixes.maxval->arr.max()
A:sklearn.utils.fixes.minval->arr.min()
sklearn.utils.fixes._get_threadpool_controller()
sklearn.utils.fixes._mode(a,axis=0)
sklearn.utils.fixes._object_dtype_isnan(X)
sklearn.utils.fixes._percentile(a,q,*,method='linear',**kwargs)
sklearn.utils.fixes._preserve_dia_indices_dtype(sparse_container,original_container_format,requested_sparse_format)
sklearn.utils.fixes._smallest_admissible_index_dtype(arrays=(),maxval=None,check_contents=False)
sklearn.utils.fixes.delayed(function)
sklearn.utils.fixes.pd_fillna(pd,frame)
sklearn.utils.fixes.threadpool_info()
sklearn.utils.fixes.threadpool_limits(limits=None,user_api=None)
sklearn.utils.threadpool_info()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/optimize.py----------------------------------------
A:sklearn.utils.optimize.ret->line_search_wolfe2(f, fprime, xk, pk, gfk, old_fval, old_old_fval, **kwargs)
A:sklearn.utils.optimize.args->kwargs.get('args', tuple())
A:sklearn.utils.optimize.fval->f(xk + pk, *args)
A:sklearn.utils.optimize.tiny_loss->numpy.abs(old_fval * eps)
A:sklearn.utils.optimize.sum_abs_grad_old->scipy.linalg.norm(gfk, ord=1)
A:sklearn.utils.optimize.grad->fprime(xk + pk, *args)
A:sklearn.utils.optimize.sum_abs_grad->scipy.linalg.norm(grad, ord=1)
A:sklearn.utils.optimize.xsupi->_cg(fhess_p, fgrad, maxiter=maxinner, tol=termcond)
A:sklearn.utils.optimize.ri->numpy.copy(fgrad)
A:sklearn.utils.optimize.dri0->numpy.dot(ri, ri)
A:sklearn.utils.optimize.Ap->fhess_p(psupi)
A:sklearn.utils.optimize.curv->numpy.dot(psupi, Ap)
A:sklearn.utils.optimize.dri1->numpy.dot(ri, ri)
A:sklearn.utils.optimize.x0->numpy.asarray(x0).flatten()
A:sklearn.utils.optimize.xk->numpy.copy(x0)
A:sklearn.utils.optimize.old_fval->func(x0, *args)
A:sklearn.utils.optimize.(fgrad, fhess_p)->grad_hess(xk, *args)
A:sklearn.utils.optimize.absgrad->numpy.abs(fgrad)
A:sklearn.utils.optimize.maggrad->numpy.sum(absgrad)
A:sklearn.utils.optimize.eta->min([0.5, np.sqrt(maggrad)])
A:sklearn.utils.optimize.(alphak, fc, gc, old_fval, old_old_fval, gfkp1)->_line_search_wolfe12(func, grad, xk, xsupi, fgrad, old_fval, old_old_fval, args=args)
A:sklearn.utils.optimize.result_message->result.message.decode('latin1')
A:sklearn.utils.optimize.warning_msg->'{} failed to converge (status={}):\n{}.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html'.format(solver, result.status, result_message)
A:sklearn.utils.optimize.n_iter_i->min(result.nit, max_iter)
sklearn.utils.optimize._LineSearchError(RuntimeError)
sklearn.utils.optimize._cg(fhess_p,fgrad,maxiter,tol)
sklearn.utils.optimize._check_optimize_result(solver,result,max_iter=None,extra_warning_msg=None)
sklearn.utils.optimize._line_search_wolfe12(f,fprime,xk,pk,gfk,old_fval,old_old_fval,**kwargs)
sklearn.utils.optimize._newton_cg(grad_hess,func,grad,x0,args=(),tol=0.0001,maxiter=100,maxinner=200,line_search=True,warn=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/stats.py----------------------------------------
A:sklearn.utils.stats.array->array.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.utils.stats.sorted_idx->numpy.argsort(array, axis=0)
A:sklearn.utils.stats.sorted_weights->numpy.take_along_axis(sample_weight, sorted_idx, axis=0)
A:sklearn.utils.stats.weight_cdf->stable_cumsum(sorted_weights, axis=0)
A:sklearn.utils.stats.adjusted_percentile[mask]->numpy.nextafter(adjusted_percentile[mask], adjusted_percentile[mask] + 1)
A:sklearn.utils.stats.percentile_idx->numpy.apply_along_axis(lambda x: np.clip(x, 0, max_idx), axis=0, arr=percentile_idx)
A:sklearn.utils.stats.col_index->numpy.arange(array.shape[1])
sklearn.utils.stats._weighted_percentile(array,sample_weight,percentile=50)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_set_output.py----------------------------------------
A:sklearn.utils._set_output.pd->check_library_installed('pandas')
A:sklearn.utils._set_output.columns->columns.tolist().tolist()
A:sklearn.utils._set_output.pl->check_library_installed('polars')
A:sklearn.utils._set_output.ADAPTERS_MANAGER->ContainerAdaptersManager()
A:sklearn.utils._set_output.est_sklearn_output_config->getattr(estimator, '_sklearn_output_config', {})
A:sklearn.utils._set_output.output_config->_get_output_config(method, estimator)
A:sklearn.utils._set_output.data_to_wrap->f(self, X, *args, **kwargs)
A:sklearn.utils._set_output.auto_wrap_output_keys->getattr(estimator, '_sklearn_auto_wrap_output_keys', set())
A:sklearn.utils._set_output.cls._sklearn_auto_wrap_output_keys->set()
A:sklearn.utils._set_output.wrapped_method->_wrap_method_output(getattr(cls, method), key)
sklearn.utils._set_output.ContainerAdapterProtocol(Protocol)
sklearn.utils._set_output.ContainerAdapterProtocol.create_container(self,X_output,X_original,columns)
sklearn.utils._set_output.ContainerAdapterProtocol.hstack(self,Xs)
sklearn.utils._set_output.ContainerAdapterProtocol.is_supported_container(self,X)
sklearn.utils._set_output.ContainerAdapterProtocol.rename_columns(self,X,columns)
sklearn.utils._set_output.ContainerAdaptersManager(self)
sklearn.utils._set_output.ContainerAdaptersManager.__init__(self)
sklearn.utils._set_output.ContainerAdaptersManager.register(self,adapter)
sklearn.utils._set_output.ContainerAdaptersManager.supported_outputs(self)
sklearn.utils._set_output.PandasAdapter
sklearn.utils._set_output.PandasAdapter.create_container(self,X_output,X_original,columns)
sklearn.utils._set_output.PandasAdapter.hstack(self,Xs)
sklearn.utils._set_output.PandasAdapter.is_supported_container(self,X)
sklearn.utils._set_output.PandasAdapter.rename_columns(self,X,columns)
sklearn.utils._set_output.PolarsAdapter
sklearn.utils._set_output.PolarsAdapter.create_container(self,X_output,X_original,columns)
sklearn.utils._set_output.PolarsAdapter.hstack(self,Xs)
sklearn.utils._set_output.PolarsAdapter.is_supported_container(self,X)
sklearn.utils._set_output.PolarsAdapter.rename_columns(self,X,columns)
sklearn.utils._set_output._SetOutputMixin
sklearn.utils._set_output._SetOutputMixin.__init_subclass__(cls,auto_wrap_output_keys=('transform',),**kwargs)
sklearn.utils._set_output._SetOutputMixin.set_output(self,*,transform=None)
sklearn.utils._set_output._auto_wrap_is_configured(estimator)
sklearn.utils._set_output._get_container_adapter(method,estimator=None)
sklearn.utils._set_output._get_output_config(method,estimator=None)
sklearn.utils._set_output._safe_set_output(estimator,*,transform=None)
sklearn.utils._set_output._wrap_data_with_container(method,data_to_wrap,original_input,estimator)
sklearn.utils._set_output._wrap_method_output(f,method)
sklearn.utils._set_output.check_library_installed(library)
sklearn.utils._set_output.get_columns(columns)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/discovery.py----------------------------------------
A:sklearn.utils.discovery.root->str(Path(__file__).parent.parent)
A:sklearn.utils.discovery.module_parts->module_name.split('.')
A:sklearn.utils.discovery.module->import_module(module_name)
A:sklearn.utils.discovery.classes->inspect.getmembers(module, inspect.isclass)
A:sklearn.utils.discovery.all_classes->set(all_classes)
A:sklearn.utils.discovery.type_filter->list(type_filter)
A:sklearn.utils.discovery.functions->inspect.getmembers(module, _is_checked_function)
sklearn.utils.all_estimators(type_filter=None)
sklearn.utils.discovery._is_checked_function(item)
sklearn.utils.discovery.all_displays()
sklearn.utils.discovery.all_estimators(type_filter=None)
sklearn.utils.discovery.all_functions()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_joblib.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_show_versions.py----------------------------------------
A:sklearn.utils._show_versions.python->sys.version.replace('\n', ' ')
A:sklearn.utils._show_versions.deps_info[modname]->version(modname)
A:sklearn.utils._show_versions.sys_info->_get_sys_info()
A:sklearn.utils._show_versions.deps_info->_get_deps_info()
A:sklearn.utils._show_versions.threadpool_results->threadpool_info()
sklearn.show_versions()
sklearn.utils._show_versions._get_deps_info()
sklearn.utils._show_versions._get_sys_info()
sklearn.utils._show_versions.show_versions()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_plotting.py----------------------------------------
A:sklearn.utils._plotting.(_, ax)->matplotlib.pyplot.subplots()
A:sklearn.utils._plotting.(y_pred, pos_label)->_get_response_values_binary(estimator, X, response_method=response_method, pos_label=pos_label)
A:sklearn.utils._plotting.pos_label->_check_pos_label_consistency(pos_label, y_true)
A:sklearn.utils._plotting.score_name->score_name.replace('_', ' ').replace('_', ' ')
A:sklearn.utils._plotting.diff->numpy.diff(np.sort(data))
sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin
sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin._validate_and_get_response_values(cls,estimator,X,y,*,response_method='auto',pos_label=None,name=None)
sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin._validate_from_predictions_params(cls,y_true,y_pred,*,sample_weight=None,pos_label=None,name=None)
sklearn.utils._plotting._BinaryClassifierCurveDisplayMixin._validate_plot_params(self,*,ax=None,name=None)
sklearn.utils._plotting._interval_max_min_ratio(data)
sklearn.utils._plotting._validate_score_name(score_name,scoring,negate_score)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/metaestimators.py----------------------------------------
A:sklearn.utils.metaestimators.out->super().get_params(deep=deep)
A:sklearn.utils.metaestimators.estimators->getattr(self, attr)
A:sklearn.utils.metaestimators.items->getattr(self, attr)
A:sklearn.utils.metaestimators.(item_names, _)->zip(*items)
A:sklearn.utils.metaestimators.new_estimators->list(getattr(self, attr))
A:sklearn.utils.metaestimators.invalid_names->set(names).intersection(self.get_params(deep=False))
A:sklearn.utils.metaestimators.X_subset->_safe_indexing(X, indices)
A:sklearn.utils.metaestimators.y_subset->_safe_indexing(y, indices)
sklearn.utils.metaestimators._BaseComposition(self)
sklearn.utils.metaestimators._BaseComposition.__init__(self)
sklearn.utils.metaestimators._BaseComposition._get_params(self,attr,deep=True)
sklearn.utils.metaestimators._BaseComposition._replace_estimator(self,attr,name,new_val)
sklearn.utils.metaestimators._BaseComposition._set_params(self,attr,**params)
sklearn.utils.metaestimators._BaseComposition._validate_names(self,names)
sklearn.utils.metaestimators._safe_split(estimator,X,y,indices,train_indices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_array_api.py----------------------------------------
A:sklearn.utils._array_api.numpy_version->parse_version(numpy.__version__)
A:sklearn.utils._array_api.complex_dtypes->set()
A:sklearn.utils._array_api.attr->getattr(numpy, name)
A:sklearn.utils._array_api.x->x.copy().copy()
A:sklearn.utils._array_api._NUMPY_API_WRAPPER_INSTANCE->_NumPyAPIWrapper()
A:sklearn.utils._array_api.namespace->_ArrayAPIWrapper(namespace)
A:sklearn.utils._array_api.(xp, _)->get_namespace(array)
A:sklearn.utils._array_api.value->xp.asarray(value, dtype=array.dtype)
A:sklearn.utils._array_api.array_np->numpy.asarray(array)
A:sklearn.utils._array_api.sample_score_np->numpy.asarray(sample_score)
A:sklearn.utils._array_api.sample_weight_np->numpy.asarray(sample_weight)
A:sklearn.utils._array_api.sample_score->xp.astype(xp.asarray(sample_score, device='cpu'), xp.float64)
A:sklearn.utils._array_api.sample_weight->xp.astype(sample_weight, xp.float64)
A:sklearn.utils._array_api.scale->xp.sum(sample_weight)
A:sklearn.utils._array_api.mask->xp.all(mask, axis=axis)
A:sklearn.utils._array_api.X->xp.where(mask, xp.asarray(xp.nan), X)
A:sklearn.utils._array_api.array->numpy.asarray(array, order=order, dtype=dtype)
A:sklearn.utils._array_api.new_estimator->clone(estimator)
A:sklearn.utils._array_api.attribute->converter(attribute)
sklearn.utils._array_api._ArrayAPIWrapper(self,array_namespace)
sklearn.utils._array_api._ArrayAPIWrapper.__eq__(self,other)
sklearn.utils._array_api._ArrayAPIWrapper.__getattr__(self,name)
sklearn.utils._array_api._ArrayAPIWrapper.__init__(self,array_namespace)
sklearn.utils._array_api._ArrayAPIWrapper.isdtype(self,dtype,kind)
sklearn.utils._array_api._NumPyAPIWrapper
sklearn.utils._array_api._NumPyAPIWrapper.__getattr__(self,name)
sklearn.utils._array_api._NumPyAPIWrapper.asarray(self,x,*,dtype=None,device=None,copy=None)
sklearn.utils._array_api._NumPyAPIWrapper.astype(self,x,dtype,*,copy=True,casting='unsafe')
sklearn.utils._array_api._NumPyAPIWrapper.bool(self)
sklearn.utils._array_api._NumPyAPIWrapper.concat(self,arrays,*,axis=None)
sklearn.utils._array_api._NumPyAPIWrapper.isdtype(self,dtype,kind)
sklearn.utils._array_api._NumPyAPIWrapper.reshape(self,x,shape,*,copy=None)
sklearn.utils._array_api._NumPyAPIWrapper.unique_counts(self,x)
sklearn.utils._array_api._NumPyAPIWrapper.unique_inverse(self,x)
sklearn.utils._array_api._NumPyAPIWrapper.unique_values(self,x)
sklearn.utils._array_api._accept_device_cpu(func)
sklearn.utils._array_api._add_to_diagonal(array,value,xp)
sklearn.utils._array_api._asarray_with_order(array,dtype=None,order=None,copy=None,*,xp=None)
sklearn.utils._array_api._atol_for_type(dtype)
sklearn.utils._array_api._check_array_api_dispatch(array_api_dispatch)
sklearn.utils._array_api._check_device_cpu(device)
sklearn.utils._array_api._convert_to_numpy(array,xp)
sklearn.utils._array_api._estimator_with_converted_arrays(estimator,converter)
sklearn.utils._array_api._expit(X)
sklearn.utils._array_api._is_numpy_namespace(xp)
sklearn.utils._array_api._isdtype_single(dtype,kind,*,xp)
sklearn.utils._array_api._nanmax(X,axis=None)
sklearn.utils._array_api._nanmin(X,axis=None)
sklearn.utils._array_api._union1d(a,b,xp)
sklearn.utils._array_api._weighted_sum(sample_score,sample_weight,normalize=False,xp=None)
sklearn.utils._array_api.device(x)
sklearn.utils._array_api.get_namespace(*arrays)
sklearn.utils._array_api.isdtype(dtype,kind,*,xp)
sklearn.utils._array_api.size(x)
sklearn.utils._array_api.supported_float_dtypes(xp)
sklearn.utils._array_api.yield_namespace_device_dtype_combinations()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_available_if.py----------------------------------------
A:sklearn.utils._available_if.attr_err->AttributeError(f'This {repr(owner.__name__)} has no attribute {repr(self.attribute_name)}')
A:sklearn.utils._available_if.out->MethodType(self.fn, obj)
sklearn.utils._available_if._AvailableIfDescriptor(self,fn,check,attribute_name)
sklearn.utils._available_if._AvailableIfDescriptor.__get__(self,obj,owner=None)
sklearn.utils._available_if._AvailableIfDescriptor.__init__(self,fn,check,attribute_name)
sklearn.utils._available_if.available_if(check)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/metadata_routing.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_param_validation.py----------------------------------------
A:sklearn.utils._param_validation.constraint->make_constraint(constraint.constraint)
A:sklearn.utils._param_validation.func_sig->signature(func)
A:sklearn.utils._param_validation.params->signature(func).bind(*args, **kwargs)
A:sklearn.utils._param_validation.msg->re.sub('parameter of \\w+ must be', f'parameter of {func.__qualname__} must be', str(e))
A:sklearn.utils._param_validation.left_bound->float(left_bound)
A:sklearn.utils._param_validation.right_bound->float(right_bound)
sklearn.utils.Interval(self,type,left,right,*,closed)
sklearn.utils.Interval.__contains__(self,val)
sklearn.utils.Interval.__str__(self)
sklearn.utils.Interval._check_params(self)
sklearn.utils.Interval.is_satisfied_by(self,val)
sklearn.utils._param_validation.HasMethods(self,methods)
sklearn.utils._param_validation.HasMethods.__init__(self,methods)
sklearn.utils._param_validation.HasMethods.__str__(self)
sklearn.utils._param_validation.HasMethods.is_satisfied_by(self,val)
sklearn.utils._param_validation.Hidden(self,constraint)
sklearn.utils._param_validation.Hidden.__init__(self,constraint)
sklearn.utils._param_validation.Interval(self,type,left,right,*,closed)
sklearn.utils._param_validation.Interval.__contains__(self,val)
sklearn.utils._param_validation.Interval.__init__(self,type,left,right,*,closed)
sklearn.utils._param_validation.Interval.__str__(self)
sklearn.utils._param_validation.Interval._check_params(self)
sklearn.utils._param_validation.Interval.is_satisfied_by(self,val)
sklearn.utils._param_validation.InvalidParameterError(ValueError,TypeError)
sklearn.utils._param_validation.MissingValues(self,numeric_only=False)
sklearn.utils._param_validation.MissingValues.__init__(self,numeric_only=False)
sklearn.utils._param_validation.MissingValues.__str__(self)
sklearn.utils._param_validation.MissingValues.is_satisfied_by(self,val)
sklearn.utils._param_validation.Options(self,type,options,*,deprecated=None)
sklearn.utils._param_validation.Options.__init__(self,type,options,*,deprecated=None)
sklearn.utils._param_validation.Options.__str__(self)
sklearn.utils._param_validation.Options._mark_if_deprecated(self,option)
sklearn.utils._param_validation.Options.is_satisfied_by(self,val)
sklearn.utils._param_validation.RealNotInt(Real)
sklearn.utils._param_validation.StrOptions(self,options,*,deprecated=None)
sklearn.utils._param_validation.StrOptions.__init__(self,options,*,deprecated=None)
sklearn.utils._param_validation._ArrayLikes(_Constraint)
sklearn.utils._param_validation._ArrayLikes.__str__(self)
sklearn.utils._param_validation._ArrayLikes.is_satisfied_by(self,val)
sklearn.utils._param_validation._Booleans(self)
sklearn.utils._param_validation._Booleans.__init__(self)
sklearn.utils._param_validation._Booleans.__str__(self)
sklearn.utils._param_validation._Booleans.is_satisfied_by(self,val)
sklearn.utils._param_validation._CVObjects(self)
sklearn.utils._param_validation._CVObjects.__init__(self)
sklearn.utils._param_validation._CVObjects.__str__(self)
sklearn.utils._param_validation._CVObjects.is_satisfied_by(self,val)
sklearn.utils._param_validation._Callables(_Constraint)
sklearn.utils._param_validation._Callables.__str__(self)
sklearn.utils._param_validation._Callables.is_satisfied_by(self,val)
sklearn.utils._param_validation._Constraint(self)
sklearn.utils._param_validation._Constraint.__init__(self)
sklearn.utils._param_validation._Constraint.__str__(self)
sklearn.utils._param_validation._Constraint.is_satisfied_by(self,val)
sklearn.utils._param_validation._InstancesOf(self,type)
sklearn.utils._param_validation._InstancesOf.__init__(self,type)
sklearn.utils._param_validation._InstancesOf.__str__(self)
sklearn.utils._param_validation._InstancesOf.is_satisfied_by(self,val)
sklearn.utils._param_validation._IterablesNotString(_Constraint)
sklearn.utils._param_validation._IterablesNotString.__str__(self)
sklearn.utils._param_validation._IterablesNotString.is_satisfied_by(self,val)
sklearn.utils._param_validation._NanConstraint(_Constraint)
sklearn.utils._param_validation._NanConstraint.__str__(self)
sklearn.utils._param_validation._NanConstraint.is_satisfied_by(self,val)
sklearn.utils._param_validation._NoneConstraint(_Constraint)
sklearn.utils._param_validation._NoneConstraint.__str__(self)
sklearn.utils._param_validation._NoneConstraint.is_satisfied_by(self,val)
sklearn.utils._param_validation._PandasNAConstraint(_Constraint)
sklearn.utils._param_validation._PandasNAConstraint.__str__(self)
sklearn.utils._param_validation._PandasNAConstraint.is_satisfied_by(self,val)
sklearn.utils._param_validation._RandomStates(self)
sklearn.utils._param_validation._RandomStates.__init__(self)
sklearn.utils._param_validation._RandomStates.__str__(self)
sklearn.utils._param_validation._RandomStates.is_satisfied_by(self,val)
sklearn.utils._param_validation._SparseMatrices(_Constraint)
sklearn.utils._param_validation._SparseMatrices.__str__(self)
sklearn.utils._param_validation._SparseMatrices.is_satisfied_by(self,val)
sklearn.utils._param_validation._VerboseHelper(self)
sklearn.utils._param_validation._VerboseHelper.__init__(self)
sklearn.utils._param_validation._VerboseHelper.__str__(self)
sklearn.utils._param_validation._VerboseHelper.is_satisfied_by(self,val)
sklearn.utils._param_validation._type_name(t)
sklearn.utils._param_validation.generate_invalid_param_val(constraint)
sklearn.utils._param_validation.generate_valid_param(constraint)
sklearn.utils._param_validation.make_constraint(constraint)
sklearn.utils._param_validation.validate_parameter_constraints(parameter_constraints,params,caller_name)
sklearn.utils._param_validation.validate_params(parameter_constraints,*,prefer_skip_nested_validation)
sklearn.utils.validate_params(parameter_constraints,*,prefer_skip_nested_validation)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_tags.py----------------------------------------
A:sklearn.utils._tags.tags->estimator._get_tags()
sklearn.utils._tags._safe_tags(estimator,key=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_metadata_requests.py----------------------------------------
A:sklearn.utils._metadata_requests.unrequested->dict()
A:sklearn.utils._metadata_requests.res->dict()
A:sklearn.utils._metadata_requests.params->set(params)
A:sklearn.utils._metadata_requests.mmr->MethodMetadataRequest(owner=cls.__name__, method=method)
A:sklearn.utils._metadata_requests.existing->set(requests.keys())
A:sklearn.utils._metadata_requests.upcoming->set(mmr.requests.keys())
A:sklearn.utils._metadata_requests.output->dict()
A:sklearn.utils._metadata_requests.output[method]->MethodMetadataRequest(owner=cls.__name__, method=method)._serialize()
A:sklearn.utils._metadata_requests.RouterMappingPair->namedtuple('RouterMappingPair', ['mapping', 'router'])
A:sklearn.utils._metadata_requests.MethodPair->namedtuple('MethodPair', ['callee', 'caller'])
A:sklearn.utils._metadata_requests.result->list()
A:sklearn.utils._metadata_requests.routing->cls()
A:sklearn.utils._metadata_requests.self._route_mappings->dict()
A:sklearn.utils._metadata_requests.self._self_request->deepcopy(obj._get_metadata_request())
A:sklearn.utils._metadata_requests.method_mapping->deepcopy(method_mapping)
A:sklearn.utils._metadata_requests.self._route_mappings[name]->RouterMappingPair(mapping=method_mapping, router=get_routing_for_object(obj))
A:sklearn.utils._metadata_requests.param_names->self._get_param_names(method=method, return_alias=False, ignore_self_request=False)
A:sklearn.utils._metadata_requests.res[name]->dict()
A:sklearn.utils._metadata_requests.res[name][_callee]->router._route_params(params=params, method=_callee)
A:sklearn.utils._metadata_requests.self_params->set()
A:sklearn.utils._metadata_requests.res['$self_request']->self._self_request._serialize()
A:sklearn.utils._metadata_requests.res[name]['mapping']->route_mapping.mapping._serialize()
A:sklearn.utils._metadata_requests.res[name]['router']->route_mapping.router._serialize()
A:sklearn.utils._metadata_requests.requests->self._get_default_requests()
A:sklearn.utils._metadata_requests.method_metadata_request->getattr(requests, self.name)
A:sklearn.utils._metadata_requests.func.__signature__->inspect.Signature(params, return_annotation=owner)
A:sklearn.utils._metadata_requests.doc->REQUESTER_DOC.format(method=self.name)
A:sklearn.utils._metadata_requests.defaults->dict(sorted(defaults.items()))
A:sklearn.utils._metadata_requests.request_routing->get_routing_for_object(_obj)
A:sklearn.utils._metadata_requests.routed_params->get_routing_for_object(_obj).route_params(params=kwargs, caller=_method)
sklearn.utils._metadata_requests.MetadataRequest(self,owner)
sklearn.utils._metadata_requests.MetadataRequest.__getattr__(self,name)
sklearn.utils._metadata_requests.MetadataRequest.__init__(self,owner)
sklearn.utils._metadata_requests.MetadataRequest.__repr__(self)
sklearn.utils._metadata_requests.MetadataRequest.__str__(self)
sklearn.utils._metadata_requests.MetadataRequest._check_warnings(self,*,method,params)
sklearn.utils._metadata_requests.MetadataRequest._get_param_names(self,method,return_alias,ignore_self_request=None)
sklearn.utils._metadata_requests.MetadataRequest._route_params(self,*,method,params)
sklearn.utils._metadata_requests.MetadataRequest._serialize(self)
sklearn.utils._metadata_requests.MetadataRequest.consumes(self,method,params)
sklearn.utils._metadata_requests.MetadataRouter(self,owner)
sklearn.utils._metadata_requests.MetadataRouter.__init__(self,owner)
sklearn.utils._metadata_requests.MetadataRouter.__iter__(self)
sklearn.utils._metadata_requests.MetadataRouter.__repr__(self)
sklearn.utils._metadata_requests.MetadataRouter.__str__(self)
sklearn.utils._metadata_requests.MetadataRouter._get_param_names(self,*,method,return_alias,ignore_self_request)
sklearn.utils._metadata_requests.MetadataRouter._route_params(self,*,params,method)
sklearn.utils._metadata_requests.MetadataRouter._serialize(self)
sklearn.utils._metadata_requests.MetadataRouter.add(self,*,method_mapping,**objs)
sklearn.utils._metadata_requests.MetadataRouter.add_self_request(self,obj)
sklearn.utils._metadata_requests.MetadataRouter.consumes(self,method,params)
sklearn.utils._metadata_requests.MetadataRouter.route_params(self,*,caller,params)
sklearn.utils._metadata_requests.MetadataRouter.validate_metadata(self,*,method,params)
sklearn.utils._metadata_requests.MethodMapping(self)
sklearn.utils._metadata_requests.MethodMapping.__init__(self)
sklearn.utils._metadata_requests.MethodMapping.__iter__(self)
sklearn.utils._metadata_requests.MethodMapping.__repr__(self)
sklearn.utils._metadata_requests.MethodMapping.__str__(self)
sklearn.utils._metadata_requests.MethodMapping._serialize(self)
sklearn.utils._metadata_requests.MethodMapping.add(self,*,callee,caller)
sklearn.utils._metadata_requests.MethodMapping.from_str(cls,route)
sklearn.utils._metadata_requests.MethodMetadataRequest(self,owner,method,requests=None)
sklearn.utils._metadata_requests.MethodMetadataRequest.__init__(self,owner,method,requests=None)
sklearn.utils._metadata_requests.MethodMetadataRequest.__repr__(self)
sklearn.utils._metadata_requests.MethodMetadataRequest.__str__(self)
sklearn.utils._metadata_requests.MethodMetadataRequest._check_warnings(self,*,params)
sklearn.utils._metadata_requests.MethodMetadataRequest._consumes(self,params)
sklearn.utils._metadata_requests.MethodMetadataRequest._get_param_names(self,return_alias)
sklearn.utils._metadata_requests.MethodMetadataRequest._route_params(self,params)
sklearn.utils._metadata_requests.MethodMetadataRequest._serialize(self)
sklearn.utils._metadata_requests.MethodMetadataRequest.add_request(self,*,param,alias)
sklearn.utils._metadata_requests.MethodMetadataRequest.requests(self)
sklearn.utils._metadata_requests.RequestMethod(self,name,keys,validate_keys=True)
sklearn.utils._metadata_requests.RequestMethod.__get__(self,instance,owner)
sklearn.utils._metadata_requests.RequestMethod.__init__(self,name,keys,validate_keys=True)
sklearn.utils._metadata_requests._MetadataRequester
sklearn.utils._metadata_requests._MetadataRequester.__init_subclass__(cls,**kwargs)
sklearn.utils._metadata_requests._MetadataRequester._build_request_for_signature(cls,router,method)
sklearn.utils._metadata_requests._MetadataRequester._get_default_requests(cls)
sklearn.utils._metadata_requests._MetadataRequester._get_metadata_request(self)
sklearn.utils._metadata_requests._MetadataRequester.get_metadata_routing(self)
sklearn.utils._metadata_requests._RoutingNotSupportedMixin
sklearn.utils._metadata_requests._RoutingNotSupportedMixin.get_metadata_routing(self)
sklearn.utils._metadata_requests._raise_for_params(params,owner,method)
sklearn.utils._metadata_requests._raise_for_unsupported_routing(obj,method,**kwargs)
sklearn.utils._metadata_requests._routing_enabled()
sklearn.utils._metadata_requests.get_routing_for_object(obj=None)
sklearn.utils._metadata_requests.process_routing(_obj,_method,/,**kwargs)
sklearn.utils._metadata_requests.request_is_alias(item)
sklearn.utils._metadata_requests.request_is_valid(item)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/graph.py----------------------------------------
A:sklearn.utils.graph.graph->scipy.sparse.lil_matrix(graph)
A:sklearn.utils.graph.next_level->set()
A:sklearn.utils.graph.idx_i->numpy.flatnonzero(component_labels == i)
A:sklearn.utils.graph.idx_j->numpy.flatnonzero(component_labels == j)
A:sklearn.utils.graph.D->pairwise_distances(Xi, Xj, metric=metric, **kwargs)
A:sklearn.utils.graph.(ii, jj)->numpy.unravel_index(D.argmin(axis=None), D.shape)
sklearn.utils.graph._fix_connected_components(X,graph,n_connected_components,component_labels,mode='distance',metric='euclidean',**kwargs)
sklearn.utils.graph.single_source_shortest_path_length(graph,source,*,cutoff=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_encode.py----------------------------------------
A:sklearn.utils._encode.uniques->numpy.array(uniques, dtype=values.dtype)
A:sklearn.utils._encode.nan_idx->numpy.searchsorted(uniques, np.nan)
A:sklearn.utils._encode.counts[nan_idx]->numpy.sum(counts[nan_idx:])
A:sklearn.utils._encode.output_missing_values->MissingValues(nan=True, none=False)
A:sklearn.utils._encode.table->_nandict({val: i for (i, val) in enumerate(uniques)})
A:sklearn.utils._encode.uniques_set->set(known_values)
A:sklearn.utils._encode.(uniques_set, missing_values)->_extract_missing(uniques_set)
A:sklearn.utils._encode.types->sorted((t.__qualname__ for t in set((type(v) for v in values))))
A:sklearn.utils._encode.diff->list(diff)
A:sklearn.utils._encode.values_set->set(values)
A:sklearn.utils._encode.(values_set, missing_in_values)->_extract_missing(values_set)
A:sklearn.utils._encode.(uniques_set, missing_in_uniques)->_extract_missing(uniques_set)
A:sklearn.utils._encode.valid_mask->numpy.ones(len(values), dtype=bool)
A:sklearn.utils._encode.unique_values->numpy.unique(values)
A:sklearn.utils._encode.diff_is_nan->numpy.isnan(diff)
A:sklearn.utils._encode.is_nan->numpy.isnan(values)
A:sklearn.utils._encode.counter->_NaNCounter(values)
A:sklearn.utils._encode.output->numpy.zeros_like(uniques, dtype=np.int64)
A:sklearn.utils._encode.(unique_values, counts)->_unique_np(values, return_counts=True)
A:sklearn.utils._encode.uniques_in_values->numpy.isin(uniques, unique_values, assume_unique=True)
A:sklearn.utils._encode.unique_valid_indices->numpy.searchsorted(unique_values, uniques[uniques_in_values])
sklearn.utils._encode.MissingValues(NamedTuple)
sklearn.utils._encode.MissingValues.to_list(self)
sklearn.utils._encode._NaNCounter(self,items)
sklearn.utils._encode._NaNCounter.__init__(self,items)
sklearn.utils._encode._NaNCounter.__missing__(self,key)
sklearn.utils._encode._NaNCounter._generate_items(self,items)
sklearn.utils._encode._check_unknown(values,known_values,return_mask=False)
sklearn.utils._encode._encode(values,*,uniques,check_unknown=True)
sklearn.utils._encode._extract_missing(values)
sklearn.utils._encode._get_counts(values,uniques)
sklearn.utils._encode._map_to_integer(values,uniques)
sklearn.utils._encode._nandict(self,mapping)
sklearn.utils._encode._nandict.__init__(self,mapping)
sklearn.utils._encode._nandict.__missing__(self,key)
sklearn.utils._encode._unique(values,*,return_inverse=False,return_counts=False)
sklearn.utils._encode._unique_np(values,return_inverse=False,return_counts=False)
sklearn.utils._encode._unique_python(values,*,return_inverse,return_counts)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/extmath.py----------------------------------------
A:sklearn.utils.extmath.x->numpy.ravel(x, order='K')
A:sklearn.utils.extmath.X->xp.exp(X)
A:sklearn.utils.extmath.norms->xp.sqrt(norms)
A:sklearn.utils.extmath.(xp, _)->get_namespace(u, v)
A:sklearn.utils.extmath.(sign, ld)->xp.linalg.slogdet(A)
A:sklearn.utils.extmath.b_->numpy.rollaxis(b, -2)
A:sklearn.utils.extmath.b_2d->numpy.rollaxis(b, -2).reshape((b.shape[-2], -1))
A:sklearn.utils.extmath.ret->numpy.dot(a, b)
A:sklearn.utils.extmath.a_2d->numpy.asarray(a).reshape(-1, a.shape[-1])
A:sklearn.utils.extmath.(xp, is_array_api_compliant)->get_namespace(X)
A:sklearn.utils.extmath.random_state->check_random_state(random_state)
A:sklearn.utils.extmath.Q->randomized_range_finder(M, size=n_random, n_iter=n_iter, power_iteration_normalizer=power_iteration_normalizer, random_state=random_state)
A:sklearn.utils.extmath.qr_normalizer->partial(linalg.qr, mode='economic')
A:sklearn.utils.extmath.normalizer->partial(linalg.lu, permute_l=True)
A:sklearn.utils.extmath.(Q, _)->qr_normalizer(A @ Q)
A:sklearn.utils.extmath.(Uhat, s, Vt)->scipy.linalg.svd(B, full_matrices=False, lapack_driver=svd_lapack_driver)
A:sklearn.utils.extmath.(U, Vt)->svd_flip(U, Vt, u_based_decision=False)
A:sklearn.utils.extmath.(U, S, Vt)->randomized_svd(M, n_components=n_components, n_oversamples=n_oversamples, n_iter=n_iter, power_iteration_normalizer=power_iteration_normalizer, flip_sign=False, random_state=random_state)
A:sklearn.utils.extmath.diag_VtU->numpy.einsum('ji,ij->j', Vt[:n_components, :], U[:, :n_components])
A:sklearn.utils.extmath.signs->numpy.sign(u[range(u.shape[0]), max_abs_rows])
A:sklearn.utils.extmath.a->numpy.asarray(a)
A:sklearn.utils.extmath.w->numpy.full(a.shape, w, dtype=w.dtype)
A:sklearn.utils.extmath.scores->numpy.unique(np.ravel(a))
A:sklearn.utils.extmath.testshape->list(a.shape)
A:sklearn.utils.extmath.oldmostfreq->numpy.zeros(testshape)
A:sklearn.utils.extmath.oldcounts->numpy.maximum(counts, oldcounts)
A:sklearn.utils.extmath.template->numpy.zeros(a.shape)
A:sklearn.utils.extmath.counts->numpy.expand_dims(np.sum(template, axis), axis)
A:sklearn.utils.extmath.mostfrequent->numpy.where(counts > oldcounts, score, oldmostfreq)
A:sklearn.utils.extmath.ix->numpy.indices(shape)
A:sklearn.utils.extmath.dtype->numpy.result_type(*arrays)
A:sklearn.utils.extmath.out->numpy.cumsum(arr, axis=axis, dtype=np.float64)
A:sklearn.utils.extmath.device->getattr(u, 'device', None)
A:sklearn.utils.extmath.max_abs_u_cols->xp.argmax(xp.abs(u.T), axis=1)
A:sklearn.utils.extmath.shift->xp.arange(v.shape[0], device=device)
A:sklearn.utils.extmath.max_abs_v_rows->xp.argmax(xp.abs(v), axis=1)
A:sklearn.utils.extmath.max_prob->xp.reshape(xp.max(X, axis=1), (-1, 1))
A:sklearn.utils.extmath.sum_prob->xp.reshape(xp.sum(X, axis=1), (-1, 1))
A:sklearn.utils.extmath.min_->xp.exp(X).min()
A:sklearn.utils.extmath.result->op(x, *args, **kwargs)
A:sklearn.utils.extmath.X_nan_mask->numpy.isnan(X)
A:sklearn.utils.extmath.new_sum->_safe_accumulator_op(sum_op, X, axis=0)
A:sklearn.utils.extmath.new_sample_count->_safe_accumulator_op(np.sum, sample_weight[:, None] * ~X_nan_mask, axis=0)
A:sklearn.utils.extmath.correction->_safe_accumulator_op(sum_op, temp, axis=0)
A:sklearn.utils.extmath.new_unnormalized_variance->_safe_accumulator_op(sum_op, temp, axis=0)
A:sklearn.utils.extmath.max_abs_rows->numpy.argmax(np.abs(u), axis=1)
A:sklearn.utils.extmath.expected->numpy.sum(arr, axis=axis, dtype=np.float64)
A:sklearn.utils.extmath.mask->numpy.isnan(a)
A:sklearn.utils.extmath.weights->numpy.array(weights, copy=False)
sklearn.utils.extmath._deterministic_vector_sign_flip(u)
sklearn.utils.extmath._incremental_mean_and_var(X,last_mean,last_variance,last_sample_count,sample_weight=None)
sklearn.utils.extmath._nanaverage(a,weights=None)
sklearn.utils.extmath._randomized_eigsh(M,n_components,*,n_oversamples=10,n_iter='auto',power_iteration_normalizer='auto',selection='module',random_state=None)
sklearn.utils.extmath._safe_accumulator_op(op,x,*args,**kwargs)
sklearn.utils.extmath.cartesian(arrays,out=None)
sklearn.utils.extmath.density(w)
sklearn.utils.extmath.fast_logdet(A)
sklearn.utils.extmath.log_logistic(X,out=None)
sklearn.utils.extmath.make_nonnegative(X,min_value=0)
sklearn.utils.extmath.randomized_range_finder(A,*,size,n_iter,power_iteration_normalizer='auto',random_state=None)
sklearn.utils.extmath.randomized_svd(M,n_components,*,n_oversamples=10,n_iter='auto',power_iteration_normalizer='auto',transpose='auto',flip_sign=True,random_state=None,svd_lapack_driver='gesdd')
sklearn.utils.extmath.row_norms(X,squared=False)
sklearn.utils.extmath.safe_sparse_dot(a,b,*,dense_output=False)
sklearn.utils.extmath.softmax(X,copy=True)
sklearn.utils.extmath.squared_norm(x)
sklearn.utils.extmath.stable_cumsum(arr,axis=None,rtol=1e-05,atol=1e-08)
sklearn.utils.extmath.svd_flip(u,v,u_based_decision=True)
sklearn.utils.extmath.weighted_mode(a,w,*,axis=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/multiclass.py----------------------------------------
A:sklearn.utils.multiclass.(xp, is_array_api_compliant)->get_namespace(y)
A:sklearn.utils.multiclass.(xp, _)->get_namespace(y)
A:sklearn.utils.multiclass.ys_types->set((type_of_target(x) for x in ys))
A:sklearn.utils.multiclass.label_type->set((type_of_target(x) for x in ys)).pop()
A:sklearn.utils.multiclass._unique_labels->_FN_UNIQUE_LABELS.get(label_type, None)
A:sklearn.utils.multiclass.unique_ys->xp.concat([_unique_labels(y) for y in ys])
A:sklearn.utils.multiclass.ys_labels->set(chain.from_iterable(((i for i in _unique_labels(y)) for y in ys)))
A:sklearn.utils.multiclass.check_y_kwargs->dict(accept_sparse=True, allow_nd=True, force_all_finite=False, ensure_2d=False, ensure_min_samples=0, ensure_min_features=0)
A:sklearn.utils.multiclass.y->y.tocsc().tocsc()
A:sklearn.utils.multiclass.labels->xp.unique_values(y)
A:sklearn.utils.multiclass.y_type->type_of_target(y, input_name='y')
A:sklearn.utils.multiclass.clf.classes_->unique_labels(classes)
A:sklearn.utils.multiclass.sample_weight->numpy.asarray(sample_weight)
A:sklearn.utils.multiclass.y_nnz->numpy.diff(y.indptr)
A:sklearn.utils.multiclass.(classes_k, y_k)->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.utils.multiclass.class_prior_k->numpy.bincount(y_k, weights=sample_weight)
A:sklearn.utils.multiclass.classes_k->numpy.insert(classes_k, 0, 0)
A:sklearn.utils.multiclass.votes->numpy.zeros((n_samples, n_classes))
A:sklearn.utils.multiclass.sum_of_confidences->numpy.zeros((n_samples, n_classes))
sklearn.utils.multiclass._check_partial_fit_first_call(clf,classes=None)
sklearn.utils.multiclass._is_integral_float(y)
sklearn.utils.multiclass._ovr_decision_function(predictions,confidences,n_classes)
sklearn.utils.multiclass._unique_indicator(y)
sklearn.utils.multiclass._unique_multiclass(y)
sklearn.utils.multiclass.check_classification_targets(y)
sklearn.utils.multiclass.class_distribution(y,sample_weight=None)
sklearn.utils.multiclass.is_multilabel(y)
sklearn.utils.multiclass.type_of_target(y,input_name='')
sklearn.utils.multiclass.unique_labels(*ys)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/parallel.py----------------------------------------
A:sklearn.utils.parallel.config->getattr(self, 'config', None)
sklearn.utils.parallel.Parallel(self,iterable)
sklearn.utils.parallel.Parallel.__call__(self,iterable)
sklearn.utils.parallel._FuncWrapper(self,function)
sklearn.utils.parallel._FuncWrapper.__init__(self,function)
sklearn.utils.parallel._FuncWrapper.with_config(self,config)
sklearn.utils.parallel._with_config(delayed_func,config)
sklearn.utils.parallel.delayed(function)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_response.py----------------------------------------
A:sklearn.utils._response.prediction_method->_check_response_method(estimator, response_method)
A:sklearn.utils._response.target_type->type_of_target(classes)
A:sklearn.utils._response.y_pred->_process_decision_function(y_pred=y_pred, target_type=target_type, classes=classes, pos_label=pos_label)
sklearn.utils._response._get_response_values(estimator,X,response_method,pos_label=None,return_response_method_used=False)
sklearn.utils._response._get_response_values_binary(estimator,X,response_method,pos_label=None)
sklearn.utils._response._process_decision_function(*,y_pred,target_type,classes,pos_label)
sklearn.utils._response._process_predict_proba(*,y_pred,target_type,classes,pos_label)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/deprecation.py----------------------------------------
A:sklearn.utils.deprecation.closures->getattr(func, '__closure__', [])
sklearn.utils.deprecated(self,extra='')
sklearn.utils.deprecated._decorate_class(self,cls)
sklearn.utils.deprecated._decorate_fun(self,fun)
sklearn.utils.deprecated._decorate_property(self,prop)
sklearn.utils.deprecation._is_deprecated(func)
sklearn.utils.deprecation.deprecated(self,extra='')
sklearn.utils.deprecation.deprecated.__init__(self,extra='')
sklearn.utils.deprecation.deprecated._decorate_class(self,cls)
sklearn.utils.deprecation.deprecated._decorate_fun(self,fun)
sklearn.utils.deprecation.deprecated._decorate_property(self,prop)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_mask.py----------------------------------------
A:sklearn.utils._mask.Xt->_get_dense_mask(X.data, value_to_mask)
A:sklearn.utils._mask.Xt_sparse->sparse_constructor((Xt, X.indices.copy(), X.indptr.copy()), shape=X.shape, dtype=bool)
sklearn.utils._mask._get_dense_mask(X,value_to_mask)
sklearn.utils._mask._get_mask(X,value_to_mask)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_mocking.py----------------------------------------
A:sklearn.utils._mocking.self.iloc->ArraySlicingWrapper(array)
A:sklearn.utils._mocking.checked_X->self.check_X(X, **params)
A:sklearn.utils._mocking.checked_y->self.check_y(y, **params)
A:sklearn.utils._mocking.(X, y)->self._check_X_y(X)
A:sklearn.utils._mocking.self.classes_->numpy.unique(y)
A:sklearn.utils._mocking.proba->numpy.zeros((_num_samples(X), len(self.classes_)))
A:sklearn.utils._mocking.decision->numpy.zeros((_num_samples(X), len(self.classes_)))
A:sklearn.utils._mocking.CheckingClassifier.set_fit_request->RequestMethod(name='fit', keys=[], validate_keys=False)
sklearn.utils._mocking.ArraySlicingWrapper(self,array)
sklearn.utils._mocking.ArraySlicingWrapper.__getitem__(self,aslice)
sklearn.utils._mocking.ArraySlicingWrapper.__init__(self,array)
sklearn.utils._mocking.CheckingClassifier(self,*,check_y=None,check_y_params=None,check_X=None,check_X_params=None,methods_to_check='all',foo_param=0,expected_sample_weight=None,expected_fit_params=None)
sklearn.utils._mocking.CheckingClassifier.__init__(self,*,check_y=None,check_y_params=None,check_X=None,check_X_params=None,methods_to_check='all',foo_param=0,expected_sample_weight=None,expected_fit_params=None)
sklearn.utils._mocking.CheckingClassifier._check_X_y(self,X,y=None,should_be_fitted=True)
sklearn.utils._mocking.CheckingClassifier._more_tags(self)
sklearn.utils._mocking.CheckingClassifier.decision_function(self,X)
sklearn.utils._mocking.CheckingClassifier.fit(self,X,y,sample_weight=None,**fit_params)
sklearn.utils._mocking.CheckingClassifier.predict(self,X)
sklearn.utils._mocking.CheckingClassifier.predict_proba(self,X)
sklearn.utils._mocking.CheckingClassifier.score(self,X=None,Y=None)
sklearn.utils._mocking.MockDataFrame(self,array)
sklearn.utils._mocking.MockDataFrame.__array__(self,dtype=None)
sklearn.utils._mocking.MockDataFrame.__eq__(self,other)
sklearn.utils._mocking.MockDataFrame.__init__(self,array)
sklearn.utils._mocking.MockDataFrame.__len__(self)
sklearn.utils._mocking.MockDataFrame.__ne__(self,other)
sklearn.utils._mocking.MockDataFrame.take(self,indices,axis=0)
sklearn.utils._mocking.NoSampleWeightWrapper(self,est=None)
sklearn.utils._mocking.NoSampleWeightWrapper.__init__(self,est=None)
sklearn.utils._mocking.NoSampleWeightWrapper._more_tags(self)
sklearn.utils._mocking.NoSampleWeightWrapper.fit(self,X,y)
sklearn.utils._mocking.NoSampleWeightWrapper.predict(self,X)
sklearn.utils._mocking.NoSampleWeightWrapper.predict_proba(self,X)
sklearn.utils._mocking._MockEstimatorOnOffPrediction(self,response_methods=None)
sklearn.utils._mocking._MockEstimatorOnOffPrediction.__init__(self,response_methods=None)
sklearn.utils._mocking._MockEstimatorOnOffPrediction.decision_function(self,X)
sklearn.utils._mocking._MockEstimatorOnOffPrediction.fit(self,X,y)
sklearn.utils._mocking._MockEstimatorOnOffPrediction.predict(self,X)
sklearn.utils._mocking._MockEstimatorOnOffPrediction.predict_proba(self,X)
sklearn.utils._mocking._check_response(method)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/class_weight.py----------------------------------------
A:sklearn.utils.class_weight.weight->numpy.ones(classes.shape[0], dtype=np.float64, order='C')
A:sklearn.utils.class_weight.le->LabelEncoder()
A:sklearn.utils.class_weight.y_ind->LabelEncoder().fit_transform(y)
A:sklearn.utils.class_weight.unweighted_classes_user_friendly_str->numpy.array(unweighted_classes).tolist()
A:sklearn.utils.class_weight.y->numpy.reshape(y, (-1, 1))
A:sklearn.utils.class_weight.y_full->y[:, [k]].toarray().flatten()
A:sklearn.utils.class_weight.classes_full->numpy.unique(y_full)
A:sklearn.utils.class_weight.classes_subsample->numpy.unique(y_subsample)
A:sklearn.utils.class_weight.weight_k->compute_class_weight(class_weight_k, classes=classes_full, y=y_full)
A:sklearn.utils.class_weight.expanded_class_weight->numpy.prod(expanded_class_weight, axis=0, dtype=np.float64)
sklearn.utils.class_weight.compute_class_weight(class_weight,*,classes,y)
sklearn.utils.class_weight.compute_sample_weight(class_weight,y,*,indices=None)
sklearn.utils.compute_class_weight(class_weight,*,classes,y)
sklearn.utils.compute_sample_weight(class_weight,y,*,indices=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_testing.py----------------------------------------
A:sklearn.utils._testing._dummy->TestCase('__init__')
A:sklearn.utils._testing.error_message->str(e)
A:sklearn.utils._testing.names->' or '.join((e.__name__ for e in exceptions))
A:sklearn.utils._testing.rtol->max(rtols)
A:sklearn.utils._testing.x->x.tocsr().tocsr()
A:sklearn.utils._testing.y->y.tocsr().tocsr()
A:sklearn.utils._testing.skip_if_32bit->pytest.mark.skipif(_IS_32BIT, reason='skipped on 32bit platforms')
A:sklearn.utils._testing.fails_if_pypy->pytest.mark.xfail(IS_PYPY, reason='not compatible with PyPy')
A:sklearn.utils._testing.fails_if_unstable_openblas->pytest.mark.xfail(_in_unstable_openblas_configuration(), reason='OpenBLAS is unstable for this configuration')
A:sklearn.utils._testing.skip_if_no_parallel->pytest.mark.skipif(not joblib.parallel.mp, reason='joblib is in serial mode')
A:sklearn.utils._testing.skip_if_array_api_compat_not_configured->pytest.mark.skipif(not ARRAY_API_COMPAT_FUNCTIONAL, reason='requires array_api_compat installed and a new enough version of NumPy')
A:sklearn.utils._testing.if_safe_multiprocessing_with_blas->pytest.mark.skipif(sys.platform == 'darwin', reason='Possible multi-process bug with some BLAS')
A:sklearn.utils._testing.(data_read_only, self.temp_folder)->create_memmap_backed_data(self.data, mmap_mode=self.mmap_mode, return_folder=True)
A:sklearn.utils._testing.temp_folder->tempfile.mkdtemp(prefix='sklearn_testing_')
A:sklearn.utils._testing.filename->os.path.join(temp_folder, 'data.pkl')
A:sklearn.utils._testing.memmap_backed_data->joblib.load(filename, mmap_mode=mmap_mode)
A:sklearn.utils._testing.module->inspect.getmodule(func)
A:sklearn.utils._testing.func_name->_get_func_name(func)
A:sklearn.utils._testing.param_signature->list(filter(lambda x: x not in ignore, _get_args(func)))
A:sklearn.utils._testing.doc->numpydoc.docscrape.FunctionDoc(func)
A:sklearn.utils._testing.param_docs->list(filter(lambda x: x not in ignore, param_docs))
A:sklearn.utils._testing.param_docs_formatted->pprint.pformat(param_docs).splitlines()
A:sklearn.utils._testing.param_signature_formatted->pprint.pformat(param_signature).splitlines()
A:sklearn.utils._testing.(fd, source_file)->tempfile.mkstemp(suffix='_src_test_sklearn.py')
A:sklearn.utils._testing.cwd->os.path.normpath(op.join(op.dirname(sklearn.__file__), '..'))
A:sklearn.utils._testing.env->os.environ.copy()
A:sklearn.utils._testing.env['PYTHONPATH']->os.pathsep.join([cwd, env['PYTHONPATH']])
A:sklearn.utils._testing.coverage_rc->os.environ.get('COVERAGE_PROCESS_START')
A:sklearn.utils._testing.out->check_output(cmd, **kwargs)
A:sklearn.utils._testing.pd->pytest.importorskip('pandas', minversion=minversion)
A:sklearn.utils._testing.result->result.with_columns(pl.col(col_name).cast(pl.Categorical)).with_columns(pl.col(col_name).cast(pl.Categorical))
A:sklearn.utils._testing.result[col_name]->result[col_name].astype('category').astype('category')
A:sklearn.utils._testing.pa->pytest.importorskip('pyarrow', minversion=minversion)
A:sklearn.utils._testing.array->numpy.asarray(container)
A:sklearn.utils._testing.pl->pytest.importorskip('polars', minversion=minversion)
A:sklearn.utils._testing.(X, y)->check_X_y(X, y)
A:sklearn.utils._testing.(self.classes_, counts)->numpy.unique(y, return_counts=True)
A:sklearn.utils._testing.self._most_frequent_class_idx->counts.argmax()
A:sklearn.utils._testing.X->check_array(X)
A:sklearn.utils._testing.y_proba->self.predict_proba(X)
A:sklearn.utils._testing.y_pred->self.predict_proba(X).argmax(axis=1)
A:sklearn.utils._testing.self._mean->numpy.mean(y)
A:sklearn.utils._testing.array_mod->importlib.import_module(array_namespace)
A:sklearn.utils._testing.xp->array_api_compat.get_namespace(array_mod.asarray(1))
sklearn.utils._testing.MinimalClassifier(self,param=None)
sklearn.utils._testing.MinimalClassifier.__init__(self,param=None)
sklearn.utils._testing.MinimalClassifier.fit(self,X,y)
sklearn.utils._testing.MinimalClassifier.get_params(self,deep=True)
sklearn.utils._testing.MinimalClassifier.predict(self,X)
sklearn.utils._testing.MinimalClassifier.predict_proba(self,X)
sklearn.utils._testing.MinimalClassifier.score(self,X,y)
sklearn.utils._testing.MinimalClassifier.set_params(self,**params)
sklearn.utils._testing.MinimalRegressor(self,param=None)
sklearn.utils._testing.MinimalRegressor.__init__(self,param=None)
sklearn.utils._testing.MinimalRegressor.fit(self,X,y)
sklearn.utils._testing.MinimalRegressor.get_params(self,deep=True)
sklearn.utils._testing.MinimalRegressor.predict(self,X)
sklearn.utils._testing.MinimalRegressor.score(self,X,y)
sklearn.utils._testing.MinimalRegressor.set_params(self,**params)
sklearn.utils._testing.MinimalTransformer(self,param=None)
sklearn.utils._testing.MinimalTransformer.__init__(self,param=None)
sklearn.utils._testing.MinimalTransformer.fit(self,X,y=None)
sklearn.utils._testing.MinimalTransformer.fit_transform(self,X,y=None)
sklearn.utils._testing.MinimalTransformer.get_params(self,deep=True)
sklearn.utils._testing.MinimalTransformer.set_params(self,**params)
sklearn.utils._testing.MinimalTransformer.transform(self,X,y=None)
sklearn.utils._testing.TempMemmap(self,data,mmap_mode='r')
sklearn.utils._testing.TempMemmap.__enter__(self)
sklearn.utils._testing.TempMemmap.__exit__(self,exc_type,exc_val,exc_tb)
sklearn.utils._testing.TempMemmap.__init__(self,data,mmap_mode='r')
sklearn.utils._testing._IgnoreWarnings(self,category)
sklearn.utils._testing._IgnoreWarnings.__enter__(self)
sklearn.utils._testing._IgnoreWarnings.__exit__(self,*exc_info)
sklearn.utils._testing._IgnoreWarnings.__init__(self,category)
sklearn.utils._testing._IgnoreWarnings.__repr__(self)
sklearn.utils._testing._Raises(self,expected_exc_type,match,may_pass,err_msg)
sklearn.utils._testing._Raises.__exit__(self,exc_type,exc_value,_)
sklearn.utils._testing._Raises.__init__(self,expected_exc_type,match,may_pass,err_msg)
sklearn.utils._testing._array_api_for_tests(array_namespace,device)
sklearn.utils._testing._convert_container(container,constructor_name,columns_name=None,dtype=None,minversion=None,categorical_feature_names=None)
sklearn.utils._testing._delete_folder(folder_path,warn=False)
sklearn.utils._testing._get_args(function,varargs=False)
sklearn.utils._testing._get_func_name(func)
sklearn.utils._testing.assert_allclose(actual,desired,rtol=None,atol=0.0,equal_nan=True,err_msg='',verbose=True)
sklearn.utils._testing.assert_allclose_dense_sparse(x,y,rtol=1e-07,atol=1e-09,err_msg='')
sklearn.utils._testing.assert_raise_message(exceptions,message,function,*args,**kwargs)
sklearn.utils._testing.assert_run_python_script(source_code,timeout=60)
sklearn.utils._testing.check_docstring_parameters(func,doc=None,ignore=None)
sklearn.utils._testing.check_skip_network()
sklearn.utils._testing.create_memmap_backed_data(data,mmap_mode='r',return_folder=False)
sklearn.utils._testing.ignore_warnings(obj=None,category=Warning)
sklearn.utils._testing.raises(expected_exc_type,match=None,may_pass=False,err_msg=None)
sklearn.utils._testing.set_random_state(estimator,random_state=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/random.py----------------------------------------
A:sklearn.utils.random.data->array.array('i')
A:sklearn.utils.random.indices->array.array('i')
A:sklearn.utils.random.indptr->array.array('i', [0])
A:sklearn.utils.random.classes[j]->numpy.insert(classes[j], 0, 0)
A:sklearn.utils.random.class_prob_j->numpy.insert(class_prob_j, 0, 0.0)
A:sklearn.utils.random.rng->check_random_state(random_state)
A:sklearn.utils.random.index_class_0->numpy.flatnonzero(classes[j] == 0).item()
A:sklearn.utils.random.nnz->int(n_samples * p_nonzero)
A:sklearn.utils.random.ind_sample->sample_without_replacement(n_population=n_samples, n_samples=nnz, random_state=random_state)
A:sklearn.utils.random.classes_ind->numpy.searchsorted(class_probability_nz_norm.cumsum(), rng.uniform(size=nnz))
sklearn.utils.random._random_choice_csc(n_samples,classes,class_probability=None,random_state=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/sparsefuncs.py----------------------------------------
A:sklearn.utils.sparsefuncs.last_n->numpy.full(last_mean.shape, last_n, dtype=last_mean.dtype)
A:sklearn.utils.sparsefuncs.weights->numpy.repeat(sample_weight, np.diff(X.indptr))
A:sklearn.utils.sparsefuncs.X.indices->numpy.concatenate([X.indices[:m_start], X.indices[n_start:n_stop], X.indices[m_stop:n_start], X.indices[m_start:m_stop], X.indices[n_stop:]])
A:sklearn.utils.sparsefuncs.X.data->numpy.concatenate([X.data[:m_start], X.data[n_start:n_stop], X.data[m_stop:n_start], X.data[m_start:m_stop], X.data[n_stop:]])
A:sklearn.utils.sparsefuncs.out->numpy.diff(X.indptr)
A:sklearn.utils.sparsefuncs.n_negative->numpy.count_nonzero(data < 0)
A:sklearn.utils.sparsefuncs.(middle, is_odd)->divmod(n_elems, 2)
A:sklearn.utils.sparsefuncs.median->numpy.zeros(n_features)
A:sklearn.utils.sparsefuncs.data->numpy.copy(X.data[start:end])
A:sklearn.utils.sparsefuncs.median[f_ind]->_get_median(data, nz)
sklearn.utils.sparsefuncs._get_elem_at_rank(rank,data,n_negative,n_zeros)
sklearn.utils.sparsefuncs._get_median(data,n_zeros)
sklearn.utils.sparsefuncs._implicit_column_offset(X,offset)
sklearn.utils.sparsefuncs._raise_error_wrong_axis(axis)
sklearn.utils.sparsefuncs._raise_typeerror(X)
sklearn.utils.sparsefuncs.count_nonzero(X,axis=None,sample_weight=None)
sklearn.utils.sparsefuncs.csc_median_axis_0(X)
sklearn.utils.sparsefuncs.incr_mean_variance_axis(X,*,axis,last_mean,last_var,last_n,weights=None)
sklearn.utils.sparsefuncs.inplace_column_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_csr_column_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_csr_row_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_row_scale(X,scale)
sklearn.utils.sparsefuncs.inplace_swap_column(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row_csc(X,m,n)
sklearn.utils.sparsefuncs.inplace_swap_row_csr(X,m,n)
sklearn.utils.sparsefuncs.mean_variance_axis(X,axis,weights=None,return_sum_weights=False)
sklearn.utils.sparsefuncs.min_max_axis(X,axis,ignore_nan=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_pprint.py----------------------------------------
A:sklearn.utils._pprint.params->object.get_params(deep=False)
A:sklearn.utils._pprint.init_func->getattr(estimator.__init__, 'deprecated_original', estimator.__init__)
A:sklearn.utils._pprint.it->iter(items)
A:sklearn.utils._pprint.next_ent->next(it)
A:sklearn.utils._pprint.krepr->krepr.strip("'").strip("'")
A:sklearn.utils._pprint.vrepr->self._repr(v, context, level)
A:sklearn.utils._pprint.rep->repr(object)
A:sklearn.utils._pprint._dispatch->pprint.PrettyPrinter._dispatch.copy()
A:sklearn.utils._pprint.typ->type(object)
A:sklearn.utils._pprint.r->getattr(typ, '__repr__', None)
A:sklearn.utils._pprint.objid->id(object)
A:sklearn.utils._pprint.items->sorted(params.items(), key=pprint._safe_tuple)
A:sklearn.utils._pprint.(krepr, kreadable, krecur)->saferepr(k, context, maxlevels, level, changed_only=changed_only)
A:sklearn.utils._pprint.(vrepr, vreadable, vrecur)->saferepr(v, context, maxlevels, level, changed_only=changed_only)
A:sklearn.utils._pprint.(orepr, oreadable, orecur)->_safe_repr(o, context, maxlevels, level, changed_only=changed_only)
sklearn.utils._pprint.KeyValTuple(tuple)
sklearn.utils._pprint.KeyValTuple.__repr__(self)
sklearn.utils._pprint.KeyValTupleParam(KeyValTuple)
sklearn.utils._pprint._EstimatorPrettyPrinter(self,indent=1,width=80,depth=None,stream=None,*,compact=False,indent_at_name=True,n_max_elements_to_show=None)
sklearn.utils._pprint._EstimatorPrettyPrinter.__init__(self,indent=1,width=80,depth=None,stream=None,*,compact=False,indent_at_name=True,n_max_elements_to_show=None)
sklearn.utils._pprint._EstimatorPrettyPrinter._format_dict_items(self,items,stream,indent,allowance,context,level)
sklearn.utils._pprint._EstimatorPrettyPrinter._format_items(self,items,stream,indent,allowance,context,level)
sklearn.utils._pprint._EstimatorPrettyPrinter._format_params(self,items,stream,indent,allowance,context,level)
sklearn.utils._pprint._EstimatorPrettyPrinter._format_params_or_dict_items(self,object,stream,indent,allowance,context,level,is_dict)
sklearn.utils._pprint._EstimatorPrettyPrinter._pprint_estimator(self,object,stream,indent,allowance,context,level)
sklearn.utils._pprint._EstimatorPrettyPrinter._pprint_key_val_tuple(self,object,stream,indent,allowance,context,level)
sklearn.utils._pprint._EstimatorPrettyPrinter.format(self,object,context,maxlevels,level)
sklearn.utils._pprint._changed_params(estimator)
sklearn.utils._pprint._safe_repr(object,context,maxlevels,level,changed_only=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_bunch.py----------------------------------------
sklearn.utils.Bunch(self,**kwargs)
sklearn.utils.Bunch.__dir__(self)
sklearn.utils.Bunch.__getattr__(self,key)
sklearn.utils.Bunch.__getitem__(self,key)
sklearn.utils.Bunch.__setattr__(self,key,value)
sklearn.utils.Bunch.__setstate__(self,state)
sklearn.utils.Bunch._set_deprecated(self,value,*,new_key,deprecated_key,warning_message)
sklearn.utils._bunch.Bunch(self,**kwargs)
sklearn.utils._bunch.Bunch.__dir__(self)
sklearn.utils._bunch.Bunch.__getattr__(self,key)
sklearn.utils._bunch.Bunch.__getitem__(self,key)
sklearn.utils._bunch.Bunch.__init__(self,**kwargs)
sklearn.utils._bunch.Bunch.__setattr__(self,key,value)
sklearn.utils._bunch.Bunch.__setstate__(self,state)
sklearn.utils._bunch.Bunch._set_deprecated(self,value,*,new_key,deprecated_key,warning_message)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/_arpack.py----------------------------------------
A:sklearn.utils._arpack.random_state->check_random_state(random_state)
A:sklearn.utils._arpack.v0->check_random_state(random_state).uniform(-1, 1, size)
sklearn.utils._arpack._init_arpack_v0(size,random_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/estimator_checks.py----------------------------------------
A:sklearn.utils.estimator_checks.tags->transformer_orig._get_tags()
A:sklearn.utils.estimator_checks.estimator->clone(estimator_orig)
A:sklearn.utils.estimator_checks.rng->numpy.random.RandomState(0)
A:sklearn.utils.estimator_checks.X->_enforce_estimator_tags_X(transformer_orig, X)
A:sklearn.utils.estimator_checks.y->_enforce_estimator_tags_y(transformer_orig, y)
A:sklearn.utils.estimator_checks.kwstring->','.join(['{}={}'.format(k, v) for (k, v) in obj.keywords.items()])
A:sklearn.utils.estimator_checks.required_parameters->getattr(Estimator, '_required_parameters', [])
A:sklearn.utils.estimator_checks.(should_be_marked, reason)->_should_be_skipped_or_marked(estimator, check)
A:sklearn.utils.estimator_checks.(should_be_skipped, reason)->_should_be_skipped_or_marked(estimator, check)
A:sklearn.utils.estimator_checks.check->_maybe_skip(estimator, check)
A:sklearn.utils.estimator_checks.(X, y)->make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]], random_state=0, n_features=2, cluster_std=0.1)
A:sklearn.utils.estimator_checks.params->clone(estimator_orig).get_params()
A:sklearn.utils.estimator_checks.estimator.n_clusters->min(estimator.n_clusters, 2)
A:sklearn.utils.estimator_checks.self.data->numpy.asarray(data)
A:sklearn.utils.estimator_checks.metric->getattr(estimator, 'metric', None)
A:sklearn.utils.estimator_checks.X_coo->scipy.sparse.csr_matrix(X).asformat('coo')
A:sklearn.utils.estimator_checks.X_coo.row->scipy.sparse.csr_matrix(X).asformat('coo').row.astype('int64')
A:sklearn.utils.estimator_checks.X_coo.col->scipy.sparse.csr_matrix(X).asformat('coo').col.astype('int64')
A:sklearn.utils.estimator_checks.X.indices->_enforce_estimator_tags_X(transformer_orig, X).indices.astype('int64')
A:sklearn.utils.estimator_checks.X.indptr->_enforce_estimator_tags_X(transformer_orig, X).indptr.astype('int64')
A:sklearn.utils.estimator_checks.xp->_array_api_for_tests(array_namespace, device)
A:sklearn.utils.estimator_checks.est->pickle.loads(pickle.dumps(est))
A:sklearn.utils.estimator_checks.X_xp->_array_api_for_tests(array_namespace, device).asarray(X, device=device)
A:sklearn.utils.estimator_checks.y_xp->_array_api_for_tests(array_namespace, device).asarray(y, device=device)
A:sklearn.utils.estimator_checks.est_xp->clone(est)
A:sklearn.utils.estimator_checks.est_xp_param->getattr(est_xp, key)
A:sklearn.utils.estimator_checks.est_xp_param_np->_convert_to_numpy(est_xp_param, xp=xp)
A:sklearn.utils.estimator_checks.method->getattr(est, method_name, None)
A:sklearn.utils.estimator_checks.result->dict()
A:sklearn.utils.estimator_checks.result_xp->getattr(est_xp, method_name)(X_xp)
A:sklearn.utils.estimator_checks.result_xp_np->_convert_to_numpy(result_xp, xp=xp)
A:sklearn.utils.estimator_checks.inverse_result->pickle.loads(pickle.dumps(est)).inverse_transform(result)
A:sklearn.utils.estimator_checks.invese_result_xp->clone(est).inverse_transform(result_xp)
A:sklearn.utils.estimator_checks.invese_result_xp_np->_convert_to_numpy(invese_result_xp, xp=xp)
A:sklearn.utils.estimator_checks.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.estimator_checks.pred->pickle.loads(pickle.dumps(est)).predict(X)
A:sklearn.utils.estimator_checks.probs->clone(estimator_orig).predict_proba(X)
A:sklearn.utils.estimator_checks.weights->_NotAnArray([1] * 12)
A:sklearn.utils.estimator_checks.rnd->numpy.random.RandomState(0)
A:sklearn.utils.estimator_checks.estimator1->clone(estimator_orig)
A:sklearn.utils.estimator_checks.estimator2->clone(estimator_orig)
A:sklearn.utils.estimator_checks.X1->numpy.array([[1, 3], [1, 3], [1, 3], [1, 3], [2, 1], [2, 1], [2, 1], [2, 1], [3, 3], [3, 3], [3, 3], [3, 3], [4, 1], [4, 1], [4, 1], [4, 1]], dtype=np.float64)
A:sklearn.utils.estimator_checks.y1->_enforce_estimator_tags_y(estimator1, y1)
A:sklearn.utils.estimator_checks.sw2->numpy.ones(shape=len(y1) * 2)
A:sklearn.utils.estimator_checks.X2->numpy.vstack([X1, X1 + 1])
A:sklearn.utils.estimator_checks.y2->_enforce_estimator_tags_y(estimator2, y2)
A:sklearn.utils.estimator_checks.(X2, y2, sw2)->shuffle(X2, y2, sw2, random_state=0)
A:sklearn.utils.estimator_checks.X_pred1->clone(clusterer_orig).fit(X).predict(X)
A:sklearn.utils.estimator_checks.X_pred2->clone(clusterer_orig).fit(X).predict(X)
A:sklearn.utils.estimator_checks.sample_weight_original->numpy.ones(y.shape[0])
A:sklearn.utils.estimator_checks.sample_weight_fit->numpy.ones(y.shape[0]).copy()
A:sklearn.utils.estimator_checks.dict_before->clone(estimator_orig).__dict__.copy()
A:sklearn.utils.estimator_checks.dict_before_fit->clone(estimator_orig).__dict__.copy()
A:sklearn.utils.estimator_checks.result_full->result_full.toarray().toarray()
A:sklearn.utils.estimator_checks.result_by_batch->list(map(lambda x: x[0], result_by_batch))
A:sklearn.utils.estimator_checks.msg->'{method} of {name} is not invariant when applied to a datasetwith different sample order.'.format(method=method, name=name)
A:sklearn.utils.estimator_checks.(result_full, result_by_batch)->_apply_on_subsets(getattr(estimator, method), X)
A:sklearn.utils.estimator_checks.idx->numpy.random.permutation(X.shape[0])
A:sklearn.utils.estimator_checks.this_X->_NotAnArray(X)
A:sklearn.utils.estimator_checks.this_y->_NotAnArray(np.asarray(y))
A:sklearn.utils.estimator_checks.transformer->clone(transformer_orig)
A:sklearn.utils.estimator_checks.X_trans->clone(transformer_orig).fit_transform(data, y)
A:sklearn.utils.estimator_checks.y_->_enforce_estimator_tags_y(estimator, y_)
A:sklearn.utils.estimator_checks.transformer_clone->clone(transformer)
A:sklearn.utils.estimator_checks.X_pred->clone(transformer).fit_transform(X, y=y_)
A:sklearn.utils.estimator_checks.X_pred3->clone(transformer_orig).fit_transform(X, y=y_)
A:sklearn.utils.estimator_checks.pipeline->make_pipeline(estimator)
A:sklearn.utils.estimator_checks.func->getattr(estimator, func_name, None)
A:sklearn.utils.estimator_checks.func_pipeline->getattr(pipeline, func_name)
A:sklearn.utils.estimator_checks.result_pipe->func_pipeline(X, y)
A:sklearn.utils.estimator_checks.X_train_32->_enforce_estimator_tags_X(estimator_orig, X_train_32)
A:sklearn.utils.estimator_checks.X_train_64->_enforce_estimator_tags_X(estimator_orig, X_train_32).astype(np.float64)
A:sklearn.utils.estimator_checks.X_train_int_64->_enforce_estimator_tags_X(estimator_orig, X_train_32).astype(np.int64)
A:sklearn.utils.estimator_checks.X_train_int_32->_enforce_estimator_tags_X(estimator_orig, X_train_32).astype(np.int32)
A:sklearn.utils.estimator_checks.X_cast->_enforce_estimator_tags_X(transformer_orig, X).astype(dtype)
A:sklearn.utils.estimator_checks.X_trans1->clone(transformer_orig).fit_transform(X_cast, y)
A:sklearn.utils.estimator_checks.X_trans2->clone(transformer_orig).fit(X_cast, y).transform(X_cast)
A:sklearn.utils.estimator_checks.e->clone(estimator_orig)
A:sklearn.utils.estimator_checks.X_zero_samples->numpy.empty(0).reshape(0, 3)
A:sklearn.utils.estimator_checks.X_zero_features->numpy.empty(0).reshape(12, 0)
A:sklearn.utils.estimator_checks.X_train_finite->_enforce_estimator_tags_X(estimator_orig, rnd.uniform(size=(10, 3)))
A:sklearn.utils.estimator_checks.X_train_nan->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.X_train_inf->numpy.random.RandomState(0).uniform(size=(10, 3))
A:sklearn.utils.estimator_checks.mask->numpy.random.RandomState(0).choice(X.size, 10, replace=False)
A:sklearn.utils.estimator_checks.unpickled_estimator->pickle.loads(pickled_estimator)
A:sklearn.utils.estimator_checks.pickled_estimator->pickle.dumps(estimator)
A:sklearn.utils.estimator_checks.result[method]->getattr(estimator, method)(X)
A:sklearn.utils.estimator_checks.unpickled_result->getattr(unpickled_estimator, method)(X)
A:sklearn.utils.estimator_checks.classes->numpy.unique(y)
A:sklearn.utils.estimator_checks.y_pred->clone(estimator_orig).fit_predict(X)
A:sklearn.utils.estimator_checks.decision->clone(estimator_orig).decision_function(X)
A:sklearn.utils.estimator_checks.dec_pred->(decision.ravel() > 0).astype(int)
A:sklearn.utils.estimator_checks.y_prob->Classifier().predict_proba(X)
A:sklearn.utils.estimator_checks.y_decision->clone(estimator_orig).decision_function(X)
A:sklearn.utils.estimator_checks.clusterer->clone(clusterer_orig)
A:sklearn.utils.estimator_checks.X_noise->numpy.concatenate([X, rng.uniform(low=-3, high=3, size=(5, 2))])
A:sklearn.utils.estimator_checks.(X, y, X_noise)->create_memmap_backed_data([X, y, X_noise])
A:sklearn.utils.estimator_checks.pred2->clone(estimator_orig).predict(X)
A:sklearn.utils.estimator_checks.labels->clone(clusterer_orig).fit_predict(X_noise)
A:sklearn.utils.estimator_checks.labels_sorted->numpy.unique(labels)
A:sklearn.utils.estimator_checks.n_clusters->getattr(clusterer, 'n_clusters')
A:sklearn.utils.estimator_checks.X_train->rbf_kernel(X_train, X_train)
A:sklearn.utils.estimator_checks.X_test->rbf_kernel(X_test, X_train)
A:sklearn.utils.estimator_checks.classifier->Classifier()
A:sklearn.utils.estimator_checks.sample_weight->_enforce_estimator_tags_y(transformer_orig, y).copy()
A:sklearn.utils.estimator_checks.(X_m, y_m)->shuffle(X_m, y_m, random_state=7)
A:sklearn.utils.estimator_checks.X_m->StandardScaler().fit_transform(X_m)
A:sklearn.utils.estimator_checks.(X_m, y_m, X_b, y_b)->create_memmap_backed_data([X_m, y_m, X_b, y_b])
A:sklearn.utils.estimator_checks.n_classes->float(len(np.unique(y)))
A:sklearn.utils.estimator_checks.y_log_prob->Classifier().predict_log_proba(X)
A:sklearn.utils.estimator_checks.sorted_decision->numpy.sort(decision)
A:sklearn.utils.estimator_checks.(X, _)->make_blobs(n_samples=n_samples, random_state=0)
A:sklearn.utils.estimator_checks.scores->clone(estimator_orig).score_samples(X)
A:sklearn.utils.estimator_checks.num_outliers->numpy.sum(y_pred != 1)
A:sklearn.utils.estimator_checks.y_train_list_of_lists->y_train.tolist()
A:sklearn.utils.estimator_checks.y_train_list_of_arrays->list(y_train)
A:sklearn.utils.estimator_checks.y_pred_list_of_lists->Classifier().fit(X_train, y_train_list_of_lists).predict(X_test)
A:sklearn.utils.estimator_checks.y_pred_list_of_arrays->Classifier().fit(X_train, y_train_list_of_arrays).predict(X_test)
A:sklearn.utils.estimator_checks.predict_method->getattr(classifier, response_method_name, None)
A:sklearn.utils.estimator_checks.predict_proba_method->getattr(classifier, response_method_name, None)
A:sklearn.utils.estimator_checks.decision_function_method->getattr(classifier, response_method_name, None)
A:sklearn.utils.estimator_checks.y_pred_2d->clone(estimator_orig).predict(X)
A:sklearn.utils.estimator_checks.decision_y->numpy.argmax(decision, axis=1).astype(int)
A:sklearn.utils.estimator_checks.(X_multiclass, y_multiclass)->shuffle(X_multiclass, y_multiclass, random_state=7)
A:sklearn.utils.estimator_checks.X_multiclass->_enforce_estimator_tags_X(classifier_orig, X_multiclass)
A:sklearn.utils.estimator_checks.X_binary->_enforce_estimator_tags_X(classifier_orig, X_binary)
A:sklearn.utils.estimator_checks.y_names_multiclass->numpy.take(labels_multiclass, y_multiclass)
A:sklearn.utils.estimator_checks.y_names_binary->numpy.take(labels_binary, y_binary)
A:sklearn.utils.estimator_checks.y_binary->_choose_check_classifiers_labels(name, y_binary, y_names_binary)
A:sklearn.utils.estimator_checks.regressor_1->clone(regressor_orig)
A:sklearn.utils.estimator_checks.regressor_2->clone(regressor_orig)
A:sklearn.utils.estimator_checks.pred1->clone(estimator_orig).predict(X_)
A:sklearn.utils.estimator_checks.regressor->clone(regressor_orig)
A:sklearn.utils.estimator_checks.(X, y, y_)->create_memmap_backed_data([X, y, y_])
A:sklearn.utils.estimator_checks.(X_train, X_test, y_train, y_test)->train_test_split(X, y, test_size=0.2, random_state=0)
A:sklearn.utils.estimator_checks.n_centers->len(np.unique(y_train))
A:sklearn.utils.estimator_checks.y_pred_balanced->Classifier().predict(X_test)
A:sklearn.utils.estimator_checks.coef_balanced->Classifier().fit(X, y).coef_.copy()
A:sklearn.utils.estimator_checks.n_samples->len(y)
A:sklearn.utils.estimator_checks.coef_manual->Classifier().fit(X, y).coef_.copy()
A:sklearn.utils.estimator_checks.original_params->deepcopy(params)
A:sklearn.utils.estimator_checks.new_params->clone(estimator_orig).get_params()
A:sklearn.utils.estimator_checks.init_params->_get_args(type(estimator).__init__)
A:sklearn.utils.estimator_checks.invalid_attr->set([attr for attr in invalid_attr if not attr.startswith('_')])
A:sklearn.utils.estimator_checks.pred_orig->pickle.loads(pickle.dumps(est)).predict(X)
A:sklearn.utils.estimator_checks.estimator_1->clone(estimator_orig)
A:sklearn.utils.estimator_checks.estimator_2->clone(estimator_orig)
A:sklearn.utils.estimator_checks.X_->pandas.DataFrame(np.asarray(X), copy=False)
A:sklearn.utils.estimator_checks.init->getattr(estimator.__init__, 'deprecated_original', estimator.__init__)
A:sklearn.utils.estimator_checks.iris->load_iris()
A:sklearn.utils.estimator_checks.(X, y_)->make_blobs(n_samples=30, centers=[[0, 0, 0], [1, 1, 1]], random_state=0, n_features=2, cluster_std=0.1)
A:sklearn.utils.estimator_checks.shallow_params->clone(estimator_orig).get_params(deep=False)
A:sklearn.utils.estimator_checks.deep_params->clone(estimator_orig).get_params(deep=True)
A:sklearn.utils.estimator_checks.orig_params->clone(estimator_orig).get_params(deep=False)
A:sklearn.utils.estimator_checks.curr_params->clone(estimator_orig).get_params(deep=False)
A:sklearn.utils.estimator_checks.test_params->deepcopy(orig_params)
A:sklearn.utils.estimator_checks.change_warning_msg->"Estimator's parameters changed after set_params raised {}".format(e_type)
A:sklearn.utils.estimator_checks.a->clone(estimator_orig).predict_proba(X_test)[:, 1].round(decimals=10)
A:sklearn.utils.estimator_checks.b->clone(estimator_orig).decision_function(X_test).round(decimals=10)
A:sklearn.utils.estimator_checks.grouped_y_score->numpy.array([b[rank_proba == group].mean() for group in np.unique(rank_proba)])
A:sklearn.utils.estimator_checks.sorted_idx->numpy.argsort(grouped_y_score)
A:sklearn.utils.estimator_checks.y_pred_2->clone(estimator_orig).fit(X).predict(X)
A:sklearn.utils.estimator_checks.(train, test)->next(ShuffleSplit(test_size=0.2, random_state=rng).split(X))
A:sklearn.utils.estimator_checks.(X_train, y_train)->_safe_split(estimator, X, y, train)
A:sklearn.utils.estimator_checks.(X_test, y_test)->_safe_split(estimator, X, y, test, train)
A:sklearn.utils.estimator_checks.new_result->getattr(estimator, method)(X_test)
A:sklearn.utils.estimator_checks.callable_method->partial(callable_method, y=y)
A:sklearn.utils.estimator_checks.tags_keys->set(estimator._get_tags().keys())
A:sklearn.utils.estimator_checks.default_tags_keys->set(_DEFAULT_TAGS.keys())
A:sklearn.utils.estimator_checks.X_orig->_enforce_estimator_tags_X(estimator, X_orig)
A:sklearn.utils.estimator_checks.names->numpy.array([f'col_{i}' for i in range(n_features)])
A:sklearn.utils.estimator_checks.early_stopping_enabled->any((value is True for value in params.values()))
A:sklearn.utils.estimator_checks.X_bad->pandas.DataFrame(X, columns=invalid_name, copy=False)
A:sklearn.utils.estimator_checks.expected_msg->re.escape(f'The feature names should match those that were passed during fit.\n{additional_message}')
A:sklearn.utils.estimator_checks.X_transform->clone(transformer_orig).fit_transform(df, y=y_)
A:sklearn.utils.estimator_checks.feature_names_out->clone(transformer_orig).get_feature_names_out(input_features)
A:sklearn.utils.estimator_checks.df->create_dataframe(X, columns=feature_names_in, index=index)
A:sklearn.utils.estimator_checks.feature_names_out_default->clone(transformer_orig).get_feature_names_out()
A:sklearn.utils.estimator_checks.feature_names_in_explicit_names->clone(transformer_orig).get_feature_names_out(feature_names_in)
A:sklearn.utils.estimator_checks.estimator_params->estimator_orig.get_params(deep=False).keys()
A:sklearn.utils.estimator_checks.validation_params->estimator_orig._parameter_constraints.keys()
A:sklearn.utils.estimator_checks.param_with_bad_type->type('BadType', (), {})()
A:sklearn.utils.estimator_checks.bad_value->generate_invalid_param_val(constraint)
A:sklearn.utils.estimator_checks.X_trans_no_setting->transform_method(transformer)
A:sklearn.utils.estimator_checks.X_trans_default->transform_method(transformer)
A:sklearn.utils.estimator_checks.(X_trans, _)->clone(transformer_orig).fit_transform(data, y)
A:sklearn.utils.estimator_checks.expected_dataframe->create_dataframe(X_trans, columns=feature_names_dataframe_lib, index=expected_index)
A:sklearn.utils.estimator_checks.transformer_default->clone(transformer).set_output(transform='default')
A:sklearn.utils.estimator_checks.outputs_default->_output_from_fit_transform(transformer_default, name, X, df, y)
A:sklearn.utils.estimator_checks.transformer_df->clone(transformer)
A:sklearn.utils.estimator_checks.context_to_use->config_context(transform_output=dataframe_lib)
A:sklearn.utils.estimator_checks.outputs_df->_output_from_fit_transform(transformer_df, name, X, df, y)
A:sklearn.utils.estimator_checks.capitalized_lib->dataframe_lib.capitalize()
A:sklearn.utils.estimator_checks.error_message->str(e)
A:sklearn.utils.estimator_checks.columns->columns.tolist().tolist()
sklearn.utils.estimator_checks._NotAnArray(self,data)
sklearn.utils.estimator_checks._NotAnArray.__array__(self,dtype=None)
sklearn.utils.estimator_checks._NotAnArray.__array_function__(self,func,types,args,kwargs)
sklearn.utils.estimator_checks._NotAnArray.__init__(self,data)
sklearn.utils.estimator_checks._apply_on_subsets(func,X)
sklearn.utils.estimator_checks._check_generated_dataframe(name,case,index,outputs_default,outputs_dataframe_lib,is_supported_dataframe,create_dataframe,assert_frame_equal)
sklearn.utils.estimator_checks._check_set_output_transform_dataframe(name,transformer_orig,*,dataframe_lib,is_supported_dataframe,create_dataframe,assert_frame_equal,context)
sklearn.utils.estimator_checks._check_set_output_transform_pandas_context(name,transformer_orig,context)
sklearn.utils.estimator_checks._check_set_output_transform_polars_context(name,transformer_orig,context)
sklearn.utils.estimator_checks._check_transformer(name,transformer_orig,X,y)
sklearn.utils.estimator_checks._choose_check_classifiers_labels(name,y,y_names)
sklearn.utils.estimator_checks._construct_instance(Estimator)
sklearn.utils.estimator_checks._enforce_estimator_tags_X(estimator,X,kernel=linear_kernel)
sklearn.utils.estimator_checks._enforce_estimator_tags_y(estimator,y)
sklearn.utils.estimator_checks._generate_sparse_matrix(X_csr)
sklearn.utils.estimator_checks._get_check_estimator_ids(obj)
sklearn.utils.estimator_checks._is_pairwise_metric(estimator)
sklearn.utils.estimator_checks._is_public_parameter(attr)
sklearn.utils.estimator_checks._maybe_mark_xfail(estimator,check,pytest)
sklearn.utils.estimator_checks._maybe_skip(estimator,check)
sklearn.utils.estimator_checks._output_from_fit_transform(transformer,name,X,df,y)
sklearn.utils.estimator_checks._regression_dataset()
sklearn.utils.estimator_checks._set_checking_parameters(estimator)
sklearn.utils.estimator_checks._should_be_skipped_or_marked(estimator,check)
sklearn.utils.estimator_checks._yield_all_checks(estimator)
sklearn.utils.estimator_checks._yield_array_api_checks(estimator)
sklearn.utils.estimator_checks._yield_checks(estimator)
sklearn.utils.estimator_checks._yield_classifier_checks(classifier)
sklearn.utils.estimator_checks._yield_clustering_checks(clusterer)
sklearn.utils.estimator_checks._yield_outliers_checks(estimator)
sklearn.utils.estimator_checks._yield_regressor_checks(regressor)
sklearn.utils.estimator_checks._yield_transformer_checks(transformer)
sklearn.utils.estimator_checks.check_array_api_input(name,estimator_orig,array_namespace,device=None,dtype_name='float64',check_values=False)
sklearn.utils.estimator_checks.check_array_api_input_and_values(name,estimator_orig,array_namespace,device=None,dtype_name='float64')
sklearn.utils.estimator_checks.check_class_weight_balanced_classifiers(name,classifier_orig,X_train,y_train,X_test,y_test,weights)
sklearn.utils.estimator_checks.check_class_weight_balanced_linear_classifier(name,Classifier)
sklearn.utils.estimator_checks.check_class_weight_classifiers(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifier_data_not_an_array(name,estimator_orig)
sklearn.utils.estimator_checks.check_classifier_multioutput(name,estimator)
sklearn.utils.estimator_checks.check_classifiers_classes(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_multilabel_output_format_decision_function(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_multilabel_output_format_predict(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_multilabel_output_format_predict_proba(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_multilabel_representation_invariance(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_one_label(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_one_label_sample_weights(name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_predictions(X,y,name,classifier_orig)
sklearn.utils.estimator_checks.check_classifiers_regression_target(name,estimator_orig)
sklearn.utils.estimator_checks.check_classifiers_train(name,classifier_orig,readonly_memmap=False,X_dtype='float64')
sklearn.utils.estimator_checks.check_clusterer_compute_labels_predict(name,clusterer_orig)
sklearn.utils.estimator_checks.check_clustering(name,clusterer_orig,readonly_memmap=False)
sklearn.utils.estimator_checks.check_complex_data(name,estimator_orig)
sklearn.utils.estimator_checks.check_dataframe_column_names_consistency(name,estimator_orig)
sklearn.utils.estimator_checks.check_decision_proba_consistency(name,estimator_orig)
sklearn.utils.estimator_checks.check_dict_unchanged(name,estimator_orig)
sklearn.utils.estimator_checks.check_dont_overwrite_parameters(name,estimator_orig)
sklearn.utils.estimator_checks.check_dtype_object(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimator(estimator=None,generate_only=False)
sklearn.utils.estimator_checks.check_estimator_get_tags_default_keys(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimator_sparse_data(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_data_not_an_array(name,estimator_orig,X,y,obj_type)
sklearn.utils.estimator_checks.check_estimators_dtypes(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_empty_data_messages(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_fit_returns_self(name,estimator_orig,readonly_memmap=False)
sklearn.utils.estimator_checks.check_estimators_nan_inf(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_overwrite_params(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_partial_fit_n_features(name,estimator_orig)
sklearn.utils.estimator_checks.check_estimators_pickle(name,estimator_orig,readonly_memmap=False)
sklearn.utils.estimator_checks.check_estimators_unfitted(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit1d(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_1feature(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_1sample(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit2d_predict1d(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit_check_is_fitted(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit_idempotent(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit_non_negative(name,estimator_orig)
sklearn.utils.estimator_checks.check_fit_score_takes_y(name,estimator_orig)
sklearn.utils.estimator_checks.check_get_feature_names_out_error(name,estimator_orig)
sklearn.utils.estimator_checks.check_get_params_invariance(name,estimator_orig)
sklearn.utils.estimator_checks.check_global_output_transform_pandas(name,transformer_orig)
sklearn.utils.estimator_checks.check_global_set_output_transform_polars(name,transformer_orig)
sklearn.utils.estimator_checks.check_methods_sample_order_invariance(name,estimator_orig)
sklearn.utils.estimator_checks.check_methods_subset_invariance(name,estimator_orig)
sklearn.utils.estimator_checks.check_n_features_in(name,estimator_orig)
sklearn.utils.estimator_checks.check_n_features_in_after_fitting(name,estimator_orig)
sklearn.utils.estimator_checks.check_no_attributes_set_in_init(name,estimator_orig)
sklearn.utils.estimator_checks.check_non_transformer_estimators_n_iter(name,estimator_orig)
sklearn.utils.estimator_checks.check_nonsquare_error(name,estimator_orig)
sklearn.utils.estimator_checks.check_outlier_contamination(name,estimator_orig)
sklearn.utils.estimator_checks.check_outlier_corruption(num_outliers,expected_outliers,decision)
sklearn.utils.estimator_checks.check_outliers_fit_predict(name,estimator_orig)
sklearn.utils.estimator_checks.check_outliers_train(name,estimator_orig,readonly_memmap=True)
sklearn.utils.estimator_checks.check_param_validation(name,estimator_orig)
sklearn.utils.estimator_checks.check_parameters_default_constructible(name,Estimator)
sklearn.utils.estimator_checks.check_pipeline_consistency(name,estimator_orig)
sklearn.utils.estimator_checks.check_regressor_data_not_an_array(name,estimator_orig)
sklearn.utils.estimator_checks.check_regressor_multioutput(name,estimator)
sklearn.utils.estimator_checks.check_regressors_int(name,regressor_orig)
sklearn.utils.estimator_checks.check_regressors_no_decision_function(name,regressor_orig)
sklearn.utils.estimator_checks.check_regressors_train(name,regressor_orig,readonly_memmap=False,X_dtype=np.float64)
sklearn.utils.estimator_checks.check_requires_y_none(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_invariance(name,estimator_orig,kind='ones')
sklearn.utils.estimator_checks.check_sample_weights_list(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_not_an_array(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_not_overwritten(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_pandas_series(name,estimator_orig)
sklearn.utils.estimator_checks.check_sample_weights_shape(name,estimator_orig)
sklearn.utils.estimator_checks.check_set_output_transform(name,transformer_orig)
sklearn.utils.estimator_checks.check_set_output_transform_pandas(name,transformer_orig)
sklearn.utils.estimator_checks.check_set_output_transform_polars(name,transformer_orig)
sklearn.utils.estimator_checks.check_set_params(name,estimator_orig)
sklearn.utils.estimator_checks.check_sparsify_coefficients(name,estimator_orig)
sklearn.utils.estimator_checks.check_supervised_y_2d(name,estimator_orig)
sklearn.utils.estimator_checks.check_supervised_y_no_nan(name,estimator_orig)
sklearn.utils.estimator_checks.check_transformer_data_not_an_array(name,transformer)
sklearn.utils.estimator_checks.check_transformer_general(name,transformer,readonly_memmap=False)
sklearn.utils.estimator_checks.check_transformer_get_feature_names_out(name,transformer_orig)
sklearn.utils.estimator_checks.check_transformer_get_feature_names_out_pandas(name,transformer_orig)
sklearn.utils.estimator_checks.check_transformer_n_iter(name,estimator_orig)
sklearn.utils.estimator_checks.check_transformer_preserve_dtypes(name,transformer_orig)
sklearn.utils.estimator_checks.check_transformers_unfitted(name,transformer)
sklearn.utils.estimator_checks.check_transformers_unfitted_stateless(name,transformer)
sklearn.utils.estimator_checks.parametrize_with_checks(estimators)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_fast_dict.py----------------------------------------
A:sklearn.utils.tests.test_fast_dict.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_fast_dict.keys->numpy.arange(100, dtype=np.intp)
A:sklearn.utils.tests.test_fast_dict.values->numpy.arange(100, dtype=np.float64)
A:sklearn.utils.tests.test_fast_dict.d->IntFloatDict(keys_in, values_in)
A:sklearn.utils.tests.test_fast_dict.keys_in->numpy.array([1, 2, 3], dtype=np.intp)
A:sklearn.utils.tests.test_fast_dict.values_in->numpy.array([4, 5, 6], dtype=np.float64)
A:sklearn.utils.tests.test_fast_dict.(keys_out, values_out)->IntFloatDict(keys_in, values_in).to_arrays()
sklearn.utils.tests.test_fast_dict.test_int_float_dict()
sklearn.utils.tests.test_fast_dict.test_int_float_dict_argmin()
sklearn.utils.tests.test_fast_dict.test_to_arrays()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_optimize.py----------------------------------------
A:sklearn.utils.tests.test_optimize.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_optimize.A->numpy.random.RandomState(0).normal(size=(10, 10))
A:sklearn.utils.tests.test_optimize.x0->numpy.ones(10)
A:sklearn.utils.tests.test_optimize.Ax->numpy.random.RandomState(0).normal(size=(10, 10)).dot(x)
sklearn.utils.tests.test_optimize.test_newton_cg()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_parallel.py----------------------------------------
A:sklearn.utils.tests.test_parallel.results->Parallel(n_jobs=n_jobs, backend=backend)((delayed(get_working_memory)() for _ in range(2)))
A:sklearn.utils.tests.test_parallel.pd->pytest.importorskip('pandas')
A:sklearn.utils.tests.test_parallel.iris->load_iris(as_frame=True)
A:sklearn.utils.tests.test_parallel.dropper->make_column_transformer(('drop', [0]), remainder='passthrough', n_jobs=n_jobs)
A:sklearn.utils.tests.test_parallel.search_cv->GridSearchCV(make_pipeline(dropper, TransformerRequiredDataFrame(), RandomForestClassifier(n_estimators=5, n_jobs=n_jobs)), param_grid, cv=5, n_jobs=n_jobs, error_score='raise')
sklearn.utils.tests.test_parallel.get_working_memory()
sklearn.utils.tests.test_parallel.test_configuration_passes_through_to_joblib(n_jobs,backend)
sklearn.utils.tests.test_parallel.test_dispatch_config_parallel(n_jobs)
sklearn.utils.tests.test_parallel.test_parallel_delayed_warnings()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_graph.py----------------------------------------
A:sklearn.utils.tests.test_graph.graph->_fix_connected_components(X, graph, n_connected_components, labels, mode='distance')
A:sklearn.utils.tests.test_graph.(n_connected_components, labels)->connected_components(graph)
A:sklearn.utils.tests.test_graph.distances->pairwise_distances(X)
sklearn.utils.tests.test_graph.test_fix_connected_components()
sklearn.utils.tests.test_graph.test_fix_connected_components_connectivity_mode()
sklearn.utils.tests.test_graph.test_fix_connected_components_distance_mode()
sklearn.utils.tests.test_graph.test_fix_connected_components_precomputed()
sklearn.utils.tests.test_graph.test_fix_connected_components_wrong_mode()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_bunch.py----------------------------------------
A:sklearn.utils.tests.test_bunch.bunch->Bunch()
A:sklearn.utils.tests.test_bunch.values->numpy.asarray([1, 2, 3])
sklearn.utils.tests.test_bunch.test_bunch_attribute_deprecation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_weight_vector.py----------------------------------------
A:sklearn.utils.tests.test_weight_vector.weights->numpy.random.rand(100).astype(dtype)
A:sklearn.utils.tests.test_weight_vector.average_weights->numpy.random.rand(100).astype(dtype)
A:sklearn.utils.tests.test_weight_vector.weight_vector->WeightVector(weights, average_weights)
sklearn.utils.tests.test_weight_vector.test_type_invariance(dtype,WeightVector)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_validation.py----------------------------------------
A:sklearn.utils.tests.test_validation.X->sparse_container([[0, 1], [1, 0]], dtype=np.float64)
A:sklearn.utils.tests.test_validation.X2->as_float_array(X, copy=True)
A:sklearn.utils.tests.test_validation.N->as_float_array(M, copy=True)
A:sklearn.utils.tests.test_validation.X_converted->as_float_array(X, force_all_finite='allow-nan')
A:sklearn.utils.tests.test_validation.M->numpy.memmap(tmp, shape=(10, 10), dtype=np.float32)
A:sklearn.utils.tests.test_validation.B->check_array(A, order='F', copy=copy)
A:sklearn.utils.tests.test_validation.X_checked->check_array(X, accept_sparse=output_format)
A:sklearn.utils.tests.test_validation.data->retype(np.arange(4).reshape(2, 2).astype(np.float64))
A:sklearn.utils.tests.test_validation.estimator->SVR()
A:sklearn.utils.tests.test_validation.pd->pytest.importorskip('pandas')
A:sklearn.utils.tests.test_validation.ser->pytest.importorskip('pandas').Series([1, 2, 3])
A:sklearn.utils.tests.test_validation.X_csr->scipy.sparse.csr_matrix(X)
A:sklearn.utils.tests.test_validation.X_array->check_array([0, 1, 2], ensure_2d=False)
A:sklearn.utils.tests.test_validation.X_ndim->numpy.arange(8).reshape(2, 2, 2)
A:sklearn.utils.tests.test_validation.X_C->numpy.arange(4).reshape(2, 2).copy('C')
A:sklearn.utils.tests.test_validation.X_F->numpy.arange(4).reshape(2, 2).copy('C').copy('F')
A:sklearn.utils.tests.test_validation.X_int->numpy.arange(4).reshape(2, 2).copy('C').astype(int)
A:sklearn.utils.tests.test_validation.X_float->numpy.arange(4).reshape(2, 2).copy('C').astype(float)
A:sklearn.utils.tests.test_validation.X_dense->check_array([[1, 2], [3, 4]])
A:sklearn.utils.tests.test_validation.X_no_array->_NotAnArray(X_dense)
A:sklearn.utils.tests.test_validation.result->check_array(input_series, dtype=None, ensure_2d=False, allow_nd=False, force_all_finite=False)
A:sklearn.utils.tests.test_validation.X['c']->X['c'].astype('float').astype('float')
A:sklearn.utils.tests.test_validation.X_int64->numpy.asarray(X_int_list, dtype=np.int64)
A:sklearn.utils.tests.test_validation.X_out->check_array(X_int64, force_all_finite=False, ensure_2d=False, dtype=np.float32)
A:sklearn.utils.tests.test_validation.X_df->pytest.importorskip('pandas').DataFrame(X, columns=['a', 'b', 'fit'])
A:sklearn.utils.tests.test_validation.cat_df->pytest.importorskip('pandas').DataFrame({'cat_col': pd.Categorical([1, 2, 3])})
A:sklearn.utils.tests.test_validation.arr->check_array(df, accept_sparse=['csr', 'csc'])
A:sklearn.utils.tests.test_validation.mock_df->MockDataFrame(arr)
A:sklearn.utils.tests.test_validation.checked_arr->check_array(mock_df, dtype=np.float32)
A:sklearn.utils.tests.test_validation.X_float32->numpy.asarray(X_int_list, dtype=np.float32)
A:sklearn.utils.tests.test_validation.X_csr_float32->scipy.sparse.csr_matrix(X_float32)
A:sklearn.utils.tests.test_validation.X_csc_float32->scipy.sparse.csc_matrix(X_float32)
A:sklearn.utils.tests.test_validation.X_csc_int32->scipy.sparse.csc_matrix(X_int64, dtype=np.int32)
A:sklearn.utils.tests.test_validation.invalid_type->SVR()
A:sklearn.utils.tests.test_validation.X.indices->sparse_container([[0, 1], [1, 0]], dtype=np.float64).indices.astype(np.int32)
A:sklearn.utils.tests.test_validation.X.row->sparse_container([[0, 1], [1, 0]], dtype=np.float64).row.astype(np.int32)
A:sklearn.utils.tests.test_validation.X.col->sparse_container([[0, 1], [1, 0]], dtype=np.float64).col.astype('int64')
A:sklearn.utils.tests.test_validation.X.indptr->sparse_container([[0, 1], [1, 0]], dtype=np.float64).indptr.astype(np.int32)
A:sklearn.utils.tests.test_validation.y->toarray(x + 1)
A:sklearn.utils.tests.test_validation.(X_checked, y_checked)->check_X_y(X, y, allow_nd=True)
A:sklearn.utils.tests.test_validation.arr_sym->numpy.array([[0, 1], [1, 2]])
A:sklearn.utils.tests.test_validation.arr_bad->numpy.ones(2)
A:sklearn.utils.tests.test_validation.arr_asym->numpy.array([[0, 2], [0, 2]])
A:sklearn.utils.tests.test_validation.output->check_symmetric(arr, raise_warning=False)
A:sklearn.utils.tests.test_validation.ard->ARDRegression()
A:sklearn.utils.tests.test_validation.svr->SVR()
A:sklearn.utils.tests.test_validation.est->PassthroughTransformer().fit(df)
A:sklearn.utils.tests.test_validation.res->check_array(pd.Series([True, False]), ensure_2d=False)
A:sklearn.utils.tests.test_validation.s->pytest.importorskip('pandas').Series(['a', 'b', 'c']).astype('category')
A:sklearn.utils.tests.test_validation.df->pytest.importorskip('polars').DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
A:sklearn.utils.tests.test_validation.array->check_array(df, dtype=None)
A:sklearn.utils.tests.test_validation.expected_array->numpy.array([[1.0, 0.0, 1.0], [2.0, 0.1, 0.0], [3.0, 2.1, 1.0]], dtype=float)
A:sklearn.utils.tests.test_validation.memory->check_memory(dummy)
A:sklearn.utils.tests.test_validation.dummy->WrongDummyMemory()
A:sklearn.utils.tests.test_validation.msg->re.escape(f'Feature names are only supported if all input features have string names, but your input has {dtypes} as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.')
A:sklearn.utils.tests.test_validation.A->numpy.array([[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])
A:sklearn.utils.tests.test_validation.scalar->check_scalar(x, 'test_name', target_type=numbers.Real, min_val=2, max_val=5, include_boundaries='both')
A:sklearn.utils.tests.test_validation.lambdas_fixed->_check_psd_eigenvalues(lambdas, enable_warnings=enable_warnings)
A:sklearn.utils.tests.test_validation.sample_weight->numpy.ones(_num_samples(X))
A:sklearn.utils.tests.test_validation.base->numpy.arange(9).reshape(3, 3)
A:sklearn.utils.tests.test_validation.x->numpy.arange(9).reshape(3, 3)
A:sklearn.utils.tests.test_validation.sp_mat->_sparse_random_matrix(10, 3)
A:sklearn.utils.tests.test_validation.sdf->pytest.importorskip('pandas').DataFrame.sparse.from_spmatrix(sp_mat)
A:sklearn.utils.tests.test_validation.names->PassthroughTransformer().fit(df).get_feature_names_out()
A:sklearn.utils.tests.test_validation.feature_names->_get_feature_names(df)
A:sklearn.utils.tests.test_validation.not_a_polars_df->NotAPolarsDataFrame()
A:sklearn.utils.tests.test_validation.my_estimator->_MockEstimatorOnOffPrediction(method_implemented)
A:sklearn.utils.tests.test_validation.method_name_predicting->_check_response_method(my_estimator, response_method)(X)
A:sklearn.utils.tests.test_validation.expected->numpy.array([True, False])
A:sklearn.utils.tests.test_validation.input_series->pytest.importorskip('pandas').array(input_values, dtype='Int32')
A:sklearn.utils.tests.test_validation.xp->pytest.importorskip(array_namespace)
A:sklearn.utils.tests.test_validation.X_nan->pytest.importorskip(array_namespace).asarray([[xp.nan, 1, 0], [0, xp.nan, 3]], dtype=xp.float32)
A:sklearn.utils.tests.test_validation.X_inf->pytest.importorskip(array_namespace).asarray([[xp.inf, 1, 0], [0, xp.inf, 3]], dtype=xp.float32)
A:sklearn.utils.tests.test_validation.X_regular->pytest.importorskip('pandas').DataFrame({'a': pd.Series([1, 0, 1, 0], dtype=regular_dtype), 'c': pd.Series([9, 8, 7, 6], dtype='int64')})
A:sklearn.utils.tests.test_validation.X_regular['b']->pytest.importorskip('pandas').Series(['a', 'b', 'c', 'd'], dtype='object')
A:sklearn.utils.tests.test_validation.X_extension->pytest.importorskip('pandas').DataFrame({'a': pd.Series([1, 0, 1, 0], dtype=regular_dtype), 'c': pd.Series([9, 8, 7, 6], dtype='int64')}).assign(a=X_regular['a'].astype(extension_dtype))
A:sklearn.utils.tests.test_validation.X_regular_checked->check_array(X_regular, dtype=None)
A:sklearn.utils.tests.test_validation.X_extension_checked->check_array(X_extension, dtype=None)
A:sklearn.utils.tests.test_validation.pl->pytest.importorskip('polars')
A:sklearn.utils.tests.test_validation.X.offsets->sparse_container([[0, 1], [1, 0]], dtype=np.float64).offsets.astype(np.int32)
sklearn.utils.tests.test_validation.DummyMemory
sklearn.utils.tests.test_validation.DummyMemory.cache(self,func)
sklearn.utils.tests.test_validation.PassthroughTransformer(BaseEstimator)
sklearn.utils.tests.test_validation.PassthroughTransformer.fit(self,X,y=None)
sklearn.utils.tests.test_validation.PassthroughTransformer.get_feature_names_out(self,input_features=None)
sklearn.utils.tests.test_validation.PassthroughTransformer.transform(self,X)
sklearn.utils.tests.test_validation.WrongDummyMemory
sklearn.utils.tests.test_validation.X_64bit(request)
sklearn.utils.tests.test_validation.test_allclose_dense_sparse_equals(toarray)
sklearn.utils.tests.test_validation.test_allclose_dense_sparse_not_equals(toarray)
sklearn.utils.tests.test_validation.test_allclose_dense_sparse_raise(toarray)
sklearn.utils.tests.test_validation.test_as_float_array()
sklearn.utils.tests.test_validation.test_as_float_array_nan(X)
sklearn.utils.tests.test_validation.test_boolean_series_remains_boolean()
sklearn.utils.tests.test_validation.test_check_X_y_informative_error()
sklearn.utils.tests.test_validation.test_check_array()
sklearn.utils.tests.test_validation.test_check_array_accept_large_sparse_no_exception(X_64bit)
sklearn.utils.tests.test_validation.test_check_array_accept_large_sparse_raise_exception(X_64bit)
sklearn.utils.tests.test_validation.test_check_array_accept_sparse_no_exception()
sklearn.utils.tests.test_validation.test_check_array_accept_sparse_type_exception()
sklearn.utils.tests.test_validation.test_check_array_array_api_has_non_finite(array_namespace)
sklearn.utils.tests.test_validation.test_check_array_complex_data_error()
sklearn.utils.tests.test_validation.test_check_array_dia_to_int32_indexed_csr_csc_coo(sparse_container,output_format)
sklearn.utils.tests.test_validation.test_check_array_dtype_stability()
sklearn.utils.tests.test_validation.test_check_array_dtype_warning()
sklearn.utils.tests.test_validation.test_check_array_force_all_finite_object()
sklearn.utils.tests.test_validation.test_check_array_force_all_finite_object_unsafe_casting(X,err_msg,force_all_finite)
sklearn.utils.tests.test_validation.test_check_array_force_all_finite_valid(value,force_all_finite,retype)
sklearn.utils.tests.test_validation.test_check_array_force_all_finiteinvalid(value,input_name,force_all_finite,match_msg,retype)
sklearn.utils.tests.test_validation.test_check_array_links_to_imputer_doc_only_for_X(input_name,retype)
sklearn.utils.tests.test_validation.test_check_array_memmap(copy)
sklearn.utils.tests.test_validation.test_check_array_min_samples_and_features_messages()
sklearn.utils.tests.test_validation.test_check_array_multiple_extensions(extension_dtype,regular_dtype,include_object)
sklearn.utils.tests.test_validation.test_check_array_numeric_error(X)
sklearn.utils.tests.test_validation.test_check_array_on_mock_dataframe()
sklearn.utils.tests.test_validation.test_check_array_panadas_na_support_series()
sklearn.utils.tests.test_validation.test_check_array_pandas_dtype_casting()
sklearn.utils.tests.test_validation.test_check_array_pandas_na_support(pd_dtype,dtype,expected_dtype)
sklearn.utils.tests.test_validation.test_check_array_series()
sklearn.utils.tests.test_validation.test_check_array_series_err_msg()
sklearn.utils.tests.test_validation.test_check_consistent_length()
sklearn.utils.tests.test_validation.test_check_dataframe_fit_attribute()
sklearn.utils.tests.test_validation.test_check_dataframe_mixed_float_dtypes(dtype,bool_dtype)
sklearn.utils.tests.test_validation.test_check_dataframe_with_only_bool()
sklearn.utils.tests.test_validation.test_check_dataframe_with_only_boolean()
sklearn.utils.tests.test_validation.test_check_feature_names_in()
sklearn.utils.tests.test_validation.test_check_feature_names_in_pandas()
sklearn.utils.tests.test_validation.test_check_is_fitted()
sklearn.utils.tests.test_validation.test_check_is_fitted_attributes()
sklearn.utils.tests.test_validation.test_check_is_fitted_with_attributes(wrap)
sklearn.utils.tests.test_validation.test_check_is_fitted_with_is_fitted()
sklearn.utils.tests.test_validation.test_check_memory()
sklearn.utils.tests.test_validation.test_check_method_params(indices)
sklearn.utils.tests.test_validation.test_check_non_negative(retype)
sklearn.utils.tests.test_validation.test_check_pandas_sparse_invalid(ntype1,ntype2)
sklearn.utils.tests.test_validation.test_check_pandas_sparse_valid(ntype1,ntype2,expected_subtype)
sklearn.utils.tests.test_validation.test_check_psd_eigenvalues_invalid(lambdas,err_type,err_msg)
sklearn.utils.tests.test_validation.test_check_psd_eigenvalues_valid(lambdas,expected_lambdas,w_type,w_msg,enable_warnings)
sklearn.utils.tests.test_validation.test_check_response_method_list_str()
sklearn.utils.tests.test_validation.test_check_response_method_not_supported_response_method(response_method)
sklearn.utils.tests.test_validation.test_check_response_method_unknown_method()
sklearn.utils.tests.test_validation.test_check_sample_weight()
sklearn.utils.tests.test_validation.test_check_scalar_invalid(x,target_name,target_type,min_val,max_val,include_boundaries,err_msg)
sklearn.utils.tests.test_validation.test_check_scalar_valid(x)
sklearn.utils.tests.test_validation.test_check_sparse_pandas_sp_format(sp_format)
sklearn.utils.tests.test_validation.test_check_symmetric()
sklearn.utils.tests.test_validation.test_deprecate_positional_args_warns_for_class()
sklearn.utils.tests.test_validation.test_deprecate_positional_args_warns_for_function()
sklearn.utils.tests.test_validation.test_deprecate_positional_args_warns_for_function_version()
sklearn.utils.tests.test_validation.test_get_feature_names_dataframe_protocol(constructor_name,minversion)
sklearn.utils.tests.test_validation.test_get_feature_names_invalid_dtypes(names,dtypes)
sklearn.utils.tests.test_validation.test_get_feature_names_numpy()
sklearn.utils.tests.test_validation.test_get_feature_names_pandas()
sklearn.utils.tests.test_validation.test_get_feature_names_pandas_with_ints_no_warning(names)
sklearn.utils.tests.test_validation.test_has_fit_parameter()
sklearn.utils.tests.test_validation.test_is_pandas_df()
sklearn.utils.tests.test_validation.test_is_pandas_df_other_libraries(constructor_name)
sklearn.utils.tests.test_validation.test_is_pandas_df_pandas_not_installed(hide_available_pandas)
sklearn.utils.tests.test_validation.test_is_polars_df_for_duck_typed_polars_dataframe()
sklearn.utils.tests.test_validation.test_is_polars_df_other_libraries(constructor_name,minversion)
sklearn.utils.tests.test_validation.test_memmap()
sklearn.utils.tests.test_validation.test_np_matrix()
sklearn.utils.tests.test_validation.test_num_features(constructor_name)
sklearn.utils.tests.test_validation.test_num_features_errors_1d_containers(X,constructor_name)
sklearn.utils.tests.test_validation.test_num_features_errors_scalars(X)
sklearn.utils.tests.test_validation.test_num_samples_dataframe_protocol()
sklearn.utils.tests.test_validation.test_ordering()
sklearn.utils.tests.test_validation.test_pandas_array_returns_ndarray(input_values)
sklearn.utils.tests.test_validation.test_retrieve_samples_from_non_standard_shape()
sklearn.utils.tests.test_validation.test_suppress_validation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_class_weight.py----------------------------------------
A:sklearn.utils.tests.test_class_weight.y->csc_container(np.asarray([[0], [1], [1]]))
A:sklearn.utils.tests.test_class_weight.classes->numpy.unique(y)
A:sklearn.utils.tests.test_class_weight.cw->compute_class_weight({2: 1.5, 4: 0.5}, classes=classes, y=y)
A:sklearn.utils.tests.test_class_weight.(X, y)->make_blobs(centers=2, random_state=0)
A:sklearn.utils.tests.test_class_weight.X_1->numpy.vstack([X] + [X[y == 1]] * 2)
A:sklearn.utils.tests.test_class_weight.y_1->numpy.hstack([y] + [y[y == 1]] * 2)
A:sklearn.utils.tests.test_class_weight.X_0->numpy.vstack([X] + [X[y == 0]] * 2)
A:sklearn.utils.tests.test_class_weight.y_0->numpy.hstack([y] + [y[y == 0]] * 2)
A:sklearn.utils.tests.test_class_weight.X_->numpy.vstack([X] * 2)
A:sklearn.utils.tests.test_class_weight.y_->numpy.hstack([y] * 2)
A:sklearn.utils.tests.test_class_weight.logreg1->LogisticRegression(class_weight='balanced').fit(X_1, y_1)
A:sklearn.utils.tests.test_class_weight.logreg0->LogisticRegression(class_weight='balanced').fit(X_0, y_0)
A:sklearn.utils.tests.test_class_weight.logreg->LogisticRegression(class_weight='balanced').fit(X_, y_)
A:sklearn.utils.tests.test_class_weight.class_counts->numpy.bincount(y + 2)
A:sklearn.utils.tests.test_class_weight.classes_len->len(classes)
A:sklearn.utils.tests.test_class_weight.sample_weight->compute_sample_weight('balanced', y)
A:sklearn.utils.tests.test_class_weight.expected_balanced->numpy.asarray([0.6, 0.6, 0.6, 3.0, 3.0, 3.0])
A:sklearn.utils.tests.test_class_weight.y_single_output->numpy.asarray([1, 1, 1, 2, 2, 2])
A:sklearn.utils.tests.test_class_weight.y_multi_output->numpy.asarray([[1, 0], [1, 0], [1, 0], [2, 1], [2, 1], [2, 1]])
A:sklearn.utils.tests.test_class_weight.indices->numpy.arange(50)
A:sklearn.utils.tests.test_class_weight.weight->compute_sample_weight('balanced', y, indices=indices)
A:sklearn.utils.tests.test_class_weight.tree->DecisionTreeClassifier(class_weight={0: 1, 1: 10, 2: 20})
sklearn.utils.tests.test_class_weight.test_class_weight_does_not_contains_more_classes()
sklearn.utils.tests.test_class_weight.test_compute_class_weight()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_balanced_negative()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_balanced_unordered()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_default()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_dict()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_invariance()
sklearn.utils.tests.test_class_weight.test_compute_class_weight_not_present(y_type,class_weight,classes,err_msg)
sklearn.utils.tests.test_class_weight.test_compute_sample_weight()
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_errors(y_type,class_weight,indices,err_msg)
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_more_than_32()
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_sparse(csc_container)
sklearn.utils.tests.test_class_weight.test_compute_sample_weight_with_subsample()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_deprecation.py----------------------------------------
A:sklearn.utils.tests.test_deprecation.val->mock_function()
sklearn.utils.tests.test_deprecation.MockClass1
sklearn.utils.tests.test_deprecation.MockClass2
sklearn.utils.tests.test_deprecation.MockClass2.method(self)
sklearn.utils.tests.test_deprecation.MockClass2.n_features_(self)
sklearn.utils.tests.test_deprecation.MockClass3(self)
sklearn.utils.tests.test_deprecation.MockClass3.__init__(self)
sklearn.utils.tests.test_deprecation.MockClass4
sklearn.utils.tests.test_deprecation.MockClass5(self,a)
sklearn.utils.tests.test_deprecation.MockClass5.__init__(self,a)
sklearn.utils.tests.test_deprecation.MockClass6(cls,*args,**kwargs)
sklearn.utils.tests.test_deprecation.MockClass6.__new__(cls,*args,**kwargs)
sklearn.utils.tests.test_deprecation.mock_function()
sklearn.utils.tests.test_deprecation.test_deprecated()
sklearn.utils.tests.test_deprecation.test_is_deprecated()
sklearn.utils.tests.test_deprecation.test_pickle()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_estimator_checks.py----------------------------------------
A:sklearn.utils.tests.test_estimator_checks.(X, y)->check_X_y(X, y)
A:sklearn.utils.tests.test_estimator_checks.X->check_array(X)
A:sklearn.utils.tests.test_estimator_checks.p->kwargs.pop('p')
A:sklearn.utils.tests.test_estimator_checks.a->kwargs.pop('a')
A:sklearn.utils.tests.test_estimator_checks.self.coef_->numpy.ones(X.shape[1])
A:sklearn.utils.tests.test_estimator_checks.label_encoder->LabelEncoder().fit(y)
A:sklearn.utils.tests.test_estimator_checks.class_weight->compute_class_weight(self.class_weight, classes=classes, y=y)
A:sklearn.utils.tests.test_estimator_checks.(self.classes_, y)->numpy.unique(y, return_inverse=True)
A:sklearn.utils.tests.test_estimator_checks.n_classes_->numpy.count_nonzero(np.bincount(y, sample_weight))
A:sklearn.utils.tests.test_estimator_checks.tags->super()._get_tags().copy()
A:sklearn.utils.tests.test_estimator_checks.(xp, _)->sklearn.utils._array_api.get_namespace(X)
A:sklearn.utils.tests.test_estimator_checks.not_array->_NotAnArray(np.ones(10))
A:sklearn.utils.tests.test_estimator_checks.msg->'{method} of {name} is not invariant when applied to a subset.'.format(method=method, name=name)
A:sklearn.utils.tests.test_estimator_checks.decision->numpy.array([0.0, 1.0, 1.0, 2.0])
A:sklearn.utils.tests.test_estimator_checks.iris->load_iris()
A:sklearn.utils.tests.test_estimator_checks.est->KNeighborsRegressor(metric='precomputed')
A:sklearn.utils.tests.test_estimator_checks.old_hash->joblib.hash(est)
A:sklearn.utils.tests.test_estimator_checks.estimator->SGDClassifier(loss='log_loss')
A:sklearn.utils.tests.test_estimator_checks.lr->LogisticRegression()
A:sklearn.utils.tests.test_estimator_checks.(_, y)->make_multilabel_classification(n_samples=n_samples, n_features=2, n_classes=n_outputs, n_labels=3, length=50, allow_unlabeled=True, random_state=0)
A:sklearn.utils.tests.test_estimator_checks.clf->MultiLabelClassifierDecisionFunction(response_output=y_test)
A:sklearn.utils.tests.test_estimator_checks.response_output->numpy.zeros_like(y_test, dtype=np.int64)
A:sklearn.utils.tests.test_estimator_checks.suite->unittest.TestSuite()
A:sklearn.utils.tests.test_estimator_checks.runner->unittest.TextTestRunner()
A:sklearn.utils.tests.test_estimator_checks.estimators->all_estimators()
A:sklearn.utils.tests.test_estimator_checks.all_tests->list(_yield_all_checks(Estimator()))
A:sklearn.utils.tests.test_estimator_checks.detector->OutlierDetectorWithConstraint()
sklearn.utils.tests.test_estimator_checks.BadBalancedWeightsClassifier(self,class_weight=None)
sklearn.utils.tests.test_estimator_checks.BadBalancedWeightsClassifier.__init__(self,class_weight=None)
sklearn.utils.tests.test_estimator_checks.BadBalancedWeightsClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.BadTransformerWithoutMixin(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.BadTransformerWithoutMixin.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.BadTransformerWithoutMixin.transform(self,X)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier(ClassifierMixin,BaseEstimator)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.BaseBadClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.BrokenArrayAPI(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.BrokenArrayAPI.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.BrokenArrayAPI.predict(self,X)
sklearn.utils.tests.test_estimator_checks.ChangesDict(self,key=0)
sklearn.utils.tests.test_estimator_checks.ChangesDict.__init__(self,key=0)
sklearn.utils.tests.test_estimator_checks.ChangesDict.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.ChangesDict.predict(self,X)
sklearn.utils.tests.test_estimator_checks.ChangesUnderscoreAttribute(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.ChangesUnderscoreAttribute.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute(self,wrong_attribute=0)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute.__init__(self,wrong_attribute=0)
sklearn.utils.tests.test_estimator_checks.ChangesWrongAttribute.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedError(ValueError)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.CorrectNotFittedErrorClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.EstimatorInconsistentForPandas(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.EstimatorInconsistentForPandas.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.EstimatorInconsistentForPandas.predict(self,X)
sklearn.utils.tests.test_estimator_checks.EstimatorMissingDefaultTags(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.EstimatorMissingDefaultTags._get_tags(self)
sklearn.utils.tests.test_estimator_checks.HasImmutableParameters(self,p=42,q=np.int32(42),r=object)
sklearn.utils.tests.test_estimator_checks.HasImmutableParameters.__init__(self,p=42,q=np.int32(42),r=object)
sklearn.utils.tests.test_estimator_checks.HasImmutableParameters.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.HasMutableParameters(self,p=object())
sklearn.utils.tests.test_estimator_checks.HasMutableParameters.__init__(self,p=object())
sklearn.utils.tests.test_estimator_checks.HasMutableParameters.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.LargeSparseNotSupportedClassifier(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.LargeSparseNotSupportedClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.ModifiesAnotherValue(self,a=0,b='method1')
sklearn.utils.tests.test_estimator_checks.ModifiesAnotherValue.__init__(self,a=0,b='method1')
sklearn.utils.tests.test_estimator_checks.ModifiesAnotherValue.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.ModifiesAnotherValue.set_params(self,**kwargs)
sklearn.utils.tests.test_estimator_checks.ModifiesValueInsteadOfRaisingError(self,p=0)
sklearn.utils.tests.test_estimator_checks.ModifiesValueInsteadOfRaisingError.__init__(self,p=0)
sklearn.utils.tests.test_estimator_checks.ModifiesValueInsteadOfRaisingError.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.ModifiesValueInsteadOfRaisingError.set_params(self,**kwargs)
sklearn.utils.tests.test_estimator_checks.NoCheckinPredict(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.NoCheckinPredict.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType.fit(self,X,y,sample_weight=None)
sklearn.utils.tests.test_estimator_checks.NoSampleWeightPandasSeriesType.predict(self,X)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier(BaseBadClassifier)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NoSparseClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.NotInvariantPredict(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.NotInvariantPredict.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NotInvariantPredict.predict(self,X)
sklearn.utils.tests.test_estimator_checks.NotInvariantSampleOrder(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.NotInvariantSampleOrder.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.NotInvariantSampleOrder.predict(self,X)
sklearn.utils.tests.test_estimator_checks.OneClassSampleErrorClassifier(self,raise_when_single_class=False)
sklearn.utils.tests.test_estimator_checks.OneClassSampleErrorClassifier.__init__(self,raise_when_single_class=False)
sklearn.utils.tests.test_estimator_checks.OneClassSampleErrorClassifier.fit(self,X,y,sample_weight=None)
sklearn.utils.tests.test_estimator_checks.OneClassSampleErrorClassifier.predict(self,X)
sklearn.utils.tests.test_estimator_checks.PartialFitChecksName(BaseEstimator)
sklearn.utils.tests.test_estimator_checks.PartialFitChecksName.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.PartialFitChecksName.partial_fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.PoorScoreLogisticRegression(LogisticRegression)
sklearn.utils.tests.test_estimator_checks.PoorScoreLogisticRegression._more_tags(self)
sklearn.utils.tests.test_estimator_checks.PoorScoreLogisticRegression.decision_function(self,X)
sklearn.utils.tests.test_estimator_checks.RaisesErrorInSetParams(self,p=0)
sklearn.utils.tests.test_estimator_checks.RaisesErrorInSetParams.__init__(self,p=0)
sklearn.utils.tests.test_estimator_checks.RaisesErrorInSetParams.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.RaisesErrorInSetParams.set_params(self,**kwargs)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveXRegressor(LinearRegression)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveXRegressor._more_tags(self)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveXRegressor.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveYRegressor(LinearRegression)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveYRegressor._more_tags(self)
sklearn.utils.tests.test_estimator_checks.RequiresPositiveYRegressor.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute(self,acceptable_key=0)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute.__init__(self,acceptable_key=0)
sklearn.utils.tests.test_estimator_checks.SetsWrongAttribute.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.SparseTransformer(self,sparse_container=None)
sklearn.utils.tests.test_estimator_checks.SparseTransformer.__init__(self,sparse_container=None)
sklearn.utils.tests.test_estimator_checks.SparseTransformer.fit(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.SparseTransformer.fit_transform(self,X,y=None)
sklearn.utils.tests.test_estimator_checks.SparseTransformer.transform(self,X)
sklearn.utils.tests.test_estimator_checks.TaggedBinaryClassifier(UntaggedBinaryClassifier)
sklearn.utils.tests.test_estimator_checks.TaggedBinaryClassifier._more_tags(self)
sklearn.utils.tests.test_estimator_checks.UntaggedBinaryClassifier(SGDClassifier)
sklearn.utils.tests.test_estimator_checks.UntaggedBinaryClassifier.fit(self,X,y,coef_init=None,intercept_init=None,sample_weight=None)
sklearn.utils.tests.test_estimator_checks.UntaggedBinaryClassifier.partial_fit(self,X,y,classes=None,sample_weight=None)
sklearn.utils.tests.test_estimator_checks._BaseMultiLabelClassifierMock(self,response_output)
sklearn.utils.tests.test_estimator_checks._BaseMultiLabelClassifierMock.__init__(self,response_output)
sklearn.utils.tests.test_estimator_checks._BaseMultiLabelClassifierMock._more_tags(self)
sklearn.utils.tests.test_estimator_checks._BaseMultiLabelClassifierMock.fit(self,X,y)
sklearn.utils.tests.test_estimator_checks.run_tests_without_pytest()
sklearn.utils.tests.test_estimator_checks.test_all_estimators_all_public()
sklearn.utils.tests.test_estimator_checks.test_check_array_api_input()
sklearn.utils.tests.test_estimator_checks.test_check_class_weight_balanced_linear_classifier()
sklearn.utils.tests.test_estimator_checks.test_check_classifier_data_not_an_array()
sklearn.utils.tests.test_estimator_checks.test_check_classifiers_multilabel_output_format_decision_function()
sklearn.utils.tests.test_estimator_checks.test_check_classifiers_multilabel_output_format_predict()
sklearn.utils.tests.test_estimator_checks.test_check_classifiers_multilabel_output_format_predict_proba()
sklearn.utils.tests.test_estimator_checks.test_check_dataframe_column_names_consistency()
sklearn.utils.tests.test_estimator_checks.test_check_estimator()
sklearn.utils.tests.test_estimator_checks.test_check_estimator_clones()
sklearn.utils.tests.test_estimator_checks.test_check_estimator_get_tags_default_keys()
sklearn.utils.tests.test_estimator_checks.test_check_estimator_pairwise()
sklearn.utils.tests.test_estimator_checks.test_check_estimator_transformer_no_mixin()
sklearn.utils.tests.test_estimator_checks.test_check_estimators_unfitted()
sklearn.utils.tests.test_estimator_checks.test_check_fit_check_is_fitted()
sklearn.utils.tests.test_estimator_checks.test_check_fit_score_takes_y_works_on_deprecated_fit()
sklearn.utils.tests.test_estimator_checks.test_check_no_attributes_set_in_init()
sklearn.utils.tests.test_estimator_checks.test_check_outlier_contamination()
sklearn.utils.tests.test_estimator_checks.test_check_outlier_corruption()
sklearn.utils.tests.test_estimator_checks.test_check_regressor_data_not_an_array()
sklearn.utils.tests.test_estimator_checks.test_check_requires_y_none()
sklearn.utils.tests.test_estimator_checks.test_decision_proba_tie_ranking()
sklearn.utils.tests.test_estimator_checks.test_minimal_class_implementation_checks()
sklearn.utils.tests.test_estimator_checks.test_non_deterministic_estimator_skip_tests()
sklearn.utils.tests.test_estimator_checks.test_not_an_array_array_function()
sklearn.utils.tests.test_estimator_checks.test_xfail_ignored_in_check_estimator()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_set_output.py----------------------------------------
A:sklearn.utils.tests.test_set_output.pd->pytest.importorskip('pandas')
A:sklearn.utils.tests.test_set_output.X_np->numpy.array([[1, 0, 3], [0, 0, 1]])
A:sklearn.utils.tests.test_set_output.columns->numpy.asarray(['f0', 'f1', 'f2'], dtype=object)
A:sklearn.utils.tests.test_set_output.index->numpy.asarray([0, 1])
A:sklearn.utils.tests.test_set_output.X_df_orig->pytest.importorskip('polars').DataFrame(X_np, schema=columns, orient='row')
A:sklearn.utils.tests.test_set_output.X_container->adapter.create_container(X_np, X_df_orig, columns=lambda : columns)
A:sklearn.utils.tests.test_set_output.new_columns->numpy.array(['a', 'c', 'g'], dtype=object)
A:sklearn.utils.tests.test_set_output.X_df->pytest.importorskip('pandas').DataFrame([[1, 2], [1, 3]], index=[10, 12])
A:sklearn.utils.tests.test_set_output.new_df->adapter.rename_columns(X_df_orig, new_columns)
A:sklearn.utils.tests.test_set_output.X_df_1->pytest.importorskip('polars').DataFrame([[1, 2, 5], [3, 4, 6]], schema=['a', 'b', 'e'], orient='row')
A:sklearn.utils.tests.test_set_output.X_df_2->pytest.importorskip('polars').DataFrame([[4], [5]], schema=['c'], orient='row')
A:sklearn.utils.tests.test_set_output.X_stacked->adapter.hstack([X_df_1, X_df_2])
A:sklearn.utils.tests.test_set_output.expected_df->pytest.importorskip('polars').DataFrame([[1, 2, 5, 4], [3, 4, 6, 5]], schema=['a', 'b', 'e', 'c'], orient='row')
A:sklearn.utils.tests.test_set_output.pl->pytest.importorskip('polars')
A:sklearn.utils.tests.test_set_output.X->numpy.asarray([[1, 2, 3]])
A:sklearn.utils.tests.test_set_output.X_csr->csr_container(X)
A:sklearn.utils.tests.test_set_output.est->EstimatorWithListInput()
A:sklearn.utils.tests.test_set_output.config->_get_output_config('transform', est)
A:sklearn.utils.tests.test_set_output.lib->pytest.importorskip(dataframe_lib)
A:sklearn.utils.tests.test_set_output.est2->EstimatorWithListInput().set_output(transform=None)
A:sklearn.utils.tests.test_set_output.X_trans_np->EstimatorWithListInput().set_output(transform=None).transform(X)
A:sklearn.utils.tests.test_set_output.X_trans_pd->EstimatorWithListInput().transform(X)
A:sklearn.utils.tests.test_set_output.X_trans->EstimatorWithListInput().transform(X)
A:sklearn.utils.tests.test_set_output.Output->namedtuple('Output', 'X, Y')
A:sklearn.utils.tests.test_set_output.self.n_features_in_->len(X[0])
A:sklearn.utils.tests.test_set_output.X_out->EstimatorWithListInput().fit(X).transform(X)
sklearn.utils.tests.test_set_output.AnotherMixin
sklearn.utils.tests.test_set_output.AnotherMixin.__init_subclass__(cls,custom_parameter,**kwargs)
sklearn.utils.tests.test_set_output.EstimatorNoSetOutputWithTransform
sklearn.utils.tests.test_set_output.EstimatorNoSetOutputWithTransform.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorNoSetOutputWithTransformNoFeatureNamesOut(_SetOutputMixin)
sklearn.utils.tests.test_set_output.EstimatorNoSetOutputWithTransformNoFeatureNamesOut.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorReturnTuple(self,OutputTuple)
sklearn.utils.tests.test_set_output.EstimatorReturnTuple.__init__(self,OutputTuple)
sklearn.utils.tests.test_set_output.EstimatorReturnTuple.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithListInput(_SetOutputMixin)
sklearn.utils.tests.test_set_output.EstimatorWithListInput.fit(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithListInput.get_feature_names_out(self,input_features=None)
sklearn.utils.tests.test_set_output.EstimatorWithListInput.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutput(_SetOutputMixin)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutput.fit(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutput.get_feature_names_out(self,input_features=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutput.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputIndex(_SetOutputMixin)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputIndex.fit(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputIndex.get_feature_names_out(self,input_features=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputIndex.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputNoAutoWrap(_SetOutputMixin,auto_wrap_output_keys=None)
sklearn.utils.tests.test_set_output.EstimatorWithSetOutputNoAutoWrap.transform(self,X,y=None)
sklearn.utils.tests.test_set_output.EstimatorWithoutSetOutputAndWithoutTransform
sklearn.utils.tests.test_set_output.test__container_error_validation(csr_container)
sklearn.utils.tests.test_set_output.test__get_output_config(transform_output)
sklearn.utils.tests.test_set_output.test__safe_set_output()
sklearn.utils.tests.test_set_output.test__safe_set_output_error()
sklearn.utils.tests.test_set_output.test_adapter_class_has_interface(name)
sklearn.utils.tests.test_set_output.test_auto_wrap_output_keys_errors_with_incorrect_input()
sklearn.utils.tests.test_set_output.test_check_library_installed(monkeypatch)
sklearn.utils.tests.test_set_output.test_get_output_auto_wrap_false()
sklearn.utils.tests.test_set_output.test_pandas_adapter()
sklearn.utils.tests.test_set_output.test_polars_adapter()
sklearn.utils.tests.test_set_output.test_set_output_list_input(dataframe_lib)
sklearn.utils.tests.test_set_output.test_set_output_method(dataframe_lib)
sklearn.utils.tests.test_set_output.test_set_output_method_error()
sklearn.utils.tests.test_set_output.test_set_output_mixin()
sklearn.utils.tests.test_set_output.test_set_output_mixin_custom_mixin()
sklearn.utils.tests.test_set_output.test_set_output_mro()
sklearn.utils.tests.test_set_output.test_set_output_named_tuple_out()
sklearn.utils.tests.test_set_output.test_set_output_pandas_keep_index()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_arrayfuncs.py----------------------------------------
A:sklearn.utils.tests.test_arrayfuncs.X->numpy.arange(12, dtype=dtype).reshape(3, 4)
A:sklearn.utils.tests.test_arrayfuncs.min_double->min_pos(X)
A:sklearn.utils.tests.test_arrayfuncs.min_float->min_pos(X.astype(np.float32))
sklearn.utils.tests.test_arrayfuncs.test_all_with_any_reduction_axis_1(dtype,value)
sklearn.utils.tests.test_arrayfuncs.test_min_pos()
sklearn.utils.tests.test_arrayfuncs.test_min_pos_no_positive(dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_plotting.py----------------------------------------
sklearn.utils.tests.test_plotting.metric()
sklearn.utils.tests.test_plotting.neg_metric()
sklearn.utils.tests.test_plotting.test_inverval_max_min_ratio(data,lower_bound,upper_bound)
sklearn.utils.tests.test_plotting.test_validate_score_name(score_name,scoring,negate_score,expected_score_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_murmurhash.py----------------------------------------
A:sklearn.utils.tests.test_murmurhash.rng->numpy.random.RandomState(42)
A:sklearn.utils.tests.test_murmurhash.keys->keys.reshape((3, 2, 1)).reshape((3, 2, 1))
A:sklearn.utils.tests.test_murmurhash.expected->numpy.full(n_bins, 1.0 / n_bins)
A:sklearn.utils.tests.test_murmurhash.previous_hashes->set()
A:sklearn.utils.tests.test_murmurhash.h->murmurhash3_32(' ' * i, 0)
A:sklearn.utils.tests.test_murmurhash.bins->numpy.zeros(n_bins, dtype=np.float64)
sklearn.utils.tests.test_murmurhash.test_mmhash3_bytes()
sklearn.utils.tests.test_murmurhash.test_mmhash3_int()
sklearn.utils.tests.test_murmurhash.test_mmhash3_int_array()
sklearn.utils.tests.test_murmurhash.test_mmhash3_unicode()
sklearn.utils.tests.test_murmurhash.test_no_collision_on_byte_range()
sklearn.utils.tests.test_murmurhash.test_uniform_distribution()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_extmath.py----------------------------------------
A:sklearn.utils.tests.test_extmath.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_extmath.X->numpy.random.RandomState(0).randn(3, 5)
A:sklearn.utils.tests.test_extmath.x->numpy.linspace(-2, 2, 50)
A:sklearn.utils.tests.test_extmath.weights->numpy.ones(x.shape)
A:sklearn.utils.tests.test_extmath.(mode, score)->weighted_mode(x, w, axis=1)
A:sklearn.utils.tests.test_extmath.(mode2, score2)->weighted_mode(x, weights, axis=axis)
A:sklearn.utils.tests.test_extmath.w->numpy.random.RandomState(0).random_sample(x.shape)
A:sklearn.utils.tests.test_extmath.dtype->numpy.dtype(dtype)
A:sklearn.utils.tests.test_extmath.(U, s, Vt)->randomized_svd(X, n_components, n_iter=i, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.U->U.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.s->s.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.Vt->Vt.astype(dtype, copy=False).astype(dtype, copy=False)
A:sklearn.utils.tests.test_extmath.(Ua, sa, Va)->randomized_svd(X, k, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.(eigvals, eigvecs)->_randomized_eigsh(X, n_components=k, selection='module', n_iter=25, random_state=0)
A:sklearn.utils.tests.test_extmath.(eigvals_qr, eigvecs_qr)->_randomized_eigsh(X, n_components=k, n_iter=25, n_oversamples=20, random_state=0, power_iteration_normalizer='QR', selection='module')
A:sklearn.utils.tests.test_extmath.(eigvals_lapack, eigvecs_lapack)->eigh(X, subset_by_index=(n_features - k, n_features - 1))
A:sklearn.utils.tests.test_extmath.(eigvecs, _)->svd_flip(eigvecs, dummy_vecs)
A:sklearn.utils.tests.test_extmath.(eigvecs_qr, _)->svd_flip(eigvecs_qr, dummy_vecs)
A:sklearn.utils.tests.test_extmath.(eigvecs_lapack, _)->svd_flip(eigvecs_lapack, dummy_vecs)
A:sklearn.utils.tests.test_extmath.v0->_init_arpack_v0(n_features, random_state=0)
A:sklearn.utils.tests.test_extmath.(eigvals_arpack, eigvecs_arpack)->eigsh(X, k, which='LA', tol=0, maxiter=None, v0=v0)
A:sklearn.utils.tests.test_extmath.(eigvecs_arpack, _)->svd_flip(eigvecs_arpack, dummy_vecs)
A:sklearn.utils.tests.test_extmath.(S, V)->_randomized_eigsh(A, n_components=rank, random_state=rng)
A:sklearn.utils.tests.test_extmath.sq_norm->(X ** 2).sum(axis=1)
A:sklearn.utils.tests.test_extmath.Xcsr->csr_container(X, dtype=dtype)
A:sklearn.utils.tests.test_extmath.Xcsr.indptr->csr_container(X, dtype=dtype).indptr.astype(csr_index_dtype, copy=False)
A:sklearn.utils.tests.test_extmath.Xcsr.indices->csr_container(X, dtype=dtype).indices.astype(csr_index_dtype, copy=False)
A:sklearn.utils.tests.test_extmath.(_, s, _)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.utils.tests.test_extmath.(_, sa, _)->randomized_svd(X, k, n_iter=0, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.(_, sap, _)->randomized_svd(X, k, n_iter=5, power_iteration_normalizer=normalizer, random_state=0)
A:sklearn.utils.tests.test_extmath.(U1, s1, V1)->randomized_svd(X, k, n_iter=3, transpose=False, random_state=0)
A:sklearn.utils.tests.test_extmath.(U2, s2, V2)->randomized_svd(X, k, n_iter=3, transpose=True, random_state=0)
A:sklearn.utils.tests.test_extmath.(U3, s3, V3)->randomized_svd(X, k, n_iter=3, transpose='auto', random_state=0)
A:sklearn.utils.tests.test_extmath.(U4, s4, V4)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.utils.tests.test_extmath.error_2->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.error_20->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.error->scipy.linalg.norm(A, ord='fro')
A:sklearn.utils.tests.test_extmath.warn_msg->'Calculating SVD of a {} is expensive. csr_matrix is more efficient.'.format(sparse_container.__name__)
A:sklearn.utils.tests.test_extmath.rs->numpy.random.RandomState(global_random_seed)
A:sklearn.utils.tests.test_extmath.(U, S, Vt)->scipy.linalg.svd(XT, full_matrices=False)
A:sklearn.utils.tests.test_extmath.(U1, V1)->svd_flip(U, Vt, u_based_decision=False)
A:sklearn.utils.tests.test_extmath.(U2, V2)->svd_flip(U, Vt, u_based_decision=True)
A:sklearn.utils.tests.test_extmath.(U_flip1, V_flip1)->svd_flip(U, Vt, u_based_decision=True)
A:sklearn.utils.tests.test_extmath.(U_flip2, V_flip2)->svd_flip(U, Vt, u_based_decision=False)
A:sklearn.utils.tests.test_extmath.(U, _, Vt)->scipy.linalg.svd(X, full_matrices=False)
A:sklearn.utils.tests.test_extmath.(U1, _)->svd_flip(U, Vt, u_based_decision=True)
A:sklearn.utils.tests.test_extmath.max_abs_U1_row_idx_for_col->numpy.argmax(np.abs(U1), axis=0)
A:sklearn.utils.tests.test_extmath.(_, V2)->svd_flip(U, Vt, u_based_decision=False)
A:sklearn.utils.tests.test_extmath.max_abs_V2_col_idx_for_row->numpy.argmax(np.abs(V2), axis=1)
A:sklearn.utils.tests.test_extmath.a->numpy.array([[2.0, 0.0], [0.0, 1.0]])
A:sklearn.utils.tests.test_extmath.(u1, s1, v1)->randomized_svd(a, 2, flip_sign=True, random_state=41)
A:sklearn.utils.tests.test_extmath.(u2, s2, v2)->randomized_svd(a, 2, flip_sign=True, random_state=seed)
A:sklearn.utils.tests.test_extmath.u_based->(np.abs(u).max(axis=0) == u.max(axis=0)).all()
A:sklearn.utils.tests.test_extmath.v_based->(np.abs(v).max(axis=1) == v.max(axis=1)).all()
A:sklearn.utils.tests.test_extmath.mat->numpy.arange(10 * 8).reshape(10, -1)
A:sklearn.utils.tests.test_extmath.(u_flipped, _, v_flipped)->randomized_svd(mat, 3, flip_sign=True, random_state=0)
A:sklearn.utils.tests.test_extmath.(u_based, v_based)->max_loading_is_positive(u_flipped_with_transpose, v_flipped_with_transpose)
A:sklearn.utils.tests.test_extmath.(u_flipped_with_transpose, _, v_flipped_with_transpose)->randomized_svd(mat, 3, flip_sign=True, transpose=True, random_state=0)
A:sklearn.utils.tests.test_extmath.(u1, s1, vt1)->randomized_svd(X, k, svd_lapack_driver='gesdd', random_state=0)
A:sklearn.utils.tests.test_extmath.(u2, s2, vt2)->randomized_svd(X, k, svd_lapack_driver='gesvd', random_state=0)
A:sklearn.utils.tests.test_extmath.true_out->numpy.array([[1, 4, 6], [1, 4, 7], [1, 5, 6], [1, 5, 7], [2, 4, 6], [2, 4, 7], [2, 5, 6], [2, 5, 7], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7]])
A:sklearn.utils.tests.test_extmath.out->cartesian(axes)
A:sklearn.utils.tests.test_extmath.output->cartesian(arrays)
A:sklearn.utils.tests.test_extmath.extreme_x->numpy.array([-100.0, 100.0])
A:sklearn.utils.tests.test_extmath.(mean, var, _)->_incremental_mean_and_var(X, 0, 0, 0, sample_weight=sample_weight)
A:sklearn.utils.tests.test_extmath.expected_mean->_safe_accumulator_op(np.mean, X, axis=0)
A:sklearn.utils.tests.test_extmath.(last_mean, last_var, last_weight_sum)->_incremental_mean_and_var(X[batch], last_mean, last_var, last_weight_sum, sample_weight=sample_weight[batch])
A:sklearn.utils.tests.test_extmath.weight->numpy.random.RandomState(0).normal(loc=weight_loc, scale=weight_scale, size=size[0])
A:sklearn.utils.tests.test_extmath.expected_var->_safe_accumulator_op(np.var, X, axis=0)
A:sklearn.utils.tests.test_extmath.ones_weight->numpy.ones(size[0])
A:sklearn.utils.tests.test_extmath.old_means->numpy.array([535.0, 535.0, 535.0, 535.0])
A:sklearn.utils.tests.test_extmath.old_variances->numpy.array([4225.0, 4225.0, 4225.0, 4225.0])
A:sklearn.utils.tests.test_extmath.old_weight_sum->numpy.array([2, 2, 2, 2], dtype=np.int32)
A:sklearn.utils.tests.test_extmath.sample_weights_X->numpy.ones(3)
A:sklearn.utils.tests.test_extmath.sample_weights_X_nan->numpy.ones(4)
A:sklearn.utils.tests.test_extmath.X_nan->numpy.array([[170, np.nan, 170, 170], [np.nan, 170, 430, 430], [430, 430, np.nan, 300], [300, 300, 300, np.nan]])
A:sklearn.utils.tests.test_extmath.(X_means, X_variances, X_count)->_incremental_mean_and_var(X, old_means, old_variances, old_sample_count)
A:sklearn.utils.tests.test_extmath.(X_nan_means, X_nan_variances, X_nan_count)->_incremental_mean_and_var(X_nan, old_means, old_variances, old_sample_count)
A:sklearn.utils.tests.test_extmath.old_sample_count->numpy.array([2, 2, 2, 2], dtype=np.int32)
A:sklearn.utils.tests.test_extmath.(final_means, final_variances, final_count)->_incremental_mean_and_var(X2, old_means, old_variances, old_sample_count)
A:sklearn.utils.tests.test_extmath.mean->numpy.random.RandomState(0).randn(3, 5).mean(axis=0)
A:sklearn.utils.tests.test_extmath.Y->numpy.random.RandomState(0).randn(3, 5).copy()
A:sklearn.utils.tests.test_extmath.x1->numpy.array(100000000.0, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.x2->numpy.log(1e-05, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.A0->numpy.full((n_samples // 2, n_features), x1, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.A1->numpy.full((n_samples // 2, n_features), x2, dtype=np.float64)
A:sklearn.utils.tests.test_extmath.A->scipy.sparse.random(30, 10, density=0.1, random_state=rng)
A:sklearn.utils.tests.test_extmath.(mean, var, n)->_incremental_mean_and_var(A1[i, :].reshape((1, A1.shape[1])), mean, var, n)
A:sklearn.utils.tests.test_extmath.n->numpy.full(n_features, n_samples // 2, dtype=np.int32)
A:sklearn.utils.tests.test_extmath.steps->numpy.hstack([steps, n_samples])
A:sklearn.utils.tests.test_extmath.incremental_means->batch.mean(axis=0)
A:sklearn.utils.tests.test_extmath.incremental_variances->batch.var(axis=0)
A:sklearn.utils.tests.test_extmath.sample_count->numpy.full(batch.shape[1], batch.shape[0], dtype=np.int32)
A:sklearn.utils.tests.test_extmath.result->_incremental_mean_and_var(batch, incremental_means, incremental_variances, sample_count)
A:sklearn.utils.tests.test_extmath.calculated_means->numpy.mean(X[:j], axis=0)
A:sklearn.utils.tests.test_extmath.calculated_variances->numpy.var(X[:j], axis=0)
A:sklearn.utils.tests.test_extmath.data->numpy.random.RandomState(36).randn(5, 5)
A:sklearn.utils.tests.test_extmath.max_abs_rows->numpy.argmax(np.abs(data), axis=1)
A:sklearn.utils.tests.test_extmath.data_flipped->_deterministic_vector_sign_flip(data)
A:sklearn.utils.tests.test_extmath.max_rows->numpy.argmax(data_flipped, axis=1)
A:sklearn.utils.tests.test_extmath.signs->numpy.sign(data[range(data.shape[0]), max_abs_rows])
A:sklearn.utils.tests.test_extmath.exp_X->numpy.exp(X)
A:sklearn.utils.tests.test_extmath.sum_exp_X->numpy.sum(exp_X, axis=1).reshape((-1, 1))
A:sklearn.utils.tests.test_extmath.r->numpy.random.RandomState(0).rand(100000)
A:sklearn.utils.tests.test_extmath.B->scipy.sparse.random(10, 20, density=0.1, random_state=rng)
A:sklearn.utils.tests.test_extmath.expected->expected.toarray().toarray()
A:sklearn.utils.tests.test_extmath.actual->safe_sparse_dot(A, B, dense_output=dense_output)
sklearn.utils.tests.test_extmath.rng()
sklearn.utils.tests.test_extmath.test_cartesian()
sklearn.utils.tests.test_extmath.test_cartesian_mix_types(arrays,output_dtype)
sklearn.utils.tests.test_extmath.test_density(sparse_container)
sklearn.utils.tests.test_extmath.test_incremental_mean_and_variance_ignore_nan()
sklearn.utils.tests.test_extmath.test_incremental_variance_ddof()
sklearn.utils.tests.test_extmath.test_incremental_variance_numerical_stability()
sklearn.utils.tests.test_extmath.test_incremental_variance_update_formulas()
sklearn.utils.tests.test_extmath.test_incremental_weighted_mean_and_variance(mean,var,weight_loc,weight_scale,rng)
sklearn.utils.tests.test_extmath.test_incremental_weighted_mean_and_variance_ignore_nan(dtype)
sklearn.utils.tests.test_extmath.test_incremental_weighted_mean_and_variance_simple(rng,dtype)
sklearn.utils.tests.test_extmath.test_logistic_sigmoid()
sklearn.utils.tests.test_extmath.test_random_weights()
sklearn.utils.tests.test_extmath.test_randomized_eigsh(dtype)
sklearn.utils.tests.test_extmath.test_randomized_eigsh_compared_to_others(k)
sklearn.utils.tests.test_extmath.test_randomized_eigsh_reconst_low_rank(n,rank)
sklearn.utils.tests.test_extmath.test_randomized_svd_infinite_rank()
sklearn.utils.tests.test_extmath.test_randomized_svd_lapack_driver(n,m,k,seed)
sklearn.utils.tests.test_extmath.test_randomized_svd_low_rank_all_dtypes(dtype)
sklearn.utils.tests.test_extmath.test_randomized_svd_low_rank_with_noise()
sklearn.utils.tests.test_extmath.test_randomized_svd_power_iteration_normalizer()
sklearn.utils.tests.test_extmath.test_randomized_svd_sign_flip()
sklearn.utils.tests.test_extmath.test_randomized_svd_sign_flip_with_transpose()
sklearn.utils.tests.test_extmath.test_randomized_svd_sparse_warnings(sparse_container)
sklearn.utils.tests.test_extmath.test_randomized_svd_transpose_consistency()
sklearn.utils.tests.test_extmath.test_row_norms(dtype,csr_container)
sklearn.utils.tests.test_extmath.test_safe_sparse_dot_2d(A_container,B_container)
sklearn.utils.tests.test_extmath.test_safe_sparse_dot_2d_1d(container)
sklearn.utils.tests.test_extmath.test_safe_sparse_dot_dense_output(dense_output)
sklearn.utils.tests.test_extmath.test_safe_sparse_dot_nd(csr_container)
sklearn.utils.tests.test_extmath.test_softmax()
sklearn.utils.tests.test_extmath.test_stable_cumsum()
sklearn.utils.tests.test_extmath.test_svd_flip()
sklearn.utils.tests.test_extmath.test_svd_flip_max_abs_cols(n_samples,n_features,global_random_seed)
sklearn.utils.tests.test_extmath.test_uniform_weights()
sklearn.utils.tests.test_extmath.test_vector_sign_flip()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_stats.py----------------------------------------
A:sklearn.utils.tests.test_stats.y->numpy.array([0, 1, 2, 3, 4, 5])
A:sklearn.utils.tests.test_stats.sw->numpy.array([0, 0, 1, 1, 1, 0])
A:sklearn.utils.tests.test_stats.score->_weighted_percentile(y, sw, 100)
A:sklearn.utils.tests.test_stats.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_stats.x->numpy.random.RandomState(0).randint(20, size=10)
A:sklearn.utils.tests.test_stats.weights->numpy.random.RandomState(0).choice(5, size=10)
A:sklearn.utils.tests.test_stats.median->numpy.median(x_manual)
A:sklearn.utils.tests.test_stats.w_median->_weighted_percentile(x_2d, w_2d)
A:sklearn.utils.tests.test_stats.x_manual->numpy.repeat(x, weights)
A:sklearn.utils.tests.test_stats.x1->numpy.random.RandomState(0).randint(10, size=10)
A:sklearn.utils.tests.test_stats.w1->numpy.random.RandomState(0).choice(5, size=10)
A:sklearn.utils.tests.test_stats.x2->numpy.random.RandomState(0).randint(20, size=10)
A:sklearn.utils.tests.test_stats.w2->numpy.random.RandomState(0).choice(5, size=10)
sklearn.utils.tests.test_stats.test_weighted_median_equal_weights()
sklearn.utils.tests.test_stats.test_weighted_median_integer_weights()
sklearn.utils.tests.test_stats.test_weighted_percentile()
sklearn.utils.tests.test_stats.test_weighted_percentile_2d()
sklearn.utils.tests.test_stats.test_weighted_percentile_equal()
sklearn.utils.tests.test_stats.test_weighted_percentile_zero_weight()
sklearn.utils.tests.test_stats.test_weighted_percentile_zero_weight_zero_percentile()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_testing.py----------------------------------------
A:sklearn.utils.tests.test_testing.lda->LinearDiscriminantAnalysis()
A:sklearn.utils.tests.test_testing.tree->DecisionTreeClassifier()
A:sklearn.utils.tests.test_testing.x->numpy.arange(9).reshape(3, 3)
A:sklearn.utils.tests.test_testing.y->csr_container(x)
A:sklearn.utils.tests.test_testing.A->scipy.sparse.diags(np.ones(5), offsets=0).tocsr()
A:sklearn.utils.tests.test_testing.B->csr_container(np.ones((1, 5)))
A:sklearn.utils.tests.test_testing.silence_warnings_func->ignore_warnings(warning_class)(_warning_function)
A:sklearn.utils.tests.test_testing.incorrect->check_docstring_parameters(f)
A:sklearn.utils.tests.test_testing.mock_meta->MockMetaEstimator(delegate=MockEst())
A:sklearn.utils.tests.test_testing.registration_counter->RegistrationCounter()
A:sklearn.utils.tests.test_testing.input_array->numpy.ones(3)
A:sklearn.utils.tests.test_testing.temp_folder->os.path.dirname(data.filename)
A:sklearn.utils.tests.test_testing.data->create_memmap_backed_data(input_array, mmap_mode=mmap_mode)
A:sklearn.utils.tests.test_testing.(data, folder)->create_memmap_backed_data(input_array, return_folder=True)
A:sklearn.utils.tests.test_testing.mmap_data_list->create_memmap_backed_data(input_list)
A:sklearn.utils.tests.test_testing.(output_data, other)->create_memmap_backed_data([input_array, 'not-an-array'])
A:sklearn.utils.tests.test_testing.container_type->container_type()
A:sklearn.utils.tests.test_testing.container_converted->_convert_container(container, constructor_name, dtype=dtype)
A:sklearn.utils.tests.test_testing.df->_convert_container([['x']], 'pyarrow', ['A'], categorical_feature_names=['A'])
A:sklearn.utils.tests.test_testing.pl->pytest.importorskip('polars')
A:sklearn.utils.tests.test_testing.pa->pytest.importorskip('pyarrow')
sklearn.utils.tests.test_testing.Klass
sklearn.utils.tests.test_testing.Klass.f_bad_sections(self,X,y)
sklearn.utils.tests.test_testing.Klass.f_missing(self,X,y)
sklearn.utils.tests.test_testing.MockEst(self)
sklearn.utils.tests.test_testing.MockEst.__init__(self)
sklearn.utils.tests.test_testing.MockEst.fit(self,X,y)
sklearn.utils.tests.test_testing.MockEst.predict(self,X)
sklearn.utils.tests.test_testing.MockEst.predict_proba(self,X)
sklearn.utils.tests.test_testing.MockEst.score(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator(self,delegate)
sklearn.utils.tests.test_testing.MockMetaEstimator.__init__(self,delegate)
sklearn.utils.tests.test_testing.MockMetaEstimator.fit(self,X,y)
sklearn.utils.tests.test_testing.MockMetaEstimator.predict(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator.predict_proba(self,X)
sklearn.utils.tests.test_testing.MockMetaEstimator.score(self,X)
sklearn.utils.tests.test_testing.RegistrationCounter(self)
sklearn.utils.tests.test_testing.RegistrationCounter.__init__(self)
sklearn.utils.tests.test_testing.TestWarns(unittest.TestCase)
sklearn.utils.tests.test_testing.TestWarns.test_warn(self)
sklearn.utils.tests.test_testing.check_memmap(input_array,mmap_data,mmap_mode='r')
sklearn.utils.tests.test_testing.f_bad_order(b,a)
sklearn.utils.tests.test_testing.f_bad_sections(a,b)
sklearn.utils.tests.test_testing.f_check_param_definition(a,b,c,d,e)
sklearn.utils.tests.test_testing.f_missing(a,b)
sklearn.utils.tests.test_testing.f_ok(a,b)
sklearn.utils.tests.test_testing.f_too_many_param_docstring(a,b)
sklearn.utils.tests.test_testing.test_assert_allclose_dense_sparse(csr_container)
sklearn.utils.tests.test_testing.test_assert_raise_message()
sklearn.utils.tests.test_testing.test_assert_raises_msg()
sklearn.utils.tests.test_testing.test_check_docstring_parameters()
sklearn.utils.tests.test_testing.test_convert_container(constructor_name,container_type,dtype,superdtype)
sklearn.utils.tests.test_testing.test_convert_container_categories_pandas()
sklearn.utils.tests.test_testing.test_convert_container_categories_polars()
sklearn.utils.tests.test_testing.test_convert_container_categories_pyarrow()
sklearn.utils.tests.test_testing.test_convert_container_raise_when_sparray_not_available(constructor_name,dtype)
sklearn.utils.tests.test_testing.test_create_memmap_backed_data(monkeypatch)
sklearn.utils.tests.test_testing.test_float32_aware_assert_allclose()
sklearn.utils.tests.test_testing.test_ignore_warning()
sklearn.utils.tests.test_testing.test_raises()
sklearn.utils.tests.test_testing.test_set_random_state()
sklearn.utils.tests.test_testing.test_tempmemmap(monkeypatch)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_encode.py----------------------------------------
A:sklearn.utils.tests.test_encode.uniques->_unique(values)
A:sklearn.utils.tests.test_encode.(result, encoded)->_unique(values, return_inverse=True)
A:sklearn.utils.tests.test_encode.encoded->_encode(values, uniques=uniques)
A:sklearn.utils.tests.test_encode.(result, counts)->_unique(values, return_counts=True)
A:sklearn.utils.tests.test_encode.(result, encoded, counts)->_unique(values, return_inverse=True, return_counts=True)
A:sklearn.utils.tests.test_encode.values->numpy.array([np.nan, 'a', 'c', 'c', None, np.nan, None], dtype=object)
A:sklearn.utils.tests.test_encode.diff->_check_unknown(values, known_values=np.array(['a', 'c'], dtype=object))
A:sklearn.utils.tests.test_encode.(diff, valid_mask)->_check_unknown(values, known_values=np.array(['a', 'c'], dtype=object), return_mask=True)
A:sklearn.utils.tests.test_encode.expected_uniques->numpy.array([1, 3, 5, np.nan], dtype=float)
A:sklearn.utils.tests.test_encode.expected_inverse->numpy.array([1, 0, 3, 2, 1, 3])
A:sklearn.utils.tests.test_encode.(uniques, inverse)->_unique(values, return_inverse=True)
A:sklearn.utils.tests.test_encode.(_, inverse)->_unique(values, return_inverse=True)
A:sklearn.utils.tests.test_encode.counts->_get_counts(values, uniques)
sklearn.utils.tests.test_encode._assert_check_unknown(values,uniques,expected_diff,expected_mask)
sklearn.utils.tests.test_encode.test_check_unknown(values,uniques,expected_diff,expected_mask)
sklearn.utils.tests.test_encode.test_check_unknown_missing_values(missing_value,pickle_uniques)
sklearn.utils.tests.test_encode.test_check_unknown_with_both_missing_values()
sklearn.utils.tests.test_encode.test_encode_util(values,expected)
sklearn.utils.tests.test_encode.test_encode_with_check_unknown()
sklearn.utils.tests.test_encode.test_get_counts(values,uniques,expected_counts)
sklearn.utils.tests.test_encode.test_unique_util_missing_values_numeric()
sklearn.utils.tests.test_encode.test_unique_util_missing_values_objects(missing_value,pickle_uniques)
sklearn.utils.tests.test_encode.test_unique_util_with_all_missing_values()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_show_versions.py----------------------------------------
A:sklearn.utils.tests.test_show_versions.sys_info->_get_sys_info()
A:sklearn.utils.tests.test_show_versions.deps_info->_get_deps_info()
A:sklearn.utils.tests.test_show_versions.(out, err)->capsys.readouterr()
A:sklearn.utils.tests.test_show_versions.info->threadpool_info()
sklearn.utils.tests.test_show_versions.test_get_deps_info()
sklearn.utils.tests.test_show_versions.test_get_sys_info()
sklearn.utils.tests.test_show_versions.test_show_versions(capsys)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_mocking.py----------------------------------------
A:sklearn.utils.tests.test_mocking.clf->CheckingClassifier(check_X=sparse.issparse, methods_to_check=methods_to_check)
A:sklearn.utils.tests.test_mocking.X->_safe_indexing(X, first_2_classes)
A:sklearn.utils.tests.test_mocking.y_pred->CheckingClassifier(check_X=sparse.issparse, methods_to_check=methods_to_check).predict(X)
A:sklearn.utils.tests.test_mocking.y_proba->CheckingClassifier(check_X=sparse.issparse, methods_to_check=methods_to_check).predict_proba(X)
A:sklearn.utils.tests.test_mocking.y_decision->CheckingClassifier(check_X=sparse.issparse, methods_to_check=methods_to_check).decision_function(X)
A:sklearn.utils.tests.test_mocking.first_2_classes->numpy.logical_or(y == 0, y == 1)
A:sklearn.utils.tests.test_mocking.y->_safe_indexing(y, first_2_classes)
A:sklearn.utils.tests.test_mocking.X_sparse->csr_container(X)
A:sklearn.utils.tests.test_mocking.sample_weight->numpy.ones(len(X) // 2)
A:sklearn.utils.tests.test_mocking.estimator->_MockEstimatorOnOffPrediction(response_methods=response_methods)
sklearn.utils.tests.test_mocking._fail(x)
sklearn.utils.tests.test_mocking._success(x)
sklearn.utils.tests.test_mocking.iris()
sklearn.utils.tests.test_mocking.test_check_X_on_predict_fail(iris,pred_func)
sklearn.utils.tests.test_mocking.test_check_X_on_predict_success(iris,pred_func)
sklearn.utils.tests.test_mocking.test_check_on_fit_fail(iris,kwargs)
sklearn.utils.tests.test_mocking.test_check_on_fit_success(iris,kwargs)
sklearn.utils.tests.test_mocking.test_checking_classifier(iris,input_type)
sklearn.utils.tests.test_mocking.test_checking_classifier_fit_params(iris)
sklearn.utils.tests.test_mocking.test_checking_classifier_methods_to_check(iris,methods_to_check,predict_method)
sklearn.utils.tests.test_mocking.test_checking_classifier_missing_fit_params(iris)
sklearn.utils.tests.test_mocking.test_checking_classifier_with_params(iris,csr_container)
sklearn.utils.tests.test_mocking.test_mock_estimator_on_off_prediction(iris,response_methods)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_array_api.py----------------------------------------
A:sklearn.utils.tests.test_array_api.pytestmark->pytest.mark.filterwarnings('ignore:The numpy.array_api submodule:UserWarning')
A:sklearn.utils.tests.test_array_api.(xp_out, is_array_api_compliant)->get_namespace(X_xp, X_np)
A:sklearn.utils.tests.test_array_api.X->_NumPyAPIWrapper().asarray([[1, 2, 3], [3, 4, 5]])
A:sklearn.utils.tests.test_array_api.(xp_out, _)->get_namespace(X)
A:sklearn.utils.tests.test_array_api.full_array->xp_out.full(10, fill_value=2.0, device='cpu')
A:sklearn.utils.tests.test_array_api.array_api_compat->pytest.importorskip('array_api_compat')
A:sklearn.utils.tests.test_array_api.X_np->numpy.asarray([[1.3, 4.5]])
A:sklearn.utils.tests.test_array_api.xp->_NumPyAPIWrapper()
A:sklearn.utils.tests.test_array_api.X_xp->_NumPyAPIWrapper().asarray(X_np)
A:sklearn.utils.tests.test_array_api.numpy_array_api->pytest.importorskip('numpy.array_api')
A:sklearn.utils.tests.test_array_api.xp_->pytest.importorskip('numpy.array_api')
A:sklearn.utils.tests.test_array_api.X_converted->_NumPyAPIWrapper().asarray(X, dtype=xp.float32)
A:sklearn.utils.tests.test_array_api.X_new->_asarray_with_order(X, order='F', xp=xp_)
A:sklearn.utils.tests.test_array_api.X_new_np->numpy.asarray(X_new)
A:sklearn.utils.tests.test_array_api.sample_score->_NumPyAPIWrapper().asarray(sample_score, device=device)
A:sklearn.utils.tests.test_array_api.sample_weight->_NumPyAPIWrapper().asarray(sample_weight, device=device)
A:sklearn.utils.tests.test_array_api.result->reduction(xp.asarray(X))
A:sklearn.utils.tests.test_array_api.expected->_NumPyAPIWrapper().asarray(expected)
A:sklearn.utils.tests.test_array_api.X_gpu->_NumPyAPIWrapper().asarray([1.0, 2.0, 3.0])
A:sklearn.utils.tests.test_array_api.X_cpu->_convert_to_numpy(X_torch, xp=torch)
A:sklearn.utils.tests.test_array_api.expected_output->numpy.asarray([1.0, 2.0, 3.0])
A:sklearn.utils.tests.test_array_api.torch->pytest.importorskip('torch')
A:sklearn.utils.tests.test_array_api.X_torch->pytest.importorskip('torch').asarray([1.0, 2.0, 3.0], device='cpu')
A:sklearn.utils.tests.test_array_api.est->SimpleEstimator().fit(X_np)
A:sklearn.utils.tests.test_array_api.new_est->_estimator_with_converted_arrays(est, lambda array: xp.asarray(array))
A:sklearn.utils.tests.test_array_api.X_no_copy->_NumPyAPIWrapper().reshape(X, (-1,), copy=False)
A:sklearn.utils.tests.test_array_api.X_copy->_NumPyAPIWrapper().reshape(X, (6, 1), copy=True)
sklearn.utils.tests.test_array_api.SimpleEstimator(BaseEstimator)
sklearn.utils.tests.test_array_api.SimpleEstimator.fit(self,X,y=None)
sklearn.utils.tests.test_array_api._AdjustableNameAPITestWrapper(self,array_namespace,name)
sklearn.utils.tests.test_array_api._AdjustableNameAPITestWrapper.__init__(self,array_namespace,name)
sklearn.utils.tests.test_array_api.test_array_api_wrapper_astype()
sklearn.utils.tests.test_array_api.test_asarray_with_order(array_api)
sklearn.utils.tests.test_array_api.test_asarray_with_order_ignored()
sklearn.utils.tests.test_array_api.test_convert_estimator_to_array_api()
sklearn.utils.tests.test_array_api.test_convert_estimator_to_ndarray(array_namespace,converter)
sklearn.utils.tests.test_array_api.test_convert_to_numpy_cpu()
sklearn.utils.tests.test_array_api.test_convert_to_numpy_gpu(library)
sklearn.utils.tests.test_array_api.test_get_namespace_array_api()
sklearn.utils.tests.test_array_api.test_get_namespace_array_api_isdtype(wrapper)
sklearn.utils.tests.test_array_api.test_get_namespace_ndarray_creation_device()
sklearn.utils.tests.test_array_api.test_get_namespace_ndarray_default(X)
sklearn.utils.tests.test_array_api.test_get_namespace_ndarray_with_dispatch()
sklearn.utils.tests.test_array_api.test_nan_reductions(library,X,reduction,expected)
sklearn.utils.tests.test_array_api.test_reshape_behavior()
sklearn.utils.tests.test_array_api.test_weighted_sum(array_namespace,device,dtype_name,sample_weight,normalize,expected)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_multiclass.py----------------------------------------
A:sklearn.utils.tests.test_multiclass.multilabel_explicit_zero->numpy.array([[0, 1], [1, 0]])
A:sklearn.utils.tests.test_multiclass.mix_clf_format->product(EXAMPLES['multilabel-indicator'], EXAMPLES['multiclass'] + EXAMPLES['binary'])
A:sklearn.utils.tests.test_multiclass.example->_array_api_for_tests(array_namespace, device).asarray(example, device=device)
A:sklearn.utils.tests.test_multiclass.xp->_array_api_for_tests(array_namespace, device)
A:sklearn.utils.tests.test_multiclass.pd->pytest.importorskip('pandas')
A:sklearn.utils.tests.test_multiclass.y->numpy.array([[1, 0, 0, 1], [2, 2, 0, 1], [1, 3, 0, 1], [4, 2, 0, 1], [2, 0, 0, 1], [1, 3, 0, 1]])
A:sklearn.utils.tests.test_multiclass.y_true->pytest.importorskip('pandas').Series([1, 0, 0, 1, 0, 1, 1, 0, 1], dtype=dtype)
A:sklearn.utils.tests.test_multiclass.y_predicted->pytest.importorskip('pandas').Series([0, 0, 1, 1, 0, 1, 1, 1, 1], dtype='int64')
A:sklearn.utils.tests.test_multiclass.labels->unique_labels(y_true, y_predicted)
A:sklearn.utils.tests.test_multiclass.data->numpy.array([1, 2, 1, 4, 2, 1, 0, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1])
A:sklearn.utils.tests.test_multiclass.indices->numpy.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 5, 0, 1, 2, 3, 4, 5])
A:sklearn.utils.tests.test_multiclass.indptr->numpy.array([0, 6, 11, 11, 17])
A:sklearn.utils.tests.test_multiclass.y_sp->csc_container((data, indices, indptr), shape=(6, 4))
A:sklearn.utils.tests.test_multiclass.(classes, n_classes, class_prior)->class_distribution(y, [1.0, 2.0, 1.0, 2.0, 1.0, 2.0])
A:sklearn.utils.tests.test_multiclass.(classes_sp, n_classes_sp, class_prior_sp)->class_distribution(y, [1.0, 2.0, 1.0, 2.0, 1.0, 2.0])
A:sklearn.utils.tests.test_multiclass.clf->SVC()
A:sklearn.utils.tests.test_multiclass.clfp->SVC(kernel='precomputed')
A:sklearn.utils.tests.test_multiclass.iris->sklearn.datasets.load_iris()
A:sklearn.utils.tests.test_multiclass.K->numpy.dot(X, X.T)
A:sklearn.utils.tests.test_multiclass.cv->ShuffleSplit(test_size=0.25, random_state=0)
A:sklearn.utils.tests.test_multiclass.(X_train, y_train)->_safe_split(clf, X, y, train)
A:sklearn.utils.tests.test_multiclass.(K_train, y_train2)->_safe_split(clfp, K, y, train)
A:sklearn.utils.tests.test_multiclass.(X_test, y_test)->_safe_split(clf, X, y, test, train)
A:sklearn.utils.tests.test_multiclass.(K_test, y_test2)->_safe_split(clfp, K, y, test, train)
A:sklearn.utils.tests.test_multiclass.predictions->numpy.array([[0, 1, 1], [0, 1, 0], [0, 1, 1], [0, 1, 1]])
A:sklearn.utils.tests.test_multiclass.confidences->numpy.array([[-1e+16, 0, -1e+16], [1.0, 2.0, -3.0], [-5.0, 2.0, 5.0], [-0.5, 0.2, 0.5]])
A:sklearn.utils.tests.test_multiclass.dec_values->_ovr_decision_function(predictions, confidences, n_classes)
A:sklearn.utils.tests.test_multiclass.votes->numpy.array([[1, 0, 2], [1, 1, 1], [1, 0, 2], [1, 0, 2]])
A:sklearn.utils.tests.test_multiclass.expected_prediction->numpy.array([2, 1, 2, 2])
sklearn.utils.tests.test_multiclass._generate_sparse(data,sparse_containers=tuple(COO_CONTAINERS+CSC_CONTAINERS+CSR_CONTAINERS+DOK_CONTAINERS+LIL_CONTAINERS),dtypes=(bool,int,np.int8,np.uint8,float,np.float32))
sklearn.utils.tests.test_multiclass.test_check_classification_targets()
sklearn.utils.tests.test_multiclass.test_class_distribution(csc_container)
sklearn.utils.tests.test_multiclass.test_is_multilabel()
sklearn.utils.tests.test_multiclass.test_is_multilabel_array_api_compliance(array_namespace,device,dtype_name)
sklearn.utils.tests.test_multiclass.test_ovr_decision_function()
sklearn.utils.tests.test_multiclass.test_safe_split_with_precomputed_kernel()
sklearn.utils.tests.test_multiclass.test_type_of_target()
sklearn.utils.tests.test_multiclass.test_type_of_target_pandas_nullable()
sklearn.utils.tests.test_multiclass.test_type_of_target_pandas_sparse()
sklearn.utils.tests.test_multiclass.test_unique_labels()
sklearn.utils.tests.test_multiclass.test_unique_labels_mixed_types()
sklearn.utils.tests.test_multiclass.test_unique_labels_non_specific()
sklearn.utils.tests.test_multiclass.test_unique_labels_pandas_nullable(dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_typedefs.py----------------------------------------
sklearn.utils.tests.test_typedefs.test_types(type_t,value,expected_dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_arpack.py----------------------------------------
A:sklearn.utils.tests.test_arpack.v0->_init_arpack_v0(size, seed)
A:sklearn.utils.tests.test_arpack.rng->check_random_state(seed)
sklearn.utils.tests.test_arpack.test_init_arpack_v0(seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_metaestimators.py----------------------------------------
A:sklearn.utils.tests.test_metaestimators.est->AvailableParameterEstimator(available=True, return_value=return_value)
A:sklearn.utils.tests.test_metaestimators.pickled_bytes->pickle.dumps(est.available_func)
A:sklearn.utils.tests.test_metaestimators.unpickled_func->pickle.loads(pickled_bytes)
sklearn.utils.tests.test_metaestimators.AvailableParameterEstimator(self,available=True,return_value=1)
sklearn.utils.tests.test_metaestimators.AvailableParameterEstimator.__init__(self,available=True,return_value=1)
sklearn.utils.tests.test_metaestimators.AvailableParameterEstimator.available_func(self)
sklearn.utils.tests.test_metaestimators.test_available_if()
sklearn.utils.tests.test_metaestimators.test_available_if_docstring()
sklearn.utils.tests.test_metaestimators.test_available_if_methods_can_be_pickled()
sklearn.utils.tests.test_metaestimators.test_available_if_unbound_method()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_fixes.py----------------------------------------
A:sklearn.utils.tests.test_fixes.X->numpy.array([[val, np.nan], [np.nan, val]], dtype=dtype)
A:sklearn.utils.tests.test_fixes.expected_mask->numpy.array([[False, True], [True, False]])
A:sklearn.utils.tests.test_fixes.mask->_object_dtype_isnan(X)
sklearn.utils.tests.test_fixes.test_delayed_deprecation()
sklearn.utils.tests.test_fixes.test_object_dtype_isnan(dtype,val)
sklearn.utils.tests.test_fixes.test_smallest_admissible_index_dtype_by_checking_contents(params,expected_dtype)
sklearn.utils.tests.test_fixes.test_smallest_admissible_index_dtype_error(params,err_type,err_msg)
sklearn.utils.tests.test_fixes.test_smallest_admissible_index_dtype_max_val(params,expected_dtype)
sklearn.utils.tests.test_fixes.test_smallest_admissible_index_dtype_without_checking_contents(params,expected_dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_sparsefuncs.py----------------------------------------
A:sklearn.utils.tests.test_sparsefuncs.(X, _)->make_classification(5, 4, random_state=0)
A:sklearn.utils.tests.test_sparsefuncs.X_lil->lil_container(X)
A:sklearn.utils.tests.test_sparsefuncs.X_csr->csr_container(X)
A:sklearn.utils.tests.test_sparsefuncs.X_csc->csc_container(X)
A:sklearn.utils.tests.test_sparsefuncs.X_test->scipy.sparse.random(100, 10, format='csr', dtype=dtype, random_state=42).astype(input_dtype)
A:sklearn.utils.tests.test_sparsefuncs.X_sparse->sparse_container(sp.random(500, 100, density=0.1, format='csr', random_state=random_state))
A:sklearn.utils.tests.test_sparsefuncs.(X_means, X_vars)->mean_variance_axis(X_sparse, axis)
A:sklearn.utils.tests.test_sparsefuncs.rng->numpy.random.default_rng(global_random_seed)
A:sklearn.utils.tests.test_sparsefuncs.X->scipy.sparse.random(100, 10, format='csr', dtype=dtype, random_state=42)
A:sklearn.utils.tests.test_sparsefuncs.missing_indices->numpy.random.default_rng(global_random_seed).choice(np.arange(X.shape[0]), 10, replace=False)
A:sklearn.utils.tests.test_sparsefuncs.sample_weight->numpy.random.default_rng(global_random_seed).rand(X.shape[0]).astype(dtype)
A:sklearn.utils.tests.test_sparsefuncs.(_, var)->mean_variance_axis(X, weights=sample_weight, axis=0)
A:sklearn.utils.tests.test_sparsefuncs.Xw_sparse->sparse_constructor(Xw).astype(dtype)
A:sklearn.utils.tests.test_sparsefuncs.last_mean->numpy.zeros(n_features)
A:sklearn.utils.tests.test_sparsefuncs.last_var->numpy.zeros_like(last_mean)
A:sklearn.utils.tests.test_sparsefuncs.last_n->numpy.zeros(X1.shape[1], dtype=np.int64)
A:sklearn.utils.tests.test_sparsefuncs.(means0, vars0, n_incr0)->incr_mean_variance_axis(X=X_sparse, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n, weights=None)
A:sklearn.utils.tests.test_sparsefuncs.(means_w0, vars_w0, n_incr_w0)->incr_mean_variance_axis(X=Xw_sparse, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n, weights=weights)
A:sklearn.utils.tests.test_sparsefuncs.(means_simple, vars_simple)->mean_variance_axis(X=X_sparse, axis=axis)
A:sklearn.utils.tests.test_sparsefuncs.(means1, vars1, n_incr1)->incr_mean_variance_axis(X=X_sparse, axis=axis, last_mean=means0, last_var=vars0, last_n=n_incr0, weights=None)
A:sklearn.utils.tests.test_sparsefuncs.(means_w1, vars_w1, n_incr_w1)->incr_mean_variance_axis(X=Xw_sparse, axis=axis, last_mean=means_w0, last_var=vars_w0, last_n=n_incr_w0, weights=weights)
A:sklearn.utils.tests.test_sparsefuncs.(X_means_incr, X_vars_incr, n_incr)->incr_mean_variance_axis(X_sparse, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n)
A:sklearn.utils.tests.test_sparsefuncs.kwargs->dict(last_mean=last_mean[:-1], last_var=last_var, last_n=last_n)
A:sklearn.utils.tests.test_sparsefuncs.(mean0, var0, _)->incr_mean_variance_axis(X, axis=0, **kwargs)
A:sklearn.utils.tests.test_sparsefuncs.X1->scipy.sparse.random(5, 1, density=0.8, random_state=0).tocsr()
A:sklearn.utils.tests.test_sparsefuncs.X2->scipy.sparse.random(0, 1, density=0.8, random_state=0).tocsr()
A:sklearn.utils.tests.test_sparsefuncs.(updated_mean, updated_var, updated_n)->incr_mean_variance_axis(X2, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n)
A:sklearn.utils.tests.test_sparsefuncs.(last_mean, last_var, last_n)->incr_mean_variance_axis(X1, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n)
A:sklearn.utils.tests.test_sparsefuncs.(_, _, new_n)->incr_mean_variance_axis(X, axis=axis, last_mean=last_mean, last_var=last_var, last_n=last_n)
A:sklearn.utils.tests.test_sparsefuncs.old_means->numpy.array([535.0, 535.0, 535.0, 535.0])
A:sklearn.utils.tests.test_sparsefuncs.old_variances->numpy.array([4225.0, 4225.0, 4225.0, 4225.0])
A:sklearn.utils.tests.test_sparsefuncs.old_sample_count->numpy.array([2, 2, 2, 2], dtype=np.int64)
A:sklearn.utils.tests.test_sparsefuncs.X_nan->sparse_constructor(np.array([[170, np.nan, 170, 170], [np.nan, 170, 430, 430], [430, 430, np.nan, 300], [300, 300, 300, np.nan]]))
A:sklearn.utils.tests.test_sparsefuncs.(X_means, X_vars, X_sample_count)->incr_mean_variance_axis(X, axis=axis, last_mean=old_means.copy(), last_var=old_variances.copy(), last_n=old_sample_count.copy())
A:sklearn.utils.tests.test_sparsefuncs.(X_nan_means, X_nan_vars, X_nan_sample_count)->incr_mean_variance_axis(X_nan, axis=axis, last_mean=old_means.copy(), last_var=old_variances.copy(), last_n=old_sample_count.copy())
A:sklearn.utils.tests.test_sparsefuncs.X_rows->numpy.array([0, 2, 3], dtype=np.intp)
A:sklearn.utils.tests.test_sparsefuncs.out->numpy.ones((6, X.shape[1]), dtype=dtype)
A:sklearn.utils.tests.test_sparsefuncs.out_rows->numpy.array([1, 3, 4], dtype=np.intp)
A:sklearn.utils.tests.test_sparsefuncs.expect->numpy.ones_like(out)
A:sklearn.utils.tests.test_sparsefuncs.expect[out_rows]->X[X_rows, :].toarray()
A:sklearn.utils.tests.test_sparsefuncs.Xr->scipy.sparse.random(100, 10, format='csr', dtype=dtype, random_state=42).tocsr()
A:sklearn.utils.tests.test_sparsefuncs.Xc->scipy.sparse.random(100, 10, format='csr', dtype=dtype, random_state=42).tocsc()
A:sklearn.utils.tests.test_sparsefuncs.XA->scipy.sparse.random(100, 10, format='csr', dtype=dtype, random_state=42).toarray()
A:sklearn.utils.tests.test_sparsefuncs.scale->scale.astype(np.float32).astype(np.float32)
A:sklearn.utils.tests.test_sparsefuncs.swap->scipy.linalg.get_blas_funcs(('swap',), (X,))
A:sklearn.utils.tests.test_sparsefuncs.(X[0], X[-1])->swap(X[0], X[-1])
A:sklearn.utils.tests.test_sparsefuncs.(X[2], X[3])->swap(X[2], X[3])
A:sklearn.utils.tests.test_sparsefuncs.(X[:, 0], X[:, -1])->swap(X[:, 0], X[:, -1])
A:sklearn.utils.tests.test_sparsefuncs.(X[:, 0], X[:, 1])->swap(X[:, 0], X[:, 1])
A:sklearn.utils.tests.test_sparsefuncs.X_sparse.indices->sparse_container(sp.random(500, 100, density=0.1, format='csr', random_state=random_state)).indices.astype('int64')
A:sklearn.utils.tests.test_sparsefuncs.X_sparse.indptr->sparse_container(sp.random(500, 100, density=0.1, format='csr', random_state=random_state)).indptr.astype('int64')
A:sklearn.utils.tests.test_sparsefuncs.(mins_sparse, maxs_sparse)->min_max_axis(X_sparse, axis=axis, ignore_nan=ignore_nan)
A:sklearn.utils.tests.test_sparsefuncs.X_csr.indices->csr_container(X).indices.astype(index_dtype)
A:sklearn.utils.tests.test_sparsefuncs.X_csr.indptr->csr_container(X).indptr.astype(index_dtype)
A:sklearn.utils.tests.test_sparsefuncs.dense_median->numpy.median(X, axis=0)
A:sklearn.utils.tests.test_sparsefuncs.csc->csc_container(X)
A:sklearn.utils.tests.test_sparsefuncs.sparse_median->csc_median_axis_0(csc)
A:sklearn.utils.tests.test_sparsefuncs.ind->numpy.random.default_rng(global_random_seed).randint(0, 50, 10)
A:sklearn.utils.tests.test_sparsefuncs.ones->numpy.ones(10)
A:sklearn.utils.tests.test_sparsefuncs.rs->RandomState(10)
A:sklearn.utils.tests.test_sparsefuncs.norms->csr_row_norms(X)
A:sklearn.utils.tests.test_sparsefuncs.random_state->numpy.random.default_rng(42)
A:sklearn.utils.tests.test_sparsefuncs.X_dense->sparse_container(sp.random(500, 100, density=0.1, format='csr', random_state=random_state)).toarray()
A:sklearn.utils.tests.test_sparsefuncs.mu->numpy.asarray(X_sparse.mean(axis=0)).ravel()
A:sklearn.utils.tests.test_sparsefuncs.X_sparse_centered->_implicit_column_offset(X_sparse, mu)
A:sklearn.utils.tests.test_sparsefuncs.Y->numpy.random.default_rng(global_random_seed).standard_normal((X_dense_centered.shape[0], 50))
A:sklearn.utils.tests.test_sparsefuncs.y->numpy.random.default_rng(global_random_seed).standard_normal(X_dense_centered.shape[0])
sklearn.utils.tests.test_sparsefuncs.centered_matrices(request)
sklearn.utils.tests.test_sparsefuncs.test_count_nonzero(csc_container,csr_container)
sklearn.utils.tests.test_sparsefuncs.test_csc_row_median(csc_container,csr_container)
sklearn.utils.tests.test_sparsefuncs.test_csr_row_norms(dtype)
sklearn.utils.tests.test_sparsefuncs.test_densify_rows(csr_container)
sklearn.utils.tests.test_sparsefuncs.test_implicit_center_matmat(global_random_seed,centered_matrices)
sklearn.utils.tests.test_sparsefuncs.test_implicit_center_matvec(global_random_seed,centered_matrices)
sklearn.utils.tests.test_sparsefuncs.test_implicit_center_rmatmat(global_random_seed,centered_matrices)
sklearn.utils.tests.test_sparsefuncs.test_implit_center_rmatvec(global_random_seed,centered_matrices)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis(csc_container,csr_container,lil_container)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis_dim_mismatch(sparse_constructor)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis_equivalence_mean_variance(X1,X2,csr_container)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis_ignore_nan(axis,sparse_constructor)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis_weighted_axis0(Xw,X,weights,sparse_constructor,dtype)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_axis_weighted_axis1(Xw,X,weights,sparse_constructor,dtype)
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_n_float()
sklearn.utils.tests.test_sparsefuncs.test_incr_mean_variance_no_new_n()
sklearn.utils.tests.test_sparsefuncs.test_inplace_column_scale()
sklearn.utils.tests.test_sparsefuncs.test_inplace_normalize(csr_container,inplace_csr_row_normalize)
sklearn.utils.tests.test_sparsefuncs.test_inplace_row_scale()
sklearn.utils.tests.test_sparsefuncs.test_inplace_swap_column(csc_container,csr_container)
sklearn.utils.tests.test_sparsefuncs.test_inplace_swap_row(csc_container,csr_container)
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_axis0(csc_container,csr_container,lil_container)
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_axis0_precision(dtype,sparse_constructor)
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_axis1(csc_container,csr_container,lil_container)
sklearn.utils.tests.test_sparsefuncs.test_mean_variance_illegal_axis(csr_container)
sklearn.utils.tests.test_sparsefuncs.test_min_max(dtype,axis,sparse_format,missing_values,min_func,max_func,ignore_nan,large_indices)
sklearn.utils.tests.test_sparsefuncs.test_min_max_axis_errors(csc_container,csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_cython_blas.py----------------------------------------
A:sklearn.utils.tests.test_cython_blas.cython->pytest.importorskip('cython')
A:sklearn.utils.tests.test_cython_blas.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_cython_blas.x->numpy.random.RandomState(0).random_sample(10).astype(dtype, copy=False)
A:sklearn.utils.tests.test_cython_blas.y->numpy.random.RandomState(0).random_sample(20).astype(dtype, copy=False)
A:sklearn.utils.tests.test_cython_blas.expected->expected_rotg(a, b)
A:sklearn.utils.tests.test_cython_blas.actual->rotg(a, b, c, s)
A:sklearn.utils.tests.test_cython_blas.a->dtype(rng.randn())
A:sklearn.utils.tests.test_cython_blas.b->dtype(rng.randn())
A:sklearn.utils.tests.test_cython_blas.c->dtype(rng.randn())
A:sklearn.utils.tests.test_cython_blas.s->dtype(rng.randn())
A:sklearn.utils.tests.test_cython_blas.A->numpy.asarray(opA(rng.random_sample((30, 10)).astype(dtype, copy=False)), order=ORDER[order])
A:sklearn.utils.tests.test_cython_blas.B->numpy.asarray(opB(rng.random_sample((10, 20)).astype(dtype, copy=False)), order=ORDER[order])
A:sklearn.utils.tests.test_cython_blas.C->numpy.asarray(rng.random_sample((30, 20)).astype(dtype, copy=False), order=ORDER[order])
sklearn.utils.tests.test_cython_blas._no_op(x)
sklearn.utils.tests.test_cython_blas._numpy_to_cython(dtype)
sklearn.utils.tests.test_cython_blas.test_asum(dtype)
sklearn.utils.tests.test_cython_blas.test_axpy(dtype)
sklearn.utils.tests.test_cython_blas.test_copy(dtype)
sklearn.utils.tests.test_cython_blas.test_dot(dtype)
sklearn.utils.tests.test_cython_blas.test_gemm(dtype,opA,transA,opB,transB,order)
sklearn.utils.tests.test_cython_blas.test_gemv(dtype,opA,transA,order)
sklearn.utils.tests.test_cython_blas.test_ger(dtype,order)
sklearn.utils.tests.test_cython_blas.test_nrm2(dtype)
sklearn.utils.tests.test_cython_blas.test_rot(dtype)
sklearn.utils.tests.test_cython_blas.test_rotg(dtype)
sklearn.utils.tests.test_cython_blas.test_scal(dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_utils.py----------------------------------------
A:sklearn.utils.tests.test_utils.X_toy->numpy.arange(9).reshape((3, 3))
A:sklearn.utils.tests.test_utils.rng_42->numpy.random.RandomState(42)
A:sklearn.utils.tests.test_utils.spam->ham()
A:sklearn.utils.tests.test_utils.ham->Ham()
A:sklearn.utils.tests.test_utils.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_utils.X->_convert_container(X_array, array_type)
A:sklearn.utils.tests.test_utils.y->numpy.random.RandomState(0).randint(0, 2, size=n_samples)
A:sklearn.utils.tests.test_utils.(_, y_not_stratified)->resample(X, y, n_samples=10, random_state=0, stratify=None)
A:sklearn.utils.tests.test_utils.(_, y_stratified)->resample(X, y, n_samples=10, random_state=0, stratify=y)
A:sklearn.utils.tests.test_utils.(X_replace, _)->resample(X, y, replace=True, n_samples=1000, random_state=rng, stratify=y)
A:sklearn.utils.tests.test_utils.(X_no_replace, _)->resample(X, y, replace=False, n_samples=50, random_state=rng, stratify=y)
A:sklearn.utils.tests.test_utils.(X, y)->resample(X, y, n_samples=50, random_state=rng, stratify=stratify)
A:sklearn.utils.tests.test_utils.stratify->csr_container(y.reshape(-1, 1))
A:sklearn.utils.tests.test_utils.random_state->check_random_state(0)
A:sklearn.utils.tests.test_utils.X_csr->csr_container(X)
A:sklearn.utils.tests.test_utils.mask->safe_mask(X_csr, mask)
A:sklearn.utils.tests.test_utils.array->_convert_container([[1, 2, 3], [4, 5, 6], [7, 8, 9]], array_type, columns_name)
A:sklearn.utils.tests.test_utils.indices->_convert_container(indices, indices_type)
A:sklearn.utils.tests.test_utils.subset->_safe_indexing(X, [0, 1], axis=0)
A:sklearn.utils.tests.test_utils.indices_converted->_convert_container(indices_converted, indices_type)
A:sklearn.utils.tests.test_utils.expected_array->_convert_container(expected_output, expected_output_type)
A:sklearn.utils.tests.test_utils.X_subset->_safe_indexing(X, None, axis=0)
A:sklearn.utils.tests.test_utils.pd->pytest.importorskip('pandas', minversion='1.5')
A:sklearn.utils.tests.test_utils.X_constructor->pytest.importorskip('pandas', minversion='1.5').Series(X)
A:sklearn.utils.tests.test_utils.X_df->pytest.importorskip('pandas', minversion='1.5').DataFrame(X_toy, columns=['col_0', 'col_1', 'col_2'])
A:sklearn.utils.tests.test_utils.toy->numpy.zeros((1, 5), dtype=int)
A:sklearn.utils.tests.test_utils.err_msg->'Selected columns, {}, are not unique in dataframe'.format(key)
A:sklearn.utils.tests.test_utils.A->numpy.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
A:sklearn.utils.tests.test_utils.S->set(to_tuple(A))
A:sklearn.utils.tests.test_utils.b->numpy.array(['a', 'b', 'c'], dtype=object)
A:sklearn.utils.tests.test_utils.d->MockDataFrame(np.array([['a', 0], ['b', 1], ['c', 2]], dtype=object))
A:sklearn.utils.tests.test_utils.e->csc_container(np.arange(6).reshape(3, 2))
A:sklearn.utils.tests.test_utils.(a_s, b_s, c_s, d_s, e_s)->shuffle(a, b, c, d, e, random_state=0)
A:sklearn.utils.tests.test_utils.some_range->range(10)
A:sklearn.utils.tests.test_utils.joined_range->list(chain(*[some_range[slice] for slice in gen_even_slices(10, 3)]))
A:sklearn.utils.tests.test_utils.actual->get_chunk_n_rows(row_bytes=row_bytes, max_n_rows=max_n_rows)
A:sklearn.utils.tests.test_utils.out->_safe_indexing(df, key, axis=0)
A:sklearn.utils.tests.test_utils.ret->_approximate_mode(class_counts=X, n_draws=25000, rng=0)
A:sklearn.utils.tests.test_utils.X_array->numpy.random.RandomState(0).randn(10, 5)
A:sklearn.utils.tests.test_utils.values->numpy.random.RandomState(0).randn(*X.shape)
A:sklearn.utils.tests.test_utils.assigned_portion->_safe_indexing(X, column_indexer, axis=1)
A:sklearn.utils.tests.test_utils.df->pytest.importorskip('polars', minversion='0.18.2').DataFrame({'a': [1, 2, 3, 4], 'b': [4, 5, 6, 8], 'c': [1, 4, 1, 10]}, orient='row')
A:sklearn.utils.tests.test_utils.df_mocked->MockDataFrame(df)
A:sklearn.utils.tests.test_utils.pl->pytest.importorskip('polars', minversion='0.18.2')
sklearn.utils.tests.test_utils.dummy_func()
sklearn.utils.tests.test_utils.test__is_polars_df()
sklearn.utils.tests.test_utils.test_approximate_mode()
sklearn.utils.tests.test_utils.test_column_or_1d()
sklearn.utils.tests.test_utils.test_deprecated()
sklearn.utils.tests.test_utils.test_deprecation_joblib_api(tmpdir)
sklearn.utils.tests.test_utils.test_determine_key_type(key,dtype)
sklearn.utils.tests.test_utils.test_determine_key_type_error()
sklearn.utils.tests.test_utils.test_determine_key_type_slice_error()
sklearn.utils.tests.test_utils.test_gen_even_slices()
sklearn.utils.tests.test_utils.test_get_chunk_n_rows(row_bytes,max_n_rows,working_memory,expected)
sklearn.utils.tests.test_utils.test_get_chunk_n_rows_warns()
sklearn.utils.tests.test_utils.test_get_column_indices_error(key,err_msg)
sklearn.utils.tests.test_utils.test_get_column_indices_interchange()
sklearn.utils.tests.test_utils.test_get_column_indices_pandas_nonunique_columns_error(key)
sklearn.utils.tests.test_utils.test_is_scalar_nan(value,result)
sklearn.utils.tests.test_utils.test_make_rng()
sklearn.utils.tests.test_utils.test_message_with_time(source,message,is_long,time,time_str)
sklearn.utils.tests.test_utils.test_polars_indexing()
sklearn.utils.tests.test_utils.test_print_elapsed_time(message,expected,capsys,monkeypatch)
sklearn.utils.tests.test_utils.test_resample()
sklearn.utils.tests.test_utils.test_resample_stratified()
sklearn.utils.tests.test_utils.test_resample_stratified_replace()
sklearn.utils.tests.test_utils.test_resample_stratify_2dy()
sklearn.utils.tests.test_utils.test_resample_stratify_sparse_error(csr_container)
sklearn.utils.tests.test_utils.test_safe_assign(array_type)
sklearn.utils.tests.test_utils.test_safe_indexing_1d_array_error(X_constructor)
sklearn.utils.tests.test_utils.test_safe_indexing_1d_container(array_type,indices_type)
sklearn.utils.tests.test_utils.test_safe_indexing_1d_container_mask(array_type,indices_type)
sklearn.utils.tests.test_utils.test_safe_indexing_1d_scalar(array_type)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_container_axis_0(array_type,indices_type)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_container_axis_1(array_type,indices_type,indices)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_mask(array_type,indices_type,axis,expected_subset)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_read_only_axis_1(array_read_only,indices_read_only,array_type,indices_type,axis,expected_array)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_scalar_axis_0(array_type,expected_output_type)
sklearn.utils.tests.test_utils.test_safe_indexing_2d_scalar_axis_1(array_type,expected_output_type,indices)
sklearn.utils.tests.test_utils.test_safe_indexing_None_axis_0(array_type)
sklearn.utils.tests.test_utils.test_safe_indexing_container_axis_0_unsupported_type()
sklearn.utils.tests.test_utils.test_safe_indexing_error_axis(axis)
sklearn.utils.tests.test_utils.test_safe_indexing_pandas_no_matching_cols_error()
sklearn.utils.tests.test_utils.test_safe_indexing_pandas_no_settingwithcopy_warning()
sklearn.utils.tests.test_utils.test_safe_mask(csr_container)
sklearn.utils.tests.test_utils.test_shuffle_dont_convert_to_array(csc_container)
sklearn.utils.tests.test_utils.test_shuffle_on_ndim_equals_three()
sklearn.utils.tests.test_utils.test_to_object_array(sequence)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_cython_templating.py----------------------------------------
A:sklearn.utils.tests.test_cython_templating.ignored_files->gitignore_file.read_text().split('\n')
A:sklearn.utils.tests.test_cython_templating.filename->filename.relative_to(base_dir.parent).relative_to(base_dir.parent)
A:sklearn.utils.tests.test_cython_templating.filename_wo_tempita_suffix->filename.relative_to(base_dir.parent).relative_to(base_dir.parent).with_suffix('')
sklearn.utils.tests.test_cython_templating.test_files_generated_by_templates_are_git_ignored()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_tags.py----------------------------------------
sklearn.utils.tests.test_tags.MoreTagsEstimator
sklearn.utils.tests.test_tags.MoreTagsEstimator._more_tags(self)
sklearn.utils.tests.test_tags.NoTagsEstimator
sklearn.utils.tests.test_tags.test_safe_tags_error(estimator,err_msg)
sklearn.utils.tests.test_tags.test_safe_tags_no_get_tags(estimator,key,expected_results)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_shortest_path.py----------------------------------------
A:sklearn.utils.tests.test_shortest_path.graph->numpy.minimum(graph, graph.T)
A:sklearn.utils.tests.test_shortest_path.graph[i, j]->min(graph[i, j], graph[i, k] + graph[k, j])
A:sklearn.utils.tests.test_shortest_path.rng->numpy.random.RandomState(0)
A:sklearn.utils.tests.test_shortest_path.dist_matrix->numpy.minimum(dist_matrix, dist_matrix.T)
A:sklearn.utils.tests.test_shortest_path.graph_py->floyd_warshall_slow(dist_matrix.copy(), directed)
A:sklearn.utils.tests.test_shortest_path.dist_dict->defaultdict(int)
sklearn.utils.tests.test_shortest_path.floyd_warshall_slow(graph,directed=False)
sklearn.utils.tests.test_shortest_path.generate_graph(N=20)
sklearn.utils.tests.test_shortest_path.test_shortest_path()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_seq_dataset.py----------------------------------------
A:sklearn.utils.tests.test_seq_dataset.iris->load_iris()
A:sklearn.utils.tests.test_seq_dataset.X64->load_iris().data.astype(np.float64)
A:sklearn.utils.tests.test_seq_dataset.y64->load_iris().target.astype(np.float64)
A:sklearn.utils.tests.test_seq_dataset.sample_weight64->numpy.arange(y64.size, dtype=np.float64)
A:sklearn.utils.tests.test_seq_dataset.X32->load_iris().data.astype(np.float32)
A:sklearn.utils.tests.test_seq_dataset.y32->load_iris().target.astype(np.float32)
A:sklearn.utils.tests.test_seq_dataset.sample_weight32->numpy.arange(y32.size, dtype=np.float32)
A:sklearn.utils.tests.test_seq_dataset.expected->expected.astype(current.dtype).astype(current.dtype)
A:sklearn.utils.tests.test_seq_dataset.X->csr_container(X)
A:sklearn.utils.tests.test_seq_dataset.X_csr64->csr_container(X64)
A:sklearn.utils.tests.test_seq_dataset.(xi_, yi, swi, idx)->dataset._random_py()
A:sklearn.utils.tests.test_seq_dataset.xi->csr_container(xi_, shape=(1, X64.shape[1]))
A:sklearn.utils.tests.test_seq_dataset.(_, _, _, idx1)->dense_dataset._random_py()
A:sklearn.utils.tests.test_seq_dataset.(_, _, _, idx2)->sparse_dataset._random_py()
A:sklearn.utils.tests.test_seq_dataset.((xi_data32, _, _), yi32, _, _)->dataset_32._next_py()
A:sklearn.utils.tests.test_seq_dataset.((xi_data64, _, _), yi64, _, _)->dataset_64._next_py()
A:sklearn.utils.tests.test_seq_dataset.X_csr32->csr_container(X32)
sklearn.utils.tests.test_seq_dataset._make_dense_dataset(float_dtype)
sklearn.utils.tests.test_seq_dataset._make_dense_datasets()
sklearn.utils.tests.test_seq_dataset._make_fused_types_datasets()
sklearn.utils.tests.test_seq_dataset._make_sparse_dataset(csr_container,float_dtype)
sklearn.utils.tests.test_seq_dataset._make_sparse_datasets()
sklearn.utils.tests.test_seq_dataset.assert_csr_equal_values(current,expected)
sklearn.utils.tests.test_seq_dataset.test_buffer_dtype_mismatch_error()
sklearn.utils.tests.test_seq_dataset.test_fused_types_consistency(dataset_32,dataset_64)
sklearn.utils.tests.test_seq_dataset.test_seq_dataset_basic_iteration(dataset,csr_container)
sklearn.utils.tests.test_seq_dataset.test_seq_dataset_shuffle(dense_dataset,sparse_dataset)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_random.py----------------------------------------
A:sklearn.utils.tests.test_random.s->sample_without_replacement(n_population, n_samples)
A:sklearn.utils.tests.test_random.unique->numpy.unique(s)
A:sklearn.utils.tests.test_random.n_expected->comb(n_population, n_samples, exact=True)
A:sklearn.utils.tests.test_random.got->_random_choice_csc(n_samples=n_samples, classes=classes, random_state=random_state)
sklearn.utils.tests.test_random.check_edge_case_of_sample_int(sample_without_replacement)
sklearn.utils.tests.test_random.check_sample_int(sample_without_replacement)
sklearn.utils.tests.test_random.check_sample_int_distribution(sample_without_replacement)
sklearn.utils.tests.test_random.test_invalid_sample_without_replacement_algorithm()
sklearn.utils.tests.test_random.test_our_rand_r()
sklearn.utils.tests.test_random.test_random_choice_csc(n_samples=10000,random_state=24)
sklearn.utils.tests.test_random.test_random_choice_csc_errors()
sklearn.utils.tests.test_random.test_sample_without_replacement_algorithms()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_estimator_html_repr.py----------------------------------------
A:sklearn.utils.tests.test_estimator_html_repr.html_label->out.getvalue()
A:sklearn.utils.tests.test_estimator_html_repr.re_compiled->re.compile(p)
A:sklearn.utils.tests.test_estimator_html_repr.est_html_info->_get_visual_block(ct)
A:sklearn.utils.tests.test_estimator_html_repr.est->FooBar()
A:sklearn.utils.tests.test_estimator_html_repr.pipe->Pipeline([('scale', StandardScaler()), ('log_Reg', LogisticRegression())])
A:sklearn.utils.tests.test_estimator_html_repr.f_union->FeatureUnion([('pca', PCA()), ('svd', TruncatedSVD())])
A:sklearn.utils.tests.test_estimator_html_repr.clf->StackingClassifier(estimators=estimators, final_estimator=final_estimator)
A:sklearn.utils.tests.test_estimator_html_repr.ct->ColumnTransformer([('pca', PCA(), ['num1', 'num2']), ('svd', TruncatedSVD, [0, 3])])
A:sklearn.utils.tests.test_estimator_html_repr.num_trans->Pipeline(steps=[('pass', 'passthrough'), ('imputer', SimpleImputer(strategy='median'))])
A:sklearn.utils.tests.test_estimator_html_repr.cat_trans->Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', missing_values='empty')), ('one-hot', OneHotEncoder(drop='first'))])
A:sklearn.utils.tests.test_estimator_html_repr.preprocess->ColumnTransformer([('num', num_trans, ['a', 'b', 'c', 'd', 'e']), ('cat', cat_trans, [0, 1, 2, 3])])
A:sklearn.utils.tests.test_estimator_html_repr.feat_u->FeatureUnion([('pca', PCA(n_components=1)), ('tsvd', Pipeline([('first', TruncatedSVD(n_components=3)), ('select', SelectPercentile())]))])
A:sklearn.utils.tests.test_estimator_html_repr.html_output->estimator_html_repr(stacker)
A:sklearn.utils.tests.test_estimator_html_repr.reg->StackingRegressor(estimators=[('svr', LinearSVR())], final_estimator=final_estimator)
A:sklearn.utils.tests.test_estimator_html_repr.birch->Birch(n_clusters=AgglomerativeClustering(n_clusters=3))
A:sklearn.utils.tests.test_estimator_html_repr.ovo->OneVsOneClassifier(LinearSVC(penalty='l1'))
A:sklearn.utils.tests.test_estimator_html_repr.kernel_ridge->KernelRidge(kernel=ExpSineSquared())
A:sklearn.utils.tests.test_estimator_html_repr.kernel_ridge_tuned->RandomizedSearchCV(kernel_ridge, param_distributions=param_distributions)
A:sklearn.utils.tests.test_estimator_html_repr.pca->PCA(n_components=10)
A:sklearn.utils.tests.test_estimator_html_repr.pca_repr->html.escape(str(pca))
A:sklearn.utils.tests.test_estimator_html_repr.stacker->StackingClassifier(estimators=[])
A:sklearn.utils.tests.test_estimator_html_repr.(X, y)->load_iris(return_X_y=True)
A:sklearn.utils.tests.test_estimator_html_repr.estimator->MyEstimator()
A:sklearn.utils.tests.test_estimator_html_repr.mixin->_HTMLDocumentationLinkMixin()
A:sklearn.utils.tests.test_estimator_html_repr.sklearn_version->parse_version(mock_version)
sklearn.utils.tests.test_estimator_html_repr.set_non_utf8_locale()
sklearn.utils.tests.test_estimator_html_repr.test_birch_duck_typing_meta()
sklearn.utils.tests.test_estimator_html_repr.test_duck_typing_nested_estimator()
sklearn.utils.tests.test_estimator_html_repr.test_estimator_get_params_return_cls()
sklearn.utils.tests.test_estimator_html_repr.test_estimator_html_repr_fitted_icon(estimator)
sklearn.utils.tests.test_estimator_html_repr.test_estimator_html_repr_pipeline()
sklearn.utils.tests.test_estimator_html_repr.test_estimator_html_repr_unfitted_vs_fitted()
sklearn.utils.tests.test_estimator_html_repr.test_fallback_exists()
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_column_transformer()
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_feature_union()
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_pipeline()
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_single_estimator()
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_single_str_none(est)
sklearn.utils.tests.test_estimator_html_repr.test_get_visual_block_voting()
sklearn.utils.tests.test_estimator_html_repr.test_html_documentation_link_mixin_doc_link_url_param_generator()
sklearn.utils.tests.test_estimator_html_repr.test_html_documentation_link_mixin_get_doc_link(module_path,expected_module)
sklearn.utils.tests.test_estimator_html_repr.test_html_documentation_link_mixin_get_doc_link_out_of_library()
sklearn.utils.tests.test_estimator_html_repr.test_html_documentation_link_mixin_sklearn(mock_version)
sklearn.utils.tests.test_estimator_html_repr.test_invalid_parameters_in_stacking()
sklearn.utils.tests.test_estimator_html_repr.test_non_utf8_locale(set_non_utf8_locale)
sklearn.utils.tests.test_estimator_html_repr.test_one_estimator_print_change_only(print_changed_only)
sklearn.utils.tests.test_estimator_html_repr.test_ovo_classifier_duck_typing_meta()
sklearn.utils.tests.test_estimator_html_repr.test_show_arrow_pipeline()
sklearn.utils.tests.test_estimator_html_repr.test_stacking_classifier(final_estimator)
sklearn.utils.tests.test_estimator_html_repr.test_stacking_regressor(final_estimator)
sklearn.utils.tests.test_estimator_html_repr.test_write_label_html(checked)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_param_validation.py----------------------------------------
A:sklearn.utils.tests.test_param_validation.interval->Interval(Real, None, None, closed='neither')
A:sklearn.utils.tests.test_param_validation.options->Options(Real, {-0.5, 0.5, np.inf}, deprecated={-0.5})
A:sklearn.utils.tests.test_param_validation.constraint->Interval(RealNotInt, 0, 1, closed='both')
A:sklearn.utils.tests.test_param_validation.bad_value->generate_invalid_param_val(constraint=integer_interval)
A:sklearn.utils.tests.test_param_validation.value->generate_valid_param(constraint)
A:sklearn.utils.tests.test_param_validation.decorated_function->deprecated()(_func)
A:sklearn.utils.tests.test_param_validation.est->_Estimator('wrong')
A:sklearn.utils.tests.test_param_validation.err_msg->str(exc_info.value)
A:sklearn.utils.tests.test_param_validation.pd->pytest.importorskip('pandas')
A:sklearn.utils.tests.test_param_validation.na_constraint->_PandasNAConstraint()
A:sklearn.utils.tests.test_param_validation.actual_skipped->g(1)
sklearn.utils.tests.test_param_validation._Class
sklearn.utils.tests.test_param_validation._Class._deprecated_method(self,a)
sklearn.utils.tests.test_param_validation._Class._method(self,a)
sklearn.utils.tests.test_param_validation._Estimator(self,a)
sklearn.utils.tests.test_param_validation._Estimator.__init__(self,a)
sklearn.utils.tests.test_param_validation._Estimator.fit(self,X=None,y=None)
sklearn.utils.tests.test_param_validation._func(a,b=0,*args,c,d=0,**kwargs)
sklearn.utils.tests.test_param_validation.test_boolean_constraint_deprecated_int()
sklearn.utils.tests.test_param_validation.test_cv_objects()
sklearn.utils.tests.test_param_validation.test_decorate_validated_function()
sklearn.utils.tests.test_param_validation.test_generate_invalid_param_val(constraint)
sklearn.utils.tests.test_param_validation.test_generate_invalid_param_val_2_intervals(integer_interval,real_interval)
sklearn.utils.tests.test_param_validation.test_generate_invalid_param_val_all_valid(constraint)
sklearn.utils.tests.test_param_validation.test_generate_valid_param(constraint)
sklearn.utils.tests.test_param_validation.test_hasmethods()
sklearn.utils.tests.test_param_validation.test_hidden_constraint()
sklearn.utils.tests.test_param_validation.test_hidden_stroptions()
sklearn.utils.tests.test_param_validation.test_instances_of_type_human_readable(type,expected_type_name)
sklearn.utils.tests.test_param_validation.test_interval_errors(params,error,match)
sklearn.utils.tests.test_param_validation.test_interval_inf_in_bounds()
sklearn.utils.tests.test_param_validation.test_interval_large_integers(interval_type)
sklearn.utils.tests.test_param_validation.test_interval_range(interval_type)
sklearn.utils.tests.test_param_validation.test_interval_real_not_int()
sklearn.utils.tests.test_param_validation.test_is_satisfied_by(constraint_declaration,value)
sklearn.utils.tests.test_param_validation.test_iterable_not_string()
sklearn.utils.tests.test_param_validation.test_make_constraint(constraint_declaration,expected_constraint_class)
sklearn.utils.tests.test_param_validation.test_make_constraint_unknown()
sklearn.utils.tests.test_param_validation.test_nan_not_in_interval(interval)
sklearn.utils.tests.test_param_validation.test_no_validation()
sklearn.utils.tests.test_param_validation.test_options()
sklearn.utils.tests.test_param_validation.test_pandas_na_constraint_with_pd_na()
sklearn.utils.tests.test_param_validation.test_real_not_int()
sklearn.utils.tests.test_param_validation.test_skip_nested_validation(prefer_skip_nested_validation)
sklearn.utils.tests.test_param_validation.test_skip_nested_validation_and_config_context(skip_parameter_validation,prefer_skip_nested_validation,expected_skipped)
sklearn.utils.tests.test_param_validation.test_skip_param_validation()
sklearn.utils.tests.test_param_validation.test_stroptions()
sklearn.utils.tests.test_param_validation.test_stroptions_deprecated_subset()
sklearn.utils.tests.test_param_validation.test_third_party_estimator()
sklearn.utils.tests.test_param_validation.test_validate_params()
sklearn.utils.tests.test_param_validation.test_validate_params_estimator()
sklearn.utils.tests.test_param_validation.test_validate_params_method()
sklearn.utils.tests.test_param_validation.test_validate_params_missing_params()
sklearn.utils.tests.test_param_validation.test_validate_params_set_param_constraints_attribute()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_pprint.py----------------------------------------
A:sklearn.utils.tests.test_pprint.lr->LogisticRegression()
A:sklearn.utils.tests.test_pprint.imputer->SimpleImputer(missing_values=float('NaN'))
A:sklearn.utils.tests.test_pprint.pipeline->Pipeline([('reduce_dim', PCA()), ('classify', SVC())])
A:sklearn.utils.tests.test_pprint.rfe->RFE(RFE(RFE(RFE(RFE(RFE(RFE(LogisticRegression())))))))
A:sklearn.utils.tests.test_pprint.gs->GridSearchCV(SVC(), param_grid)
A:sklearn.utils.tests.test_pprint.pp->_EstimatorPrettyPrinter(compact=True, indent=1, indent_at_name=True, n_max_elements_to_show=n_max_elements_to_show)
A:sklearn.utils.tests.test_pprint.gspipline->GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid)
A:sklearn.utils.tests.test_pprint.repr_->re.sub('function chi2 at 0x.*>', 'function chi2 at some_address>', repr_)
A:sklearn.utils.tests.test_pprint.vectorizer->CountVectorizer(vocabulary=vocabulary)
A:sklearn.utils.tests.test_pprint.full_repr->LogisticRegression().__repr__(N_CHAR_MAX=float('inf'))
A:sklearn.utils.tests.test_pprint.n_nonblank->len(''.join(full_repr.split()))
A:sklearn.utils.tests.test_pprint.params->super().get_params(deep=deep)
A:sklearn.utils.tests.test_pprint.est->WithKWargs(a='something', c='abcd', d=None)
A:sklearn.utils.tests.test_pprint.estimator->DummyEstimator(make_pipeline(DummyEstimator(DummyEstimator()), DummyEstimator(), 'passthrough'))
sklearn.utils.tests.test_pprint.CountVectorizer(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.utils.tests.test_pprint.CountVectorizer.__init__(self,input='content',encoding='utf-8',decode_error='strict',strip_accents=None,lowercase=True,preprocessor=None,tokenizer=None,stop_words=None,token_pattern='(?u)\\b\\w\\w+\\b',ngram_range=(1,1),analyzer='word',max_df=1.0,min_df=1,max_features=None,vocabulary=None,binary=False,dtype=np.int64)
sklearn.utils.tests.test_pprint.GridSearchCV(self,estimator,param_grid,scoring=None,n_jobs=None,iid='warn',refit=True,cv='warn',verbose=0,pre_dispatch='2*n_jobs',error_score='raise-deprecating',return_train_score=False)
sklearn.utils.tests.test_pprint.GridSearchCV.__init__(self,estimator,param_grid,scoring=None,n_jobs=None,iid='warn',refit=True,cv='warn',verbose=0,pre_dispatch='2*n_jobs',error_score='raise-deprecating',return_train_score=False)
sklearn.utils.tests.test_pprint.LogisticRegression(self,penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='warn',max_iter=100,multi_class='warn',verbose=0,warm_start=False,n_jobs=None,l1_ratio=None)
sklearn.utils.tests.test_pprint.LogisticRegression.__init__(self,penalty='l2',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,random_state=None,solver='warn',max_iter=100,multi_class='warn',verbose=0,warm_start=False,n_jobs=None,l1_ratio=None)
sklearn.utils.tests.test_pprint.LogisticRegression.fit(self,X,y)
sklearn.utils.tests.test_pprint.NMF(self,n_components=None,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha=0.0,l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.utils.tests.test_pprint.NMF.__init__(self,n_components=None,init=None,solver='cd',beta_loss='frobenius',tol=0.0001,max_iter=200,random_state=None,alpha=0.0,l1_ratio=0.0,verbose=0,shuffle=False)
sklearn.utils.tests.test_pprint.PCA(self,n_components=None,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)
sklearn.utils.tests.test_pprint.PCA.__init__(self,n_components=None,copy=True,whiten=False,svd_solver='auto',tol=0.0,iterated_power='auto',random_state=None)
sklearn.utils.tests.test_pprint.Pipeline(self,steps,memory=None)
sklearn.utils.tests.test_pprint.Pipeline.__init__(self,steps,memory=None)
sklearn.utils.tests.test_pprint.RFE(self,estimator,n_features_to_select=None,step=1,verbose=0)
sklearn.utils.tests.test_pprint.RFE.__init__(self,estimator,n_features_to_select=None,step=1,verbose=0)
sklearn.utils.tests.test_pprint.SVC(self,C=1.0,kernel='rbf',degree=3,gamma='auto_deprecated',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.utils.tests.test_pprint.SVC.__init__(self,C=1.0,kernel='rbf',degree=3,gamma='auto_deprecated',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',random_state=None)
sklearn.utils.tests.test_pprint.SimpleImputer(self,missing_values=np.nan,strategy='mean',fill_value=None,verbose=0,copy=True)
sklearn.utils.tests.test_pprint.SimpleImputer.__init__(self,missing_values=np.nan,strategy='mean',fill_value=None,verbose=0,copy=True)
sklearn.utils.tests.test_pprint.StandardScaler(self,copy=True,with_mean=True,with_std=True)
sklearn.utils.tests.test_pprint.StandardScaler.__init__(self,copy=True,with_mean=True,with_std=True)
sklearn.utils.tests.test_pprint.StandardScaler.transform(self,X,copy=None)
sklearn.utils.tests.test_pprint.test_basic(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_bruteforce_ellipsis(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_builtin_prettyprinter()
sklearn.utils.tests.test_pprint.test_changed_only()
sklearn.utils.tests.test_pprint.test_complexity_print_changed_only()
sklearn.utils.tests.test_pprint.test_deeply_nested(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_gridsearch(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_gridsearch_pipeline(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_kwargs_in_init()
sklearn.utils.tests.test_pprint.test_n_max_elements_to_show(print_changed_only_false)
sklearn.utils.tests.test_pprint.test_pipeline(print_changed_only_false)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/utils/tests/test_response.py----------------------------------------
A:sklearn.utils.tests.test_response.(X, y)->make_classification(n_samples=10, n_classes=2, weights=[0.3, 0.7], random_state=0)
A:sklearn.utils.tests.test_response.X->scale(X, copy=False)
A:sklearn.utils.tests.test_response.my_estimator->_MockEstimatorOnOffPrediction(response_methods=[response_method])
A:sklearn.utils.tests.test_response.regressor->LinearRegression().fit(X, y)
A:sklearn.utils.tests.test_response.results->_get_response_values(classifier, X, response_method=response_method, pos_label=None, return_response_method_used=return_response_method_used)
A:sklearn.utils.tests.test_response.outlier_detector->IsolationForest(random_state=0).fit(X, y)
A:sklearn.utils.tests.test_response.prediction_method->getattr(outlier_detector, chosen_response_method)
A:sklearn.utils.tests.test_response.classifier->LogisticRegression().fit(X_binary, y_binary)
A:sklearn.utils.tests.test_response.(X, y_two_class)->make_classification(n_samples=10, n_classes=2, random_state=0)
A:sklearn.utils.tests.test_response.y_single_class->numpy.zeros_like(y_two_class)
A:sklearn.utils.tests.test_response.(y_pred, pos_label, *_)->_get_response_values(classifier, X, response_method=response_method, pos_label=classifier.classes_[0], return_response_method_used=return_response_method_used)
A:sklearn.utils.tests.test_response.(y_proba, pos_label)->_get_response_values_binary(classifier, X_binary, response_method='predict_proba', pos_label=0)
A:sklearn.utils.tests.test_response.(y_score, pos_label)->_get_response_values_binary(classifier, X_binary, response_method='decision_function', pos_label=0)
A:sklearn.utils.tests.test_response.(predictions, pos_label)->_get_response_values(estimator, X, response_method=response_method)
A:sklearn.utils.tests.test_response.(y_pred, pos_label, response_method)->_get_response_values(classifier, X_binary, response_method=['decision_function', 'predict_proba'], return_response_method_used=True)
A:sklearn.utils.tests.test_response.(X, Y)->make_multilabel_classification(random_state=0)
A:sklearn.utils.tests.test_response.estimator->ClassifierChain(LogisticRegression()).fit(X, Y)
A:sklearn.utils.tests.test_response.(y_pred, pos_label)->_get_response_values(estimator, X, response_method=response_method)
sklearn.utils.tests.test_response.test_get_response_decision_function()
sklearn.utils.tests.test_response.test_get_response_error(estimator,X,y,err_msg,params)
sklearn.utils.tests.test_response.test_get_response_predict_proba()
sklearn.utils.tests.test_response.test_get_response_values_binary_classifier_decision_function(return_response_method_used)
sklearn.utils.tests.test_response.test_get_response_values_binary_classifier_predict_proba(return_response_method_used,response_method)
sklearn.utils.tests.test_response.test_get_response_values_classifier_inconsistent_y_pred_for_binary_proba(response_method)
sklearn.utils.tests.test_response.test_get_response_values_classifier_unknown_pos_label(response_method)
sklearn.utils.tests.test_response.test_get_response_values_multiclass(estimator,response_method)
sklearn.utils.tests.test_response.test_get_response_values_multilabel_indicator(response_method)
sklearn.utils.tests.test_response.test_get_response_values_outlier_detection(response_method,return_response_method_used)
sklearn.utils.tests.test_response.test_get_response_values_regressor(return_response_method_used)
sklearn.utils.tests.test_response.test_get_response_values_regressor_error(response_method)
sklearn.utils.tests.test_response.test_get_response_values_with_response_list()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/_bounds.py----------------------------------------
A:sklearn.svm._bounds.X->check_array(X, accept_sparse='csc')
A:sklearn.svm._bounds.den->max(den, abs(np.dot(Y, bias)).max())
A:sklearn.svm._bounds.bias->numpy.full((np.size(y), 1), intercept_scaling, dtype=np.array(intercept_scaling).dtype)
sklearn.svm._bounds.l1_min_c(X,y,*,loss='squared_hinge',fit_intercept=True,intercept_scaling=1.0)
sklearn.svm.l1_min_c(X,y,*,loss='squared_hinge',fit_intercept=True,intercept_scaling=1.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/_base.py----------------------------------------
A:sklearn.svm._base.sv_locs->numpy.cumsum(np.hstack([[0], n_support]))
A:sklearn.svm._base.rnd->check_random_state(random_state)
A:sklearn.svm._base.sparse->scipy.sparse.issparse(X)
A:sklearn.svm._base.(X, y)->self._validate_data(X, y, dtype=np.float64, order='C', accept_sparse='csr', accept_large_sparse=False)
A:sklearn.svm._base.y->super().predict(X)
A:sklearn.svm._base.sample_weight->_check_sample_weight(sample_weight, X, dtype=np.float64)
A:sklearn.svm._base.solver_type->_get_liblinear_solver_type(multi_class, penalty, loss, dual)
A:sklearn.svm._base.n_samples->_num_samples(X)
A:sklearn.svm._base.seed->check_random_state(random_state).randint(np.iinfo('i').max)
A:sklearn.svm._base.self._intercept_->self.intercept_.copy()
A:sklearn.svm._base.intercept_finiteness->numpy.isfinite(self._intercept_).all()
A:sklearn.svm._base.dual_coef_finiteness->numpy.isfinite(dual_coef).all()
A:sklearn.svm._base.self.n_iter_->self._num_iter.item()
A:sklearn.svm._base.X->self._compute_kernel(X)
A:sklearn.svm._base.(self.support_, self.support_vectors_, self._n_support, self.dual_coef_, self.intercept_, self._probA, self._probB, self.fit_status_, self._num_iter)->libsvm.fit(X, y, svm_type=solver_type, sample_weight=sample_weight, class_weight=getattr(self, 'class_weight_', np.empty(0)), kernel=kernel, C=self.C, nu=self.nu, probability=self.probability, degree=self.degree, shrinking=self.shrinking, tol=self.tol, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma, epsilon=self.epsilon, max_iter=self.max_iter, random_seed=random_seed)
A:sklearn.svm._base.X.data->numpy.asarray(X.data, dtype=np.float64, order='C')
A:sklearn.svm._base.kernel_type->self._sparse_kernels.index(kernel)
A:sklearn.svm._base.(self.support_, self.support_vectors_, dual_coef_data, self.intercept_, self._n_support, self._probA, self._probB, self.fit_status_, self._num_iter)->libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices, X.indptr, y, solver_type, kernel_type, self.degree, self._gamma, self.coef0, self.tol, self.C, getattr(self, 'class_weight_', np.empty(0)), sample_weight, self.nu, self.cache_size, self.epsilon, int(self.shrinking), int(self.probability), self.max_iter, random_seed)
A:sklearn.svm._base.dual_coef_indices->numpy.tile(np.arange(n_SV), n_class)
A:sklearn.svm._base.self.dual_coef_->scipy.sparse.csr_matrix((dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV))
A:sklearn.svm._base.dual_coef_indptr->numpy.arange(0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class)
A:sklearn.svm._base.svm_type->LIBSVM_IMPL.index(self._impl)
A:sklearn.svm._base.kernel->kernel.toarray().toarray()
A:sklearn.svm._base.dec_func->self._dense_decision_function(X)
A:sklearn.svm._base.coef->numpy.vstack(coef)
A:sklearn.svm._base.y_->column_or_1d(y, warn=True)
A:sklearn.svm._base.(cls, y)->numpy.unique(y_, return_inverse=True)
A:sklearn.svm._base.self.class_weight_->compute_class_weight(self.class_weight, classes=cls, y=y_)
A:sklearn.svm._base.dec->self._decision_function(X)
A:sklearn.svm._base.pprob->libsvm.predict_proba(X, self.support_, self.support_vectors_, self._n_support, self._dual_coef_, self._intercept_, self._probA, self._probB, svm_type=svm_type, kernel=kernel, degree=self.degree, cache_size=self.cache_size, coef0=self.coef0, gamma=self._gamma)
A:sklearn.svm._base._solver_pen->_solver_type_dict.get(loss, None)
A:sklearn.svm._base._solver_dual->_solver_type_dict.get(loss, None).get(penalty, None)
A:sklearn.svm._base.solver_num->_solver_type_dict.get(loss, None).get(penalty, None).get(dual, None)
A:sklearn.svm._base.enc->LabelEncoder()
A:sklearn.svm._base.y_ind->numpy.require(y_ind, requirements='W')
A:sklearn.svm._base.class_weight_->numpy.empty(0, dtype=np.float64)
A:sklearn.svm._base.(raw_coef_, n_iter_)->liblinear.train_wrap(X, y_ind, sp.issparse(X), solver_type, tol, bias, C, class_weight_, max_iter, rnd.randint(np.iinfo('i').max), epsilon, sample_weight)
A:sklearn.svm._base.n_iter_max->max(n_iter_)
sklearn.svm._base.BaseLibSVM(self,kernel,degree,gamma,coef0,tol,C,nu,epsilon,shrinking,probability,cache_size,class_weight,verbose,max_iter,random_state)
sklearn.svm._base.BaseLibSVM.__init__(self,kernel,degree,gamma,coef0,tol,C,nu,epsilon,shrinking,probability,cache_size,class_weight,verbose,max_iter,random_state)
sklearn.svm._base.BaseLibSVM._compute_kernel(self,X)
sklearn.svm._base.BaseLibSVM._decision_function(self,X)
sklearn.svm._base.BaseLibSVM._dense_decision_function(self,X)
sklearn.svm._base.BaseLibSVM._dense_fit(self,X,y,sample_weight,solver_type,kernel,random_seed)
sklearn.svm._base.BaseLibSVM._dense_predict(self,X)
sklearn.svm._base.BaseLibSVM._get_coef(self)
sklearn.svm._base.BaseLibSVM._more_tags(self)
sklearn.svm._base.BaseLibSVM._sparse_decision_function(self,X)
sklearn.svm._base.BaseLibSVM._sparse_fit(self,X,y,sample_weight,solver_type,kernel,random_seed)
sklearn.svm._base.BaseLibSVM._sparse_predict(self,X)
sklearn.svm._base.BaseLibSVM._validate_for_predict(self,X)
sklearn.svm._base.BaseLibSVM._validate_targets(self,y)
sklearn.svm._base.BaseLibSVM._warn_from_fit_status(self)
sklearn.svm._base.BaseLibSVM.coef_(self)
sklearn.svm._base.BaseLibSVM.fit(self,X,y,sample_weight=None)
sklearn.svm._base.BaseLibSVM.n_support_(self)
sklearn.svm._base.BaseLibSVM.predict(self,X)
sklearn.svm._base.BaseSVC(self,kernel,degree,gamma,coef0,tol,C,nu,shrinking,probability,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state,break_ties)
sklearn.svm._base.BaseSVC.__init__(self,kernel,degree,gamma,coef0,tol,C,nu,shrinking,probability,cache_size,class_weight,verbose,max_iter,decision_function_shape,random_state,break_ties)
sklearn.svm._base.BaseSVC._check_proba(self)
sklearn.svm._base.BaseSVC._dense_predict_proba(self,X)
sklearn.svm._base.BaseSVC._get_coef(self)
sklearn.svm._base.BaseSVC._sparse_predict_proba(self,X)
sklearn.svm._base.BaseSVC._validate_targets(self,y)
sklearn.svm._base.BaseSVC.decision_function(self,X)
sklearn.svm._base.BaseSVC.predict(self,X)
sklearn.svm._base.BaseSVC.predict_log_proba(self,X)
sklearn.svm._base.BaseSVC.predict_proba(self,X)
sklearn.svm._base.BaseSVC.probA_(self)
sklearn.svm._base.BaseSVC.probB_(self)
sklearn.svm._base._fit_liblinear(X,y,C,fit_intercept,intercept_scaling,class_weight,penalty,dual,verbose,max_iter,tol,random_state=None,multi_class='ovr',loss='logistic_regression',epsilon=0.1,sample_weight=None)
sklearn.svm._base._get_liblinear_solver_type(multi_class,penalty,loss,dual)
sklearn.svm._base._one_vs_one_coef(dual_coef,n_support,support_vectors)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/_classes.py----------------------------------------
A:sklearn.svm._classes.(X, y)->self._validate_data(X, y, accept_sparse='csr', dtype=np.float64, order='C', accept_large_sparse=False)
A:sklearn.svm._classes.self.classes_->numpy.unique(y)
A:sklearn.svm._classes._dual->_validate_dual_parameter(self.dual, self.loss, penalty, 'ovr', X)
A:sklearn.svm._classes.(self.coef_, self.intercept_, n_iter_)->_fit_liblinear(X, y, self.C, self.fit_intercept, self.intercept_scaling, None, penalty, _dual, self.verbose, self.max_iter, self.tol, self.random_state, loss=self.loss, epsilon=self.epsilon, sample_weight=sample_weight)
A:sklearn.svm._classes.self.n_iter_->n_iter_.max().item()
A:sklearn.svm._classes.self.coef_->self.coef_.ravel()
A:sklearn.svm._classes.self.intercept_->numpy.array([intercept])
A:sklearn.svm._classes.dec->self._decision_function(X).ravel()
A:sklearn.svm._classes.y->super().predict(X)
sklearn.svm.LinearSVC(self,penalty='l2',loss='squared_hinge',*,dual='warn',tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm.LinearSVC._more_tags(self)
sklearn.svm.LinearSVC.fit(self,X,y,sample_weight=None)
sklearn.svm.LinearSVR(self,*,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual='warn',verbose=0,random_state=None,max_iter=1000)
sklearn.svm.LinearSVR._more_tags(self)
sklearn.svm.LinearSVR.fit(self,X,y,sample_weight=None)
sklearn.svm.NuSVC(self,*,nu=0.5,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm.NuSVC._more_tags(self)
sklearn.svm.NuSVR(self,*,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.NuSVR._more_tags(self)
sklearn.svm.OneClassSVM(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.OneClassSVM._more_tags(self)
sklearn.svm.OneClassSVM.decision_function(self,X)
sklearn.svm.OneClassSVM.fit(self,X,y=None,sample_weight=None)
sklearn.svm.OneClassSVM.predict(self,X)
sklearn.svm.OneClassSVM.score_samples(self,X)
sklearn.svm.SVC(self,*,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm.SVC._more_tags(self)
sklearn.svm.SVR(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm.SVR._more_tags(self)
sklearn.svm._classes.LinearSVC(self,penalty='l2',loss='squared_hinge',*,dual='warn',tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm._classes.LinearSVC.__init__(self,penalty='l2',loss='squared_hinge',*,dual='warn',tol=0.0001,C=1.0,multi_class='ovr',fit_intercept=True,intercept_scaling=1,class_weight=None,verbose=0,random_state=None,max_iter=1000)
sklearn.svm._classes.LinearSVC._more_tags(self)
sklearn.svm._classes.LinearSVC.fit(self,X,y,sample_weight=None)
sklearn.svm._classes.LinearSVR(self,*,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual='warn',verbose=0,random_state=None,max_iter=1000)
sklearn.svm._classes.LinearSVR.__init__(self,*,epsilon=0.0,tol=0.0001,C=1.0,loss='epsilon_insensitive',fit_intercept=True,intercept_scaling=1.0,dual='warn',verbose=0,random_state=None,max_iter=1000)
sklearn.svm._classes.LinearSVR._more_tags(self)
sklearn.svm._classes.LinearSVR.fit(self,X,y,sample_weight=None)
sklearn.svm._classes.NuSVC(self,*,nu=0.5,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm._classes.NuSVC.__init__(self,*,nu=0.5,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm._classes.NuSVC._more_tags(self)
sklearn.svm._classes.NuSVR(self,*,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.NuSVR.__init__(self,*,nu=0.5,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,tol=0.001,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.NuSVR._more_tags(self)
sklearn.svm._classes.OneClassSVM(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.OneClassSVM.__init__(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,nu=0.5,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.OneClassSVM._more_tags(self)
sklearn.svm._classes.OneClassSVM.decision_function(self,X)
sklearn.svm._classes.OneClassSVM.fit(self,X,y=None,sample_weight=None)
sklearn.svm._classes.OneClassSVM.predict(self,X)
sklearn.svm._classes.OneClassSVM.score_samples(self,X)
sklearn.svm._classes.SVC(self,*,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm._classes.SVC.__init__(self,*,C=1.0,kernel='rbf',degree=3,gamma='scale',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbose=False,max_iter=-1,decision_function_shape='ovr',break_ties=False,random_state=None)
sklearn.svm._classes.SVC._more_tags(self)
sklearn.svm._classes.SVR(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.SVR.__init__(self,*,kernel='rbf',degree=3,gamma='scale',coef0=0.0,tol=0.001,C=1.0,epsilon=0.1,shrinking=True,cache_size=200,verbose=False,max_iter=-1)
sklearn.svm._classes.SVR._more_tags(self)
sklearn.svm._classes._validate_dual_parameter(dual,loss,penalty,multi_class,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/tests/test_svm.py----------------------------------------
A:sklearn.svm.tests.test_svm.iris->sklearn.datasets.load_iris()
A:sklearn.svm.tests.test_svm.rng->numpy.random.RandomState(0)
A:sklearn.svm.tests.test_svm.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.svm.tests.test_svm.clf->Estimator().SVC(kernel='linear').fit(X, Y)
A:sklearn.svm.tests.test_svm.(libsvm_support, libsvm_support_vectors, libsvm_n_class_SV, libsvm_sv_coef, libsvm_intercept, libsvm_probA, libsvm_probB, libsvm_fit_status, libsvm_n_iter)->sklearn.svm._libsvm.fit(iris.data, iris.target.astype(np.float64), kernel='linear')
A:sklearn.svm.tests.test_svm.pred->Estimator().predict(np.c_[xx.ravel(), yy.ravel()])
A:sklearn.svm.tests.test_svm.pred2->sklearn.svm.LinearSVC(dual='auto', random_state=0, tol=1e-12, max_iter=1000).fit(X_flat, y_flat).predict(T)
A:sklearn.svm.tests.test_svm.K->string_kernel(data, data)
A:sklearn.svm.tests.test_svm.KT->numpy.zeros_like(KT)
A:sklearn.svm.tests.test_svm.KT[i, j]->numpy.dot(T[i], X[j])
A:sklearn.svm.tests.test_svm.clf2->Estimator().SVC(kernel='linear')
A:sklearn.svm.tests.test_svm.K[i, j]->numpy.dot(iris.data[i], iris.data[j])
A:sklearn.svm.tests.test_svm.diabetes->sklearn.datasets.load_diabetes()
A:sklearn.svm.tests.test_svm.lsvr->Estimator().LinearSVR(dual='auto', random_state=0, max_iter=2)
A:sklearn.svm.tests.test_svm.score1->Estimator().LinearSVR(dual='auto', random_state=0, max_iter=2).score(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.svr->Estimator().SVR(kernel='linear', C=1000.0).fit(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.score2->sklearn.svm.LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(diabetes.data, diabetes.target).score(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.n_samples->len(X)
A:sklearn.svm.tests.test_svm.unit_weight->numpy.ones(n_samples)
A:sklearn.svm.tests.test_svm.lsvr_no_weight->Estimator().LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(diabetes.data, diabetes.target)
A:sklearn.svm.tests.test_svm.random_state->check_random_state(0)
A:sklearn.svm.tests.test_svm.random_weight->check_random_state(0).randint(0, 10, n_samples)
A:sklearn.svm.tests.test_svm.lsvr_unflat->Estimator().LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(diabetes.data, diabetes.target, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.score3->Estimator().LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(diabetes.data, diabetes.target, sample_weight=random_weight).score(diabetes.data, diabetes.target, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.X_flat->numpy.repeat(X, random_weight, axis=0)
A:sklearn.svm.tests.test_svm.y_flat->numpy.repeat(Y, random_weight, axis=0)
A:sklearn.svm.tests.test_svm.lsvr_flat->Estimator().LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.score4->Estimator().LinearSVR(dual='auto', C=1000.0, tol=1e-12, max_iter=10000).fit(X_flat, y_flat).score(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.rnd->check_random_state(2)
A:sklearn.svm.tests.test_svm.X_outliers->check_random_state(2).uniform(low=-4, high=4, size=(20, 2))
A:sklearn.svm.tests.test_svm.y_pred_test->Estimator().SVC(kernel='linear').fit(X, Y).predict(X_test)
A:sklearn.svm.tests.test_svm.y_pred_outliers->Estimator().SVC(kernel='linear').fit(X, Y).predict(X_outliers)
A:sklearn.svm.tests.test_svm.dec_func_test->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(X_test)
A:sklearn.svm.tests.test_svm.dec_func_outliers->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(X_outliers)
A:sklearn.svm.tests.test_svm.clf._dual_coef_->numpy.array([[0.0, 1.0]])
A:sklearn.svm.tests.test_svm.prob_predict->Estimator().SVC(kernel='linear').fit(X, Y).predict_proba(iris.data)
A:sklearn.svm.tests.test_svm.prediction->Estimator().SVC(kernel='linear').fit(X, Y).predict(X)
A:sklearn.svm.tests.test_svm.expected->numpy.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])
A:sklearn.svm.tests.test_svm.rbfs->rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)
A:sklearn.svm.tests.test_svm.dec->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(iris.data)
A:sklearn.svm.tests.test_svm.(X, y)->make_blobs(random_state=0, n_samples=20, n_features=2)
A:sklearn.svm.tests.test_svm.(X_train, X_test, y_train, y_test)->train_test_split(X, y, random_state=0)
A:sklearn.svm.tests.test_svm.reg->Estimator().SVR(kernel='rbf', gamma=1).fit(X, y)
A:sklearn.svm.tests.test_svm.(X_, y_)->make_classification(n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2)
A:sklearn.svm.tests.test_svm.y_pred->Estimator().SVC(kernel='linear').fit(X, Y).predict(X_test)
A:sklearn.svm.tests.test_svm.est->Klass()
A:sklearn.svm.tests.test_svm.coef->numpy.abs(est.coef_).ravel()
A:sklearn.svm.tests.test_svm.unbalanced->numpy.delete(np.arange(y.size), np.where(y > 2)[0][::2])
A:sklearn.svm.tests.test_svm.classes->numpy.unique(y[unbalanced])
A:sklearn.svm.tests.test_svm.class_weights->compute_class_weight('balanced', classes=classes, y=y[unbalanced])
A:sklearn.svm.tests.test_svm.y_pred_balanced->Estimator().SVC(kernel='linear').fit(X, Y).fit(X[unbalanced], y[unbalanced]).predict(X)
A:sklearn.svm.tests.test_svm.Xf->numpy.asfortranarray(X)
A:sklearn.svm.tests.test_svm.yf->numpy.ascontiguousarray(np.tile(Y, (2, 1)).T)
A:sklearn.svm.tests.test_svm.y->numpy.array([1, 1, 2, 2, 1])
A:sklearn.svm.tests.test_svm.sparse_gram->csr_container([[1, 0], [0, 1]])
A:sklearn.svm.tests.test_svm.X_train->numpy.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])
A:sklearn.svm.tests.test_svm.y_train->numpy.array([0.04, 0.04, 0.1, 0.16])
A:sklearn.svm.tests.test_svm.model->Estimator().SVR(kernel='linear')
A:sklearn.svm.tests.test_svm.ovr_clf->Estimator().LinearSVC(dual='auto', random_state=0).fit(iris.data, iris.target)
A:sklearn.svm.tests.test_svm.cs_clf->Estimator().LinearSVC(dual='auto', multi_class='crammer_singer', random_state=0)
A:sklearn.svm.tests.test_svm.clf_unitweight->Estimator().LinearSVC(dual='auto', random_state=0, tol=1e-12, max_iter=1000).fit(X, Y, sample_weight=unit_weight)
A:sklearn.svm.tests.test_svm.lsvc_unflat->Estimator().LinearSVC(dual='auto', random_state=0, tol=1e-12, max_iter=1000).fit(X, Y, sample_weight=random_weight)
A:sklearn.svm.tests.test_svm.pred1->Estimator().LinearSVC(dual='auto', random_state=0, tol=1e-12, max_iter=1000).fit(X, Y, sample_weight=random_weight).predict(T)
A:sklearn.svm.tests.test_svm.lsvc_flat->Estimator().LinearSVC(dual='auto', random_state=0, tol=1e-12, max_iter=1000).fit(X_flat, y_flat)
A:sklearn.svm.tests.test_svm.acc->Estimator().LinearSVC(dual='auto', fit_intercept=fit_intercept, multi_class='crammer_singer', random_state=0).fit(X, y).score(X, y)
A:sklearn.svm.tests.test_svm.values->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(X)
A:sklearn.svm.tests.test_svm.clf.coef_->Estimator().SVC(kernel='linear').fit(X, Y).coef_.copy()
A:sklearn.svm.tests.test_svm.clf.intercept_->Estimator().SVC(kernel='linear').fit(X, Y).intercept_.copy()
A:sklearn.svm.tests.test_svm.values2->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(X)
A:sklearn.svm.tests.test_svm.stdout->os.dup(1)
A:sklearn.svm.tests.test_svm.svm_callable->Estimator().SVC(kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.svm_cloned->sklearn.base.clone(svm_callable)
A:sklearn.svm.tests.test_svm.svm_builtin->Estimator().SVC(kernel='linear', probability=True, random_state=0, decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.svc->Estimator().SVC(kernel=lambda x, y: x)
A:sklearn.svm.tests.test_svm.a->Estimator().SVC(probability=True, max_iter=1, random_state=0)
A:sklearn.svm.tests.test_svm.proba_1->Estimator().SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_svm.proba_2->Estimator().SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_svm.lsvc->Estimator().LinearSVC(dual='auto', fit_intercept=False)
A:sklearn.svm.tests.test_svm.X->numpy.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])
A:sklearn.svm.tests.test_svm.G->Estimator().SVC(probability=False)
A:sklearn.svm.tests.test_svm.base_points->numpy.array([[5, 5], [10, 10]])
A:sklearn.svm.tests.test_svm.X_test->numpy.vstack((base_points * [1, 1], base_points * [-1, 1], base_points * [-1, -1], base_points * [1, -1]))
A:sklearn.svm.tests.test_svm.deci_val->Estimator().SVC(kernel='linear').fit(X, Y).decision_function(X_test)
A:sklearn.svm.tests.test_svm.pred_class_deci_val->deci_val[range(8), y_pred].reshape((4, 2))
A:sklearn.svm.tests.test_svm.svm->Estimator()
A:sklearn.svm.tests.test_svm.xs->numpy.linspace(X[:, 0].min(), X[:, 0].max(), 100)
A:sklearn.svm.tests.test_svm.ys->numpy.linspace(X[:, 1].min(), X[:, 1].max(), 100)
A:sklearn.svm.tests.test_svm.(xx, yy)->numpy.meshgrid(xs, ys)
A:sklearn.svm.tests.test_svm.common_params->dict(kernel='rbf', gamma=1000000.0, random_state=42, decision_function_shape='ovr')
A:sklearn.svm.tests.test_svm.dv->Estimator().decision_function(np.c_[xx.ravel(), yy.ravel()])
A:sklearn.svm.tests.test_svm.X2->numpy.vstack([X, X])
A:sklearn.svm.tests.test_svm.y2->numpy.hstack([y, 3 - y])
A:sklearn.svm.tests.test_svm.sample_weight->numpy.ones(shape=len(y) * 2)
A:sklearn.svm.tests.test_svm.(X2, y2, sample_weight)->shuffle(X2, y2, sample_weight, random_state=0)
A:sklearn.svm.tests.test_svm.base_estimator->SVM(random_state=42)
A:sklearn.svm.tests.test_svm.est_no_weight->sklearn.base.clone(base_estimator).fit(X, y)
A:sklearn.svm.tests.test_svm.est_with_weight->sklearn.base.clone(base_estimator).fit(X2, y2, sample_weight=sample_weight)
A:sklearn.svm.tests.test_svm.X_est_no_weight->getattr(est_no_weight, method)(X)
A:sklearn.svm.tests.test_svm.X_est_with_weight->getattr(est_with_weight, method)(X)
A:sklearn.svm.tests.test_svm.n_samples1->_num_samples(X1)
A:sklearn.svm.tests.test_svm.n_samples2->_num_samples(X2)
A:sklearn.svm.tests.test_svm.svc1->Estimator(kernel=string_kernel).fit(data, y)
A:sklearn.svm.tests.test_svm.svc2->Estimator(kernel='linear').fit(X, y)
A:sklearn.svm.tests.test_svm.svc3->Estimator(kernel='precomputed').fit(K, y)
A:sklearn.svm.tests.test_svm.n_classes->len(np.unique(y))
A:sklearn.svm.tests.test_svm.dual->_validate_dual_parameter('auto', 'squared_hinge', 'l1', 'ovr', np.asarray(X).T)
sklearn.svm.tests.test_svm.test_auto_weight()
sklearn.svm.tests.test_svm.test_bad_input(lil_container)
sklearn.svm.tests.test_svm.test_consistent_proba()
sklearn.svm.tests.test_svm.test_crammer_singer_binary()
sklearn.svm.tests.test_svm.test_custom_kernel_not_array_input(Estimator)
sklearn.svm.tests.test_svm.test_decision_function()
sklearn.svm.tests.test_svm.test_decision_function_shape(SVM)
sklearn.svm.tests.test_svm.test_decision_function_shape_two_class()
sklearn.svm.tests.test_svm.test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC)
sklearn.svm.tests.test_svm.test_dual_auto(loss)
sklearn.svm.tests.test_svm.test_dual_auto_deprecation_warning(Estimator)
sklearn.svm.tests.test_svm.test_dual_auto_edge_cases()
sklearn.svm.tests.test_svm.test_gamma_scale()
sklearn.svm.tests.test_svm.test_hasattr_predict_proba()
sklearn.svm.tests.test_svm.test_immutable_coef_property()
sklearn.svm.tests.test_svm.test_liblinear_set_coef()
sklearn.svm.tests.test_svm.test_libsvm_convergence_warnings()
sklearn.svm.tests.test_svm.test_libsvm_iris()
sklearn.svm.tests.test_svm.test_libsvm_parameters()
sklearn.svm.tests.test_svm.test_linear_svm_convergence_warnings()
sklearn.svm.tests.test_svm.test_linearsvc()
sklearn.svm.tests.test_svm.test_linearsvc_crammer_singer()
sklearn.svm.tests.test_svm.test_linearsvc_fit_sampleweight()
sklearn.svm.tests.test_svm.test_linearsvc_iris()
sklearn.svm.tests.test_svm.test_linearsvc_parameters(loss,penalty,dual)
sklearn.svm.tests.test_svm.test_linearsvc_verbose()
sklearn.svm.tests.test_svm.test_linearsvm_liblinear_sample_weight(SVM,params)
sklearn.svm.tests.test_svm.test_linearsvr()
sklearn.svm.tests.test_svm.test_linearsvr_fit_sampleweight()
sklearn.svm.tests.test_svm.test_lsvc_intercept_scaling_zero()
sklearn.svm.tests.test_svm.test_n_iter_libsvm(estimator,expected_n_iter_type,dataset)
sklearn.svm.tests.test_svm.test_n_support(Klass)
sklearn.svm.tests.test_svm.test_negative_sample_weights_mask_all_samples(Estimator,err_msg,sample_weight)
sklearn.svm.tests.test_svm.test_negative_weight_equal_coeffs(Estimator,sample_weight)
sklearn.svm.tests.test_svm.test_negative_weights_svc_leave_just_one_label(Classifier,err_msg,sample_weight)
sklearn.svm.tests.test_svm.test_negative_weights_svc_leave_two_labels(Classifier,model,sample_weight,mask_side)
sklearn.svm.tests.test_svm.test_oneclass()
sklearn.svm.tests.test_svm.test_oneclass_decision_function()
sklearn.svm.tests.test_svm.test_oneclass_score_samples()
sklearn.svm.tests.test_svm.test_ovr_decision_function()
sklearn.svm.tests.test_svm.test_precomputed()
sklearn.svm.tests.test_svm.test_probability()
sklearn.svm.tests.test_svm.test_sparse_fit_support_vectors_empty(csr_container)
sklearn.svm.tests.test_svm.test_sparse_precomputed(csr_container)
sklearn.svm.tests.test_svm.test_svc_bad_kernel()
sklearn.svm.tests.test_svm.test_svc_clone_with_callable_kernel()
sklearn.svm.tests.test_svm.test_svc_invalid_break_ties_param(SVCClass)
sklearn.svm.tests.test_svm.test_svc_nonfinite_params()
sklearn.svm.tests.test_svm.test_svc_ovr_tie_breaking(SVCClass)
sklearn.svm.tests.test_svm.test_svc_raises_error_internal_representation()
sklearn.svm.tests.test_svm.test_svm_classifier_sided_sample_weight(estimator)
sklearn.svm.tests.test_svm.test_svm_equivalence_sample_weight_C()
sklearn.svm.tests.test_svm.test_svm_regressor_sided_sample_weight(estimator)
sklearn.svm.tests.test_svm.test_svr()
sklearn.svm.tests.test_svm.test_svr_coef_sign()
sklearn.svm.tests.test_svm.test_svr_errors()
sklearn.svm.tests.test_svm.test_svr_predict()
sklearn.svm.tests.test_svm.test_tweak_params()
sklearn.svm.tests.test_svm.test_unfitted()
sklearn.svm.tests.test_svm.test_unicode_kernel()
sklearn.svm.tests.test_svm.test_weight()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/tests/test_bounds.py----------------------------------------
A:sklearn.svm.tests.test_bounds.X->X_container(dense_X)
A:sklearn.svm.tests.test_bounds.min_c->l1_min_c(X, y, loss=loss, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling)
A:sklearn.svm.tests.test_bounds.generated->bounded_rand_int_wrap(100)
A:sklearn.svm.tests.test_bounds.uniform_dist->scipy.stats.uniform(loc=0, scale=range_)
A:sklearn.svm.tests.test_bounds.res->scipy.stats.kstest(sample, uniform_dist.cdf)
A:sklearn.svm.tests.test_bounds.uniform_p_vals_dist->scipy.stats.uniform(loc=0, scale=1)
A:sklearn.svm.tests.test_bounds.res_pvals->scipy.stats.kstest(ks_pvals, uniform_p_vals_dist.cdf)
A:sklearn.svm.tests.test_bounds.min_10pct_pval->numpy.percentile(ks_pvals, q=10)
sklearn.svm.tests.test_bounds.check_l1_min_c(X,y,loss,fit_intercept=True,intercept_scaling=1.0)
sklearn.svm.tests.test_bounds.test_ill_posed_min_c()
sklearn.svm.tests.test_bounds.test_l1_min_c(X_container,loss,Y_label,intercept_label)
sklearn.svm.tests.test_bounds.test_newrand_bounded_rand_int(range_,n_pts)
sklearn.svm.tests.test_bounds.test_newrand_bounded_rand_int_limits(range_)
sklearn.svm.tests.test_bounds.test_newrand_default()
sklearn.svm.tests.test_bounds.test_newrand_set_seed(seed,expected)
sklearn.svm.tests.test_bounds.test_newrand_set_seed_overflow(seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/svm/tests/test_sparse.py----------------------------------------
A:sklearn.svm.tests.test_sparse.X->csr_container((data, indices, indptr))
A:sklearn.svm.tests.test_sparse.T->numpy.array([[-1, -1], [2, 2], [3, 2]])
A:sklearn.svm.tests.test_sparse.X2->numpy.array([[0, 0, 0], [1, 1, 1], [2, 0, 0], [0, 0, 2], [3, 3, 3]])
A:sklearn.svm.tests.test_sparse.T2->numpy.array([[-1, -1, -1], [1, 1, 1], [2, 2, 2]])
A:sklearn.svm.tests.test_sparse.iris->sklearn.datasets.load_iris()
A:sklearn.svm.tests.test_sparse.rng->numpy.random.RandomState(0)
A:sklearn.svm.tests.test_sparse.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.svm.tests.test_sparse.(X_blobs, y_blobs)->make_blobs(n_samples=100, centers=10, random_state=0)
A:sklearn.svm.tests.test_sparse.sparse_svm->sklearn.base.clone(dense_svm)
A:sklearn.svm.tests.test_sparse.X_test_dense->csr_container(X[50:100]).toarray()
A:sklearn.svm.tests.test_sparse.X_train->sparse_container(X_train)
A:sklearn.svm.tests.test_sparse.clf->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y)
A:sklearn.svm.tests.test_sparse.(X, y)->load_digits(return_X_y=True)
A:sklearn.svm.tests.test_sparse.X_test->csr_container(X[50:100])
A:sklearn.svm.tests.test_sparse.X_sparse->csr_container(X)
A:sklearn.svm.tests.test_sparse.sparse_svc->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse, y)
A:sklearn.svm.tests.test_sparse.row_slice->slice(*X.indptr[i - 1:i + 1])
A:sklearn.svm.tests.test_sparse.X_sparse_unsorted->scramble_indices(X_sparse)
A:sklearn.svm.tests.test_sparse.X_test_unsorted->scramble_indices(X_test)
A:sklearn.svm.tests.test_sparse.unsorted_svc->sklearn.svm.SVC(kernel='linear', probability=True, random_state=0).fit(X_sparse_unsorted, y)
A:sklearn.svm.tests.test_sparse.X_sp->lil_container(X)
A:sklearn.svm.tests.test_sparse.clf_lin->sklearn.svm.SVC(kernel='linear').fit(X_sp, Y)
A:sklearn.svm.tests.test_sparse.clf_mylin->sklearn.svm.SVC(kernel=kfunc).fit(X_sp, Y)
A:sklearn.svm.tests.test_sparse.iris_data_sp->csr_container(iris.data)
A:sklearn.svm.tests.test_sparse.sp_clf->sklearn.svm.SVC(kernel='linear').fit(X.tocoo(), y)
A:sklearn.svm.tests.test_sparse.svc->sklearn.svm.SVC(kernel='linear', C=0.1, decision_function_shape='ovo')
A:sklearn.svm.tests.test_sparse.prediction->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y).predict(X)
A:sklearn.svm.tests.test_sparse.expected->numpy.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])
A:sklearn.svm.tests.test_sparse.X2_sp->dok_container(X2)
A:sklearn.svm.tests.test_sparse.pred->sklearn.base.clone(a).predict(X_sp)
A:sklearn.svm.tests.test_sparse.(X_, y_)->make_classification(n_samples=200, n_features=100, weights=[0.833, 0.167], random_state=0)
A:sklearn.svm.tests.test_sparse.X_->csr_container(X_)
A:sklearn.svm.tests.test_sparse.y_pred->sklearn.svm.SVC(kernel='linear').fit(X.toarray(), y).predict(X_[180:])
A:sklearn.svm.tests.test_sparse.data->numpy.array([0.03771744, 0.1003567, 0.01174647, 0.027069])
A:sklearn.svm.tests.test_sparse.indices->numpy.array([6, 5, 35, 31], dtype=np.int32)
A:sklearn.svm.tests.test_sparse.indptr->numpy.array([0] * 8 + [1] * 32 + [2] * 38 + [4] * 3, dtype=np.int32)
A:sklearn.svm.tests.test_sparse.y->numpy.array([1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 3.0, 0.0, 1.0, 1.0, 3.0, 2.0, 3.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 1.0, 3.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 3.0, 0.0, 0.0, 2.0, 2.0, 1.0, 3.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 3.0])
A:sklearn.svm.tests.test_sparse.a->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0)
A:sklearn.svm.tests.test_sparse.b->sklearn.base.clone(a)
A:sklearn.svm.tests.test_sparse.dense_svm->sklearn.svm.SVC(C=1, kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0)
A:sklearn.svm.tests.test_sparse.pred_dense->sklearn.svm.SVC(C=1, kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0).fit(X, Y).predict(X)
A:sklearn.svm.tests.test_sparse.sp->sklearn.svm.SVC(C=1, kernel=lambda x, y: x @ y.T, probability=True, random_state=0, max_iter=1)
A:sklearn.svm.tests.test_sparse.proba_1->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
A:sklearn.svm.tests.test_sparse.proba_2->sklearn.svm.SVC(probability=True, max_iter=1, random_state=0).fit(X, Y).predict_proba(X)
sklearn.svm.tests.test_sparse.check_svm_model_equal(dense_svm,X_train,y_train,X_test)
sklearn.svm.tests.test_sparse.test_consistent_proba()
sklearn.svm.tests.test_sparse.test_error(lil_container)
sklearn.svm.tests.test_sparse.test_linearsvc(lil_container,dok_container)
sklearn.svm.tests.test_sparse.test_linearsvc_iris(csr_container)
sklearn.svm.tests.test_sparse.test_sample_weights(lil_container)
sklearn.svm.tests.test_sparse.test_sparse_decision_function(csr_container)
sklearn.svm.tests.test_sparse.test_sparse_liblinear_intercept_handling()
sklearn.svm.tests.test_sparse.test_sparse_oneclasssvm(X_train,y_train,X_test,kernel,sparse_container)
sklearn.svm.tests.test_sparse.test_sparse_realdata(csr_container)
sklearn.svm.tests.test_sparse.test_sparse_svc_clone_with_callable_kernel(lil_container)
sklearn.svm.tests.test_sparse.test_svc(X_train,y_train,X_test,kernel,sparse_container)
sklearn.svm.tests.test_sparse.test_svc_iris(csr_container,kernel)
sklearn.svm.tests.test_sparse.test_svc_with_custom_kernel(lil_container)
sklearn.svm.tests.test_sparse.test_timeout(lil_container)
sklearn.svm.tests.test_sparse.test_unsorted_indices(csr_container)
sklearn.svm.tests.test_sparse.test_weight(csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_lof.py----------------------------------------
A:sklearn.neighbors._lof.self.n_neighbors_->max(1, min(self.n_neighbors, n_samples - 1))
A:sklearn.neighbors._lof.(self._distances_fit_X_, _neighbors_indices_fit_X_)->self.kneighbors(n_neighbors=self.n_neighbors_)
A:sklearn.neighbors._lof.self._distances_fit_X_->self._distances_fit_X_.astype(self._fit_X.dtype, copy=False)
A:sklearn.neighbors._lof.self._lrd->self._local_reachability_density(self._distances_fit_X_, _neighbors_indices_fit_X_)
A:sklearn.neighbors._lof.self.offset_->numpy.percentile(self.negative_outlier_factor_, 100.0 * self.contamination)
A:sklearn.neighbors._lof.X->check_array(X, accept_sparse='csr')
A:sklearn.neighbors._lof.is_inlier->numpy.ones(self.n_samples_fit_, dtype=int)
A:sklearn.neighbors._lof.(distances_X, neighbors_indices_X)->self.kneighbors(X, n_neighbors=self.n_neighbors_)
A:sklearn.neighbors._lof.distances_X->distances_X.astype(X.dtype, copy=False).astype(X.dtype, copy=False)
A:sklearn.neighbors._lof.X_lrd->self._local_reachability_density(distances_X, neighbors_indices_X)
A:sklearn.neighbors._lof.reach_dist_array->numpy.maximum(distances_X, dist_k)
sklearn.neighbors.LocalOutlierFactor(self,n_neighbors=20,*,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination='auto',novelty=False,n_jobs=None)
sklearn.neighbors.LocalOutlierFactor._check_novelty_decision_function(self)
sklearn.neighbors.LocalOutlierFactor._check_novelty_fit_predict(self)
sklearn.neighbors.LocalOutlierFactor._check_novelty_predict(self)
sklearn.neighbors.LocalOutlierFactor._check_novelty_score_samples(self)
sklearn.neighbors.LocalOutlierFactor._local_reachability_density(self,distances_X,neighbors_indices)
sklearn.neighbors.LocalOutlierFactor._more_tags(self)
sklearn.neighbors.LocalOutlierFactor._predict(self,X=None)
sklearn.neighbors.LocalOutlierFactor.decision_function(self,X)
sklearn.neighbors.LocalOutlierFactor.fit(self,X,y=None)
sklearn.neighbors.LocalOutlierFactor.fit_predict(self,X,y=None)
sklearn.neighbors.LocalOutlierFactor.predict(self,X=None)
sklearn.neighbors.LocalOutlierFactor.score_samples(self,X)
sklearn.neighbors._lof.LocalOutlierFactor(self,n_neighbors=20,*,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination='auto',novelty=False,n_jobs=None)
sklearn.neighbors._lof.LocalOutlierFactor.__init__(self,n_neighbors=20,*,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,contamination='auto',novelty=False,n_jobs=None)
sklearn.neighbors._lof.LocalOutlierFactor._check_novelty_decision_function(self)
sklearn.neighbors._lof.LocalOutlierFactor._check_novelty_fit_predict(self)
sklearn.neighbors._lof.LocalOutlierFactor._check_novelty_predict(self)
sklearn.neighbors._lof.LocalOutlierFactor._check_novelty_score_samples(self)
sklearn.neighbors._lof.LocalOutlierFactor._local_reachability_density(self,distances_X,neighbors_indices)
sklearn.neighbors._lof.LocalOutlierFactor._more_tags(self)
sklearn.neighbors._lof.LocalOutlierFactor._predict(self,X=None)
sklearn.neighbors._lof.LocalOutlierFactor.decision_function(self,X)
sklearn.neighbors._lof.LocalOutlierFactor.fit(self,X,y=None)
sklearn.neighbors._lof.LocalOutlierFactor.fit_predict(self,X,y=None)
sklearn.neighbors._lof.LocalOutlierFactor.predict(self,X=None)
sklearn.neighbors._lof.LocalOutlierFactor.score_samples(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_nca.py----------------------------------------
A:sklearn.neighbors._nca.(X, y)->self._validate_data(X, y, ensure_min_samples=2)
A:sklearn.neighbors._nca.y->LabelEncoder().fit_transform(y)
A:sklearn.neighbors._nca.init->check_array(init)
A:sklearn.neighbors._nca.self.random_state_->check_random_state(self.random_state)
A:sklearn.neighbors._nca.t_train->time.time()
A:sklearn.neighbors._nca.transformation->transformation.reshape(-1, X.shape[1]).reshape(-1, X.shape[1])
A:sklearn.neighbors._nca.opt_result->minimize(**optimizer_params)
A:sklearn.neighbors._nca.self.components_->minimize(**optimizer_params).x.reshape(-1, X.shape[1])
A:sklearn.neighbors._nca.X->self._validate_data(X, reset=False)
A:sklearn.neighbors._nca.n_classes->len(np.unique(y))
A:sklearn.neighbors._nca.init_time->time.time()
A:sklearn.neighbors._nca.pca->PCA(n_components=n_components, random_state=self.random_state_)
A:sklearn.neighbors._nca.lda->LinearDiscriminantAnalysis(n_components=n_components)
A:sklearn.neighbors._nca.header->header_fmt.format(*header_fields)
A:sklearn.neighbors._nca.t_funcall->time.time()
A:sklearn.neighbors._nca.X_embedded->numpy.dot(X, transformation.T)
A:sklearn.neighbors._nca.p_ij->softmax(-p_ij)
A:sklearn.neighbors._nca.p->numpy.sum(masked_p_ij, axis=1, keepdims=True)
A:sklearn.neighbors._nca.loss->numpy.sum(p)
sklearn.neighbors.NeighborhoodComponentsAnalysis(self,n_components=None,*,init='auto',warm_start=False,max_iter=50,tol=1e-05,callback=None,verbose=0,random_state=None)
sklearn.neighbors.NeighborhoodComponentsAnalysis._callback(self,transformation)
sklearn.neighbors.NeighborhoodComponentsAnalysis._initialize(self,X,y,init)
sklearn.neighbors.NeighborhoodComponentsAnalysis._loss_grad_lbfgs(self,transformation,X,same_class_mask,sign=1.0)
sklearn.neighbors.NeighborhoodComponentsAnalysis._more_tags(self)
sklearn.neighbors.NeighborhoodComponentsAnalysis.fit(self,X,y)
sklearn.neighbors.NeighborhoodComponentsAnalysis.transform(self,X)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis(self,n_components=None,*,init='auto',warm_start=False,max_iter=50,tol=1e-05,callback=None,verbose=0,random_state=None)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis.__init__(self,n_components=None,*,init='auto',warm_start=False,max_iter=50,tol=1e-05,callback=None,verbose=0,random_state=None)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis._callback(self,transformation)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis._initialize(self,X,y,init)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis._loss_grad_lbfgs(self,transformation,X,same_class_mask,sign=1.0)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis._more_tags(self)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis.fit(self,X,y)
sklearn.neighbors._nca.NeighborhoodComponentsAnalysis.transform(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_classification.py----------------------------------------
A:sklearn.neighbors._classification.probabilities->metrics._pairwise_distances_reduction.RadiusNeighborsClassMode.compute(X=X, Y=self._fit_X, radius=self.radius, weights=self.weights, Y_labels=self._y, unique_Y_labels=self.classes_, outlier_label=self.outlier_label, metric=metric, metric_kwargs=metric_kwargs, strategy='parallel_on_X')
A:sklearn.neighbors._classification.neigh_ind->self.kneighbors(X, return_distance=False)
A:sklearn.neighbors._classification.(neigh_dist, neigh_ind)->self.radius_neighbors(X)
A:sklearn.neighbors._classification._y->self._y.reshape((-1, 1))
A:sklearn.neighbors._classification.n_outputs->len(classes_)
A:sklearn.neighbors._classification.n_queries->_num_samples(X)
A:sklearn.neighbors._classification.weights->_get_weights(neigh_dist, self.weights)
A:sklearn.neighbors._classification.y_pred->y_pred.ravel().ravel()
A:sklearn.neighbors._classification.(mode, _)->weighted_mode(_y[neigh_ind, k], weights, axis=1)
A:sklearn.neighbors._classification.mode->numpy.asarray(mode.ravel(), dtype=np.intp)
A:sklearn.neighbors._classification.y_pred[:, k]->classes_[k].take(max_prob_index)
A:sklearn.neighbors._classification.(metric, metric_kwargs)->_adjusted_metric(metric=self.metric, metric_kwargs=self.metric_params, p=self.p)
A:sklearn.neighbors._classification.X->self._validate_data(X, accept_sparse='csr', reset=False, order='C')
A:sklearn.neighbors._classification.all_rows->numpy.arange(n_queries)
A:sklearn.neighbors._classification.proba_k->numpy.zeros((n_queries, classes_k.size))
A:sklearn.neighbors._classification.label_count->numpy.bincount(_y[:, k])
A:sklearn.neighbors._classification.probs->self.predict_proba(X)
A:sklearn.neighbors._classification.max_prob_index->prob.argmax(axis=1)
A:sklearn.neighbors._classification.outlier_zero_probs->(prob == 0).all(axis=1)
A:sklearn.neighbors._classification.zero_prob_index->numpy.flatnonzero(outlier_zero_probs)
A:sklearn.neighbors._classification.outlier_mask->numpy.zeros(n_queries, dtype=bool)
A:sklearn.neighbors._classification.outliers->numpy.flatnonzero(outlier_mask)
A:sklearn.neighbors._classification.inliers->numpy.flatnonzero(~outlier_mask)
A:sklearn.neighbors._classification.pred_labels->numpy.zeros(len(neigh_ind), dtype=object)
A:sklearn.neighbors._classification.proba_inl->numpy.zeros((len(inliers), classes_k.size))
A:sklearn.neighbors._classification.proba_inl[i, :]->numpy.bincount(idx, weights[i], minlength=classes_k.size)
A:sklearn.neighbors._classification.label_index->numpy.flatnonzero(classes_k == _outlier_label)
sklearn.neighbors.KNeighborsClassifier(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors.KNeighborsClassifier._more_tags(self)
sklearn.neighbors.KNeighborsClassifier.fit(self,X,y)
sklearn.neighbors.KNeighborsClassifier.predict(self,X)
sklearn.neighbors.KNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors.RadiusNeighborsClassifier(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,n_jobs=None)
sklearn.neighbors.RadiusNeighborsClassifier._more_tags(self)
sklearn.neighbors.RadiusNeighborsClassifier.fit(self,X,y)
sklearn.neighbors.RadiusNeighborsClassifier.predict(self,X)
sklearn.neighbors.RadiusNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors._classification.KNeighborsClassifier(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._classification.KNeighborsClassifier.__init__(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._classification.KNeighborsClassifier._more_tags(self)
sklearn.neighbors._classification.KNeighborsClassifier.fit(self,X,y)
sklearn.neighbors._classification.KNeighborsClassifier.predict(self,X)
sklearn.neighbors._classification.KNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors._classification.RadiusNeighborsClassifier(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,n_jobs=None)
sklearn.neighbors._classification.RadiusNeighborsClassifier.__init__(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',outlier_label=None,metric_params=None,n_jobs=None)
sklearn.neighbors._classification.RadiusNeighborsClassifier._more_tags(self)
sklearn.neighbors._classification.RadiusNeighborsClassifier.fit(self,X,y)
sklearn.neighbors._classification.RadiusNeighborsClassifier.predict(self,X)
sklearn.neighbors._classification.RadiusNeighborsClassifier.predict_proba(self,X)
sklearn.neighbors._classification._adjusted_metric(metric,metric_kwargs,p=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_regression.py----------------------------------------
A:sklearn.neighbors._regression.neigh_ind->self.kneighbors(X, return_distance=False)
A:sklearn.neighbors._regression.(neigh_dist, neigh_ind)->self.radius_neighbors(X)
A:sklearn.neighbors._regression.weights->_get_weights(neigh_dist, self.weights)
A:sklearn.neighbors._regression._y->_y.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.neighbors._regression.y_pred->y_pred.ravel().ravel()
A:sklearn.neighbors._regression.denom->numpy.sum(weights, axis=1)
A:sklearn.neighbors._regression.num->numpy.sum(_y[neigh_ind, j] * weights, axis=1)
A:sklearn.neighbors._regression.empty_obs->numpy.full_like(_y[0], np.nan)
sklearn.neighbors.KNeighborsRegressor(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors.KNeighborsRegressor._more_tags(self)
sklearn.neighbors.KNeighborsRegressor.fit(self,X,y)
sklearn.neighbors.KNeighborsRegressor.predict(self,X)
sklearn.neighbors.RadiusNeighborsRegressor(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors.RadiusNeighborsRegressor.fit(self,X,y)
sklearn.neighbors.RadiusNeighborsRegressor.predict(self,X)
sklearn.neighbors._regression.KNeighborsRegressor(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._regression.KNeighborsRegressor.__init__(self,n_neighbors=5,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._regression.KNeighborsRegressor._more_tags(self)
sklearn.neighbors._regression.KNeighborsRegressor.fit(self,X,y)
sklearn.neighbors._regression.KNeighborsRegressor.predict(self,X)
sklearn.neighbors._regression.RadiusNeighborsRegressor(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._regression.RadiusNeighborsRegressor.__init__(self,radius=1.0,*,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None,n_jobs=None)
sklearn.neighbors._regression.RadiusNeighborsRegressor.fit(self,X,y)
sklearn.neighbors._regression.RadiusNeighborsRegressor.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_unsupervised.py----------------------------------------
sklearn.neighbors.NearestNeighbors(self,*,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors.NearestNeighbors.fit(self,X,y=None)
sklearn.neighbors._unsupervised.NearestNeighbors(self,*,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._unsupervised.NearestNeighbors.__init__(self,*,n_neighbors=5,radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._unsupervised.NearestNeighbors.fit(self,X,y=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_graph.py----------------------------------------
A:sklearn.neighbors._graph.params->zip(['metric', 'p', 'metric_params'], [metric, p, metric_params])
A:sklearn.neighbors._graph.est_params->NearestNeighbors(radius=radius, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs).fit(X).get_params()
A:sklearn.neighbors._graph.X->NearestNeighbors(radius=radius, metric=metric, p=p, metric_params=metric_params, n_jobs=n_jobs).fit(X)
A:sklearn.neighbors._graph.query->_query_include_self(X._fit_X, include_self, mode)
sklearn.neighbors.KNeighborsTransformer(self,*,mode='distance',n_neighbors=5,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors.KNeighborsTransformer._more_tags(self)
sklearn.neighbors.KNeighborsTransformer.fit(self,X,y=None)
sklearn.neighbors.KNeighborsTransformer.fit_transform(self,X,y=None)
sklearn.neighbors.KNeighborsTransformer.transform(self,X)
sklearn.neighbors.RadiusNeighborsTransformer(self,*,mode='distance',radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors.RadiusNeighborsTransformer._more_tags(self)
sklearn.neighbors.RadiusNeighborsTransformer.fit(self,X,y=None)
sklearn.neighbors.RadiusNeighborsTransformer.fit_transform(self,X,y=None)
sklearn.neighbors.RadiusNeighborsTransformer.transform(self,X)
sklearn.neighbors._graph.KNeighborsTransformer(self,*,mode='distance',n_neighbors=5,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._graph.KNeighborsTransformer.__init__(self,*,mode='distance',n_neighbors=5,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._graph.KNeighborsTransformer._more_tags(self)
sklearn.neighbors._graph.KNeighborsTransformer.fit(self,X,y=None)
sklearn.neighbors._graph.KNeighborsTransformer.fit_transform(self,X,y=None)
sklearn.neighbors._graph.KNeighborsTransformer.transform(self,X)
sklearn.neighbors._graph.RadiusNeighborsTransformer(self,*,mode='distance',radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._graph.RadiusNeighborsTransformer.__init__(self,*,mode='distance',radius=1.0,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._graph.RadiusNeighborsTransformer._more_tags(self)
sklearn.neighbors._graph.RadiusNeighborsTransformer.fit(self,X,y=None)
sklearn.neighbors._graph.RadiusNeighborsTransformer.fit_transform(self,X,y=None)
sklearn.neighbors._graph.RadiusNeighborsTransformer.transform(self,X)
sklearn.neighbors._graph._check_params(X,metric,p,metric_params)
sklearn.neighbors._graph._query_include_self(X,include_self,mode)
sklearn.neighbors._graph.kneighbors_graph(X,n_neighbors,*,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=None)
sklearn.neighbors._graph.radius_neighbors_graph(X,radius,*,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=None)
sklearn.neighbors.kneighbors_graph(X,n_neighbors,*,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=None)
sklearn.neighbors.radius_neighbors_graph(X,radius,*,mode='connectivity',metric='minkowski',p=2,metric_params=None,include_self=False,n_jobs=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_kde.py----------------------------------------
A:sklearn.neighbors._kde.algorithm->self._choose_algorithm(self.algorithm, self.metric)
A:sklearn.neighbors._kde.X->check_random_state(random_state).normal(size=(n_samples, dim))
A:sklearn.neighbors._kde.sample_weight->_check_sample_weight(sample_weight, X, dtype=np.float64, only_non_negative=True)
A:sklearn.neighbors._kde.self.tree_->TREE_DICT[algorithm](X, metric=self.metric, leaf_size=self.leaf_size, sample_weight=sample_weight, **kwargs)
A:sklearn.neighbors._kde.log_density->self.tree_.kernel_density(X, h=self.bandwidth_, kernel=self.kernel, atol=atol_N, rtol=self.rtol, breadth_first=self.breadth_first, return_log=True)
A:sklearn.neighbors._kde.data->numpy.asarray(self.tree_.data)
A:sklearn.neighbors._kde.rng->check_random_state(random_state)
A:sklearn.neighbors._kde.u->check_random_state(random_state).uniform(0, 1, size=n_samples)
A:sklearn.neighbors._kde.i->numpy.searchsorted(cumsum_weight, u * sum_weight)
A:sklearn.neighbors._kde.cumsum_weight->numpy.cumsum(np.asarray(self.tree_.sample_weight))
A:sklearn.neighbors._kde.s_sq->row_norms(X, squared=True)
sklearn.neighbors.KernelDensity(self,*,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors.KernelDensity._choose_algorithm(self,algorithm,metric)
sklearn.neighbors.KernelDensity._more_tags(self)
sklearn.neighbors.KernelDensity.fit(self,X,y=None,sample_weight=None)
sklearn.neighbors.KernelDensity.sample(self,n_samples=1,random_state=None)
sklearn.neighbors.KernelDensity.score(self,X,y=None)
sklearn.neighbors.KernelDensity.score_samples(self,X)
sklearn.neighbors._kde.KernelDensity(self,*,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors._kde.KernelDensity.__init__(self,*,bandwidth=1.0,algorithm='auto',kernel='gaussian',metric='euclidean',atol=0,rtol=0,breadth_first=True,leaf_size=40,metric_params=None)
sklearn.neighbors._kde.KernelDensity._choose_algorithm(self,algorithm,metric)
sklearn.neighbors._kde.KernelDensity._more_tags(self)
sklearn.neighbors._kde.KernelDensity.fit(self,X,y=None,sample_weight=None)
sklearn.neighbors._kde.KernelDensity.sample(self,n_samples=1,random_state=None)
sklearn.neighbors._kde.KernelDensity.score(self,X,y=None)
sklearn.neighbors._kde.KernelDensity.score_samples(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_base.py----------------------------------------
A:sklearn.neighbors._base.VALID_METRICS->dict(ball_tree=BallTree.valid_metrics, kd_tree=KDTree.valid_metrics, brute=sorted(set(PAIRWISE_DISTANCE_FUNCTIONS).union(SCIPY_METRICS)))
A:sklearn.neighbors._base.VALID_METRICS_SPARSE->dict(ball_tree=[], kd_tree=[], brute=PAIRWISE_DISTANCE_FUNCTIONS.keys() - {'haversine', 'nan_euclidean'})
A:sklearn.neighbors._base.inf_mask->numpy.isinf(dist)
A:sklearn.neighbors._base.inf_row->numpy.any(inf_mask, axis=1)
A:sklearn.neighbors._base.line_change->numpy.unique(graph.indptr[1:-1] - 1)
A:sklearn.neighbors._base.X->self._validate_data(X, accept_sparse='csr', reset=False, order='C')
A:sklearn.neighbors._base.graph->graph.copy().copy()
A:sklearn.neighbors._base.row_nnz->numpy.diff(graph.indptr)
A:sklearn.neighbors._base.distances->graph.copy().copy().data.reshape(n_samples, -1)
A:sklearn.neighbors._base.order->numpy.argsort(neigh_dist[ii], kind='mergesort')
A:sklearn.neighbors._base.row_nnz_min->numpy.diff(graph.indptr).min()
A:sklearn.neighbors._base.idx->numpy.tile(np.arange(n_neighbors), (n_samples, 1))
A:sklearn.neighbors._base.no_filter_needed->bool(graph.data.max() <= radius)
A:sklearn.neighbors._base.data->numpy.compress(mask, graph.data)
A:sklearn.neighbors._base.indices->indices.astype(np.intp, copy=no_filter_needed).astype(np.intp, copy=no_filter_needed)
A:sklearn.neighbors._base.neigh_dist->_to_object_array(neigh_dist_list)
A:sklearn.neighbors._base.neigh_ind->_to_object_array(neigh_ind_list)
A:sklearn.neighbors._base.(X, y)->self._validate_data(X, y, accept_sparse='csr', multi_output=True, order='C')
A:sklearn.neighbors._base.y->y.reshape((-1, 1)).reshape((-1, 1))
A:sklearn.neighbors._base.self._y->self._y.ravel()
A:sklearn.neighbors._base.(classes, self._y[:, k])->numpy.unique(y[:, k], return_inverse=True)
A:sklearn.neighbors._base.self.effective_metric_params_->self.metric_params.copy()
A:sklearn.neighbors._base.effective_p->self.effective_metric_params_.get('p', self.p)
A:sklearn.neighbors._base.p->self.effective_metric_params_.pop('p', 2)
A:sklearn.neighbors._base.w->self.effective_metric_params_.pop('w', None)
A:sklearn.neighbors._base.self._fit_X->self._validate_data(X, accept_sparse='csr', reset=False, order='C').copy()
A:sklearn.neighbors._base.self._tree->KDTree(X, self.leaf_size, metric=self.effective_metric_, **self.effective_metric_params_)
A:sklearn.neighbors._base.n_jobs->effective_n_jobs(self.n_jobs)
A:sklearn.neighbors._base.results->numpy.hstack(chunked_results)
A:sklearn.neighbors._base.reduce_func->partial(self._radius_neighbors_reduce_func, radius=radius, return_distance=return_distance)
A:sklearn.neighbors._base.chunked_results->Parallel(n_jobs, prefer='threads')((delayed_query(self._tree, X[s], radius, return_distance, sort_results=sort_results) for s in gen_even_slices(X.shape[0], n_jobs)))
A:sklearn.neighbors._base.(neigh_dist, neigh_ind)->zip(*chunked_results)
A:sklearn.neighbors._base.dup_gr_nbrs->numpy.all(sample_mask, axis=1)
A:sklearn.neighbors._base.A_ind->numpy.concatenate(list(A_ind))
A:sklearn.neighbors._base.A_data->numpy.ones(len(A_ind))
A:sklearn.neighbors._base.(A_data, A_ind)->self.kneighbors(X, n_neighbors, return_distance=True)
A:sklearn.neighbors._base.A_indptr->numpy.concatenate((np.zeros(1, dtype=int), np.cumsum(n_neighbors)))
A:sklearn.neighbors._base.kneighbors_graph->csr_matrix((A_data, A_ind.ravel(), A_indptr), shape=(n_queries, n_samples_fit))
A:sklearn.neighbors._base.(neigh_dist_chunks, neigh_ind_chunks)->zip(*chunked_results)
A:sklearn.neighbors._base.neigh_dist_list->sum(neigh_dist_chunks, [])
A:sklearn.neighbors._base.neigh_ind_list->sum(chunked_results, [])
A:sklearn.neighbors._base.delayed_query->delayed(_tree_query_radius_parallel_helper)
A:sklearn.neighbors._base.(neigh_ind, neigh_dist)->tuple(zip(*chunked_results))
A:sklearn.neighbors._base.(dist, A_ind)->self.radius_neighbors(X, radius, return_distance=True, sort_results=sort_results)
A:sklearn.neighbors._base.n_neighbors->numpy.array([len(a) for a in A_ind])
sklearn.neighbors._base.KNeighborsMixin
sklearn.neighbors._base.KNeighborsMixin._kneighbors_reduce_func(self,dist,start,n_neighbors,return_distance)
sklearn.neighbors._base.KNeighborsMixin.kneighbors(self,X=None,n_neighbors=None,return_distance=True)
sklearn.neighbors._base.KNeighborsMixin.kneighbors_graph(self,X=None,n_neighbors=None,mode='connectivity')
sklearn.neighbors._base.NeighborsBase(self,n_neighbors=None,radius=None,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._base.NeighborsBase.__init__(self,n_neighbors=None,radius=None,algorithm='auto',leaf_size=30,metric='minkowski',p=2,metric_params=None,n_jobs=None)
sklearn.neighbors._base.NeighborsBase._check_algorithm_metric(self)
sklearn.neighbors._base.NeighborsBase._fit(self,X,y=None)
sklearn.neighbors._base.NeighborsBase._more_tags(self)
sklearn.neighbors._base.RadiusNeighborsMixin
sklearn.neighbors._base.RadiusNeighborsMixin._radius_neighbors_reduce_func(self,dist,start,radius,return_distance)
sklearn.neighbors._base.RadiusNeighborsMixin.radius_neighbors(self,X=None,radius=None,return_distance=True,sort_results=False)
sklearn.neighbors._base.RadiusNeighborsMixin.radius_neighbors_graph(self,X=None,radius=None,mode='connectivity',sort_results=False)
sklearn.neighbors._base._check_precomputed(X)
sklearn.neighbors._base._get_weights(dist,weights)
sklearn.neighbors._base._is_sorted_by_data(graph)
sklearn.neighbors._base._kneighbors_from_graph(graph,n_neighbors,return_distance)
sklearn.neighbors._base._radius_neighbors_from_graph(graph,radius,return_distance)
sklearn.neighbors._base._tree_query_parallel_helper(tree,*args,**kwargs)
sklearn.neighbors._base._tree_query_radius_parallel_helper(tree,*args,**kwargs)
sklearn.neighbors._base.sort_graph_by_row_values(graph,copy=False,warn_when_not_sorted=True)
sklearn.neighbors.sort_graph_by_row_values(graph,copy=False,warn_when_not_sorted=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/_nearest_centroid.py----------------------------------------
A:sklearn.neighbors._nearest_centroid.(X, y)->self._validate_data(X, y, accept_sparse=['csr', 'csc'])
A:sklearn.neighbors._nearest_centroid.is_X_sparse->scipy.sparse.issparse(X)
A:sklearn.neighbors._nearest_centroid.le->LabelEncoder()
A:sklearn.neighbors._nearest_centroid.y_ind->LabelEncoder().fit_transform(y)
A:sklearn.neighbors._nearest_centroid.self.centroids_->numpy.empty((n_classes, n_features), dtype=np.float64)
A:sklearn.neighbors._nearest_centroid.nk->numpy.zeros(n_classes)
A:sklearn.neighbors._nearest_centroid.nk[cur_class]->numpy.sum(center_mask)
A:sklearn.neighbors._nearest_centroid.self.centroids_[cur_class]->X[center_mask].mean(axis=0)
A:sklearn.neighbors._nearest_centroid.dataset_centroid_->numpy.mean(X, axis=0)
A:sklearn.neighbors._nearest_centroid.m->numpy.sqrt(1.0 / nk - 1.0 / n_samples)
A:sklearn.neighbors._nearest_centroid.variance->variance.sum(axis=0).sum(axis=0)
A:sklearn.neighbors._nearest_centroid.s->numpy.sqrt(variance / (n_samples - n_classes))
A:sklearn.neighbors._nearest_centroid.mm->numpy.sqrt(1.0 / nk - 1.0 / n_samples).reshape(len(m), 1)
A:sklearn.neighbors._nearest_centroid.signs->numpy.sign(deviation)
A:sklearn.neighbors._nearest_centroid.X->self._validate_data(X, accept_sparse='csr', reset=False)
sklearn.neighbors.NearestCentroid(self,metric='euclidean',*,shrink_threshold=None)
sklearn.neighbors.NearestCentroid.fit(self,X,y)
sklearn.neighbors.NearestCentroid.predict(self,X)
sklearn.neighbors._nearest_centroid.NearestCentroid(self,metric='euclidean',*,shrink_threshold=None)
sklearn.neighbors._nearest_centroid.NearestCentroid.__init__(self,metric='euclidean',*,shrink_threshold=None)
sklearn.neighbors._nearest_centroid.NearestCentroid.fit(self,X,y)
sklearn.neighbors._nearest_centroid.NearestCentroid.predict(self,X)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_lof.py----------------------------------------
A:sklearn.neighbors.tests.test_lof.rng->numpy.random.RandomState(random_state)
A:sklearn.neighbors.tests.test_lof.iris->load_iris()
A:sklearn.neighbors.tests.test_lof.perm->numpy.random.RandomState(random_state).permutation(iris.target.size)
A:sklearn.neighbors.tests.test_lof.X->numpy.concatenate([inliers, outliers], axis=0).astype(np.float32)
A:sklearn.neighbors.tests.test_lof.clf->sklearn.neighbors.LocalOutlierFactor(contamination=contamination)
A:sklearn.neighbors.tests.test_lof.X_outliers->numpy.random.RandomState(random_state).uniform(low=-4, high=4, size=(20, 2)).astype(global_dtype, copy=False)
A:sklearn.neighbors.tests.test_lof.y_test->numpy.array([0] * 20 + [1] * 20)
A:sklearn.neighbors.tests.test_lof.X_train->numpy.asarray([[1, 1], [1, 2], [2, 1]], dtype=global_dtype)
A:sklearn.neighbors.tests.test_lof.clf1->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, contamination=0.1, novelty=True).fit(X_train)
A:sklearn.neighbors.tests.test_lof.clf2->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, novelty=True).fit(X_train)
A:sklearn.neighbors.tests.test_lof.Y->numpy.random.RandomState(random_state).random_sample((3, 4)).astype(global_dtype, copy=False)
A:sklearn.neighbors.tests.test_lof.DXX->sklearn.metrics.pairwise_distances(X, metric='euclidean')
A:sklearn.neighbors.tests.test_lof.DYX->sklearn.metrics.pairwise_distances(Y, X, metric='euclidean')
A:sklearn.neighbors.tests.test_lof.lof_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, novelty=True)
A:sklearn.neighbors.tests.test_lof.pred_X_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, novelty=True)._predict()
A:sklearn.neighbors.tests.test_lof.pred_X_Y->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, novelty=True).predict(Y)
A:sklearn.neighbors.tests.test_lof.lof_D->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed', novelty=True)
A:sklearn.neighbors.tests.test_lof.pred_D_X->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed', novelty=True)._predict()
A:sklearn.neighbors.tests.test_lof.pred_D_Y->sklearn.neighbors.LocalOutlierFactor(n_neighbors=3, algorithm='brute', metric='precomputed', novelty=True).predict(DYX)
A:sklearn.neighbors.tests.test_lof.X_test->numpy.asarray([[2.0, 2.0]], dtype=global_dtype)
A:sklearn.neighbors.tests.test_lof.clf1_scores->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, contamination=0.1, novelty=True).fit(X_train).score_samples(X_test)
A:sklearn.neighbors.tests.test_lof.clf1_decisions->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, contamination=0.1, novelty=True).fit(X_train).decision_function(X_test)
A:sklearn.neighbors.tests.test_lof.clf2_scores->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, novelty=True).fit(X_train).score_samples(X_test)
A:sklearn.neighbors.tests.test_lof.clf2_decisions->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2, novelty=True).fit(X_train).decision_function(X_test)
A:sklearn.neighbors.tests.test_lof.msg->'{} is not available when novelty=False'.format(method)
A:sklearn.neighbors.tests.test_lof.clf_1->sklearn.neighbors.LocalOutlierFactor()
A:sklearn.neighbors.tests.test_lof.clf_2->sklearn.neighbors.LocalOutlierFactor(novelty=True)
A:sklearn.neighbors.tests.test_lof.y_pred->getattr(iso, method)(X)
A:sklearn.neighbors.tests.test_lof.num_outliers->numpy.sum(y_pred != 1)
A:sklearn.neighbors.tests.test_lof.lof->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2).fit(X[:2])
A:sklearn.neighbors.tests.test_lof.(distances, indices)->sklearn.neighbors.LocalOutlierFactor(n_neighbors=2).fit(X[:2]).kneighbors(X, n_neighbors=2)
A:sklearn.neighbors.tests.test_lof.iso->sklearn.neighbors.LocalOutlierFactor(n_neighbors=5, algorithm=algorithm, contamination=contamination, novelty=novelty)
A:sklearn.neighbors.tests.test_lof.lof_32->sklearn.neighbors.LocalOutlierFactor(algorithm=algorithm, novelty=novelty, contamination=contamination)
A:sklearn.neighbors.tests.test_lof.X_32->numpy.concatenate([inliers, outliers], axis=0).astype(np.float32).astype(np.float32, copy=True)
A:sklearn.neighbors.tests.test_lof.lof_64->sklearn.neighbors.LocalOutlierFactor(algorithm=algorithm, novelty=novelty, contamination=contamination)
A:sklearn.neighbors.tests.test_lof.X_64->numpy.concatenate([inliers, outliers], axis=0).astype(np.float32).astype(np.float64, copy=True)
A:sklearn.neighbors.tests.test_lof.y_pred_32->getattr(lof_32, method)(X_32)
A:sklearn.neighbors.tests.test_lof.y_pred_64->getattr(lof_64, method)(X_64)
sklearn.neighbors.tests.test_lof.test_hasattr_prediction()
sklearn.neighbors.tests.test_lof.test_lof(global_dtype)
sklearn.neighbors.tests.test_lof.test_lof_dtype_equivalence(algorithm,novelty,contamination)
sklearn.neighbors.tests.test_lof.test_lof_error_n_neighbors_too_large()
sklearn.neighbors.tests.test_lof.test_lof_input_dtype_preservation(global_dtype,algorithm,contamination,novelty)
sklearn.neighbors.tests.test_lof.test_lof_performance(global_dtype)
sklearn.neighbors.tests.test_lof.test_lof_precomputed(global_dtype,random_state=42)
sklearn.neighbors.tests.test_lof.test_lof_values(global_dtype)
sklearn.neighbors.tests.test_lof.test_n_neighbors_attribute()
sklearn.neighbors.tests.test_lof.test_novelty_errors()
sklearn.neighbors.tests.test_lof.test_novelty_training_scores(global_dtype)
sklearn.neighbors.tests.test_lof.test_novelty_true_common_tests(estimator,check)
sklearn.neighbors.tests.test_lof.test_predicted_outlier_number(expected_outliers)
sklearn.neighbors.tests.test_lof.test_score_samples(global_dtype)
sklearn.neighbors.tests.test_lof.test_sparse(csr_container)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_nca.py----------------------------------------
A:sklearn.neighbors.tests.test_nca.rng->numpy.random.RandomState(42)
A:sklearn.neighbors.tests.test_nca.iris->load_iris()
A:sklearn.neighbors.tests.test_nca.perm->numpy.random.RandomState(42).permutation(iris.target.size)
A:sklearn.neighbors.tests.test_nca.X->numpy.random.RandomState(42).randn(n_samples, n_features)
A:sklearn.neighbors.tests.test_nca.y->LabelEncoder().fit_transform(y)
A:sklearn.neighbors.tests.test_nca.nca->NeighborhoodComponentsAnalysis(**{param: value})
A:sklearn.neighbors.tests.test_nca.X_t->NeighborhoodComponentsAnalysis(**{param: value}).fit_transform(X, y)
A:sklearn.neighbors.tests.test_nca.two_points->numpy.random.RandomState(42).randn(2, input_dim)
A:sklearn.neighbors.tests.test_nca.self.fake_nca->NeighborhoodComponentsAnalysis()
A:sklearn.neighbors.tests.test_nca.(self.X, y)->self.fake_nca._validate_data(X, y, ensure_min_samples=2)
A:sklearn.neighbors.tests.test_nca.(self.loss, _)->self.fake_nca._loss_grad_lbfgs(transformation, self.X, self.same_class_mask, -1.0)
A:sklearn.neighbors.tests.test_nca.loss_storer->LossStorer(X, y)
A:sklearn.neighbors.tests.test_nca.(X, y)->make_blobs(n_samples=30, centers=6, n_features=5, random_state=0)
A:sklearn.neighbors.tests.test_nca.M->numpy.random.RandomState(42).randn(rng.randint(1, X.shape[1] + 1), X.shape[1])
A:sklearn.neighbors.tests.test_nca.diff->check_grad(fun, grad, M.ravel())
A:sklearn.neighbors.tests.test_nca.init->numpy.random.RandomState(42).randn(X.shape[1], X.shape[1])
A:sklearn.neighbors.tests.test_nca.transformation->numpy.arange(9).reshape(3, 3)
A:sklearn.neighbors.tests.test_nca.nca_random->NeighborhoodComponentsAnalysis(init='random')
A:sklearn.neighbors.tests.test_nca.nca_auto->NeighborhoodComponentsAnalysis(init='auto')
A:sklearn.neighbors.tests.test_nca.nca_pca->NeighborhoodComponentsAnalysis(init='pca')
A:sklearn.neighbors.tests.test_nca.nca_lda->NeighborhoodComponentsAnalysis(init='lda')
A:sklearn.neighbors.tests.test_nca.nca_base->NeighborhoodComponentsAnalysis(init='auto', n_components=n_components, max_iter=1, random_state=rng)
A:sklearn.neighbors.tests.test_nca.nca_other->clone(nca_base).set_params(init='identity')
A:sklearn.neighbors.tests.test_nca.(X_less_features, y)->make_classification(n_samples=30, n_features=4, n_classes=4, n_redundant=0, n_informative=4, random_state=0)
A:sklearn.neighbors.tests.test_nca.nca_warm->NeighborhoodComponentsAnalysis(warm_start=True, random_state=0)
A:sklearn.neighbors.tests.test_nca.nca_cold->NeighborhoodComponentsAnalysis(warm_start=False, random_state=0)
A:sklearn.neighbors.tests.test_nca.diff_warm->numpy.sum(np.abs(transformation_warm_plus_one - transformation_warm))
A:sklearn.neighbors.tests.test_nca.diff_cold->numpy.sum(np.abs(transformation_cold_plus_one - transformation_cold))
A:sklearn.neighbors.tests.test_nca.(out, _)->capsys.readouterr()
A:sklearn.neighbors.tests.test_nca.lines->re.split('\n+', out)
A:sklearn.neighbors.tests.test_nca.header->'{:>10} {:>20} {:>10}'.format('Iteration', 'Objective Value', 'Time(s)')
A:sklearn.neighbors.tests.test_nca.(ind_singleton,)->numpy.where(y == singleton_class)
A:sklearn.neighbors.tests.test_nca.(ind_1,)->numpy.where(y == 1)
A:sklearn.neighbors.tests.test_nca.(ind_2,)->numpy.where(y == 2)
A:sklearn.neighbors.tests.test_nca.(ind_0,)->numpy.where(y == 0)
A:sklearn.neighbors.tests.test_nca.transformation_storer->TransformationStorer(X, y)
A:sklearn.neighbors.tests.test_nca.msg->'[{}] NCA did not converge'.format(cls_name)
A:sklearn.neighbors.tests.test_nca.est->NeighborhoodComponentsAnalysis().fit(X, y)
A:sklearn.neighbors.tests.test_nca.names_out->NeighborhoodComponentsAnalysis().fit(X, y).get_feature_names_out()
A:sklearn.neighbors.tests.test_nca.class_name_lower->NeighborhoodComponentsAnalysis().fit(X, y).__class__.__name__.lower()
A:sklearn.neighbors.tests.test_nca.expected_names_out->numpy.array([f'{class_name_lower}{i}' for i in range(est.components_.shape[1])], dtype=object)
sklearn.neighbors.tests.test_nca.test_auto_init(n_samples,n_features,n_classes,n_components)
sklearn.neighbors.tests.test_nca.test_callback(capsys)
sklearn.neighbors.tests.test_nca.test_convergence_warning()
sklearn.neighbors.tests.test_nca.test_expected_transformation_shape()
sklearn.neighbors.tests.test_nca.test_finite_differences(global_random_seed)
sklearn.neighbors.tests.test_nca.test_init_transformation()
sklearn.neighbors.tests.test_nca.test_n_components()
sklearn.neighbors.tests.test_nca.test_nca_feature_names_out()
sklearn.neighbors.tests.test_nca.test_no_verbose(capsys)
sklearn.neighbors.tests.test_nca.test_one_class()
sklearn.neighbors.tests.test_nca.test_parameters_valid_types(param,value)
sklearn.neighbors.tests.test_nca.test_params_validation()
sklearn.neighbors.tests.test_nca.test_simple_example()
sklearn.neighbors.tests.test_nca.test_singleton_class()
sklearn.neighbors.tests.test_nca.test_toy_example_collapse_points()
sklearn.neighbors.tests.test_nca.test_transformation_dimensions()
sklearn.neighbors.tests.test_nca.test_verbose(init_name,capsys)
sklearn.neighbors.tests.test_nca.test_warm_start_effectiveness()
sklearn.neighbors.tests.test_nca.test_warm_start_validation()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_graph.py----------------------------------------
A:sklearn.neighbors.tests.test_graph.rng->numpy.random.RandomState(42)
A:sklearn.neighbors.tests.test_graph.X->numpy.random.RandomState(42).randn(n_samples_fit, n_features)
A:sklearn.neighbors.tests.test_graph.X2->numpy.random.RandomState(42).randn(n_samples_transform, n_features)
A:sklearn.neighbors.tests.test_graph.radius->numpy.percentile(euclidean_distances(X), 10)
A:sklearn.neighbors.tests.test_graph.nnt->KNeighborsTransformer(n_neighbors=n_neighbors)
A:sklearn.neighbors.tests.test_graph.Xt->KNeighborsTransformer(n_neighbors=n_neighbors).transform(X)
A:sklearn.neighbors.tests.test_graph.X2t->KNeighborsTransformer(n_neighbors=n_neighbors).transform(X2)
A:sklearn.neighbors.tests.test_graph.est->Klass().fit(X)
A:sklearn.neighbors.tests.test_graph.names_out->Klass().fit(X).get_feature_names_out()
A:sklearn.neighbors.tests.test_graph.class_name_lower->Klass.__name__.lower()
A:sklearn.neighbors.tests.test_graph.expected_names_out->numpy.array([f'{class_name_lower}{i}' for i in range(est.n_samples_fit_)], dtype=object)
sklearn.neighbors.tests.test_graph._has_explicit_diagonal(X)
sklearn.neighbors.tests.test_graph.test_explicit_diagonal()
sklearn.neighbors.tests.test_graph.test_graph_feature_names_out(Klass)
sklearn.neighbors.tests.test_graph.test_transformer_result()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_quad_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_quad_tree.tree->_QuadTree(n_dimensions=n_dimensions, verbose=0)
A:sklearn.neighbors.tests.test_quad_tree.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_quad_tree.X->numpy.array([[-10.0, -10.0], [9.0, 10.0], [10.0, 9.0], [10.0, 10.0]], dtype=np.float32)
A:sklearn.neighbors.tests.test_quad_tree.s->pickle.dumps(tree, protocol=protocol)
A:sklearn.neighbors.tests.test_quad_tree.bt2->pickle.loads(s)
A:sklearn.neighbors.tests.test_quad_tree.cell_x_tree->_QuadTree(n_dimensions=n_dimensions, verbose=0).get_cell(x)
A:sklearn.neighbors.tests.test_quad_tree.cell_x_bt2->pickle.loads(s).get_cell(x)
A:sklearn.neighbors.tests.test_quad_tree.cell_id->_QuadTree(n_dimensions=n_dimensions, verbose=0).get_cell(x)
A:sklearn.neighbors.tests.test_quad_tree.qt->_QuadTree(n_dimensions, verbose=0)
A:sklearn.neighbors.tests.test_quad_tree.(idx, summary)->_QuadTree(n_dimensions, verbose=0)._py_summarize(query_pt, X, 0.0)
A:sklearn.neighbors.tests.test_quad_tree.barycenter->X[1:].mean(axis=0)
A:sklearn.neighbors.tests.test_quad_tree.ds2c->((X[0] - X[i + 1]) ** 2).sum()
sklearn.neighbors.tests.test_quad_tree.test_qt_insert_duplicate(n_dimensions)
sklearn.neighbors.tests.test_quad_tree.test_quad_tree_pickle(n_dimensions,protocol)
sklearn.neighbors.tests.test_quad_tree.test_quadtree_boundary_computation()
sklearn.neighbors.tests.test_quad_tree.test_quadtree_similar_point()
sklearn.neighbors.tests.test_quad_tree.test_summarize()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_neighbors_pipeline.py----------------------------------------
A:sklearn.neighbors.tests.test_neighbors_pipeline.(X, _)->make_blobs(random_state=0)
A:sklearn.neighbors.tests.test_neighbors_pipeline.est_chain->make_pipeline(KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance'), LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors, novelty=True, contamination='auto'))
A:sklearn.neighbors.tests.test_neighbors_pipeline.est_compact->LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto')
A:sklearn.neighbors.tests.test_neighbors_pipeline.labels_compact->LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto').fit_predict(X)
A:sklearn.neighbors.tests.test_neighbors_pipeline.labels_chain->make_pipeline(KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance'), LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors, novelty=True, contamination='auto')).fit_predict(X)
A:sklearn.neighbors.tests.test_neighbors_pipeline.centers->numpy.array([[0.0, 5.0, 0.0, 0.0, 0.0], [0.0, 0.0, 4.0, 0.0, 0.0], [1.0, 0.0, 0.0, 5.0, 1.0]])
A:sklearn.neighbors.tests.test_neighbors_pipeline.(S, true_labels)->make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.0, random_state=42)
A:sklearn.neighbors.tests.test_neighbors_pipeline.St_compact->LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto').fit_transform(S)
A:sklearn.neighbors.tests.test_neighbors_pipeline.St_chain->make_pipeline(KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance'), LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors, novelty=True, contamination='auto')).fit_transform(S)
A:sklearn.neighbors.tests.test_neighbors_pipeline.X->numpy.random.RandomState(0).randn(40, 2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.(X2, _)->make_blobs(random_state=1)
A:sklearn.neighbors.tests.test_neighbors_pipeline.Xt_chain->make_pipeline(KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance'), LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors, novelty=True, contamination='auto')).fit_transform(X)
A:sklearn.neighbors.tests.test_neighbors_pipeline.Xt_compact->LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto').fit_transform(X)
A:sklearn.neighbors.tests.test_neighbors_pipeline.n_neighbors->int(3.0 * perplexity + 1)
A:sklearn.neighbors.tests.test_neighbors_pipeline.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_neighbors_pipeline.pred_chain->make_pipeline(KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance'), LocalOutlierFactor(metric='precomputed', n_neighbors=n_neighbors, novelty=True, contamination='auto')).fit(X1).predict(X2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.pred_compact->LocalOutlierFactor(n_neighbors=n_neighbors, novelty=True, contamination='auto').fit(X1).predict(X2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.X1->numpy.random.RandomState(0).randn(40, 2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.X2->numpy.random.RandomState(0).randn(40, 2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.y->numpy.random.RandomState(0).rand(40, 1)
A:sklearn.neighbors.tests.test_neighbors_pipeline.k_trans->KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance')
A:sklearn.neighbors.tests.test_neighbors_pipeline.k_trans_factor->KNeighborsTransformer(n_neighbors=int(n_neighbors * factor), mode='distance')
A:sklearn.neighbors.tests.test_neighbors_pipeline.r_trans->RadiusNeighborsTransformer(radius=radius, mode='distance')
A:sklearn.neighbors.tests.test_neighbors_pipeline.r_trans_factor->RadiusNeighborsTransformer(radius=int(radius * factor), mode='distance')
A:sklearn.neighbors.tests.test_neighbors_pipeline.k_reg->KNeighborsRegressor(n_neighbors=n_neighbors)
A:sklearn.neighbors.tests.test_neighbors_pipeline.r_reg->RadiusNeighborsRegressor(radius=radius)
A:sklearn.neighbors.tests.test_neighbors_pipeline.reg_compact->clone(reg)
A:sklearn.neighbors.tests.test_neighbors_pipeline.reg_precomp->clone(reg)
A:sklearn.neighbors.tests.test_neighbors_pipeline.reg_chain->make_pipeline(clone(trans), reg_precomp)
A:sklearn.neighbors.tests.test_neighbors_pipeline.y_pred_chain->make_pipeline(clone(trans), reg_precomp).fit(X, y).predict(X2)
A:sklearn.neighbors.tests.test_neighbors_pipeline.y_pred_compact->clone(reg).fit(X, y).predict(X2)
sklearn.neighbors.tests.test_neighbors_pipeline.test_dbscan()
sklearn.neighbors.tests.test_neighbors_pipeline.test_isomap()
sklearn.neighbors.tests.test_neighbors_pipeline.test_kneighbors_regressor()
sklearn.neighbors.tests.test_neighbors_pipeline.test_lof_novelty_false()
sklearn.neighbors.tests.test_neighbors_pipeline.test_lof_novelty_true()
sklearn.neighbors.tests.test_neighbors_pipeline.test_spectral_clustering()
sklearn.neighbors.tests.test_neighbors_pipeline.test_spectral_embedding()
sklearn.neighbors.tests.test_neighbors_pipeline.test_tsne()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_neighbors_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_neighbors_tree.rng->check_random_state(0)
A:sklearn.neighbors.tests.test_neighbors_tree.V_mahalanobis->numpy.dot(V_mahalanobis, V_mahalanobis.T)
A:sklearn.neighbors.tests.test_neighbors_tree.BALL_TREE_METRICS->list(METRICS)
A:sklearn.neighbors.tests.test_neighbors_tree.d->numpy.sqrt(((query_pt - X[ind]) ** 2).sum(1))
A:sklearn.neighbors.tests.test_neighbors_tree.norm->kernel_norm(h, X.shape[1], kernel)
A:sklearn.neighbors.tests.test_neighbors_tree.D->sklearn.metrics.DistanceMetric.get_metric('euclidean').pairwise(Y, X)
A:sklearn.neighbors.tests.test_neighbors_tree.X->check_random_state(0).random_sample((10, 3))
A:sklearn.neighbors.tests.test_neighbors_tree.Y->check_random_state(0).random_sample((10, DIMENSION))
A:sklearn.neighbors.tests.test_neighbors_tree.dens_true->compute_kernel_slow(Y, X, kernel, h)
A:sklearn.neighbors.tests.test_neighbors_tree.tree->Cls(x_in[:, None])
A:sklearn.neighbors.tests.test_neighbors_tree.dens->Cls(x_in[:, None]).kernel_density(Y, h, atol=atol, rtol=rtol, kernel=kernel, breadth_first=breadth_first)
A:sklearn.neighbors.tests.test_neighbors_tree.query_pt->numpy.zeros(n_features, dtype=float)
A:sklearn.neighbors.tests.test_neighbors_tree.rad->numpy.sqrt(((X - query_pt) ** 2).sum(1))
A:sklearn.neighbors.tests.test_neighbors_tree.(ind, dist)->Cls(x_in[:, None]).query_radius([query_pt], r + eps, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors_tree.r->numpy.linspace(0, 1, 10)
A:sklearn.neighbors.tests.test_neighbors_tree.counts->Cls(x_in[:, None]).two_point_correlation(Y, r=r, dualtree=dualtree)
A:sklearn.neighbors.tests.test_neighbors_tree.heap->NeighborsHeap(n_pts, n_nbrs)
A:sklearn.neighbors.tests.test_neighbors_tree.d_in->check_random_state(0).random_sample(2 * n_nbrs).astype(np.float64, copy=False)
A:sklearn.neighbors.tests.test_neighbors_tree.i_in->numpy.arange(2 * n_nbrs, dtype=np.intp)
A:sklearn.neighbors.tests.test_neighbors_tree.ind->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(np.intp, copy=False)
A:sklearn.neighbors.tests.test_neighbors_tree.(d_heap, i_heap)->NeighborsHeap(n_pts, n_nbrs).get_arrays(sort=True)
A:sklearn.neighbors.tests.test_neighbors_tree.vals->check_random_state(0).random_sample(n_nodes).astype(np.float64, copy=False)
A:sklearn.neighbors.tests.test_neighbors_tree.i1->numpy.argsort(vals)
A:sklearn.neighbors.tests.test_neighbors_tree.(vals2, i2)->nodeheap_sort(vals)
A:sklearn.neighbors.tests.test_neighbors_tree.dist->check_random_state(0).random_sample((n_rows, n_pts)).astype(np.float64, copy=False)
A:sklearn.neighbors.tests.test_neighbors_tree.dist2->check_random_state(0).random_sample((n_rows, n_pts)).astype(np.float64, copy=False).copy()
A:sklearn.neighbors.tests.test_neighbors_tree.ind2->(np.arange(n_pts) + np.zeros((n_rows, 1))).astype(np.intp, copy=False).copy()
A:sklearn.neighbors.tests.test_neighbors_tree.i->numpy.argsort(dist2, axis=1)
A:sklearn.neighbors.tests.test_neighbors_tree.x_in->check_random_state(0).normal(0, 1, n_samples)
A:sklearn.neighbors.tests.test_neighbors_tree.x_out->numpy.linspace(-5, 5, 30)
A:sklearn.neighbors.tests.test_neighbors_tree.gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in))
A:sklearn.neighbors.tests.test_neighbors_tree.dens_gkde->gaussian_kde(x_in, bw_method=h / np.std(x_in)).evaluate(x_out)
A:sklearn.neighbors.tests.test_neighbors_tree.kdt->Cls(X, leaf_size=1, metric=metric, **kwargs)
A:sklearn.neighbors.tests.test_neighbors_tree.(dist1, ind1)->Cls(X, leaf_size=1, metric=metric, **kwargs).query(Y, k, dualtree=dualtree, breadth_first=breadth_first)
A:sklearn.neighbors.tests.test_neighbors_tree.(dist2, ind2)->brute_force_neighbors(X, Y, k, metric, **kwargs)
A:sklearn.neighbors.tests.test_neighbors_tree.tree1->Cls(X, leaf_size=1, metric=metric, **kwargs)
A:sklearn.neighbors.tests.test_neighbors_tree.(ind1, dist1)->Cls(X, leaf_size=1, metric=metric, **kwargs).query(X)
A:sklearn.neighbors.tests.test_neighbors_tree.s->pickle.dumps(tree1, protocol=protocol)
A:sklearn.neighbors.tests.test_neighbors_tree.tree2->pickle.loads(s)
A:sklearn.neighbors.tests.test_neighbors_tree.(ind2, dist2)->pickle.loads(s).query(X)
sklearn.neighbors.tests.test_neighbors_tree.brute_force_neighbors(X,Y,k,metric,**kwargs)
sklearn.neighbors.tests.test_neighbors_tree.compute_kernel_slow(Y,X,kernel,h)
sklearn.neighbors.tests.test_neighbors_tree.dist_func(x1,x2,p)
sklearn.neighbors.tests.test_neighbors_tree.test_gaussian_kde(Cls,n_samples=1000)
sklearn.neighbors.tests.test_neighbors_tree.test_kernel_density(Cls,kernel,h,rtol,atol,breadth_first,n_samples=100,n_features=3)
sklearn.neighbors.tests.test_neighbors_tree.test_neighbor_tree_query_radius(Cls,n_samples=100,n_features=10)
sklearn.neighbors.tests.test_neighbors_tree.test_neighbor_tree_query_radius_distance(Cls,n_samples=100,n_features=10)
sklearn.neighbors.tests.test_neighbors_tree.test_neighbor_tree_two_point(Cls,dualtree,n_samples=100,n_features=3)
sklearn.neighbors.tests.test_neighbors_tree.test_neighbors_heap(NeighborsHeap,n_pts=5,n_nbrs=10)
sklearn.neighbors.tests.test_neighbors_tree.test_nn_tree_query(Cls,metric,k,dualtree,breadth_first)
sklearn.neighbors.tests.test_neighbors_tree.test_node_heap(nodeheap_sort,n_nodes=50)
sklearn.neighbors.tests.test_neighbors_tree.test_pickle(Cls,metric,protocol)
sklearn.neighbors.tests.test_neighbors_tree.test_simultaneous_sort(simultaneous_sort,n_rows=10,n_pts=201)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_kde.py----------------------------------------
A:sklearn.neighbors.tests.test_kde.d->numpy.sqrt(((Y[:, None, :] - X) ** 2).sum(-1))
A:sklearn.neighbors.tests.test_kde.kde->KernelDensity(bandwidth=bandwidth).fit(X)
A:sklearn.neighbors.tests.test_kde.log_dens->KernelDensity(bandwidth=bandwidth).fit(X).fit(X).score_samples(Y)
A:sklearn.neighbors.tests.test_kde.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_kde.X->numpy.random.RandomState(0).randn(n_samples, n_features)
A:sklearn.neighbors.tests.test_kde.Y->numpy.random.RandomState(0).randn(10, 2)
A:sklearn.neighbors.tests.test_kde.dens_true->compute_kernel_slow(Y, X, kernel, bandwidth)
A:sklearn.neighbors.tests.test_kde.samp->KernelDensity(bandwidth=bandwidth).fit(X).sample(100)
A:sklearn.neighbors.tests.test_kde.nbrs->NearestNeighbors(n_neighbors=1).fit(X)
A:sklearn.neighbors.tests.test_kde.(dist, ind)->NearestNeighbors(n_neighbors=1).fit(X).kneighbors(X, return_distance=True)
A:sklearn.neighbors.tests.test_kde.y_dens->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(Y)
A:sklearn.neighbors.tests.test_kde.(X, _)->make_blobs(cluster_std=0.1, random_state=1, centers=[[0, 1], [1, 0], [0, 0]])
A:sklearn.neighbors.tests.test_kde.pipe1->make_pipeline(StandardScaler(with_mean=False, with_std=False), KernelDensity(kernel='gaussian'))
A:sklearn.neighbors.tests.test_kde.params->dict(kerneldensity__bandwidth=[0.001, 0.01, 0.1, 1, 10])
A:sklearn.neighbors.tests.test_kde.search->GridSearchCV(pipe1, param_grid=params)
A:sklearn.neighbors.tests.test_kde.weights_neutral->numpy.full(n_samples, 3.0)
A:sklearn.neighbors.tests.test_kde.X_repetitions->numpy.repeat(X, weights, axis=0)
A:sklearn.neighbors.tests.test_kde.test_points->numpy.random.RandomState(0).rand(n_samples_test, d)
A:sklearn.neighbors.tests.test_kde.scores_const_weight->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(test_points)
A:sklearn.neighbors.tests.test_kde.sample_const_weight->KernelDensity(bandwidth=bandwidth).fit(X).sample(random_state=1234)
A:sklearn.neighbors.tests.test_kde.scores_no_weight->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(test_points)
A:sklearn.neighbors.tests.test_kde.sample_no_weight->KernelDensity(bandwidth=bandwidth).fit(X).sample(random_state=1234)
A:sklearn.neighbors.tests.test_kde.scores_weight->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(test_points)
A:sklearn.neighbors.tests.test_kde.sample_weight->KernelDensity(bandwidth=bandwidth).fit(X).sample(random_state=1234)
A:sklearn.neighbors.tests.test_kde.scores_ref_sampling->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(test_points)
A:sklearn.neighbors.tests.test_kde.sample_ref_sampling->KernelDensity(bandwidth=bandwidth).fit(X).sample(random_state=1234)
A:sklearn.neighbors.tests.test_kde.diff->numpy.max(np.abs(scores_no_weight - scores_weight))
A:sklearn.neighbors.tests.test_kde.scale_factor->numpy.random.RandomState(0).rand()
A:sklearn.neighbors.tests.test_kde.scores_scaled_weight->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(test_points)
A:sklearn.neighbors.tests.test_kde.data->numpy.reshape([1.0, 2.0, 3.0], (-1, 1))
A:sklearn.neighbors.tests.test_kde.scores->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(X)
A:sklearn.neighbors.tests.test_kde.file_path->str(tmpdir.join('dump.pkl'))
A:sklearn.neighbors.tests.test_kde.scores_pickled->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(X)
A:sklearn.neighbors.tests.test_kde.kde_sc->KernelDensity(bandwidth=bandwidth).fit(X).score_samples(X)
sklearn.neighbors.tests.test_kde.check_results(kernel,bandwidth,atol,rtol,X,Y,dens_true)
sklearn.neighbors.tests.test_kde.compute_kernel_slow(Y,X,kernel,h)
sklearn.neighbors.tests.test_kde.test_bandwidth(bandwidth)
sklearn.neighbors.tests.test_kde.test_check_is_fitted(method)
sklearn.neighbors.tests.test_kde.test_kde_algorithm_metric_choice(algorithm,metric)
sklearn.neighbors.tests.test_kde.test_kde_pipeline_gridsearch()
sklearn.neighbors.tests.test_kde.test_kde_sample_weights()
sklearn.neighbors.tests.test_kde.test_kde_sample_weights_error()
sklearn.neighbors.tests.test_kde.test_kde_score(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kde.test_kernel_density(kernel,bandwidth)
sklearn.neighbors.tests.test_kde.test_kernel_density_sampling(n_samples=100,n_features=3)
sklearn.neighbors.tests.test_kde.test_pickling(tmpdir,sample_weight)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_ball_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_ball_tree.rng->numpy.random.RandomState(random_seed)
A:sklearn.neighbors.tests.test_ball_tree.V_mahalanobis->numpy.dot(V_mahalanobis, V_mahalanobis.T)
A:sklearn.neighbors.tests.test_ball_tree.D->sklearn.metrics.DistanceMetric.get_metric(metric, **kwargs).pairwise(Y, X)
A:sklearn.neighbors.tests.test_ball_tree.X->numpy.ones((5, 2))
A:sklearn.neighbors.tests.test_ball_tree.Y->_convert_container(Y, array_type)
A:sklearn.neighbors.tests.test_ball_tree.bt->BallTreeImplementation(X, leaf_size=1, metric='haversine')
A:sklearn.neighbors.tests.test_ball_tree.(dist1, ind1)->BallTreeImplementation(X, leaf_size=1, metric='haversine').query(X, k=5)
A:sklearn.neighbors.tests.test_ball_tree.(dist2, ind2)->brute_force_neighbors(X, X, k=5, metric='haversine')
A:sklearn.neighbors.tests.test_ball_tree.(X_64, X_32, Y_64, Y_32)->get_dataset_for_binary_tree(random_seed=global_random_seed)
A:sklearn.neighbors.tests.test_ball_tree.metric_params->METRICS.get(metric, {})
A:sklearn.neighbors.tests.test_ball_tree.bt_64->BallTree64(X_64, leaf_size=10)
A:sklearn.neighbors.tests.test_ball_tree.bt_32->BallTree32(X_32, leaf_size=10)
A:sklearn.neighbors.tests.test_ball_tree.(dist_64, ind_64)->BallTree64(X_64, leaf_size=10).query(Y_64, k=k)
A:sklearn.neighbors.tests.test_ball_tree.(dist_32, ind_32)->BallTree32(X_32, leaf_size=10).query(Y_32, k=k)
A:sklearn.neighbors.tests.test_ball_tree.ind_64->BallTree64(X_64, leaf_size=10).query_radius(Y_64, r=r)
A:sklearn.neighbors.tests.test_ball_tree.ind_32->BallTree32(X_32, leaf_size=10).query_radius(Y_32, r=r)
A:sklearn.neighbors.tests.test_ball_tree.(ind_64, dist_64)->BallTree64(X_64, leaf_size=10).query_radius(Y_64, r=r, return_distance=True)
A:sklearn.neighbors.tests.test_ball_tree.(ind_32, dist_32)->BallTree32(X_32, leaf_size=10).query_radius(Y_32, r=r, return_distance=True)
A:sklearn.neighbors.tests.test_ball_tree.density64->BallTree64(X_64, leaf_size=10).kernel_density(Y_64, h=h, kernel=kernel, breadth_first=True)
A:sklearn.neighbors.tests.test_ball_tree.density32->BallTree32(X_32, leaf_size=10).kernel_density(Y_32, h=h, kernel=kernel, breadth_first=True)
A:sklearn.neighbors.tests.test_ball_tree.r->numpy.linspace(0, 1, 10)
A:sklearn.neighbors.tests.test_ball_tree.counts_64->BallTree64(X_64, leaf_size=10).two_point_correlation(Y_64, r=r, dualtree=True)
A:sklearn.neighbors.tests.test_ball_tree.counts_32->BallTree32(X_32, leaf_size=10).two_point_correlation(Y_32, r=r, dualtree=True)
A:sklearn.neighbors.tests.test_ball_tree._X->numpy.random.RandomState(random_seed).rand(100, features)
A:sklearn.neighbors.tests.test_ball_tree._Y->numpy.random.RandomState(random_seed).rand(5, features)
A:sklearn.neighbors.tests.test_ball_tree.X_64->numpy.random.RandomState(random_seed).rand(100, features).astype(dtype=np.float64, copy=False)
A:sklearn.neighbors.tests.test_ball_tree.Y_64->numpy.random.RandomState(random_seed).rand(5, features).astype(dtype=np.float64, copy=False)
A:sklearn.neighbors.tests.test_ball_tree.X_32->numpy.random.RandomState(random_seed).rand(100, features).astype(dtype=np.float32, copy=False)
A:sklearn.neighbors.tests.test_ball_tree.Y_32->numpy.random.RandomState(random_seed).rand(5, features).astype(dtype=np.float32, copy=False)
sklearn.neighbors.tests.test_ball_tree.brute_force_neighbors(X,Y,k,metric,**kwargs)
sklearn.neighbors.tests.test_ball_tree.get_dataset_for_binary_tree(random_seed,features=3)
sklearn.neighbors.tests.test_ball_tree.test_BallTree_is_BallTree64_subclass()
sklearn.neighbors.tests.test_ball_tree.test_array_object_type(BallTreeImplementation)
sklearn.neighbors.tests.test_ball_tree.test_bad_pyfunc_metric(BallTreeImplementation)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_numerical_consistency(global_random_seed,metric)
sklearn.neighbors.tests.test_ball_tree.test_ball_tree_query_metrics(metric,array_type,BallTreeImplementation)
sklearn.neighbors.tests.test_ball_tree.test_kernel_density_numerical_consistency(global_random_seed,metric)
sklearn.neighbors.tests.test_ball_tree.test_query_haversine(BallTreeImplementation,decimal_tol)
sklearn.neighbors.tests.test_ball_tree.test_two_point_correlation_numerical_consistency(global_random_seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_kd_tree.py----------------------------------------
A:sklearn.neighbors.tests.test_kd_tree.X->numpy.random.RandomState(0).random_sample((10, 3))
A:sklearn.neighbors.tests.test_kd_tree.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_kd_tree.tree->BinarySearchTree(X, leaf_size=2)
A:sklearn.neighbors.tests.test_kd_tree.(X_64, X_32, Y_64, Y_32)->get_dataset_for_binary_tree(random_seed=global_random_seed)
A:sklearn.neighbors.tests.test_kd_tree.metric_params->METRICS.get(metric, {})
A:sklearn.neighbors.tests.test_kd_tree.kd_64->KDTree64(X_64, leaf_size=2, metric=metric, **metric_params)
A:sklearn.neighbors.tests.test_kd_tree.kd_32->KDTree32(X_32, leaf_size=2, metric=metric, **metric_params)
A:sklearn.neighbors.tests.test_kd_tree.(dist_64, ind_64)->KDTree64(X_64, leaf_size=2, metric=metric, **metric_params).query(Y_64, k=k)
A:sklearn.neighbors.tests.test_kd_tree.(dist_32, ind_32)->KDTree32(X_32, leaf_size=2, metric=metric, **metric_params).query(Y_32, k=k)
A:sklearn.neighbors.tests.test_kd_tree.ind_64->KDTree64(X_64, leaf_size=2, metric=metric, **metric_params).query_radius(Y_64, r=r)
A:sklearn.neighbors.tests.test_kd_tree.ind_32->KDTree32(X_32, leaf_size=2, metric=metric, **metric_params).query_radius(Y_32, r=r)
A:sklearn.neighbors.tests.test_kd_tree.(ind_64, dist_64)->KDTree64(X_64, leaf_size=2, metric=metric, **metric_params).query_radius(Y_64, r=r, return_distance=True)
A:sklearn.neighbors.tests.test_kd_tree.(ind_32, dist_32)->KDTree32(X_32, leaf_size=2, metric=metric, **metric_params).query_radius(Y_32, r=r, return_distance=True)
A:sklearn.neighbors.tests.test_kd_tree.density64->KDTree64(X_64, leaf_size=2, metric=metric, **metric_params).kernel_density(Y_64, h=h, kernel=kernel, breadth_first=True)
A:sklearn.neighbors.tests.test_kd_tree.density32->KDTree32(X_32, leaf_size=2, metric=metric, **metric_params).kernel_density(Y_32, h=h, kernel=kernel, breadth_first=True)
sklearn.neighbors.tests.test_kd_tree.test_KDTree_is_KDTree64_subclass()
sklearn.neighbors.tests.test_kd_tree.test_array_object_type(BinarySearchTree)
sklearn.neighbors.tests.test_kd_tree.test_kd_tree_numerical_consistency(global_random_seed,metric)
sklearn.neighbors.tests.test_kd_tree.test_kdtree_picklable_with_joblib(BinarySearchTree)
sklearn.neighbors.tests.test_kd_tree.test_kernel_density_numerical_consistency(global_random_seed,metric)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_nearest_centroid.py----------------------------------------
A:sklearn.neighbors.tests.test_nearest_centroid.iris->sklearn.datasets.load_iris()
A:sklearn.neighbors.tests.test_nearest_centroid.rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_nearest_centroid.perm->numpy.random.RandomState(0).permutation(iris.target.size)
A:sklearn.neighbors.tests.test_nearest_centroid.X_csr->csr_container(X)
A:sklearn.neighbors.tests.test_nearest_centroid.T_csr->csr_container(T)
A:sklearn.neighbors.tests.test_nearest_centroid.clf->NearestCentroid(shrink_threshold=0.1)
A:sklearn.neighbors.tests.test_nearest_centroid.score->NearestCentroid().score(iris.data, iris.target)
A:sklearn.neighbors.tests.test_nearest_centroid.obj->NearestCentroid()
A:sklearn.neighbors.tests.test_nearest_centroid.s->pickle.dumps(obj)
A:sklearn.neighbors.tests.test_nearest_centroid.obj2->pickle.loads(s)
A:sklearn.neighbors.tests.test_nearest_centroid.score2->pickle.loads(s).score(iris.data, iris.target)
A:sklearn.neighbors.tests.test_nearest_centroid.X->numpy.empty((10, 2))
A:sklearn.neighbors.tests.test_nearest_centroid.y->numpy.zeros(10)
A:sklearn.neighbors.tests.test_nearest_centroid.expected_result->numpy.array([[0.778731, 0.8545292], [2.814179, 2.763647]])
A:sklearn.neighbors.tests.test_nearest_centroid.y_ind->numpy.asarray(y)
A:sklearn.neighbors.tests.test_nearest_centroid.noise->numpy.random.RandomState(0).rand(50)
A:sklearn.neighbors.tests.test_nearest_centroid.y_init->NearestCentroid(shrink_threshold=0.1).predict(X)
A:sklearn.neighbors.tests.test_nearest_centroid.y_translate->NearestCentroid(shrink_threshold=0.1).predict(X_noise)
sklearn.neighbors.tests.test_nearest_centroid.test_classification_toy(csr_container)
sklearn.neighbors.tests.test_nearest_centroid.test_deprecated_distance_metric_supports(metric)
sklearn.neighbors.tests.test_nearest_centroid.test_features_zero_var()
sklearn.neighbors.tests.test_nearest_centroid.test_iris()
sklearn.neighbors.tests.test_nearest_centroid.test_iris_shrinkage()
sklearn.neighbors.tests.test_nearest_centroid.test_manhattan_metric(csr_container)
sklearn.neighbors.tests.test_nearest_centroid.test_pickle()
sklearn.neighbors.tests.test_nearest_centroid.test_predict_translated_data()
sklearn.neighbors.tests.test_nearest_centroid.test_shrinkage_correct()
sklearn.neighbors.tests.test_nearest_centroid.test_shrinkage_threshold_decoded_y()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/neighbors/tests/test_neighbors.py----------------------------------------
A:sklearn.neighbors.tests.test_neighbors.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.neighbors.tests.test_neighbors.iris->sklearn.datasets.load_iris()
A:sklearn.neighbors.tests.test_neighbors.perm->numpy.random.RandomState(global_random_seed).permutation(digits.target.size)
A:sklearn.neighbors.tests.test_neighbors.digits->sklearn.datasets.load_digits()
A:sklearn.neighbors.tests.test_neighbors.SPARSE_TYPES->tuple(BSR_CONTAINERS + COO_CONTAINERS + CSC_CONTAINERS + CSR_CONTAINERS + DOK_CONTAINERS + LIL_CONTAINERS)
A:sklearn.neighbors.tests.test_neighbors.COMMON_VALID_METRICS->sorted(set.intersection(*map(set, neighbors.VALID_METRICS.values())))
A:sklearn.neighbors.tests.test_neighbors.neighbors.kneighbors_graph->ignore_warnings(neighbors.kneighbors_graph)
A:sklearn.neighbors.tests.test_neighbors.neighbors.radius_neighbors_graph->ignore_warnings(neighbors.radius_neighbors_graph)
A:sklearn.neighbors.tests.test_neighbors.A->sklearn.neighbors.radius_neighbors_graph(X, 1.5, mode='distance')
A:sklearn.neighbors.tests.test_neighbors.metric->_parse_metric(metric, global_dtype)
A:sklearn.neighbors.tests.test_neighbors.local_rng->numpy.random.RandomState(0)
A:sklearn.neighbors.tests.test_neighbors.X->numpy.array([[1.0, 0.0], [0.0, 0.0], [0.0, 1.0]])
A:sklearn.neighbors.tests.test_neighbors.neigh->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5})
A:sklearn.neighbors.tests.test_neighbors.y->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).kneighbors(X[0].reshape(1, -1), return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.query->numpy.random.RandomState(0).rand(n_query_pts, n_features).astype(global_dtype, copy=False)
A:sklearn.neighbors.tests.test_neighbors.nbrs_fid->sklearn.neighbors.NearestNeighbors(n_neighbors=1)
A:sklearn.neighbors.tests.test_neighbors.(dist1, ind1)->sklearn.neighbors.NearestNeighbors(metric=metric, radius=radius).fit(X).kneighbors(X)
A:sklearn.neighbors.tests.test_neighbors.nbrs->sklearn.neighbors.NearestNeighbors().fit(X)
A:sklearn.neighbors.tests.test_neighbors.(dist2, ind2)->sklearn.neighbors.KNeighborsRegressor(1, algorithm='ball_tree').kneighbors(X)
A:sklearn.neighbors.tests.test_neighbors.neighbors_->sklearn.neighbors.NearestNeighbors()
A:sklearn.neighbors.tests.test_neighbors.Y->csr_container([[1, 1, 0, 1, 1], [1, 0, 0, 1, 1]])
A:sklearn.neighbors.tests.test_neighbors.(DXX, DYX)->make_train_test(X, Y)
A:sklearn.neighbors.tests.test_neighbors.nbrs_X->sklearn.neighbors.NearestNeighbors(n_neighbors=3)
A:sklearn.neighbors.tests.test_neighbors.(dist_X, ind_X)->getattr(nbrs_X, method)(None)
A:sklearn.neighbors.tests.test_neighbors.nbrs_D->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='auto', metric='precomputed')
A:sklearn.neighbors.tests.test_neighbors.(dist_D, ind_D)->getattr(nbrs_D, method)(None)
A:sklearn.neighbors.tests.test_neighbors.target->numpy.arange(X.shape[0])
A:sklearn.neighbors.tests.test_neighbors.est->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, weights=_weights)
A:sklearn.neighbors.tests.test_neighbors.pred_X->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, weights=_weights).fit(X, target).predict(Y)
A:sklearn.neighbors.tests.test_neighbors.pred_D->sklearn.neighbors.KNeighborsClassifier(n_neighbors=3, weights=_weights).fit(DXX, target).predict(DYX)
A:sklearn.neighbors.tests.test_neighbors.nn->sklearn.neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2, metric=sparse_metric).fit(X)
A:sklearn.neighbors.tests.test_neighbors.Xt->function(X)
A:sklearn.neighbors.tests.test_neighbors.mask->numpy.random.RandomState(42).randint(2, size=(10, 10))
A:sklearn.neighbors.tests.test_neighbors.X_->csr_container(np.abs(np.random.RandomState(42).randn(10, 10)))
A:sklearn.neighbors.tests.test_neighbors.dist->numpy.array([[5.0, 2.0, 1.0], [-2.0, 0.0, 3.0], [1.0, 3.0, 0.0]])
A:sklearn.neighbors.tests.test_neighbors.dist_csr->csr_container(dist)
A:sklearn.neighbors.tests.test_neighbors.D->pairwise_distances(X, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.metric_score->cross_val_score(Est(), X, y)
A:sklearn.neighbors.tests.test_neighbors.precomp_score->cross_val_score(Est(metric='precomputed'), D, y)
A:sklearn.neighbors.tests.test_neighbors.test->numpy.arange(train_test_boundary, n_samples)
A:sklearn.neighbors.tests.test_neighbors.ind1->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).radius_neighbors(test, return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.(dist, ind)->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).radius_neighbors(X_test)
A:sklearn.neighbors.tests.test_neighbors.j->d.argsort()
A:sklearn.neighbors.tests.test_neighbors.y_str->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).kneighbors(X[0].reshape(1, -1), return_distance=False).astype(str)
A:sklearn.neighbors.tests.test_neighbors.knn->sklearn.neighbors.KNeighborsClassifier(n_neighbors=2).fit(X, y)
A:sklearn.neighbors.tests.test_neighbors.y_pred->sklearn.neighbors.RadiusNeighborsRegressor(weights=weights, algorithm=algorithm).predict(X[:n_test_pts] + epsilon)
A:sklearn.neighbors.tests.test_neighbors.cls->sklearn.neighbors.KNeighborsClassifier(n_neighbors=2, p=1, weights='distance')
A:sklearn.neighbors.tests.test_neighbors.y_prob->sklearn.neighbors.KNeighborsClassifier(n_neighbors=2, p=1, weights='distance').predict_proba(np.array([[0, 2, 0], [2, 2, 2]]))
A:sklearn.neighbors.tests.test_neighbors.real_prob->numpy.array([[0, 1, 0], [0, 0.4, 0.6]])
A:sklearn.neighbors.tests.test_neighbors.z1->numpy.array([[1.01, 1.01], [2.0, 2.0]])
A:sklearn.neighbors.tests.test_neighbors.z2->numpy.array([[1.4, 1.4], [1.01, 1.01], [2.01, 2.01]], dtype=global_dtype)
A:sklearn.neighbors.tests.test_neighbors.clf->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label)
A:sklearn.neighbors.tests.test_neighbors.correct_labels1->numpy.array([1, 2])
A:sklearn.neighbors.tests.test_neighbors.correct_labels2->numpy.array([-1, 1, 2])
A:sklearn.neighbors.tests.test_neighbors.outlier_proba->numpy.array([0, 0])
A:sklearn.neighbors.tests.test_neighbors.proba->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).predict_proba(X_te)
A:sklearn.neighbors.tests.test_neighbors.pred->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).predict(X_te)
A:sklearn.neighbors.tests.test_neighbors.z->numpy.array([[1.1, 1.1], [2.0, 2.0]])
A:sklearn.neighbors.tests.test_neighbors.rnn_correct_labels->numpy.array([1.25, 2.0])
A:sklearn.neighbors.tests.test_neighbors.knn_correct_unif->numpy.array([1.25, 1.0])
A:sklearn.neighbors.tests.test_neighbors.knn_correct_dist->numpy.array([1.25, 2.0])
A:sklearn.neighbors.tests.test_neighbors.rnn->sklearn.neighbors.RadiusNeighborsRegressor(weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.results->sklearn.neighbors.NearestNeighbors().fit(X).radius_neighbors([[0.0]], return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.(neigh_dist, neigh_ind)->sklearn.neighbors.NearestNeighbors().fit(X).radius_neighbors(X, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors.expected_dist->numpy.empty(X.shape[0], dtype=object)
A:sklearn.neighbors.tests.test_neighbors.expected_ind->numpy.empty(X.shape[0], dtype=object)
A:sklearn.neighbors.tests.test_neighbors.query_point->numpy.array([[0, 0]])
A:sklearn.neighbors.tests.test_neighbors.equidistant_points->numpy.array([[1, 0], [0, 1], [-1, 0], [0, -1]])
A:sklearn.neighbors.tests.test_neighbors.knn_indices->numpy.array([[0, 1]])
A:sklearn.neighbors.tests.test_neighbors.indices->numpy.sort(nn.kneighbors(query_point, n_neighbors=k, return_distance=False))
A:sklearn.neighbors.tests.test_neighbors.model->sklearn.neighbors.NearestNeighbors(n_neighbors=4, algorithm='auto', metric=metric, metric_params=metric_params)
A:sklearn.neighbors.tests.test_neighbors.(distances, indices)->sklearn.neighbors.NearestNeighbors(n_neighbors=4, algorithm='auto', metric=metric, metric_params=metric_params).radius_neighbors(X=X, radius=np.inf, sort_results=True)
A:sklearn.neighbors.tests.test_neighbors.graph->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).radius_neighbors_graph(X_test, mode='distance').toarray()
A:sklearn.neighbors.tests.test_neighbors.(X_train, X_test, y_train, y_test)->train_test_split(X, y)
A:sklearn.neighbors.tests.test_neighbors.rnn_mo->sklearn.neighbors.RadiusNeighborsClassifier(weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.y_pred_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm).predict(X_test)
A:sklearn.neighbors.tests.test_neighbors.X_eps->sparsev(X[:n_test_pts] + epsilon)
A:sklearn.neighbors.tests.test_neighbors.knn_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.y_pred_proba_mo->sklearn.neighbors.KNeighborsClassifier(weights=weights, algorithm=algorithm).predict_proba(X_test)
A:sklearn.neighbors.tests.test_neighbors.neigh_idx->sklearn.neighbors.RadiusNeighborsRegressor(weights=weights, algorithm=algorithm).radius_neighbors(X_test, return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.y_pred_idx->numpy.array(y_pred_idx)
A:sklearn.neighbors.tests.test_neighbors.X_test_nan->numpy.full((1, n_features), -1.0)
A:sklearn.neighbors.tests.test_neighbors.knn_pre->sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, metric='precomputed')
A:sklearn.neighbors.tests.test_neighbors.X2->sparsev(X)
A:sklearn.neighbors.tests.test_neighbors.X2_pre->sparsev(pairwise_distances(X, metric='euclidean'))
A:sklearn.neighbors.tests.test_neighbors.rgs->sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, algorithm=algorithm)
A:sklearn.neighbors.tests.test_neighbors.train_test_boundary->int(n_samples * 0.8)
A:sklearn.neighbors.tests.test_neighbors.train->numpy.arange(0, train_test_boundary)
A:sklearn.neighbors.tests.test_neighbors.score_uint8->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).fit(X_train, Y_train).score(X_test, Y_test)
A:sklearn.neighbors.tests.test_neighbors.score_float->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).fit(X_train.astype(float, copy=False), Y_train).score(X_test.astype(float, copy=False), Y_test)
A:sklearn.neighbors.tests.test_neighbors.Xcsr->csr_container(X)
A:sklearn.neighbors.tests.test_neighbors.Xsparse->csr_container(X)
A:sklearn.neighbors.tests.test_neighbors.X3->numpy.random.RandomState(global_random_seed).random_sample((10, 3))
A:sklearn.neighbors.tests.test_neighbors.msg->re.escape('Found array with 0 feature(s)')
A:sklearn.neighbors.tests.test_neighbors.X_train->numpy.ascontiguousarray(X_train[:, feature_sl])
A:sklearn.neighbors.tests.test_neighbors.X_test->numpy.ascontiguousarray(X_test[:, feature_sl])
A:sklearn.neighbors.tests.test_neighbors.metric_params_list->_generate_test_params_for(metric, n_features)
A:sklearn.neighbors.tests.test_neighbors.p->metric_params.pop('p', 2)
A:sklearn.neighbors.tests.test_neighbors.feature_sl->slice(None, 2)
A:sklearn.neighbors.tests.test_neighbors.results[algorithm]->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).kneighbors(X_test, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors.(legacy_brute_dst, legacy_brute_idx)->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).radius_neighbors(X_test, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors.(pdr_brute_dst, pdr_brute_idx)->sklearn.neighbors.NearestNeighbors(n_neighbors=3, algorithm='brute', metric_params={'p': 0.5}).radius_neighbors(X_test, return_distance=True)
A:sklearn.neighbors.tests.test_neighbors.nbrs1->sklearn.neighbors.NearestNeighbors(metric=metric, radius=radius).fit(X)
A:sklearn.neighbors.tests.test_neighbors.nbrs2->sklearn.neighbors.KNeighborsRegressor(1, algorithm='ball_tree')
A:sklearn.neighbors.tests.test_neighbors.X_precomputed->numpy.random.RandomState(global_random_seed).random_sample((10, 4))
A:sklearn.neighbors.tests.test_neighbors.Y_precomputed->numpy.random.RandomState(global_random_seed).random_sample((3, 4))
A:sklearn.neighbors.tests.test_neighbors.DXX->sklearn.metrics.pairwise_distances(X_precomputed, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.DYX->sklearn.metrics.pairwise_distances(Y_precomputed, X_precomputed, metric='euclidean')
A:sklearn.neighbors.tests.test_neighbors.nb_p->sklearn.neighbors.NearestNeighbors(n_neighbors=3, metric='precomputed')
A:sklearn.neighbors.tests.test_neighbors.dist_array->pairwise_distances(X).flatten()
A:sklearn.neighbors.tests.test_neighbors.nbrs_graph->sklearn.neighbors.radius_neighbors_graph(X, radius, metric=metric, mode='connectivity', include_self=True).toarray()
A:sklearn.neighbors.tests.test_neighbors.X_nbrs->sklearn.neighbors.NearestNeighbors(radius=radius, metric='manhattan')
A:sklearn.neighbors.tests.test_neighbors.kng->sklearn.neighbors.kneighbors_graph(X, 1, include_self=True).toarray()
A:sklearn.neighbors.tests.test_neighbors.kng_not_self->sklearn.neighbors.kneighbors_graph(X, 1, include_self=False).toarray()
A:sklearn.neighbors.tests.test_neighbors.rng_not_self->sklearn.neighbors.radius_neighbors_graph(X, 5.0, include_self=False).toarray()
A:sklearn.neighbors.tests.test_neighbors.(X, y)->sklearn.datasets.make_classification(n_samples=50, n_features=5, n_informative=3, n_redundant=0, n_classes=3, random_state=seed)
A:sklearn.neighbors.tests.test_neighbors.y_parallel->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).predict(X_test)
A:sklearn.neighbors.tests.test_neighbors.(dist_parallel, ind_parallel)->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).radius_neighbors(X_test)
A:sklearn.neighbors.tests.test_neighbors.graph_parallel->sklearn.neighbors.RadiusNeighborsClassifier(radius=2, outlier_label=outlier_label).radius_neighbors_graph(X_test, mode='distance').toarray()
A:sklearn.neighbors.tests.test_neighbors.classifier->sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)
A:sklearn.neighbors.tests.test_neighbors.result->sklearn.neighbors.KNeighborsClassifier(n_neighbors=1).fit(X, y).predict(X)
A:sklearn.neighbors.tests.test_neighbors.N->sklearn.neighbors.NearestNeighbors(algorithm='brute', n_neighbors=2, metric=sparse_metric).fit(X).kneighbors(Y, return_distance=False)
A:sklearn.neighbors.tests.test_neighbors.gold_standard_nn->numpy.array([[2, 1], [2, 1]])
A:sklearn.neighbors.tests.test_neighbors.nn1->NN(metric='jaccard', algorithm='brute').fit(X)
A:sklearn.neighbors.tests.test_neighbors.nn2->NN(metric='jaccard', algorithm='ball_tree').fit(X)
A:sklearn.neighbors.tests.test_neighbors.(X_tr, X_te, y_tr, y_te)->train_test_split(X, y, random_state=0)
A:sklearn.neighbors.tests.test_neighbors.outlier_label->int(2 - seed)
A:sklearn.neighbors.tests.test_neighbors.proba_label->numpy.where(proba.sum(axis=1) == 0, outlier_label, proba_label)
A:sklearn.neighbors.tests.test_neighbors.k_trans->sklearn.neighbors.KNeighborsTransformer(n_neighbors=n_neighbors, mode='distance')
A:sklearn.neighbors.tests.test_neighbors.k_trans_factor->sklearn.neighbors.KNeighborsTransformer(n_neighbors=int(n_neighbors * factor), mode='distance')
A:sklearn.neighbors.tests.test_neighbors.r_trans->sklearn.neighbors.RadiusNeighborsTransformer(radius=radius, mode='distance')
A:sklearn.neighbors.tests.test_neighbors.r_trans_factor->sklearn.neighbors.RadiusNeighborsTransformer(radius=int(radius * factor), mode='distance')
A:sklearn.neighbors.tests.test_neighbors.k_reg->sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors)
A:sklearn.neighbors.tests.test_neighbors.r_reg->sklearn.neighbors.RadiusNeighborsRegressor(radius=radius)
A:sklearn.neighbors.tests.test_neighbors.reg_compact->clone(reg)
A:sklearn.neighbors.tests.test_neighbors.reg_precomp->clone(reg)
A:sklearn.neighbors.tests.test_neighbors.reg_chain->make_pipeline(clone(trans), reg_precomp)
A:sklearn.neighbors.tests.test_neighbors.y_pred_chain->make_pipeline(clone(trans), reg_precomp).fit(X, y).predict(X2)
A:sklearn.neighbors.tests.test_neighbors.y_pred_compact->clone(reg).fit(X, y).predict(X2)
A:sklearn.neighbors.tests.test_neighbors.pd->pytest.importorskip('pandas')
sklearn.neighbors.tests.test_neighbors._generate_test_params_for(metric:str,n_features:int)
sklearn.neighbors.tests.test_neighbors._parse_metric(metric:str,dtype=None)
sklearn.neighbors.tests.test_neighbors._weight_func(dist)
sklearn.neighbors.tests.test_neighbors.check_object_arrays(nparray,list_check)
sklearn.neighbors.tests.test_neighbors.check_precomputed(make_train_test,estimators)
sklearn.neighbors.tests.test_neighbors.test_KNeighborsClassifier_multioutput()
sklearn.neighbors.tests.test_neighbors.test_KNeighborsClassifier_raise_on_all_zero_weights()
sklearn.neighbors.tests.test_neighbors.test_KNeighborsRegressor_multioutput_uniform_weight()
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsClassifier_multioutput()
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsRegressor_multioutput(n_samples=40,n_features=5,n_test_pts=10,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_RadiusNeighborsRegressor_multioutput_with_uniform_weight()
sklearn.neighbors.tests.test_neighbors.test_auto_algorithm(X,metric,metric_params,expected_algo)
sklearn.neighbors.tests.test_neighbors.test_callable_metric()
sklearn.neighbors.tests.test_neighbors.test_dtype_convert()
sklearn.neighbors.tests.test_neighbors.test_include_self_neighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_is_sorted_by_data(csr_container)
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_X_None(algorithm)
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_duplicates(algorithm)
sklearn.neighbors.tests.test_neighbors.test_k_and_radius_neighbors_train_is_not_query()
sklearn.neighbors.tests.test_neighbors.test_kneighbors_brute_backend(metric,global_dtype,global_random_seed,n_samples=2000,n_features=30,n_query_pts=5,n_neighbors=5)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier(global_dtype,algorithm,weights,n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_float_labels(global_dtype,n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_predict_proba(global_dtype)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_classifier_sparse(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_kneighbors_graph_sparse(n_neighbors,mode,csr_container,seed=36)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=3,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor_multioutput(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=3,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_kneighbors_regressor_sparse(n_samples=40,n_features=5,n_test_pts=10,n_neighbors=5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_knn_forcing_backend(backend,algorithm)
sklearn.neighbors.tests.test_neighbors.test_metric_params_interface()
sklearn.neighbors.tests.test_neighbors.test_nearest_neighbors_validate_params()
sklearn.neighbors.tests.test_neighbors.test_nearest_neighbours_works_with_p_less_than_1()
sklearn.neighbors.tests.test_neighbors.test_neigh_predictions_algorithm_agnosticity(global_dtype,n_samples,n_features,n_query_pts,metric,n_neighbors,radius,NeighborsMixinSubclass)
sklearn.neighbors.tests.test_neighbors.test_neighbors_digits()
sklearn.neighbors.tests.test_neighbors.test_neighbors_iris()
sklearn.neighbors.tests.test_neighbors.test_neighbors_metrics(global_dtype,metric,n_samples=20,n_features=3,n_query_pts=2,n_neighbors=5)
sklearn.neighbors.tests.test_neighbors.test_neighbors_minkowski_semimetric_algo_error(Estimator,n_features,algorithm)
sklearn.neighbors.tests.test_neighbors.test_neighbors_minkowski_semimetric_algo_warn(Estimator,n_features,algorithm)
sklearn.neighbors.tests.test_neighbors.test_neighbors_regressors_zero_distance()
sklearn.neighbors.tests.test_neighbors.test_neighbors_validate_parameters(Estimator,csr_container)
sklearn.neighbors.tests.test_neighbors.test_non_euclidean_kneighbors()
sklearn.neighbors.tests.test_neighbors.test_not_fitted_error_gets_raised()
sklearn.neighbors.tests.test_neighbors.test_pairwise_boolean_distance()
sklearn.neighbors.tests.test_neighbors.test_pipeline_with_nearest_neighbors_transformer()
sklearn.neighbors.tests.test_neighbors.test_precomputed_cross_validation()
sklearn.neighbors.tests.test_neighbors.test_precomputed_dense()
sklearn.neighbors.tests.test_neighbors.test_precomputed_sparse_invalid(csr_container)
sklearn.neighbors.tests.test_neighbors.test_precomputed_sparse_knn(fmt)
sklearn.neighbors.tests.test_neighbors.test_precomputed_sparse_radius(fmt)
sklearn.neighbors.tests.test_neighbors.test_predict_dataframe()
sklearn.neighbors.tests.test_neighbors.test_predict_sparse_ball_kd_tree(csr_container)
sklearn.neighbors.tests.test_neighbors.test_query_equidistant_kth_nn(algorithm)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_boundary_handling()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_brute_backend(metric,global_random_seed,global_dtype,n_samples=2000,n_features=30,n_query_pts=5,radius=1.0)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier(global_dtype,algorithm,weights,n_samples=40,n_features=5,n_test_pts=10,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_outlier_labeling(global_dtype,algorithm,weights)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_when_no_neighbors(global_dtype,algorithm,weights,outlier_label)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_classifier_zero_distance()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_graph()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_graph_sparse(n_neighbors,mode,csr_container,seed=36)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_predict_proba()
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_regressor(n_samples=40,n_features=3,n_test_pts=10,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_returns_array_of_objects(csr_container)
sklearn.neighbors.tests.test_neighbors.test_radius_neighbors_sort_results(algorithm,metric)
sklearn.neighbors.tests.test_neighbors.test_regressor_predict_on_arraylikes()
sklearn.neighbors.tests.test_neighbors.test_same_knn_parallel(algorithm)
sklearn.neighbors.tests.test_neighbors.test_same_radius_neighbors_parallel(algorithm)
sklearn.neighbors.tests.test_neighbors.test_sort_graph_by_row_values(function,csr_container)
sklearn.neighbors.tests.test_neighbors.test_sort_graph_by_row_values_bad_sparse_format(sparse_container)
sklearn.neighbors.tests.test_neighbors.test_sort_graph_by_row_values_copy(csr_container)
sklearn.neighbors.tests.test_neighbors.test_sort_graph_by_row_values_warning(csr_container)
sklearn.neighbors.tests.test_neighbors.test_sparse_metric_callable(csr_container)
sklearn.neighbors.tests.test_neighbors.test_unsupervised_inputs(global_dtype,KNeighborsMixinSubclass)
sklearn.neighbors.tests.test_neighbors.test_unsupervised_kneighbors(global_dtype,n_samples,n_features,n_query_pts,n_neighbors,query_is_train,metric)
sklearn.neighbors.tests.test_neighbors.test_unsupervised_radius_neighbors(global_dtype,n_samples=20,n_features=5,n_query_pts=2,radius=0.5,random_state=0)
sklearn.neighbors.tests.test_neighbors.test_valid_brute_metric_for_auto_algorithm(global_dtype,metric,csr_container,n_samples=20,n_features=12)
sklearn.neighbors.tests.test_neighbors.test_valid_metrics_has_no_duplicate()


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/_bayesian_mixture.py----------------------------------------
A:sklearn.mixture._bayesian_mixture.self.mean_prior_->check_array(self.mean_prior, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture._bayesian_mixture.self.covariance_prior_->check_array(self.covariance_prior, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture._bayesian_mixture.(nk, xk, sk)->_estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)
A:sklearn.mixture._bayesian_mixture.self.precisions_cholesky_->_compute_precision_cholesky(self.covariances_, self.covariance_type)
A:sklearn.mixture._bayesian_mixture.self.covariances_->numpy.empty((self.n_components, n_features, n_features))
A:sklearn.mixture._bayesian_mixture.digamma_sum->digamma(self.weight_concentration_[0] + self.weight_concentration_[1])
A:sklearn.mixture._bayesian_mixture.digamma_a->digamma(self.weight_concentration_[0])
A:sklearn.mixture._bayesian_mixture.digamma_b->digamma(self.weight_concentration_[1])
A:sklearn.mixture._bayesian_mixture.log_wishart->numpy.sum(_log_wishart_norm(self.degrees_of_freedom_, log_det_precisions_chol, n_features))
A:sklearn.mixture._bayesian_mixture.log_norm_weight->_log_dirichlet_norm(self.weight_concentration_)
A:sklearn.mixture._bayesian_mixture.self.precisions_->numpy.dot(self.precisions_cholesky_, self.precisions_cholesky_.T)
sklearn.mixture.BayesianGaussianMixture(self,*,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.BayesianGaussianMixture._check_means_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_precision_parameters(self,X)
sklearn.mixture.BayesianGaussianMixture._check_weights_parameters(self)
sklearn.mixture.BayesianGaussianMixture._checkcovariance_prior_parameter(self,X)
sklearn.mixture.BayesianGaussianMixture._compute_lower_bound(self,log_resp,log_prob_norm)
sklearn.mixture.BayesianGaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.BayesianGaussianMixture._estimate_log_weights(self)
sklearn.mixture.BayesianGaussianMixture._estimate_means(self,nk,xk)
sklearn.mixture.BayesianGaussianMixture._estimate_precisions(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_weights(self,nk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_diag(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_full(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_spherical(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._estimate_wishart_tied(self,nk,xk,sk)
sklearn.mixture.BayesianGaussianMixture._get_parameters(self)
sklearn.mixture.BayesianGaussianMixture._initialize(self,X,resp)
sklearn.mixture.BayesianGaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.BayesianGaussianMixture._set_parameters(self,params)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture(self,*,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture.__init__(self,*,n_components=1,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weight_concentration_prior_type='dirichlet_process',weight_concentration_prior=None,mean_precision_prior=None,mean_prior=None,degrees_of_freedom_prior=None,covariance_prior=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._check_means_parameters(self,X)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._check_parameters(self,X)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._check_precision_parameters(self,X)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._check_weights_parameters(self)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._checkcovariance_prior_parameter(self,X)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._compute_lower_bound(self,log_resp,log_prob_norm)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_log_prob(self,X)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_log_weights(self)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_means(self,nk,xk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_precisions(self,nk,xk,sk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_weights(self,nk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_wishart_diag(self,nk,xk,sk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_wishart_full(self,nk,xk,sk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_wishart_spherical(self,nk,xk,sk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._estimate_wishart_tied(self,nk,xk,sk)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._get_parameters(self)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._initialize(self,X,resp)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._m_step(self,X,log_resp)
sklearn.mixture._bayesian_mixture.BayesianGaussianMixture._set_parameters(self,params)
sklearn.mixture._bayesian_mixture._log_dirichlet_norm(dirichlet_concentration)
sklearn.mixture._bayesian_mixture._log_wishart_norm(degrees_of_freedom,log_det_precisions_chol,n_features)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/_gaussian_mixture.py----------------------------------------
A:sklearn.mixture._gaussian_mixture.weights->check_array(weights, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture._gaussian_mixture.means->check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)
A:sklearn.mixture._gaussian_mixture.precisions->check_array(precisions, dtype=[np.float64, np.float32], ensure_2d=False, allow_nd=covariance_type == 'full')
A:sklearn.mixture._gaussian_mixture.covariances->{'full': _estimate_gaussian_covariances_full, 'tied': _estimate_gaussian_covariances_tied, 'diag': _estimate_gaussian_covariances_diag, 'spherical': _estimate_gaussian_covariances_spherical}[covariance_type](resp, X, nk, means, reg_covar)
A:sklearn.mixture._gaussian_mixture.avg_X2->numpy.dot(X.T, X)
A:sklearn.mixture._gaussian_mixture.avg_means2->numpy.dot(nk * means.T, means)
A:sklearn.mixture._gaussian_mixture.precisions_chol->numpy.empty((n_components, n_features, n_features))
A:sklearn.mixture._gaussian_mixture.cov_chol->scipy.linalg.cholesky(covariances, lower=True)
A:sklearn.mixture._gaussian_mixture.precisions_cholesky->numpy.sqrt(precisions)
A:sklearn.mixture._gaussian_mixture.log_det_chol->numpy.sum(np.log(matrix_chol), axis=1)
A:sklearn.mixture._gaussian_mixture.log_det->_compute_log_det_cholesky(precisions_chol, covariance_type, n_features)
A:sklearn.mixture._gaussian_mixture.log_prob->numpy.empty((n_samples, n_components))
A:sklearn.mixture._gaussian_mixture.log_prob[:, k]->numpy.sum(np.square(y), axis=1)
A:sklearn.mixture._gaussian_mixture.self.weights_init->_check_weights(self.weights_init, self.n_components)
A:sklearn.mixture._gaussian_mixture.self.means_init->_check_means(self.means_init, self.n_components, n_features)
A:sklearn.mixture._gaussian_mixture.self.precisions_init->_check_precisions(self.precisions_init, self.covariance_type, self.n_components, n_features)
A:sklearn.mixture._gaussian_mixture.(weights, means, covariances)->_estimate_gaussian_parameters(X, resp, self.reg_covar, self.covariance_type)
A:sklearn.mixture._gaussian_mixture.self.precisions_cholesky_->_compute_precision_cholesky(self.covariances_, self.covariance_type)
A:sklearn.mixture._gaussian_mixture.(self.weights_, self.means_, self.covariances_)->_estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar, self.covariance_type)
A:sklearn.mixture._gaussian_mixture.self.precisions_->numpy.dot(self.precisions_cholesky_, self.precisions_cholesky_.T)
A:sklearn.mixture._gaussian_mixture.self.precisions_[k]->numpy.dot(prec_chol, prec_chol.T)
sklearn.mixture.GaussianMixture(self,n_components=1,*,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture.GaussianMixture._check_parameters(self,X)
sklearn.mixture.GaussianMixture._compute_lower_bound(self,_,log_prob_norm)
sklearn.mixture.GaussianMixture._estimate_log_prob(self,X)
sklearn.mixture.GaussianMixture._estimate_log_weights(self)
sklearn.mixture.GaussianMixture._get_parameters(self)
sklearn.mixture.GaussianMixture._initialize(self,X,resp)
sklearn.mixture.GaussianMixture._initialize_parameters(self,X,random_state)
sklearn.mixture.GaussianMixture._m_step(self,X,log_resp)
sklearn.mixture.GaussianMixture._n_parameters(self)
sklearn.mixture.GaussianMixture._set_parameters(self,params)
sklearn.mixture.GaussianMixture.aic(self,X)
sklearn.mixture.GaussianMixture.bic(self,X)
sklearn.mixture._gaussian_mixture.GaussianMixture(self,n_components=1,*,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture._gaussian_mixture.GaussianMixture.__init__(self,n_components=1,*,covariance_type='full',tol=0.001,reg_covar=1e-06,max_iter=100,n_init=1,init_params='kmeans',weights_init=None,means_init=None,precisions_init=None,random_state=None,warm_start=False,verbose=0,verbose_interval=10)
sklearn.mixture._gaussian_mixture.GaussianMixture._check_parameters(self,X)
sklearn.mixture._gaussian_mixture.GaussianMixture._compute_lower_bound(self,_,log_prob_norm)
sklearn.mixture._gaussian_mixture.GaussianMixture._estimate_log_prob(self,X)
sklearn.mixture._gaussian_mixture.GaussianMixture._estimate_log_weights(self)
sklearn.mixture._gaussian_mixture.GaussianMixture._get_parameters(self)
sklearn.mixture._gaussian_mixture.GaussianMixture._initialize(self,X,resp)
sklearn.mixture._gaussian_mixture.GaussianMixture._initialize_parameters(self,X,random_state)
sklearn.mixture._gaussian_mixture.GaussianMixture._m_step(self,X,log_resp)
sklearn.mixture._gaussian_mixture.GaussianMixture._n_parameters(self)
sklearn.mixture._gaussian_mixture.GaussianMixture._set_parameters(self,params)
sklearn.mixture._gaussian_mixture.GaussianMixture.aic(self,X)
sklearn.mixture._gaussian_mixture.GaussianMixture.bic(self,X)
sklearn.mixture._gaussian_mixture._check_means(means,n_components,n_features)
sklearn.mixture._gaussian_mixture._check_precision_matrix(precision,covariance_type)
sklearn.mixture._gaussian_mixture._check_precision_positivity(precision,covariance_type)
sklearn.mixture._gaussian_mixture._check_precisions(precisions,covariance_type,n_components,n_features)
sklearn.mixture._gaussian_mixture._check_precisions_full(precisions,covariance_type)
sklearn.mixture._gaussian_mixture._check_weights(weights,n_components)
sklearn.mixture._gaussian_mixture._compute_log_det_cholesky(matrix_chol,covariance_type,n_features)
sklearn.mixture._gaussian_mixture._compute_precision_cholesky(covariances,covariance_type)
sklearn.mixture._gaussian_mixture._compute_precision_cholesky_from_precisions(precisions,covariance_type)
sklearn.mixture._gaussian_mixture._estimate_gaussian_covariances_diag(resp,X,nk,means,reg_covar)
sklearn.mixture._gaussian_mixture._estimate_gaussian_covariances_full(resp,X,nk,means,reg_covar)
sklearn.mixture._gaussian_mixture._estimate_gaussian_covariances_spherical(resp,X,nk,means,reg_covar)
sklearn.mixture._gaussian_mixture._estimate_gaussian_covariances_tied(resp,X,nk,means,reg_covar)
sklearn.mixture._gaussian_mixture._estimate_gaussian_parameters(X,resp,reg_covar,covariance_type)
sklearn.mixture._gaussian_mixture._estimate_log_gaussian_prob(X,means,precisions_chol,covariance_type)
sklearn.mixture._gaussian_mixture._flipudlr(array)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/_base.py----------------------------------------
A:sklearn.mixture._base.param->numpy.array(param)
A:sklearn.mixture._base.resp->numpy.zeros((n_samples, self.n_components))
A:sklearn.mixture._base.indices->check_random_state(self.random_state).choice(n_samples, size=self.n_components, replace=False)
A:sklearn.mixture._base.(_, indices)->kmeans_plusplus(X, self.n_components, random_state=random_state)
A:sklearn.mixture._base.X->numpy.vstack([mean + rng.standard_normal(size=(sample, n_features)) * np.sqrt(covariance) for (mean, covariance, sample) in zip(self.means_, self.covariances_, n_samples_comp)])
A:sklearn.mixture._base.random_state->check_random_state(self.random_state)
A:sklearn.mixture._base.best_params->self._get_parameters()
A:sklearn.mixture._base.(log_prob_norm, log_resp)->self._estimate_log_prob_resp(X)
A:sklearn.mixture._base.lower_bound->self._compute_lower_bound(log_resp, log_prob_norm)
A:sklearn.mixture._base.(_, log_resp)->self._estimate_log_prob_resp(X)
A:sklearn.mixture._base.rng->check_random_state(self.random_state)
A:sklearn.mixture._base.n_samples_comp->check_random_state(self.random_state).multinomial(n_samples, self.weights_)
A:sklearn.mixture._base.y->numpy.concatenate([np.full(sample, j, dtype=int) for (j, sample) in enumerate(n_samples_comp)])
A:sklearn.mixture._base.weighted_log_prob->self._estimate_weighted_log_prob(X)
A:sklearn.mixture._base.log_prob_norm->logsumexp(weighted_log_prob, axis=1)
A:sklearn.mixture._base.self._init_prev_time->time()
A:sklearn.mixture._base.cur_time->time()
sklearn.mixture._base.BaseMixture(self,n_components,tol,reg_covar,max_iter,n_init,init_params,random_state,warm_start,verbose,verbose_interval)
sklearn.mixture._base.BaseMixture.__init__(self,n_components,tol,reg_covar,max_iter,n_init,init_params,random_state,warm_start,verbose,verbose_interval)
sklearn.mixture._base.BaseMixture._check_parameters(self,X)
sklearn.mixture._base.BaseMixture._e_step(self,X)
sklearn.mixture._base.BaseMixture._estimate_log_prob(self,X)
sklearn.mixture._base.BaseMixture._estimate_log_prob_resp(self,X)
sklearn.mixture._base.BaseMixture._estimate_log_weights(self)
sklearn.mixture._base.BaseMixture._estimate_weighted_log_prob(self,X)
sklearn.mixture._base.BaseMixture._get_parameters(self)
sklearn.mixture._base.BaseMixture._initialize(self,X,resp)
sklearn.mixture._base.BaseMixture._initialize_parameters(self,X,random_state)
sklearn.mixture._base.BaseMixture._m_step(self,X,log_resp)
sklearn.mixture._base.BaseMixture._print_verbose_msg_init_beg(self,n_init)
sklearn.mixture._base.BaseMixture._print_verbose_msg_init_end(self,ll)
sklearn.mixture._base.BaseMixture._print_verbose_msg_iter_end(self,n_iter,diff_ll)
sklearn.mixture._base.BaseMixture._set_parameters(self,params)
sklearn.mixture._base.BaseMixture.fit(self,X,y=None)
sklearn.mixture._base.BaseMixture.fit_predict(self,X,y=None)
sklearn.mixture._base.BaseMixture.predict(self,X)
sklearn.mixture._base.BaseMixture.predict_proba(self,X)
sklearn.mixture._base.BaseMixture.sample(self,n_samples=1)
sklearn.mixture._base.BaseMixture.score(self,X,y=None)
sklearn.mixture._base.BaseMixture.score_samples(self,X)
sklearn.mixture._base._check_shape(param,param_shape,name)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/tests/test_gaussian_mixture.py----------------------------------------
A:sklearn.mixture.tests.test_gaussian_mixture.rng->numpy.random.RandomState(global_random_seed)
A:sklearn.mixture.tests.test_gaussian_mixture.X->numpy.random.RandomState(global_random_seed).multivariate_normal(np.zeros(2), np.identity(2), size=3)
A:sklearn.mixture.tests.test_gaussian_mixture.self.weights->numpy.random.RandomState(global_random_seed).rand(n_components)
A:sklearn.mixture.tests.test_gaussian_mixture.self.X->dict(zip(COVARIANCE_TYPE, [generate_data(n_samples, n_features, self.weights, self.means, self.covariances, covar_type) for covar_type in COVARIANCE_TYPE]))
A:sklearn.mixture.tests.test_gaussian_mixture.self.Y->numpy.hstack([np.full(int(np.round(w * n_samples)), k, dtype=int) for (k, w) in enumerate(self.weights)])
A:sklearn.mixture.tests.test_gaussian_mixture.gmm->GaussianMixture(covariance_type=covariance_type, precisions_init=precisions_init)
A:sklearn.mixture.tests.test_gaussian_mixture.rand_data->RandomData(rng)
A:sklearn.mixture.tests.test_gaussian_mixture.g->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06)
A:sklearn.mixture.tests.test_gaussian_mixture.weights_bad_shape->numpy.random.RandomState(global_random_seed).rand(n_components, 1)
A:sklearn.mixture.tests.test_gaussian_mixture.msg->re.escape('Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.')
A:sklearn.mixture.tests.test_gaussian_mixture.weights_bad_norm->numpy.random.RandomState(global_random_seed).rand(n_components)
A:sklearn.mixture.tests.test_gaussian_mixture.means_bad_shape->numpy.random.RandomState(global_random_seed).rand(n_components + 1, n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_not_pos->numpy.ones((n_components, n_features, n_features))
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_not_pos[0]->numpy.eye(n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.resp->numpy.random.RandomState(seed).random_sample((n_samples, n_components))
A:sklearn.mixture.tests.test_gaussian_mixture.nk->numpy.array([n_samples])
A:sklearn.mixture.tests.test_gaussian_mixture.xk->numpy.random.RandomState(global_random_seed).multivariate_normal(np.zeros(2), np.identity(2), size=3).mean()
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred->_estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.ecov->EmpiricalCovariance()
A:sklearn.mixture.tests.test_gaussian_mixture.precs_chol_pred->_compute_precision_cholesky(covars_pred_spherical, 'spherical')
A:sklearn.mixture.tests.test_gaussian_mixture.precs_pred->numpy.dot(precs_chol_pred, precs_chol_pred.T)
A:sklearn.mixture.tests.test_gaussian_mixture.precs_est->scipy.linalg.inv(covars_pred_tied)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_full->_estimate_gaussian_covariances_full(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_tied->_estimate_gaussian_covariances_tied(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_diag->_estimate_gaussian_covariances_diag(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.ecov.covariance_->numpy.diag(np.diag(cov_full))
A:sklearn.mixture.tests.test_gaussian_mixture.cov_diag->numpy.diag(cov_diag)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_pred_spherical->_estimate_gaussian_covariances_spherical(resp, X, nk, xk, 0)
A:sklearn.mixture.tests.test_gaussian_mixture.predected_det->numpy.array([np.prod(cov) for cov in covariance])
A:sklearn.mixture.tests.test_gaussian_mixture.expected_det->_compute_log_det_cholesky(_compute_precision_cholesky(covariance, covar_type), covar_type, n_features=n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.stds->numpy.sqrt(covars)
A:sklearn.mixture.tests.test_gaussian_mixture.resp[:, i]->scipy.stats.norm.logpdf(X, mean, std).sum(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.covars_diag->numpy.random.RandomState(global_random_seed).rand(n_components, n_features)
A:sklearn.mixture.tests.test_gaussian_mixture.log_prob_naive->_naive_lmvnpdf_diag(X, means, [[k] * n_features for k in covars_spherical])
A:sklearn.mixture.tests.test_gaussian_mixture.precs_full->numpy.array([np.diag(1.0 / np.sqrt(x)) for x in covars_diag])
A:sklearn.mixture.tests.test_gaussian_mixture.log_prob->_estimate_log_gaussian_prob(X, means, precs_spherical, 'spherical')
A:sklearn.mixture.tests.test_gaussian_mixture.covars_tied->numpy.array([x for x in covars_diag]).mean(axis=0)
A:sklearn.mixture.tests.test_gaussian_mixture.precs_tied->numpy.diag(np.sqrt(1.0 / covars_tied))
A:sklearn.mixture.tests.test_gaussian_mixture.covars_spherical->numpy.random.RandomState(global_random_seed).rand(n_components, n_features).mean(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred_proba->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).predict_proba(X).argmax(axis=1)
A:sklearn.mixture.tests.test_gaussian_mixture.f->copy.deepcopy(g)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred1->copy.deepcopy(g).fit(X).predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.Y_pred2->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=False, tol=1e-06).fit_predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gm->GaussianMixture(n_components=rand_data.n_components, weights_init=rand_data.weights, means_init=rand_data.means, precisions_init=rand_data.precisions['full'], random_state=rng)
A:sklearn.mixture.tests.test_gaussian_mixture.y_pred1->GaussianMixture(n_components=rand_data.n_components, weights_init=rand_data.weights, means_init=rand_data.means, precisions_init=rand_data.precisions['full'], random_state=rng).fit_predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.y_pred2->GaussianMixture(n_components=rand_data.n_components, weights_init=rand_data.weights, means_init=rand_data.means, precisions_init=rand_data.precisions['full'], random_state=rng).predict(X)
A:sklearn.mixture.tests.test_gaussian_mixture.arg_idx1->numpy.trace(prec_pred, axis1=1, axis2=2).argsort()
A:sklearn.mixture.tests.test_gaussian_mixture.arg_idx2->numpy.trace(prec_test, axis1=1, axis2=2).argsort()
A:sklearn.mixture.tests.test_gaussian_mixture.prec_pred->numpy.array([np.diag(d) for d in g.precisions_])
A:sklearn.mixture.tests.test_gaussian_mixture.prec_test->numpy.array([np.diag(d) for d in rand_data.precisions['diag']])
A:sklearn.mixture.tests.test_gaussian_mixture.ll->numpy.array(ll)
A:sklearn.mixture.tests.test_gaussian_mixture.g_best->GaussianMixture(n_components=n_components, n_init=n_init, reg_covar=0, random_state=rng, covariance_type=covar_type)
A:sklearn.mixture.tests.test_gaussian_mixture.train1->GaussianMixture(n_components=n_components, covariance_type=cv_type, random_state=0).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.train2->GaussianMixture(n_components=n_components, covariance_type=cv_type, random_state=0, n_init=5).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.bic_full->GaussianMixture(n_components=n_components, covariance_type='full', random_state=rng).fit(X).bic(X)
A:sklearn.mixture.tests.test_gaussian_mixture.bic->GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=rng).fit(X).bic(X)
A:sklearn.mixture.tests.test_gaussian_mixture.h->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06)
A:sklearn.mixture.tests.test_gaussian_mixture.sys.stdout->StringIO()
A:sklearn.mixture.tests.test_gaussian_mixture.score1->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.score2->GaussianMixture(n_components=n_components, n_init=1, max_iter=5, reg_covar=0, random_state=random_state, warm_start=True, tol=1e-06).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm1->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score_proba->GaussianMixture(n_components=n_components, n_init=1, max_iter=1, random_state=random_state).fit(X).score_samples(X).mean()
A:sklearn.mixture.tests.test_gaussian_mixture.gmm2->GaussianMixture(n_components=n_components, n_init=10, max_iter=1, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gmm_score_samples->GaussianMixture(covariance_type=covariance_type, precisions_init=precisions_init).fit(X).score_samples(X)
A:sklearn.mixture.tests.test_gaussian_mixture.current_log_likelihood->GaussianMixture(covariance_type=covariance_type, precisions_init=precisions_init).fit(X).score(X)
A:sklearn.mixture.tests.test_gaussian_mixture.(X_s, y_s)->GaussianMixture(covariance_type=covariance_type, precisions_init=precisions_init).sample(n_samples)
A:sklearn.mixture.tests.test_gaussian_mixture.means_s->numpy.array([np.mean(X_s[y_s == k], 0) for k in range(n_components)])
A:sklearn.mixture.tests.test_gaussian_mixture.(X_s, _)->GaussianMixture(covariance_type=covariance_type, precisions_init=precisions_init).sample(sample_size)
A:sklearn.mixture.tests.test_gaussian_mixture.rnd->numpy.random.RandomState(0)
A:sklearn.mixture.tests.test_gaussian_mixture.means_init->numpy.array([[0.670637869618158, 0.21038256107384043, 0.12892629765485303], [0.09394051075844147, 0.5759464955561779, 0.929296197576212], [0.5033230372781258, 0.9569852381759425, 0.08654043447295741], [0.18578301420435747, 0.5531158970919143, 0.19388943970532435], [0.4548589928173794, 0.35182513658825276, 0.568146063202464], [0.609279894978321, 0.7929063819678847, 0.9620097270828052]])
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_init->numpy.array([999999.999604483, 999999.9990869573, 553.7603944542167, 204.78596008931834, 15.867423501783637, 85.4595728389735])
A:sklearn.mixture.tests.test_gaussian_mixture.C->numpy.array([[0.0, -0.7], [3.5, 0.7]])
A:sklearn.mixture.tests.test_gaussian_mixture.stretched_gaussian->numpy.dot(rng.randn(n_samples, 2), C)
A:sklearn.mixture.tests.test_gaussian_mixture.(_, _, covariance)->_estimate_gaussian_parameters(X, resp, reg_covar=reg_covar, covariance_type=covariance_type)
A:sklearn.mixture.tests.test_gaussian_mixture.gm_with_init->GaussianMixture(n_components=n_components, covariance_type=covariance_type, reg_covar=reg_covar, precisions_init=precisions_init, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.gm_without_init->GaussianMixture(n_components=n_components, covariance_type=covariance_type, reg_covar=reg_covar, random_state=random_state).fit(X)
A:sklearn.mixture.tests.test_gaussian_mixture.rs->numpy.random.RandomState(seed)
A:sklearn.mixture.tests.test_gaussian_mixture.(weights, means, covariances)->_estimate_gaussian_parameters(X, resp, reg_covar, covariance_type)
A:sklearn.mixture.tests.test_gaussian_mixture.precisions_cholesky->_compute_precision_cholesky(covariances, covariance_type)
A:sklearn.mixture.tests.test_gaussian_mixture.(X, resp)->_generate_data(seed=global_random_seed, n_samples=100, n_features=3, n_components=4)
A:sklearn.mixture.tests.test_gaussian_mixture.(precisions_init, desired_precisions_cholesky)->_calculate_precisions(X, resp, covariance_type)
A:sklearn.mixture.tests.test_gaussian_mixture.mock->Mock(side_effect=_estimate_gaussian_parameters)
sklearn.mixture.tests.test_gaussian_mixture.RandomData(self,rng,n_samples=200,n_components=2,n_features=2,scale=50)
sklearn.mixture.tests.test_gaussian_mixture.RandomData.__init__(self,rng,n_samples=200,n_components=2,n_features=2,scale=50)
sklearn.mixture.tests.test_gaussian_mixture._calculate_precisions(X,resp,covariance_type)
sklearn.mixture.tests.test_gaussian_mixture._generate_data(seed,n_samples,n_features,n_components)
sklearn.mixture.tests.test_gaussian_mixture._naive_lmvnpdf_diag(X,means,covars)
sklearn.mixture.tests.test_gaussian_mixture.generate_data(n_samples,n_features,weights,means,precisions,covariance_type)
sklearn.mixture.tests.test_gaussian_mixture.test_bic_1d_1component()
sklearn.mixture.tests.test_gaussian_mixture.test_check_means()
sklearn.mixture.tests.test_gaussian_mixture.test_check_precisions()
sklearn.mixture.tests.test_gaussian_mixture.test_check_weights()
sklearn.mixture.tests.test_gaussian_mixture.test_compute_log_det_cholesky()
sklearn.mixture.tests.test_gaussian_mixture.test_convergence_detected_with_warm_start()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_aic_bic()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_all_init_does_not_estimate_gaussian_parameters(monkeypatch,global_random_seed)
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_attributes()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_estimate_log_prob_resp()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_best_params()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_convergence_warning()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_predict(seed,max_iter,tol)
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_fit_predict_n_init()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_log_probabilities()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_n_parameters()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_precisions_init(covariance_type,global_random_seed)
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_precisions_init_diag()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_predict_predict_proba()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_setting_best_params()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_single_component_stable()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_mixture_verbose()
sklearn.mixture.tests.test_gaussian_mixture.test_gaussian_suffstat_sk_spherical()
sklearn.mixture.tests.test_gaussian_mixture.test_init()
sklearn.mixture.tests.test_gaussian_mixture.test_init_means_not_duplicated(init_params,global_random_seed)
sklearn.mixture.tests.test_gaussian_mixture.test_max_iter_zero()
sklearn.mixture.tests.test_gaussian_mixture.test_means_for_all_inits(init_params,global_random_seed)
sklearn.mixture.tests.test_gaussian_mixture.test_monotonic_likelihood()
sklearn.mixture.tests.test_gaussian_mixture.test_multiple_init()
sklearn.mixture.tests.test_gaussian_mixture.test_property()
sklearn.mixture.tests.test_gaussian_mixture.test_regularisation()
sklearn.mixture.tests.test_gaussian_mixture.test_sample()
sklearn.mixture.tests.test_gaussian_mixture.test_score()
sklearn.mixture.tests.test_gaussian_mixture.test_score_samples()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_diag()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_full()
sklearn.mixture.tests.test_gaussian_mixture.test_suffstat_sk_tied()
sklearn.mixture.tests.test_gaussian_mixture.test_warm_start(seed)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/tests/test_mixture.py----------------------------------------
A:sklearn.mixture.tests.test_mixture.rng->numpy.random.RandomState(0)
A:sklearn.mixture.tests.test_mixture.X->numpy.random.RandomState(0).rand(10, 5)
sklearn.mixture.tests.test_mixture.test_gaussian_mixture_n_iter(estimator)
sklearn.mixture.tests.test_mixture.test_mixture_n_components_greater_than_n_samples_error(estimator)


----------------------------------------/dataset/nuaa/anaconda3/envs/sklearn1.4.0/lib/python3.9/site-packages/sklearn/mixture/tests/test_bayesian_mixture.py----------------------------------------
A:sklearn.mixture.tests.test_bayesian_mixture.rng->numpy.random.RandomState(0)
A:sklearn.mixture.tests.test_bayesian_mixture.weight_concentration->numpy.random.RandomState(0).rand(2)
A:sklearn.mixture.tests.test_bayesian_mixture.predected_norm->_log_wishart_norm(degrees_of_freedom, log_det_precisions_chol, n_features)
A:sklearn.mixture.tests.test_bayesian_mixture.expected_norm->numpy.empty(5)
A:sklearn.mixture.tests.test_bayesian_mixture.X->numpy.random.RandomState(0).randn(50, 5)
A:sklearn.mixture.tests.test_bayesian_mixture.weight_concentration_prior->numpy.random.RandomState(0).rand()
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm->BayesianGaussianMixture(n_components=rand_data.n_components, random_state=rng, weight_concentration_prior_type=prior_type, covariance_type=covar_type)
A:sklearn.mixture.tests.test_bayesian_mixture.mean_precision_prior->numpy.random.RandomState(0).rand()
A:sklearn.mixture.tests.test_bayesian_mixture.mean_prior->numpy.random.RandomState(0).rand(n_features)
A:sklearn.mixture.tests.test_bayesian_mixture.dpgmm->BayesianGaussianMixture(weight_concentration_prior_type='dirichlet_process', n_components=3, random_state=rng).fit(X)
A:sklearn.mixture.tests.test_bayesian_mixture.rand_data->RandomData(rng)
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm1->BayesianGaussianMixture(n_components=n_components, max_iter=max_iter, random_state=rng, tol=tol, reg_covar=0)
A:sklearn.mixture.tests.test_bayesian_mixture.bgmm2->copy.deepcopy(bgmm1)
A:sklearn.mixture.tests.test_bayesian_mixture.Y_pred1->BayesianGaussianMixture(n_components=n_components, max_iter=max_iter, random_state=rng, tol=tol, reg_covar=0).fit(X).predict(X)
A:sklearn.mixture.tests.test_bayesian_mixture.Y_pred2->copy.deepcopy(bgmm1).fit_predict(X)
A:sklearn.mixture.tests.test_bayesian_mixture.gm->BayesianGaussianMixture(n_components=5, n_init=10, random_state=0)
A:sklearn.mixture.tests.test_bayesian_mixture.y_pred1->BayesianGaussianMixture(n_components=5, n_init=10, random_state=0).fit_predict(X)
A:sklearn.mixture.tests.test_bayesian_mixture.y_pred2->BayesianGaussianMixture(n_components=5, n_init=10, random_state=0).predict(X)
A:sklearn.mixture.tests.test_bayesian_mixture.Y_pred->BayesianGaussianMixture(n_components=rand_data.n_components, random_state=rng, weight_concentration_prior_type=prior_type, covariance_type=covar_type).predict(X)
A:sklearn.mixture.tests.test_bayesian_mixture.Y_pred_proba->BayesianGaussianMixture(n_components=rand_data.n_components, random_state=rng, weight_concentration_prior_type=prior_type, covariance_type=covar_type).predict_proba(X).argmax(axis=1)
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_check_is_fitted()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_fit_predict(seed,max_iter,tol)
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_fit_predict_n_init()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_mean_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_precisions_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_predict_predict_proba()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_weights()
sklearn.mixture.tests.test_bayesian_mixture.test_bayesian_mixture_weights_prior_initialisation()
sklearn.mixture.tests.test_bayesian_mixture.test_check_covariance_precision()
sklearn.mixture.tests.test_bayesian_mixture.test_compare_covar_type()
sklearn.mixture.tests.test_bayesian_mixture.test_invariant_translation()
sklearn.mixture.tests.test_bayesian_mixture.test_log_dirichlet_norm()
sklearn.mixture.tests.test_bayesian_mixture.test_log_wishart_norm()
sklearn.mixture.tests.test_bayesian_mixture.test_monotonic_likelihood()

