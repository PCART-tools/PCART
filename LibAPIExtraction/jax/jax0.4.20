
----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/__init__.py----------------------------------------
A:jax.__init__.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/ad_checkpoint.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/stages.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/api_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/custom_transpose.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/flatten_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/core.py----------------------------------------
A:jax.core.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/dtypes.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/custom_batching.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/version.py----------------------------------------
A:jax.version.datestring->datetime.date.fromtimestamp(timestamp).strftime('%Y%m%d')
A:jax.version.root_directory->os.path.dirname(os.path.realpath(__file__))
A:jax.version.p->subprocess.Popen(['git', 'describe', '--long', '--always'], cwd=root_directory, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
A:jax.version.(stdout, _)->subprocess.Popen(['git', 'describe', '--long', '--always'], cwd=root_directory, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()
A:jax.version.timestamp->int(stdout.decode().strip())
A:jax.version.release_version->_get_version_for_build()
A:jax.version.fhandle->pathlib.Path(fname)
A:jax.version.contents->contents.replace(old_version_string, new_version_string).replace(old_version_string, new_version_string)
A:jax.version.__version__->_get_version_string()
A:jax.version.__version_info__->_version_as_tuple(__version__)
A:jax.version._minimum_jaxlib_version_info->_version_as_tuple(_minimum_jaxlib_version)
jax.version._get_cmdclass(pkg_source_path)
jax.version._get_version_for_build()->str
jax.version._get_version_string()->str
jax.version._version_as_tuple(version_str)
jax.version._version_from_git_tree(base_version:str)->str | None
jax.version._version_from_todays_date(base_version:str)->str
jax.version._write_version(fname:str)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/prng.py----------------------------------------
A:jax.prng.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/config.py----------------------------------------
A:jax.config.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/tree_util.py----------------------------------------
A:jax.tree_util.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/debug.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/dlpack.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/profiler.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/cloud_tpu_init.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/sharding.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/distributed.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/linear_util.py----------------------------------------
A:jax.linear_util.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/errors.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/collect_profile.py----------------------------------------
A:jax.collect_profile.parser->argparse.ArgumentParser(description=_DESCRIPTION)
A:jax.collect_profile.options->tensorflow.python.profiler.profiler_v2.ProfilerOptions(host_tracer_level=host_tracer_level, device_tracer_level=device_tracer_level, python_tracer_level=python_tracer_level)
A:jax.collect_profile.log_dir_->pathlib.Path(log_dir if log_dir is not None else tempfile.mkdtemp())
A:jax.collect_profile.curr_path->pathlib.Path(log_dir if log_dir is not None else tempfile.mkdtemp()).resolve()
A:jax.collect_profile.latest_folder->max(trace_folders, key=os.path.getmtime)
A:jax.collect_profile.xplane->next(latest_folder.glob('*.xplane.pb'))
A:jax.collect_profile.(result, _)->tensorboard_plugin_profile.convert.raw_to_tool_data.xspace_to_tool_data([xplane], 'trace_viewer^', {})
A:jax.collect_profile.path->jax._src.profiler._write_perfetto_trace_file(str(log_dir_))
jax.collect_profile.collect_profile(port:int,duration_in_ms:int,host:str,log_dir:Optional[str],host_tracer_level:int,device_tracer_level:int,python_tracer_level:int,no_perfetto_link:bool)
jax.collect_profile.main(args)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/monitoring.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/test_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/random.py----------------------------------------
A:jax.random.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/custom_derivatives.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/typing.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/topologies.py----------------------------------------
A:jax.experimental.topologies.devices->jax.experimental.mesh_utils.create_device_mesh(mesh_shape, list(topo.devices), contiguous_submeshes=contiguous_submeshes)
jax.experimental.topologies.TopologyDescription(self,devices:list[Device])
jax.experimental.topologies.TopologyDescription.__init__(self,devices:list[Device])
jax.experimental.topologies.get_attached_topology(platform=None)->TopologyDescription
jax.experimental.topologies.get_topology_desc(topology_name:str='',platform:Optional[str]=None,**kwargs)->TopologyDescription
jax.experimental.topologies.make_mesh(topo:TopologyDescription,mesh_shape:Sequence[int],axis_names:tuple[str,...],*,contiguous_submeshes:bool=False)->jax.sharding.Mesh


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/rnn.py----------------------------------------
A:jax.experimental.rnn.layer_shape->w_kind(i, input_size, hidden_size, bidirectional)
A:jax.experimental.rnn.layer_shapes->_get_params_shapes_in_lstm(input_size, hidden_size, num_layers, bidirectional)
A:jax.experimental.rnn.param_count->get_num_params_in_lstm(input_size, hidden_size, num_layers, bidirectional)
A:jax.experimental.rnn.k->numpy.sqrt(1.0 / hidden_size)
A:jax.experimental.rnn.flat_shapes->_get_params_shapes_in_lstm(input_size, hidden_size, num_layers, bidirectional)
A:jax.experimental.rnn.num_elems->math.prod(shape)
A:jax.experimental.rnn.w_kind[l]->weights[w_offsets:w_offsets + num_elems].reshape(shape)
A:jax.experimental.rnn.precision->jax._src.lax.lax.canonicalize_precision(precision)
A:jax.experimental.rnn.((y, h_n, c_n), _)->lstm_fwd(x, h_0, c_0, weights, seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, precision=precision)
A:jax.experimental.rnn.(W_ii, W_if, W_ig, W_io)->jax.numpy.split(W_ih, 4, axis=0)
A:jax.experimental.rnn.(W_hi, W_hf, W_hg, W_ho)->jax.numpy.split(W_hh, 4, axis=0)
A:jax.experimental.rnn.(b_ii, b_if, b_ig, b_io)->jax.numpy.split(b_ih, 4, axis=0)
A:jax.experimental.rnn.(b_hi, b_hf, b_hg, b_ho)->jax.numpy.split(b_hh, 4, axis=0)
A:jax.experimental.rnn.i->sigmoid(x @ W_ii.T + b_ii[None] + h @ W_hi.T + b_hi[None])
A:jax.experimental.rnn.f->sigmoid(x @ W_if.T + b_if[None] + h @ W_hf.T + b_hf[None])
A:jax.experimental.rnn.g->tanh(x @ W_ig.T + b_ig[None] + h @ W_hg.T + b_hg[None])
A:jax.experimental.rnn.o->sigmoid(x @ W_io.T + b_io[None] + h @ W_ho.T + b_ho[None])
A:jax.experimental.rnn.(carry, y)->cell(carry, x)
A:jax.experimental.rnn.seq_first_y->jax.numpy.where(mask[..., None], seq_first_y, 0)
A:jax.experimental.rnn.cell->partial(lstm_cell, W_ih=W_ih[l], W_hh=W_hh[l], b_ih=b_ih[l], b_hh=b_hh[l])
A:jax.experimental.rnn.cell_fn->partial(scan_fn, cell)
A:jax.experimental.rnn.out->jax.lax.scan(cell_fn, (h_0[l], c_0[l]), seq_first_y_reversed)
A:jax.experimental.rnn.((h_t, c_t), seq_first_y)->_extract_output(seq_lengths, out)
A:jax.experimental.rnn.h_n->jax.numpy.stack(final_h)
A:jax.experimental.rnn.c_n->jax.numpy.stack(final_c)
A:jax.experimental.rnn.((h_t, c_t), seq_first_y_fwd)->_extract_output(seq_lengths, out)
A:jax.experimental.rnn.seq_first_y_reversed->_flip_sequence(seq_first_y, seq_lengths)
A:jax.experimental.rnn.((h_t, c_t), seq_first_y_bwd)->_extract_output(seq_lengths, out)
A:jax.experimental.rnn.seq_first_y_bwd->_flip_sequence(seq_first_y_bwd, seq_lengths)
A:jax.experimental.rnn.h_t->_select_last_carry(hs, seq_lengths)
A:jax.experimental.rnn.c_t->_select_last_carry(cs, seq_lengths)
A:jax.experimental.rnn.cudnn_allow_tf32->_lstm_cudnn_allow_tf32(precision)
A:jax.experimental.rnn.(y, h_n, c_n, reserve_space)->jax._src.core.Primitive('rnn_fwd').bind(x, h_0, c_0, w, seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, cudnn_allow_tf32=cudnn_allow_tf32)
A:jax.experimental.rnn.output_aval->jax._src.core.ShapedArray(output_shape, x_aval.dtype)
A:jax.experimental.rnn.(_, reserve_space_size)->jax._src.lib.gpu_rnn.compute_rnn_workspace_reserve_space_sizes(input_size, hidden_size, num_layers, batch_size, max_seq_length, dropout, bidirectional)
A:jax.experimental.rnn.reserve_space_aval->jax._src.core.ShapedArray((reserve_space_size,), jnp.float32)
A:jax.experimental.rnn.rnn_fwd_p->jax._src.core.Primitive('rnn_fwd')
A:jax.experimental.rnn.(dx, dh_0, dc_0, dw)->jax._src.core.Primitive('rnn_bwd').bind(dy, dh_n, dc_n, x, h_0, c_0, w, y, reserve_space, seq_lengths, input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, bidirectional=bidirectional, cudnn_allow_tf32=cudnn_allow_tf32)
A:jax.experimental.rnn.rnn_bwd_p->jax._src.core.Primitive('rnn_bwd')
jax.experimental.rnn._W_hh_l(layer_i:int,input_size:int,hidden_size:int,bidirectional:bool)->Shape
jax.experimental.rnn._W_ih_l(layer_i:int,input_size:int,hidden_size:int,bidirectional:bool)->Shape
jax.experimental.rnn._b_hh_l(layer_i:int,input_size:int,hidden_size:int,bidirectional:bool)->Shape
jax.experimental.rnn._b_ih_l(layer_i:int,input_size:int,hidden_size:int,bidirectional:bool)->Shape
jax.experimental.rnn._extract_output(seq_lengths:Array,out)->tuple[tuple[Array, Array], Array]
jax.experimental.rnn._flip_sequence(sequences:Array,seq_lengths:Array)->Array
jax.experimental.rnn._get_params_shapes_in_lstm(input_size:int,hidden_size:int,num_layers:int,bidirectional:bool)->list[Shape]
jax.experimental.rnn._gpu_lowering_strip_tf32(fn,*args,cudnn_allow_tf32,**kw)
jax.experimental.rnn._lstm_cudnn_allow_tf32(precision:lax.PrecisionLike)->bool
jax.experimental.rnn._select_last_carry(carry_seq:Array,seq_lengths:Array)
jax.experimental.rnn.get_num_params_in_lstm(input_size:int,hidden_size:int,num_layers:int,bidirectional:bool)->int
jax.experimental.rnn.init_lstm_weight(rng:PRNGKeyArray,input_size:int,hidden_size:int,num_layers:int,bidirectional:bool)
jax.experimental.rnn.lstm(x:Array,h_0:Array,c_0:Array,weights:Array,seq_lengths:Array,input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool,precision:lax.PrecisionLike=None)->tuple[Array, Array, Array]
jax.experimental.rnn.lstm_bwd(input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool,precision:lax.PrecisionLike,residuals,gradients)
jax.experimental.rnn.lstm_fwd(x:Array,h_0:Array,c_0:Array,w:Array,seq_lengths:Array,input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool,precision:lax.PrecisionLike)
jax.experimental.rnn.lstm_ref(x:Array,h_0:Array,c_0:Array,W_ih:dict[int,Array],W_hh:dict[int,Array],b_ih:dict[int,Array],b_hh:dict[int,Array],seq_lengths:Array,input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool)->tuple[Array, Array, Array]
jax.experimental.rnn.rnn_abstract_eval(x_aval,h_0_aval,c_0_aval,w_aval,seq_lengths_aval,input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool,cudnn_allow_tf32:bool)
jax.experimental.rnn.rnn_bwd_abstract_eval(dy_aval,dhn_aval,dcn_aval,x_aval,h0_aval,c0_aval,w_aval,y_aval,reserve_space_aval,seq_lengths_aval,input_size:int,hidden_size:int,num_layers:int,dropout:float,bidirectional:bool,cudnn_allow_tf32:bool)
jax.experimental.rnn.unpack_lstm_weights(weights:Array,input_size:int,hidden_size:int,num_layers:int,bidirectional:bool)->tuple[dict[int, Array], dict[int, Array], dict[int, Array], dict[int, Array]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/multihost_utils.py----------------------------------------
A:jax.experimental.multihost_utils.global_mesh->jax.sharding.Mesh(devices, ('processes', 'local_devices'))
A:jax.experimental.multihost_utils.pspec->P('processes')
A:jax.experimental.multihost_utils.inp->numpy.expand_dims(inp, axis=0)
A:jax.experimental.multihost_utils.in_tree->jax.tree_map(pre_jit, in_tree)
A:jax.experimental.multihost_utils.out_tree->jax.jit(_psum, out_shardings=jax.sharding.NamedSharding(global_mesh, P()))(in_tree)
A:jax.experimental.multihost_utils.h->numpy.uint32(zlib.crc32(name.encode()))
A:jax.experimental.multihost_utils.reps->jax._src.sharding_impls.GSPMDSharding.get_replicated(inp.sharding._device_assignment)
A:jax.experimental.multihost_utils.out->pjit(_identity_fn, out_shardings=None)(global_arr)
A:jax.experimental.multihost_utils.devices->numpy.array(jax.devices()).reshape(jax.process_count(), jax.local_device_count())
A:jax.experimental.multihost_utils.s->jax.sharding.NamedSharding(global_mesh, pspec)
A:jax.experimental.multihost_utils.host_np_arr->numpy.expand_dims(host_np_arr, axis=0)
A:jax.experimental.multihost_utils.aval->jax._src.core.ShapedArray(host_np_arr.shape, host_np_arr.dtype)
A:jax.experimental.multihost_utils.global_aval->_local_to_global_aval(core.ShapedArray(arr.shape, arr.dtype), global_mesh, pspec)
A:jax.experimental.multihost_utils.global_arr->jax._src.array.make_array_from_single_device_arrays(global_aval.shape, s, bufs)
A:jax.experimental.multihost_utils.expected->broadcast_one_to_all(in_tree)
A:jax.experimental.multihost_utils.arr->jax.interpreters.xla.canonicalize_dtype(arr)
A:jax.experimental.multihost_utils.local_sharding->jax.sharding.NamedSharding(global_mesh.local_mesh, pspec)
A:jax.experimental.multihost_utils.arrays->list((arr[index] for (d, index) in local_sharding.devices_indices_map(arr.shape).items()))
A:jax.experimental.multihost_utils.(flat_inps, in_tree)->tree_flatten(local_inputs)
A:jax.experimental.multihost_utils.in_pspecs->_flatten_pspecs('input pspecs', in_tree, pjit_lib.hashable_pytree(pspecs))
A:jax.experimental.multihost_utils.host_local_array_to_global_array_p->jax._src.core.Primitive('host_local_array_to_global_array')
A:jax.experimental.multihost_utils.new_pspec->P(*new_pspec)
A:jax.experimental.multihost_utils.y->jax._src.core.Primitive('host_local_array_to_global_array').bind(x, global_mesh=global_mesh, pspec=new_pspec)
A:jax.experimental.multihost_utils.batching.spmd_axis_primitive_batchers[host_local_array_to_global_array_p]->partial(ltg_batcher, False)
A:jax.experimental.multihost_utils.batching.axis_primitive_batchers[host_local_array_to_global_array_p]->partial(ltg_batcher, False, None)
A:jax.experimental.multihost_utils.global_sharding->jax.sharding.NamedSharding(global_mesh, pspec)
A:jax.experimental.multihost_utils.local_aval->_global_to_local_aval(core.ShapedArray(arr.shape, arr.dtype), global_mesh, pspec)
A:jax.experimental.multihost_utils.resharded_array->jax.device_put(arr, global_sharding)
A:jax.experimental.multihost_utils.(flat_inps, out_tree)->tree_flatten(global_inputs)
A:jax.experimental.multihost_utils.out_pspecs->_flatten_pspecs('output pspecs', out_tree, pjit_lib.hashable_pytree(pspecs))
A:jax.experimental.multihost_utils.global_array_to_host_local_array_p->jax._src.core.Primitive('global_array_to_host_local_array')
jax.experimental.multihost_utils._flatten_pspecs(name,in_tree,pspecs_thunk)
jax.experimental.multihost_utils._global_to_local_aval(global_aval,mesh,pspec)
jax.experimental.multihost_utils._gtl_lowering(ctx,x,*,global_mesh,pspec)
jax.experimental.multihost_utils._handle_array_process_allgather(inp,tiled)
jax.experimental.multihost_utils._identity_fn(x)
jax.experimental.multihost_utils._local_to_global_aval(local_aval,mesh,pspec)
jax.experimental.multihost_utils._ltg_lowering(ctx,x,*,global_mesh,pspec)
jax.experimental.multihost_utils._psum(x:Any)->Any
jax.experimental.multihost_utils.assert_equal(in_tree,fail_message:str='')
jax.experimental.multihost_utils.broadcast_one_to_all(in_tree:Any,is_source:Optional[bool]=None)->Any
jax.experimental.multihost_utils.global_array_to_host_local_array(global_inputs:Any,global_mesh:jax.sharding.Mesh,pspecs:Any)
jax.experimental.multihost_utils.global_array_to_host_local_array_impl(arr:Any,*,global_mesh:jax.sharding.Mesh,pspec:Any)
jax.experimental.multihost_utils.gtl_abstract_eval(arr,*,global_mesh,pspec)
jax.experimental.multihost_utils.host_local_array_to_global_array(local_inputs:Any,global_mesh:jax.sharding.Mesh,pspecs:Any)
jax.experimental.multihost_utils.host_local_array_to_global_array_impl(arr:Any,*,global_mesh:jax.sharding.Mesh,pspec:Any)
jax.experimental.multihost_utils.ltg_abstract_eval(arr,*,global_mesh,pspec)
jax.experimental.multihost_utils.ltg_batcher(insert_axis,spmd_axis_name,axis_size,axis_name,main_type,vals_in,dims_in,global_mesh,pspec)
jax.experimental.multihost_utils.process_allgather(in_tree:Any,tiled:bool=False)->Any
jax.experimental.multihost_utils.reached_preemption_sync_point(step_id:int)->bool
jax.experimental.multihost_utils.sync_global_devices(name:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/mesh_utils.py----------------------------------------
A:jax.experimental.mesh_utils.logger->logging.getLogger(__name__)
A:jax.experimental.mesh_utils.device_mesh->numpy.block(blocks.tolist())
A:jax.experimental.mesh_utils.perm->numpy.array(_TRAY_RING_ORDER)
A:jax.experimental.mesh_utils.assignable_physical_mesh->list(physical_mesh.shape)
A:jax.experimental.mesh_utils.axes->itertools.combinations(assignable_physical_mesh, num_axes)
A:jax.experimental.mesh_utils.indices->itertools.combinations(range(len(assignable_physical_mesh)), num_axes)
A:jax.experimental.mesh_utils.dims->tuple((d + 1 for d in max(device_coords)))
A:jax.experimental.mesh_utils.out->numpy.empty(dims, dtype=object)
A:jax.experimental.mesh_utils.mesh_shape->tuple(mesh_shape)
A:jax.experimental.mesh_utils.devices->jax._src.xla_bridge.devices()
A:jax.experimental.mesh_utils.handler->device_kind_handler_dict.get(last_device.device_kind, None)
A:jax.experimental.mesh_utils.result->handler(mesh_shape, devices, contiguous_submeshes=contiguous_submeshes)
A:jax.experimental.mesh_utils.physical_mesh->_transpose_trick(physical_mesh, mesh_shape)
A:jax.experimental.mesh_utils.(device_mesh, assignment)->_create_device_mesh_for_nd_torus(physical_mesh, mesh_shape)
A:jax.experimental.mesh_utils.granule_dict->collections.defaultdict(list)
A:jax.experimental.mesh_utils.granules->list((granule_dict[key] for key in sorted(granule_dict.keys())))
A:jax.experimental.mesh_utils.granule_mesh->numpy.arange(len(granules)).reshape(dcn_mesh_shape)
A:jax.experimental.mesh_utils.blocks->numpy.vectorize(lambda i: per_granule_meshes[i], otypes=[object])(granule_mesh)
jax.experimental.mesh_utils._bounds_from_last_device(last_device)->Sequence[int]
jax.experimental.mesh_utils._create_device_mesh_for_nd_torus(physical_mesh:np.ndarray,mesh_shape:Sequence[int])->tuple[np.ndarray, list[tuple[int, ...]]]
jax.experimental.mesh_utils._get_physical_tpu_mesh(jax_devices:Sequence[Any])->np.ndarray
jax.experimental.mesh_utils._tpu_v2_v3_create_device_mesh(mesh_shape:Sequence[int],devices:Sequence[Any],**unused_kwargs)->np.ndarray
jax.experimental.mesh_utils._transpose_trick(physical_mesh:np.ndarray,mesh_shape:Sequence[int])->np.ndarray
jax.experimental.mesh_utils.create_device_mesh(mesh_shape:Sequence[int],devices:Optional[Sequence[Any]]=None,*,contiguous_submeshes:bool=False)->np.ndarray
jax.experimental.mesh_utils.create_hybrid_device_mesh(mesh_shape:Sequence[int],dcn_mesh_shape:Sequence[int],devices:Optional[Sequence[Any]]=None,*,process_is_granule:bool=False)->np.ndarray


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pjit.py----------------------------------------
A:jax.experimental.pjit.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jet.py----------------------------------------
A:jax.experimental.jet.(order,)->set(map(len, series))
A:jax.experimental.jet.treedef->tree_structure(t)
A:jax.experimental.jet.(f, out_tree)->flatten_fun_output(lu.wrap_init(fun))
A:jax.experimental.jet.(out_primals, out_terms)->unzip2(((t.primal, t.terms) for t in out_tracers))
A:jax.experimental.jet.trace->JetTrace(main, core.cur_sublevel())
A:jax.experimental.jet.in_tracers->map(partial(JetTracer, trace), primals, series)
A:jax.experimental.jet.out_tracers->map(trace.full_raise, ans)
A:jax.experimental.jet.(primals_in, series_in)->unzip2(((t.primal, t.terms) for t in tracers))
A:jax.experimental.jet.(out_flat, out_tree_def)->tree_flatten((primals_out, series_out))
A:jax.experimental.jet.(primal_out, terms_out)->rule(primals_in, series_in, **params)
A:jax.experimental.jet.(primals_and_series, in_tree_def)->tree_flatten((primals_in, series_in))
A:jax.experimental.jet.(f_jet, out_tree_def)->traceable(jet_fun(jet_subtrace(f), order), in_tree_def)
A:jax.experimental.jet.update_params->call_param_updaters.get(call_primitive)
A:jax.experimental.jet.result->jax.experimental.pjit.pjit_p.bind(*primals_and_series, **new_params)
A:jax.experimental.jet.(primals_out, series_out)->tree_unflatten(out_tree_def(), result)
A:jax.experimental.jet.(primals, series)->tree_unflatten(treedef, x)
A:jax.experimental.jet.(out, treedef)->tree_flatten((primals, series))
A:jax.experimental.jet.zero_term->ZeroTerm()
A:jax.experimental.jet.zero_series->ZeroSeries()
A:jax.experimental.jet.jet_rules[prim]->partial(jet, comp)
A:jax.experimental.jet.primal_out->bind(operand, scatter_indices, updates)
A:jax.experimental.jet.jet_rules[lax.cumprod_p]->partial(_cumulative_jet_rule, combine_fn=lax.mul)
A:jax.experimental.jet.jet_rules[lax.cummax_p]->partial(_cumulative_jet_rule, combine_fn=lax.max)
A:jax.experimental.jet.jet_rules[lax.cummin_p]->partial(_cumulative_jet_rule, combine_fn=lax.min)
A:jax.experimental.jet.(c0, cs)->jet(lambda x: lax.div(one, 1 + lax.square(x)), (x,), (series,))
A:jax.experimental.jet.k->len(series)
A:jax.experimental.jet.(x, series)->jet(lax.div, primals_in, series_in)
A:jax.experimental.jet.vu->sum((_scale(k, j) * v[k - j] * u[j] for j in range(1, k + 1)))
A:jax.experimental.jet.uv->sum((_scale(k, j) * u[k - j] * v[j] for j in range(1, k)))
A:jax.experimental.jet.v[k]->jax.numpy.where(x == 0, 0, fact(k - 1) * (y * vu - uv) / x)
A:jax.experimental.jet.(primal_out, series_out)->_logistic_taylor((primals_in,), (series_in,))
A:jax.experimental.jet.conv->sum((scale(k, j) * v[j] * w[k - j] for j in range(0, k)))
A:jax.experimental.jet.one->jax._src.lax.lax._const(x, 1)
A:jax.experimental.jet.jet_rules[lax.sin_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 0)
A:jax.experimental.jet.jet_rules[lax.cos_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 1)
A:jax.experimental.jet.jet_rules[lax.sinh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 0)
A:jax.experimental.jet.jet_rules[lax.cosh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 1)
A:jax.experimental.jet.op->partial(prim.bind, **params)
A:jax.experimental.jet.jet_rules[lax.dot_general_p]->partial(_bilinear_taylor_rule, lax.dot_general_p)
A:jax.experimental.jet.jet_rules[lax.mul_p]->partial(_bilinear_taylor_rule, lax.mul_p)
A:jax.experimental.jet.jet_rules[lax.conv_general_dilated_p]->partial(_bilinear_taylor_rule, lax.conv_general_dilated_p)
A:jax.experimental.jet.axes->params.pop('axes', None)
A:jax.experimental.jet.location_indicators->jax.lax.convert_element_type(lax_internal._eq_meet(operand, lax.reshape(primal_out, shape)), primal_dtype)
A:jax.experimental.jet.counts->jax._src.lax.lax._reduce_sum(location_indicators, axes)
A:jax.experimental.jet.jet_rules[lax.reduce_max_p]->_gen_reduce_choose_taylor_rule(lax_internal._reduce_max)
A:jax.experimental.jet.jet_rules[lax.reduce_min_p]->_gen_reduce_choose_taylor_rule(lax_internal._reduce_min)
A:jax.experimental.jet.zero->jax.lax.full_like(x, 0, shape=())
A:jax.experimental.jet.negs->jax.lax.select(lax.lt(x, zero), lax.full_like(x, -1), lax.full_like(x, 1.0))
A:jax.experimental.jet.(x, y)->jax.numpy.broadcast_arrays(*primal_in)
A:jax.experimental.jet.max_i->jax.lax.select(xey, (x_i + y_i) / 2, max_i)
A:jax.experimental.jet.min_i->jax.lax.select(xey, (x_i + y_i) / 2, min_i)
A:jax.experimental.jet.bind->partial(lax.scatter_add_p.bind, update_jaxpr=update_jaxpr, update_consts=update_consts, dimension_numbers=dimension_numbers, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax.experimental.jet.f->jax._src.linear_util.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.experimental.jet.(jaxpr_jet, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f_jet, primals_and_series_avals)
A:jax.experimental.jet.order->len(series_in[0])
A:jax.experimental.jet.primals_and_series_avals->tuple((shaped_abstractify(x) for x in primals_and_series))
A:jax.experimental.jet.(jaxpr_jet, out_tree_def)->_jet_jaxpr(params['jaxpr'], order, primals_and_series_avals, in_tree_def)
jax.experimental.jet.JetTrace(core.Trace)
jax.experimental.jet.JetTrace.lift(self,val)
jax.experimental.jet.JetTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.experimental.jet.JetTrace.process_call(self,call_primitive,f,tracers,params)
jax.experimental.jet.JetTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers,*,symbolic_zeros)
jax.experimental.jet.JetTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.experimental.jet.JetTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.jet.JetTrace.pure(self,val)
jax.experimental.jet.JetTrace.sublift(self,val)
jax.experimental.jet.JetTracer(self,trace,primal,terms)
jax.experimental.jet.JetTracer.__init__(self,trace,primal,terms)
jax.experimental.jet.JetTracer.aval(self)
jax.experimental.jet.JetTracer.full_lower(self)
jax.experimental.jet.ZeroSeries
jax.experimental.jet.ZeroTerm
jax.experimental.jet._abs_taylor_rule(x,series_in,**params)
jax.experimental.jet._atan2_taylor(primals_in,series_in)
jax.experimental.jet._bilinear_taylor_rule(prim,primals_in,series_in,**params)
jax.experimental.jet._cumulative_jet_rule(primals_in,series_in,*,axis:int,reverse:bool,combine_fn:Callable)
jax.experimental.jet._div_taylor_rule(primals_in,series_in)
jax.experimental.jet._dynamic_slice_jet_rule(primals_in,series_in,**params)
jax.experimental.jet._dynamic_update_slice_jet_rule(primals_in,series_in,**params)
jax.experimental.jet._erf_inv_rule(primals_in,series_in)
jax.experimental.jet._exp_taylor(primals_in,series_in)
jax.experimental.jet._gather_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet._gen_reduce_choose_taylor_rule(chooser_fun)
jax.experimental.jet._get_ind(f,ind)
jax.experimental.jet._integer_pow_taylor(primals_in,series_in,*,y)
jax.experimental.jet._jet_jaxpr(jaxpr:core.ClosedJaxpr,order:int,primals_and_series_avals,in_tree_def)->tuple[core.ClosedJaxpr, Any]
jax.experimental.jet._lax_max_taylor_rule(primal_in,series_in)
jax.experimental.jet._lax_min_taylor_rule(primal_in,series_in)
jax.experimental.jet._log_taylor(primals_in,series_in)
jax.experimental.jet._logistic_taylor(primals_in,series_in)
jax.experimental.jet._pjit_jet_rule(primals_in,series_in,**params)
jax.experimental.jet._pow_taylor(primals_in,series_in)
jax.experimental.jet._scale(k,j)
jax.experimental.jet._scale2(k,j)
jax.experimental.jet._scatter_add_rule(primals_in,series_in,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax.experimental.jet._select_n_taylor_rule(primal_in,series_in,**params)
jax.experimental.jet._sinusoidal_rule(sign,prims,primals_in,series_in)
jax.experimental.jet._tanh_taylor(primals_in,series_in)
jax.experimental.jet.def_comp(prim,comp)
jax.experimental.jet.def_deriv(prim,deriv)
jax.experimental.jet.deflinear(prim)
jax.experimental.jet.defzero(prim)
jax.experimental.jet.deriv_prop(prim,deriv,primals_in,series_in)
jax.experimental.jet.fact(n)
jax.experimental.jet.jet(fun,primals,series)
jax.experimental.jet.jet_fun(order,primals,series)
jax.experimental.jet.jet_subtrace(main,primals,series)
jax.experimental.jet.linear_prop(prim,primals_in,series_in,**params)
jax.experimental.jet.traceable(in_tree_def,*primals_and_series)
jax.experimental.jet.zero_prop(prim,primals_in,series_in,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/serialize_executable.py----------------------------------------
A:jax.experimental.serialize_executable.unloaded_executable->getattr(compiled._executable, '_unloaded_executable', None)
A:jax.experimental.serialize_executable.(args_info_flat, in_tree)->jax.tree_util.tree_flatten(compiled.args_info)
A:jax.experimental.serialize_executable.(unloaded_executable, args_info_flat, no_kwargs)->_JaxPjrtUnpickler(io.BytesIO(serialized), backend).load()
A:jax.experimental.serialize_executable.args_info->in_tree.unflatten(args_info_flat)
A:jax.experimental.serialize_executable.loaded_compiled_obj->getattr(compiled._executable, '_unloaded_executable', None).load()
jax.experimental.serialize_executable._JaxPjrtPickler(pickle.Pickler)
jax.experimental.serialize_executable._JaxPjrtPickler.persistent_id(self,obj)
jax.experimental.serialize_executable._JaxPjrtUnpickler(self,file,backend)
jax.experimental.serialize_executable._JaxPjrtUnpickler.__init__(self,file,backend)
jax.experimental.serialize_executable._JaxPjrtUnpickler.persistent_load(self,pid)
jax.experimental.serialize_executable.deserialize_and_load(serialized,in_tree,out_tree,backend:Optional[Union[str,xc.Client]]=None)
jax.experimental.serialize_executable.serialize(compiled:jax.stages.Compiled)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/host_callback.py----------------------------------------
A:jax.experimental.host_callback._HOST_CALLBACK_INLINE->jax._src.config.DEFINE_bool('jax_host_callback_inline', config.bool_env('JAX_HOST_CALLBACK_INLINE', False), help='Inline the host_callback, if not in a staged context.')
A:jax.experimental.host_callback._HOST_CALLBACK_MAX_QUEUE_BYTE_SIZE->jax._src.config.DEFINE_integer('jax_host_callback_max_queue_byte_size', config.int_env('JAX_HOST_CALLBACK_MAX_QUEUE_BYTE_SIZE', int(256 * 1000000.0)), help='The size in bytes of the buffer used to hold outfeeds from each device. When this capacity is reached consuming outfeeds from the device is paused, thus potentially pausing the device computation, until the Python callback consume more outfeeds.', lower_bound=int(16 * 1000000.0))
A:jax.experimental.host_callback._HOST_CALLBACK_OUTFEED->jax._src.config.DEFINE_bool('jax_host_callback_outfeed', config.bool_env('JAX_HOST_CALLBACK_OUTFEED', False), help='Use outfeed implementation for host_callback, even on CPU and GPU. If false, use the CustomCall implementation. Has no effect on TPU, since only the outfeed mechanism is implemented.')
A:jax.experimental.host_callback.logger->logging.getLogger(__name__)
A:jax.experimental.host_callback.(flat_results, _)->jax._src.tree_util.tree_flatten(result)
A:jax.experimental.host_callback.call_res->_call(tap_func, arg, call_with_device=tap_with_device, result_shape=None, identity=True, device_index=device_index)
A:jax.experimental.host_callback.printer->functools.partial(_print_tap_func, output_stream=output_stream, threshold=threshold, **kwargs)
A:jax.experimental.host_callback.(flat_args, arg_treedef)->jax._src.tree_util.tree_flatten(arg)
A:jax.experimental.host_callback.params['callback']->_CallbackWrapper(callback_func, identity, call_with_device)
A:jax.experimental.host_callback.(flat_results_shape, result_treedef)->jax._src.tree_util.tree_flatten(result_shape)
A:jax.experimental.host_callback.params['flat_results_aval']->tuple(flat_results_aval)
A:jax.experimental.host_callback.flat_results->jax._src.core.Primitive('outside_call').bind(*flat_args, **params)
A:jax.experimental.host_callback._print_tap_lock->threading.Lock()
A:jax.experimental.host_callback.kv_pairs->' '.join([f'{k}: {v}' for (k, v) in sorted(kwargs.items())])
A:jax.experimental.host_callback.outside_call_p->jax._src.core.Primitive('outside_call')
A:jax.experimental.host_callback.results->list(args_to_outfeed)
A:jax.experimental.host_callback.use_outfeed->_use_outfeed(platform)
A:jax.experimental.host_callback.non_empty_flat_results_aval->list(filter(lambda aval: not _aval_is_empty(aval), flat_results_aval))
A:jax.experimental.host_callback.callback_id->_CallbackHandlerData().callback_registry.get(callback)
A:jax.experimental.host_callback.next_token->_CallbackHandlerData().receiver.add_outfeed(comp, current_token, callback_id, args_to_outfeed, device_index)
A:jax.experimental.host_callback.after_outfeed_itoken->xops.AfterAll(comp, [current_itoken, next_token])
A:jax.experimental.host_callback.array_sharding_proto->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.token_sharding_proto->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.infeed_sharding_proto->jax._src.interpreters.xla.tuple_sharding_proto([array_sharding_proto] * len(non_empty_flat_results_aval) + [token_sharding_proto])
A:jax.experimental.host_callback.build_infeed->functools.partial(xops.InfeedWithToken, after_outfeed_itoken, xla_client.Shape.tuple_shape(shape))
A:jax.experimental.host_callback.outs_and_token->_with_sharding_proto(comp, infeed_sharding_proto, build_infeed)
A:jax.experimental.host_callback.outs->xops.GetTupleElement(outs_and_token, 0)
A:jax.experimental.host_callback.next_itoken->xops.GetTupleElement(outs_and_token, 1)
A:jax.experimental.host_callback.replica_id->jax._src.lib.mlir.dialects.hlo.ReplicaIdOp()
A:jax.experimental.host_callback.result_arrays->_outside_call_run_callback(arrays, xb.local_devices()[replica_id], send_infeed=False, identity=identity, flat_results_aval=flat_results_aval, **params)
A:jax.experimental.host_callback.sharding->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.(results, next_token, keep_alive)->jax._src.interpreters.mlir.emit_python_callback(ctx, wrapped_callback, current_token, callback_operands, callback_operand_avals, callback_flat_results_aval, has_side_effect=True, sharding=sharding)
A:jax.experimental.host_callback.arg->jax._src.api.tree_unflatten(arg_treedef, arrays)
A:jax.experimental.host_callback.unpacked_transforms->_unpack_transforms(transforms)
A:jax.experimental.host_callback.res->jax._src.core.Primitive('outside_call').bind(*batched_args, **new_params)
A:jax.experimental.host_callback.(actual_flat_results, actual_result_treedef)->jax._src.tree_util.tree_flatten(res)
A:jax.experimental.host_callback.canonical_flat_results->tuple(util.safe_map(xla.canonicalize_dtype, actual_flat_results))
A:jax.experimental.host_callback.actual_flat_results_aval->_values_to_avals(canonical_flat_results)
A:jax.experimental.host_callback.non_empty_canonical_flat_results->tuple(filter(lambda r: not _aval_is_empty(r), canonical_flat_results))
A:jax.experimental.host_callback.out_primals_tapped->jax._src.core.Primitive('outside_call').bind(*primals, **params)
A:jax.experimental.host_callback.cts_instantiated->tuple(map(_instantiate_zeros, cts, args))
A:jax.experimental.host_callback.transforms->params.get('transforms', ())
A:jax.experimental.host_callback.new_params->_add_transform(params, 'batch', batch_dims)
A:jax.experimental.host_callback.new_jaxpr->new_jaxpr.replace(jaxpr=new_jaxpr.jaxpr.replace(outvars=new_jaxpr_outvars)).replace(jaxpr=new_jaxpr.jaxpr.replace(outvars=new_jaxpr_outvars))
A:jax.experimental.host_callback.mk_new_var->jax._src.core.gensym([jaxpr])
A:jax.experimental.host_callback.last_token_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.last_itoken_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.output_token_var->mk_new_var(last_token_var.aval)
A:jax.experimental.host_callback.output_itoken_var->mk_new_var(last_itoken_var.aval)
A:jax.experimental.host_callback.(cond_jaxpr, _, body_jaxpr, _)->jax._src.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.(branches, linear)->jax._src.util.split_dict(eqn.params, ['branches', 'linear'])
A:jax.experimental.host_callback.(num_consts, num_carry, carry_jaxpr, linear, _, _, _)->jax._src.util.split_dict(eqn.params, ['num_consts', 'num_carry', 'jaxpr', 'linear', 'reverse', 'length', 'unroll'])
A:jax.experimental.host_callback.call_jaxpr->cast(core.Jaxpr, eqn.params['call_jaxpr'])
A:jax.experimental.host_callback.jaxpr->cast(core.ClosedJaxpr, eqn.params['jaxpr'])
A:jax.experimental.host_callback.jaxpr_->cast(core.Jaxpr, eqn.params['jaxpr'])
A:jax.experimental.host_callback.(cond_jaxpr, cond_nconsts, body_jaxpr, body_nconsts)->jax._src.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.transformed_cond_jaxpr->_rewrite_closed_jaxpr(cond_jaxpr, True, True)
A:jax.experimental.host_callback.new_cond_pred_invar->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_cond_jaxpr->jax._src.core.ClosedJaxpr(core.Jaxpr([], new_cond_invars, [new_cond_pred_invar], [], set()), [])
A:jax.experimental.host_callback.transformed_body_jaxpr->_rewrite_closed_jaxpr(body_jaxpr, True, True)
A:jax.experimental.host_callback.new_body_invars_pred->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_invars_token->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_invars_itoken->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.new_body_token2->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_itoken2->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.new_body_pred2->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_token3->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_itoken3->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.effects->jax._src.core.join_effects(*(eqn.effects for eqn in new_body_eqns))
A:jax.experimental.host_callback.new_body_jaxpr->jax._src.core.ClosedJaxpr(core.Jaxpr([], new_body_invars_cond_constvars + new_body_invars_body_constvars + [new_body_invars_pred] + new_body_invars_carry + [new_body_invars_token, new_body_invars_itoken], [new_body_pred2] + new_body_carry2 + [new_body_token3, new_body_itoken3], new_body_eqns, effects), [])
A:jax.experimental.host_callback.pred_out->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.id_p->jax._src.core.Primitive('id')
A:jax.experimental.host_callback.self.lock->threading.Lock()
A:jax.experimental.host_callback._callback_handler_data->_CallbackHandlerData()
A:jax.experimental.host_callback.array_repr->', '.join([f'({a.dtype}{a.shape})' for a in arrays])
A:jax.experimental.host_callback.callback->_CallbackHandlerData().callback_registry_by_id.get(consumer_id)
A:jax.experimental.host_callback.formatted_e->traceback.format_exc()
A:jax.experimental.host_callback.devices->list(itertools.chain(*[backend.local_devices() for backend in clients]))
A:jax.experimental.host_callback.devices_with_outfeed->list(itertools.chain(*[backend.local_devices() for backend in clients_with_outfeed]))
A:jax.experimental.host_callback.device_repr->', '.join([str(d) for d in devices_with_outfeed])
A:jax.experimental.host_callback._callback_handler_data.receiver->outfeed_receiver_module.start(_callback_input_received, tuple(clients_with_outfeed), max_callback_queue_size_bytes, compiler.get_compile_options(1, 1).executable_build_options)
A:jax.experimental.host_callback.lock->threading.Lock()
A:jax.experimental.host_callback.cv->threading.Condition(lock=lock)
A:jax.experimental.host_callback.x_on_dev->jax._src.api.device_put(d_idx, device=d)
jax.experimental.host_callback.CallbackException(Exception)
jax.experimental.host_callback._CallbackHandlerData(self)
jax.experimental.host_callback._CallbackHandlerData.__init__(self)
jax.experimental.host_callback._CallbackHandlerData.stop(self)
jax.experimental.host_callback._CallbackWrapper(self,callback_func,identity,call_with_device)
jax.experimental.host_callback._CallbackWrapper.__eq__(self,other)
jax.experimental.host_callback._CallbackWrapper.__hash__(self)
jax.experimental.host_callback._CallbackWrapper.__init__(self,callback_func,identity,call_with_device)
jax.experimental.host_callback._add_transform(params:dict,name:str,*transform_params)->dict
jax.experimental.host_callback._aval_is_empty(aval)->bool
jax.experimental.host_callback._call(callback_func:Callable,arg,*,result_shape=None,call_with_device=False,device_index=0,identity=False)
jax.experimental.host_callback._callback_input_received(device,consumer_id,arrays:tuple)
jax.experimental.host_callback._initialize_outfeed_receiver(max_callback_queue_size_bytes:int=int(256*1000000.0))
jax.experimental.host_callback._instantiate_zeros(tan,arg)
jax.experimental.host_callback._outside_call_abstract_eval(*args_a:pe.AbstractValue,identity,**params)->Sequence[pe.AbstractValue]
jax.experimental.host_callback._outside_call_batching_rule(batched_args,batch_dims,**params)
jax.experimental.host_callback._outside_call_impl(*args,**params)
jax.experimental.host_callback._outside_call_jvp_rule(primals,tangents,**params)
jax.experimental.host_callback._outside_call_lowering(ctx:mlir.LoweringRuleContext,*args,has_token:bool,identity:bool,device_index:int,flat_results_aval=(),**params)
jax.experimental.host_callback._outside_call_run_callback(arrays,device,*,send_infeed=True,callback,arg_treedef,identity,result_treedef=None,flat_results_aval=None,transforms=(),has_token=False)
jax.experimental.host_callback._outside_call_translation_rule(ctx,avals_in,avals_out,*args_op:XlaOp,has_token,identity,device_index,flat_results_aval=(),**params)
jax.experimental.host_callback._outside_call_transpose_rule(cts,*args,**params)
jax.experimental.host_callback._print_tap_func(arg,transforms,*,device=None,output_stream=None,threshold=1024,**kwargs)
jax.experimental.host_callback._raise_if_using_outfeed_with_pjrt_c_api(backend:xb.XlaBackend)
jax.experimental.host_callback._register_callback(callback:Callable)->int
jax.experimental.host_callback._rewrite_closed_jaxpr(cjaxpr:core.ClosedJaxpr,has_input_token:bool,has_output_token:bool)->core.ClosedJaxpr
jax.experimental.host_callback._rewrite_eqn(eqn:core.JaxprEqn,eqns:list[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,input_itoken_var:core.Var,output_itoken_var:core.Var,mk_new_var:Callable[[core.AbstractValue],core.Var])
jax.experimental.host_callback._rewrite_jaxpr(jaxpr:core.Jaxpr,has_input_token:bool,has_output_token:bool)->core.Jaxpr
jax.experimental.host_callback._rewrite_while_outfeed_cond(eqn:core.JaxprEqn,eqns:list[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,input_itoken_var:core.Var,output_itoken_var:core.Var,mk_new_var:Callable)
jax.experimental.host_callback._use_outfeed(platform:str)->bool
jax.experimental.host_callback._values_to_avals(vals)->Sequence[core.ShapedArray]
jax.experimental.host_callback._with_sharding_proto(builder,sharding_proto,op_fn,*args,**kwargs)
jax.experimental.host_callback.barrier_wait(logging_name:Optional[str]=None)
jax.experimental.host_callback.call(callback_func:Callable,arg,*,result_shape=None,call_with_device=False,device_index=0)
jax.experimental.host_callback.id_print(arg,*,result=None,tap_with_device=False,device_index=0,output_stream=None,threshold=None,**kwargs)
jax.experimental.host_callback.id_tap(tap_func,arg,*,result=None,tap_with_device=False,device_index=0,**kwargs)
jax.experimental.host_callback.stop_outfeed_receiver()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/profiler.py----------------------------------------
jax.experimental.profiler.get_profiled_instructions_proto(tensorboard_dir:str)->bytes


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/checkify.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/custom_partitioning.py----------------------------------------
A:jax.experimental.custom_partitioning.ba->inspect.signature(fun).bind(*args, **kwargs)
A:jax.experimental.custom_partitioning._sharding_callbacks->weakref.WeakValueDictionary()
A:jax.experimental.custom_partitioning.user_shapes->shape.tuple_shapes()
A:jax.experimental.custom_partitioning.user_shardings->user_sharding.tuple_elements()
A:jax.experimental.custom_partitioning.user_shape->info.out_tree.unflatten([info.unflatten_arg_shape(s, sharding) for (s, sharding) in zip(user_shapes, user_shardings)])
A:jax.experimental.custom_partitioning.result_sharding->info.infer_sharding_from_operands(*info.static_args, info.mesh, info.unflatten_arg_shapes(arg_shapes, arg_shardings), info.out_tree.unflatten([_to_jax_shape(s) for s in result_shapes]))
A:jax.experimental.custom_partitioning.result_shardings->_flatten_sharding(info.out_tree, result_sharding, result_shapes)
A:jax.experimental.custom_partitioning.result_shapes->result_shape.tuple_shapes()
A:jax.experimental.custom_partitioning.(mesh, lower_fn, result_sharding, arg_shardings)->info.partition(*info.static_args, info.mesh, info.unflatten_arg_shapes(arg_shapes, arg_shardings), info.out_tree.unflatten([info.unflatten_arg_shape(s, sharding) for (s, sharding) in zip(result_shapes, result_shardings)]))
A:jax.experimental.custom_partitioning.arg_shardings->_flatten_sharding(info.in_tree, arg_shardings, arg_shapes)
A:jax.experimental.custom_partitioning.closed_jaxpr->jax.make_jaxpr(lower_fn, axis_env=list(mesh.shape.items()))(*tiled_args)
A:jax.experimental.custom_partitioning.axis_context->jax._src.sharding_impls.SPMDAxisContext(mesh)
A:jax.experimental.custom_partitioning.built->jax._src.interpreters.mlir.build_xla_computation_helper(closed_jaxpr, name='tmp_xla_computation', platforms=module_context.platforms, backend_or_name=module_context.backend_or_name, axis_context=axis_context.extend_manual(frozenset(mesh.axis_names)))
A:jax.experimental.custom_partitioning.custom_partitioning_p->jax._src.core.Primitive('custom_partitioning')
A:jax.experimental.custom_partitioning.args->tuple((x if i in static_argnums else x for (i, x) in enumerate(args)))
A:jax.experimental.custom_partitioning.static_argnums->set(self.static_argnums)
A:jax.experimental.custom_partitioning.(f_, dyn_args)->argnums_partial(lu.wrap_init(self.fun), dyn_argnums, args, require_static_args_hashable=False)
A:jax.experimental.custom_partitioning.(args_flat, in_tree)->jax.tree_util.tree_flatten(dyn_args)
A:jax.experimental.custom_partitioning.(flat_fun, out_tree)->flatten_fun_nokwargs(f_, in_tree)
A:jax.experimental.custom_partitioning.debug->jax._src.interpreters.partial_eval.debug_info(self.fun, in_tree, out_tree, False, 'custom_partitioning')
A:jax.experimental.custom_partitioning.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals, debug)
A:jax.experimental.custom_partitioning.closed_call->jax._src.core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr), ())
A:jax.experimental.custom_partitioning.out_flat->jax._src.core.Primitive('custom_partitioning').bind(*consts, *args_flat, call=closed_call, partition=self.partition, propagate_user_sharding=self.propagate_user_sharding, infer_sharding_from_operands=self.infer_sharding_from_operands, decode_shardings=self.decode_shardings, in_tree=in_tree, out_tree=out_tree(), static_args=static_args)
A:jax.experimental.custom_partitioning.devices->list(axis_context.mesh.devices.flat)
A:jax.experimental.custom_partitioning.pspec->jax.sharding.PartitionSpec(*pspec, *(None,) * (ndim - len(pspec)))
A:jax.experimental.custom_partitioning.sharding_callback_info->_ShardingCallbackInfo(propagate_user_sharding, partition, to_mesh_pspec_sharding, in_tree, out_tree, infer_sharding_from_operands, ctx.module_context, mesh, static_args)
A:jax.experimental.custom_partitioning.key->str(id(sharding_callback_info))
A:jax.experimental.custom_partitioning.out->jax._src.lib.mlir.dialects.hlo.CustomCallOp(result_types, list(values), call_target_name=ir.StringAttr.get(_CUSTOM_PARTITIONING_CALL_NAME), has_side_effect=ir.BoolAttr.get(False), api_version=mlir.i32_attr(2), called_computations=ir.ArrayAttr.get([]), backend_config=ir.StringAttr.get(key), operand_layouts=None, result_layouts=None)
jax.experimental.custom_partitioning._ShardingCallbackInfo(self,propagate_user_sharding,partition,to_mesh_pspec_sharding,in_tree,out_tree,infer_sharding_from_operands,module_context,mesh,static_args)
jax.experimental.custom_partitioning._ShardingCallbackInfo.__init__(self,propagate_user_sharding,partition,to_mesh_pspec_sharding,in_tree,out_tree,infer_sharding_from_operands,module_context,mesh,static_args)
jax.experimental.custom_partitioning._ShardingCallbackInfo.unflatten_arg_shape(self,s,sharding)
jax.experimental.custom_partitioning._ShardingCallbackInfo.unflatten_arg_shapes(self,arg_shapes,arg_shardings)
jax.experimental.custom_partitioning._check_for_tracers(x)
jax.experimental.custom_partitioning._custom_partitioning_abstract_eval(*avals,call,in_tree,out_tree,propagate_user_sharding,partition,infer_sharding_from_operands,decode_shardings,static_args)
jax.experimental.custom_partitioning._custom_partitioning_impl(*args,call,in_tree,out_tree,propagate_user_sharding,partition,infer_sharding_from_operands,decode_shardings,static_args)
jax.experimental.custom_partitioning._custom_partitioning_infer_sharding_from_operands(arg_shapes,arg_shardings,result_shape,backend_string)
jax.experimental.custom_partitioning._custom_partitioning_lowering_rule(ctx:mlir.LoweringRuleContext,*values,call,in_tree,out_tree,propagate_user_sharding,partition,infer_sharding_from_operands,decode_shardings,static_args)
jax.experimental.custom_partitioning._custom_partitioning_partition(arg_shapes,arg_shardings,result_shape,result_sharding,backend_string)
jax.experimental.custom_partitioning._custom_partitioning_propagate_user_sharding(user_sharding,shape,backend_string)
jax.experimental.custom_partitioning._flatten_sharding(tree,shardings,shapes)
jax.experimental.custom_partitioning._pack_result_sharding(shape,result_shardings)
jax.experimental.custom_partitioning._resolve_kwargs(fun,args,kwargs)
jax.experimental.custom_partitioning._to_hlo_sharding(sharding,num_dimensions)
jax.experimental.custom_partitioning._to_jax_shape(s)
jax.experimental.custom_partitioning._to_jax_sharded_shape(s,sharding)
jax.experimental.custom_partitioning.custom_partitioning(self,fun,static_argnums=())
jax.experimental.custom_partitioning.custom_partitioning.__init__(self,fun,static_argnums=())
jax.experimental.custom_partitioning.custom_partitioning.def_partition(self,partition,infer_sharding_from_operands,propagate_user_sharding=None,decode_shardings=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/shard_map.py----------------------------------------
A:jax.experimental.shard_map.fun->_handle_reshapes(flat_fun, in_axes_flat, out_axes_thunk)
A:jax.experimental.shard_map.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.shard_map.in_specs_flat->broadcast_prefix(in_specs, args)
A:jax.experimental.shard_map.(e, *_)->prefix_errors(out_specs_, dummy)
A:jax.experimental.shard_map.in_names_flat->tuple(map(_canonicalize_spec, in_specs_flat))
A:jax.experimental.shard_map.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree)
A:jax.experimental.shard_map.out_specs_->out_specs()
A:jax.experimental.shard_map.dummy->tree_unflatten(out_tree(), [object()] * out_tree().num_leaves)
A:jax.experimental.shard_map.out_specs_flat->broadcast_prefix(out_specs_, dummy)
A:jax.experimental.shard_map.out_flat->ShardMapPrimitive('shard_map').bind(fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names), out_names_thunk=new_out_names_thunk, check_rep=check_rep, rewrite=rewrite, auto=auto)
A:jax.experimental.shard_map.msg->_spec_divisibility_error(f, mesh, in_tree, in_specs, fail)
A:jax.experimental.shard_map.SpecErrorType->enum.Enum('SpecErrorType', ['input', 'out'])
A:jax.experimental.shard_map.no_fail->NoFail()
A:jax.experimental.shard_map.in_avals->map(shaped_abstractify, xs)
A:jax.experimental.shard_map.fun_name->getattr(f, '__name__', str(f))
A:jax.experimental.shard_map.ba->_try_infer_args(f, tree)
A:jax.experimental.shard_map.names->_canonicalize_spec(spec)
A:jax.experimental.shard_map.sz->prod((mesh.shape[n] for n in ns))
A:jax.experimental.shard_map.dst->_names_to_pspec(dict(dst_tup))
A:jax.experimental.shard_map.unmentioned->_unmentioned(mesh, dst)
A:jax.experimental.shard_map.need_rep->','.join(map(str, unmentioned))
A:jax.experimental.shard_map.got_rep->','.join(map(str, rep))
A:jax.experimental.shard_map.diff->','.join(map(str, [n for n in unmentioned if n not in rep]))
A:jax.experimental.shard_map.dummy_args->tree_unflatten(tree, [False] * tree.num_leaves)
A:jax.experimental.shard_map.T->TypeVar('T')
A:jax.experimental.shard_map.failures->tree_unflatten(tree, fails)
A:jax.experimental.shard_map.failures_aug->generate_key_paths(failures)
A:jax.experimental.shard_map.specs_->tree_unflatten(tree_structure(specs), generate_key_paths(specs))
A:jax.experimental.shard_map.specs_aug->broadcast_prefix(specs_, failures, is_leaf=leaf)
A:jax.experimental.shard_map.top_trace->jax._src.core.find_top_trace(args)
A:jax.experimental.shard_map.(fun, env_todo)->process_env_traces(fun, top_trace.level, mesh, in_names, out_names_thunk, check_rep, rewrite, auto)
A:jax.experimental.shard_map.out_names->t(out_names)
A:jax.experimental.shard_map.(_, xforms)->env_todo()
A:jax.experimental.shard_map.tracers->map(top_trace.full_raise, args)
A:jax.experimental.shard_map.outs->global_array_to_host_local_array(outs, mesh, out_specs())
A:jax.experimental.shard_map.(todos, _)->env_todo()
A:jax.experimental.shard_map.new_params->dict(eqn.params, jaxpr=jaxpr, in_names=tuple(in_names), out_names=tuple(out_names))
A:jax.experimental.shard_map.jaxpr->_promote_scalar_residuals_jaxpr(jaxpr, which)
A:jax.experimental.shard_map.subfun->jax._src.linear_util.hashable_partial(lu.wrap_init(core.eval_jaxpr), jaxpr, ())
A:jax.experimental.shard_map.axes->dict(eqn.params, jaxpr=jaxpr, in_names=tuple(in_names), out_names=tuple(out_names)).pop('out_names')
A:jax.experimental.shard_map.new_params['out_names_thunk']->HashableFunction(lambda : axes, closure=axes)
A:jax.experimental.shard_map.shard_map_p->ShardMapPrimitive('shard_map')
A:jax.experimental.shard_map.ans->_handle_reshapes(flat_fun, in_axes_flat, out_axes_thunk).call_wrapped(*in_tracers)
A:jax.experimental.shard_map.trace->main.with_cur_sublevel()
A:jax.experimental.shard_map.(outs, (todo, xform))->main.with_cur_sublevel().post_process_shard_map(outs, mesh, in_names, out_names_thunk, check_rep, rewrite, auto)
A:jax.experimental.shard_map.in_avals_->map(partial(_shard_aval, mesh), in_names, in_avals)
A:jax.experimental.shard_map.(jaxpr, genavals, consts)->jax._src.interpreters.partial_eval.trace_to_subjaxpr_dynamic(f, main, in_avals_)
A:jax.experimental.shard_map.out_avals_->map(_check_shapedarray, genavals)
A:jax.experimental.shard_map.in_rep->map(partial(_in_names_to_rep, mesh), in_names)
A:jax.experimental.shard_map.out_rep->_check_rep(mesh, jaxpr.jaxpr, in_rep)
A:jax.experimental.shard_map.out_avals->map(partial(_unshard_aval, mesh), out_names_unknown, out_avals_)
A:jax.experimental.shard_map.source_info->jax._src.source_info_util.current()
A:jax.experimental.shard_map.invars->map(trace.getvar, in_tracers)
A:jax.experimental.shard_map.constvars->map(trace.getvar, map(trace.instantiate_const, consts))
A:jax.experimental.shard_map.outvars->map(trace.makevar, out_tracers)
A:jax.experimental.shard_map.params->dict(mesh=mesh, in_names=(*in_names, *tangent_in_names), out_names_thunk=new_out_names_thunk, check_rep=check_rep, rewrite=rewrite, auto=auto)
A:jax.experimental.shard_map.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe((*const_tracers, *env_tracers), out_tracers, shard_map_p, staged_params, jaxpr.effects, source)
A:jax.experimental.shard_map.f->_promote_scalar_residuals(f)
A:jax.experimental.shard_map.(jaxpr_, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, jaxpr.in_avals)
A:jax.experimental.shard_map.rule->_rewrite_rules.get(prim, partial(_rule_missing, prim))
A:jax.experimental.shard_map.(in_vals, in_reps)->unzip2(((t.val, t.rep) for t in tracers))
A:jax.experimental.shard_map.(out_vals, out_reps)->unzip2(((t.val, t.rep) for t in out_tracers))
A:jax.experimental.shard_map.(f, out_rep)->_grab_out_rep(f)
A:jax.experimental.shard_map.last_used->jax._src.core.last_used(jaxpr)
A:jax.experimental.shard_map.in_nodes_->map(partial(_xla_shard, ctx, mesh, auto), in_names, ctx.avals_in, in_avals_, in_nodes)
A:jax.experimental.shard_map.new_axis_context->jax._src.sharding_impls.SPMDAxisContext(mesh, frozenset(mesh.axis_names))
A:jax.experimental.shard_map.sub_ctx->ctx.module_context.replace(axis_context=new_axis_context)
A:jax.experimental.shard_map.(out_nodes_, tokens_out)->jax._src.interpreters.mlir._call_lowering('shmap_body', (), jaxpr, None, sub_ctx, in_avals_, out_avals_, ctx.tokens_in, *in_nodes_, dim_var_values=ctx.dim_var_values, arg_names=map(_pspec_mhlo_attrs, in_names, in_avals_), result_names=map(_pspec_mhlo_attrs, out_names, out_avals_))
A:jax.experimental.shard_map.manual_proto->jax._src.interpreters.pxla.manual_proto(aval_in, frozenset(mesh.axis_names) - auto, mesh)
A:jax.experimental.shard_map.shard_proto->aval_out.dtype._rules.physical_hlo_sharding(aval_out, shard_proto)
A:jax.experimental.shard_map.sx->jax._src.interpreters.mlir.wrap_with_sharding_op(ctx, x, aval_in, manual_proto, unspecified_dims=set())
A:jax.experimental.shard_map.args->tree_map(lambda x, ax: x if ax is None else jnp.squeeze(x, axis=ax), list(args), list(in_axes))
A:jax.experimental.shard_map.t->main.with_cur_sublevel()
A:jax.experimental.shard_map.in_tracers->map(partial(RewriteTracer, t), in_reps, in_vals)
A:jax.experimental.shard_map.out_tracers->map(t.full_raise, outs)
A:jax.experimental.shard_map.(outs_, out_rep)->unzip2(((t.val, t.rep) for t in out_tracers))
A:jax.experimental.shard_map.src->P(mesh.axis_names)
A:jax.experimental.shard_map.fn->HashablePartial(_match, mesh, check_rep, tuple(dst.items()))
A:jax.experimental.shard_map.val_->_unmatch_spec(self.mesh, {}, val)
A:jax.experimental.shard_map.(in_vals, in_rep)->unzip2(((t.val, t.rep) for t in tracers))
A:jax.experimental.shard_map.eager_rule->eager_rules.get(prim)
A:jax.experimental.shard_map.out_vals->prim.bind(fun, fwd, bwd, *in_vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)
A:jax.experimental.shard_map.rep_rule->_check_rules.get(prim, partial(_rule_missing, prim))
A:jax.experimental.shard_map.aval->jax._src.core.raise_to_shaped(aval)
A:jax.experimental.shard_map.blocks->list(self.val)
A:jax.experimental.shard_map.spec->P(mesh.axis_names)
A:jax.experimental.shard_map.all_blocks->zip(*map(list, args))
A:jax.experimental.shard_map.psum2_p->jax._src.core.AxisPrimitive('psum2')
A:jax.experimental.shard_map.batching.primitive_batchers[psum2_p]->partial(lax_parallel._reduction_batcher, psum2_p)
A:jax.experimental.shard_map.batching.axis_primitive_batchers[psum2_p]->partial(lax_parallel._batched_reduction_collective, psum2_p, lambda v, axis_size: axis_size * v)
A:jax.experimental.shard_map.core.axis_substitution_rules[psum2_p]->partial(lax_parallel._subst_all_names_in_param, 'axes')
A:jax.experimental.shard_map.(xs, treedef)->tree_flatten(x)
A:jax.experimental.shard_map.ys->jax._src.core.AxisPrimitive('pbroadcast').bind(*xs, axes=axes, axis_index_groups=None)
A:jax.experimental.shard_map.pbroadcast_p->jax._src.core.AxisPrimitive('pbroadcast')
A:jax.experimental.shard_map.vals_out->jax._src.core.AxisPrimitive('pbroadcast').bind(*vals_in, axes=axes, axis_index_groups=axis_index_groups)
A:jax.experimental.shard_map.core.axis_substitution_rules[pbroadcast_p]->partial(lax_parallel._subst_all_names_in_param, 'axes')
A:jax.experimental.shard_map.out_vals_->prim.bind(*args_, **params)
A:jax.experimental.shard_map.x->pbroadcast(x, tuple((n for n in src if n not in dst)))
A:jax.experimental.shard_map.out_val->jax._src.core.AxisPrimitive('psum2').bind(*args_, axes=axes, axis_index_groups=axis_index_groups)
A:jax.experimental.shard_map.(jaxpr_, out_rep)->_replication_rewrite_nomatch(mesh, jaxpr, in_rep)
A:jax.experimental.shard_map.(_, carry_rep_in, _)->split_list(in_rep, [num_consts, num_carry])
A:jax.experimental.shard_map.(carry_rep_out, _)->split_list(out_rep, [num_carry])
A:jax.experimental.shard_map.(const_rep, carry_rep_in, xs_rep)->split_list(in_rep, [num_consts, num_carry])
A:jax.experimental.shard_map.(_, out_rep)->_replication_rewrite_nomatch(mesh, jaxpr, in_rep_)
A:jax.experimental.shard_map.(carry_rep_out, ys_rep)->split_list(out_rep, [num_carry])
A:jax.experimental.shard_map.carry_rep_out->map(op.and_, carry_rep_in, carry_rep_out)
A:jax.experimental.shard_map.jaxpr_->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr)
A:jax.experimental.shard_map.(new_jaxpr, out_rep)->_replication_rewrite_nomatch(mesh, call_jaxpr, in_rep)
A:jax.experimental.shard_map.(fun_jaxpr_, out_rep)->_replication_rewrite_nomatch(mesh, fun_jaxpr, in_rep)
A:jax.experimental.shard_map.(_, in_rep_)->split_list(in_rep, [num_consts])
A:jax.experimental.shard_map.fwd_jaxpr->jax._src.core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))
A:jax.experimental.shard_map.(fwd_jaxpr_, out_rep)->_replication_rewrite_nomatch(mesh, fwd_jaxpr, in_rep_)
A:jax.experimental.shard_map.bwd_->_rewrite_bwd(bwd, mesh, lambda : out_rep2[0], in_rep_)
A:jax.experimental.shard_map.(in_vals, in_dims)->unzip2(((t.val, t.batch_dim) for t in in_tracers))
A:jax.experimental.shard_map.(fun, out_dims)->jax._src.interpreters.batching.batch_subtrace(fun, trace.main, tuple(in_dims))
A:jax.experimental.shard_map.make_tracer->partial(batching.BatchTracer, trace, source_info=source_info_util.current())
A:jax.experimental.shard_map.(vals, dims, srcs)->unzip3(((t.val, t.batch_dim, t.source_info) for t in out_tracers))
A:jax.experimental.shard_map.out_names_transform->partial(_batch_out_names, trace.spmd_axis_name, dims)
A:jax.experimental.shard_map.(primals, tangents)->tree_unflatten(treedef, x)
A:jax.experimental.shard_map.(args, in_tree)->tree_flatten((primals, tangents))
A:jax.experimental.shard_map.f_jvp->jax.interpreters.ad.jvp_subtrace(f, trace.main)
A:jax.experimental.shard_map.(f_jvp, which_nz_out)->jax.interpreters.ad.nonzero_tangent_outputs(f_jvp)
A:jax.experimental.shard_map.out_ax->out_names_thunk()
A:jax.experimental.shard_map.(f_jvp, out_tree)->jax.interpreters.ad.traceable(f_jvp, in_tree)
A:jax.experimental.shard_map.result->ShardMapPrimitive('shard_map').bind(f_jvp, *args, **params)
A:jax.experimental.shard_map.(primal_out, tangent_out)->tree_unflatten(out_tree(), result)
A:jax.experimental.shard_map.(out, treedef)->tree_flatten((primals, tangents))
A:jax.experimental.shard_map.(in_knowns, in_avals, in_consts)->jax._src.interpreters.partial_eval.partition_pvals(in_pvals)
A:jax.experimental.shard_map.(unk_in_names, known_in_names)->jax._src.interpreters.partial_eval.partition_list(in_knowns, in_names)
A:jax.experimental.shard_map.in_avals_sharded->map(partial(_shard_aval, mesh), unk_in_names, in_avals)
A:jax.experimental.shard_map.(f_known, aux)->jax._src.interpreters.partial_eval.partial_eval_wrapper_nounits(f, (*in_knowns,), (*in_avals_sharded,))
A:jax.experimental.shard_map.(in_fwd, out_fwd, out_knowns, _, jaxpr, _)->aux()
A:jax.experimental.shard_map.(_, out_known_names)->jax._src.interpreters.partial_eval.partition_list(out_knowns, out_names_thunk())
A:jax.experimental.shard_map.num_res->sum((f1 is None and f2 is None for (f1, f2) in zip(in_fwd, out_fwd)))
A:jax.experimental.shard_map.known_params->dict(mesh=mesh, in_names=(*known_in_names,), out_names_thunk=known_out_names, check_rep=check_rep, rewrite=rewrite, auto=auto)
A:jax.experimental.shard_map.out->_rewrite_bwd(bwd, mesh, lambda : out_rep2[0], in_rep_).call_wrapped(*args)
A:jax.experimental.shard_map.(in_fwd, out_fwd, out_knowns, out_avals_sharded, jaxpr, env)->aux()
A:jax.experimental.shard_map.(out_consts, non_fwd_res)->split_list(out, [len(out) - num_res])
A:jax.experimental.shard_map.(unk_out_names, _)->jax._src.interpreters.partial_eval.partition_list(out_knowns, out_names_thunk())
A:jax.experimental.shard_map.known_out_names_->known_out_names()
A:jax.experimental.shard_map.res->subs_list2(in_fwd, out_fwd, in_consts, out_consts, non_fwd_res)
A:jax.experimental.shard_map.const_tracers->map(trace.new_instantiated_const, res_)
A:jax.experimental.shard_map.env_tracers->map(trace.full_raise, env)
A:jax.experimental.shard_map.unk_params->dict(mesh=mesh, in_names=unk_in_names, out_names=unk_out_names, jaxpr=jaxpr, check_rep=False, rewrite=rewrite, auto=auto)
A:jax.experimental.shard_map.(jaxpr, res, env)->jax._src.interpreters.partial_eval.tracers_to_jaxpr([], unk_tracers)
A:jax.experimental.shard_map.(out_knowns, out_avals_, consts)->jax._src.interpreters.partial_eval.partition_pvals([t.pval for t in tracers])
A:jax.experimental.shard_map.(out_consts, res_)->split_list(out, [len(out) - len(res)])
A:jax.experimental.shard_map.staged_params->dict(jaxpr=jaxpr_, mesh=mesh, in_names=staged_in_names, out_names=(*out_names_unknown,), check_rep=False, rewrite=rewrite, auto=auto)
A:jax.experimental.shard_map.name_stack->main.with_cur_sublevel()._current_truncated_name_stack()
A:jax.experimental.shard_map.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax.experimental.shard_map.(out_names_unknown, out_names_known)->partition_list(out_knowns, out_names)
A:jax.experimental.shard_map.(res, args)->split_list(res_and_args, [len(jaxpr.constvars)])
A:jax.experimental.shard_map.(jaxpr, _, _)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_avals)
A:jax.experimental.shard_map.(all_args, in_tree)->tree_flatten((out_cts, args))
A:jax.experimental.shard_map.(res, undefs)->partition_list(map(ad.is_undefined_primal, args), args)
A:jax.experimental.shard_map.(jaxpr_known, jaxpr_unknown, _, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(pe.close_jaxpr(jaxpr), map(ad.is_undefined_primal, args), False)
A:jax.experimental.shard_map.res_reshaped->jax._src.core.jaxpr_as_fun(jaxpr_known)(*res)
A:jax.experimental.shard_map.(fun_trans, nz_arg_cts)->jax.interpreters.ad.nonzero_outputs(fun_trans)
A:jax.experimental.shard_map.(fun_trans_flat, out_tree)->flatten_fun_nokwargs(fun_trans, in_tree)
A:jax.experimental.shard_map.new_jaxpr->jax._src.core.subst_axis_names_jaxpr(params['jaxpr'], shadowed_subst)
A:jax.experimental.shard_map.(jaxpr_known, jaxpr_staged, unks_out, inst_out, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr, unks_in, inst_in, False, False, saveable)
A:jax.experimental.shard_map.(out_vars, res_vars)->split_list(jaxpr_known.outvars, [num_out_primals])
A:jax.experimental.shard_map.jaxpr_known->jax._src.interpreters.partial_eval.prune_jaxpr_outputs(jaxpr_known, [True] * num_out_primals + which)
A:jax.experimental.shard_map.(jaxpr_known, jaxpr_staged)->_add_reshapes(which, jaxpr_known, jaxpr_staged)
A:jax.experimental.shard_map.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax.experimental.shard_map.(out_binders_known, _)->partition_list(unks_out, eqn.outvars)
A:jax.experimental.shard_map.(_, ins_staged)->partition_list(inst_in, eqn.invars)
A:jax.experimental.shard_map.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax.experimental.shard_map.newvar->jax._src.core.gensym([jaxpr_known, jaxpr_staged])
A:jax.experimental.shard_map.(params_known, params_staged)->_pe_custom_params(unks_in, inst_in, map(op.not_, unks_out), inst_out, in_fwd, out_fwd, which, dict(eqn.params, jaxpr=jaxpr_known), dict(eqn.params, jaxpr=jaxpr_staged))
A:jax.experimental.shard_map.eqn_known->jax._src.interpreters.partial_eval.new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals], eqn.primitive, params_known, jaxpr_known.effects, eqn.source_info)
A:jax.experimental.shard_map.full_res->subs_list2(in_fwd, out_fwd, ins_known, out_binders_known, residuals)
A:jax.experimental.shard_map.eqn_staged->jax._src.interpreters.partial_eval.new_jaxpr_eqn([*full_res, *ins_staged], out_binders_staged, eqn.primitive, params_staged, jaxpr_staged.effects, eqn.source_info)
A:jax.experimental.shard_map.(out_known, res)->split_list(out, [len(out) - sum(which)])
A:jax.experimental.shard_map.(jaxpr_known, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(known, avals_in)
A:jax.experimental.shard_map.(res_, ins)->split_list(args, [len(which)])
A:jax.experimental.shard_map.(jaxpr_staged, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(staged, avals_in)
A:jax.experimental.shard_map.(in_names_known, _)->partition_list(unks_in, params_known['in_names'])
A:jax.experimental.shard_map.(_, out_names_known)->partition_list(kept_outs_known, params_known['out_names'])
A:jax.experimental.shard_map.new_params_known->dict(params_known, in_names=tuple(in_names_known), out_names=tuple(out_names_known))
A:jax.experimental.shard_map.(_, in_names_staged)->partition_list(inst_in, params_staged['in_names'])
A:jax.experimental.shard_map.(_, out_names_staged)->partition_list(kept_outs_staged, params_staged['out_names'])
A:jax.experimental.shard_map.new_params_staged->dict(params_staged, in_names=tuple(in_names_staged), out_names=tuple(out_names_staged), check_rep=False)
A:jax.experimental.shard_map.(jaxpr, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(eqn.params['jaxpr'], used_outputs)
A:jax.experimental.shard_map.(_, in_names)->partition_list(used_inputs, eqn.params['in_names'])
A:jax.experimental.shard_map.(_, out_names)->partition_list(used_outputs, eqn.params['out_names'])
A:jax.experimental.shard_map.new_eqn->jax._src.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [x for (x, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, jaxpr.effects, eqn.source_info)
A:jax.experimental.shard_map.(axis_name, static_broadcasted_tuple, donate_tuple)->_shared_code_pmap(f, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)
A:jax.experimental.shard_map.p->_prepare_pmap(f, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, devices, backend, axis_size, args, kwargs)
A:jax.experimental.shard_map.mesh->Mesh(_get_devices(p, backend), (axis_name,))
A:jax.experimental.shard_map.(_pmapped, in_specs, out_specs)->_cached_shard_map(p.flat_fun, mesh, p.in_axes_flat, p.out_axes_thunk, axis_name)
A:jax.experimental.shard_map.flat_global_args->host_local_array_to_global_array(p.flat_args, mesh, list(in_specs))
A:jax.experimental.shard_map.jitted_f->jax.jit(_pmapped, donate_argnums=(i for (i, val) in enumerate(p.donated_invars) if val))
A:jax.experimental.shard_map.(jitted_f, flat_global_args, out_tree, mesh, out_specs)->infer_params(*args, **kwargs)
A:jax.experimental.shard_map.(jitted_f, _, _, _, _)->infer_params(*args, **kwargs)
A:jax.experimental.shard_map.in_specs->tuple(map(partial(_axis_to_spec, axis_name), in_axes_flat))
A:jax.experimental.shard_map.devs->jax.devices(backend=backend)
A:jax.experimental.shard_map.(f, out_reps)->_rewrite_subtrace(f, self.main, tuple(in_reps))
A:jax.experimental.shard_map.(fun, out_reps1)->_rewrite_subtrace(fun, self.main, in_reps)
A:jax.experimental.shard_map.(jvp, out_reps2)->_rewrite_subtrace(jvp, self.main, in_reps * 2)
A:jax.experimental.shard_map.(fst, out_reps)->jax._src.linear_util.merge_linear_aux(out_reps1, out_reps2)
A:jax.experimental.shard_map.(fwd, out_reps2)->_rewrite_subtrace(fwd, self.main, fwd_in_reps)
A:jax.experimental.shard_map.bwd->_rewrite_bwd(bwd, self.mesh, out_reps2, in_reps)
A:jax.experimental.shard_map.(_, res_tree)->out_trees()
A:jax.experimental.shard_map.(_, out_reps)->split_list(out_reps, [res_tree.num_leaves])
A:jax.experimental.shard_map.in_reps->map(partial(_in_names_to_rep, mesh), in_names)
A:jax.experimental.shard_map.lvl->jax._src.core.dynamic_level()
A:jax.experimental.shard_map.(bwd_, reps_thunk)->_rewrite_subtrace(lu.wrap_init(bwd), main, in_reps())
A:jax.experimental.shard_map.(x,)->jax._src.core.AxisPrimitive('psum2').bind(x, axes=tuple((n for n in dst if n not in src)), axis_index_groups=None)
jax.experimental.shard_map.NoFail
jax.experimental.shard_map.RewriteTrace(self,*args,mesh,dyna)
jax.experimental.shard_map.RewriteTrace.__init__(self,*args,mesh,dyna)
jax.experimental.shard_map.RewriteTrace.lift(self,tracer:core.Tracer)->RewriteTracer
jax.experimental.shard_map.RewriteTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.experimental.shard_map.RewriteTrace.post_process_custom_jvp_call(self,out_tracers,jvp_was_run)
jax.experimental.shard_map.RewriteTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.experimental.shard_map.RewriteTrace.process_call(self,call_primitive,f,in_tracers,params)
jax.experimental.shard_map.RewriteTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax.experimental.shard_map.RewriteTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax.experimental.shard_map.RewriteTrace.process_primitive(self,prim,in_tracers,params)
jax.experimental.shard_map.RewriteTrace.pure(self,val)->RewriteTracer
jax.experimental.shard_map.RewriteTrace.sublift(self,tracer:core.Tracer)->RewriteTracer
jax.experimental.shard_map.RewriteTracer(self,trace,rep,val)
jax.experimental.shard_map.RewriteTracer.__init__(self,trace,rep,val)
jax.experimental.shard_map.RewriteTracer.__str__(self)->str
jax.experimental.shard_map.RewriteTracer.aval(self)->core.AbstractValue
jax.experimental.shard_map.RewriteTracer.full_lower(self)->RewriteTracer
jax.experimental.shard_map.ShardMapPrimitive(core.Primitive)
jax.experimental.shard_map.ShardMapPrimitive.bind(self,fun:lu.WrappedFun,*args:MaybeTracer,mesh:Mesh,in_names:tuple[AxisNames,...],out_names_thunk:Callable[[],tuple[AxisNames,...]],check_rep:bool,rewrite:bool,auto:frozenset[AxisName])->Sequence[MaybeTracer]
jax.experimental.shard_map.ShardMapPrimitive.get_bind_params(self,params)
jax.experimental.shard_map.ShardMapTrace(self,*args,mesh,check)
jax.experimental.shard_map.ShardMapTrace.__init__(self,*args,mesh,check)
jax.experimental.shard_map.ShardMapTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.experimental.shard_map.ShardMapTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.experimental.shard_map.ShardMapTrace.process_axis_index(self,frame)
jax.experimental.shard_map.ShardMapTrace.process_call(self,call_primitive,fun,tracers,params)
jax.experimental.shard_map.ShardMapTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax.experimental.shard_map.ShardMapTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax.experimental.shard_map.ShardMapTrace.process_map(self,map_primitive,fun,tracers,params)
jax.experimental.shard_map.ShardMapTrace.process_primitive(self,prim,tracers,params)
jax.experimental.shard_map.ShardMapTrace.pure(self,val)
jax.experimental.shard_map.ShardMapTrace.sublift(self,tracer)
jax.experimental.shard_map.ShardMapTracer(self,trace,rep,val)
jax.experimental.shard_map.ShardMapTracer.__init__(self,trace,rep,val)
jax.experimental.shard_map.ShardMapTracer.__str__(self)->str
jax.experimental.shard_map.ShardMapTracer.aval(self)
jax.experimental.shard_map.ShardMapTracer.full_lower(self)->ShardMapTracer
jax.experimental.shard_map._RepError(Exception)
jax.experimental.shard_map._SpecError(Exception)
jax.experimental.shard_map._add_reshapes(which,jaxpr_known,jaxpr_staged)
jax.experimental.shard_map._add_singleton(x)
jax.experimental.shard_map._axis_index_check(mesh,*,axis_name)
jax.experimental.shard_map._axis_to_spec(axis_name,ax)
jax.experimental.shard_map._batch_out_names(spmd_axis_name,dims,out_names)
jax.experimental.shard_map._cached_shard_map(flat_fun,mesh,in_axes_flat,out_axes_thunk,axis_name)
jax.experimental.shard_map._canonicalize_spec(spec:PartitionSpec)->AxisNames
jax.experimental.shard_map._check_names(names:Sequence[AxisNames],avals:Sequence[core.ShapedArray])->None
jax.experimental.shard_map._check_rep(mesh:Mesh,jaxpr:core.Jaxpr,in_rep:Sequence[set[AxisName]])->Sequence[set[AxisName]]
jax.experimental.shard_map._check_reps(mesh,names,reps)
jax.experimental.shard_map._check_reps2(mesh,reps_dest,reps)
jax.experimental.shard_map._check_shapedarray(aval:core.AbstractValue)->core.ShapedArray
jax.experimental.shard_map._check_specs(error_type:SpecErrorType,specs:Any)->None
jax.experimental.shard_map._check_specs_vs_args(f:Callable,mesh:Mesh,in_tree:PyTreeDef,in_specs:Specs,in_specs_flat:list[P],xs:list)->None
jax.experimental.shard_map._closed_call_check(mesh,*in_rep,call_jaxpr,**kwargs)
jax.experimental.shard_map._closed_call_rewrite(mesh,in_rep,*args,call_jaxpr,**kwargs)
jax.experimental.shard_map._core_call_check(mesh,*in_rep,call_jaxpr,**kwargs)
jax.experimental.shard_map._custom_jvp_call_check(mesh,*in_rep,call_jaxpr,jvp_jaxpr_thunk,num_consts,symbolic_zeros)
jax.experimental.shard_map._custom_lin_rule(mesh,*_,out_avals,**__)
jax.experimental.shard_map._custom_vjp_call_jaxpr_check(mesh,*in_rep,fun_jaxpr,**_)
jax.experimental.shard_map._custom_vjp_call_jaxpr_rewrite(mesh,in_rep,*args,fun_jaxpr,fwd_jaxpr_thunk,bwd,num_consts,out_trees,symbolic_zeros)
jax.experimental.shard_map._debug_callback_eager_rule(mesh,*args,callback:Callable[...,Any],effect:debugging.DebugEffect)
jax.experimental.shard_map._debug_callback_rule(mesh,*in_rep,**_)
jax.experimental.shard_map._device_put_eager_rule(mesh,x,*,src,device)
jax.experimental.shard_map._device_put_rule(mesh,x,**_)
jax.experimental.shard_map._efficient_transpose_rewrite(mesh,in_names,out_names_thunk,*args)
jax.experimental.shard_map._get_devices(p,backend)
jax.experimental.shard_map._grab_out_rep(*args)
jax.experimental.shard_map._handle_reshapes(in_axes,out_axes_thunk,*args,**kwargs)
jax.experimental.shard_map._in_names_to_rep(mesh:Mesh,names:AxisNames)->set[AxisName]
jax.experimental.shard_map._inout_rep_error(f:Callable,mesh:Mesh,tree:PyTreeDef,specs:Specs,fails:list[set|NoFail])->str
jax.experimental.shard_map._io_callback_rule(mesh,*_,result_avals,**__)
jax.experimental.shard_map._iter_paths(tree:PyTreeDef,specs:Specs,fails:list[T|NoFail])->list[tuple[tuple[KeyPath, P], tuple[KeyPath, T]]]
jax.experimental.shard_map._match(mesh,check_rep,dst_tup,x)
jax.experimental.shard_map._match_rep(mesh:Mesh,out_rep_dst:Sequence[set[AxisName]],*args)
jax.experimental.shard_map._match_replication(src,dst,x)
jax.experimental.shard_map._match_spec(mesh:Mesh,check_rep:bool,rep:set[AxisName],dst:AxisNames,x:JaxType)->JaxType
jax.experimental.shard_map._names_to_pspec(names:AxisNames)->PartitionSpec
jax.experimental.shard_map._no_rewrite(prim,rule,mesh,in_rep,*args,**params)
jax.experimental.shard_map._partial_eval_jaxpr_custom_rule(saveable:Callable[...,pe.RematCases_],unks_in:Sequence[bool],inst_in:Sequence[bool],eqn:core.JaxprEqn)->tuple[core.JaxprEqn, core.JaxprEqn, Sequence[bool], Sequence[bool], list[core.Var]]
jax.experimental.shard_map._pbroadcast_axis_batcher(size,name,trace_type,vals_in,dims_in,*,axes,groups)
jax.experimental.shard_map._pbroadcast_batcher(vals_in,dims_in,*,axes,axis_index_groups)
jax.experimental.shard_map._pbroadcast_check(_,*in_rep,axes,axis_index_groups)
jax.experimental.shard_map._pe_custom_params(unks_in,inst_in,kept_outs_known,kept_outs_staged,in_fwd,out_fwd,which,params_known,params_staged)
jax.experimental.shard_map._pjit_check(mesh,*in_rep,jaxpr,**kwargs)
jax.experimental.shard_map._pjit_rewrite(mesh,in_rep,*args,jaxpr,**kwargs)
jax.experimental.shard_map._prim_applier(prim,params_tup,mesh,*args)
jax.experimental.shard_map._promote_scalar_residuals(*args,**kwargs)
jax.experimental.shard_map._promote_scalar_residuals_jaxpr(jaxpr,which)
jax.experimental.shard_map._pspec_mhlo_attrs(names:AxisNames,aval:core.AbstractValue)->str
jax.experimental.shard_map._psum2_check(_,*in_rep,axes,axis_index_groups)
jax.experimental.shard_map._psum2_transpose_rule(cts,*args,axes,axis_index_groups)
jax.experimental.shard_map._psum_check(_,*in_rep,axes,axis_index_groups)
jax.experimental.shard_map._psum_rewrite(_,in_rep,*args,axes,axis_index_groups)
jax.experimental.shard_map._pure_callback_rule(mesh,*_,result_avals,**__)
jax.experimental.shard_map._rem_singleton(x)
jax.experimental.shard_map._rep_rewrite(mesh:Mesh,jaxpr_:core.ClosedJaxpr,in_rep:Sequence[set[AxisName]],*args:Val)->tuple[tuple[Val], tuple[set[AxisName]]]
jax.experimental.shard_map._replication_rewrite_match(mesh:Mesh,jaxpr:core.ClosedJaxpr,in_rep:Sequence[set[AxisName]],out_rep_dst:Sequence[set[AxisName]])->core.ClosedJaxpr
jax.experimental.shard_map._replication_rewrite_nomatch(mesh:Mesh,jaxpr:core.ClosedJaxpr,in_rep:Sequence[set[AxisName]])->tuple[core.ClosedJaxpr, list[set[AxisName]]]
jax.experimental.shard_map._rewrite_bwd(bwd,mesh,in_reps,reps_dst)
jax.experimental.shard_map._rewrite_subtrace(main,in_reps,*in_vals)
jax.experimental.shard_map._rule_missing(prim:core.Primitive,*_,**__)
jax.experimental.shard_map._scan_check(mesh,*in_rep,jaxpr,num_consts,num_carry,**_)
jax.experimental.shard_map._scan_rewrite(mesh,in_rep,*args,jaxpr,num_consts,num_carry,**params)
jax.experimental.shard_map._shard_aval(mesh:Mesh,names:AxisNames,aval:core.AbstractValue)->core.AbstractValue
jax.experimental.shard_map._shard_map(f:Callable,mesh:Mesh,in_specs:Specs,out_specs:Specs|Callable[[],Specs],check_rep:bool,auto:frozenset[AxisName])
jax.experimental.shard_map._shard_map_axis_subst(params,subst,traverse)
jax.experimental.shard_map._shard_map_batch(trace:batching.BatchTrace,prim:core.Primitive,fun:lu.WrappedFun,in_tracers:Sequence[batching.BatchTracer],mesh:Mesh,in_names:tuple[AxisNames,...],out_names_thunk:Callable[[],tuple[AxisNames,...]],check_rep:bool,rewrite:bool,auto:frozenset)->Sequence[batching.BatchTracer]
jax.experimental.shard_map._shard_map_batch_post_process(trace,out_tracers,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_dce(used_outputs:list[bool],eqn:core.JaxprEqn)->tuple[list[bool], core.JaxprEqn | None]
jax.experimental.shard_map._shard_map_impl(trace,prim,fun,args,*,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_jvp(trace,shard_map_p,f,tracers,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_jvp_post_process(trace,out_tracers,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_lowering(ctx,*in_nodes,jaxpr,mesh,in_names,out_names,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_partial_eval(trace,shard_map_p,f,tracers,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_partial_eval_post_process(trace,tracers,mesh,in_names,out_names_thunk,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_staging(trace:pe.DynamicJaxprTrace,prim:core.Primitive,f:lu.WrappedFun,in_tracers:Sequence[pe.DynamicJaxprTracer],*,mesh:Mesh,in_names:tuple[AxisNames,...],out_names_thunk:Callable[[],tuple[AxisNames,...]],check_rep:bool,rewrite:bool,auto:frozenset)->Sequence[pe.DynamicJaxprTracer]
jax.experimental.shard_map._shard_map_transpose(out_cts,*args,jaxpr,mesh,in_names,out_names,check_rep,rewrite,auto)
jax.experimental.shard_map._shard_map_typecheck(_,*in_atoms,jaxpr,mesh,in_names,out_names,check_rep,rewrite,auto)
jax.experimental.shard_map._spec_divisibility_error(f:Callable,mesh:Mesh,tree:PyTreeDef,specs:Specs,fails:list[core.ShapedArray|NoFail])->str
jax.experimental.shard_map._spec_rank_error(error_type:SpecErrorType,f:Callable,tree:PyTreeDef,specs:Specs,fails:list[core.ShapedArray|NoFail])->str
jax.experimental.shard_map._standard_check(prim,mesh,*in_rep,**__)
jax.experimental.shard_map._standard_collective_check(prim,mesh,x_rep,*,axis_name,**params)
jax.experimental.shard_map._standard_collective_rewrite(prim,mesh,in_rep,x,axis_name,**params)
jax.experimental.shard_map._standard_rewrite_rule(prim,mesh,in_rep,*args,**params)
jax.experimental.shard_map._tie_check(mesh,x_rep,y_rep)
jax.experimental.shard_map._try_infer_args(f,tree)
jax.experimental.shard_map._unmatch(mesh,src_tup,x)
jax.experimental.shard_map._unmatch_spec(mesh:Mesh,src:AxisNames,x:JaxType)->JaxType
jax.experimental.shard_map._unmentioned(mesh:Mesh,names:AxisNames)->list[AxisName]
jax.experimental.shard_map._unshard_aval(mesh:Mesh,names:AxisNames,aval:core.AbstractValue)->core.AbstractValue
jax.experimental.shard_map._valid_repeats(mesh:Mesh,rep:set[AxisName],dst:AxisNames)->bool
jax.experimental.shard_map._xla_shard(ctx:mlir.LoweringRuleContext,mesh,auto,names,aval_in,aval_out,x)
jax.experimental.shard_map._xla_unshard(ctx:mlir.LoweringRuleContext,mesh,auto,names,aval_in,aval_out,xs)
jax.experimental.shard_map.pbroadcast(x,axis_name)
jax.experimental.shard_map.pmap(f,axis_name=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums=(),devices=None,backend=None,axis_size=None,donate_argnums=(),global_arg_shapes=None)
jax.experimental.shard_map.process_env_traces(level:int,mesh,in_names,out_names_thunk,check_rep,rewrite,auto,*args:Any)
jax.experimental.shard_map.register_standard_collective(prim)
jax.experimental.shard_map.shard_map(f:Callable,mesh:Mesh,in_specs:Specs,out_specs:Specs,check_rep:bool=True,auto:frozenset[AxisName]=frozenset())


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/x64_context.py----------------------------------------
jax.experimental.disable_x64()
jax.experimental.enable_x64(new_val:bool=True)
jax.experimental.x64_context.disable_x64()
jax.experimental.x64_context.enable_x64(new_val:bool=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/ode.py----------------------------------------
A:jax.experimental.ode.y->unravel(y_flat)
A:jax.experimental.ode.(ans_flat, _)->ravel_pytree(ans)
A:jax.experimental.ode.dps_c_mid->jax.numpy.array([6025192743 / 30085553152 / 2, 0, 51252292925 / 65400821598 / 2, -2691868925 / 45128329728 / 2, 187940372067 / 1594534317056 / 2, -1776094331 / 19743644256 / 2, 11237099 / 235043384 / 2], dtype=y0.dtype)
A:jax.experimental.ode.dt->jax.numpy.clip(initial_step_size(func_, ts[0], y0, 4, rtol, atol, f0), a_min=0.0, a_max=hmax)
A:jax.experimental.ode.(y0, f0)->promote_dtypes_inexact(y0, f0)
A:jax.experimental.ode.d0->jax.numpy.linalg.norm(y0 / scale.astype(dtype))
A:jax.experimental.ode.d1->jax.numpy.linalg.norm(f0 / scale.astype(dtype))
A:jax.experimental.ode.h0->jax.numpy.where((d0 < 1e-05) | (d1 < 1e-05), 1e-06, 0.01 * d0 / d1)
A:jax.experimental.ode.f1->fun(y1, t0 + h0)
A:jax.experimental.ode.h1->jax.numpy.where((d1 <= 1e-15) & (d2 <= 1e-15), jnp.maximum(1e-06, h0 * 0.001), (0.01 / jnp.maximum(d1, d2)) ** (1.0 / (order + 1.0)))
A:jax.experimental.ode.alpha->jax.numpy.array([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1.0, 1.0, 0], dtype=dt.dtype)
A:jax.experimental.ode.beta->jax.numpy.array([[1 / 5, 0, 0, 0, 0, 0, 0], [3 / 40, 9 / 40, 0, 0, 0, 0, 0], [44 / 45, -56 / 15, 32 / 9, 0, 0, 0, 0], [19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729, 0, 0, 0], [9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656, 0, 0], [35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0]], dtype=f0.dtype)
A:jax.experimental.ode.c_sol->jax.numpy.array([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0], dtype=f0.dtype)
A:jax.experimental.ode.c_error->jax.numpy.array([35 / 384 - 1951 / 21600, 0, 500 / 1113 - 22642 / 50085, 125 / 192 - 451 / 720, -2187 / 6784 - -12231 / 42400, 11 / 84 - 649 / 6300, -1.0 / 60.0], dtype=f0.dtype)
A:jax.experimental.ode.ft->func(yi, ti)
A:jax.experimental.ode.k->jax.lax.fori_loop(1, 7, body_fun, k)
A:jax.experimental.ode.dfactor->jax.numpy.where(mean_error_ratio < 1, 1.0, dfactor)
A:jax.experimental.ode.factor->jax.numpy.minimum(ifactor, jnp.maximum(mean_error_ratio ** (-1.0 / order) * safety, dfactor))
A:jax.experimental.ode.(converted, consts)->jax.custom_derivatives.closure_convert(func, y0, t[0], *args)
A:jax.experimental.ode.(y0, unravel)->ravel_pytree(y0)
A:jax.experimental.ode.func->ravel_first_arg(func, unravel)
A:jax.experimental.ode.out->_odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)
A:jax.experimental.ode.(next_y, next_f, next_y_error, k)->runge_kutta_step(func_, y, f, t, dt)
A:jax.experimental.ode.error_ratio->mean_error_ratio(next_y_error, rtol, atol, y, next_y)
A:jax.experimental.ode.new_interp_coeff->interp_fit_dopri(y, next_y, k, dt)
A:jax.experimental.ode.(_, *carry)->jax.lax.while_loop(cond_fun, body_fun, [0] + carry)
A:jax.experimental.ode.y_target->jax.numpy.polyval(interp_coeff, relative_output_time.astype(interp_coeff.dtype))
A:jax.experimental.ode.f0->func_(y0, ts[0])
A:jax.experimental.ode.interp_coeff->jax.numpy.array([y0] * 5)
A:jax.experimental.ode.(_, ys)->jax.lax.scan(scan_fun, init_carry, ts[1:])
A:jax.experimental.ode.ys->_odeint(func, rtol, atol, mxstep, hmax, y0, ts, *args)
A:jax.experimental.ode.(y_dot, vjpfun)->jax.vjp(func, y, -t, *args)
A:jax.experimental.ode.(_, y_bar, t0_bar, args_bar)->odeint(aug_dynamics, (ys[i], y_bar, t0_bar, args_bar), jnp.array([-ts[i], -ts[i - 1]]), *args, rtol=rtol, atol=atol, mxstep=mxstep, hmax=hmax)
A:jax.experimental.ode.(y_bar, t0_bar, args_bar)->tree_map(op.itemgetter(1), (y_bar, t0_bar, args_bar))
A:jax.experimental.ode.((y_bar, t0_bar, args_bar), rev_ts_bar)->jax.lax.scan(scan_fun, init_carry, jnp.arange(len(ts) - 1, 0, -1))
A:jax.experimental.ode.ts_bar->jax.numpy.concatenate([jnp.array([t0_bar]), rev_ts_bar[::-1]])
jax.experimental.ode._odeint(func,rtol,atol,mxstep,hmax,y0,ts,*args)
jax.experimental.ode._odeint_fwd(func,rtol,atol,mxstep,hmax,y0,ts,*args)
jax.experimental.ode._odeint_rev(func,rtol,atol,mxstep,hmax,res,g)
jax.experimental.ode._odeint_wrapper(func,rtol,atol,mxstep,hmax,y0,ts,*args)
jax.experimental.ode.abs2(x)
jax.experimental.ode.fit_4th_order_polynomial(y0,y1,y_mid,dy0,dy1,dt)
jax.experimental.ode.initial_step_size(fun,t0,y0,order,rtol,atol,f0)
jax.experimental.ode.interp_fit_dopri(y0,y1,k,dt)
jax.experimental.ode.mean_error_ratio(error_estimate,rtol,atol,y0,y1)
jax.experimental.ode.odeint(func,y0,t,*args,rtol=1.4e-08,atol=1.4e-08,mxstep=jnp.inf,hmax=jnp.inf)
jax.experimental.ode.optimal_step_size(last_step,mean_error_ratio,safety=0.9,ifactor=10.0,dfactor=0.2,order=5.0)
jax.experimental.ode.ravel_first_arg(f,unravel)
jax.experimental.ode.ravel_first_arg_(unravel,y_flat,*args)
jax.experimental.ode.runge_kutta_step(func,y0,f0,t0,dt)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/maps.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/impl_no_xla.py----------------------------------------
A:jax.experimental.jax2tf.impl_no_xla.(lhs, lhs_shape)->_transpose_with_shape(lhs, lhs_shape, (0, 2, 3, 1))
A:jax.experimental.jax2tf.impl_no_xla.(rhs, rhs_shape)->_transpose_with_shape(rhs, rhs_shape, (2, 3, 1, 0))
A:jax.experimental.jax2tf.impl_no_xla.pads->jax.lax.padtype_to_pads(in_shape, window_shape, window_strides, pad_str)
A:jax.experimental.jax2tf.impl_no_xla.padding->tuple(padding)
A:jax.experimental.jax2tf.impl_no_xla.x->tensorflow.pad(x, padding)
A:jax.experimental.jax2tf.impl_no_xla.x_shape->tuple((p0 + xs + p1 for (xs, (p0, p1)) in zip(x_shape, padding)))
A:jax.experimental.jax2tf.impl_no_xla.pad_a->int(np.ceil(pad_len_same / 2))
A:jax.experimental.jax2tf.impl_no_xla.tf_window_strides->_normalize_window_strides(window_strides)
A:jax.experimental.jax2tf.impl_no_xla.(padding, lhs_dilation, rhs_dilation)->_normalize_padding_and_dilations(padding, lhs_dilation, rhs_dilation, is_conv1d)
A:jax.experimental.jax2tf.impl_no_xla.(lhs, lhs_shape, rhs, rhs_shape)->_transpose_for_tf_conv(lhs, lhs_shape, rhs, rhs_shape, dimension_numbers)
A:jax.experimental.jax2tf.impl_no_xla.is_transpose->any((d != 1 for d in lhs_dilation))
A:jax.experimental.jax2tf.impl_no_xla.is_atrous->any((d != 1 for d in rhs_dilation))
A:jax.experimental.jax2tf.impl_no_xla.padding_type->pads_to_padtype(operand_shape, window_dimensions, window_strides, padding)
A:jax.experimental.jax2tf.impl_no_xla.(lhs, lhs_shape, padding)->_check_pad_spatial_dims(lhs, lhs_shape, padding)
A:jax.experimental.jax2tf.impl_no_xla.output->tensorflow.transpose(output, inverse_perm)
A:jax.experimental.jax2tf.impl_no_xla.rhs_t->tensorflow.transpose(rhs_t, (0, 1, 3, 2))
A:jax.experimental.jax2tf.impl_no_xla.tf_out_shape->tuple((tf_out_shape[i] for i in (0, 2, 3, 1)))
A:jax.experimental.jax2tf.impl_no_xla.inverse_perm->_invert_permutation(output_perm)
A:jax.experimental.jax2tf.impl_no_xla.(lhs, rhs, convert_result)->jax.experimental.jax2tf.jax2tf._dot_general_convert_to_common_dtype(lhs, _in_avals[0], rhs, _in_avals[1], _out_aval)
A:jax.experimental.jax2tf.impl_no_xla.lhs->tensorflow.expand_dims(lhs, lhs_ndim - 1)
A:jax.experimental.jax2tf.impl_no_xla.rhs->tensorflow.expand_dims(rhs, rhs_ndim)
A:jax.experimental.jax2tf.impl_no_xla.result->tensorflow.reshape(result, jax2tf._eval_shape(args.out_aval.shape))
A:jax.experimental.jax2tf.impl_no_xla.new_id->iter(string.ascii_letters)
A:jax.experimental.jax2tf.impl_no_xla.shared_id->next(new_id)
A:jax.experimental.jax2tf.impl_no_xla.out_axis_ids->list(filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids))
A:jax.experimental.jax2tf.impl_no_xla.spec->'{},{}->{}'.format(''.join(lhs_axis_ids), ''.join(rhs_axis_ids), ''.join(out_axis_ids))
A:jax.experimental.jax2tf.impl_no_xla.expansion[d]->slice(None, None, None)
A:jax.experimental.jax2tf.impl_no_xla.indices_cartesian->tensorflow.concat(indices_by_dim, axis=len(operand_shape))
A:jax.experimental.jax2tf.impl_no_xla.scattered->tensorflow.scatter_nd(indices_cartesian, operand, output_shape)
A:jax.experimental.jax2tf.impl_no_xla.mask->tensorflow.scatter_nd(indices_cartesian, tf.ones_like(operand, dtype=np.bool_), output_shape)
A:jax.experimental.jax2tf.impl_no_xla.operand->tensorflow.reshape(operand, (1,) + operand_shape + (1,))
A:jax.experimental.jax2tf.impl_no_xla.output_shape->jax.experimental.jax2tf.jax2tf._eval_shape(args.out_aval.shape)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.argmin_p]->partial(_argminmax, True)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.argmax_p]->partial(_argminmax, False)
A:jax.experimental.jax2tf.impl_no_xla.(operand, operand_shape)->_pad_spatial_dims(operand, operand_shape, padding)
A:jax.experimental.jax2tf.impl_no_xla.operand_shape->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[0].shape)
A:jax.experimental.jax2tf.impl_no_xla.has_only_spatial_dims->_validate_reduce_window_inputs(operand_shape, computation_name, dtype, window_dimensions, window_strides, base_dilation, window_dilation)
A:jax.experimental.jax2tf.impl_no_xla.(operand, operand_shape, padding_type)->_padding_reduce_window(operand, operand_shape, computation_name, window_dimensions, window_strides, padding)
A:jax.experimental.jax2tf.impl_no_xla.(operand, window_dimensions, window_strides, dilations)->_reshape_reduce_window(operand, operand_shape, window_dimensions, window_strides, window_dilation, has_only_spatial_dims=has_only_spatial_dims)
A:jax.experimental.jax2tf.impl_no_xla.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_min_p]->partial(_reduce_monoid, computation_name='min')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_max_p]->partial(_reduce_monoid, computation_name='max')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_sum_p]->partial(_reduce_monoid, computation_name='add')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_p]->_unimplemented('reduce')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.select_and_scatter_add_p]->_unimplemented('select_and_scatter_add')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.rng_bit_generator_p]->_unimplemented('rng_bit_generator')
A:jax.experimental.jax2tf.impl_no_xla.max_start->tensorflow.cast(tf.subtract(max_indices, slice_sizes), dtype=tf.int32)
A:jax.experimental.jax2tf.impl_no_xla.indices->tensorflow.expand_dims(args.dnums.start_index_map, 1)
A:jax.experimental.jax2tf.impl_no_xla.op_shape->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[0].shape)
A:jax.experimental.jax2tf.impl_no_xla.slice_sizes_tf->jax.experimental.jax2tf.jax2tf._eval_shape(slice_sizes)
A:jax.experimental.jax2tf.impl_no_xla.begin->_clip(op_shape, begin, slice_sizes_tf)
A:jax.experimental.jax2tf.impl_no_xla.shrink_mask->sum((2 ** x for x in args.dnums.collapsed_slice_dims))
A:jax.experimental.jax2tf.impl_no_xla.res->jax.experimental.jax2tf.jax2tf._ensure_tf_shape_if_dynamic(res, jax2tf._aval_to_tf_shape(args.out_aval))
A:jax.experimental.jax2tf.impl_no_xla.expected_offset_dims->tuple(list(range(axis)) + list(range(axis + index_dims, len(op_shape) + index_dims - 1)))
A:jax.experimental.jax2tf.impl_no_xla.squeezed_indices->tensorflow.squeeze(args.start_indices, -1)
A:jax.experimental.jax2tf.impl_no_xla.start_indices->_clip(op_shape, start_indices, update_shape_tf)
A:jax.experimental.jax2tf.impl_no_xla.batch_indices->_gather_generate_indices(tuple((output_shape[i] for i in args.batch_dims)))
A:jax.experimental.jax2tf.impl_no_xla.offset_indices->_gather_generate_indices(tuple((output_shape[i] for i in args.dnums.offset_dims)))
A:jax.experimental.jax2tf.impl_no_xla.mask_output_shape->tuple((output_shape[x] for x in dim_mask))
A:jax.experimental.jax2tf.impl_no_xla.tiled_indices->tensorflow.tile(tf.expand_dims(indices, 0), [batch_indices_size, 1, 1])
A:jax.experimental.jax2tf.impl_no_xla.temp_batch_indices->tensorflow.reshape(temp_batch_indices, (batch_indices_size, size_of_index_map, 1))
A:jax.experimental.jax2tf.impl_no_xla.slice_start_indices->tensorflow.gather_nd(args.start_indices, batch_indices)
A:jax.experimental.jax2tf.impl_no_xla.scatter_indices->get_scatter_indices(indices, batch_indices_size, len(args.dnums.start_index_map))
A:jax.experimental.jax2tf.impl_no_xla.indices_in_operand->tensorflow.scatter_nd(scatter_indices, slice_start_indices, [batch_indices_size, len(op_shape)])
A:jax.experimental.jax2tf.impl_no_xla.clipped_start_indices->_clip(op_shape, indices_in_operand, args.slice_sizes)
A:jax.experimental.jax2tf.impl_no_xla.slice_element_indices->tensorflow.add(tf.repeat(clipped_start_indices, offset_indices_size, axis=0), tf.tile(offset_indices, (batch_indices_size, 1)))
A:jax.experimental.jax2tf.impl_no_xla.results->tensorflow.gather_nd(args.operand, slice_element_indices)
A:jax.experimental.jax2tf.impl_no_xla.temp->tensorflow.reshape(results, shape=mask_output_shape)
A:jax.experimental.jax2tf.impl_no_xla.gather_fill_fn->jax.experimental.jax2tf.jax2tf._convert_jax_impl(lax_slicing._gather_fill, multiple_results=False)
A:jax.experimental.jax2tf.impl_no_xla.gather_args->GatherArgs(operand=operand, start_indices=start_indices, dnums=dimension_numbers, slice_sizes=slice_sizes, op_shape=_in_avals[0].shape, start_indices_shape=_in_avals[1].shape, out_aval=_out_aval)
A:jax.experimental.jax2tf.impl_no_xla.op_size->tensorflow.size(operand)
A:jax.experimental.jax2tf.impl_no_xla.update_shape_tf->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[1].shape)
A:jax.experimental.jax2tf.impl_no_xla.end_indices->tensorflow.add(start_indices, update_shape_tf)
A:jax.experimental.jax2tf.impl_no_xla.id_tensor->tensorflow.reshape(tf.range(op_size), op_shape)
A:jax.experimental.jax2tf.impl_no_xla.scattered_indices->tensorflow.strided_slice(id_tensor, start_indices, end_indices)
A:jax.experimental.jax2tf.impl_no_xla.flat_indices->tensorflow.expand_dims(tf.nest.flatten(scattered_indices), -1)
A:jax.experimental.jax2tf.impl_no_xla.flat_update->tensorflow.nest.flatten(update)
A:jax.experimental.jax2tf.impl_no_xla.update->tensorflow.reshape(update, op_shape)
A:jax.experimental.jax2tf.impl_no_xla.update_mask->tensorflow.reshape(update_mask, op_shape)
A:jax.experimental.jax2tf.impl_no_xla.other_axes->tuple((i for i in range(len(operand.shape)) if i not in axes))
A:jax.experimental.jax2tf.impl_no_xla.suboperand->tensorflow.gather_nd(operand, scatter_indices)
A:jax.experimental.jax2tf.impl_no_xla.updated_suboperand->update_op(suboperand, updates)
A:jax.experimental.jax2tf.impl_no_xla.y->update_op(operand, operand_update)
A:jax.experimental.jax2tf.impl_no_xla.operand_update->unsorted_segment_op(updates, tf.squeeze(scatter_indices, -1), operand.shape[0])
A:jax.experimental.jax2tf.impl_no_xla.fwd->partial(shift_axes_forward, axes=scatter_to_operand_dims)
A:jax.experimental.jax2tf.impl_no_xla.inv->partial(fwd, inverse=True)
A:jax.experimental.jax2tf.impl_no_xla.updates_shifted->shift_axes_forward(updates, axes=update_window_dims, forward=False)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_p]->convert_scatter_jax_to_tf(lambda x, y: y)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_add_p]->convert_scatter_jax_to_tf(tf.add, tf.math.unsorted_segment_sum)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_mul_p]->convert_scatter_jax_to_tf(tf.multiply, tf.math.unsorted_segment_prod)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_min_p]->convert_scatter_jax_to_tf(tf.minimum, tf.math.unsorted_segment_min)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_max_p]->convert_scatter_jax_to_tf(tf.maximum, tf.math.unsorted_segment_max)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.sort_p]->_unimplemented('sort')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_precision_p]->_unimplemented('reduce_precision')
jax.experimental.jax2tf.impl_no_xla.GatherArgs
jax.experimental.jax2tf.impl_no_xla.GatherArgs.__post_init__(self)
jax.experimental.jax2tf.impl_no_xla.GatherArgs.__repr__(self)
jax.experimental.jax2tf.impl_no_xla.GatherArgs.batch_dims(self)
jax.experimental.jax2tf.impl_no_xla._argminmax(is_min:bool,operand:TfVal,axes:Sequence[int],index_dtype:DType,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._check_pad_spatial_dims(x,x_shape,padding)
jax.experimental.jax2tf.impl_no_xla._clip(max_indices:Sequence[TfVal],start_indices:Sequence[TfVal],slice_sizes:Sequence[TfVal])
jax.experimental.jax2tf.impl_no_xla._conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:lax.ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,precision:Optional[tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._conv_transpose_pads_to_padtype(kernel_sdims,lhs_dilation,padding)
jax.experimental.jax2tf.impl_no_xla._dot_general(lhs,rhs,*,dimension_numbers,precision:Optional[tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._dynamic_slice(operand,*start_indices,slice_sizes:core.Shape,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._dynamic_update_slice(operand,update,*start_indices,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._error(primitive_name:str,suffix_msg:str='')->Exception
jax.experimental.jax2tf.impl_no_xla._gather(operand,start_indices,*,dimension_numbers,slice_sizes:core.Shape,indices_are_sorted,unique_indices,mode,fill_value,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._gather_for_multidim_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._gather_for_scalar_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._gather_generate_indices(shape:tuple[int,...])
jax.experimental.jax2tf.impl_no_xla._gather_with_batch_dim(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._gather_with_batch_dims(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._interior_padding(operand,padding_value,padding_config,operand_shape)
jax.experimental.jax2tf.impl_no_xla._invert_permutation(perm)
jax.experimental.jax2tf.impl_no_xla._normalize_padding_and_dilations(padding,lhs_dilation,rhs_dilation,is_conv1d)
jax.experimental.jax2tf.impl_no_xla._normalize_window_strides(window_strides)
jax.experimental.jax2tf.impl_no_xla._pad(operand,padding_value,*,padding_config,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._pad_spatial_dims(x,x_shape,padding)
jax.experimental.jax2tf.impl_no_xla._padding_reduce_window(operand,operand_shape,computation_name,window_dimensions,window_strides,padding)
jax.experimental.jax2tf.impl_no_xla._pre_gather_for_multidim_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._pre_gather_for_scalar_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._pre_gather_with_batch_dim(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._pre_gather_with_batch_dims(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._reduce_monoid(operand,window_dimensions,window_strides,padding,base_dilation,window_dilation,computation_name,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._reduce_window(*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals:Sequence[core.ShapedArray],_out_aval:tuple[core.ShapedArray,...])->tuple[TfVal, ...]
jax.experimental.jax2tf.impl_no_xla._reshape_reduce_window(operand,operand_shape,window_dimensions,window_strides,window_dilation,*,has_only_spatial_dims)
jax.experimental.jax2tf.impl_no_xla._transpose_for_tf_conv(lhs,lhs_shape:core.Shape,rhs,rhs_shape:core.Shape,dimension_numbers)
jax.experimental.jax2tf.impl_no_xla._transpose_with_shape(x:TfVal,x_shape:core.Shape,permutation)->tuple[TfVal, core.Shape]
jax.experimental.jax2tf.impl_no_xla._unimplemented(name)
jax.experimental.jax2tf.impl_no_xla._validate_conv_features(is_transpose,is_atrous,is_depthwise,feature_group_count,batch_group_count,preferred_element_type,lhs_dtype)
jax.experimental.jax2tf.impl_no_xla._validate_reduce_window_inputs(operand_shape,computation_name,dtype,window_dimensions,window_strides,base_dilation,window_dilation)
jax.experimental.jax2tf.impl_no_xla._validate_spatial_dimensions(lhs:TfVal,lhs_shape:core.Shape,rhs:TfVal,rhs_shape:core.Shape)
jax.experimental.jax2tf.impl_no_xla.convert_scatter_jax_to_tf(update_op,unsorted_segment_op=None)
jax.experimental.jax2tf.impl_no_xla.gather_precondition(precondition_fn:Callable[[GatherArgs],None])
jax.experimental.jax2tf.impl_no_xla.pads_to_padtype(in_shape,window_shape,window_strides,padding)->str
jax.experimental.jax2tf.impl_no_xla.shift_axes_forward(operand,axes:tuple[int,...],inverse:bool=False,forward:bool=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/shape_poly.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/jax_export.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/call_tf.py----------------------------------------
A:jax.experimental.jax2tf.call_tf.(args_flat_jax, args_treedef)->jax.tree_util.tree_flatten(args_jax)
A:jax.experimental.jax2tf.call_tf.dtype->jax.dtypes.canonicalize_dtype(v.dtype)
A:jax.experimental.jax2tf.call_tf.v->v.astype(dtype).astype(dtype)
A:jax.experimental.jax2tf.call_tf.args_flat_jax->tuple(map(canonical_arg, args_flat_jax))
A:jax.experimental.jax2tf.call_tf.a_tf_dtype->jax.experimental.jax2tf.jax2tf._to_tf_dtype(a_jax.dtype)
A:jax.experimental.jax2tf.call_tf.args_flat_sig_tf->tuple(map(make_tensorspec, args_flat_jax))
A:jax.experimental.jax2tf.call_tf.(output_shape_dtype_flat, output_shape_dtype_tree)->jax.tree_util.tree_flatten(output_shape_dtype)
A:jax.experimental.jax2tf.call_tf.output_avals->tuple((core.ShapedArray(st.shape, st.dtype) for st in output_shape_dtype_flat))
A:jax.experimental.jax2tf.call_tf.args_tf->args_treedef.unflatten(args_tf_flat)
A:jax.experimental.jax2tf.call_tf.res_tf->callable_tf(*args_tf)
A:jax.experimental.jax2tf.call_tf.t_out->callable_tf(*args_tf).get_attr('Tout')
A:jax.experimental.jax2tf.call_tf.(res_tf_flat, res_treedef_now)->jax.tree_util.tree_flatten(res_tf)
A:jax.experimental.jax2tf.call_tf.function_flat_tf->tensorflow.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph)
A:jax.experimental.jax2tf.call_tf.res_jax_flat->jax._src.core.Primitive('call_tf').bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf, output_avals=output_avals, has_side_effects=has_side_effects, ordered=ordered, call_tf_graph=call_tf_graph)
A:jax.experimental.jax2tf.call_tf.watched_args_tf->tensorflow.nest.map_structure(replace_non_float, args_tf)
A:jax.experimental.jax2tf.call_tf.res->callable_tf(*args_tf)
A:jax.experimental.jax2tf.call_tf.dres_darg->jax.tree_util.tree_map(lambda x: x if x is None else tf.convert_to_tensor(x), dres_darg)
A:jax.experimental.jax2tf.call_tf.ct_args_jax->call_tf(tf_vjp_fun)(args_jax, ct_res_jax)
A:jax.experimental.jax2tf.call_tf.arg_dtype->jax.dtypes.result_type(arg_jax)
A:jax.experimental.jax2tf.call_tf.ct_arg_dtype->jax._src.core.primal_dtype_to_tangent_dtype(arg_dtype)
A:jax.experimental.jax2tf.call_tf.ct_args_jax_fixed->jax.tree_util.tree_map(fix_float0, args_jax, ct_args_jax)
A:jax.experimental.jax2tf.call_tf.r_aval_dtype_tf->jax.experimental.jax2tf.jax2tf._to_tf_dtype(r_aval.dtype)
A:jax.experimental.jax2tf.call_tf.r_aval_shape_tf->jax.experimental.jax2tf.jax2tf._aval_to_tf_shape(r_aval)
A:jax.experimental.jax2tf.call_tf._->len(r_tf.shape)
A:jax.experimental.jax2tf.call_tf.call_tf_p->jax._src.core.Primitive('call_tf')
A:jax.experimental.jax2tf.call_tf.arg_dlpack->jax.dlpack.to_dlpack(arg_jax, take_ownership=False)
A:jax.experimental.jax2tf.call_tf.args_tf_flat->tuple(map(_arg_jax_to_tf, args_jax_flat))
A:jax.experimental.jax2tf.call_tf.res_tf_flat->callable_flat_tf(*args)
A:jax.experimental.jax2tf.call_tf.(res_tf, _)->jax.experimental.jax2tf.jax2tf._tfval_to_tensor_jax_dtype(res_tf)
A:jax.experimental.jax2tf.call_tf.res_jax_platform->res_tf_platform.lower()
A:jax.experimental.jax2tf.call_tf.res_dlpack->tensorflow.experimental.dlpack.to_dlpack(res_tf)
A:jax.experimental.jax2tf.call_tf.call_tf_effect->CallTfEffect()
A:jax.experimental.jax2tf.call_tf.call_tf_ordered_effect->CallTfOrderedEffect()
A:jax.experimental.jax2tf.call_tf.effects->set()
A:jax.experimental.jax2tf.call_tf.concrete_function_flat_tf->_get_concrete_function_tf(function_flat_tf, args_flat_sig_tf)
A:jax.experimental.jax2tf.call_tf.avals_from_tf->tuple((core.ShapedArray(shape, jax2tf_internal._to_jax_dtype(dtype)) for (dtype, shape) in zip(concrete_function_flat_tf.output_dtypes, concrete_function_flat_tf.output_shapes)))
A:jax.experimental.jax2tf.call_tf.tf_platform->platform.upper()
A:jax.experimental.jax2tf.call_tf.captured_ops->tuple((mlir.ir_constant(np.asarray(inp)) for inp in captured_inputs))
A:jax.experimental.jax2tf.call_tf.func_tf_hlo->tensorflow.function(callable_flat_tf, autograph=False, jit_compile=not call_tf_graph).experimental_get_compiler_ir(*args_tf_flat)(stage='hlo_serialized', device_name=tf_device_name)
A:jax.experimental.jax2tf.call_tf.xla_comp->jax._src.lib.xla_client.XlaComputation(func_tf_hlo)
A:jax.experimental.jax2tf.call_tf.res_dtype->res_shape.numpy_dtype()
A:jax.experimental.jax2tf.call_tf.jax_res_dtype->jax.dtypes.canonicalize_dtype(res_dtype)
A:jax.experimental.jax2tf.call_tf.result_shape->jax._src.lib.xla_client.XlaComputation(func_tf_hlo).program_shape().result_shape()
A:jax.experimental.jax2tf.call_tf.result_shapes->jax._src.lib.xla_client.XlaComputation(func_tf_hlo).program_shape().result_shape().tuple_shapes()
A:jax.experimental.jax2tf.call_tf.result_avals->tuple(map(canonical_res_aval, result_shapes))
A:jax.experimental.jax2tf.call_tf.submodule->jax.interpreters.mlir.xla_computation_to_mlir_module(xla_comp)
A:jax.experimental.jax2tf.call_tf.symtab->jax._src.lib.mlir.ir.SymbolTable(submodule.operation)
A:jax.experimental.jax2tf.call_tf.fn->jax.interpreters.mlir.merge_mlir_modules(ctx.module_context.module, f'call_tf_{function_flat_tf.name}', submodule)
A:jax.experimental.jax2tf.call_tf.call->jax._src.lib.mlir.dialects.func.CallOp(callee_result_types, ir.FlatSymbolRefAttr.get(fn), tuple(args_op) + captured_ops)
A:jax.experimental.jax2tf.call_tf.call_tf_concrete_function_list->jax.experimental.jax2tf.jax2tf.get_thread_local_state_call_tf_concrete_function_list()
A:jax.experimental.jax2tf.call_tf.called_index->len(call_tf_concrete_function_list)
A:jax.experimental.jax2tf.call_tf.operands->list(operands)
A:jax.experimental.jax2tf.call_tf.result_types->list(util.flatten([mlir.aval_to_ir_types(aval) for aval in result_avals]))
A:jax.experimental.jax2tf.call_tf.custom_call->jax._src.lib.mlir.dialects.hlo.CustomCallOp(result_types, operands, call_target_name=ir.StringAttr.get('tf.call_tf_function'), has_side_effect=ir.BoolAttr.get(has_side_effects), api_version=mlir.i32_attr(2), called_computations=ir.ArrayAttr.get([]), backend_config=ir.StringAttr.get(''))
A:jax.experimental.jax2tf.call_tf.custom_call.attributes['tf.backend_config']->jax._src.lib.mlir.ir.DictAttr.get(tf_backend_config)
A:jax.experimental.jax2tf.call_tf.results->list(custom_call.results)
A:jax.experimental.jax2tf.call_tf.token->list(custom_call.results).pop(0)
jax.experimental.jax2tf.call_tf(callable_tf:Callable,has_side_effects=True,ordered=False,output_shape_dtype=UnspecifiedOutputShapeDtype(),call_tf_graph=False)->Callable
jax.experimental.jax2tf.call_tf.CallTfEffect(effects.Effect)
jax.experimental.jax2tf.call_tf.CallTfOrderedEffect(effects.Effect)
jax.experimental.jax2tf.call_tf.UnspecifiedOutputShapeDtype
jax.experimental.jax2tf.call_tf._call_tf_abstract_eval(*args_flat_avals,function_flat_tf,args_flat_sig_tf,has_side_effects,ordered,output_avals,call_tf_graph,**__)
jax.experimental.jax2tf.call_tf._call_tf_impl(*args_jax_flat,callable_flat_tf,**_)
jax.experimental.jax2tf.call_tf._call_tf_lowering(ctx:mlir.LoweringRuleContext,*args_op,platform,function_flat_tf,args_flat_sig_tf,has_side_effects,ordered,call_tf_graph,output_avals,**_)
jax.experimental.jax2tf.call_tf._get_concrete_function_tf(function_flat_tf,args_flat_sig_tf)
jax.experimental.jax2tf.call_tf._jax2tf_call_tf(*args:TfVal,callable_flat_tf:Callable,**_)->TfVal
jax.experimental.jax2tf.call_tf._register_call_lowering(platform)
jax.experimental.jax2tf.call_tf.add_to_call_tf_concrete_function_list(concrete_tf_fn:Any,call_tf_concrete_function_list:list[Any])->int
jax.experimental.jax2tf.call_tf.call_tf(callable_tf:Callable,has_side_effects=True,ordered=False,output_shape_dtype=UnspecifiedOutputShapeDtype(),call_tf_graph=False)->Callable
jax.experimental.jax2tf.call_tf.check_tf_result(idx:int,r_tf:TfVal,r_aval:Optional[core.ShapedArray])->TfVal
jax.experimental.jax2tf.call_tf.emit_tf_embedded_graph_custom_call(ctx:mlir.LoweringRuleContext,concrete_function_flat_tf,operands:Sequence[ir.Value],has_side_effects,ordered,output_avals)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/jax2tf.py----------------------------------------
A:jax.experimental.jax2tf.jax2tf._VALID_SCOPE_REGEX->re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\/>-]*$')
A:jax.experimental.jax2tf.jax2tf._INVALID_SCOPE_CHAR->re.compile('[^A-Za-z0-9_.\\/-]')
A:jax.experimental.jax2tf.jax2tf.scope_name->re.compile('[^A-Za-z0-9_.\\/-]').sub('_', name)
A:jax.experimental.jax2tf.jax2tf.DEFAULT_NATIVE_SERIALIZATION->_DefaultNativeSerialization()
A:jax.experimental.jax2tf.jax2tf._thread_local_state->_ThreadLocalState()
A:jax.experimental.jax2tf.jax2tf.native_serialization_platforms->tuple(native_serialization_platforms)
A:jax.experimental.jax2tf.jax2tf.tf_arg_shape->tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))
A:jax.experimental.jax2tf.jax2tf.(_, a_jax_dtype)->_tfval_to_tensor_jax_dtype(a)
A:jax.experimental.jax2tf.jax2tf.args_specs->jax.experimental.export.export.poly_specs(args_tf, polymorphic_shapes=polymorphic_shapes, get_shape_and_dtype=shape_and_dtype_tf)
A:jax.experimental.jax2tf.jax2tf.kwargs_specs->jax.experimental.export.export.poly_specs(kwargs_tf, polymorphic_shapes=None, get_shape_and_dtype=shape_and_dtype_tf)
A:jax.experimental.jax2tf.jax2tf.(args_flat_tf, args_kwargs_tree)->jax.tree_util.tree_flatten(combined_args_tf)
A:jax.experimental.jax2tf.jax2tf.args_flat_tf->tuple(map(partial(_shard_value, skip_replicated_sharding=tf.executing_eagerly()), kept_args_flat_tf, kept_args_avals, exported.in_shardings))
A:jax.experimental.jax2tf.jax2tf.impl->GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla)
A:jax.experimental.jax2tf.jax2tf.(outs_tf, outs_avals, outs_tree)->GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla).run_fun_tf(args_flat_tf)
A:jax.experimental.jax2tf.jax2tf.outs_flat_tf->converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)
A:jax.experimental.jax2tf.jax2tf.(outs_tf, _, outs_tree)->GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla).run_fun_tf(args_flat_tf)
A:jax.experimental.jax2tf.jax2tf.out_tf->jax.tree_util.tree_unflatten(outs_tree, outs_flat_tf)
A:jax.experimental.jax2tf.jax2tf.self.convert_kwargs->dict(native_serialization=False)
A:jax.experimental.jax2tf.jax2tf.self.exported->jax.experimental.export.export.export(self.fun_jax, lowering_platforms=self.native_serialization_platforms, disabled_checks=self.native_serialization_disabled_checks)(*self.args_specs, **self.kwargs_specs)
A:jax.experimental.jax2tf.jax2tf.results->_interpret_jaxpr(jaxpr, *sharded_args, extra_name_stack=util.wrap_name(name, 'pjit'), fresh_constant_cache=False)
A:jax.experimental.jax2tf.jax2tf.fun_name->getattr(fun_jax, '__name__', 'unknown')
A:jax.experimental.jax2tf.jax2tf.name_stack->jax._src.util.wrap_name(fun_name, 'jax2tf')
A:jax.experimental.jax2tf.jax2tf._thread_local_state.tf_outer_name_scope->tensorflow.get_current_name_scope()
A:jax.experimental.jax2tf.jax2tf.(args_specs_flat, self.in_tree)->jax.tree_util.tree_flatten((self.args_specs, self.kwargs_specs))
A:jax.experimental.jax2tf.jax2tf.self.args_avals_flat->tuple(map(lambda a: core.raise_to_shaped(core.get_aval(a)), args_specs_flat))
A:jax.experimental.jax2tf.jax2tf.dim_vars->jax.experimental.export.shape_poly.all_dim_vars(self.args_avals_flat)
A:jax.experimental.jax2tf.jax2tf.(dim_values, _)->_interpret_fun_jax(partial(shape_poly.compute_dim_vars_from_arg_shapes, self.args_avals_flat, args_kwargs_tree=self.in_tree), self.args_flat_tf, self.args_avals_flat, self.name_stack)
A:jax.experimental.jax2tf.jax2tf._thread_local_state.shape_env->zip(dim_vars, dim_values)
A:jax.experimental.jax2tf.jax2tf.(fun_flat_jax, out_tree_thunk)->flatten_fun_jax(self.fun_jax, self.in_tree)
A:jax.experimental.jax2tf.jax2tf.(outs_tf, self.outs_avals)->_interpret_fun_jax(fun_flat_jax, args_flat_tf, self.args_avals_flat, self.name_stack, fresh_constant_cache=True)
A:jax.experimental.jax2tf.jax2tf.(tval, _)->_tfval_to_tensor_jax_dtype(val)
A:jax.experimental.jax2tf.jax2tf.args_poly_specs->jax.experimental.export.export.poly_specs(args_specs, polymorphic_shapes=polymorphic_shapes)
A:jax.experimental.jax2tf.jax2tf.res_poly_spec->jax.eval_shape(fun_jax, *args_poly_specs)
A:jax.experimental.jax2tf.jax2tf.res_polymorphic_shape->jax.tree_util.tree_map(lambda r: str(r.shape), res_poly_spec)
A:jax.experimental.jax2tf.jax2tf.(tree_args, tree_kwargs)->jax.tree_util.tree_unflatten(in_tree, args_flat_jax)
A:jax.experimental.jax2tf.jax2tf.tree_res->fun_jax(*tree_args, **tree_kwargs)
A:jax.experimental.jax2tf.jax2tf.(res_flat_jax, out_tree)->jax.tree_util.tree_flatten(tree_res)
A:jax.experimental.jax2tf.jax2tf.(arg_tf, _)->_tfval_to_tensor_jax_dtype(arg_tf)
A:jax.experimental.jax2tf.jax2tf.arg_tf->tensorflow.identity(arg_tf, f'jax2tf_arg_{arg_idx}')
A:jax.experimental.jax2tf.jax2tf.out_cts_fixed_flat_tf->tuple(map(fix_out_ct, out_cts_flat_tf, outs_avals, outs_tf))
A:jax.experimental.jax2tf.jax2tf.(fun_vjp_jax, vjp_in_avals)->GraphSerializationImpl(fun_jax, args_specs=args_specs, kwargs_specs=kwargs_specs, args_flat_tf=args_flat_tf, enable_xla=enable_xla).get_vjp_fun()
A:jax.experimental.jax2tf.jax2tf.vjp_polymorphic_shapes->tuple((str(a.shape) for a in vjp_in_avals))
A:jax.experimental.jax2tf.jax2tf.in_cts_flat->convert(fun_vjp_jax, with_gradient=with_gradient, polymorphic_shapes=vjp_polymorphic_shapes, **impl.convert_kwargs)(*vjp_args_flat_tf)
A:jax.experimental.jax2tf.jax2tf.subtrace_fun->_interpret_subtrace(lu.wrap_init(fun_jax), main, args_avals)
A:jax.experimental.jax2tf.jax2tf.conversion_dtype->promote_tf_dtype(operand.dtype)
A:jax.experimental.jax2tf.jax2tf.out_shapes_tf->tuple((tuple((d if core.is_constant_dim(d) else None for d in out_aval.shape)) for out_aval in exported.out_avals))
A:jax.experimental.jax2tf.jax2tf.out_types->tuple((_to_tf_dtype(out_aval.dtype) for out_aval in exported.out_avals))
A:jax.experimental.jax2tf.jax2tf.max_supported_version->get_max_supported_version()
A:jax.experimental.jax2tf.jax2tf.call_module_attrs->dict(version=version, Tout=out_types, Sout=out_shapes_tf, function_list=[concrete_fn.function_def.signature.name for concrete_fn in _thread_local_state.call_tf_concrete_function_list] if _thread_local_state.call_tf_concrete_function_list is not None else [], has_token_input_output=False)
A:jax.experimental.jax2tf.jax2tf.call_module_attrs['platforms']->tuple((p.upper() for p in exported.lowering_platforms))
A:jax.experimental.jax2tf.jax2tf.call_module_attrs['disabled_checks']->tuple((str(dc) for dc in exported.disabled_checks))
A:jax.experimental.jax2tf.jax2tf.res->tf_func(x)
A:jax.experimental.jax2tf.jax2tf._in_avals->map(_jax_physical_aval, _in_avals)
A:jax.experimental.jax2tf.jax2tf._out_aval->_jax_physical_aval(_out_aval)
A:jax.experimental.jax2tf.jax2tf.results_jax->impl_jax(*args_jax, **kwargs)
A:jax.experimental.jax2tf.jax2tf.(results_tf, _)->_interpret_fun_jax(impl_multiple_results_jax, args_tf, _in_avals, extra_name_stack)
A:jax.experimental.jax2tf.jax2tf.trace->TensorFlowTrace(main, core.cur_sublevel())
A:jax.experimental.jax2tf.jax2tf.in_tracers->tuple((TensorFlowTracer(trace, val, aval) for (val, aval) in zip(in_vals, in_avals)))
A:jax.experimental.jax2tf.jax2tf.(outs_tf, _)->_interpret_fun_jax(core.jaxpr_as_fun(jaxpr), args_tf, jaxpr.in_avals, extra_name_stack, fresh_constant_cache=fresh_constant_cache)
A:jax.experimental.jax2tf.jax2tf.physical_aval->jax._src.core.physical_aval(aval)
A:jax.experimental.jax2tf.jax2tf.aval->jax._src.core.ShapedArray((), _to_jax_dtype(x.dtype))
A:jax.experimental.jax2tf.jax2tf.jax_dtype->_jax_physical_dtype(jax_dtype)
A:jax.experimental.jax2tf.jax2tf.dt->jax._src.dtypes.canonicalize_dtype(tf_dtype.as_numpy_dtype)
A:jax.experimental.jax2tf.jax2tf.(_, tf_val)->_ThreadLocalState().constant_cache.get(const_key, (None, None))
A:jax.experimental.jax2tf.jax2tf.val->val.__jax_array__().__jax_array__()
A:jax.experimental.jax2tf.jax2tf.tf_val->tensorflow.convert_to_tensor(val, dtype=conversion_dtype)
A:jax.experimental.jax2tf.jax2tf.(dim_vars, dim_values)->jax._src.util.unzip2(_thread_local_state.shape_env)
A:jax.experimental.jax2tf.jax2tf.(shape_values_tf, _)->_interpret_fun_jax(partial(core.evaluate_shape, shape, dim_vars), dim_values, [core.dim_value_aval()] * len(dim_values), '')
A:jax.experimental.jax2tf.jax2tf.phys_aval->_jax_physical_aval(self._aval)
A:jax.experimental.jax2tf.jax2tf.aval_int->int(_eval_shape([aval_dim]))
A:jax.experimental.jax2tf.jax2tf.frame->jax._src.source_info_util.user_frame(source_info)
A:jax.experimental.jax2tf.jax2tf.(tf_val, jax_dtype)->_tfval_to_tensor_jax_dtype(val, memoize_constants=True)
A:jax.experimental.jax2tf.jax2tf.(impl, impl_needs_avals)->self.get_primitive_impl(primitive)
A:jax.experimental.jax2tf.jax2tf.(out_aval, _)->primitive.abstract_eval(*args_avals, **params)
A:jax.experimental.jax2tf.jax2tf.current_name_stack->_get_current_name_stack()
A:jax.experimental.jax2tf.jax2tf.scope->'/'.join([s.name for s in current_name_stack.stack])
A:jax.experimental.jax2tf.jax2tf.op_metadata->_make_op_metadata(primitive, params, source_info=source_info_util.current())
A:jax.experimental.jax2tf.jax2tf.op_metadata_proto->tensorflow.compiler.xla.xla_data_pb2.OpMetadata(op_type=op_metadata.op_type, op_name=op_metadata.op_name, source_file=op_metadata.source_file, source_line=op_metadata.source_line)
A:jax.experimental.jax2tf.jax2tf.val_out->invoke_impl()
A:jax.experimental.jax2tf.jax2tf.out->tensorflow.stop_gradient(out)
A:jax.experimental.jax2tf.jax2tf.interpreted_fun->_interpret_subtrace(fun, self.main, avals)
A:jax.experimental.jax2tf.jax2tf.vals_out->_interpret_subtrace(fun, self.main, avals).call_wrapped(*vals)
A:jax.experimental.jax2tf.jax2tf.vals->tuple((t.val for t in out_tracers))
A:jax.experimental.jax2tf.jax2tf.tf_impl[unexpected]->partial(_unexpected_primitive, unexpected)
A:jax.experimental.jax2tf.jax2tf.x_signed->tensorflow.cast(x, signed_dtype)
A:jax.experimental.jax2tf.jax2tf.res_signed->tensorflow.math.negative(x_signed)
A:jax.experimental.jax2tf.jax2tf.sign->_sign(operand)
A:jax.experimental.jax2tf.jax2tf.floor->tensorflow.math.floor(operand)
A:jax.experimental.jax2tf.jax2tf.cond->tensorflow.math.equal(operand, tf.constant(np.array(0.5), operand.dtype))
A:jax.experimental.jax2tf.jax2tf.x->tensorflow.broadcast_to(x, tf.shape(y))
A:jax.experimental.jax2tf.jax2tf.y->tensorflow.cast(y, unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.acos_p]->_convert_jax_impl(lax_internal.acos_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.asin_p]->_convert_jax_impl(lax_internal.asin_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.atan_p]->_convert_jax_impl(lax_internal.atan_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.logistic_p]->_convert_jax_impl(lax_internal.logistic_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.complex_component_dtype->{tf.complex64: tf.float32, tf.complex128: tf.float64}.get(y.dtype)
A:jax.experimental.jax2tf.jax2tf.zero->tensorflow.constant(0, complex_component_dtype)
A:jax.experimental.jax2tf.jax2tf.one->tensorflow.constant(1, complex_component_dtype)
A:jax.experimental.jax2tf.jax2tf.i->tensorflow.complex(zero, one)
A:jax.experimental.jax2tf.jax2tf.dtype->_to_tf_dtype(dtype)
A:jax.experimental.jax2tf.jax2tf.shape_tf->_eval_shape(shape)
A:jax.experimental.jax2tf.jax2tf.vec->tensorflow.range(tf.cast(shape_tf[dimension], tf.int32), dtype=tf.int32)
A:jax.experimental.jax2tf.jax2tf.quotient->tensorflow.math.floordiv(lhs, rhs)
A:jax.experimental.jax2tf.jax2tf.select->tensorflow.math.logical_and(tf.not_equal(_sign(lhs), _sign(rhs)), tf.not_equal(tf.math.floormod(lhs, rhs), 0))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.max_p]->partial(_minmax, is_min=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.min_p]->partial(_minmax, is_min=True)
A:jax.experimental.jax2tf.jax2tf.clamp_y->tensorflow.where(_shift_in_bounds(x, y), y, x_bits - 1)
A:jax.experimental.jax2tf.jax2tf.y_lt_x_bits->tensorflow.math.less(y_comp, x_bits)
A:jax.experimental.jax2tf.jax2tf.y_ge_0->tensorflow.math.greater_equal(y_comp, 0)
A:jax.experimental.jax2tf.jax2tf.argnums->tensorflow.nest.flatten(argnums)
A:jax.experimental.jax2tf.jax2tf._out_aval_cast->tensorflow.nest.map_structure(cast_aval, kwargs['_out_aval'])
A:jax.experimental.jax2tf.jax2tf.kwargs->dict(kwargs, _in_avals=_in_avals_cast, _out_aval=_out_aval_cast)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.or_p]->handle_boolean_args(tf.bitwise.bitwise_or, argnums=(0, 1), boolean_f=tf.logical_or)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.and_p]->handle_boolean_args(tf.bitwise.bitwise_and, argnums=(0, 1), boolean_f=tf.logical_and)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.xor_p]->handle_boolean_args(tf.bitwise.bitwise_xor, argnums=(0, 1), boolean_f=tf.math.logical_xor)
A:jax.experimental.jax2tf.jax2tf.signed->tensorflow.bitcast(x, signed_dtype)
A:jax.experimental.jax2tf.jax2tf.sign_mask->tensorflow.bitcast(tf.bitwise.right_shift(signed, nbits - 1), unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.sign_magnitude_mask->tensorflow.bitcast(tf.bitwise.right_shift(sign_mask, 1), signed_dtype)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.gt_p]->handle_boolean_args(tf.math.greater, argnums=(0, 1), boolean_f=boolean_greater)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.lt_p]->handle_boolean_args(tf.math.less, argnums=(0, 1), boolean_f=boolean_less)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.ge_p]->handle_boolean_args(tf.math.greater_equal, argnums=(0, 1), boolean_f=boolean_greater_or_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.le_p]->handle_boolean_args(tf.math.less_equal, argnums=(0, 1), boolean_f=boolean_less_or_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.lt_to_p]->handle_boolean_args(partial(_total_order_cond, tf.math.less), argnums=(0, 1), boolean_f=boolean_less)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.le_to_p]->handle_boolean_args(partial(_total_order_cond, tf.math.less_equal), argnums=(0, 1), boolean_f=boolean_less_or_equal)
A:jax.experimental.jax2tf.jax2tf.operand->tensorflow.linalg.adjoint(operand)
A:jax.experimental.jax2tf.jax2tf.op_shape_tf_val->_eval_shape(_in_avals[1].shape, _in_avals[1].dtype)
A:jax.experimental.jax2tf.jax2tf.maxval->tensorflow.broadcast_to(maxval, op_shape_tf_val)
A:jax.experimental.jax2tf.jax2tf.minval->tensorflow.math.minimum(tf.broadcast_to(minval, op_shape_tf_val), maxval)
A:jax.experimental.jax2tf.jax2tf.proto->_scatter_dimensions_proto(scatter_indices.shape, dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.out_tf_shape->_aval_to_tf_shape(_out_aval)
A:jax.experimental.jax2tf.jax2tf.dnums_proto->tensorflow.compiler.xla.xla_data_pb2.DotDimensionNumbers()
A:jax.experimental.jax2tf.jax2tf.precision_config_proto->_precision_config_proto(precision)
A:jax.experimental.jax2tf.jax2tf.tf_version->tuple((int(v) for v in tf.__version__.split('.')[:2]))
A:jax.experimental.jax2tf.jax2tf.k1->gen_conv(_add(lhs_real, lhs_imag), rhs_real, preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.k2->gen_conv(lhs_real, tf.math.subtract(rhs_imag, rhs_real), preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.k3->gen_conv(lhs_imag, _add(rhs_real, rhs_imag), preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.(lhs, rhs, convert_result)->_dot_general_convert_to_common_dtype(lhs, _in_avals[0], rhs, _in_avals[1], _out_aval)
A:jax.experimental.jax2tf.jax2tf.common_dtype->jax._src.dtypes.result_type(lhs_aval, rhs_aval)
A:jax.experimental.jax2tf.jax2tf.lhs->_convert_element_type(lhs, new_dtype=common_dtype)
A:jax.experimental.jax2tf.jax2tf.rhs->_convert_element_type(rhs, new_dtype=common_dtype)
A:jax.experimental.jax2tf.jax2tf.with_1s->tensorflow.reshape(operand, _eval_shape(add_1s_shape, dtype=dtype))
A:jax.experimental.jax2tf.jax2tf.dimensions->tensorflow.range(tf.rank(operand))
A:jax.experimental.jax2tf.jax2tf.new_sizes_tf->_eval_shape(new_sizes, _in_avals[0].dtype)
A:jax.experimental.jax2tf.jax2tf.op_aval->_jax_physical_aval(_in_avals[0])
A:jax.experimental.jax2tf.jax2tf.new_shape->tuple((d for (i, d) in enumerate(op_shape) if i not in dimensions))
A:jax.experimental.jax2tf.jax2tf.new_shape_tf->_eval_shape(new_shape, op_aval.dtype)
A:jax.experimental.jax2tf.jax2tf.(low, high, interior)->jax._src.util.unzip3(map(_eval_shape, padding_config))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_sum_p]->axes_to_axis(tf.reduce_sum)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_prod_p]->axes_to_axis(tf.reduce_prod)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_max_p]->handle_boolean_args(axes_to_axis(tf.reduce_max), argnums=[0], boolean_f=axes_to_axis(tf.reduce_any))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_min_p]->handle_boolean_args(axes_to_axis(tf.reduce_min), argnums=[0], boolean_f=axes_to_axis(tf.reduce_all))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_or_p]->axes_to_axis(tf.reduce_any)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_and_p]->axes_to_axis(tf.reduce_all)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.argmin_p]->partial(_argminmax, True)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.argmax_p]->partial(_argminmax, False)
A:jax.experimental.jax2tf.jax2tf._add_fn->tensorflow.function(_add, autograph=False)
A:jax.experimental.jax2tf.jax2tf._ge_fn->tensorflow.function(tf.math.greater_equal, autograph=False)
A:jax.experimental.jax2tf.jax2tf.a->tensorflow.math.conj(a)
A:jax.experimental.jax2tf.jax2tf.b->tensorflow.transpose(b, transpose_dimensions)
A:jax.experimental.jax2tf.jax2tf.st->_shift_right_logical(t, const(double_word_dtype, nbits))
A:jax.experimental.jax2tf.jax2tf.o_spec->tensorflow.TensorSpec((), dtype=op.dtype)
A:jax.experimental.jax2tf.jax2tf.reducer_fn->tensorflow.function(reducer, autograph=False).get_concrete_function(o_spec, o_spec)
A:jax.experimental.jax2tf.jax2tf.init_val->tensorflow.constant(init_val, operand.dtype)
A:jax.experimental.jax2tf.jax2tf.window_dimensions_tf->_eval_shape(window_dimensions)
A:jax.experimental.jax2tf.jax2tf.window_strides_tf->_eval_shape(window_strides)
A:jax.experimental.jax2tf.jax2tf.window_dilation_tf->_eval_shape(window_dilation)
A:jax.experimental.jax2tf.jax2tf.base_dilation_tf->_eval_shape(base_dilation)
A:jax.experimental.jax2tf.jax2tf.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax.experimental.jax2tf.jax2tf.closed_jaxpr->jax._src.core.ClosedJaxpr(update_jaxpr, update_consts)
A:jax.experimental.jax2tf.jax2tf.(res,)->_interpret_jaxpr(closed_jaxpr, arg1, arg2, extra_name_stack=None)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_sum_p]->partial(_specialized_reduce_window, _add, lambda x: 0, name='reduce_window_sum')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_min_p]->partial(_specialized_reduce_window, partial(_minmax_scalar, is_min=True), _get_min_identity, name='reduce_window_min')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_max_p]->partial(_specialized_reduce_window, partial(_minmax_scalar, is_min=False), _get_max_identity, name='reduce_window_max')
A:jax.experimental.jax2tf.jax2tf.reducer_arg_spec->tuple([tf.TensorSpec((), op.dtype) for op in init_vals] * 2)
A:jax.experimental.jax2tf.jax2tf.xla_reducer_computation->tensorflow.function(reducer_computation, autograph=False).get_concrete_function(*reducer_arg_spec)
A:jax.experimental.jax2tf.jax2tf.outs->tuple((tf.stop_gradient(out) for out in outs))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cummax_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_max, lax_reduce_fn=lax.max, extra_name_stack='cummax')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cummin_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_min, lax_reduce_fn=lax.min, extra_name_stack='cummin')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cumlogsumexp_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_logaddexp, lax_reduce_fn=logaddexp, extra_name_stack='cumlogsumexp')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cumsum_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_sum, lax_reduce_fn=lax.add, extra_name_stack='cumsum')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cumprod_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_prod, lax_reduce_fn=lax.mul, extra_name_stack='cumprod')
A:jax.experimental.jax2tf.jax2tf.init_value->tensorflow.zeros((), operand.dtype)
A:jax.experimental.jax2tf.jax2tf.select_fn->tensorflow.function(tf_impl[select_prim], autograph=False).get_concrete_function(init_value, init_value)
A:jax.experimental.jax2tf.jax2tf.scatter_fn->tensorflow.function(_add, autograph=False).get_concrete_function(init_value, init_value)
A:jax.experimental.jax2tf.jax2tf.converted_impl->_convert_jax_impl(impl_wrapper, multiple_results=False, with_physical_avals=True, extra_name_stack='random_bits')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[random.random_gamma_p]->_convert_jax_impl(partial(random_internal._gamma_impl, use_vmap=True), multiple_results=False, extra_name_stack='random_gamma')
A:jax.experimental.jax2tf.jax2tf.key->tensorflow.compiler.tf2xla.python.xla.bitcast_convert_type(key, _to_tf_dtype(jnp.uint64))
A:jax.experimental.jax2tf.jax2tf.(new_key, res)->tensorflow.compiler.tf2xla.python.xla.rng_bit_generator(algorithm_tf.value, key, shape_tf, dtype=_to_tf_dtype(dtype))
A:jax.experimental.jax2tf.jax2tf.new_key->tensorflow.stop_gradient(new_key)
A:jax.experimental.jax2tf.jax2tf.counts->jax._src.prng.bcast_iotas_to_reshaped_iota(_add, _mul, shape, iotas)
A:jax.experimental.jax2tf.jax2tf.counts_lo->_cast32(counts)
A:jax.experimental.jax2tf.jax2tf.counts_hi->_cast32(tf.bitwise.right_shift(counts, 32))
A:jax.experimental.jax2tf.jax2tf.gather_fill_fn->_convert_jax_impl(lax_slicing._gather_fill, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.start_indices->tensorflow.concat([start_indices, tf.zeros((len(opaque_shape),), dtype=start_indices.dtype)], axis=0)
A:jax.experimental.jax2tf.jax2tf.dimension_numbers->dimension_numbers._replace(offset_dims=(*dimension_numbers.offset_dims, *trailing_offset_dims))._replace(offset_dims=(*dimension_numbers.offset_dims, *trailing_offset_dims))
A:jax.experimental.jax2tf.jax2tf.slice_sizes_tf->_eval_shape(slice_sizes)
A:jax.experimental.jax2tf.jax2tf.slices->tuple(map(slice, _eval_shape(start_indices), _eval_shape(limit_indices), _eval_shape(strides)))
A:jax.experimental.jax2tf.jax2tf.(update_jaxpr, update_consts)->jax._src.lax.lax._reduction_jaxpr(_scatter_reduction_computation, core.ShapedArray((), operand.dtype.as_numpy_dtype))
A:jax.experimental.jax2tf.jax2tf.clip_fn->_convert_jax_impl(lax_slicing._clamp_scatter_indices, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.scatter_indices->clip_fn(operand, scatter_indices, updates, dnums=dimension_numbers, _in_avals=_in_avals, _out_aval=_in_avals[1])
A:jax.experimental.jax2tf.jax2tf.xla_update_computation->tensorflow.function(update_computation, autograph=False).get_concrete_function(o_spec, o_spec)
A:jax.experimental.jax2tf.jax2tf.branches_tf->list(map(source_info_util.extend_name_stack('cond'), branches_tf))
A:jax.experimental.jax2tf.jax2tf.(cond_consts, body_consts, init_carry)->jax._src.util.split_list(args, [cond_nconsts, body_nconsts])
A:jax.experimental.jax2tf.jax2tf.(pred,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *args, extra_name_stack='while/cond')
A:jax.experimental.jax2tf.jax2tf.body_tf_func->partial(_interpret_jaxpr, body_jaxpr, *body_consts, extra_name_stack='while/body')
A:jax.experimental.jax2tf.jax2tf.(init_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *init_carry, extra_name_stack='while/body_pred')
A:jax.experimental.jax2tf.jax2tf.pred->tensorflow.reduce_any(pred_b, axis=list(range(len(pred_b.shape))))
A:jax.experimental.jax2tf.jax2tf.pred_b_bcast->_broadcast_in_dim(pred_b, shape=_jax_physical_aval(c_aval).shape, broadcast_dimensions=list(range(len(pred_b.shape))), _in_avals=cond_jaxpr.out_avals, _out_aval=core.ShapedArray(c_aval.shape, np.bool_))
A:jax.experimental.jax2tf.jax2tf.(next_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *selected_carry, extra_name_stack='body_pred')
A:jax.experimental.jax2tf.jax2tf.(_, *res_carry)->tensorflow.while_loop(new_cond_tf_func, new_body_tf_func, (init_pred_b, *init_carry))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.scan_p]->_convert_jax_impl(lax_control_flow._scan_impl, extra_name_stack='scan')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[ad_checkpoint.remat_p]->_convert_jax_impl(partial(ad_checkpoint.remat_lowering, is_gpu_platform=False), multiple_results=True, extra_name_stack='checkpoint')
A:jax.experimental.jax2tf.jax2tf.k_tf->tensorflow.cast(k_tf, tf.int32)
A:jax.experimental.jax2tf.jax2tf.(values, indices)->tensorflow.math.top_k(tf.dtypes.cast(operand, conversion_dtype), k=k_tf, sorted=True)
A:jax.experimental.jax2tf.jax2tf.o_aval->jax._src.core.ShapedArray((), _to_jax_dtype(op.dtype))
A:jax.experimental.jax2tf.jax2tf.xla_comparator_computation->tensorflow.function(lexicographic_comparator, autograph=False).get_concrete_function(*comparator_spec)
A:jax.experimental.jax2tf.jax2tf.(FFT, IFFT, RFFT, IRFFT)->list(map(xla_client.FftType, [0, 1, 2, 3]))
A:jax.experimental.jax2tf.jax2tf.tf_func->partial(tf_func, fft_length=_eval_shape(fft_lengths))
A:jax.experimental.jax2tf.jax2tf.result->tensorflow.transpose(result, transpose_dimensions)
A:jax.experimental.jax2tf.jax2tf.(wH, vl)->tensorflow.linalg.eig(tf.linalg.adjoint(operand))
A:jax.experimental.jax2tf.jax2tf.wHH->tensorflow.math.conj(wH)
A:jax.experimental.jax2tf.jax2tf.(w, v)->tensorflow.linalg.eigh(operand)
A:jax.experimental.jax2tf.jax2tf.cast_type->{tf.complex64: tf.float32, tf.complex128: tf.float64}.get(operand.dtype)
A:jax.experimental.jax2tf.jax2tf.w->tensorflow.cast(w, cast_type)
A:jax.experimental.jax2tf.jax2tf.a_shape->_eval_shape(a_aval.shape)
A:jax.experimental.jax2tf.jax2tf.rank->len(a.shape)
A:jax.experimental.jax2tf.jax2tf.num_partition_splits->math.prod(partition_dimensions)
A:jax.experimental.jax2tf.jax2tf.tile_assignment->numpy.arange(num_partition_splits).reshape(partition_dimensions)
A:jax.experimental.jax2tf.jax2tf.tad->list(np.arange(math.prod(sharding_proto.tile_assignment_dimensions)).reshape(sharding_proto.iota_reshape_dims).transpose(sharding_proto.iota_transpose_perm).flat)
A:jax.experimental.jax2tf.jax2tf.(dim_tf,)->_eval_shape((dim,))
A:jax.experimental.jax2tf.jax2tf.m->tensorflow.Module()
A:jax.experimental.jax2tf.jax2tf.tuple_wrapper->type(m.a)
A:jax.experimental.jax2tf.jax2tf.list_wrapper->type(m.b)
A:jax.experimental.jax2tf.jax2tf.dict_wrapper->type(m.c)
jax.experimental.jax2tf.convert(fun_jax:Callable,*,polymorphic_shapes:Optional[str]=None,with_gradient:bool=True,enable_xla:bool=True,native_serialization:Union[bool,_DefaultNativeSerialization]=DEFAULT_NATIVE_SERIALIZATION,native_serialization_platforms:Optional[Sequence[str]]=None,native_serialization_disabled_checks:Sequence[DisabledSafetyCheck]=())->Callable
jax.experimental.jax2tf.dtype_of_val(val:TfVal)->DType
jax.experimental.jax2tf.eval_polymorphic_shape(fun_jax:Callable,*,polymorphic_shapes=None)->Callable
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl(self,fun_jax,*,args_specs,kwargs_specs,args_flat_tf:Sequence[TfVal],enable_xla:bool)
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl.__init__(self,fun_jax,*,args_specs,kwargs_specs,args_flat_tf:Sequence[TfVal],enable_xla:bool)
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl.after_conversion(self)
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl.before_conversion(self)
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl.get_vjp_fun(self)->tuple[Callable, Sequence[core.AbstractValue]]
jax.experimental.jax2tf.jax2tf.GraphSerializationImpl.run_fun_tf(self,args_flat_tf:Sequence[TfVal])->tuple[Sequence[TfVal], Sequence[core.ShapedArray], tree_util.PyTreeDef]
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl(self,fun_jax,*,args_specs,kwargs_specs,native_serialization_platforms:Optional[Sequence[str]],native_serialization_disabled_checks:Sequence[DisabledSafetyCheck])
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl.__init__(self,fun_jax,*,args_specs,kwargs_specs,native_serialization_platforms:Optional[Sequence[str]],native_serialization_disabled_checks:Sequence[DisabledSafetyCheck])
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl.after_conversion(self)
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl.before_conversion(self)
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl.get_vjp_fun(self)->tuple[Callable, Sequence[core.AbstractValue]]
jax.experimental.jax2tf.jax2tf.NativeSerializationImpl.run_fun_tf(self,args_flat_tf:Sequence[TfVal])->tuple[Sequence[TfVal], Sequence[core.ShapedArray], tree_util.PyTreeDef]
jax.experimental.jax2tf.jax2tf.SerializationImpl
jax.experimental.jax2tf.jax2tf.SerializationImpl.after_conversion(self)
jax.experimental.jax2tf.jax2tf.SerializationImpl.before_conversion(self)
jax.experimental.jax2tf.jax2tf.SerializationImpl.get_vjp_fun(self)->tuple[Callable, Sequence[core.AbstractValue]]
jax.experimental.jax2tf.jax2tf.SerializationImpl.run_fun_tf(self,args_flat_tf:Sequence[TfVal])->tuple[Sequence[TfVal], Sequence[core.ShapedArray], tree_util.PyTreeDef]
jax.experimental.jax2tf.jax2tf.TensorFlowTrace(core.Trace)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.get_primitive_impl(self,p:core.Primitive)->tuple[Callable, bool]
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.lift(self,val:core.Tracer)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_call(self,call_primitive:core.Primitive,out_tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_vjp_call_fwd(self,*_,**__)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_call(self,call_primitive:core.Primitive,fun:lu.WrappedFun,tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_map(self,map_primitive,f,tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_primitive(self,primitive:core.Primitive,tracers:Sequence[TensorFlowTracer],params)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.pure(self,val:TfVal)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.sublift(self,val:TensorFlowTracer)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTracer(self,trace:'TensorFlowTrace',val:TfVal,aval:core.AbstractValue)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.__init__(self,trace:'TensorFlowTrace',val:TfVal,aval:core.AbstractValue)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.aval(self)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.full_lower(self)
jax.experimental.jax2tf.jax2tf._DefaultNativeSerialization
jax.experimental.jax2tf.jax2tf._ThreadLocalState(self)
jax.experimental.jax2tf.jax2tf._ThreadLocalState.__init__(self)
jax.experimental.jax2tf.jax2tf._abs(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._add(x:TfVal,y:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._approx_top_k(operand:TfVal,k:int,reduction_dimension:int,recall_target:float,is_max_k:bool,reduction_input_size_override:int,aggregate_to_topk:bool)->tuple[TfVal, TfVal]
jax.experimental.jax2tf.jax2tf._argminmax(is_min:bool,operand:TfVal,axes:Sequence[int],index_dtype:DType,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._assert_matching_abstract_shape(x:TfVal,shape:Sequence[shape_poly.DimSize])
jax.experimental.jax2tf.jax2tf._atan2(y,x,**kwargs)
jax.experimental.jax2tf.jax2tf._aval_to_tf_shape(aval:core.ShapedArray)->tuple[Optional[int], ...]
jax.experimental.jax2tf.jax2tf._batched_cond_while(*args:TfVal,cond_nconsts:int,cond_jaxpr:core.ClosedJaxpr,body_nconsts:int,body_jaxpr:core.ClosedJaxpr)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._bitcast_convert_type(operand,new_dtype)
jax.experimental.jax2tf.jax2tf._broadcast_in_dim(operand,*,shape,broadcast_dimensions,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._call_wrapped_with_new_constant_cache(fun:lu.WrappedFun,in_vals:Sequence[TfVal],fresh_constant_cache:bool=False)->Sequence[tuple[TfVal, core.ShapedArray]]
jax.experimental.jax2tf.jax2tf._cbrt(x)
jax.experimental.jax2tf.jax2tf._clamp(minval,operand,maxval,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._common_reduce_window(operand,init_val,reducer,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._concatenate(*operands,dimension)
jax.experimental.jax2tf.jax2tf._cond(index:TfVal,*operands:TfVal,branches:Sequence[core.ClosedJaxpr],linear:Sequence[bool])->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._conj(x,**kwargs)
jax.experimental.jax2tf.jax2tf._conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:lax.ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,precision:Optional[tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._conv_general_dimension_numbers_proto(dimension_numbers)
jax.experimental.jax2tf.jax2tf._convert_element_type(operand,*,new_dtype,weak_type=False)
jax.experimental.jax2tf.jax2tf._convert_jax_impl(impl_jax:Callable,*,multiple_results=True,with_physical_avals=False,extra_name_stack:Optional[str]=None)->Callable
jax.experimental.jax2tf.jax2tf._cumred(lax_reduce_fn:Callable,lax_reduce_window_fn:Callable,extra_name_stack:str)
jax.experimental.jax2tf.jax2tf._custom_jvp_call(*args:TfVal,call_jaxpr:core.ClosedJaxpr,jvp_jaxpr_thunk:Callable,num_consts:int)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._custom_lin(*args:TfVal,**_)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._custom_vjp_call_jaxpr(*args:TfVal,fun_jaxpr:core.ClosedJaxpr,**_)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._dim_as_value_jax2tf(dim:shape_poly.DimSize)
jax.experimental.jax2tf.jax2tf._dimension_size_jax2tf(op:TfVal,*,dimension,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._div(lhs,rhs)
jax.experimental.jax2tf.jax2tf._dot_general(lhs,rhs,*,dimension_numbers,precision:Optional[tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._dot_general_convert_to_common_dtype(lhs:TfVal,lhs_aval:core.ShapedArray,rhs:TfVal,rhs_aval:core.ShapedArray,out_aval:core.ShapedArray)->Tuple[TfVal, TfVal, Callable[[TfVal], TfVal]]
jax.experimental.jax2tf.jax2tf._dynamic_slice(operand,*start_indices,slice_sizes:core.Shape,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._dynamic_update_slice(operand,update,*start_indices,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._eig(operand:TfVal,compute_left_eigenvectors:bool,compute_right_eigenvectors:bool)
jax.experimental.jax2tf.jax2tf._eigh(operand:TfVal,lower:bool,sort_eigenvalues:bool,subset_by_index:tuple,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._empty(*,dtype)
jax.experimental.jax2tf.jax2tf._ensure_tf_shape_if_dynamic(x:TfVal,shape)
jax.experimental.jax2tf.jax2tf._eval_shape(shape:Sequence[shape_poly.DimSize],dtype=None)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._extended_name_stack(extra_name_stack:Optional[str])
jax.experimental.jax2tf.jax2tf._fft(x,*,fft_type,fft_lengths,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._gather(operand,start_indices,*,dimension_numbers,slice_sizes:core.Shape,indices_are_sorted,unique_indices,mode,fill_value,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._get_current_name_stack()->Union[NameStack, str]
jax.experimental.jax2tf.jax2tf._get_max_identity(tf_dtype)
jax.experimental.jax2tf.jax2tf._get_min_identity(tf_dtype)
jax.experimental.jax2tf.jax2tf._integer_pow(x,*,y:int,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._interpret_fun_jax(fun_jax:Callable,args_tf:Sequence[TfVal],args_avals:Sequence[core.ShapedArray],extra_name_stack:Optional[str],fresh_constant_cache:bool=False)->tuple[tuple[TfVal, ...], tuple[core.ShapedArray, ...]]
jax.experimental.jax2tf.jax2tf._interpret_jaxpr(jaxpr:core.ClosedJaxpr,*args_tf:TfVal,extra_name_stack:Optional[str],fresh_constant_cache:bool=True)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._interpret_subtrace(main:core.MainTrace,in_avals:Sequence[core.ShapedArray],*in_vals:TfVal)
jax.experimental.jax2tf.jax2tf._iota(*,dtype,shape,dimension)
jax.experimental.jax2tf.jax2tf._iota_2x32_shape(*,shape)
jax.experimental.jax2tf.jax2tf._is_tfval(v:TfVal)->bool
jax.experimental.jax2tf.jax2tf._jax_physical_aval(aval:core.ShapedArray)->core.ShapedArray
jax.experimental.jax2tf.jax2tf._jax_physical_dtype(dtype)
jax.experimental.jax2tf.jax2tf._linear_solve(*args:TfVal,const_lengths,jaxprs,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._lu(operand:TfVal,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._make_custom_gradient_fn_tf(fun_jax,*,impl:SerializationImpl,with_gradient:bool,args_specs,kwargs_specs,args_tf:Sequence[TfVal],outs_avals:Sequence[core.ShapedArray],outs_tf:Sequence[TfVal])
jax.experimental.jax2tf.jax2tf._make_op_metadata(primitive:core.Primitive,params:dict,*,source_info:source_info_util.SourceInfo)->xla_client.OpMetadata
jax.experimental.jax2tf.jax2tf._maybe_cast_to_int64(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._minmax(x:TfVal,y:TfVal,*,is_min:bool,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)->TfVal
jax.experimental.jax2tf.jax2tf._minmax_scalar(x:TfVal,y:TfVal,*,is_min:bool)->TfVal
jax.experimental.jax2tf.jax2tf._neg(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._not(x)
jax.experimental.jax2tf.jax2tf._pad(operand,padding_value,*,padding_config,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._pjit(*args:TfVal,jaxpr:core.ClosedJaxpr,in_shardings:Sequence[sharding.XLACompatibleSharding],out_shardings:Sequence[sharding.XLACompatibleSharding],resource_env:maps.ResourceEnv,donated_invars,name:str,keep_unused:bool,inline:bool,_in_avals:Sequence[core.ShapedArray],_out_aval:Sequence[core.ShapedArray])->TfVal
jax.experimental.jax2tf.jax2tf._pjit_sharding_constraint(arg:TfVal,*,sharding:sharding.NamedSharding,resource_env:maps.ResourceEnv,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray,**kwargs)->TfVal
jax.experimental.jax2tf.jax2tf._population_count(x)
jax.experimental.jax2tf.jax2tf._pow(x:TfVal,y:TfVal,*,_in_avals,_out_aval)->TfVal
jax.experimental.jax2tf.jax2tf._precision__config_module_proto(precision:Optional[tuple[PrecisionType,PrecisionType]])
jax.experimental.jax2tf.jax2tf._precision_config_proto(precision:Optional[tuple[PrecisionType,PrecisionType]])
jax.experimental.jax2tf.jax2tf._qr(operand,full_matrices)
jax.experimental.jax2tf.jax2tf._random_bits_impl(keys:TfVal,*,bit_width,shape,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_fold_in_impl(keys:TfVal,msgs:TfVal,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_seed_impl(seeds:TfVal,*,impl,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_split_impl(keys:TfVal,*,shape,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_unwrap_impl(keys:TfVal,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_wrap_impl(base_arr:TfVal,*,impl,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._reduce(*operands:TfVal,computation:Callable,jaxpr:core.Jaxpr,consts:Sequence[Any],dimensions:Sequence[int],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._reduce_precision(x,*,exponent_bits,mantissa_bits)
jax.experimental.jax2tf.jax2tf._reduce_window(*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._register_checkpoint_pytrees()
jax.experimental.jax2tf.jax2tf._rem(lhs,rhs)
jax.experimental.jax2tf.jax2tf._reshape(operand,*,new_sizes,dimensions,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._rev(operand,*,dimensions)
jax.experimental.jax2tf.jax2tf._rng_bit_generator(key:TfVal,*,shape,dtype,algorithm)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._rng_uniform(minval:TfVal,maxval:TfVal,*,shape)->TfVal
jax.experimental.jax2tf.jax2tf._round(operand,*,rounding_method,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._run_exported_as_tf(args_flat_tf:Sequence[TfVal],exported:export.Exported)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._sanitize_scope_name(name)
jax.experimental.jax2tf.jax2tf._scatter(operand,scatter_indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._select_and_gather_add(tangents:TfVal,operand:TfVal,select_prim:core.Primitive,window_dimensions:Sequence[int],window_strides:Sequence[int],base_dilation:Sequence[int],window_dilation:Sequence[int],padding:Sequence[tuple[int,int]],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._select_and_scatter(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.experimental.jax2tf.jax2tf._select_and_scatter_add(source,operand,*,select_prim,window_dimensions,window_strides,padding,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._shape_assertion_jax2tf(assert_what,*error_message_inputs,error_message:str)
jax.experimental.jax2tf.jax2tf._shard_value(val:TfVal,aval:core.ShapedArray,sd:sharding.XLACompatibleSharding,*,skip_replicated_sharding:bool)->TfVal
jax.experimental.jax2tf.jax2tf._shift_in_bounds(x:TfVal,y:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._shift_left(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_arithmetic(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_arithmetic_raw(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_logical(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_logical_raw(x,y)
jax.experimental.jax2tf.jax2tf._sign(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._slice(operand,start_indices,limit_indices,strides,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._sort(*operands:TfVal,dimension:int,is_stable:bool,num_keys:int)->tuple[TfVal, ...]
jax.experimental.jax2tf.jax2tf._specialized_reduce_window(reducer,identity,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval,name=None)
jax.experimental.jax2tf.jax2tf._squeeze(operand,*,dimensions,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._svd(operand,full_matrices,compute_uv)
jax.experimental.jax2tf.jax2tf._tfval_to_tensor_jax_dtype(val:TfVal,jax_dtype:Optional[DType]=None,memoize_constants=False)->tuple[TfVal, DType]
jax.experimental.jax2tf.jax2tf._threefry2x32_jax_impl(*args:TfVal,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._to_jax_dtype(tf_dtype)
jax.experimental.jax2tf.jax2tf._to_tf_dtype(jax_dtype)
jax.experimental.jax2tf.jax2tf._top_k(operand:TfVal,k:int)->tuple[TfVal, TfVal]
jax.experimental.jax2tf.jax2tf._total_order_adjustment(x)
jax.experimental.jax2tf.jax2tf._total_order_cond(cond,x,y)
jax.experimental.jax2tf.jax2tf._total_order_equal(x,y)
jax.experimental.jax2tf.jax2tf._transpose(operand,*,permutation)
jax.experimental.jax2tf.jax2tf._triangular_solve(a:TfVal,b:TfVal,*,left_side:bool,lower:bool,transpose_a:bool,conjugate_a:bool,unit_diagonal:bool,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._tridiagonal_solve(*args:TfVal,_in_avals,_out_aval,**params)
jax.experimental.jax2tf.jax2tf._unexpected_primitive(p:core.Primitive,*args,**kwargs)
jax.experimental.jax2tf.jax2tf._where(which,*cases)
jax.experimental.jax2tf.jax2tf._while(*args:TfVal,cond_nconsts:int,cond_jaxpr:core.ClosedJaxpr,body_nconsts:int,body_jaxpr:core.ClosedJaxpr)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf.convert(fun_jax:Callable,*,polymorphic_shapes:Optional[str]=None,with_gradient:bool=True,enable_xla:bool=True,native_serialization:Union[bool,_DefaultNativeSerialization]=DEFAULT_NATIVE_SERIALIZATION,native_serialization_platforms:Optional[Sequence[str]]=None,native_serialization_disabled_checks:Sequence[DisabledSafetyCheck]=())->Callable
jax.experimental.jax2tf.jax2tf.dtype_of_val(val:TfVal)->DType
jax.experimental.jax2tf.jax2tf.eval_polymorphic_shape(fun_jax:Callable,*,polymorphic_shapes=None)->Callable
jax.experimental.jax2tf.jax2tf.flatten_fun_jax(fun_jax:Callable,in_tree)->tuple[Callable, Callable]
jax.experimental.jax2tf.jax2tf.get_thread_local_state_call_tf_concrete_function_list()->Optional[list[Any]]
jax.experimental.jax2tf.jax2tf.handle_boolean_args(f,argnums:Sequence[int],boolean_f=None)
jax.experimental.jax2tf.jax2tf.inside_call_tf()
jax.experimental.jax2tf.jax2tf.preprocess_arg_tf(arg_idx:int,arg_tf:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf.split_to_logical_devices(tensor:TfVal,partition_dimensions:PartitionsOrReplicated)
jax.experimental.jax2tf.split_to_logical_devices(tensor:TfVal,partition_dimensions:PartitionsOrReplicated)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/back_compat_test_util.py----------------------------------------
A:jax.experimental.jax2tf.tests.back_compat_test_util.dummy_data_dict->dict(testdata_version=CURRENT_TESTDATA_VERSION, platform='cpu', custom_call_targets=[], serialized_date=datetime.date(2023, 3, 15), inputs=(array(0.0, dtype=float32),), expected_outputs=(array(0.0, dtype=float32),), mlir_module_text='\n  module @jit_sin {\n  func.func public @main(%arg0: tensor<f32>) -> tensor<f32> {\n    %0 = stablehlo.sine %arg0 : tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n', mlir_module_serialized=b"ML\xefR\x03MLIRxxx-trunk\x00\x01\x17\x05\x01\x05\x01\x03\x05\x03\x07\x07\t\x0b\x03K5\x07\x01\x1b\x07\x0b\x13\x0b3\x0b\x0b\x0b\x0b\x0f\x0b\x13\x0b\x03\x1b\x0f\x1b\x0b\x0b\x0b\x0b\x0b\x0f\x13\x0b\x0b\x0b\x0b\x03\x07\x0f\x17\x07\x02\xa7\x1f\x05\r\x03\x03\x03\x07\x05\x0f\x03\x0b\x0b\x1b\r'\x0f)\x031\x113\x05\x11\x05\x13\x05\x15\x05\x17\x1d\x15\x17\x05\x19\x17\x19\xef\x01\x05\x1b\x03\x03\x1d\r\x05\x1f!#%\x1d\x1d\x1d\x1f\x1d!\x1d##\x03\x03\x03+\r\x03-/\x1d%\x1d'\x1d)\x1d+)\x01\x05\x11\x03\x01\x03\x01\t\x04A\x05\x01\x11\x01\x05\x07\x03\x01\x05\x03\x11\x01\t\x05\x03\x05\x0b\x03\x01\x01\x05\x06\x13\x03\x01\x03\x01\x07\x04\x01\x03\x03\x06\x03\x01\x05\x01\x00\x9a\x04-\x0f\x0b\x03!\x1b\x1d\x05\x1b\x83/\x1f\x15\x1d\x15\x11\x13\x15\x11\x11\x0f\x0b\x11builtin\x00vhlo\x00module\x00func_v1\x00sine_v1\x00return_v1\x00sym_name\x00jit_sin\x00arg_attrs\x00function_type\x00res_attrs\x00sym_visibility\x00jit(sin)/jit(main)/sin\x00third_party/py/jax/experimental/jax2tf/tests/back_compat_test.py\x00jax.arg_info\x00x\x00mhlo.sharding\x00{replicated}\x00jax.result_info\x00\x00main\x00public\x00", xla_call_module_version=4)
A:jax.experimental.jax2tf.tests.back_compat_test_util.res_run_current->tuple((np.array(a) for a in res_run_current))
A:jax.experimental.jax2tf.tests.back_compat_test_util.(serialized, module_str, module_version)->self.serialize(func, data, polymorphic_shapes=polymorphic_shapes, allow_unstable_custom_call_targets=allow_unstable_custom_call_targets)
A:jax.experimental.jax2tf.tests.back_compat_test_util.current_custom_call_targets->sorted(list(set(re.findall(custom_call_re, module_str))))
A:jax.experimental.jax2tf.tests.back_compat_test_util.updated_testdata->re.sub('google.', 'googlex', updated_testdata)
A:jax.experimental.jax2tf.tests.back_compat_test_util.output_dir->os.getenv('TEST_UNDECLARED_OUTPUTS_DIR', '/tmp/back_compat_testdata')
A:jax.experimental.jax2tf.tests.back_compat_test_util.output_file->os.path.join(output_dir, f'{self._testMethodName}.py')
A:jax.experimental.jax2tf.tests.back_compat_test_util.res_run_serialized->self.run_serialized(data, polymorphic_shapes=polymorphic_shapes)
A:jax.experimental.jax2tf.tests.back_compat_test_util.args_specs->jax.experimental.export.export.poly_specs(data.inputs, polymorphic_shapes)
A:jax.experimental.jax2tf.tests.back_compat_test_util.exported->jax.experimental.export.export.Exported(fun_name='run_serialized', in_tree=in_tree, in_avals=tuple(in_avals), out_tree=out_tree, out_avals=tuple(out_avals), in_shardings=(pxla.UNSPECIFIED,) * len(in_avals), out_shardings=(pxla.UNSPECIFIED,) * len(out_avals), lowering_platforms=(data.platform,), ordered_effects=(), unordered_effects=(), disabled_checks=(), mlir_module_serialized=data.mlir_module_serialized, serialization_version=data.xla_call_module_version, module_kept_var_idx=tuple(range(len(in_avals))), uses_shape_polymorphism=any((not core.is_constant_shape(a.shape) for a in in_avals)), _get_vjp=_get_vjp)
A:jax.experimental.jax2tf.tests.back_compat_test_util.module_str->str(exported.mlir_module())
A:jax.experimental.jax2tf.tests.back_compat_test_util.in_avals_tree->jax.tree_util.tree_map(ndarray_to_aval, args_specs)
A:jax.experimental.jax2tf.tests.back_compat_test_util.out_avals_tree->jax.tree_util.tree_map(ndarray_to_aval, data.expected_outputs)
A:jax.experimental.jax2tf.tests.back_compat_test_util.(in_avals, in_tree)->jax.tree_util.tree_flatten((in_avals_tree, {}))
A:jax.experimental.jax2tf.tests.back_compat_test_util.(out_avals, out_tree)->jax.tree_util.tree_flatten(out_avals_tree)
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.default_jax_backend(self)->str
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.load_testdata(self,testdata_dict:dict[str,Any])->CompatTestData
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.load_testdata_nested(self,testdata_nest)->Iterable[CompatTestData]
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.run_current(self,func:Callable,data:CompatTestData)
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.run_one_test(self,func:Callable[...,jax.Array],data:CompatTestData,polymorphic_shapes:Optional[Sequence[str]]=None,rtol:Optional[float]=None,atol:Optional[float]=None,allow_unstable_custom_call_targets:Sequence[str]=(),check_results:Optional[Callable[...,None]]=None,expect_current_custom_calls:Optional[Sequence[str]]=None)
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.run_serialized(self,data:CompatTestData,polymorphic_shapes:Optional[Sequence[str]]=None)
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.serialize(self,func:Callable,data:CompatTestData,*,polymorphic_shapes:Optional[Sequence[str]]=None,allow_unstable_custom_call_targets:Sequence[str]=())->tuple[bytes, str, int]
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestBase.starter_data(self,inputs:Sequence[np.ndarray])->CompatTestData
jax.experimental.jax2tf.tests.back_compat_test_util.CompatTestData


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax2tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf->jax.experimental.jax2tf.convert(f_jax, native_serialization=True, native_serialization_platforms=('cpu', 'cuda', 'tpu'))
A:jax.experimental.jax2tf.tests.jax2tf_test.x->numpy.float32(0.42)
A:jax.experimental.jax2tf.tests.jax2tf_test.sin_x->numpy.sin(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.xs->numpy.ones((8, const_size), dtype=np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.y->tensorflow.compat.v1.placeholder(tf.dtypes.float32, (None, 5), 'y')
A:jax.experimental.jax2tf.tests.jax2tf_test.cf->f_tf(inputs).get_concrete_function([1.0, 2.0, 3.0], 4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.v->tensorflow.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))
A:jax.experimental.jax2tf.tests.jax2tf_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.jax2tf_test.f_conc->tensorflow.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))
A:jax.experimental.jax2tf.tests.jax2tf_test.n->jax.local_device_count()
A:jax.experimental.jax2tf.tests.jax2tf_test.result->jax.lax.while_loop(condition, body, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.big_const->numpy.full((5,), 2 ** 33, dtype=dtype)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_conv->tensorflow.function(f_conv, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test._->func_to_convert(*args)
A:jax.experimental.jax2tf.tests.jax2tf_test.default_float_type->jax.experimental.jax2tf.dtype_of_val(4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.(u, v)->f_tf(x, y)
A:jax.experimental.jax2tf.tests.jax2tf_test.dy_dx->t1.gradient(y, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.d2y_dx2->t2.gradient(dy_dx, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.default_float_dtype->jax.experimental.jax2tf.dtype_of_val(4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.uv->f_tf((x, y))
A:jax.experimental.jax2tf.tests.jax2tf_test.res_jax->f_jax(x, y_unused)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf->f_tf(x_v)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf_2->tensorflow.function(f_tf, autograph=False, jit_compile=True)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.my_model->tensorflow.Module()
A:jax.experimental.jax2tf.tests.jax2tf_test.my_model.f->tensorflow.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])
A:jax.experimental.jax2tf.tests.jax2tf_test.model_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))
A:jax.experimental.jax2tf.tests.jax2tf_test.restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf_3->restored_f(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_jax->jax.grad(f_jax)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.res->f_tf(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_tf->tape.gradient(res, xv)
A:jax.experimental.jax2tf.tests.jax2tf_test.inputs->numpy.ones(10, dtype=np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.u->f_tf(inputs)
A:jax.experimental.jax2tf.tests.jax2tf_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_g->jax.grad(g, allow_int=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.d_dx_jax->grad_g(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.d_dx_tf->jax.experimental.jax2tf.convert(grad_g)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_tf_native->tape.gradient(res, xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_tf_native_0->tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.conv_fn->tensorflow.function(conv_fn, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_jax2tf->tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.state->dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))
A:jax.experimental.jax2tf.tests.jax2tf_test.(vjp_jax_fun, args_vjp)->jax.experimental.jax2tf.tests.tf_test_util.TransformJaxVJP(jax_f, args, res_jax)
A:jax.experimental.jax2tf.tests.jax2tf_test.(grad_jax,)->vjp_jax_fun(*args_vjp)
A:jax.experimental.jax2tf.tests.jax2tf_test.what_keys->set(what.keys())
A:jax.experimental.jax2tf.tests.jax2tf_test.expected_keys->set(expected.keys())
A:jax.experimental.jax2tf.tests.jax2tf_test.e->numpy.ones_like(w)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_0,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_None,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_jax->tensorflow.function(f_tf_jax, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_jax_0,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_jax_None,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_vjp_jax_fun->jax.experimental.jax2tf.convert(vjp_jax_fun)
A:jax.experimental.jax2tf.tests.jax2tf_test.(grad_tf_vjp_jax,)->tf_vjp_jax_fun(*args_vjp)
A:jax.experimental.jax2tf.tests.jax2tf_test.xv->tensorflow.nest.map_structure(tf.Variable, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.m->tensorflow.Module()
A:jax.experimental.jax2tf.tests.jax2tf_test.net->JaxModule()
A:jax.experimental.jax2tf.tests.jax2tf_test.images->tensorflow.ones([1, 784])
A:jax.experimental.jax2tf.tests.jax2tf_test.loss->tensorflow.reduce_sum(net(images))
A:jax.experimental.jax2tf.tests.jax2tf_test.params->tape.watched_variables()
A:jax.experimental.jax2tf.tests.jax2tf_test.grads->tape.gradient(loss, params)
A:jax.experimental.jax2tf.tests.jax2tf_test.x2->jax.numpy.sin(x1)
A:jax.experimental.jax2tf.tests.jax2tf_test.x3->jax.numpy.sin(x2)
A:jax.experimental.jax2tf.tests.jax2tf_test.x4->jax.numpy.sin(x3)
A:jax.experimental.jax2tf.tests.jax2tf_test.remat_f->jax.ad_checkpoint.checkpoint(f)
A:jax.experimental.jax2tf.tests.jax2tf_test.arg->numpy.array(3.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_hlo->self.TfToHlo(f_tf, arg)
A:jax.experimental.jax2tf.tests.jax2tf_test.(grad_tf_x, grad_tf_y)->tape.gradient(res_tf, (x, y_unused))
A:jax.experimental.jax2tf.tests.jax2tf_test.(res_tf, grad_tf_x, grad_tf_y)->grad_tf(x_tf, y_unused_tf)
A:jax.experimental.jax2tf.tests.jax2tf_test.sin_1->jax.experimental.jax2tf.convert(jnp.sin)(1.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.out->jax.experimental.jax2tf.convert(caller_jax, with_gradient=False)(2.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.graph_def->tensorflow.function(f_grad_tf, autograph=False).get_concrete_function(x).graph.as_graph_def()
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn_scalar->jax.experimental.jax2tf.convert(jax_fn_scalar)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn_array->jax.experimental.jax2tf.convert(jax_fn_array)
A:jax.experimental.jax2tf.tests.jax2tf_test.const->numpy.random.uniform(size=(16, 16)).astype(np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_consts->self.FindLargeTfConstants(jax2tf.convert(f), const)
A:jax.experimental.jax2tf.tests.jax2tf_test.f1_consts->self.FindLargeTfConstants(jax2tf.convert(f1), xs, at_least=const_size)
A:jax.experimental.jax2tf.tests.jax2tf_test.f2_consts->self.FindLargeTfConstants(jax2tf.convert(f2), xs, at_least=const_size)
A:jax.experimental.jax2tf.tests.jax2tf_test.(res, _)->jax.lax.scan(lambda carry, x: (carry + x + const, None), jnp.zeros((const_size,), dtype=np.float32), xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_graph_consts->self.FindLargeTfConstants(jax2tf.convert(f), const)
A:jax.experimental.jax2tf.tests.jax2tf_test.key->jax.random.PRNGKey(0).reshape(1).squeeze()
A:jax.experimental.jax2tf.tests.jax2tf_test.mul->jax.jit(jnp.multiply)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn->jax.experimental.jax2tf.convert(lambda x: mul(x, 2.0))
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun_with_xla->jax.experimental.jax2tf.convert(fun, enable_xla=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun_without_xla->jax.experimental.jax2tf.convert(fun, enable_xla=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun2_without_xla->jax.experimental.jax2tf.convert(lambda x: fun(x), enable_xla=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun2_with_xla->jax.experimental.jax2tf.convert(lambda x: fun(x), enable_xla=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.user_frame->jax._src.source_info_util.user_frame(source_info_util.current())
A:jax.experimental.jax2tf.tests.jax2tf_test.z->jax.named_call(f_callee, name='callee')(y)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, acc)->jax.lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))
A:jax.experimental.jax2tf.tests.jax2tf_test.new_carry->jax.numpy.sin(carry)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, carry)->jax.lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_comp->jax.xla_computation(f_while)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.backend->jax._src.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.jax2tf_test.modules->jax._src.xla_bridge.get_backend().compile(jax_comp).hlo_modules()
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_opt_hlo->modules[0].to_string()
A:jax.experimental.jax2tf.tests.jax2tf_test.func_tf->jax.experimental.jax2tf.convert(func_jax, polymorphic_shapes='(b,...)', with_gradient=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.g->tensorflow.Graph()
A:jax.experimental.jax2tf.tests.jax2tf_test.g2->tensorflow.Graph()
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_f->tensorflow.function(jax2tf.convert(f), jit_compile=True, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.mesh->jax.sharding.Mesh(jax.devices()[:1], ('a',))
A:jax.experimental.jax2tf.tests.jax2tf_test.transformed1_func->apply_transform(func_shard_map if transform1 == 'shard_map' else func, transform1)
A:jax.experimental.jax2tf.tests.jax2tf_test.transformed2_func->apply_transform(transformed1_func, transform2)
A:jax.experimental.jax2tf.tests.jax2tf_test.exported->jax.experimental.export.export.export(func_to_convert, lowering_platforms=('tpu',))(*(core.ShapedArray(a.shape, a.dtype) for a in args))
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_fun->tensorflow.function(f_tf, jit_compile=True, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_cpus->tensorflow.config.list_logical_devices('CPU')
A:jax.experimental.jax2tf.tests.jax2tf_test.x_v->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_grad_tf_fun->tensorflow.function(f_grad_tf, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.lowered->jax.jit(f_jax).lower(*many_args)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_jax_nested->jax.experimental.jax2tf.call_tf(f_tf)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_nested->jax.experimental.jax2tf.convert(f_jax_nested, native_serialization=True)
A:jax.experimental.jax2tf.tests.jax2tf_test._testing_multi_platform_p->jax._src.core.Primitive('testing_multi_platform')
A:jax.experimental.jax2tf.tests.jax2tf_test._testing_multi_platform_to_add->dict(cpu=2.0, tpu=3.0, cuda=4.0, rocm=5.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.to_add_value->jax._src.interpreters.mlir.broadcast_in_dim(ctx, mlir.ir_constant(np.float32(to_add)), ctx.avals_in[0], broadcast_dimensions=())
A:jax.experimental.jax2tf.tests.jax2tf_test.key_raw->jax.random.key_data(key)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_result->jax.experimental.jax2tf.convert(func)()
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_result->func()
A:jax.experimental.jax2tf.tests.jax2tf_test.global_key->jax.random.PRNGKey(0)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.assertAllOperationStartWith(self,g:tf.Graph,scope_name:str)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.setUpClass(cls)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_64bit_behavior_enable_x64_readme(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_64bit_behavior_not_enable_x64_readme(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_argument_eager_tensor(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_basics(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_constant(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_passed_by_tf(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_returned_by_jax(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_tf_grad(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_checkpoint_name(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_checkpoint_wrapper_types(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_callable_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_tensor_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_nullary_func(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_of_nested_dependent_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_of_nested_independent_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_under_transform_error(self,transform='vmap')
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_under_transform_error_non_tracer(self,transform='vmap')
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_64bit(self,dtype=np.int64,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_jax_arrays(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_cross_platform(self,with_mesh=True,transform1='pjit_in_shardings_P',transform2='pjit_in_shardings_P',nullary=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_cross_platform_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_jvp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_pytree_readme(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_vjp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_device_array_arg(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_effects_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_empty(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_enable_xla(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_function(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_grad_kwargs(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradient_with_float0_intermediate(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradient_with_float0_result(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_disabled(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_int_argument(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_pytree(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_unused_argument_readme(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_jvp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_vjp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_ordered_dict_input(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_higher_order_gradients(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_input_output_naming(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_issue_10586(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_jit_unused(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_jit_unused_grad(self,mode='eager')
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_kwargs(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_multi_platform(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_name_scope(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_name_scope_cond(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_name_scope_polymorphic(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_name_scope_while_loop(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_native_parameters_for_non_native(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_native_serialization_grad(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_convert(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_convert_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_convert_error_non_tracer(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit_is_compiled(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit_pytree(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_batched_while(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_disabled(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_named(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_simple(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_sub_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_while_and_cond(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_pytrees(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_randint(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_readme_gradient_int(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat_free_var(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_randint(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_cond(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_scan(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_sin(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_tuple_args(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_variable_input(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_weak_types(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfVersioningTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfVersioningTest.setUp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfVersioningTest.test_simple(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2tfWithCustomPRNGTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2tfWithCustomPRNGTest.test_key_argument(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2tfWithCustomPRNGTest.test_key_closure(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2tfWithCustomPRNGTest.test_key_from_seed(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/multi_platform_export_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.multi_platform_export_test._known_failures_gpu->make_disjunction_regexp('custom_linear_solve_', 'lu_', 'svd_', 'tridiagonal_solve_')
A:jax.experimental.jax2tf.tests.multi_platform_export_test._skip_cuda_lowering_unless_have_gpus->make_disjunction_regexp('svd_', 'lu_', 'eigh_', 'qr_', 'custom_linear_', 'tridiagonal_solve_', 'random_')
A:jax.experimental.jax2tf.tests.multi_platform_export_test.devices->jax.devices(backend)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.args->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.multi_platform_export_test.unimplemented_platforms->unimplemented_platforms.union(l.devices).union(l.devices)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.exp->jax.experimental.export.export.export(func_jax, lowering_platforms=lowering_platforms)(*args)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.device_args->jax.tree_util.tree_map(lambda x: jax.device_put(x, device), args)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.native_res->func_jax(*device_args)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.exported_res->jax.experimental.export.export.call_exported(exp)(*device_args)
A:jax.experimental.jax2tf.tests.multi_platform_export_test.f->jax.jit(jax.pmap(lambda x: lax.all_gather(x, 'i'), axis_name='i', devices=jax.devices()[:1]))
A:jax.experimental.jax2tf.tests.multi_platform_export_test.x->(x % 2).astype(np.bool_)
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest.export_and_compare_to_native(self,func_jax:Callable,*args:jax.Array,unimplemented_platforms:set[str]=set(),skip_run_on_platforms:set[str]=set())
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest.setUpClass(cls)
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest.test_all_gather(self,*,dtype)
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest.test_prim(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.multi_platform_export_test.PrimitiveTest.test_psum_scatter(self)
jax.experimental.jax2tf.tests.multi_platform_export_test.make_disjunction_regexp(*parts:str)->re.Pattern[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/savedmodel_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.savedmodel_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.savedmodel_test.model->tensorflow.Module()
A:jax.experimental.jax2tf.tests.savedmodel_test.model.f->tensorflow.function(prediction_tf, jit_compile=True, autograph=False)
A:jax.experimental.jax2tf.tests.savedmodel_test.x->numpy.ones((2, 3), dtype=np.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.restored_model->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model, save_gradients=False)
A:jax.experimental.jax2tf.tests.savedmodel_test.primal_out->f_jax(x)
A:jax.experimental.jax2tf.tests.savedmodel_test.xv->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.savedmodel_test.y->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model, save_gradients=False).f(xv)
A:jax.experimental.jax2tf.tests.savedmodel_test._->tensorflow.saved_model.save(model, save_dir, options=options)
A:jax.experimental.jax2tf.tests.savedmodel_test.params_vars->tensorflow.nest.map_structure(tf.Variable, params)
A:jax.experimental.jax2tf.tests.savedmodel_test.model._variables->tensorflow.nest.flatten(params_vars)
A:jax.experimental.jax2tf.tests.savedmodel_test.state->numpy.array([1], dtype=np.int32)
A:jax.experimental.jax2tf.tests.savedmodel_test.params->dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))
A:jax.experimental.jax2tf.tests.savedmodel_test.res_out->tensorflow.zeros((batch_size, 2), dtype=tf.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.(res, state_out)->converted_fun_with_custom_gradient(params, state)
A:jax.experimental.jax2tf.tests.savedmodel_test.params_v->tensorflow.nest.map_structure(tf.Variable, params)
A:jax.experimental.jax2tf.tests.savedmodel_test.loss->tensorflow.reduce_sum(res)
A:jax.experimental.jax2tf.tests.savedmodel_test.g->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.model.fn->tensorflow.function(tf_predict, autograph=False)
A:jax.experimental.jax2tf.tests.savedmodel_test.save_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))
A:jax.experimental.jax2tf.tests.savedmodel_test.options->tensorflow.saved_model.SaveOptions(experimental_custom_gradients=True)
A:jax.experimental.jax2tf.tests.savedmodel_test.restored_module->tensorflow.saved_model.load(save_dir)
A:jax.experimental.jax2tf.tests.savedmodel_test.f_tf->jax.experimental.jax2tf.convert(f_jax)
A:jax.experimental.jax2tf.tests.savedmodel_test.res->restored_f(params_v, x)
A:jax.experimental.jax2tf.tests.savedmodel_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])
A:jax.experimental.jax2tf.tests.savedmodel_test.res_restored->restored_f(*args)
A:jax.experimental.jax2tf.tests.savedmodel_test.res_jax->f_jax(params, x)
A:jax.experimental.jax2tf.tests.savedmodel_test.res_tf->composed_fn(x_str)
A:jax.experimental.jax2tf.tests.savedmodel_test.(restored_f, restored_model)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)
A:jax.experimental.jax2tf.tests.savedmodel_test.g_tf->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.g_restored_f->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.arr->numpy.arange(10, dtype=np.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.numbers_f32->tensorflow.strings.to_number(x_str, out_type=tf.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.numbers_f16->tensorflow.cast(numbers_f32, tf.float16)
A:jax.experimental.jax2tf.tests.savedmodel_test.x_str->numpy.array(['3.14', '2.78'])
A:jax.experimental.jax2tf.tests.savedmodel_test.res_tf_restored->restored_f(x_str)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest._compare_with_saved_model(self,f_jax,*args)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_eval(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient_disabled(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient_nested(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_pytree(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_grad_integers(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_without_embedding_params(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_without_gradients(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_tf_mix_jax_with_uncompilableble(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_xla_context_preserved_gather(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_xla_context_preserved_slice(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/sharding_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.sharding_test.resolver->tensorflow.distribute.cluster_resolver.TPUClusterResolver(tpu='')
A:jax.experimental.jax2tf.tests.sharding_test.topology->tensorflow.tpu.experimental.initialize_tpu_system(resolver)
A:jax.experimental.jax2tf.tests.sharding_test.prev_xla_flags->os.getenv('XLA_FLAGS')
A:jax.experimental.jax2tf.tests.sharding_test.self.devices->numpy.array(jax.devices()[:2])
A:jax.experimental.jax2tf.tests.sharding_test.jax_comp->xmap(lambda a: jnp.concatenate([a, a], axis=0) * 2.0, in_axes={0: 'a', 1: 'b'}, out_axes={0: 'a', 1: 'b'}, axis_resources={'a': 'x', 'b': 'y'}).lower(*args).compiler_ir(dialect='mhlo')
A:jax.experimental.jax2tf.tests.sharding_test.jax_hlo->str(jax_comp)
A:jax.experimental.jax2tf.tests.sharding_test.backend->jax._src.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.sharding_test.device_assignment->numpy.reshape(device_assignment, (-1, num_partitions))
A:jax.experimental.jax2tf.tests.sharding_test.compile_options->jax._src.compiler.get_compile_options(num_replicas=num_replicas, num_partitions=num_partitions, device_assignment=device_assignment, use_spmd_partitioning=use_spmd_partitioning)
A:jax.experimental.jax2tf.tests.sharding_test.jax_optimized_hlo->jax._src.xla_bridge.get_backend().compile(jax_hlo, compile_options).hlo_modules()[0].to_string()
A:jax.experimental.jax2tf.tests.sharding_test.f_tf_fun->tensorflow.function(f_tf, autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.sharding_test.tf_hlo_generator->tensorflow.function(f_tf, autograph=False, jit_compile=True).experimental_get_compiler_ir(*args_tf)
A:jax.experimental.jax2tf.tests.sharding_test.tf_hlo->self.tf_hlo(f_tf, args_tf)
A:jax.experimental.jax2tf.tests.sharding_test.count->len(re.findall(check_re, tf_hlo))
A:jax.experimental.jax2tf.tests.sharding_test.f_jax->xmap(lambda a: jnp.concatenate([a, a], axis=0) * 2.0, in_axes={0: 'a', 1: 'b'}, out_axes={0: 'a', 1: 'b'}, axis_resources={'a': 'x', 'b': 'y'})
A:jax.experimental.jax2tf.tests.sharding_test.x->numpy.arange(np.prod(shape), dtype=np.float32).reshape(shape)
A:jax.experimental.jax2tf.tests.sharding_test.f_converted->jax.experimental.jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=poly)
A:jax.experimental.jax2tf.tests.sharding_test.res_jax->f_jax(a)
A:jax.experimental.jax2tf.tests.sharding_test.res_tf->f_tf(a)
A:jax.experimental.jax2tf.tests.sharding_test.y->jax.lax.with_sharding_constraint(y, constraint_sharding)
A:jax.experimental.jax2tf.tests.sharding_test.x_v->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.sharding_test.const->jax.numpy.full((10, 20), 7, dtype=np.float32)
A:jax.experimental.jax2tf.tests.sharding_test.f_tf->tensorflow.function(jax2tf.convert(f_jax, native_serialization=True), autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.sharding_test.shardings_map->dict(none=None, P=P('x'))
A:jax.experimental.jax2tf.tests.sharding_test.res->f_converted(a)
A:jax.experimental.jax2tf.tests.sharding_test.a->numpy.arange(4 * 4, dtype=np.float32).reshape((4, 4))
A:jax.experimental.jax2tf.tests.sharding_test._->f_jax(a)
A:jax.experimental.jax2tf.tests.sharding_test.devices->numpy.reshape(self.devices, (1, 2))
A:jax.experimental.jax2tf.tests.sharding_test.b->numpy.arange(np.prod(bshape), dtype=np.float32).reshape(bshape)
A:jax.experimental.jax2tf.tests.sharding_test.mesh->Mesh(self.devices, axis_names='x')
A:jax.experimental.jax2tf.tests.sharding_test.(b0, b1)->numpy.split(a, 2, axis=0)
A:jax.experimental.jax2tf.tests.sharding_test.(b00, b01)->numpy.split(b0, 2, axis=1)
A:jax.experimental.jax2tf.tests.sharding_test.(b10, b11)->numpy.split(b1, 2, axis=1)
A:jax.experimental.jax2tf.tests.sharding_test.b0->numpy.concatenate([b00, b10], axis=1)
A:jax.experimental.jax2tf.tests.sharding_test.b1->numpy.concatenate([b01, b11], axis=1)
A:jax.experimental.jax2tf.tests.sharding_test.axis_size->jax.lax.psum(1, 'x')
A:jax.experimental.jax2tf.tests.sharding_test.expected->numpy.concatenate([b0, b1], axis=0)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.GEQ(self,value)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.check_sharding(self,f_tf,args_tf:Sequence[Any],*,checks=())
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.device_assignment(self,computation_shape=(1,1,1,2),num_replicas=1)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.log_jax_hlo(self,f_jax,args:Sequence[Any],*,num_replicas=1,num_partitions=2)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.setUp(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_grad_pjit(self,in_shardings='P',out_shardings=None)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_grad_xmap(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_basic(self,in_shardings='P',out_shardings='P')
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_closed_over_const(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_eager_error(self,func='pjit_sharded')
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_error_inner_sharding(self,kind='pjit',in_shardings='P',out_shardings='none')
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_sharding_constraint(self,nested_pjit=True,constraint='P',poly='2*b1,b2')
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_pjit_variable_arg(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_repro_xla_bug_shmap_collective_permute(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_shmap_all_to_all(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_shmap_collective_permute(self,poly=None)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_xmap_basic(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.test_xmap_collective_reduce(self)
jax.experimental.jax2tf.tests.sharding_test.ShardingTest.tf_hlo(self,f_tf,args_tf:Sequence[Any])->str
jax.experimental.jax2tf.tests.sharding_test.setUpModule()
jax.experimental.jax2tf.tests.sharding_test.tearDownModule()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/cross_compilation_check.py----------------------------------------
A:jax.experimental.jax2tf.tests.cross_compilation_check.basename->re.sub('[^a-zA-Z0-9_\\-]', '_', self.harness.fullname)
A:jax.experimental.jax2tf.tests.cross_compilation_check.f->open(filename, mode=mode)
A:jax.experimental.jax2tf.tests.cross_compilation_check.rng->numpy.random.RandomState(zlib.adler32(harness.fullname.encode()))
A:jax.experimental.jax2tf.tests.cross_compilation_check.args->harness.dyn_args_maker(rng)
A:jax.experimental.jax2tf.tests.cross_compilation_check.scenario1->Scenario(harness, jax.default_backend(), for_platform)
A:jax.experimental.jax2tf.tests.cross_compilation_check.output_file->Scenario(harness, jax.default_backend(), for_platform).output_file(save_directory)
A:jax.experimental.jax2tf.tests.cross_compilation_check.output_dir->os.path.dirname(output_file)
A:jax.experimental.jax2tf.tests.cross_compilation_check.hlo->cross_platform_lowering(func_jax, args, platforms=[for_platform]).compiler_ir(dialect='stablehlo')
A:jax.experimental.jax2tf.tests.cross_compilation_check.lowered->cross_platform_lowering(func_jax, args, platforms=[for_platform])
A:jax.experimental.jax2tf.tests.cross_compilation_check.scenario2->Scenario(harness, on_platform, for_platform)
A:jax.experimental.jax2tf.tests.cross_compilation_check.other_file->Scenario(harness, on_platform, for_platform).output_file(save_directory)
A:jax.experimental.jax2tf.tests.cross_compilation_check.other_hlo->open(filename, mode=mode).read()
A:jax.experimental.jax2tf.tests.cross_compilation_check.nr_harnesses->len(primitive_harness.all_harnesses)
A:jax.experimental.jax2tf.tests.cross_compilation_check.enable_xla->harness.params.get('enable_xla', True)
jax.experimental.jax2tf.tests.cross_compilation_check.Io(self,use_gfile=False)
jax.experimental.jax2tf.tests.cross_compilation_check.Io.__init__(self,use_gfile=False)
jax.experimental.jax2tf.tests.cross_compilation_check.Io.exists(self,filename:str)->bool
jax.experimental.jax2tf.tests.cross_compilation_check.Io.makedirs(self,dirname:str)
jax.experimental.jax2tf.tests.cross_compilation_check.Io.open(self,filename:str,mode:str)
jax.experimental.jax2tf.tests.cross_compilation_check.Scenario
jax.experimental.jax2tf.tests.cross_compilation_check.Scenario.__str__(self)
jax.experimental.jax2tf.tests.cross_compilation_check.Scenario.output_file(self,save_directory:str)->str
jax.experimental.jax2tf.tests.cross_compilation_check.Scenario.short_name(self)->str
jax.experimental.jax2tf.tests.cross_compilation_check.main(argv:Sequence[str])->None
jax.experimental.jax2tf.tests.cross_compilation_check.write_and_check_harness(harness:primitive_harness.Harness,io:Io,save_directory:str,for_platforms:Sequence[str]=('cpu','tpu'))->Sequence[str]
jax.experimental.jax2tf.tests.cross_compilation_check.write_and_check_harnesses(io:Io,save_directory:str,*,filter_harness:Optional[Callable[[str],bool]]=None,for_platforms:Sequence[str]=('cpu','tpu'),verbose=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/back_compat_tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.back_compat_tf_test.tar_buffer->io.BytesIO()
A:jax.experimental.jax2tf.tests.back_compat_tf_test.serialized_string->base64.b64encode(tar_buffer.getvalue())
A:jax.experimental.jax2tf.tests.back_compat_tf_test.tar_data->base64.b64decode(serialized_string)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.res->jax.experimental.jax2tf.convert(func, native_serialization=True)(the_input)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.serialized_module->op.get_attr('module')
A:jax.experimental.jax2tf.tests.back_compat_tf_test.module_str->jax._src.lib.xla_extension.mlir.deserialize_portable_artifact(serialized_module)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.module_version->op.get_attr('version')
A:jax.experimental.jax2tf.tests.back_compat_tf_test.tf_graph_def->tf_graph.as_graph_def()
A:jax.experimental.jax2tf.tests.back_compat_tf_test.module->tensorflow.Module()
A:jax.experimental.jax2tf.tests.back_compat_tf_test.module.call->self.tf_func.get_concrete_function(*data.inputs)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.root_dir->self.create_tempdir()
A:jax.experimental.jax2tf.tests.back_compat_tf_test.saved_model_dir->os.path.join(root_dir, 'saved_model')
A:jax.experimental.jax2tf.tests.back_compat_tf_test.serialized->serialize_directory(saved_model_dir)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.loaded_model->tensorflow.saved_model.load(saved_model_dir)
A:jax.experimental.jax2tf.tests.back_compat_tf_test.data->self.load_testdata(tf_call_tf_function.data_2023_07_29)
jax.experimental.jax2tf.tests.back_compat_tf_test.CompatTensoflowTest(bctu.CompatTestBase)
jax.experimental.jax2tf.tests.back_compat_tf_test.CompatTensoflowTest.run_current(self,func:Callable,data:bctu.CompatTestData)
jax.experimental.jax2tf.tests.back_compat_tf_test.CompatTensoflowTest.run_serialized(self,data:bctu.CompatTestData,polymorphic_shapes:Optional[Sequence[str]]=None)
jax.experimental.jax2tf.tests.back_compat_tf_test.CompatTensoflowTest.serialize(self,func:Callable,data:bctu.CompatTestData,polymorphic_shapes:Optional[Sequence[str]]=None,allow_unstable_custom_call_targets:Sequence[str]=())
jax.experimental.jax2tf.tests.back_compat_tf_test.CompatTensoflowTest.test_tf_call_tf_function(self)
jax.experimental.jax2tf.tests.back_compat_tf_test.deserialize_directory(serialized_string,output_directory)
jax.experimental.jax2tf.tests.back_compat_tf_test.serialize_directory(directory_path)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax_primitives_coverage_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.all_dtypes->set(jtu.dtypes.all)
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.dtypes_tested->dtypes_tested.union({h.dtype}).union({h.dtype})
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.devices->', '.join(l.devices)
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.template->f.read()
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.output_file->os.path.join(os.path.dirname(__file__), '../g3doc/jax_primitives_coverage.md')
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest.test_generate_primitives_coverage_doc(self)
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest.test_jax_implemented(self,harness:primitive_harness.Harness)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/back_compat_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.back_compat_test.dummy_data->self.load_testdata(bctu.dummy_data_dict)
A:jax.experimental.jax2tf.tests.back_compat_test.platform_dummy_data->dataclasses.replace(dummy_data, platform=self.default_jax_backend(), custom_call_targets=['missing'])
A:jax.experimental.jax2tf.tests.back_compat_test.targets_to_cover->set(export._CUSTOM_CALL_TARGETS_GUARANTEED_STABLE)
A:jax.experimental.jax2tf.tests.back_compat_test.covering_testdatas->itertools.chain(*[self.load_testdata_nested(d) for d in covering_testdatas])
A:jax.experimental.jax2tf.tests.back_compat_test.covered_targets->covered_targets.union({'tpu_custom_call'}).union({'tpu_custom_call'})
A:jax.experimental.jax2tf.tests.back_compat_test.not_covered->set(export._CUSTOM_CALL_TARGETS_GUARANTEED_STABLE).difference(covered_targets)
A:jax.experimental.jax2tf.tests.back_compat_test.data->self.load_testdata(stablehlo_dynamic_top_k.data_2023_07_16)
A:jax.experimental.jax2tf.tests.back_compat_test.a->numpy.arange(12, dtype=np.float32).reshape((4, 3))
A:jax.experimental.jax2tf.tests.back_compat_test.input->jax._src.test_util.rand_default(self.rng())(shape, dtype)
A:jax.experimental.jax2tf.tests.back_compat_test.operand->numpy.reshape(np.arange(math.prod(shape), dtype=dtype), shape)
A:jax.experimental.jax2tf.tests.back_compat_test.norm->numpy.linalg.norm(x, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.back_compat_test.rank->len(a.shape)
A:jax.experimental.jax2tf.tests.back_compat_test.aH->jax.numpy.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))
A:jax.experimental.jax2tf.tests.back_compat_test.wC->jax.numpy.conj(w)
A:jax.experimental.jax2tf.tests.back_compat_test.closest_diff->min(abs(eigenvalues_array - eigenvalue))
A:jax.experimental.jax2tf.tests.back_compat_test.w_now_like_v->w_now[np.newaxis, :].astype(v_now.dtype)
A:jax.experimental.jax2tf.tests.back_compat_test.u->numpy.triu(lu_now)
A:jax.experimental.jax2tf.tests.back_compat_test.operand_copy->numpy.reshape(np.arange(math.prod(shape), dtype=dtype), shape).copy()
A:jax.experimental.jax2tf.tests.back_compat_test.error_norm->numpy.linalg.norm(operand - reconstructed_operand, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.back_compat_test.max_backward_error->compute_max_backward_error(a, np.matmul(out[1][..., None, :] * out[0], out[2]))
A:jax.experimental.jax2tf.tests.back_compat_test.out->list(out)
A:jax.experimental.jax2tf.tests.back_compat_test.out[1]->out[1].astype(out[0].dtype).astype(out[0].dtype)
A:jax.experimental.jax2tf.tests.back_compat_test.k->min(m, n)
A:jax.experimental.jax2tf.tests.back_compat_test.unitary_mat->numpy.real(np.matmul(out[2], np.conj(np.T(out[2]))))
A:jax.experimental.jax2tf.tests.back_compat_test.eye_slice->numpy.eye(out[2].shape[-2], dtype=unitary_mat.dtype)
A:jax.experimental.jax2tf.tests.back_compat_test.b->numpy.arange(math.prod(b_shape), dtype=dtype).reshape(b_shape)
A:jax.experimental.jax2tf.tests.back_compat_test.matmul->partial(jnp.matmul, precision=lax.Precision.HIGHEST)
A:jax.experimental.jax2tf.tests.back_compat_test.x->numpy.arange(math.prod(shape), dtype=np.float32).reshape(shape)
A:jax.experimental.jax2tf.tests.back_compat_test.y->jax.lax.approx_max_k(x, 3)
A:jax.experimental.jax2tf.tests.back_compat_test.z->jax.lax.approx_max_k(x, 3)
A:jax.experimental.jax2tf.tests.back_compat_test.mesh->Mesh(devices, axis_names='a')
A:jax.experimental.jax2tf.tests.back_compat_test.axis_size->jax.lax.psum(1, 'a')
A:jax.experimental.jax2tf.tests.back_compat_test._->numpy.arange(math.prod(shape), dtype=np.float32).reshape(shape)
A:jax.experimental.jax2tf.tests.back_compat_test.key->numpy.arange(42, 42 + 4, dtype=np.uint32)
A:jax.experimental.jax2tf.tests.back_compat_test.data_2->self.load_testdata(stablehlo_dynamic_top_k.data_2023_08_11)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest(bctu.CompatTestBase)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.check_eigh_results(self,operand,res_now,res_expected,*,rtol,atol=None)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.check_lu_results(self,operand,res_now,res_expected,*,dtype,rtol=None,atol=None)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.check_svd_results(self,input,res_run,res_exp,rtol=None,atol=None)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.cholesky_input(self,shape,dtype)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.eigh_harness(shape,dtype)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.eigh_input(shape,dtype)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.lu_harness(shape,dtype)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.qr_harness(shape,dtype)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_approx_top_k(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_cholesky_lapack_potrf(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_eig_lapack_geev(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_eigh_lapack_syevd(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_lu_lapack_getrf(self,dtype_name:str)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_qr_lapack_geqrf(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_schur_lapack_gees(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_svd_lapack_gesdd(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cpu_triangular_solve_blas_trsm(self,dtype_name='f32')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cuda_eigh_cusolver_syev(self,dtype_name='f32',variant='syevj')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cuda_qr_cusolver_geqrf(self,dtype_name='f32',batched='unbatched')
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_cuda_threefry2x32(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_custom_call_coverage(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_detect_different_custom_calls(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_detect_different_output(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_ducc_fft(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_dummy(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_sharding(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_stablehlo_dynamic_rbg_bit_generator(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_stablehlo_dynamic_top_k(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_tpu_Eigh(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_tpu_Lu(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_tpu_Qr(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_tpu_stablehlo_dynamic_reduce_window_unary(self)
jax.experimental.jax2tf.tests.back_compat_test.CompatTest.test_tpu_stablehlo_dynamic_reduce_window_variadic(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax2tf_limitations.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax2tf_limitations.group_method->getattr(cls, harness.group_name, None)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.limitations->group_method(harness)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.supported_dtypes->jax._src.test_util.supported_dtypes()
A:jax.experimental.jax2tf.tests.jax2tf_limitations.nr_special_cases->numpy.count_nonzero(special_cases)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.norm->numpy.linalg.norm(x, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.rank->len(a.shape)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.aH->jax.numpy.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.wC->jax.numpy.conj(w)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.closest_diff->min(abs(eigenvalues_array - eigenvalue))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.result->numpy.reshape(np.array(result, dtype=dtype), [*batch_dims, m, m])
A:jax.experimental.jax2tf.tests.jax2tf_limitations.k->min(m, n)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.p_mat->_make_permutation_matrix(perm)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.mask->numpy.isnan(result_jax)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.operand_jax->reconstruct_operand(r_jax)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.operand_tf->reconstruct_operand(r_tf)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.error_norm->jax.numpy.linalg.norm(operand_jax - operand_tf, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.max_backward_error->jax.numpy.amax(backward_error)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.forward_diff->jax.numpy.diff(s, axis=-1, append=forward_appendant)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.absolute_gap->compute_absolute_gap(r_jax[0], m, n)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.sum_of_ratios->jax.numpy.sum(jnp.divide(y, x), -2, keepdims=True)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.phases->jax.numpy.divide(sum_of_ratios, jnp.abs(sum_of_ratios))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.output->jax.numpy.sum(jnp.einsum('...ij,...ij->...ij', a.conj(), b, precision=lax.Precision.HIGHEST), axis=-2)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.cos_angular_diff->jax.numpy.clip(cos_angular_diff, a_min=0.0, a_max=1.0)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.angular_diff->jax.numpy.arccos(cos_angular_diff)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.v_jax->jax.numpy.swapaxes(r_jax[2][..., :rank, :], -2, -1).conj()
A:jax.experimental.jax2tf.tests.jax2tf_limitations.v_tf->jax.numpy.swapaxes(r_tf[2][..., :rank, :], -2, -1).conj()
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation(self,description:str,*,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Sequence[DType]=(),enabled:bool=True,modes=('eager','graph','compiled'),native_serialization=FOR_NON_NATIVE,skip_tf_run=False,expect_tf_error:bool=True,skip_comparison=False,custom_assert:Optional[Callable]=None,tol=None)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.__init__(self,description:str,*,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Sequence[DType]=(),enabled:bool=True,modes=('eager','graph','compiled'),native_serialization=FOR_NON_NATIVE,skip_tf_run=False,expect_tf_error:bool=True,skip_comparison=False,custom_assert:Optional[Callable]=None,tol=None)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation._pow_test_util(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.acos(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.acosh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.approx_top_k(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.argmax(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.argmin(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.asin(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.asinh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.atan(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.atanh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.bessel_i0e(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.bessel_i1e(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cbrt(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cholesky(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.conv_general_dilated(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cumlogsumexp(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cumprod(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cumsum(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.custom_linear_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.digamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.div(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.dot_general(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.eig(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.eigh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erf(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erf_inv(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erfc(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.expm1(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.fft(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.filter(self,dtype:Optional[DType]=None,device:Optional[str]=None,mode:Optional[str]=None)->bool
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.get_max_tolerance_limitation(self,limitations:Sequence['Jax2TfLimitation'])->Optional['Jax2TfLimitation']
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.helper_get_trig_custom_limitation(cls,np_inverse)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.igamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.igammac(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.integer_pow(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.lgamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.limitations_for_harness(cls,harness:primitive_harness.Harness)->Sequence['Jax2TfLimitation']
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.log1p(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.lu(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.max(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.min(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.nextafter(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.pow(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.qr(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_fold_in(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_gamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_seed(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_split(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_max(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_min(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_window_add(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.regularized_incomplete_beta(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.rem(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.rng_bit_generator(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.round(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_add(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_max(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_min(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_mul(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.select_and_gather_add(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.sort(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.svd(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tan(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tanh(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.top_k(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.triangular_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tridiagonal_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.custom_numeric(*,description='customnumericcomparison',dtypes=(),modes=('eager','graph'),devices=('cpu','gpu','tpu'),custom_assert=None,enabled=True,native_serialization=Jax2TfLimitation.FOR_NON_NATIVE,tol=None)->Jax2TfLimitation
jax.experimental.jax2tf.tests.jax2tf_limitations.custom_random_keys_output()
jax.experimental.jax2tf.tests.jax2tf_limitations.missing_tf_kernel(*,description='opnotdefinedfordtype',dtypes,modes=('eager','graph','compiled'),devices=('cpu','gpu','tpu'),native_serialization=Jax2TfLimitation.FOR_NON_NATIVE,enabled=True)->Jax2TfLimitation


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/control_flow_ops_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.control_flow_ops_test.res->jax.lax.cond(True, lambda op: op * x, lambda op: op + x, x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.cond_const->numpy.ones(3, dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const1->numpy.full_like(cond_const, 1.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const2->numpy.full_like(cond_const, 2.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.xs->numpy.arange(4, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.ys->numpy.arange(5, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const->numpy.ones((2,), dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.arg->numpy.full((5,), 0.7)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(c_out, _)->jax.lax.scan(body, 0.0, (xs, ys))
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(res1, res2)->jax.lax.scan(body_fun, 0.0, xs + 1.0)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_multiple_results(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_units(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_remat(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_batched_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_single_carry(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/models_test_main.py----------------------------------------
A:jax.experimental.jax2tf.tests.models_test_main._CONVERTERS->absl.flags.DEFINE_list('converters', [x.name for x in ALL_CONVERTERS], 'Which converters to test.')
A:jax.experimental.jax2tf.tests.models_test_main._EXAMPLES->absl.flags.DEFINE_list('examples', [], "List of examples to test, e.g.: 'flax/mnist,flax/seq2seq'. If empty, will test all examples.")
A:jax.experimental.jax2tf.tests.models_test_main._EXAMPLE_PREFIX->absl.flags.DEFINE_string('example_prefix', '', "Prefix for filtering tests. For instance 'flax/mnist' will test all examples starting with 'flax/mnist' (including all polymorphic tests).")
A:jax.experimental.jax2tf.tests.models_test_main._WRITE_MARKDOWN->absl.flags.DEFINE_bool('write_markdown', True, 'If true, write results as Markdown. Otherwise, only output to stdout.')
A:jax.experimental.jax2tf.tests.models_test_main._FAIL_ON_ERROR->absl.flags.DEFINE_bool('fail_on_error', False, 'If true, exit with an error when a conversion fails. Useful for debugging because it will show the entire stack trace.')
A:jax.experimental.jax2tf.tests.models_test_main.header->header.lower().replace(' ', '-').lower().replace(' ', '-')
A:jax.experimental.jax2tf.tests.models_test_main.output_path->os.path.join(g3doc_path, 'convert_models_results.md')
A:jax.experimental.jax2tf.tests.models_test_main.template->template.replace('{{errors}}', '\n'.join(error_lines)).replace('{{errors}}', '\n'.join(error_lines))
A:jax.experimental.jax2tf.tests.models_test_main.msg->msg.replace('\n\n', '\n').replace('\n\n', '\n')
A:jax.experimental.jax2tf.tests.models_test_main.dtype->jax._src.dtypes.canonicalize_dtype(x.dtype)
A:jax.experimental.jax2tf.tests.models_test_main.converters->list(filter(lambda x: x.name in _CONVERTERS.value, ALL_CONVERTERS))
A:jax.experimental.jax2tf.tests.models_test_main.harness->harness_fn()
A:jax.experimental.jax2tf.tests.models_test_main.np_assert_allclose->functools.partial(np.testing.assert_allclose, rtol=harness.rtol)
A:jax.experimental.jax2tf.tests.models_test_main.apply_tf->converter.convert_fn(harness)
A:jax.experimental.jax2tf.tests.models_test_main.jax_result->harness_fn().apply_with_vars(*xs)
A:jax.experimental.jax2tf.tests.models_test_main.tf_result->apply_tf(*xs)
A:jax.experimental.jax2tf.tests.models_test_main.error_msg->_format(e)
jax.experimental.jax2tf.tests.models_test_main._crop_convert_error(msg:str)->str
jax.experimental.jax2tf.tests.models_test_main._get_random_data(x:jax.Array)->np.ndarray
jax.experimental.jax2tf.tests.models_test_main._write_markdown(results:dict[str,list[tuple[str,str]]])->None
jax.experimental.jax2tf.tests.models_test_main.main(argv:Sequence[str])->None
jax.experimental.jax2tf.tests.models_test_main.test_converters()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/primitive_harness.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitive_harness.self.group_name->jax._src.test_util.sanitize_test_name(group_name)
A:jax.experimental.jax2tf.tests.primitive_harness.self.name->jax._src.test_util.sanitize_test_name(name)
A:jax.experimental.jax2tf.tests.primitive_harness.all_args->self._args_from_dynargs(dyn_args)
A:jax.experimental.jax2tf.tests.primitive_harness.group_name->str(group_name)
A:jax.experimental.jax2tf.tests.primitive_harness.h->Harness(group_name, name, fun, arg_descriptors, rng_factory=rng_factory, jax_unimplemented=jax_unimplemented, dtype=dtype, **params)
A:jax.experimental.jax2tf.tests.primitive_harness.devices->tuple(devices)
A:jax.experimental.jax2tf.tests.primitive_harness.dtypes->tuple(dtypes)
A:jax.experimental.jax2tf.tests.primitive_harness.cases->tuple((dict(testcase_name=harness.fullname if one_containing is None else '', harness=harness) for harness in harnesses if harness.filter(jtu.device_under_test(), one_containing=one_containing, include_jax_unimpl=include_jax_unimpl)))
A:jax.experimental.jax2tf.tests.primitive_harness.operand->jax._src.test_util.rand_default(rng)(shape, dtype)
A:jax.experimental.jax2tf.tests.primitive_harness._LAX_COMPARATORS->dict(eq=jnp.equal, ne=jnp.not_equal, ge=jnp.greater_equal, gt=jnp.greater, le=jnp.less_equal, lt=jnp.less)
A:jax.experimental.jax2tf.tests.primitive_harness.arg->numpy.array([-1, -2, 0, 1], dtype=dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.index_dtype->tuple(dtypes).canonicalize_dtype(index_dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.indices->numpy.array(2, dtype=np.int32)
A:jax.experimental.jax2tf.tests.primitive_harness._gather_input->numpy.arange(1000, dtype=np.float32).reshape((10, 10, 10))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_2d->jax.lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_2d_2->jax.lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_3d->jax.lax.GatherDimensionNumbers(offset_dims=(1, 2), collapsed_slice_dims=(0,), start_index_map=(0, 1, 2))
A:jax.experimental.jax2tf.tests.primitive_harness.start_indices->numpy.array(start_indices)
A:jax.experimental.jax2tf.tests.primitive_harness.gather_2d_bd->jax.lax.GatherDimensionNumbers(offset_dims=(2, 3, 4), collapsed_slice_dims=(), start_index_map=(1, 2))
A:jax.experimental.jax2tf.tests.primitive_harness.gather_2d_bd_nid->jax.lax.GatherDimensionNumbers(offset_dims=(2, 3, 4), collapsed_slice_dims=(), start_index_map=(2, 1))
A:jax.experimental.jax2tf.tests.primitive_harness.gather_3d_bd->jax.lax.GatherDimensionNumbers(offset_dims=(1, 2, 3, 4), collapsed_slice_dims=(), start_index_map=(1, 2, 3))
A:jax.experimental.jax2tf.tests.primitive_harness.gather_2d_bd2->jax.lax.GatherDimensionNumbers(offset_dims=(2, 3, 4), collapsed_slice_dims=(), start_index_map=(1, 2))
A:jax.experimental.jax2tf.tests.primitive_harness.dimension_numbers->jax.lax.ScatterDimensionNumbers(*dimension_numbers)
A:jax.experimental.jax2tf.tests.primitive_harness._lax_sort_multiple_array_first_arg->numpy.random.uniform(0, 2, _lax_sort_multiple_array_shape).astype(np.int32)
A:jax.experimental.jax2tf.tests.primitive_harness.a->jax._src.test_util.rand_default(rng)(shape, dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.matvec->partial(lax.dot, a, precision=lax.Precision.HIGHEST)
A:jax.experimental.jax2tf.tests.primitive_harness.padding->tuple(lax.padtype_to_pads(shape, window_dimensions, window_strides, padding))
A:jax.experimental.jax2tf.tests.primitive_harness.init_val->numpy.array(init_value, dtype=dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.key->jax.random.wrap_key_data(key)
A:jax.experimental.jax2tf.tests.primitive_harness.result->jax.random.split(key, 2)
A:jax.experimental.jax2tf.tests.primitive_harness.maxval->{np.uint8: 256}.get(dtype, 5)
A:jax.experimental.jax2tf.tests.primitive_harness.shapes_str->'_'.join((jtu.format_shape_dtype_string(s, dtype) for s in shapes))
A:jax.experimental.jax2tf.tests.primitive_harness.shapestr->','.join((str(dim) for dim in shape))
A:jax.experimental.jax2tf.tests.primitive_harness.out_iinfo->tuple(dtypes).finfo(out_dtype)
jax.experimental.jax2tf.tests.primitive_harness.CustomArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.Harness(self,group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.Harness.__init__(self,group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.Harness.__str__(self)
jax.experimental.jax2tf.tests.primitive_harness.Harness._arg_maker(self,arg_descriptor,rng:Rng)
jax.experimental.jax2tf.tests.primitive_harness.Harness._args_from_dynargs(self,dyn_args:Sequence)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_fun(self,*dyn_args)
jax.experimental.jax2tf.tests.primitive_harness.Harness.filter(self,device_under_test:str,*,include_jax_unimpl:bool=False,one_containing:Optional[str]=None)->bool
jax.experimental.jax2tf.tests.primitive_harness.Limitation(self,description:str,*,enabled:bool=True,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Sequence[DType]=(),skip_run:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.__init__(self,description:str,*,enabled:bool=True,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Sequence[DType]=(),skip_run:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.__str__(self)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.filter(self,device:Optional[str]=None,dtype:Optional[DType]=None)->bool
jax.experimental.jax2tf.tests.primitive_harness.RandArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.StaticArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness._can_bitcast(dtype,target_dtype)
jax.experimental.jax2tf.tests.primitive_harness._get_max_identity(dtype)
jax.experimental.jax2tf.tests.primitive_harness._get_min_identity(dtype)
jax.experimental.jax2tf.tests.primitive_harness._make_add_any_harness(name,*,shapes=((2,),(2,)),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_argminmax_harness(prim,name,*,shape=(15,),dtype=jnp.float32,axes=(0,),index_dtype=np.int32,arr=None,works_without_xla=True)
jax.experimental.jax2tf.tests.primitive_harness._make_binary_elementwise_harnesses(prim,dtypes,default_dtype=np.float32,broadcasting_dtypes=None,jax_unimplemented=lambda**kwargs:[])
jax.experimental.jax2tf.tests.primitive_harness._make_bitcast_convert_type_harness(name,*,shape=(2,3),dtype=np.float32,new_dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_broadcast_in_dim_harness(name,*,dtype=np.float32,shape=(2,),outshape=(2,),broadcast_dimensions=(0,))
jax.experimental.jax2tf.tests.primitive_harness._make_cholesky_arg(shape,dtype,rng)
jax.experimental.jax2tf.tests.primitive_harness._make_clamp_harness(name,*,min_shape=(),operand_shape=(2,3),max_shape=(),dtype=np.float32,min_max=None)
jax.experimental.jax2tf.tests.primitive_harness._make_comparator_harness(name,*,dtype=np.float32,op=lax.eq_p,op_name='eq',lhs_shape=(),rhs_shape=())
jax.experimental.jax2tf.tests.primitive_harness._make_complex_harness(name,*,shapes=((3,4),(3,4)),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_concatenate_harness(name,*,shapes=[(2,3),(2,3)],dimension=0,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_conj_harness(name,*,shape=(3,4),dtype=np.float32,**kwargs)
jax.experimental.jax2tf.tests.primitive_harness._make_conv_harness(name,*,lhs_shape=(2,3,9,10),rhs_shape=(3,3,4,5),dtype=np.float32,window_strides=(1,1),precision=None,padding=((0,0),(0,0)),lhs_dilation=(1,1),rhs_dilation=(1,1),feature_group_count=1,dimension_numbers=('NCHW','OIHW','NCHW'),batch_group_count=1,preferred_element_type=None,works_without_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_convert_element_type_harness(name,*,shape=(100,100),dtype=np.float32,new_dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_cumreduce_harness(name,*,f_jax=lax_control_flow.cummin,shape=(8,9),dtype=np.float32,axis=0,reverse=False)
jax.experimental.jax2tf.tests.primitive_harness._make_device_put_harness(name,*,shape=(3,4),dtype=np.float32,device=None)
jax.experimental.jax2tf.tests.primitive_harness._make_div_rem_harness(prim,name,*,shapes=((2,),(2,)),dtype=np.float32,arrs=(None,None))
jax.experimental.jax2tf.tests.primitive_harness._make_dot_general_harness(name,*,lhs_shape=(3,4),rhs_shape=(4,2),lhs_dtype=np.float32,rhs_dtype=np.float32,precision=None,dimension_numbers=(((1,),(0,)),((),())),preferred_element_type=None,enable_xla=True)
jax.experimental.jax2tf.tests.primitive_harness._make_dynamic_slice_harness(name,shape=(3,),start_indices=(1,),limit_indices=(2,),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_dynamic_update_slice_harness(name,shape=(3,),start_indices=(1,),dtype=np.float32,update_shape=(1,))
jax.experimental.jax2tf.tests.primitive_harness._make_fft_harness(name,*,shape=(14,15,16,17),dtype=np.float32,fft_type=xla_client.FftType.FFT,fft_lengths=(17,))
jax.experimental.jax2tf.tests.primitive_harness._make_integer_pow_harness(name,*,shape=(20,30),dtype=np.int32,y=3)
jax.experimental.jax2tf.tests.primitive_harness._make_iota_2x32_shape_harness(shape)
jax.experimental.jax2tf.tests.primitive_harness._make_iota_harness(name,*,shape=(2,3),dtype=np.float32,dimension=0)
jax.experimental.jax2tf.tests.primitive_harness._make_linear_solve_harnesses()
jax.experimental.jax2tf.tests.primitive_harness._make_pow_harness(name,*,shapes=((20,30),(20,30)),dtype=np.float32,lhs=None,rhs=None)
jax.experimental.jax2tf.tests.primitive_harness._make_real_imag_harness(prim,name,*,shape=(2,3),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_reduce_harness(name,*,shape=(4,6),nr_operands=1,computation=lax.add,dimensions:Sequence[int]=(0,),init_value=0,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_reduce_window_harness(name,*,shape=(4,6),base_dilation=(1,1),computation=lax.add,window_dimensions=(2,2),window_dilation=(1,1),init_value=0,window_strides=(1,1),dtype=np.float32,padding=((0,0),(0,0)),requires_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_reducer_harness(prim,name,*,shape=(2,3),axes=(0,),dtype=np.int32)
jax.experimental.jax2tf.tests.primitive_harness._make_reshape_harness(name,*,shape=(2,3),new_sizes=(3,2),dimensions=(0,1),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_rev_harness(name,*,shape=(4,5),dtype=np.float32,dimensions=(0,))
jax.experimental.jax2tf.tests.primitive_harness._make_round_harness(name,*,shape=(100,100),dtype=np.float32,rounding_method=lax.RoundingMethod.AWAY_FROM_ZERO,operand=None)
jax.experimental.jax2tf.tests.primitive_harness._make_scatter_harness(name,*,shape=(5,),f_lax=lax.scatter_min,indices_are_sorted=False,unique_indices=False,scatter_indices=np.array([[0],[2]]),update_shape=(2,),mode=lax.GatherScatterMode.FILL_OR_DROP,dtype=np.float32,dimension_numbers=((),(0,),(0,)),enable_and_disable_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_select_and_gather_add_harness(name,*,shape=(4,6),dtype=np.float32,select_prim=lax.le_p,padding='VALID',window_dimensions=(2,2),window_strides=(1,1),base_dilation=(1,1),window_dilation=(1,1))
jax.experimental.jax2tf.tests.primitive_harness._make_select_and_scatter_add_harness(name,*,shape=(2,4,6),dtype=np.float32,select_prim=lax.ge_p,window_dimensions=(2,2,2),window_strides=(1,1,1),padding=((0,0),(0,0),(0,0)),nb_inactive_dims=0)
jax.experimental.jax2tf.tests.primitive_harness._make_select_n_harness(name,*,shape_pred=(2,3),shape_args=(2,3),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_slice_harness(name,shape=(3,),start_indices=(1,),limit_indices=(2,),strides=None,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_sort_harness(name,*,operands=None,shape=(5,7),dtype=np.float32,dimension=0,is_stable=False,num_keys=1)
jax.experimental.jax2tf.tests.primitive_harness._make_squeeze_harness(name,shape=(1,2),dimensions=(0,),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_top_k_harness(name,*,operand=None,shape=(5,3),dtype=np.float32,k=2)
jax.experimental.jax2tf.tests.primitive_harness._make_transpose_harness(name,*,shape=(2,3),permutation=(1,0),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_triangular_eigh_operand(shape,dtype,lower:bool,rng:Rng)
jax.experimental.jax2tf.tests.primitive_harness._make_triangular_solve_harness(name,*,left_side=True,lower=False,ab_shapes=((4,4),(4,1)),dtype=np.float32,transpose_a=False,conjugate_a=False,unit_diagonal=False)
jax.experimental.jax2tf.tests.primitive_harness._make_unary_elementwise_harness(*,prim,shape=(20,20),dtype)
jax.experimental.jax2tf.tests.primitive_harness.define(group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.dtypes_to_str(dtype_list:Sequence[DType],empty_means_all=False)->str
jax.experimental.jax2tf.tests.primitive_harness.parameterized(harnesses:Iterable[Harness],*,one_containing:Optional[str]=None,include_jax_unimpl:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.requires_xla_for_reduce(name,dtype)
jax.experimental.jax2tf.tests.primitive_harness.wrap_and_split()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/call_tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.call_tf_test._parameterized_jit->absl.testing.parameterized.named_parameters((_named_test(with_jit=with_jit) for with_jit in [True, False]))
A:jax.experimental.jax2tf.tests.call_tf_test._->tensorflow.function(tf_func, autograph=False, jit_compile=True).get_concrete_function(*data_inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.res->tensorflow.saved_model.load(saved_model_dir).call(*data_inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.x->numpy.arange(3, dtype=np.int32)
A:jax.experimental.jax2tf.tests.call_tf_test.fun_jax->jax.experimental.jax2tf.call_tf(fun_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.x1->numpy.arange(3, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x2->numpy.ones(5, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax->jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=True))
A:jax.experimental.jax2tf.tests.call_tf_test.ta->ta.unstack([0, 1, 2, 3, 4]).unstack([0, 1, 2, 3, 4])
A:jax.experimental.jax2tf.tests.call_tf_test.(_, acc)->tensorflow.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])
A:jax.experimental.jax2tf.tests.call_tf_test.y->tensorflow.math.sin(3.0)
A:jax.experimental.jax2tf.tests.call_tf_test.res_call_tf->_maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.res_jax->f_jax(x)
A:jax.experimental.jax2tf.tests.call_tf_test.res_call_tf_jit->jax.jit(jax2tf.call_tf(f_tf))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_var_array->numpy.array([3.0, 4.0], dtype=np.float64)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_var->tensorflow.Variable(np.array([3.0], dtype=np.float32))
A:jax.experimental.jax2tf.tests.call_tf_test.v->tensorflow.Variable((4.0, 2.0), dtype=tf.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_out->tf_func(x)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_func->jax.jit(jax2tf.call_tf(tf_func))
A:jax.experimental.jax2tf.tests.call_tf_test.jax_out->jax_func(x)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_tensor->tensorflow.constant(3.0, dtype=np.float64)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_val->numpy.array(3.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.v2->tensorflow.Variable(2.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.v3->tensorflow.Variable(3.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.t4->tensorflow.constant(4.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.t5->tensorflow.constant(5.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_x->_maybe_jit(with_jit, jax.grad(jax2tf.call_tf(func_square_tf)))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.b->numpy.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.c->numpy.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x_dict->dict(b=b, c=c)
A:jax.experimental.jax2tf.tests.call_tf_test.prediction->inference_fn(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.weights->numpy.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.weighted_pred->jax.numpy.matmul(weights, prediction['r'])
A:jax.experimental.jax2tf.tests.call_tf_test.g_fun_with_tf->jax.grad(partial(loss, jax2tf.call_tf(f_tf)))
A:jax.experimental.jax2tf.tests.call_tf_test.g_fun_with_jax->jax.grad(partial(loss, f_jax))
A:jax.experimental.jax2tf.tests.call_tf_test.g_tf->tape.gradient(res, xv)
A:jax.experimental.jax2tf.tests.call_tf_test.g_jax->grad_g(x, y)
A:jax.experimental.jax2tf.tests.call_tf_test.param->numpy.array([1.0, 2.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.state->dict(array=np.float32(1.0), counter=7, truth=True)
A:jax.experimental.jax2tf.tests.call_tf_test.f_call_tf->jax.experimental.jax2tf.call_tf(f)
A:jax.experimental.jax2tf.tests.call_tf_test.g_call_tf->grad_g_call_tf(x, y)
A:jax.experimental.jax2tf.tests.call_tf_test.g->jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)
A:jax.experimental.jax2tf.tests.call_tf_test.inputs->jax.numpy.ones([10], dtype=jnp.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.rng->numpy.array([1, 2], dtype=np.uint32)
A:jax.experimental.jax2tf.tests.call_tf_test.params->numpy.float32(0.5)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_model->jax.experimental.jax2tf.convert(jax_model, with_gradient=True)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_loss_fn->partial(_loss_fn, jax_model)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_grad->jax.grad(jax_loss_fn)(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.paramsv->tensorflow.Variable(params)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_prediction->tf_model(paramsv, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_loss->tensorflow.reduce_mean(tf_prediction)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_grad->tape.gradient(tf_loss, paramsv)
A:jax.experimental.jax2tf.tests.call_tf_test.call_tf_loss_fn->partial(_loss_fn, jax2tf.call_tf(tf_model))
A:jax.experimental.jax2tf.tests.call_tf_test.call_tf_grad->jax.grad(call_tf_loss_fn)(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_g->jax.grad(partial(wrapper, f_jax), allow_int=True, argnums=(0, 1))
A:jax.experimental.jax2tf.tests.call_tf_test.grad_g_call_tf->jax.grad(partial(wrapper, jax2tf.call_tf(f_tf)), allow_int=True, argnums=(0, 1))
A:jax.experimental.jax2tf.tests.call_tf_test.grad_jax->jax.grad(grad_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_jax_pure->jax.grad(grad_jax_pure)
A:jax.experimental.jax2tf.tests.call_tf_test.res1->jax.experimental.jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)
A:jax.experimental.jax2tf.tests.call_tf_test.hlo->tensorflow.function(fun_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(x)()
A:jax.experimental.jax2tf.tests.call_tf_test.outer_ct->numpy.array([3.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x_const->tensorflow.constant(0, shape=x.shape, dtype=x.dtype)
A:jax.experimental.jax2tf.tests.call_tf_test.f_tf->jax.experimental.jax2tf.convert(f_jax, polymorphic_shapes=polymorphic_shapes)
A:jax.experimental.jax2tf.tests.call_tf_test.(f_tf_rt, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax2->jax.experimental.jax2tf.call_tf(f_tf_rt)
A:jax.experimental.jax2tf.tests.call_tf_test.f_tf2->jax.experimental.jax2tf.convert(f_jax2)
A:jax.experimental.jax2tf.tests.call_tf_test.lower_effect->jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=True)).lower(x)
A:jax.experimental.jax2tf.tests.call_tf_test.lower_no_effect->jax.jit(jax2tf.call_tf(tf.math.sin, has_side_effects=False)).lower(x)
A:jax.experimental.jax2tf.tests.call_tf_test.operand->jax.numpy.array(np.random.uniform(size=(100, 128)))
A:jax.experimental.jax2tf.tests.call_tf_test.indices->jax.numpy.array(np.random.randint(low=0, high=100, size=(4000,)))
A:jax.experimental.jax2tf.tests.call_tf_test.grad_fun_jax->jax.grad(jax2tf.call_tf(tf_f), holomorphic=True)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_res->grad_fun_jax(operand, indices)
A:jax.experimental.jax2tf.tests.call_tf_test.fun_jax_1->jax.experimental.jax2tf.call_tf(fun_tf, output_shape_dtype=jax.ShapeDtypeStruct((10,), jnp.float32))
A:jax.experimental.jax2tf.tests.call_tf_test.fun_jax_2->jax.experimental.jax2tf.call_tf(fun_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.fun_jax_3->jax.experimental.jax2tf.call_tf(fun_tf, output_shape_dtype=None)
A:jax.experimental.jax2tf.tests.call_tf_test.devices->jax.devices(backend)
A:jax.experimental.jax2tf.tests.call_tf_test.lowering_platforms->tuple((p if p != 'gpu' else 'cuda' for p in jax_and_tf_platforms))
A:jax.experimental.jax2tf.tests.call_tf_test.exp->jax.experimental.export.export.export(f_jax, lowering_platforms=lowering_platforms)(x)
A:jax.experimental.jax2tf.tests.call_tf_test.x_device->jax.device_put(x, jax_device)
A:jax.experimental.jax2tf.tests.call_tf_test.native_res->f_jax(x_device)
A:jax.experimental.jax2tf.tests.call_tf_test.exported_res->jax.experimental.export.export.call_exported(exp)(x_device)
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax_rt->jax.experimental.jax2tf.call_tf(restored_f)
A:jax.experimental.jax2tf.tests.call_tf_test.f_rt->jax.experimental.jax2tf.call_tf(f_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.(restored_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.restored_jax->jax.experimental.jax2tf.call_tf(restored_model.f)
A:jax.experimental.jax2tf.tests.call_tf_test.param_v->tensorflow.Variable(param)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, restored_model)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])
A:jax.experimental.jax2tf.tests.call_tf_test.res_jax_y->f_jax(y)
A:jax.experimental.jax2tf.tests.call_tf_test.(g_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True), input_signature=[tf.TensorSpec(shape=(1,), dtype=tf.float32)])
A:jax.experimental.jax2tf.tests.call_tf_test.g_rt->jax.experimental.jax2tf.call_tf(g_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.(f_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)
A:jax.experimental.jax2tf.tests.call_tf_test.converted_fun->tensorflow.function(jax2tf.convert(fun_jax, native_serialization=True))
A:jax.experimental.jax2tf.tests.call_tf_test.jax_f->jax.experimental.jax2tf.call_tf(tf.function(tf_f), call_tf_graph=True)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_f_rt->jax.experimental.jax2tf.convert(jax_f, native_serialization=True, with_gradient=False)
A:jax.experimental.jax2tf.tests.call_tf_test.baseline->jax.tree_util.tree_map(conversion, baseline)
A:jax.experimental.jax2tf.tests.call_tf_test.candidate->jax.tree_util.tree_map(conversion, candidate)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_fun_jax_rt->jax.experimental.jax2tf.call_tf(jax2tf.convert(grad_fun_jax))
A:jax.experimental.jax2tf.tests.call_tf_test.y_jax->jax.numpy.cos(x_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.z_jax->jax.experimental.jax2tf.call_tf(f_tf_inner)(y_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.y_tf->tensorflow.math.sin(x_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.z_tf->jax.experimental.jax2tf.convert(f_jax)(y_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.xv->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, gf)->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))
A:jax.experimental.jax2tf.tests.call_tf_test.expected_res->numpy.sin(np.cos(np.sin(np.cos(np.sin(x)))))
A:jax.experimental.jax2tf.tests.call_tf_test.fun_tf_rt->_maybe_tf_jit(with_jit, jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...']))
A:jax.experimental.jax2tf.tests.call_tf_test.(reloaded_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.z->jax.numpy.array(5.0, dtype=jnp.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.w->jax.experimental.jax2tf.call_tf(lambda z: tf.concat([z, z], axis=0), output_shape_dtype=jax.ShapeDtypeStruct((2 * z.shape[0],), z.dtype))(z)
A:jax.experimental.jax2tf.tests.call_tf_test.res_tf->fun_tf_rt(x)
A:jax.experimental.jax2tf.tests.call_tf_test.f_inner_tf->jax.experimental.jax2tf.convert(f_inner_jax, native_serialization=True)
A:jax.experimental.jax2tf.tests.call_tf_test.f_outer_tf->tensorflow.function(jax2tf.convert(f_outer_jax, native_serialization=False), autograph=False)
A:jax.experimental.jax2tf.tests.call_tf_test.f_outer_graph->str(f_outer_tf.get_concrete_function(tf.convert_to_tensor(x)).graph.as_graph_def())
A:jax.experimental.jax2tf.tests.call_tf_test.acc->numpy.array(2.0, dtype=x.dtype)
A:jax.experimental.jax2tf.tests.call_tf_test.f2_tf->tensorflow.function(f2_tf, autograph=False)
A:jax.experimental.jax2tf.tests.call_tf_test.(f2_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.(_, (g_f2_ft,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])
A:jax.experimental.jax2tf.tests.call_tf_test.(_, (g_f4_ft,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])
A:jax.experimental.jax2tf.tests.call_tf_test.f4_tf->tensorflow.function(f4_tf, autograph=False)
A:jax.experimental.jax2tf.tests.call_tf_test.(f4_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.const->tensorflow.Variable(0.0, dtype=tf.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.stablehlo_module->f_jax.lower(x).compiler_ir('stablehlo')
A:jax.experimental.jax2tf.tests.call_tf_test.tf_backend_config->jax._src.lib.mlir.ir.DictAttr(op.attributes['tf.backend_config'])
A:jax.experimental.jax2tf.tests.call_tf_test.res2->jax.experimental.jax2tf.call_tf(tf_f, call_tf_graph=True, output_shape_dtype=output_shape_dtype)(x)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(tf_f_rt_2, input_args=[])
A:jax.experimental.jax2tf.tests.call_tf_test.jax_f_2->jax.experimental.jax2tf.call_tf(tf.function(tf_f_2), call_tf_graph=True)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_f_rt_2->jax.experimental.jax2tf.convert(jax_f_2, native_serialization=True, with_gradient=False)
A:jax.experimental.jax2tf.tests.call_tf_test.call_tf_print->jax.experimental.jax2tf.call_tf(tf_print, call_tf_graph=True, ordered=True)
A:jax.experimental.jax2tf.tests.call_tf_test.lower->jax.jit(jax2tf.call_tf(tf_func_3, call_tf_graph=True)).lower(x)
A:jax.experimental.jax2tf.tests.call_tf_test.x_dead->numpy.arange(4, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x3->numpy.arange(5, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.jit_tf_func->tensorflow.function(tf_func, autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.call_tf_test.module->tensorflow.Module()
A:jax.experimental.jax2tf.tests.call_tf_test.root_dir->self.create_tempdir()
A:jax.experimental.jax2tf.tests.call_tf_test.saved_model_dir->os.path.join(root_dir, 'saved_model')
A:jax.experimental.jax2tf.tests.call_tf_test.loaded_model->tensorflow.saved_model.load(saved_model_dir)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.setUpClass(cls)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_bool(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_control_flow(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_dtypes(self,dtype=np.int32,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_effectful(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_error_bad_result_string(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_error_bad_result_tensorarray(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_error_non_compilable_dynamic_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_error_non_compilable_strings(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_devicearray_arg(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_numpy_arg(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_numpy_res(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_pytree(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_scalar_arg(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_scalar_res(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_experimental_get_compiler_ir_design_doc(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_function_compile_time_constant_inputs(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_custom(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_int_argument(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_int_argument_unused(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_nested(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_pytree(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_with_float0_result(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_higher_order_grad(self,degree=2,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_module_documentation(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_multi_platform(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_multi_platform_call_tf_graph(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_output_shape_dtype_none(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_output_shape_dtype_not_none(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_pmap(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_repro_193754660(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_result_tuple(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_tf_gather(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_multiple_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_tensor_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_tensor_capture_x64(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_value_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_different_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_read(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_read_x64(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_write_error(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_x64_input(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_x64_output(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_all_floating_input_gradient(self,dtype)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_call_tf_under_function_context(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_complex_input_gradient(self,dtype)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_custom_grad(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_custom_grad_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_pytree(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_no_gradients(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_shape_poly(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_simple(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_variables(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_shape_poly(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_simple(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_without_gradient_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest._walk_stablehlo_operations(cls,op,cb)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.override_serialization_version(self,version_override:int)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_alternate(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_b279454591(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_graph(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_graph_non_compilable(self,tf_f,output_shape_dtype)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_graph_ordered(self,*,version:int)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_graph_polymorphic(self,ordered:bool,version:int)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_graph_save_and_load(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_call_tf_ordered_dead_inputs(self,*,poly:bool,version:int)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_function_dynamic_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_inner_native_serialization(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_saved_model_polymorphic_input_static_output(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_several_round_trips(self,f2_function=False,f2_saved_model=False,f4_function=False,f4_saved_model=False)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly_error_mismatch_output_shape_dtype(self,with_jit=False,kind='bad_rank')
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly_error_mismatch_output_shape_dtype_tree(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly_error_no_output_shape_dtype(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly_pytree_result(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_poly_static_output_shape(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test._maybe_jit(with_jit:bool,func:Callable)->Callable
jax.experimental.jax2tf.tests.call_tf_test._maybe_tf_jit(with_jit:bool,func:Callable)->Callable
jax.experimental.jax2tf.tests.call_tf_test._named_test(**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/primitives_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitives_test.limitations->tuple(filter(lambda l: l.filter(device=device, dtype=harness.dtype), limitations))
A:jax.experimental.jax2tf.tests.primitives_test.device->jax._src.test_util.device_under_test()
A:jax.experimental.jax2tf.tests.primitives_test.args->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.primitives_test.enable_xla->harness.params.get('enable_xla', True)
A:jax.experimental.jax2tf.tests.primitives_test.associative_scan_reductions->harness.params.get('associative_scan_reductions', False)
A:jax.experimental.jax2tf.tests.primitives_test.tf_not_yet_impl->set(jax.experimental.jax2tf.jax2tf.tf_not_yet_impl)
A:jax.experimental.jax2tf.tests.primitives_test.all_primitives->tuple(sorted(all_primitives, key=str))
A:jax.experimental.jax2tf.tests.primitives_test.tfl->Jax2TfLimitation(description='Not implemented in JAX: ' + l.description, devices=l.devices, dtypes=l.dtypes, expect_tf_error=False, skip_tf_run=True)
A:jax.experimental.jax2tf.tests.primitives_test.tf_numerical_discrepancies_table->list(tf_error_table)
A:jax.experimental.jax2tf.tests.primitives_test.devices->', '.join(sorted(l.devices))
A:jax.experimental.jax2tf.tests.primitives_test.modes->', '.join(sorted(l.modes))
A:jax.experimental.jax2tf.tests.primitives_test.template->f.read()
A:jax.experimental.jax2tf.tests.primitives_test.output_file->os.path.join(os.path.dirname(__file__), '../g3doc/primitives_with_limited_support.md')
A:jax.experimental.jax2tf.tests.primitives_test.x->jax.numpy.array([-4, -3, -1, 0, 1, 3, 6])
A:jax.experimental.jax2tf.tests.primitives_test.y->numpy.int32(3)
A:jax.experimental.jax2tf.tests.primitives_test.expected->jax.numpy.floor_divide(x, y)
A:jax.experimental.jax2tf.tests.primitives_test.tf1_res->sess.run(jax2tf.convert(jnp.floor_divide)(x, y))
A:jax.experimental.jax2tf.tests.primitives_test.values->numpy.array([True, False, True], dtype=np.bool_)
A:jax.experimental.jax2tf.tests.primitives_test.indices->jax.numpy.array([[1, 1, 2], [0, 1, 0]])
A:jax.experimental.jax2tf.tests.primitives_test.f_jax->jax.jit(lambda v, u: getattr(v.at[::2, 3:], op)(u))
A:jax.experimental.jax2tf.tests.primitives_test.params->jax.numpy.array([[1.0, 1.5, 2.0], [2.0, 2.5, 3.0], [3.0, 3.5, 4.0]])
A:jax.experimental.jax2tf.tests.primitives_test.update->numpy.float32(6.0)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_boolean_gather(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_gather_rank_change(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_generate_limitations_doc(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_integer_div(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_prim(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_primitive_coverage(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_boolean_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_numerical_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_scatter_static(self,op)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_type_promotion(self,f_jax=jnp.add)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/converters.py----------------------------------------
A:jax.experimental.jax2tf.tests.converters.tf_fn->tensorflow.function(jax2tf_convert(harness, enable_xla=False), input_signature=harness.tf_input_signature, autograph=False)
A:jax.experimental.jax2tf.tests.converters.apply_tf->tensorflow.function(jax2tf_convert(harness, enable_xla=False), input_signature=harness.tf_input_signature, autograph=False).get_concrete_function()
A:jax.experimental.jax2tf.tests.converters.converter->tensorflow.lite.TFLiteConverter.from_concrete_functions([apply_tf], tf_fn)
A:jax.experimental.jax2tf.tests.converters.tflite_model->tensorflow.lite.TFLiteConverter.from_concrete_functions([apply_tf], tf_fn).convert()
A:jax.experimental.jax2tf.tests.converters.interpreter->tensorflow.lite.Interpreter(model_content=tflite_model)
A:jax.experimental.jax2tf.tests.converters.inputs->tensorflow.lite.Interpreter(model_content=tflite_model).get_input_details()
A:jax.experimental.jax2tf.tests.converters.output_details->tensorflow.lite.Interpreter(model_content=tflite_model).get_output_details()
A:jax.experimental.jax2tf.tests.converters.outputs->tuple((interpreter.tensor(out['index']) for out in output_details))
jax.experimental.jax2tf.tests.converters.Converter
jax.experimental.jax2tf.tests.converters.jax2tf_convert(harness:ModelHarness,enable_xla:bool=True)
jax.experimental.jax2tf.tests.converters.jax2tfjs(harness:ModelHarness)
jax.experimental.jax2tf.tests.converters.jax2tflite(harness:ModelHarness,use_flex_ops:bool=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/shape_poly_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.shape_poly_test.computed_sym->fun(*operands_sym)
A:jax.experimental.jax2tf.tests.shape_poly_test.dim_vars->jax.experimental.export.shape_poly.all_dim_vars(avals)
A:jax.experimental.jax2tf.tests.shape_poly_test.dim_vars_tuple->tuple(dim_vars)
A:jax.experimental.jax2tf.tests.shape_poly_test.compute_concrete->fun(*map(eval, operands_sym))
A:jax.experimental.jax2tf.tests.shape_poly_test.expected_concrete->eval(expected_sym)
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, b)->jax.experimental.export.shape_poly._parse_spec('a, b', (2, 3))
A:jax.experimental.jax2tf.tests.shape_poly_test.tshape->tensorflow.TensorShape([None, 3])
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, b, a1)->jax.experimental.export.shape_poly._parse_spec('a, b, a', (2, 3, 2))
A:jax.experimental.jax2tf.tests.shape_poly_test.(lb, ub)->atom.bounds()
A:jax.experimental.jax2tf.tests.shape_poly_test.atom_val->atom.evaluate(dict(a=a_val))
A:jax.experimental.jax2tf.tests.shape_poly_test.(d1, d2)->divmod(dividend, divisor)
A:jax.experimental.jax2tf.tests.shape_poly_test.(a,)->jax.experimental.export.shape_poly._parse_spec('a,', (2,))
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, stride)->jax.experimental.export.shape_poly._parse_spec('a, s', (2, 3))
A:jax.experimental.jax2tf.tests.shape_poly_test.other->PolyHarness(self.group_name, f'{self.name}_enable_xla_False', self.fun, arg_descriptors=self.arg_descriptors, polymorphic_shapes=self.polymorphic_shapes, input_signature=self.input_signature, expected_output_signature=self.expected_output_signature, expect_error=self.expect_error, tol=self.tol, enable_xla=False)
A:jax.experimental.jax2tf.tests.shape_poly_test.args->self.dyn_args_maker(tst.rng())
A:jax.experimental.jax2tf.tests.shape_poly_test.args_specs->jax.experimental.export.export.poly_specs(args, polymorphic_shapes)
A:jax.experimental.jax2tf.tests.shape_poly_test.f_tf->jax.experimental.jax2tf.convert(f_jax, native_serialization=True, polymorphic_shapes=['(t, )', '(3, 1)', '(t)'])
A:jax.experimental.jax2tf.tests.shape_poly_test.f_tf_func->tensorflow.function(f_tf, autograph=False, input_signature=input_signature)
A:jax.experimental.jax2tf.tests.shape_poly_test.concrete_f_tf->tensorflow.function(f_tf, autograph=False, input_signature=input_signature).get_concrete_function(*input_signature)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax->f_jax(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_tf->f_tf(operand, rand_idxs, slice_x)
A:jax.experimental.jax2tf.tests.shape_poly_test.max_lim->self.limitations[0].get_max_tolerance_limitation(self.limitations)
A:jax.experimental.jax2tf.tests.shape_poly_test.h->PolyHarness('', '', f_jax, arg_descriptors=arg_descriptors, skip_jax_run=skip_jax_run, polymorphic_shapes=polymorphic_shapes, input_signature=input_signature, expected_output_signature=expected_output_signature, expect_error=expect_error)
A:jax.experimental.jax2tf.tests.shape_poly_test.x->numpy.arange(np.prod((3, 5)), dtype=np.float32).reshape((3, 5))
A:jax.experimental.jax2tf.tests.shape_poly_test.y->numpy.arange(np.prod((4, 5)), dtype=np.float32).reshape((4, 5))
A:jax.experimental.jax2tf.tests.shape_poly_test.avals->tuple(map(shape_poly.arg_aval, arg_shapes, arg_dtypes, polymorphic_shapes))
A:jax.experimental.jax2tf.tests.shape_poly_test.(dim_values, _)->jax.experimental.jax2tf.jax2tf._interpret_fun_jax(partial(shape_poly.compute_dim_vars_from_arg_shapes, avals, args_kwargs_tree=tree_util.tree_flatten((avals, {}))[1]), args_tf, avals, '')
A:jax.experimental.jax2tf.tests.shape_poly_test.shape_env->f_tf(*[tf.ones(a_s, dtype=_f32) for a_s in arg_shapes])
A:jax.experimental.jax2tf.tests.shape_poly_test.arg->ad.make(rng)
A:jax.experimental.jax2tf.tests.shape_poly_test._->jax.experimental.jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'], native_serialization=True)(x45)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax_grad->jax.grad(lambda x: jnp.sum(f(x)))(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.xv->numpy.arange(24.0).reshape((2, 3, 4))
A:jax.experimental.jax2tf.tests.shape_poly_test.res_tf_grad->tape.gradient(res_tf, xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_tf, res_tf_grad)->tf_value_and_grad(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.tf_grad->tensorflow.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))
A:jax.experimental.jax2tf.tests.shape_poly_test.grad_tf->tape.gradient(res_tf, xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.xi->numpy.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape)
A:jax.experimental.jax2tf.tests.shape_poly_test.yf->numpy.arange(math.prod(x_shape), dtype=np.int16).reshape(x_shape).astype(np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.zb->numpy.array([True, False], dtype=np.bool_)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_tf, g_tf)->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf, args)
A:jax.experimental.jax2tf.tests.shape_poly_test.key->jax.random.PRNGKey(123)
A:jax.experimental.jax2tf.tests.shape_poly_test.broadcast_keys->jax.lax.broadcast_in_dim(key, x.shape, ())
A:jax.experimental.jax2tf.tests.shape_poly_test.gather_keys->jax.lax.broadcast_in_dim(broadcast_keys[0], (1, x.shape[1]), (1,))
A:jax.experimental.jax2tf.tests.shape_poly_test.slice_keys1->jax.lax.slice(broadcast_keys, (0, 0), (1, x.shape[1]), (1, 1))
A:jax.experimental.jax2tf.tests.shape_poly_test.slice_keys2->jax.lax.dynamic_slice(broadcast_keys, (0, 0), slice_sizes=(1, x.shape[1]))
A:jax.experimental.jax2tf.tests.shape_poly_test.upd1->jax.lax.dynamic_update_slice(slice_keys2, slice_keys1, start_indices=(0, 0))
A:jax.experimental.jax2tf.tests.shape_poly_test.counts->jax.numpy.arange(broadcast_keys.shape[0], dtype=np.int32)
A:jax.experimental.jax2tf.tests.shape_poly_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])
A:jax.experimental.jax2tf.tests.shape_poly_test.f_jax_rt->jax.experimental.jax2tf.call_tf(restored_f)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax_rt->f_jax_rt(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.four_ones->numpy.ones((4,))
A:jax.experimental.jax2tf.tests.shape_poly_test.x0->numpy.array([], np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.f1_tf->tensorflow.function(jax2tf.convert(f1_jax, polymorphic_shapes=['b'], native_serialization=False), autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.shape_poly_test.x45->numpy.ones((4, 5), dtype=np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.f2_tf->tensorflow.function(jax2tf.convert(f2_jax, polymorphic_shapes=['b, b'], native_serialization=False), autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_primal, res_tangent)->jax.experimental.jax2tf.convert(lambda x, xt: jax.jvp(f, (x,), (xt,)), polymorphic_shapes=['b', 'b'])(x, np.array([0.1, 0.2, 0.3]))
A:jax.experimental.jax2tf.tests.shape_poly_test.res_vmap->jax.vmap(f, in_axes=1)(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_iter->jax.numpy.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])
A:jax.experimental.jax2tf.tests.shape_poly_test.res_vmap_tf->jax.experimental.jax2tf.convert(jax.vmap(f, in_axes=1), polymorphic_shapes=['b1, b2, ...'])(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.orig_hash->getattr(shape_poly._DimExpr, '__hash__')
A:jax.experimental.jax2tf.tests.shape_poly_test.xs->numpy.ones((3, 5, 6), dtype=np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.f_toconvert->jax.vmap(pjit.pjit(f_jax))
A:jax.experimental.jax2tf.tests.shape_poly_test.res_1->jax.experimental.jax2tf.convert(f_toconvert)(xs)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_2->jax.experimental.jax2tf.convert(f_toconvert, polymorphic_shapes='b1, b2, ...')(xs)
A:jax.experimental.jax2tf.tests.shape_poly_test.res->f_jax(operand, rand_idxs, slice_x)
A:jax.experimental.jax2tf.tests.shape_poly_test.d0->jax.numpy.array(x.shape[0])
A:jax.experimental.jax2tf.tests.shape_poly_test.xy->numpy.ones((3, 5), dtype=np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.(z_spec, z_polymorphic_shape)->jax.experimental.jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)
A:jax.experimental.jax2tf.tests.shape_poly_test.z->jax.experimental.jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)
A:jax.experimental.jax2tf.tests.shape_poly_test.(zw_specs, zw_polymorphic_shapes)->jax.experimental.jax2tf.eval_polymorphic_shape(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)
A:jax.experimental.jax2tf.tests.shape_poly_test.(z, w)->jax.experimental.jax2tf.convert(f1, polymorphic_shapes=[x_polymorphic_shape, y_polymorphic_shape])(x, y)
A:jax.experimental.jax2tf.tests.shape_poly_test.operand->jax.numpy.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.rand_idxs->numpy.random.randint(0, high=max(operand.shape), size=(3, 1), dtype=np.int32)
A:jax.experimental.jax2tf.tests.shape_poly_test.slice_x->numpy.zeros((10,), dtype=jnp.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.dnums->jax.lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,))
A:jax.experimental.jax2tf.tests.shape_poly_test.limitations->jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.limitations_for_harness(h)
A:jax.experimental.jax2tf.tests.shape_poly_test.device->jax._src.test_util.device_under_test()
A:jax.experimental.jax2tf.tests.shape_poly_test.c->collections.Counter([h.dtype for h in hlist])
A:jax.experimental.jax2tf.tests.shape_poly_test.((dtype, _),)->collections.Counter([h.dtype for h in hlist]).most_common(1)
A:jax.experimental.jax2tf.tests.shape_poly_test.vmap_harness->PolyHarness('vmap_' + h.group_name, h.name, jax.vmap(h.dyn_fun, in_axes=0, out_axes=0), arg_descriptors=new_args, polymorphic_shapes=['b, ...'] * len(new_args), limitations=limitations)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.sampled_assert_equal(self,expected_sym:shape_poly.DimSize,fun:Callable,*operands_sym:shape_poly.DimSize)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_dilate_dim(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_dim_vars(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_dim_vars_symbolic_equal(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_evaluate(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_get_vars(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_non_negative_dim(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_parse_dim(self,dim_spec='-2*a^2*b+b^2',dim_poly=-2*a*a*b+b*b)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_parse_error(self,shape_spec='a+aa')
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_parse_mismatch_error(self,shape_spec='3',arg_shape=(4,))
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_parse_shape(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_bounds(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_compare(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_compare_overload(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_divmod(self,*,dividend,quotient,divisor,remainder)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_equal(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_poly_int_results(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimExprTest.test_stride_dim(self)
jax.experimental.jax2tf.tests.shape_poly_test.PolyHarness(self,group_name:str,name:str,fun:Callable,*,arg_descriptors:Sequence[primitive_harness.ArgDescriptor]=(),polymorphic_shapes:Sequence[Optional[str]]=(),input_signature:Optional[Sequence[tf.TensorSpec]]=None,expected_output_signature:Optional[tf.TensorSpec]=None,enable_xla:bool=True,expect_error:tuple[Optional[Any],Optional[str]]=(None,None),skip_jax_run:bool=False,check_result:bool=True,tol:Optional[float]=None,limitations:Sequence[Jax2TfLimitation]=(),override_jax_config_flags:dict[str,Any]={})
jax.experimental.jax2tf.tests.shape_poly_test.PolyHarness.__init__(self,group_name:str,name:str,fun:Callable,*,arg_descriptors:Sequence[primitive_harness.ArgDescriptor]=(),polymorphic_shapes:Sequence[Optional[str]]=(),input_signature:Optional[Sequence[tf.TensorSpec]]=None,expected_output_signature:Optional[tf.TensorSpec]=None,enable_xla:bool=True,expect_error:tuple[Optional[Any],Optional[str]]=(None,None),skip_jax_run:bool=False,check_result:bool=True,tol:Optional[float]=None,limitations:Sequence[Jax2TfLimitation]=(),override_jax_config_flags:dict[str,Any]={})
jax.experimental.jax2tf.tests.shape_poly_test.PolyHarness.both_enable_and_disable_xla(self)->tuple['PolyHarness', 'PolyHarness']
jax.experimental.jax2tf.tests.shape_poly_test.PolyHarness.run_test(self,tst:tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest.test_harness(self,harness:PolyHarness)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_arange(self,make_args)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_arange_error(self,make_args=lambdab:(0.0,b,2),expect_error=ValueError,expect_msg='mustbeeitherdimensionexpressionsorintegers')
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_arg_avals_errors(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_arg_avals_non_native(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_argmax(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_cond(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_dim_as_value_weak_type(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_dynamic_shapes(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_eval_poly_shapes(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_eval_poly_shapes_tuple_output(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_forgot_polymorphic_shapes_error(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_gather_1d(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_grad_int(self,with_function=False)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_grad_not_var_output(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_gradients_pytree(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_kwargs(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_mean0(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_non_trivial_dim_expr(self,expr=lambdad:d%-2)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_non_trivial_polynomials_spec(self,polymorphic_shapes='2*b1,4*b2,b1+b2+18')
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_poly_binary_op(self,*,op=op.add,other=np.arange(2,dtype=np.int32),other_jnp_array=False,swap=True)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_poly_unary_op(self,*,op=jnp.array)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_prng(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_pytree(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_pytree_errors(self,polymorphic_shapes=('b','b','b'))
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_readme_examples(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_reshape_compiled(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model_constant_gradient(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model_int_function(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_shape_as_array(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_shape_constraints_errors(self,*,shape,poly_spec:str,expect_error:Optional[str]=None)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_simple_binary(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_simple_unary(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_static_shape_result(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_unused_args(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_vmap_error(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_vmap_while(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_while(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_with_custom_vjp(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_with_hash_collision_vmap(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_with_nested_jit(self)
jax.experimental.jax2tf.tests.shape_poly_test._flatten_harnesses(harnesses)
jax.experimental.jax2tf.tests.shape_poly_test._get_jax2tf_limitations(device,h:primitive_harness.Harness)->Sequence[Jax2TfLimitation]
jax.experimental.jax2tf.tests.shape_poly_test._make_vmap_primitive_harnesses()->Sequence[PolyHarness]
jax.experimental.jax2tf.tests.shape_poly_test.check_shape_poly(tst,f_jax:Callable,*,arg_descriptors:Sequence[primitive_harness.ArgDescriptor]=(),skip_jax_run:bool=False,polymorphic_shapes:Sequence[Optional[str]]=(),input_signature:Optional[Sequence[tf.TensorSpec]]=None,expected_output_signature:Optional[tf.TensorSpec]=None,expect_error=(None,None))


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/model_harness.py----------------------------------------
A:jax.experimental.jax2tf.tests.model_harness.partial_fn->functools.partial(harness_fn, name=name, polymorphic_shapes=poly_shapes, tensor_spec=tensor_specs)
A:jax.experimental.jax2tf.tests.model_harness.model->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3)
A:jax.experimental.jax2tf.tests.model_harness.x->numpy.zeros((1, 8, 8, 3), np.float32)
A:jax.experimental.jax2tf.tests.model_harness.variables->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3).init(rng1, x, rng2)
A:jax.experimental.jax2tf.tests.model_harness.lengths->numpy.array([2, 3], np.int32)
A:jax.experimental.jax2tf.tests.model_harness.apply->functools.partial(model.apply, train=False)
A:jax.experimental.jax2tf.tests.model_harness.n_node->numpy.arange(3, 11)
A:jax.experimental.jax2tf.tests.model_harness.n_edge->numpy.arange(4, 12)
A:jax.experimental.jax2tf.tests.model_harness.total_n_node->numpy.sum(n_node)
A:jax.experimental.jax2tf.tests.model_harness.total_n_edge->numpy.sum(n_edge)
A:jax.experimental.jax2tf.tests.model_harness.graphs->_get_gnn_graphs()
A:jax.experimental.jax2tf.tests.model_harness.encoder_inputs->numpy.zeros((1, 2, 4), np.float32)
A:jax.experimental.jax2tf.tests.model_harness.decoder_inputs->numpy.zeros((1, 3, 4), np.float32)
A:jax.experimental.jax2tf.tests.model_harness.kwargs->dict(decode=True, deterministic=True, logits_via_embedding=False, share_embeddings=False)
A:jax.experimental.jax2tf.tests.model_harness.config->jax.experimental.jax2tf.tests.flax_models.transformer_wmt.TransformerConfig(**_full_transformer_kwargs())
A:jax.experimental.jax2tf.tests.model_harness.(rng1, rng2)->jax.random.split(random.PRNGKey(0))
A:jax.experimental.jax2tf.tests.model_harness.(output, _)->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3).apply(*args, mutable=['cache'])
jax.experimental.jax2tf.tests.model_harness.ModelHarness
jax.experimental.jax2tf.tests.model_harness.ModelHarness.__post_init__(self)
jax.experimental.jax2tf.tests.model_harness.ModelHarness.apply_with_vars(self,*args,**kwargs)
jax.experimental.jax2tf.tests.model_harness.ModelHarness.tf_input_signature(self)
jax.experimental.jax2tf.tests.model_harness._actor_critic_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._bilstm_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._cnn_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._full_transformer_kwargs()
jax.experimental.jax2tf.tests.model_harness._get_gnn_graphs()
jax.experimental.jax2tf.tests.model_harness._gnn_conv_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._gnn_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._make_harness(harness_fn,name,poly_shapes=None,tensor_specs=None)
jax.experimental.jax2tf.tests.model_harness._min_transformer_kwargs()
jax.experimental.jax2tf.tests.model_harness._resnet50_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._seq2seq_lstm_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._transformer_lm1b_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._transformer_nlp_seq_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._transformer_wmt_harness(name,**kwargs)
jax.experimental.jax2tf.tests.model_harness._vae_harness(name,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/tf_test_util.py----------------------------------------
A:jax.experimental.jax2tf.tests.tf_test_util.model_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))
A:jax.experimental.jax2tf.tests.tf_test_util.restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.tests.tf_test_util.model->tensorflow.train.Checkpoint()
A:jax.experimental.jax2tf.tests.tf_test_util.input_signature->tensorflow.nest.map_structure(lambda a: tf.TensorSpec(a.shape, a.dtype), input_args)
A:jax.experimental.jax2tf.tests.tf_test_util.model.f->tensorflow.function(f_tf, autograph=False, input_signature=input_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.restored->SaveAndLoadModel(model, save_gradients=save_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.res_dtype->numpy.result_type(res)
A:jax.experimental.jax2tf.tests.tf_test_util.cts->jax.tree_util.tree_map(make_ct, res_f_of_args)
A:jax.experimental.jax2tf.tests.tf_test_util.(res, pullback)->jax.vjp(f, *args)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_vars->tensorflow.nest.map_structure(tf.Variable, tf_args)
A:jax.experimental.jax2tf.tests.tf_test_util.res_tf->tf_f(*tf_vars)
A:jax.experimental.jax2tf.tests.tf_test_util.grad->tape.gradient(res_tf, tf_vars, unconnected_gradients=unconnected_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.(f1, args1)->TransformTfValueAndGrad(tf_f, tf_args, unconnected_gradients=unconnected_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.version->min(export.maximum_supported_serialization_version, tfxla.call_module_maximum_supported_version())
A:jax.experimental.jax2tf.tests.tf_test_util.result_jax->func_jax(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.func_tf->jax.experimental.jax2tf.convert(func_jax, enable_xla=enable_xla)
A:jax.experimental.jax2tf.tests.tf_test_util.jax2tf_limits->tuple(filter(lambda l: l.filter(mode=mode), limitations))
A:jax.experimental.jax2tf.tests.tf_test_util.result_tf->tensorflow.nest.map_structure(lambda t: t.numpy(), result_tf)
A:jax.experimental.jax2tf.tests.tf_test_util.jax_lowered->jax.jit(func_jax).lower(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_args_signature->_make_tf_input_signature(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_args_no_scalars->tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))
A:jax.experimental.jax2tf.tests.tf_test_util.tf_func_compiled->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_hlo->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature).experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')
A:jax.experimental.jax2tf.tests.tf_test_util.backend->jax._src.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.tf_test_util.modules->jax._src.xla_bridge.get_backend().compile(str(jax_lowered.compiler_ir())).hlo_modules()
A:jax.experimental.jax2tf.tests.tf_test_util.jax_opt_hlo->modules[0].to_string()
A:jax.experimental.jax2tf.tests.tf_test_util.tf_opt_hlo->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature).experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')
A:jax.experimental.jax2tf.tests.tf_test_util.t_arg->numpy.stack([arg] * 4)
A:jax.experimental.jax2tf.tests.tf_test_util.grad_func->jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))
A:jax.experimental.jax2tf.tests.tf_test_util.tf_function->tensorflow.function(tf_fun, autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf_graph->tensorflow.function(tf_fun, autograph=False).get_concrete_function(*args).graph.as_graph_def()
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf->tensorflow.function(jax2tf.convert(jax_fun, include_xla_op_metadata=include_xla_op_metadata), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf_concrete->tensorflow.function(jax2tf.convert(jax_fun, include_xla_op_metadata=include_xla_op_metadata), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)]).get_concrete_function(tf.convert_to_tensor(x))
A:jax.experimental.jax2tf.tests.tf_test_util.op_metadata->n.get_attr('_XlaOpMetadata')
A:jax.experimental.jax2tf.tests.tf_test_util.op_metadata_proto->tensorflow.compiler.xla.xla_data_pb2.OpMetadata()
A:jax.experimental.jax2tf.tests.tf_test_util.branch->getattr(n, f'_branch_graph_{idx}', None)
jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(tf_f:Callable,tf_args:Sequence,unconnected_gradients=tf.UnconnectedGradients.ZERO)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.CheckOpMetadata(self,jax_fun,x,expected:Sequence[OpMetadataGraph],include_xla_op_metadata=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.ConvertAndCompare(self,func_jax:Callable,*args,enable_xla:bool=True,limitations:Sequence=())
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.FindLargeTfConstants(self,tf_fun:Callable,*args,at_least=256)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.TfToHlo(self,tf_fun:Callable,*args)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.TransformConvertAndCompare(self,func:Callable,arg,transform:Optional[str])
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.setUp(self)
jax.experimental.jax2tf.tests.tf_test_util.OpMetadataGraph
jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf:Callable,*,input_signature:Optional[Sequence[tf.TensorSpec]]=None,input_args:Optional[Sequence[Any]]=None,variables:Sequence[tf.Variable]=(),save_gradients=True)->tuple[Callable, tf.train.Checkpoint]
jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model:tf.Module,save_gradients=True)->tf.Module
jax.experimental.jax2tf.tests.tf_test_util.TransformJaxVJP(f:Callable,args,res_f_of_args)
jax.experimental.jax2tf.tests.tf_test_util.TransformTfValueAndGrad(tf_f:Callable,tf_args,unconnected_gradients=tf.UnconnectedGradients.ZERO)
jax.experimental.jax2tf.tests.tf_test_util._make_tf_input_signature(*tf_args)->list[tf.TensorSpec]
jax.experimental.jax2tf.tests.tf_test_util._run_tf_function(func_tf:Callable,*tf_args,mode:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_main_test.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_main_test.FLAGS.model_path->os.path.join(absltest.get_default_test_tmpdir(), 'saved_models')
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.setUp(self)
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.test_train_and_save_features(self,model='mnist_flax')
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.test_train_and_save_full(self,model='mnist_flax',serving_batch_size=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/keras_reuse_main.py----------------------------------------
A:jax.experimental.jax2tf.examples.keras_reuse_main.(tf_accelerator, _)->jax.experimental.jax2tf.examples.saved_model_main.tf_accelerator_and_tolerances()
A:jax.experimental.jax2tf.examples.keras_reuse_main.feature_model_dir->jax.experimental.jax2tf.examples.saved_model_main.savedmodel_dir()
A:jax.experimental.jax2tf.examples.keras_reuse_main.strategy->tensorflow.distribute.OneDeviceStrategy(tf_accelerator)
A:jax.experimental.jax2tf.examples.keras_reuse_main.images->tensorflow.keras.layers.Input(mnist_lib.input_shape, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.keras_reuse_main.keras_feature_extractor->tensorflow_hub.KerasLayer(feature_model_dir, trainable=True)
A:jax.experimental.jax2tf.examples.keras_reuse_main.features->keras_feature_extractor(images)
A:jax.experimental.jax2tf.examples.keras_reuse_main.predictor->tensorflow.keras.layers.Dense(10, activation='softmax')
A:jax.experimental.jax2tf.examples.keras_reuse_main.predictions->predictor(features)
A:jax.experimental.jax2tf.examples.keras_reuse_main.keras_model->tensorflow.keras.Model(images, predictions)
A:jax.experimental.jax2tf.examples.keras_reuse_main.train_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TRAIN, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.keras_reuse_main.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=mnist_lib.test_batch_size)
jax.experimental.jax2tf.examples.keras_reuse_main.main(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/keras_reuse_main_test.py----------------------------------------
A:jax.experimental.jax2tf.examples.keras_reuse_main_test.FLAGS.model_path->os.path.join(absltest.get_default_test_tmpdir(), 'saved_models')
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest.setUp(self)
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest.test_keras_reuse(self,model='mnist_pure_jax')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_lib.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_lib.tf_fn->jax.experimental.jax2tf.convert(jax_fn, with_gradient=with_gradient, polymorphic_shapes=[None, polymorphic_shapes], enable_xla=enable_xla)
A:jax.experimental.jax2tf.examples.saved_model_lib.param_vars->tensorflow.nest.map_structure(lambda param: tf.Variable(param, trainable=with_gradient), params)
A:jax.experimental.jax2tf.examples.saved_model_lib.tf_graph->tensorflow.function(lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=compile_model)
A:jax.experimental.jax2tf.examples.saved_model_lib.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]->tensorflow.function(lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=compile_model).get_concrete_function(input_signatures[0])
A:jax.experimental.jax2tf.examples.saved_model_lib.wrapper->_ReusableSavedModelWrapper(tf_graph, param_vars)
A:jax.experimental.jax2tf.examples.saved_model_lib.saved_model_options->tensorflow.saved_model.SaveOptions(experimental_custom_gradients=True)
A:jax.experimental.jax2tf.examples.saved_model_lib.self.variables->tensorflow.nest.flatten(param_vars)
jax.experimental.jax2tf.examples.saved_model_lib._ReusableSavedModelWrapper(self,tf_graph,param_vars)
jax.experimental.jax2tf.examples.saved_model_lib._ReusableSavedModelWrapper.__init__(self,tf_graph,param_vars)
jax.experimental.jax2tf.examples.saved_model_lib.convert_and_save_model(jax_fn:Callable[[Any,Any],Any],params,model_dir:str,*,input_signatures:Sequence[tf.TensorSpec],polymorphic_shapes:Optional[Union[str,jax2tf.PolyShape]]=None,with_gradient:bool=False,enable_xla:bool=True,compile_model:bool=True,saved_model_options:Optional[tf.saved_model.SaveOptions]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_main.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_main._MODEL->absl.flags.DEFINE_enum('model', 'mnist_flax', ['mnist_flax', 'mnist_pure_jax'], 'Which model to use.')
A:jax.experimental.jax2tf.examples.saved_model_main._MODEL_CLASSIFIER_LAYER->absl.flags.DEFINE_boolean('model_classifier_layer', True, 'The model should include the classifier layer, or just the last layer of logits. Set this to False when you want to reuse the classifier-less model in a larger model. See keras_reuse_main.py and README.md.')
A:jax.experimental.jax2tf.examples.saved_model_main._MODEL_PATH->absl.flags.DEFINE_string('model_path', '/tmp/jax2tf/saved_models', 'Path under which to save the SavedModel.')
A:jax.experimental.jax2tf.examples.saved_model_main._MODEL_VERSION->absl.flags.DEFINE_integer('model_version', 1, 'The version number for the SavedModel. Needed for serving, larger versions will take precedence', lower_bound=1)
A:jax.experimental.jax2tf.examples.saved_model_main._SERVING_BATCH_SIZE->absl.flags.DEFINE_integer('serving_batch_size', 1, 'For what batch size to prepare the serving signature. Use -1 for converting and saving with batch polymorphism.')
A:jax.experimental.jax2tf.examples.saved_model_main._NUM_EPOCHS->absl.flags.DEFINE_integer('num_epochs', 3, 'For how many epochs to train.', lower_bound=1)
A:jax.experimental.jax2tf.examples.saved_model_main._GENERATE_MODEL->absl.flags.DEFINE_boolean('generate_model', True, 'Train and save a new model. Otherwise, use an existing SavedModel.')
A:jax.experimental.jax2tf.examples.saved_model_main._COMPILE_MODEL->absl.flags.DEFINE_boolean('compile_model', True, 'Enable TensorFlow jit_compiler for the SavedModel. This is necessary if you want to use the model for TensorFlow serving.')
A:jax.experimental.jax2tf.examples.saved_model_main._SHOW_MODEL->absl.flags.DEFINE_boolean('show_model', True, 'Show details of saved SavedModel.')
A:jax.experimental.jax2tf.examples.saved_model_main.SHOW_IMAGES->absl.flags.DEFINE_boolean('show_images', False, 'Plot some sample images with labels and inference results.')
A:jax.experimental.jax2tf.examples.saved_model_main._TEST_SAVEDMODEL->absl.flags.DEFINE_boolean('test_savedmodel', True, 'Test TensorFlow inference using the SavedModel w.r.t. the JAX model.')
A:jax.experimental.jax2tf.examples.saved_model_main.train_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TRAIN, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.saved_model_main.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=mnist_lib.test_batch_size)
A:jax.experimental.jax2tf.examples.saved_model_main.the_model_class->pick_model_class()
A:jax.experimental.jax2tf.examples.saved_model_main.model_dir->os.path.join(model_dir, str(_MODEL_VERSION.value))
A:jax.experimental.jax2tf.examples.saved_model_main.model_descr->model_description()
A:jax.experimental.jax2tf.examples.saved_model_main.(predict_fn, predict_params)->pick_model_class().train(train_ds, test_ds, num_epochs=_NUM_EPOCHS.value, with_classifier=_MODEL_CLASSIFIER_LAYER.value)
A:jax.experimental.jax2tf.examples.saved_model_main.(tf_accelerator, tolerances)->tf_accelerator_and_tolerances()
A:jax.experimental.jax2tf.examples.saved_model_main.pure_restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.examples.saved_model_main.test_input->numpy.ones((mnist_lib.test_batch_size,) + mnist_lib.input_shape, dtype=np.float32)
A:jax.experimental.jax2tf.examples.saved_model_main.tolerances->dict(atol=1e-05, rtol=1e-05)
jax.experimental.jax2tf.examples.saved_model_main.model_description()->str
jax.experimental.jax2tf.examples.saved_model_main.pick_model_class()
jax.experimental.jax2tf.examples.saved_model_main.savedmodel_dir(with_version:bool=True)->str
jax.experimental.jax2tf.examples.saved_model_main.tf_accelerator_and_tolerances()
jax.experimental.jax2tf.examples.saved_model_main.train_and_save()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/mnist_lib.py----------------------------------------
A:jax.experimental.jax2tf.examples.mnist_lib._MOCK_DATA->absl.flags.DEFINE_boolean('mock_data', False, 'Use fake data, for testing.')
A:jax.experimental.jax2tf.examples.mnist_lib.ds->ds.cache().shuffle(1000).batch(batch_size, drop_remainder=True).cache().shuffle(1000).batch(batch_size, drop_remainder=True)
A:jax.experimental.jax2tf.examples.mnist_lib.m->re.search('metadata files were not found in (.+/)mnist/', str(e))
A:jax.experimental.jax2tf.examples.mnist_lib.label->tensorflow.one_hot(x['label'], 10)
A:jax.experimental.jax2tf.examples.mnist_lib.x->flax.linen.log_softmax(x)
A:jax.experimental.jax2tf.examples.mnist_lib.predictions->FlaxMNIST.predict(params, inputs, with_classifier=True)
A:jax.experimental.jax2tf.examples.mnist_lib.target_class->jax.numpy.argmax(labels, axis=1)
A:jax.experimental.jax2tf.examples.mnist_lib.predicted_class->jax.numpy.argmax(predict(params, inputs), axis=1)
A:jax.experimental.jax2tf.examples.mnist_lib.grads->jax.grad(PureJaxMNIST.loss)(params, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.rng->jax.random.PRNGKey(0)
A:jax.experimental.jax2tf.examples.mnist_lib.start_time->time.time()
A:jax.experimental.jax2tf.examples.mnist_lib.params->optax.apply_updates(params, updates)
A:jax.experimental.jax2tf.examples.mnist_lib.train_acc->PureJaxMNIST.accuracy(FlaxMNIST.predict, params, train_ds)
A:jax.experimental.jax2tf.examples.mnist_lib.test_acc->PureJaxMNIST.accuracy(FlaxMNIST.predict, params, test_ds)
A:jax.experimental.jax2tf.examples.mnist_lib.model->Module()
A:jax.experimental.jax2tf.examples.mnist_lib.grad->jax.grad(FlaxMNIST.loss)(params, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.(updates, opt_state)->optax.sgd(learning_rate=step_size, momentum=momentum_mass).update(grad, opt_state)
A:jax.experimental.jax2tf.examples.mnist_lib.init_shape->jax.numpy.ones((1,) + input_shape, jnp.float32)
A:jax.experimental.jax2tf.examples.mnist_lib.tx->optax.sgd(learning_rate=step_size, momentum=momentum_mass)
A:jax.experimental.jax2tf.examples.mnist_lib.opt_state->optax.sgd(learning_rate=step_size, momentum=momentum_mass).init(params)
A:jax.experimental.jax2tf.examples.mnist_lib.(params, opt_state)->jax.jit(FlaxMNIST.update, static_argnums=0)(tx, params, opt_state, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.predict_fn->functools.partial(FlaxMNIST.predict, with_classifier=with_classifier)
A:jax.experimental.jax2tf.examples.mnist_lib.fig->matplotlib.pyplot.figure(figsize=(8.0, 4.0), num=title)
A:jax.experimental.jax2tf.examples.mnist_lib.((images, labels),)->list(tfds.as_numpy(ds.take(1)))
A:jax.experimental.jax2tf.examples.mnist_lib.inferred_labels->inference_fn(images)
A:jax.experimental.jax2tf.examples.mnist_lib.digit->matplotlib.pyplot.figure(figsize=(8.0, 4.0), num=title).add_subplot(nr_rows, nr_cols, i + 1)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.Module(self,x,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.Module.__call__(self,x,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.loss(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.predict(params,inputs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.train(train_ds,test_ds,num_epochs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.update(tx,params,opt_state,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.accuracy(predict:Callable,params,dataset)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.loss(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.predict(params:Sequence[tuple[Any,Any]],inputs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.train(train_ds,test_ds,num_epochs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.update(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.load_mnist(split:tfds.Split,batch_size:int)
jax.experimental.jax2tf.examples.mnist_lib.plot_images(ds,nr_rows:int,nr_cols:int,title:str,inference_fn:Optional[Callable]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/serving/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/serving/model_server_request.py----------------------------------------
A:jax.experimental.jax2tf.examples.serving.model_server_request._USE_GRPC->absl.flags.DEFINE_boolean('use_grpc', True, 'Use the gRPC API (default), or the HTTP REST API.')
A:jax.experimental.jax2tf.examples.serving.model_server_request._MODEL_SPEC_NAME->absl.flags.DEFINE_string('model_spec_name', '', 'The name you used to export your model to model server (e.g., mnist_flax).')
A:jax.experimental.jax2tf.examples.serving.model_server_request._PREDICTION_SERVICE_ADDR->absl.flags.DEFINE_string('prediction_service_addr', 'localhost:8500', 'Stubby endpoint for the prediction service. If you serve your model locally using TensorFlow model server, then you can use "localhost:8500"for the gRPC server and "localhost:8501" for the HTTP REST server.')
A:jax.experimental.jax2tf.examples.serving.model_server_request._SERVING_BATCH_SIZE->absl.flags.DEFINE_integer('serving_batch_size', 1, 'Batch size for the serving request. Must match the batch size at which the model was saved. Must divide --count_images', lower_bound=1)
A:jax.experimental.jax2tf.examples.serving.model_server_request._COUNT_IMAGES->absl.flags.DEFINE_integer('count_images', 16, 'How many images to test.', lower_bound=1)
A:jax.experimental.jax2tf.examples.serving.model_server_request.channel->grpc.insecure_channel(_PREDICTION_SERVICE_ADDR.value)
A:jax.experimental.jax2tf.examples.serving.model_server_request.stub->tensorflow_serving.apis.prediction_service_pb2_grpc.PredictionServiceStub(channel)
A:jax.experimental.jax2tf.examples.serving.model_server_request.request->tensorflow_serving.apis.predict_pb2.PredictRequest()
A:jax.experimental.jax2tf.examples.serving.model_server_request.response->requests.post(predict_url, data=data)
A:jax.experimental.jax2tf.examples.serving.model_server_request.(outputs,)->requests.post(predict_url, data=data).outputs.values()
A:jax.experimental.jax2tf.examples.serving.model_server_request.images_json->json.dumps(images.tolist())
A:jax.experimental.jax2tf.examples.serving.model_server_request.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=_SERVING_BATCH_SIZE.value)
A:jax.experimental.jax2tf.examples.serving.model_server_request.images_and_labels->tensorflow_datasets.as_numpy(test_ds.take(_COUNT_IMAGES.value // _SERVING_BATCH_SIZE.value))
A:jax.experimental.jax2tf.examples.serving.model_server_request.predictions_one_hot->serving_call_mnist(images)
A:jax.experimental.jax2tf.examples.serving.model_server_request.predictions_digit->numpy.argmax(predictions_one_hot, axis=1)
A:jax.experimental.jax2tf.examples.serving.model_server_request.labels_digit->numpy.argmax(labels, axis=1)
jax.experimental.jax2tf.examples.serving.model_server_request.main(_)
jax.experimental.jax2tf.examples.serving.model_server_request.serving_call_mnist(images)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/ad.py----------------------------------------
A:jax.experimental.sparse.ad.argnums_tup->_ensure_index_tuple(argnums)
A:jax.experimental.sparse.ad.(args_flat1, tree1)->jax.tree_util.tree_flatten(args, is_leaf=is_sparse)
A:jax.experimental.sparse.ad.(*leaf_argnums1, end)->split_list(range(tree1.num_leaves), [child.num_leaves for child in tree1.children()])
A:jax.experimental.sparse.ad.argnums_flat1->list(itertools.chain.from_iterable((nums for (i, nums) in enumerate(leaf_argnums1) if i in argnums_tup)))
A:jax.experimental.sparse.ad.(args_flat, tree2)->jax.tree_util.tree_flatten(args_flat1)
A:jax.experimental.sparse.ad.(*leaf_argnums2, end)->split_list(range(tree2.num_leaves), [child.num_leaves for child in tree2.children()])
A:jax.experimental.sparse.ad.argnums_flat->tuple(itertools.chain.from_iterable((nums for (i, nums) in enumerate(leaf_argnums2) if i in argnums_flat1)))
A:jax.experimental.sparse.ad.args->jax.tree_util.tree_unflatten(tree1, tree_util.tree_unflatten(tree2, args_flat))
A:jax.experimental.sparse.ad.(bufs, tree)->jax.tree_util.tree_flatten(args_flat1[i])
A:jax.experimental.sparse.ad.f_recons->jax.vmap(f_recons)
A:jax.experimental.sparse.ad.raw_value_and_grad_fun->jax.value_and_grad(fun, argnums=argnums, has_aux=has_aux, **kwargs)
A:jax.experimental.sparse.ad.argnums->jax._src.core.concrete_or_error(_ensure_index, argnums)
A:jax.experimental.sparse.ad.(fun_flat, argnums_flat, args_flat, postprocess_gradients)->flatten_fun_for_sparse_ad(fun, argnums, args)
A:jax.experimental.sparse.ad.(val_out, grad_out)->jax.value_and_grad(fun_flat, argnums=argnums_flat, has_aux=has_aux, **kwargs)(*args_flat)
A:jax.experimental.sparse.ad.raw_grad_fun->jax.grad(fun, argnums=argnums, **kwargs)
A:jax.experimental.sparse.ad.out->jax.jacrev(fun_flat, argnums=argnums_flat, has_aux=has_aux, **kwargs)(*args_flat)
A:jax.experimental.sparse.ad.raw_jacfwd_fun->jax.jacfwd(fun, argnums=argnums, **kwargs)
A:jax.experimental.sparse.ad.raw_jacrev_fun->jax.jacrev(fun, argnums=argnums, **kwargs)
jax.experimental.sparse.ad.flatten_fun_for_sparse_ad(fun,argnums:Union[int,tuple[int]],args:tuple[Any])
jax.experimental.sparse.ad.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable
jax.experimental.sparse.ad.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,**kwargs)->Callable
jax.experimental.sparse.ad.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,**kwargs)->Callable
jax.experimental.sparse.ad.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable[..., tuple[Any, Any]]
jax.experimental.sparse.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable
jax.experimental.sparse.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,**kwargs)->Callable
jax.experimental.sparse.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,**kwargs)->Callable
jax.experimental.sparse.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable[..., tuple[Any, Any]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/bcsr.py----------------------------------------
A:jax.experimental.sparse.bcsr.batch_size->max((arg.shape[dim] for (arg, dim) in zip(batched_args, batch_dims) if dim is not None))
A:jax.experimental.sparse.bcsr.batched_spinfo->SparseInfo((batch_size, *spinfo.shape), indices_sorted=spinfo.indices_sorted, unique_indices=spinfo.unique_indices)
A:jax.experimental.sparse.bcsr.shape->tuple(shape)
A:jax.experimental.sparse.bcsr.props->_validate_bcsr(lhs_data_aval, lhs_indices_aval, lhs_indptr_aval, lhs_spinfo.shape)
A:jax.experimental.sparse.bcsr.(n_batch, _, _)->_validate_bcsr_indices(indices, indptr, shape)
A:jax.experimental.sparse.bcsr.csr_to_coo->nfold_vmap(_csr_to_coo, n_batch)
A:jax.experimental.sparse.bcsr.(n_batch, n_sparse, _, _)->jax.experimental.sparse.bcoo._validate_bcoo_indices(indices, shape)
A:jax.experimental.sparse.bcsr.indptr->jax.numpy.asarray(mat.indptr).astype(index_dtype or jnp.int32)
A:jax.experimental.sparse.bcsr.bcsr_fromdense_p->jax._src.core.Primitive('bcsr_fromdense')
A:jax.experimental.sparse.bcsr.mat_array->jax.numpy.asarray(mat)
A:jax.experimental.sparse.bcsr.nse->property(lambda self: self.indices.shape[-1])
A:jax.experimental.sparse.bcsr.mat->mat.tocsr().tocsr()
A:jax.experimental.sparse.bcsr.bcoo_mat->jax.experimental.sparse.bcoo.bcoo_fromdense(mat, nse=nse, index_dtype=index_dtype, n_dense=n_dense, n_batch=n_batch)
A:jax.experimental.sparse.bcsr.(indices, indptr)->_bcoo_to_bcsr(arr.indices, shape=arr.shape)
A:jax.experimental.sparse.bcsr.primals_out->_bcsr_fromdense(M, nse=nse, n_batch=n_batch, n_dense=n_dense, index_dtype=index_dtype)
A:jax.experimental.sparse.bcsr.data_dot->bcsr_extract(indices, indptr, Mdot)
A:jax.experimental.sparse.bcsr.bcsr_todense_p->jax._src.core.Primitive('bcsr_todense')
A:jax.experimental.sparse.bcsr.bcoo_indices->_bcsr_to_bcoo(indices, indptr, shape=mat.shape)
A:jax.experimental.sparse.bcsr.(data, indices, indptr, spinfo)->_bcsr_batch_dims_to_front(batched_args, batch_dims, spinfo)
A:jax.experimental.sparse.bcsr.bcsr_extract_p->jax._src.core.Primitive('bcsr_extract')
A:jax.experimental.sparse.bcsr.(n_batch, n_dense, nse)->_validate_bcsr_indices(indices, indptr, mat.shape)
A:jax.experimental.sparse.bcsr.bdim->next(iter(bdim_set))
A:jax.experimental.sparse.bcsr.indices->jax.numpy.asarray(mat.indices).astype(index_dtype or jnp.int32)
A:jax.experimental.sparse.bcsr.result_shape->list(arr.shape)
A:jax.experimental.sparse.bcsr.arr->arr.sort_indices().sort_indices()
A:jax.experimental.sparse.bcsr.bcsr_dot_general_p->jax._src.core.Primitive('bcsr_dot_general')
A:jax.experimental.sparse.bcsr.lhs_data->jax.numpy.asarray(lhs_data)
A:jax.experimental.sparse.bcsr.lhs_bcsr_indices->jax.numpy.asarray(lhs_indices)
A:jax.experimental.sparse.bcsr.lhs_bcsr_indptr->jax.numpy.asarray(lhs_indptr)
A:jax.experimental.sparse.bcsr.rhs->jax.numpy.asarray(rhs)
A:jax.experimental.sparse.bcsr.lhs_bcoo_indices->_bcsr_to_bcoo(lhs_indices, lhs_indptr, shape=lhs_spinfo.shape)
A:jax.experimental.sparse.bcsr.out_aval->jax.eval_shape(partial(lax.dot_general, dimension_numbers=dimension_numbers, preferred_element_type=preferred_element_type), jax.ShapeDtypeStruct(lhs_spinfo.shape, lhs_data.dtype), jax.ShapeDtypeStruct(rhs.shape, rhs.dtype))
A:jax.experimental.sparse.bcsr.(data_out, _, rhs_out)->jax.experimental.sparse.bcoo._bcoo_dot_general_transpose(ct, lhs_data, lhs_bcoo_indices, rhs, dimension_numbers=dimension_numbers, preferred_element_type=preferred_element_type, lhs_spinfo=lhs_spinfo)
A:jax.experimental.sparse.bcsr.(*new_lhs_args, new_lhs_spinfo)->_bcsr_batch_dims_to_front(lhs_args, lhs_dims, lhs_spinfo, batch_size=None if rhs_bdim is None else rhs.shape[rhs_bdim])
A:jax.experimental.sparse.bcsr.(new_dimension_numbers, result_batch_dim)->_dot_general_batch_dim_nums((len(lhs_spinfo.shape), rhs.ndim), (0, rhs_bdim), dimension_numbers)
A:jax.experimental.sparse.bcsr.batched_out->_bcsr_dot_general(*new_lhs_args, rhs, lhs_spinfo=new_lhs_spinfo, dimension_numbers=new_dimension_numbers, preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcsr.f->partial(_bcsr_correct_out_of_bound_indices, rhs=rhs, shape=shape[props.n_batch:])
A:jax.experimental.sparse.bcsr.i_data->jax.lax.broadcasted_iota(indptr.dtype, data.shape, 0)
A:jax.experimental.sparse.bcsr.data->jax.numpy.asarray(mat.data)
A:jax.experimental.sparse.bcsr.i_indices->jax.lax.broadcasted_iota(indptr.dtype, indices.shape, 0)
A:jax.experimental.sparse.bcsr._bcsr_correct_out_of_bound_indices_lowered->jax._src.interpreters.mlir.lower_fun(_bcsr_correct_out_of_bound_indices, multiple_results=True)
A:jax.experimental.sparse.bcsr.((lhs_data,), (lhs_indices,))->_bcsr_correct_out_of_bound_indices_lowered(ctx, lhs_data, lhs_indices, lhs_indptr, rhs, shape=lhs_spinfo.shape)
A:jax.experimental.sparse.bcsr._bcsr_dot_general_default_lowering->jax._src.interpreters.mlir.lower_fun(_bcsr_dot_general_impl, multiple_results=False)
A:jax.experimental.sparse.bcsr.result_bcoo->jax.experimental.sparse.bcoo.bcoo_broadcast_in_dim(mat.to_bcoo(), shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax.experimental.sparse.bcsr.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.bcsr.n_batch->property(lambda self: self.indices.ndim - 1)
A:jax.experimental.sparse.bcsr.n_sparse->property(lambda _: 2)
A:jax.experimental.sparse.bcsr.n_dense->property(lambda self: self.data.ndim - self.indices.ndim)
A:jax.experimental.sparse.bcsr._bufs->property(lambda self: (self.data, self.indices, self.indptr))
A:jax.experimental.sparse.bcsr._info->property(lambda self: SparseInfo(self.shape, self.indices_sorted, self.unique_indices))
A:jax.experimental.sparse.bcsr.(self.data, self.indices, self.indptr)->map(jnp.asarray, args)
A:jax.experimental.sparse.bcsr.obj->object.__new__(cls)
A:jax.experimental.sparse.bcsr.(batch_shape, sparse_shape, dense_shape)->split_list(shape, [n_batch, n_sparse])
A:jax.experimental.sparse.bcsr.coo_indices->_bcsr_to_bcoo(self.indices, self.indptr, shape=self.shape)
jax.experimental.sparse.BCSR(self,args:tuple[Array,Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.BCSR.__repr__(self)
jax.experimental.sparse.BCSR._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.BCSR._sparse_shape(self)
jax.experimental.sparse.BCSR.from_bcoo(cls,arr:bcoo.BCOO)->BCSR
jax.experimental.sparse.BCSR.from_scipy_sparse(cls,mat,*,index_dtype=None,n_dense=0,n_batch=0)
jax.experimental.sparse.BCSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.BCSR.sum_duplicates(self,nse:int|None=None,remove_zeros:bool=True)->BCSR
jax.experimental.sparse.BCSR.to_bcoo(self)->bcoo.BCOO
jax.experimental.sparse.BCSR.todense(self)
jax.experimental.sparse.BCSR.transpose(self,*args,**kwargs)
jax.experimental.sparse.BCSR.tree_flatten(self)
jax.experimental.sparse.BCSR.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.BCSRProperties(NamedTuple)
jax.experimental.sparse.bcsr.BCSR(self,args:tuple[Array,Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.bcsr.BCSR.__init__(self,args:tuple[Array,Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.bcsr.BCSR.__repr__(self)
jax.experimental.sparse.bcsr.BCSR._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.bcsr.BCSR._sparse_shape(self)
jax.experimental.sparse.bcsr.BCSR.from_bcoo(cls,arr:bcoo.BCOO)->BCSR
jax.experimental.sparse.bcsr.BCSR.from_scipy_sparse(cls,mat,*,index_dtype=None,n_dense=0,n_batch=0)
jax.experimental.sparse.bcsr.BCSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.bcsr.BCSR.sum_duplicates(self,nse:int|None=None,remove_zeros:bool=True)->BCSR
jax.experimental.sparse.bcsr.BCSR.to_bcoo(self)->bcoo.BCOO
jax.experimental.sparse.bcsr.BCSR.todense(self)
jax.experimental.sparse.bcsr.BCSR.transpose(self,*args,**kwargs)
jax.experimental.sparse.bcsr.BCSR.tree_flatten(self)
jax.experimental.sparse.bcsr.BCSR.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.bcsr.BCSRProperties(NamedTuple)
jax.experimental.sparse.bcsr._bcoo_to_bcsr(indices:Array,*,shape:Sequence[int],index_dtype:DTypeLike=jnp.int32)->tuple[Array, Array]
jax.experimental.sparse.bcsr._bcsr_batch_dims_to_front(batched_args,batch_dims,spinfo,batch_size=None)
jax.experimental.sparse.bcsr._bcsr_correct_out_of_bound_indices(data,indices,indptr,rhs,*,shape)
jax.experimental.sparse.bcsr._bcsr_dot_general(lhs_data:jax.Array,lhs_indices:jax.Array,lhs_indptr:jax.Array,rhs:Array,*,dimension_numbers:DotDimensionNumbers,preferred_element_type:Any,lhs_spinfo:SparseInfo)->Array
jax.experimental.sparse.bcsr._bcsr_dot_general_abstract_eval(lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_gpu_lowering(csr_matvec_lowering,csr_matmat_lowering,ctx,lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_impl(lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_jvp_lhs(lhs_data_dot,lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_jvp_rhs(rhs_dot,lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_dot_general_transpose(ct,lhs_data,lhs_indices,lhs_indptr,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcsr._bcsr_extract_abstract_eval(indices,indptr,mat)
jax.experimental.sparse.bcsr._bcsr_extract_batching_rule(batched_args,batch_dims)
jax.experimental.sparse.bcsr._bcsr_extract_impl(indices,indptr,mat)
jax.experimental.sparse.bcsr._bcsr_extract_jvp(arr_dot,indices,indptr,arr)
jax.experimental.sparse.bcsr._bcsr_extract_transpose(ct,indices,indptr,arr)
jax.experimental.sparse.bcsr._bcsr_from_elt(cont,axis_size,elt,axis)
jax.experimental.sparse.bcsr._bcsr_fromdense(mat:ArrayLike,*,nse:int,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->tuple[Array, Array, Array]
jax.experimental.sparse.bcsr._bcsr_fromdense_abstract_eval(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_fromdense_batching_rule(batched_args,batch_dims,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_fromdense_impl(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_fromdense_jvp(primals,tangents,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_fromdense_transpose(ct,M,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_to_bcoo(indices:jax.Array,indptr:jax.Array,*,shape:Sequence[int])->jax.Array
jax.experimental.sparse.bcsr._bcsr_to_elt(cont,_,val,axis)
jax.experimental.sparse.bcsr._bcsr_todense(data:ArrayLike,indices:ArrayLike,indptr:ArrayLike,*,spinfo:SparseInfo)->Array
jax.experimental.sparse.bcsr._bcsr_todense_abstract_eval(data,indices,indptr,*,spinfo)
jax.experimental.sparse.bcsr._bcsr_todense_batching_rule(batched_args,batch_dims,*,spinfo)
jax.experimental.sparse.bcsr._bcsr_todense_impl(data,indices,indptr,*,spinfo)
jax.experimental.sparse.bcsr._bcsr_todense_jvp(data_dot,data,indices,indptr,*,spinfo)
jax.experimental.sparse.bcsr._bcsr_todense_transpose(ct,data,indices,indptr,*,spinfo)
jax.experimental.sparse.bcsr._compatible(shape1:Sequence[int],shape2:Sequence[int])->bool
jax.experimental.sparse.bcsr._validate_bcsr(data:jax.Array,indices:jax.Array,indptr:jax.Array,shape:Sequence[int])->BCSRProperties
jax.experimental.sparse.bcsr._validate_bcsr_indices(indices:jax.Array,indptr:jax.Array,shape:Sequence[int])->BCSRProperties
jax.experimental.sparse.bcsr.bcsr_broadcast_in_dim(mat:BCSR,*,shape:Shape,broadcast_dimensions:Sequence[int])->BCSR
jax.experimental.sparse.bcsr.bcsr_concatenate(operands:Sequence[BCSR],*,dimension:int)->BCSR
jax.experimental.sparse.bcsr.bcsr_dot_general(lhs:BCSR|Array,rhs:Array,*,dimension_numbers:DotDimensionNumbers,precision:None=None,preferred_element_type:None=None)->Array
jax.experimental.sparse.bcsr.bcsr_eliminate_zeros(mat:BCSR,nse:int|None=None)->BCSR
jax.experimental.sparse.bcsr.bcsr_extract(indices:ArrayLike,indptr:ArrayLike,mat:ArrayLike)->Array
jax.experimental.sparse.bcsr.bcsr_fromdense(mat:ArrayLike,*,nse:int|None=None,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->BCSR
jax.experimental.sparse.bcsr.bcsr_sum_duplicates(mat:BCSR,nse:int|None=None)->BCSR
jax.experimental.sparse.bcsr.bcsr_todense(mat:BCSR)->Array
jax.experimental.sparse.bcsr_broadcast_in_dim(mat:BCSR,*,shape:Shape,broadcast_dimensions:Sequence[int])->BCSR
jax.experimental.sparse.bcsr_concatenate(operands:Sequence[BCSR],*,dimension:int)->BCSR
jax.experimental.sparse.bcsr_dot_general(lhs:BCSR|Array,rhs:Array,*,dimension_numbers:DotDimensionNumbers,precision:None=None,preferred_element_type:None=None)->Array
jax.experimental.sparse.bcsr_extract(indices:ArrayLike,indptr:ArrayLike,mat:ArrayLike)->Array
jax.experimental.sparse.bcsr_fromdense(mat:ArrayLike,*,nse:int|None=None,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->BCSR
jax.experimental.sparse.bcsr_sum_duplicates(mat:BCSR,nse:int|None=None)->BCSR
jax.experimental.sparse.bcsr_todense(mat:BCSR)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/csr.py----------------------------------------
A:jax.experimental.sparse.csr.nse->jax._src.core.concrete_or_error(operator.index, nse, 'nse argument of csr_fromdense()')
A:jax.experimental.sparse.csr.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.csr._bufs->property(lambda self: (self.data, self.indices, self.indptr))
A:jax.experimental.sparse.csr.(self.data, self.indices, self.indptr)->map(jnp.asarray, args)
A:jax.experimental.sparse.csr.shape->tuple(shape)
A:jax.experimental.sparse.csr.data->jax._src.core.ShapedArray((nse,), mat.dtype)
A:jax.experimental.sparse.csr.indices->jax._src.core.ShapedArray((nse,), index_dtype)
A:jax.experimental.sparse.csr.indptr->jax._src.core.ShapedArray((mat.shape[0] + 1,), index_dtype)
A:jax.experimental.sparse.csr.diag_size->min(N + k, M)
A:jax.experimental.sparse.csr.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.csr.zero->_const(idx, 0)
A:jax.experimental.sparse.csr.k->_const(idx, k)
A:jax.experimental.sparse.csr.col->jax.lax.add(idx, lax.cond(k <= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.csr.row->jax.numpy.where(true_nonzeros, row, m)
A:jax.experimental.sparse.csr.other->jax.numpy.asarray(other)
A:jax.experimental.sparse.csr.(data, other)->promote_dtypes(self.data, other)
A:jax.experimental.sparse.csr.obj->object.__new__(cls)
A:jax.experimental.sparse.csr.csr_todense_p->jax._src.core.Primitive('csr_todense')
A:jax.experimental.sparse.csr._csr_todense_lowering->jax.interpreters.mlir.lower_fun(_csr_todense_impl, multiple_results=False)
A:jax.experimental.sparse.csr.csr_fromdense_p->jax._src.core.Primitive('csr_fromdense')
A:jax.experimental.sparse.csr.nse_int->jax._src.core.concrete_or_error(operator.index, nse, 'coo_fromdense nse argument')
A:jax.experimental.sparse.csr.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.csr.(row, col)->_csr_to_coo(indices, indptr)
A:jax.experimental.sparse.csr._csr_fromdense_lowering->jax.interpreters.mlir.lower_fun(_csr_fromdense_impl, multiple_results=True)
A:jax.experimental.sparse.csr.(data, indices, indptr)->csr_fromdense_hlo(mat, nnz=nse, index_dtype=np.dtype(index_dtype), data_dtype=dtype, index_type=mlir.dtype_to_ir_type(np.dtype(index_dtype)))
A:jax.experimental.sparse.csr.primals_out->_csr_fromdense(M, nse=nse, index_dtype=index_dtype)
A:jax.experimental.sparse.csr.data_dot->_csr_extract(indices, indptr, Mdot)
A:jax.experimental.sparse.csr.csr_matvec_p->jax._src.core.Primitive('csr_matvec')
A:jax.experimental.sparse.csr._csr_matvec_lowering->jax.interpreters.mlir.lower_fun(_csr_matvec_impl, multiple_results=False)
A:jax.experimental.sparse.csr.v->jax.numpy.asarray(v)
A:jax.experimental.sparse.csr.csr_matmat_p->jax._src.core.Primitive('csr_matmat')
A:jax.experimental.sparse.csr._csr_matmat_lowering->jax.interpreters.mlir.lower_fun(_csr_matmat_impl, multiple_results=False)
A:jax.experimental.sparse.csr.B->jax.numpy.asarray(B)
jax.experimental.sparse.CSC(self,args,*,shape)
jax.experimental.sparse.CSC.__matmul__(self,other)
jax.experimental.sparse.CSC._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSC._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSC.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.CSC.todense(self)
jax.experimental.sparse.CSC.transpose(self,axes=None)
jax.experimental.sparse.CSC.tree_flatten(self)
jax.experimental.sparse.CSC.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.CSR(self,args,*,shape)
jax.experimental.sparse.CSR.__matmul__(self,other)
jax.experimental.sparse.CSR._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSR._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.CSR.todense(self)
jax.experimental.sparse.CSR.transpose(self,axes=None)
jax.experimental.sparse.CSR.tree_flatten(self)
jax.experimental.sparse.CSR.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.csr.CSC(self,args,*,shape)
jax.experimental.sparse.csr.CSC.__init__(self,args,*,shape)
jax.experimental.sparse.csr.CSC.__matmul__(self,other)
jax.experimental.sparse.csr.CSC._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSC._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSC.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.csr.CSC.todense(self)
jax.experimental.sparse.csr.CSC.transpose(self,axes=None)
jax.experimental.sparse.csr.CSC.tree_flatten(self)
jax.experimental.sparse.csr.CSC.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.csr.CSR(self,args,*,shape)
jax.experimental.sparse.csr.CSR.__init__(self,args,*,shape)
jax.experimental.sparse.csr.CSR.__matmul__(self,other)
jax.experimental.sparse.csr.CSR._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSR._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.csr.CSR.todense(self)
jax.experimental.sparse.csr.CSR.transpose(self,axes=None)
jax.experimental.sparse.csr.CSR.tree_flatten(self)
jax.experimental.sparse.csr.CSR.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.csr._csr_fromdense(mat:Array,*,nse:int,index_dtype:DTypeLike=np.int32)->tuple[Array, Array, Array]
jax.experimental.sparse.csr._csr_fromdense_abstract_eval(mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_gpu_lowering(csr_fromdense_hlo,ctx,mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_impl(mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_jvp(primals,tangents,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_transpose(ct,M,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_matmat(data:Array,indices:Array,indptr:Array,B:Array,*,shape:Shape,transpose:bool=False)->Array
jax.experimental.sparse.csr._csr_matmat_abstract_eval(data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_gpu_lowering(csr_matmat_hlo,ctx,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_impl(data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_jvp_left(data_dot,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_jvp_right(B_dot,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_transpose(ct,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec(data,indices,indptr,v,*,shape,transpose=False)
jax.experimental.sparse.csr._csr_matvec_abstract_eval(data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_gpu_lowering(csr_matvec_hlo,ctx,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_impl(data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_jvp_mat(data_dot,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_jvp_vec(v_dot,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_transpose(ct,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_todense(data:Array,indices:Array,indptr:Array,*,shape:Shape)->Array
jax.experimental.sparse.csr._csr_todense_abstract_eval(data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_gpu_lowering(csr_todense_hlo,ctx,data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_impl(data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_jvp(data_dot,data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_transpose(ct,data,indices,indptr,*,shape)
jax.experimental.sparse.csr.csr_fromdense(mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32)->CSR
jax.experimental.sparse.csr.csr_matmat(mat:CSR,B:Array,*,transpose:bool=False)->Array
jax.experimental.sparse.csr.csr_matvec(mat:CSR,v:Array,transpose:bool=False)->Array
jax.experimental.sparse.csr.csr_todense(mat:CSR)->Array
jax.experimental.sparse.csr_fromdense(mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32)->CSR
jax.experimental.sparse.csr_matmat(mat:CSR,B:Array,*,transpose:bool=False)->Array
jax.experimental.sparse.csr_matvec(mat:CSR,v:Array,transpose:bool=False)->Array
jax.experimental.sparse.csr_todense(mat:CSR)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/coo.py----------------------------------------
A:jax.experimental.sparse.coo.nse->jax._src.core.concrete_or_error(operator.index, nse, 'nse argument of coo_fromdense()')
A:jax.experimental.sparse.coo.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.coo._info->property(lambda self: COOInfo(shape=self.shape, rows_sorted=self._rows_sorted, cols_sorted=self._cols_sorted))
A:jax.experimental.sparse.coo._bufs->property(lambda self: (self.data, self.row, self.col))
A:jax.experimental.sparse.coo.(self.data, self.row, self.col)->map(jnp.asarray, args)
A:jax.experimental.sparse.coo.(row, col, data)->jax.lax.sort((self.row, self.col, self.data), num_keys=2)
A:jax.experimental.sparse.coo.shape->tuple(shape)
A:jax.experimental.sparse.coo.data->jax._src.core.ShapedArray((nse,), mat.dtype)
A:jax.experimental.sparse.coo.rowcol->jax._src.core.ShapedArray((nse,), index_dtype)
A:jax.experimental.sparse.coo.diag_size->min(N + k, M)
A:jax.experimental.sparse.coo.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.coo.zero->_const(idx, 0)
A:jax.experimental.sparse.coo.k->_const(idx, k)
A:jax.experimental.sparse.coo.row->jax.lax.sub(idx, lax.cond(k >= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.coo.col->jax.lax.add(idx, lax.cond(k <= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.coo.obj->object.__new__(cls)
A:jax.experimental.sparse.coo.other->jax.numpy.asarray(other)
A:jax.experimental.sparse.coo.(data, other)->promote_dtypes(self.data, other)
A:jax.experimental.sparse.coo.self_promoted->COO((data, self.row, self.col), **self._info._asdict())
A:jax.experimental.sparse.coo.coo_todense_p->jax._src.core.Primitive('coo_todense')
A:jax.experimental.sparse.coo._coo_todense_lowering->jax.interpreters.mlir.lower_fun(_coo_todense_impl, multiple_results=False)
A:jax.experimental.sparse.coo.result->coo_todense_hlo(data, row, col, shape=shape, data_dtype=dtype, index_dtype=row_aval.dtype)
A:jax.experimental.sparse.coo.coo_fromdense_p->jax._src.core.Primitive('coo_fromdense')
A:jax.experimental.sparse.coo.nse_int->jax._src.core.concrete_or_error(operator.index, nse, 'coo_fromdense nse argument')
A:jax.experimental.sparse.coo.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.coo.(row, col)->jax.numpy.nonzero(mat, size=nse)
A:jax.experimental.sparse.coo._coo_fromdense_lowering->jax.interpreters.mlir.lower_fun(_coo_fromdense_impl, multiple_results=True)
A:jax.experimental.sparse.coo.(data, row, col)->coo_fromdense_hlo(mat, nnz=nse, data_dtype=dtype, index_dtype=np.dtype(index_dtype), index_type=mlir.dtype_to_ir_type(np.dtype(index_dtype)))
A:jax.experimental.sparse.coo.primals_out->_coo_fromdense(M, nse=nse, index_dtype=index_dtype)
A:jax.experimental.sparse.coo.data_dot->_coo_extract(row, col, Mdot)
A:jax.experimental.sparse.coo.coo_matvec_p->jax._src.core.Primitive('coo_matvec')
A:jax.experimental.sparse.coo.v->jax.numpy.asarray(v)
A:jax.experimental.sparse.coo._coo_matvec_lowering->jax.interpreters.mlir.lower_fun(_coo_matvec_impl, multiple_results=False)
A:jax.experimental.sparse.coo.coo_matmat_p->jax._src.core.Primitive('coo_matmat')
A:jax.experimental.sparse.coo.B->jax.numpy.asarray(B)
A:jax.experimental.sparse.coo._coo_matmat_lowering->jax.interpreters.mlir.lower_fun(_coo_matmat_impl, multiple_results=False)
jax.experimental.sparse.COO(self,args:tuple[Array,Array,Array],*,shape:Shape,rows_sorted:bool=False,cols_sorted:bool=False)
jax.experimental.sparse.COO.__matmul__(self,other:ArrayLike)->Array
jax.experimental.sparse.COO._empty(cls,shape:Sequence[int],*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32')->COO
jax.experimental.sparse.COO._eye(cls,N:int,M:int,k:int,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32')->COO
jax.experimental.sparse.COO._sort_indices(self)->COO
jax.experimental.sparse.COO.fromdense(cls,mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32)->COO
jax.experimental.sparse.COO.todense(self)->Array
jax.experimental.sparse.COO.transpose(self,axes:tuple[int,...]|None=None)->COO
jax.experimental.sparse.COO.tree_flatten(self)->tuple[tuple[Array, Array, Array], dict[str, Any]]
jax.experimental.sparse.COO.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.COOInfo(NamedTuple)
jax.experimental.sparse.coo.COO(self,args:tuple[Array,Array,Array],*,shape:Shape,rows_sorted:bool=False,cols_sorted:bool=False)
jax.experimental.sparse.coo.COO.__init__(self,args:tuple[Array,Array,Array],*,shape:Shape,rows_sorted:bool=False,cols_sorted:bool=False)
jax.experimental.sparse.coo.COO.__matmul__(self,other:ArrayLike)->Array
jax.experimental.sparse.coo.COO._empty(cls,shape:Sequence[int],*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32')->COO
jax.experimental.sparse.coo.COO._eye(cls,N:int,M:int,k:int,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32')->COO
jax.experimental.sparse.coo.COO._sort_indices(self)->COO
jax.experimental.sparse.coo.COO.fromdense(cls,mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32)->COO
jax.experimental.sparse.coo.COO.todense(self)->Array
jax.experimental.sparse.coo.COO.transpose(self,axes:tuple[int,...]|None=None)->COO
jax.experimental.sparse.coo.COO.tree_flatten(self)->tuple[tuple[Array, Array, Array], dict[str, Any]]
jax.experimental.sparse.coo.COO.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.coo.COOInfo(NamedTuple)
jax.experimental.sparse.coo._coo_fromdense(mat:Array,*,nse:int,index_dtype:DTypeLike=jnp.int32)->tuple[Array, Array, Array]
jax.experimental.sparse.coo._coo_fromdense_abstract_eval(mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_gpu_lowering(coo_fromdense_hlo,ctx,mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_impl(mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_jvp(primals,tangents,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_transpose(ct,M,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_matmat(data:Array,row:Array,col:Array,B:Array,*,spinfo:COOInfo,transpose:bool=False)->Array
jax.experimental.sparse.coo._coo_matmat_abstract_eval(data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_gpu_lowering(coo_matmat_hlo,ctx,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_impl(data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_jvp_left(data_dot,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_jvp_right(B_dot,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_transpose(ct,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec(data:Array,row:Array,col:Array,v:Array,*,spinfo:COOInfo,transpose:bool=False)->Array
jax.experimental.sparse.coo._coo_matvec_abstract_eval(data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_gpu_lowering(coo_matvec_hlo,ctx,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_impl(data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_jvp_mat(data_dot,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_jvp_vec(v_dot,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_transpose(ct,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_todense(data:Array,row:Array,col:Array,*,spinfo:COOInfo)->Array
jax.experimental.sparse.coo._coo_todense_abstract_eval(data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_gpu_lowering(coo_todense_hlo,ctx,data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_impl(data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_jvp(data_dot,data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_transpose(ct,data,row,col,*,spinfo)
jax.experimental.sparse.coo.coo_fromdense(mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=jnp.int32)->COO
jax.experimental.sparse.coo.coo_matmat(mat:COO,B:Array,*,transpose:bool=False)->Array
jax.experimental.sparse.coo.coo_matvec(mat:COO,v:Array,transpose:bool=False)->Array
jax.experimental.sparse.coo.coo_todense(mat:COO)->Array
jax.experimental.sparse.coo_fromdense(mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=jnp.int32)->COO
jax.experimental.sparse.coo_matmat(mat:COO,B:Array,*,transpose:bool=False)->Array
jax.experimental.sparse.coo_matvec(mat:COO,v:Array,transpose:bool=False)->Array
jax.experimental.sparse.coo_todense(mat:COO)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/api.py----------------------------------------
A:jax.experimental.sparse.api.todense_p->jax._src.core.Primitive('todense')
A:jax.experimental.sparse.api.(bufs, tree)->jax.tree_util.tree_flatten(arr)
A:jax.experimental.sparse.api.arr->jax.tree_util.tree_unflatten(tree, bufs)
A:jax.experimental.sparse.api.primals_out->jax._src.core.Primitive('todense').bind(*primals, tree=tree)
A:jax.experimental.sparse.api.tangents_out->jax._src.core.Primitive('todense').bind(tangents[0], *primals[1:], tree=tree)
A:jax.experimental.sparse.api.standin->object()
A:jax.experimental.sparse.api.obj->jax.tree_util.tree_unflatten(tree, [standin] * len(bufs))
A:jax.experimental.sparse.api.N->jax._src.core.concrete_or_error(operator.index, N)
A:jax.experimental.sparse.api.M->jax._src.core.concrete_or_error(operator.index, M)
A:jax.experimental.sparse.api.k->jax._src.core.concrete_or_error(operator.index, k)
jax.experimental.sparse.api._todense_abstract_eval(*bufs,tree)
jax.experimental.sparse.api._todense_batching_rule(batched_args,batch_dims,*,tree)
jax.experimental.sparse.api._todense_impl(*bufs,tree)
jax.experimental.sparse.api._todense_jvp(primals,tangents,*,tree)
jax.experimental.sparse.api._todense_transpose(ct,*bufs,tree)
jax.experimental.sparse.api.empty(shape:Shape,dtype:Optional[DTypeLike]=None,index_dtype:DTypeLike='int32',sparse_format:str='bcoo',**kwds)->JAXSparse
jax.experimental.sparse.api.eye(N:int,M:Optional[int]=None,k:int=0,dtype:Optional[DTypeLike]=None,index_dtype:DTypeLike='int32',sparse_format:str='bcoo',**kwds)->JAXSparse
jax.experimental.sparse.api.todense(arr:Union[JAXSparse,Array])->Array
jax.experimental.sparse.empty(shape:Shape,dtype:Optional[DTypeLike]=None,index_dtype:DTypeLike='int32',sparse_format:str='bcoo',**kwds)->JAXSparse
jax.experimental.sparse.eye(N:int,M:Optional[int]=None,k:int=0,dtype:Optional[DTypeLike]=None,index_dtype:DTypeLike='int32',sparse_format:str='bcoo',**kwds)->JAXSparse
jax.experimental.sparse.todense(arr:Union[JAXSparse,Array])->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/_lowerings.py----------------------------------------
A:jax.experimental.sparse._lowerings.coo_spmv_p->jax.core.Primitive('coo_spmv')
A:jax.experimental.sparse._lowerings.coo_spmm_p->jax.core.Primitive('coo_spmm')
A:jax.experimental.sparse._lowerings.csr_spmv_p->jax.core.Primitive('csr_spmv')
A:jax.experimental.sparse._lowerings.csr_spmm_p->jax.core.Primitive('csr_spmm')
jax.experimental.sparse._lowerings._coo_spmm_abstract_eval(data,row,col,x,*,transpose,shape)
jax.experimental.sparse._lowerings._coo_spmm_gpu_lowering(ctx,data,row,col,x,*,transpose,shape)
jax.experimental.sparse._lowerings._coo_spmv_abstract_eval(data,row,col,x,*,transpose,shape)
jax.experimental.sparse._lowerings._coo_spmv_gpu_lowering(ctx,data,row,col,x,*,transpose,shape)
jax.experimental.sparse._lowerings._csr_spmm_abstract_eval(data,indices,indptr,x,*,transpose,shape)
jax.experimental.sparse._lowerings._csr_spmm_gpu_lowering(ctx,data,indices,indptr,x,*,transpose,shape)
jax.experimental.sparse._lowerings._csr_spmv_abstract_eval(data,indices,indptr,x,*,transpose,shape)
jax.experimental.sparse._lowerings._csr_spmv_gpu_lowering(ctx,data,indices,indptr,x,*,transpose,shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/bcoo.py----------------------------------------
A:jax.experimental.sparse.bcoo.batch_size->math.prod(batch_shape)
A:jax.experimental.sparse.bcoo.batched_spinfo->SparseInfo((batch_size, *spinfo.shape), indices_sorted=spinfo.indices_sorted, unique_indices=spinfo.unique_indices)
A:jax.experimental.sparse.bcoo.nse->property(lambda self: self.indices.shape[-2])
A:jax.experimental.sparse.bcoo.data->data.at[M - abs(k):].set(0).at[M - abs(k):].set(0)
A:jax.experimental.sparse.bcoo.indices->indices.at[M - abs(k)].set(M).at[M - abs(k)].set(M)
A:jax.experimental.sparse.bcoo.props->_validate_bcoo(data, indices, shape)
A:jax.experimental.sparse.bcoo.mask->jax.numpy.all(lhs_indices[:, None, dims] == rhs_indices[None, :, dims], -1)
A:jax.experimental.sparse.bcoo.dims_to_contract->tuple((i for (i, s) in enumerate(indices.shape[:props.n_batch]) if s == 1))
A:jax.experimental.sparse.bcoo.fill_value->jax.numpy.expand_dims(jnp.array(shape[n_batch:n_batch + n_sparse], dtype=indices.dtype), range(indices.ndim - 1))
A:jax.experimental.sparse.bcoo.shape->tuple(shape)
A:jax.experimental.sparse.bcoo.bcoo_todense_p->jax._src.core.Primitive('bcoo_todense')
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, _, _)->_validate_bcoo(lhs_data, lhs_indices, lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.ind_slices->tuple((np.zeros(s, int) if i_s == 1 else np.arange(s) for (s, i_s) in zip(arr.shape[:props.n_batch], indices.shape[:props.n_batch])))
A:jax.experimental.sparse.bcoo.grid->numpy.meshgrid(*batch_slices, np.arange(1), indexing='ij', sparse=True)
A:jax.experimental.sparse.bcoo.sparse_ind->tuple((indices[grid + (slice(None), i)] for i in range(props.n_sparse)))
A:jax.experimental.sparse.bcoo.batch_slices->tuple((np.arange(s) for s in arr.shape[:props.n_batch]))
A:jax.experimental.sparse.bcoo.(data, indices, spinfo)->_bcoo_batch_dims_to_front(batched_args, batch_dims, spinfo)
A:jax.experimental.sparse.bcoo.bcoo_fromdense_p->jax._src.core.Primitive('bcoo_fromdense')
A:jax.experimental.sparse.bcoo.mat->mat.tocoo().tocoo()
A:jax.experimental.sparse.bcoo.nse_int->jax._src.core.concrete_or_error(operator.index, nse, _TRACED_NSE_ERROR)
A:jax.experimental.sparse.bcoo.primals_out->_bcoo_spdot_general(*primals, **kwds)
A:jax.experimental.sparse.bcoo.data_dot->data_dot.sum(props.n_batch, keepdims=True).sum(props.n_batch, keepdims=True)
A:jax.experimental.sparse.bcoo.bcoo_extract_p->jax._src.core.Primitive('bcoo_extract')
A:jax.experimental.sparse.bcoo.a->jax.numpy.asarray(arr)
A:jax.experimental.sparse.bcoo.arr->jax.lax.broadcast_in_dim(arr, result_shape, (bdim,))
A:jax.experimental.sparse.bcoo.(indices, sort_ind)->_unique_indices(indices, shape=arr.shape, return_index=True)
A:jax.experimental.sparse.bcoo.result->bcoo_transpose(result, permutation=tuple(permutation))
A:jax.experimental.sparse.bcoo.out_shape->tuple((shape[i] for i in range(len(shape)) if i not in axes))
A:jax.experimental.sparse.bcoo.f->partial(_coo_correct_out_of_bound_indices, shape=shape[row.ndim:], transpose=transpose)
A:jax.experimental.sparse.bcoo._->jax.eval_shape(partial(lax.rev, dimensions=dimensions), jax.ShapeDtypeStruct(operand.shape, operand.dtype))
A:jax.experimental.sparse.bcoo.(n_batch, _, n_dense, nse)->_validate_bcoo_indices(indices, arr.shape)
A:jax.experimental.sparse.bcoo.result_shape->list(arr.shape)
A:jax.experimental.sparse.bcoo.bcoo_transpose_p->jax._src.core.Primitive('bcoo_transpose')
A:jax.experimental.sparse.bcoo.buffers->_bcoo_transpose(mat.data, mat.indices, permutation=permutation, spinfo=mat._info)
A:jax.experimental.sparse.bcoo.permutation->numpy.zeros(result.ndim, dtype=int)
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, n_dense, _)->_validate_bcoo(lhs_data, lhs_indices, lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.(batch_perm, sparse_perm, dense_perm)->_validate_permutation(mat.data, mat.indices, dimensions or tuple(range(mat.ndim)), mat.shape)
A:jax.experimental.sparse.bcoo.n_batch->property(lambda self: self.indices.ndim - 2)
A:jax.experimental.sparse.bcoo.(batch_perm, _, dense_perm)->_validate_permutation(data, indices, permutation, spinfo.shape)
A:jax.experimental.sparse.bcoo.(data_dot_out, _)->_bcoo_transpose(data_dot, indices, permutation=permutation, spinfo=spinfo)
A:jax.experimental.sparse.bcoo.ct_spinfo->SparseInfo(tuple((spinfo.shape[p] for p in permutation)))
A:jax.experimental.sparse.bcoo.rev_permutation->list(np.argsort(permutation))
A:jax.experimental.sparse.bcoo.dummy_indices->jax.numpy.zeros([1 for i in range(indices.ndim - 2)] + list(indices.shape[-2:]), dtype=int)
A:jax.experimental.sparse.bcoo.(data_trans, _)->_bcoo_transpose(data_ct, dummy_indices, permutation=rev_permutation, spinfo=ct_spinfo)
A:jax.experimental.sparse.bcoo.(data, indices)->_mul(lhs_data, lhs_indices, rhs_data, rhs_indices)
A:jax.experimental.sparse.bcoo.bcoo_dot_general_p->jax._src.core.Primitive('bcoo_dot_general')
A:jax.experimental.sparse.bcoo.bufs->_bcoo_spdot_general(lhs.data, lhs.indices, rhs.data, rhs.indices, lhs_spinfo=lhs._info, rhs_spinfo=rhs._info, dimension_numbers=dimension_numbers, preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcoo.preferred_element_type->numpy.dtype(preferred_element_type)
A:jax.experimental.sparse.bcoo.lhs_data->lhs_data.transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_data.ndim)]).transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_data.ndim)])
A:jax.experimental.sparse.bcoo.lhs_indices->lhs_indices.transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_indices.ndim)]).transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_indices.ndim)])
A:jax.experimental.sparse.bcoo.rhs->_validate_bcoo(rhs_data, rhs_indices, rhs_shape)
A:jax.experimental.sparse.bcoo.out_aval->jax.eval_shape(partial(lax.gather, **kwds), jax.ShapeDtypeStruct(operand.shape, operand.dtype), jax.ShapeDtypeStruct(start_indices.shape, start_indices.dtype))
A:jax.experimental.sparse.bcoo.(lhs_contracting_b, rhs_contracting_b)->unzip2([(l, r) for (l, r) in safe_zip(lhs_contracting, rhs_contracting) if l < n_batch])
A:jax.experimental.sparse.bcoo.(lhs_contracting_s, rhs_contracting_s)->unzip2([(l, r) for (l, r) in safe_zip(lhs_contracting, rhs_contracting) if l >= n_batch])
A:jax.experimental.sparse.bcoo.lhs_contracting_s->tuple((d - n_batch for d in lhs_contracting_s))
A:jax.experimental.sparse.bcoo.sparse_perm->jax.numpy.array([*lhs_contracting_s, *remaining(range(n_sparse), lhs_contracting_s)])
A:jax.experimental.sparse.bcoo.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.bcoo.batch_dims->tuple((dim for dim in range(len(out_aval.shape)) if dim not in offset_dims))
A:jax.experimental.sparse.bcoo.prod->jax.lax.dot_general(lhs_data, rhs.at[idx_right].get(mode='fill', fill_value=0), (([], []), (batch_dims, batch_dims)), preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcoo.out_array->jax.numpy.zeros(out_aval.shape, out_aval.dtype)
A:jax.experimental.sparse.bcoo._bcoo_dot_general_default_lowering->jax._src.interpreters.mlir.lower_fun(_bcoo_dot_general_impl, multiple_results=False)
A:jax.experimental.sparse.bcoo.(row, col, shape)->_coo_correct_out_of_bound_indices(row, col, shape, transpose)
A:jax.experimental.sparse.bcoo.out->_unique(indices, axis=0, return_inverse=return_inverse, return_index=return_index, return_true_size=return_true_size, size=props.nse, fill_value=fill_value)
A:jax.experimental.sparse.bcoo._bcoo_dot_general_gpu_lowering->jax._src.interpreters.mlir.lower_fun(_bcoo_dot_general_gpu_impl, multiple_results=False)
A:jax.experimental.sparse.bcoo.lhs_ndim->len(lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.lhs_kept->remaining(range(lhs_ndim), lhs_contract, lhs_batch)
A:jax.experimental.sparse.bcoo.rhs_kept->remaining(range(rhs_ndim), rhs_contract, rhs_batch)
A:jax.experimental.sparse.bcoo.(ans_batch, ans_lhs, ans_rhs)->map(list, ranges_like(lhs_batch, lhs_kept, rhs_kept))
A:jax.experimental.sparse.bcoo.lhs_contract_sorted_by_rhs->list(np.take(lhs_contract, np.argsort(rhs_contract)))
A:jax.experimental.sparse.bcoo.out_axes->list(np.argsort(list(rhs_batch) + rhs_contract_sorted_by_lhs + rhs_kept))
A:jax.experimental.sparse.bcoo.placeholder_data->jax.numpy.empty((lhs_indices.ndim - 2) * (1,) + (lhs_indices.shape[-2],))
A:jax.experimental.sparse.bcoo.(_, lhs_indices_T)->_bcoo_transpose(placeholder_data, lhs_indices, permutation=permutation, spinfo=SparseInfo(placeholder_shape))
A:jax.experimental.sparse.bcoo.result_T_shape->tuple((placeholder_shape[i] for i in permutation))
A:jax.experimental.sparse.bcoo.result_T->bcoo_dot_general_sampled(ct, rhs, lhs_indices_T, dimension_numbers=dims)
A:jax.experimental.sparse.bcoo.(result, _)->_bcoo_transpose(result_T, lhs_indices_T, permutation=out_axes, spinfo=SparseInfo(result_T_shape))
A:jax.experimental.sparse.bcoo.out_dense_T->jax.lax.dot_general(ct, rhs, dimension_numbers=dims)
A:jax.experimental.sparse.bcoo.out_dense->jax.lax.transpose(out_dense_T, out_axes)
A:jax.experimental.sparse.bcoo.rhs_contract_sorted_by_lhs->list(np.take(rhs_contract, np.argsort(lhs_contract)))
A:jax.experimental.sparse.bcoo.(new_lhs_data, new_lhs_indices, new_lhs_spinfo)->_bcoo_batch_dims_to_front(batched_args[:2], batch_dims[:2], lhs_spinfo, batch_size=None if rhs_bdim is None else rhs.shape[rhs_bdim])
A:jax.experimental.sparse.bcoo.(new_dimension_numbers, result_batch_dim)->_dot_general_batch_dim_nums((len(lhs_spinfo.shape), rhs.ndim), (0, rhs_bdim), dimension_numbers)
A:jax.experimental.sparse.bcoo.batched_out->_bcoo_spdot_general(lhs_data, lhs_indices, rhs_data, rhs_indices, dimension_numbers=dimension_numbers, lhs_spinfo=lhs_spinfo, rhs_spinfo=rhs_spinfo, preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcoo.bcoo_dot_general_sampled_p->jax._src.core.Primitive('bcoo_dot_general_sampled')
A:jax.experimental.sparse.bcoo.A->jax.numpy.asarray(A)
A:jax.experimental.sparse.bcoo.B->jax.numpy.asarray(B)
A:jax.experimental.sparse.bcoo.(dense_result,)->jax._src.interpreters.partial_eval.abstract_eval_fun(lambda *args: [lax.dot_general(*args, dimension_numbers=dimension_numbers)], A, B)
A:jax.experimental.sparse.bcoo.(sparse_result,)->jax._src.interpreters.partial_eval.abstract_eval_fun(lambda *args: [_bcoo_extract(*args)], indices, dense_result)
A:jax.experimental.sparse.bcoo.mat_shape->_dot_general_validated_shape(A_shape, B_shape, dimension_numbers)
A:jax.experimental.sparse.bcoo.(indices, ct)->_bcoo_extract_transpose(ct, indices, mat, assume_unique=True)
A:jax.experimental.sparse.bcoo.(A, B)->jax._src.interpreters.ad.get_primitive_transpose(lax.dot_general_p)(ct, A, B, **kwds)
A:jax.experimental.sparse.bcoo.bcoo_spdot_general_p->jax._src.core.Primitive('bcoo_spdot_general')
A:jax.experimental.sparse.bcoo.lhs->_validate_bcoo(lhs_data, lhs_indices, lhs_shape)
A:jax.experimental.sparse.bcoo.overlap->(lhs_i[:, None] == rhs_i[None, :]).all(-1)
A:jax.experimental.sparse.bcoo.lhs_fill_value->jax.numpy.expand_dims(jnp.array([lhs_shape[d] for d in lhs_contracting], dtype=lhs_i.dtype), range(lhs_i.ndim - 1))
A:jax.experimental.sparse.bcoo.rhs_fill_value->jax.numpy.expand_dims(jnp.array([rhs_shape[d] for d in rhs_contracting], dtype=rhs_i.dtype), range(rhs_i.ndim - 1))
A:jax.experimental.sparse.bcoo.lhs_valid->(lhs_i < lhs_fill_value).all(-1)
A:jax.experimental.sparse.bcoo.rhs_valid->(rhs_i < rhs_fill_value).all(-1)
A:jax.experimental.sparse.bcoo.out_data->jax.numpy.where(overlap & lhs_valid[:, None] & rhs_valid[None, :], lhs_data[:, None] * rhs_data[None, :], 0).ravel()
A:jax.experimental.sparse.bcoo.out_indices->out_indices.reshape(len(out_data), out_indices.shape[-1]).reshape(len(out_data), out_indices.shape[-1])
A:jax.experimental.sparse.bcoo.(data_aval, _)->_bcoo_spdot_general_abstract_eval(lhs_data.aval, lhs_indices.aval, rhs_data.aval, rhs_indices.aval, lhs_spinfo=lhs_spinfo, rhs_spinfo=rhs_spinfo, dimension_numbers=dimension_numbers, preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcoo.rhs_data->rhs_data.transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_data.ndim)]).transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_data.ndim)])
A:jax.experimental.sparse.bcoo.rhs_indices->rhs_indices.transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_indices.ndim)]).transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_indices.ndim)])
A:jax.experimental.sparse.bcoo.func->functools.partial(lax.conv_general_dilated, window_strides=window_strides, padding=padding, lhs_dilation=lhs_dilation, rhs_dilation=rhs_dilation, dimension_numbers=dimension_numbers, feature_group_count=feature_group_count, batch_group_count=batch_group_count, precision=precision, preferred_element_type=preferred_element_type)
A:jax.experimental.sparse.bcoo.out_nse->min(out_nse, math.prod(out_aval.shape[out_n_batch:]))
A:jax.experimental.sparse.bcoo.data_aval->jax._src.core.ShapedArray(data_shape, out_aval.dtype)
A:jax.experimental.sparse.bcoo.indices_aval->jax._src.core.ShapedArray(indices_shape, lhs_indices.dtype)
A:jax.experimental.sparse.bcoo.rhs_ndim->len(rhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.(lhs_data, lhs_indices, lhs_spinfo)->_bcoo_batch_dims_to_front(batched_args[:2], batch_dims[:2], lhs_spinfo, batch_size=batch_size)
A:jax.experimental.sparse.bcoo.(rhs_data, rhs_indices, rhs_spinfo)->_bcoo_batch_dims_to_front(batched_args[2:], batch_dims[2:], rhs_spinfo, batch_size=batch_size)
A:jax.experimental.sparse.bcoo.(dimension_numbers, result_batch_dim)->_dot_general_batch_dim_nums((lhs_ndim, rhs_ndim), (0, 0), dimension_numbers)
A:jax.experimental.sparse.bcoo.bcoo_sort_indices_p->jax._src.core.Primitive('bcoo_sort_indices')
A:jax.experimental.sparse.bcoo.(indices, perm)->f(indices)
A:jax.experimental.sparse.bcoo.permute->nfold_vmap(permute, props.n_batch)
A:jax.experimental.sparse.bcoo.(*indices, perm)->jax.lax.sort((*idx_cols, lax.iota(indices.dtype, nse)), num_keys=N)
A:jax.experimental.sparse.bcoo.data_out->jax.lax.squeeze(arr.data, batch_dims + dense_dims)
A:jax.experimental.sparse.bcoo.(data_out, indices_out)->jax._src.core.Primitive('bcoo_sum_duplicates').bind(data, indices, spinfo=new_spinfo, nse=nse)
A:jax.experimental.sparse.bcoo.(indices_out, perm)->f(indices)
A:jax.experimental.sparse.bcoo.indices_dot_out->jax._src.interpreters.ad.Zero.from_value(indices_out)
A:jax.experimental.sparse.bcoo._bcoo_sort_indices_hlo->jax._src.interpreters.mlir.lower_fun(_bcoo_sort_indices_impl, multiple_results=True)
A:jax.experimental.sparse.bcoo.bcoo_sum_duplicates_p->jax._src.core.Primitive('bcoo_sum_duplicates')
A:jax.experimental.sparse.bcoo.(indices_out, mapping, nse_batched)->_unique_indices(indices, shape=spinfo.shape, return_inverse=True, return_true_size=True)
A:jax.experimental.sparse.bcoo.indices_out->jax.lax.squeeze(arr.indices[..., sparse_dims], batch_dims)
A:jax.experimental.sparse.bcoo.fill->jax.lax.broadcast_in_dim(operand=jnp.array(shape[props.n_batch:props.n_batch + props.n_sparse], dtype=indices.dtype), shape=(*indices.shape[:-2], nse - props.nse, indices.shape[-1]), broadcast_dimensions=(indices.ndim - 1,))
A:jax.experimental.sparse.bcoo.out_of_bounds->(indices >= fill_value).any(-1, keepdims=True)
A:jax.experimental.sparse.bcoo.row->jax.numpy.where(mask, shape[0], row)
A:jax.experimental.sparse.bcoo.col->jax.numpy.where(mask, 0, col)
A:jax.experimental.sparse.bcoo.(data, indices, new_spinfo)->_bcoo_batch_dims_to_front(batched_args, batch_dims, spinfo)
A:jax.experimental.sparse.bcoo._bcoo_sum_duplicates_hlo->jax._src.interpreters.mlir.lower_fun(_bcoo_sum_duplicates_impl, multiple_results=True)
A:jax.experimental.sparse.bcoo.new_d->jax.numpy.zeros_like(d, shape=new_d_shape).at[idx].set(d)
A:jax.experimental.sparse.bcoo.meshes->jax.numpy.meshgrid(*(jnp.arange(d, dtype=i.dtype) for d in (*i.shape[:n], nse)), indexing='ij')
A:jax.experimental.sparse.bcoo.new_i->jax.numpy.broadcast_to(i[:, n:], new_i_shape)
A:jax.experimental.sparse.bcoo.(new_data, new_indices)->_bcoo_sum_duplicates(new_data, new_indices, spinfo=SparseInfo(shape=new_shape), nse=new_nse)
A:jax.experimental.sparse.bcoo.new_data->jax.lax.expand_dims(new_data, (0, 1))
A:jax.experimental.sparse.bcoo.new_indices->jax.lax.expand_dims(new_indices, (0, 1, 3))
A:jax.experimental.sparse.bcoo.(batch_dims, sparse_dims, dense_dims)->split_list(broadcast_dimensions, [props.n_batch, props.n_sparse])
A:jax.experimental.sparse.bcoo.new_n_batch->min(broadcast_dimensions[props.n_batch:], default=len(shape))
A:jax.experimental.sparse.bcoo.dimension->operator.index(dimension)
A:jax.experimental.sparse.bcoo.offsets->numpy.cumsum([0] + [op.shape[dimension] for op in operands[:-1]], dtype=operands[0].indices.dtype)
A:jax.experimental.sparse.bcoo.(batch_shape, sparse_shape, dense_shape)->split_list(shape, [n_batch, n_sparse])
A:jax.experimental.sparse.bcoo.sparse_size->math.prod(sparse_shape)
A:jax.experimental.sparse.bcoo.cuml_shape->numpy.cumprod(new_sizes)
A:jax.experimental.sparse.bcoo.i1->numpy.cumprod(new_sizes).searchsorted(batch_size, side='right')
A:jax.experimental.sparse.bcoo.i2->numpy.cumprod(new_sizes).searchsorted(batch_size * sparse_size, side='right')
A:jax.experimental.sparse.bcoo.(new_batch_shape, new_sparse_shape, new_dense_shape)->split_list(new_sizes, [int(i1), int(i2)])
A:jax.experimental.sparse.bcoo.index_cols->tuple((indices[..., i] for i in sparse_perm))
A:jax.experimental.sparse.bcoo.flat_indices->jax.numpy.ravel_multi_index(index_cols, dims=tuple(sparse_shape), mode='clip')
A:jax.experimental.sparse.bcoo.oob_indices->(indices >= jnp.array(mat.shape[mat.n_batch:mat.n_batch + mat.n_sparse], dtype=indices.dtype)).any(-1, keepdims=True)
A:jax.experimental.sparse.bcoo.new_index_cols->jax.numpy.unravel_index(flat_indices, new_sparse_shape)
A:jax.experimental.sparse.bcoo.sparse_shape->jax.numpy.expand_dims(sparse_shape, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.spdims->jax.numpy.array([d - operand.n_batch for d in sparse_dims])
A:jax.experimental.sparse.bcoo.dimensions->tuple((canonicalize_axis(dim, arr.ndim) for dim in dimensions))
A:jax.experimental.sparse.bcoo.sparse_dims->numpy.array([i for i in range(arr.n_sparse) if i + arr.n_batch not in dimensions], dtype=int)
A:jax.experimental.sparse.bcoo.dense_dims->tuple((d - arr.n_sparse + 1 for d in dimensions if d >= arr.n_batch + arr.n_sparse))
A:jax.experimental.sparse.bcoo.(start_batch, start_sparse, start_dense)->split_list(start_indices, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.(end_batch, end_sparse, end_dense)->split_list(limit_indices, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.(stride_batch, stride_sparse, stride_dense)->split_list(strides, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.new_shape->tuple(((end - start + stride - 1) // stride for (start, end, stride) in safe_zip(start_indices, limit_indices, strides)))
A:jax.experimental.sparse.bcoo.(_, new_shape_sparse, _)->split_list(new_shape, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.starts->jax.numpy.expand_dims(starts, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.ends->jax.numpy.expand_dims(jnp.array(end_sparse, dtype=new_indices.dtype), range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.strides_->jax.numpy.expand_dims(jnp.array(stride_sparse, dtype=new_indices.dtype), range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.keep->jax.numpy.all((new_indices >= starts) & (new_indices < starts + sizes), -1, keepdims=True)
A:jax.experimental.sparse.bcoo.keep_data->jax.lax.expand_dims(keep[..., 0], range(mat.n_batch + 1, mat.n_batch + 1 + mat.n_dense))
A:jax.experimental.sparse.bcoo.new_nse->math.prod(size_sparse)
A:jax.experimental.sparse.bcoo.start_indices->tuple((jnp.asarray(i) for i in start_indices))
A:jax.experimental.sparse.bcoo.slice_sizes->tuple((operator.index(i) for i in slice_sizes))
A:jax.experimental.sparse.bcoo.(size_batch, size_sparse, size_dense)->split_list(slice_sizes, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.zero->_const(idx, 0)
A:jax.experimental.sparse.bcoo.sizes->jax.numpy.expand_dims(sizes, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.(out_data, out_indices, out_shape)->_bcoo_multiply_sparse(lhs.data, lhs.indices, rhs.data, rhs.indices, lhs_spinfo=lhs._info, rhs_spinfo=rhs._info)
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, _, nse)->_validate_bcoo(data, indices, shape)
A:jax.experimental.sparse.bcoo.axes->sorted(set(axes))
A:jax.experimental.sparse.bcoo.dense_axes->tuple((ax - n_sparse + 1 for ax in axes if ax >= n_batch + n_sparse))
A:jax.experimental.sparse.bcoo.new_batch_dims->tuple(sorted(set(range(n_batch)) - batch_axes))
A:jax.experimental.sparse.bcoo.new_batch_shape->tuple((data.shape[i] for i in new_batch_dims))
A:jax.experimental.sparse.bcoo._mul->nfold_vmap(_mul, n_batch)
A:jax.experimental.sparse.bcoo.dims->jax.numpy.array([i for (i, (s1, s2)) in enumerate(safe_zip(lhs_shape[:lhs.n_sparse], rhs_shape[:rhs.n_sparse])) if s1 != 1 and s2 != 1], dtype=int)
A:jax.experimental.sparse.bcoo.(i_lhs, i_rhs)->jax.numpy.nonzero(mask, size=nse, fill_value=(lhs.nse, rhs.nse))
A:jax.experimental.sparse.bcoo.v->jax.lax.expand_dims(v, range(len(shape) - v.ndim))
A:jax.experimental.sparse.bcoo.ind->tuple((i if s != 1 else 0 for (i, s) in zip(ind, v.shape)))
A:jax.experimental.sparse.bcoo.parsed_mode->jax._src.lax.slicing.GatherScatterMode.from_any(mode)
A:jax.experimental.sparse.bcoo.kwds->dict(dimension_numbers=dimension_numbers, slice_sizes=slice_sizes, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)
A:jax.experimental.sparse.bcoo.full_slice_sizes->list(operand.shape)
A:jax.experimental.sparse.bcoo.full_start_indices[j]->start_indices[..., i].ravel()
A:jax.experimental.sparse.bcoo.slc->bcoo_dynamic_slice(operand, indices, slice_sizes=full_slice_sizes)
A:jax.experimental.sparse.bcoo.permutation[np.array(batch_dims + offset_dims)]->numpy.arange(result.ndim)
A:jax.experimental.sparse.bcoo.jaxpr->jax.make_jaxpr(func)(jax.ShapeDtypeStruct(lhs.shape, lhs.dtype), jax.ShapeDtypeStruct(rhs.shape, rhs.dtype))
A:jax.experimental.sparse.bcoo.padding->tuple(map(int, padding))
A:jax.experimental.sparse.bcoo.dimsize->max(0, lhs.shape[0] + padding[0] + padding[1] - rhs.shape[0] + 1)
A:jax.experimental.sparse.bcoo.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.bcoo.n_sparse->property(lambda self: self.indices.shape[-1])
A:jax.experimental.sparse.bcoo.n_dense->property(lambda self: self.data.ndim - 1 - self.n_batch)
A:jax.experimental.sparse.bcoo._info->property(lambda self: SparseInfo(self.shape, self.indices_sorted, self.unique_indices))
A:jax.experimental.sparse.bcoo._bufs->property(lambda self: (self.data, self.indices))
A:jax.experimental.sparse.bcoo.(self.data, self.indices)->map(jnp.asarray, args)
A:jax.experimental.sparse.bcoo.diag_size->min(N + k, M)
A:jax.experimental.sparse.bcoo.k->_const(idx, k)
A:jax.experimental.sparse.bcoo.mat_T->bcoo_transpose(self, permutation=perm)
A:jax.experimental.sparse.bcoo.shape_T->tuple((self.shape[i] for i in perm))
A:jax.experimental.sparse.bcoo.obj->object.__new__(cls)
jax.experimental.sparse.BCOO(self,args:tuple[Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.BCOO.__repr__(self)
jax.experimental.sparse.BCOO._empty(cls,shape:Shape,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32',n_dense:int=0,n_batch:int=0,nse:int=0)->BCOO
jax.experimental.sparse.BCOO._eye(cls,N:int,M:int,k:int,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32',n_batch:int=0,n_dense:int=0)->BCOO
jax.experimental.sparse.BCOO.astype(self,*args,**kwargs)->BCOO
jax.experimental.sparse.BCOO.from_scipy_sparse(cls,mat,*,index_dtype:DTypeLike|None=None,n_dense:int=0,n_batch:int=0)->BCOO
jax.experimental.sparse.BCOO.fromdense(cls,mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32,n_dense:int=0,n_batch:int=0)->BCOO
jax.experimental.sparse.BCOO.reshape(self,*args,**kwargs)->BCOO
jax.experimental.sparse.BCOO.sort_indices(self)->BCOO
jax.experimental.sparse.BCOO.sum(self)->BCOO
jax.experimental.sparse.BCOO.sum_duplicates(self,nse:int|None=None,remove_zeros:bool=True)->BCOO
jax.experimental.sparse.BCOO.todense(self)->Array
jax.experimental.sparse.BCOO.transpose(self,axes:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.BCOO.tree_flatten(self)
jax.experimental.sparse.BCOO.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.BCOO.update_layout(self,*,n_batch:int|None=None,n_dense:int|None=None,on_inefficient:str='error')->BCOO
jax.experimental.sparse.BCOOProperties(NamedTuple)
jax.experimental.sparse.bcoo.BCOO(self,args:tuple[Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.bcoo.BCOO.__init__(self,args:tuple[Array,Array],*,shape:Sequence[int],indices_sorted:bool=False,unique_indices:bool=False)
jax.experimental.sparse.bcoo.BCOO.__repr__(self)
jax.experimental.sparse.bcoo.BCOO._empty(cls,shape:Shape,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32',n_dense:int=0,n_batch:int=0,nse:int=0)->BCOO
jax.experimental.sparse.bcoo.BCOO._eye(cls,N:int,M:int,k:int,*,dtype:DTypeLike|None=None,index_dtype:DTypeLike='int32',n_batch:int=0,n_dense:int=0)->BCOO
jax.experimental.sparse.bcoo.BCOO.astype(self,*args,**kwargs)->BCOO
jax.experimental.sparse.bcoo.BCOO.from_scipy_sparse(cls,mat,*,index_dtype:DTypeLike|None=None,n_dense:int=0,n_batch:int=0)->BCOO
jax.experimental.sparse.bcoo.BCOO.fromdense(cls,mat:Array,*,nse:int|None=None,index_dtype:DTypeLike=np.int32,n_dense:int=0,n_batch:int=0)->BCOO
jax.experimental.sparse.bcoo.BCOO.reshape(self,*args,**kwargs)->BCOO
jax.experimental.sparse.bcoo.BCOO.sort_indices(self)->BCOO
jax.experimental.sparse.bcoo.BCOO.sum(self)->BCOO
jax.experimental.sparse.bcoo.BCOO.sum_duplicates(self,nse:int|None=None,remove_zeros:bool=True)->BCOO
jax.experimental.sparse.bcoo.BCOO.todense(self)->Array
jax.experimental.sparse.bcoo.BCOO.transpose(self,axes:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.bcoo.BCOO.tree_flatten(self)
jax.experimental.sparse.bcoo.BCOO.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse.bcoo.BCOO.update_layout(self,*,n_batch:int|None=None,n_dense:int|None=None,on_inefficient:str='error')->BCOO
jax.experimental.sparse.bcoo.BCOOProperties(NamedTuple)
jax.experimental.sparse.bcoo.Buffer(Protocol)
jax.experimental.sparse.bcoo.Buffer.dtype(self)->Any
jax.experimental.sparse.bcoo.Buffer.shape(self)->Shape
jax.experimental.sparse.bcoo._adjust_indices_nse(indices,*,nse,shape)
jax.experimental.sparse.bcoo._bcoo_batch_dims_to_front(batched_args,batch_dims,spinfo,batch_size=None)
jax.experimental.sparse.bcoo._bcoo_broadcast_in_dim(data:Array,indices:Array,*,spinfo:SparseInfo,shape:Shape,broadcast_dimensions:Sequence[int])->tuple[Array, Array]
jax.experimental.sparse.bcoo._bcoo_conv_1d(lhs:BCOO,rhs:BCOO,padding:Sequence[int])->BCOO
jax.experimental.sparse.bcoo._bcoo_dot_general(lhs_data:Array,lhs_indices:Array,rhs:Array,*,dimension_numbers:DotDimensionNumbers,preferred_element_type:Any,lhs_spinfo:SparseInfo)->Array
jax.experimental.sparse.bcoo._bcoo_dot_general_abstract_eval(lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_fallback(data,indices,spinfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_gpu_impl(lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_impl(lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_jvp_lhs(lhs_data_dot,lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_jvp_rhs(rhs_dot,lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_abstract_eval(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_batch_rule(batched_args,batch_dims,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_impl(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_jvp_A(A_dot,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_jvp_B(B_dot,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_simple(A,B,indices,*,dimension_numbers,precision)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_simple2(A,B,indices,*,dimension_numbers,precision)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_slow(A,B,indices,*,dimension_numbers,precision)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_transpose(ct,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_transpose(ct,lhs_data,lhs_indices,rhs,*,dimension_numbers,preferred_element_type,lhs_spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_extract(indices:Array,arr:Array,*,assume_unique=True)->Array
jax.experimental.sparse.bcoo._bcoo_extract_abstract_eval(indices,arr,*,assume_unique)
jax.experimental.sparse.bcoo._bcoo_extract_batching_rule(batched_args,batch_dims,*,assume_unique)
jax.experimental.sparse.bcoo._bcoo_extract_impl(indices,arr,*,assume_unique)
jax.experimental.sparse.bcoo._bcoo_extract_jvp(arr_dot,indices,arr,*,assume_unique)
jax.experimental.sparse.bcoo._bcoo_extract_transpose(ct,indices,arr,*,assume_unique)
jax.experimental.sparse.bcoo._bcoo_from_elt(cont,axis_size,elt,axis)
jax.experimental.sparse.bcoo._bcoo_fromdense(mat:Array,*,nse:int,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->tuple[Array, Array]
jax.experimental.sparse.bcoo._bcoo_fromdense_abstract_eval(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_batching_rule(batched_args,batch_dims,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_impl(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_jvp(primals,tangents,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_transpose(ct,M,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_multiply_dense(data:Array,indices:Array,v:Array,*,spinfo:SparseInfo)->Array
jax.experimental.sparse.bcoo._bcoo_multiply_sparse(lhs_data:Array,lhs_indices:Array,rhs_data:Array,rhs_indices:Array,*,lhs_spinfo:SparseInfo,rhs_spinfo:SparseInfo)->tuple[Array, Array, Shape]
jax.experimental.sparse.bcoo._bcoo_multiply_sparse_unbatched(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_shape,rhs_shape)
jax.experimental.sparse.bcoo._bcoo_rdot_general(lhs:Array,rhs_data:Array,rhs_indices:Array,*,dimension_numbers:DotDimensionNumbers,preferred_element_type:Any,rhs_spinfo:SparseInfo)->Array
jax.experimental.sparse.bcoo._bcoo_reduce_sum(data:Array,indices:Array,*,spinfo:SparseInfo,axes:Sequence[int])->tuple[Array, Array, Shape]
jax.experimental.sparse.bcoo._bcoo_set_nse(mat:BCOO,nse:int)->BCOO
jax.experimental.sparse.bcoo._bcoo_sort_indices_abstract_eval(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_batching_rule(batched_args,batch_dims,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_impl(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_jvp(primals,tangents,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_unbatched(indices)
jax.experimental.sparse.bcoo._bcoo_spdot_general(lhs_data:Array,lhs_indices:Array,rhs_data:Array,rhs_indices:Array,*,lhs_spinfo:SparseInfo,rhs_spinfo:SparseInfo,dimension_numbers:DotDimensionNumbers,preferred_element_type:Any)->tuple[Array, Array]
jax.experimental.sparse.bcoo._bcoo_spdot_general_abstract_eval(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo:SparseInfo,rhs_spinfo:SparseInfo,dimension_numbers,preferred_element_type)
jax.experimental.sparse.bcoo._bcoo_spdot_general_batch_rule(batched_args,batch_dims,*,lhs_spinfo:SparseInfo,rhs_spinfo:SparseInfo,preferred_element_type,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_spdot_general_impl(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo:SparseInfo,rhs_spinfo:SparseInfo,dimension_numbers,preferred_element_type)
jax.experimental.sparse.bcoo._bcoo_spdot_general_jvp(primals,tangents,**kwds)
jax.experimental.sparse.bcoo._bcoo_spdot_general_unbatched(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo,rhs_spinfo,lhs_contracting,rhs_contracting,out_nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates(data:Array,indices:Array,*,spinfo:SparseInfo,nse:int|None)->tuple[Array, Array]
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_abstract_eval(data,indices,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_batching_rule(batched_args,batch_dims,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_impl(data,indices,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_jvp(primals,tangents,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_to_elt(cont,_,val,axis)
jax.experimental.sparse.bcoo._bcoo_todense(data:Array,indices:Array,*,spinfo:SparseInfo)->Array
jax.experimental.sparse.bcoo._bcoo_todense_abstract_eval(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_batching_rule(batched_args,batch_dims,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_impl(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_jvp(data_dot,data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_transpose(ct,data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_transpose(data:Array,indices:Array,*,permutation:Sequence[int],spinfo:SparseInfo)->tuple[Array, Array]
jax.experimental.sparse.bcoo._bcoo_transpose_abstract_eval(data,indices,*,permutation:Sequence[int],spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_batch_rule(batched_args,batch_dims,*,permutation:Sequence[int],spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_impl(data,indices,*,permutation:Sequence[int],spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_jvp(primals,tangents,*,permutation:Sequence[int],spinfo:SparseInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_transpose(ct,data,indices,*,permutation:Sequence[int],spinfo:SparseInfo)
jax.experimental.sparse.bcoo._convert_to_1d_for_conv(mat,index_dtype)
jax.experimental.sparse.bcoo._coo_correct_out_of_bound_indices(row,col,shape,transpose)
jax.experimental.sparse.bcoo._tuple_replace(tup,ind,val)
jax.experimental.sparse.bcoo._unique_indices(indices,*,shape,return_inverse=False,return_index=False,return_true_size=False)
jax.experimental.sparse.bcoo._unique_indices_unbatched(indices,*,shape,return_inverse=False,return_index=False,return_true_size=False)
jax.experimental.sparse.bcoo._validate_bcoo(data:Buffer,indices:Buffer,shape:Sequence[int])->BCOOProperties
jax.experimental.sparse.bcoo._validate_bcoo_indices(indices:Buffer,shape:Sequence[int])->BCOOProperties
jax.experimental.sparse.bcoo._validate_permutation(data,indices,permutation,shape)
jax.experimental.sparse.bcoo.bcoo_broadcast_in_dim(mat:BCOO,*,shape:Shape,broadcast_dimensions:Sequence[int])->BCOO
jax.experimental.sparse.bcoo.bcoo_concatenate(operands:Sequence[BCOO],*,dimension:int)->BCOO
jax.experimental.sparse.bcoo.bcoo_conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,batch_group_count=1,precision=None,preferred_element_type=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_dot_general(lhs:BCOO|Array,rhs:BCOO|Array,*,dimension_numbers:DotDimensionNumbers,precision:None=None,preferred_element_type:None=None)->BCOO | Array
jax.experimental.sparse.bcoo.bcoo_dot_general_sampled(A:Array,B:Array,indices:Array,*,dimension_numbers:DotDimensionNumbers)->Array
jax.experimental.sparse.bcoo.bcoo_dynamic_slice(mat:BCOO,start_indices:Sequence[Any],slice_sizes:Sequence[int])->BCOO
jax.experimental.sparse.bcoo.bcoo_eliminate_zeros(mat:BCOO,nse:int|None=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_extract(sparr:BCOO,arr:ArrayLike,*,assume_unique:bool|None=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_fromdense(mat:Array,*,nse:int|None=None,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->BCOO
jax.experimental.sparse.bcoo.bcoo_gather(operand:BCOO,start_indices:Array,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape,*,unique_indices:bool=False,indices_are_sorted:bool=False,mode:str|GatherScatterMode|None=None,fill_value=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_multiply_dense(sp_mat:BCOO,v:Array)->Array
jax.experimental.sparse.bcoo.bcoo_multiply_sparse(lhs:BCOO,rhs:BCOO)->BCOO
jax.experimental.sparse.bcoo.bcoo_reduce_sum(mat:BCOO,*,axes:Sequence[int])->BCOO
jax.experimental.sparse.bcoo.bcoo_reshape(mat:BCOO,*,new_sizes:Sequence[int],dimensions:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_rev(operand,dimensions)
jax.experimental.sparse.bcoo.bcoo_slice(mat:BCOO,*,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_sort_indices(mat:BCOO)->BCOO
jax.experimental.sparse.bcoo.bcoo_squeeze(arr:BCOO,*,dimensions:Sequence[int])->BCOO
jax.experimental.sparse.bcoo.bcoo_sum_duplicates(mat:BCOO,nse:int|None=None)->BCOO
jax.experimental.sparse.bcoo.bcoo_todense(mat:BCOO)->Array
jax.experimental.sparse.bcoo.bcoo_transpose(mat:BCOO,*,permutation:Sequence[int])->BCOO
jax.experimental.sparse.bcoo.bcoo_update_layout(mat:BCOO,*,n_batch:int|None=None,n_dense:int|None=None,on_inefficient:str|None='error')->BCOO
jax.experimental.sparse.bcoo_broadcast_in_dim(mat:BCOO,*,shape:Shape,broadcast_dimensions:Sequence[int])->BCOO
jax.experimental.sparse.bcoo_concatenate(operands:Sequence[BCOO],*,dimension:int)->BCOO
jax.experimental.sparse.bcoo_conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,batch_group_count=1,precision=None,preferred_element_type=None)->BCOO
jax.experimental.sparse.bcoo_dot_general(lhs:BCOO|Array,rhs:BCOO|Array,*,dimension_numbers:DotDimensionNumbers,precision:None=None,preferred_element_type:None=None)->BCOO | Array
jax.experimental.sparse.bcoo_dot_general_sampled(A:Array,B:Array,indices:Array,*,dimension_numbers:DotDimensionNumbers)->Array
jax.experimental.sparse.bcoo_dynamic_slice(mat:BCOO,start_indices:Sequence[Any],slice_sizes:Sequence[int])->BCOO
jax.experimental.sparse.bcoo_extract(sparr:BCOO,arr:ArrayLike,*,assume_unique:bool|None=None)->BCOO
jax.experimental.sparse.bcoo_fromdense(mat:Array,*,nse:int|None=None,n_batch:int=0,n_dense:int=0,index_dtype:DTypeLike=jnp.int32)->BCOO
jax.experimental.sparse.bcoo_gather(operand:BCOO,start_indices:Array,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape,*,unique_indices:bool=False,indices_are_sorted:bool=False,mode:str|GatherScatterMode|None=None,fill_value=None)->BCOO
jax.experimental.sparse.bcoo_multiply_dense(sp_mat:BCOO,v:Array)->Array
jax.experimental.sparse.bcoo_multiply_sparse(lhs:BCOO,rhs:BCOO)->BCOO
jax.experimental.sparse.bcoo_reduce_sum(mat:BCOO,*,axes:Sequence[int])->BCOO
jax.experimental.sparse.bcoo_reshape(mat:BCOO,*,new_sizes:Sequence[int],dimensions:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.bcoo_rev(operand,dimensions)
jax.experimental.sparse.bcoo_slice(mat:BCOO,*,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Sequence[int]|None=None)->BCOO
jax.experimental.sparse.bcoo_sort_indices(mat:BCOO)->BCOO
jax.experimental.sparse.bcoo_squeeze(arr:BCOO,*,dimensions:Sequence[int])->BCOO
jax.experimental.sparse.bcoo_sum_duplicates(mat:BCOO,nse:int|None=None)->BCOO
jax.experimental.sparse.bcoo_todense(mat:BCOO)->Array
jax.experimental.sparse.bcoo_transpose(mat:BCOO,*,permutation:Sequence[int])->BCOO
jax.experimental.sparse.bcoo_update_layout(mat:BCOO,*,n_batch:int|None=None,n_dense:int|None=None,on_inefficient:str|None='error')->BCOO


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/linalg.py----------------------------------------
A:jax.experimental.sparse.linalg.X->_mm(XPR, B)
A:jax.experimental.sparse.linalg.P->_mm(XPR, diff_rayleigh_ortho)
A:jax.experimental.sparse.linalg.AX->A(X)
A:jax.experimental.sparse.linalg.theta->jax.numpy.sum(X * AX, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.R->_project_out(jnp.concatenate((X, P), axis=1), R)
A:jax.experimental.sparse.linalg.XPR->jax.numpy.concatenate((X, P, R), axis=1)
A:jax.experimental.sparse.linalg.(theta, Q)->_rayleigh_ritz_orth(A, XPR)
A:jax.experimental.sparse.linalg.normB->jax.numpy.linalg.norm(B, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.normX->jax.numpy.linalg.norm(X, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.(q, _)->jax.numpy.linalg.qr(Q[:k, k:].T)
A:jax.experimental.sparse.linalg.diff_rayleigh_ortho->_mm(Q[:, k:], q)
A:jax.experimental.sparse.linalg.normP->jax.numpy.linalg.norm(P, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.resid_norms->jax.numpy.linalg.norm(R, ord=2, axis=0)
A:jax.experimental.sparse.linalg.converged->jax.numpy.sum(res_converged)
A:jax.experimental.sparse.linalg.diagnostics->_generate_diagnostics(XPR, X, P, R, theta, converged, resid_norms / reltol)
A:jax.experimental.sparse.linalg.(state, diagnostics)->jax.lax.scan(lambda state, _: body(state), state, xs=None, length=m)
A:jax.experimental.sparse.linalg.state->jax.lax.while_loop(cond, body, state)
A:jax.experimental.sparse.linalg.test_output->A(jnp.zeros((n, 1), dtype=X.dtype))
A:jax.experimental.sparse.linalg.XTX->_mm(X.T, X)
A:jax.experimental.sparse.linalg.DX->diagdiag(XTX)
A:jax.experimental.sparse.linalg.orthX->abserr(XTX - DX)
A:jax.experimental.sparse.linalg.PTP->_mm(P.T, P)
A:jax.experimental.sparse.linalg.DP->diagdiag(PTP)
A:jax.experimental.sparse.linalg.orthP->abserr(PTP - DP)
A:jax.experimental.sparse.linalg.PX->abserr(X.T @ P)
A:jax.experimental.sparse.linalg.(w, V)->_eigh_ascending(inner)
A:jax.experimental.sparse.linalg.norms->jax.numpy.linalg.norm(orthoX, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.inner->_mm(X.T, X)
A:jax.experimental.sparse.linalg.padded->jax.numpy.maximum(w, tau)
A:jax.experimental.sparse.linalg.orthoX->_mm(X, scaledV)
A:jax.experimental.sparse.linalg.U->_orthonormalize(U)
A:jax.experimental.sparse.linalg.normU->jax.numpy.linalg.norm(U, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.basis->_svqb(basis)
A:jax.experimental.sparse.linalg.SAS->_mm(S.T, A(S))
A:jax.experimental.sparse.linalg.(Xupper, Xlower)->jax.numpy.split(X, [k], axis=0)
A:jax.experimental.sparse.linalg.(u, s, vt)->jax.numpy.linalg.svd(Xupper)
A:jax.experimental.sparse.linalg.y->jax.numpy.concatenate([Xupper + _mm(u, vt), Xlower], axis=0)
A:jax.experimental.sparse.linalg.other->jax.numpy.concatenate([jnp.eye(m, dtype=X.dtype), jnp.zeros((n - k - m, m), dtype=X.dtype)], axis=0)
A:jax.experimental.sparse.linalg.w->_mm(y, vt.T * ((2 * (1 + s)) ** (-1 / 2))[jnp.newaxis, :])
A:jax.experimental.sparse.linalg.tol->float(tol)
A:jax.experimental.sparse.linalg.A->csr_matrix((data, indices, indptr), shape=(b.size, b.size))
A:jax.experimental.sparse.linalg.(result, _, _)->jax.interpreters.mlir.emit_python_callback(ctx, _callback, None, args, ctx.avals_in, ctx.avals_out, has_side_effect=False)
A:jax.experimental.sparse.linalg.p->spsolve(data, indices, indptr, b, **kwds)
A:jax.experimental.sparse.linalg.q->jax.experimental.sparse.csr_matvec_p.bind(data_dot, indices, indptr, p, shape=(indptr.size - 1, len(b)), transpose=False)
A:jax.experimental.sparse.linalg.(row_T, indices_T, data_T)->jax.lax.sort((indices, row, data), num_keys=2)
A:jax.experimental.sparse.linalg.indptr_T->jax.numpy.zeros_like(indptr).at[1:].set(jnp.cumsum(jnp.bincount(row_T, length=m)).astype(indptr.dtype))
A:jax.experimental.sparse.linalg.(data_T, indices_T, indptr_T)->_csr_transpose(data, indices, indptr)
A:jax.experimental.sparse.linalg.ct_out->spsolve(data_T, indices_T, indptr_T, ct, **kwds)
A:jax.experimental.sparse.linalg.spsolve_p->jax._src.core.Primitive('spsolve')
jax.experimental.sparse.linalg._check_inputs(A,X)
jax.experimental.sparse.linalg._csr_transpose(data,indices,indptr)
jax.experimental.sparse.linalg._eigh_ascending(A)
jax.experimental.sparse.linalg._extend_basis(X,m)
jax.experimental.sparse.linalg._generate_diagnostics(prev_XPR,X,P,R,theta,converged,adj_resid)
jax.experimental.sparse.linalg._lobpcg_standard_callable(A:Callable[[jax.Array],jax.Array],X:jax.Array,m:int,tol:Union[jax.Array,float,None],debug:bool=False)
jax.experimental.sparse.linalg._lobpcg_standard_matrix(A:jax.Array,X:jax.Array,m:int,tol:Union[jax.Array,float,None],debug:bool=False)
jax.experimental.sparse.linalg._mm(a,b,precision=jax.lax.Precision.HIGHEST)
jax.experimental.sparse.linalg._orthonormalize(basis)
jax.experimental.sparse.linalg._project_out(basis,U)
jax.experimental.sparse.linalg._rayleigh_ritz_orth(A,S)
jax.experimental.sparse.linalg._spsolve_abstract_eval(data,indices,indptr,b,*,tol,reorder)
jax.experimental.sparse.linalg._spsolve_cpu_lowering(ctx,data,indices,indptr,b,tol,reorder)
jax.experimental.sparse.linalg._spsolve_gpu_lowering(ctx,data,indices,indptr,b,*,tol,reorder)
jax.experimental.sparse.linalg._spsolve_jvp_lhs(data_dot,data,indices,indptr,b,**kwds)
jax.experimental.sparse.linalg._spsolve_jvp_rhs(b_dot,data,indices,indptr,b,**kwds)
jax.experimental.sparse.linalg._spsolve_transpose(ct,data,indices,indptr,b,**kwds)
jax.experimental.sparse.linalg._svqb(X)
jax.experimental.sparse.linalg.lobpcg_standard(A:Union[jax.Array,Callable[[jax.Array],jax.Array]],X:jax.Array,m:int=100,tol:Union[jax.Array,float,None]=None)
jax.experimental.sparse.linalg.spsolve(data,indices,indptr,b,tol=1e-06,reorder=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/_base.py----------------------------------------
A:jax.experimental.sparse._base.self.shape->tuple(shape)
A:jax.experimental.sparse._base.shape->list(self.shape)
jax.experimental.sparse.JAXSparse(self,args:tuple[Array,...],*,shape:Sequence[int])
jax.experimental.sparse.JAXSparse.T(self)
jax.experimental.sparse.JAXSparse.__add__(self,other)
jax.experimental.sparse.JAXSparse.__getitem__(self,item)
jax.experimental.sparse.JAXSparse.__len__(self)
jax.experimental.sparse.JAXSparse.__matmul__(self,other)
jax.experimental.sparse.JAXSparse.__mul__(self,other)
jax.experimental.sparse.JAXSparse.__neg__(self)
jax.experimental.sparse.JAXSparse.__pos__(self)
jax.experimental.sparse.JAXSparse.__radd__(self,other)
jax.experimental.sparse.JAXSparse.__repr__(self)
jax.experimental.sparse.JAXSparse.__rmatmul__(self,other)
jax.experimental.sparse.JAXSparse.__rmul__(self,other)
jax.experimental.sparse.JAXSparse.__rsub__(self,other)
jax.experimental.sparse.JAXSparse.__sub__(self,other)
jax.experimental.sparse.JAXSparse.block_until_ready(self)
jax.experimental.sparse.JAXSparse.ndim(self)->int
jax.experimental.sparse.JAXSparse.size(self)->int
jax.experimental.sparse.JAXSparse.sum(self,*args,**kwargs)
jax.experimental.sparse.JAXSparse.transpose(self,axes=None)
jax.experimental.sparse.JAXSparse.tree_flatten(self)
jax.experimental.sparse.JAXSparse.tree_unflatten(cls,aux_data,children)
jax.experimental.sparse._base.JAXSparse(self,args:tuple[Array,...],*,shape:Sequence[int])
jax.experimental.sparse._base.JAXSparse.T(self)
jax.experimental.sparse._base.JAXSparse.__add__(self,other)
jax.experimental.sparse._base.JAXSparse.__getitem__(self,item)
jax.experimental.sparse._base.JAXSparse.__init__(self,args:tuple[Array,...],*,shape:Sequence[int])
jax.experimental.sparse._base.JAXSparse.__len__(self)
jax.experimental.sparse._base.JAXSparse.__matmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__mul__(self,other)
jax.experimental.sparse._base.JAXSparse.__neg__(self)
jax.experimental.sparse._base.JAXSparse.__pos__(self)
jax.experimental.sparse._base.JAXSparse.__radd__(self,other)
jax.experimental.sparse._base.JAXSparse.__repr__(self)
jax.experimental.sparse._base.JAXSparse.__rmatmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__rmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__rsub__(self,other)
jax.experimental.sparse._base.JAXSparse.__sub__(self,other)
jax.experimental.sparse._base.JAXSparse.block_until_ready(self)
jax.experimental.sparse._base.JAXSparse.ndim(self)->int
jax.experimental.sparse._base.JAXSparse.size(self)->int
jax.experimental.sparse._base.JAXSparse.sum(self,*args,**kwargs)
jax.experimental.sparse._base.JAXSparse.transpose(self,axes=None)
jax.experimental.sparse._base.JAXSparse.tree_flatten(self)
jax.experimental.sparse._base.JAXSparse.tree_unflatten(cls,aux_data,children)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/test_util.py----------------------------------------
A:jax.experimental.sparse.test_util.(x_bufs, x_tree)->jax.tree_util.tree_flatten(x)
A:jax.experimental.sparse.test_util.(y_bufs, y_tree)->jax.tree_util.tree_flatten(y)
A:jax.experimental.sparse.test_util.sparse_args->args_maker()
A:jax.experimental.sparse.test_util.dense_args->jax.tree_util.tree_map(sparse.todense, sparse_args, is_leaf=is_sparse)
A:jax.experimental.sparse.test_util.expected->dense_op(*dense_args)
A:jax.experimental.sparse.test_util.sparse_ans->sparse_op(*sparse_args)
A:jax.experimental.sparse.test_util.actual->jax.tree_util.tree_map(sparse.todense, sparse_ans, is_leaf=is_sparse)
A:jax.experimental.sparse.test_util.sparse_ans_jit->jax.jit(sparse_op)(*sparse_args)
A:jax.experimental.sparse.test_util.args->list(zip(*(args_maker() for _ in range(batch_size))))
A:jax.experimental.sparse.test_util.(args_flat, tree)->jax.tree_util.tree_flatten(args)
A:jax.experimental.sparse.test_util.argnums_flat->numpy.cumsum([0, *num_bufs[:-1]]).tolist()
A:jax.experimental.sparse.test_util.args_dense->jax.tree_util.tree_map(sparse.todense, args, is_leaf=is_sparse)
A:jax.experimental.sparse.test_util.out->sparse_fun(*tree_util.tree_unflatten(tree, args_flat))
A:jax.experimental.sparse.test_util.result_de->jax.jacfwd(dense_fun_flat, argnums=argnums_flat)(*args_flat)
A:jax.experimental.sparse.test_util.result_sp->jax.jacfwd(sparse_fun_flat, argnums=argnums_flat)(*args_flat)
A:jax.experimental.sparse.test_util.rng->self.rng()
A:jax.experimental.sparse.test_util.bdims->self._random_bdims(*(arg.n_batch if is_sparse(arg) else arg.ndim for arg in args_maker()))
A:jax.experimental.sparse.test_util.(batch_shape, sparse_shape, dense_shape)->split_list(shape, [n_batch, n_sparse])
A:jax.experimental.sparse.test_util.nse->min(size, int(nse))
A:jax.experimental.sparse.test_util.nse_int->int(nse)
A:jax.experimental.sparse.test_util.data_rng->rand_method(rng)
A:jax.experimental.sparse.test_util.data->jax.numpy.array(data_rng(data_shape, dtype))
A:jax.experimental.sparse.test_util.int32->numpy.dtype('int32')
A:jax.experimental.sparse.test_util.indices->self.rng().choice(size, size - nse, replace=False)
A:jax.experimental.sparse.test_util.indptr->indptr.at[..., 0].set(0).at[..., 0].set(0)
A:jax.experimental.sparse.test_util.rand->rand_method(rng)
A:jax.experimental.sparse.test_util.size->math.prod(shape)
A:jax.experimental.sparse.test_util.M->rand(shape, dtype)
jax.experimental.sparse.test_util.BatchedDotGeneralProperties(NamedTuple)
jax.experimental.sparse.test_util.SparseLayout(NamedTuple)
jax.experimental.sparse.test_util.SparseTestCase(jtu.JaxTestCase)
jax.experimental.sparse.test_util.SparseTestCase._CheckAgainstDense(self,dense_op,sparse_op,args_maker,check_jit=True,check_dtypes=True,tol=None,atol=None,rtol=None,canonicalize_dtypes=True)
jax.experimental.sparse.test_util.SparseTestCase._CheckBatchingSparse(self,dense_fun,sparse_fun,args_maker,*,batch_size=3,bdims=None,check_jit=False,check_dtypes=True,tol=None,atol=None,rtol=None,canonicalize_dtypes=True)
jax.experimental.sparse.test_util.SparseTestCase._CheckGradsSparse(self,dense_fun,sparse_fun,args_maker,*,argnums=None,modes=('fwd','rev'),atol=None,rtol=None)
jax.experimental.sparse.test_util.SparseTestCase._random_bdims(self,*args)
jax.experimental.sparse.test_util.SparseTestCase.assertSparseArraysEquivalent(self,x,y,*,check_dtypes=True,atol=None,rtol=None,canonicalize_dtypes=True,err_msg='')
jax.experimental.sparse.test_util._rand_sparse(shape:Sequence[int],dtype:DTypeLike,*,rng:np.random.RandomState,rand_method:Callable[...,Any],nse:Union[int,float],n_batch:int,n_dense:int,sparse_format:str)->Union[sparse.BCOO, sparse.BCSR]
jax.experimental.sparse.test_util.is_sparse(x)
jax.experimental.sparse.test_util.iter_bcsr_layouts(shape:Sequence[int],min_n_batch=0)->Iterator[SparseLayout]
jax.experimental.sparse.test_util.iter_sparse_layouts(shape:Sequence[int],min_n_batch=0)->Iterator[SparseLayout]
jax.experimental.sparse.test_util.iter_subsets(s:Sequence)->Iterable[tuple]
jax.experimental.sparse.test_util.rand_bcoo(rng:np.random.RandomState,rand_method:Callable[...,Any]=jtu.rand_default,nse:Union[int,float]=0.5,n_batch:int=0,n_dense:int=0)
jax.experimental.sparse.test_util.rand_bcsr(rng:np.random.RandomState,rand_method:Callable[...,Any]=jtu.rand_default,nse:Union[int,float]=0.5,n_batch:int=0,n_dense:int=0)
jax.experimental.sparse.test_util.rand_sparse(rng,nse=0.5,post=lambdax:x,rand_method=jtu.rand_default)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/random.py----------------------------------------
A:jax.experimental.sparse.random.shape->tuple(map(operator.index, shape))
A:jax.experimental.sparse.random.n_batch->operator.index(n_batch)
A:jax.experimental.sparse.random.n_dense->operator.index(n_dense)
A:jax.experimental.sparse.random.(batch_shape, sparse_shape, dense_shape)->map(tuple, split_list(shape, [n_batch, n_sparse]))
A:jax.experimental.sparse.random.batch_size->math.prod(batch_shape)
A:jax.experimental.sparse.random.sparse_size->math.prod(sparse_shape)
A:jax.experimental.sparse.random.nse->operator.index(nse)
A:jax.experimental.sparse.random.indices_dtype->jax.dtypes.canonicalize_dtype(jnp.int_)
A:jax.experimental.sparse.random.flat_ind->jax.random.choice(key, sparse_size, shape=(nse,), replace=not unique_indices).astype(indices_dtype)
A:jax.experimental.sparse.random.keys->jax.random.split(key, batch_size + 1)
A:jax.experimental.sparse.random.data->generator(data_key, shape=data_shape, dtype=dtype, **kwds)
A:jax.experimental.sparse.random.indices->_indices(index_keys).reshape(indices_shape)
A:jax.experimental.sparse.random.mat->jax.experimental.sparse.BCOO((data, indices), shape=shape)
jax.experimental.sparse.random.random_bcoo(key,shape,*,dtype=jnp.float_,indices_dtype=None,nse=0.2,n_batch=0,n_dense=0,unique_indices=True,sorted_indices=False,generator=random.uniform,**kwds)
jax.experimental.sparse.random_bcoo(key,shape,*,dtype=jnp.float_,indices_dtype=None,nse=0.2,n_batch=0,n_dense=0,unique_indices=True,sorted_indices=False,generator=random.uniform,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/util.py----------------------------------------
A:jax.experimental.sparse.util.fun->_vmap(fun, in_axes=in_axes)
A:jax.experimental.sparse.util.(args_flat, in_tree)->jax.tree_util.tree_flatten(args)
A:jax.experimental.sparse.util.in_axes_flat->flatten_axes('vmap in_axes', in_tree, in_axes, kws=False)
A:jax.experimental.sparse.util.size->max((arg.shape[i] for (arg, i) in safe_zip(args_flat, in_axes_flat) if i is not None))
A:jax.experimental.sparse.util.(args_flat, in_axes_flat)->zip(*((arg, None) if i is None else (lax.squeeze(arg, (i,)), None) if arg.shape[i] == 1 else (arg, i) for (arg, i) in zip(args_flat, in_axes_flat)))
A:jax.experimental.sparse.util.new_args->jax.tree_util.tree_unflatten(in_tree, args_flat)
A:jax.experimental.sparse.util.new_in_axes->jax.tree_util.tree_unflatten(in_tree, in_axes_flat)
A:jax.experimental.sparse.util.(row, col)->_csr_to_coo(indices, indptr)
A:jax.experimental.sparse.util.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.util.mask->mask.sum(tuple(range(n_batch, mask.ndim))).sum(tuple(range(n_batch, mask.ndim)))
A:jax.experimental.sparse.util.lhs->jax._src.core.ShapedArray(lhs_shape, np.float32)
A:jax.experimental.sparse.util.rhs->jax._src.core.ShapedArray(rhs_shape, np.float32)
jax.experimental.sparse.CuSparseEfficiencyWarning(SparseEfficiencyWarning)
jax.experimental.sparse.SparseEfficiencyError(ValueError)
jax.experimental.sparse.SparseEfficiencyWarning(UserWarning)
jax.experimental.sparse.util.CuSparseEfficiencyWarning(SparseEfficiencyWarning)
jax.experimental.sparse.util.SparseEfficiencyError(ValueError)
jax.experimental.sparse.util.SparseEfficiencyWarning(UserWarning)
jax.experimental.sparse.util.SparseInfo(NamedTuple)
jax.experimental.sparse.util._coo_extract(row:Array,col:Array,mat:Array)->Array
jax.experimental.sparse.util._count_stored_elements(mat:Array,n_batch:int=0,n_dense:int=0)->int
jax.experimental.sparse.util._count_stored_elements_per_batch(mat:Array,n_batch:int=0,n_dense:int=0)->Array
jax.experimental.sparse.util._csr_extract(indices:Array,indptr:Array,mat:Array)->Array
jax.experimental.sparse.util._csr_to_coo(indices:Array,indptr:Array)->tuple[Array, Array]
jax.experimental.sparse.util._dot_general_validated_shape(lhs_shape:tuple[int,...],rhs_shape:tuple[int,...],dimension_numbers:DotDimensionNumbers)->tuple[int, ...]
jax.experimental.sparse.util.broadcasting_vmap(fun,in_axes=0,out_axes=0)
jax.experimental.sparse.util.nfold_vmap(fun,N,*,broadcasted=True,in_axes=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/sparse/transform.py----------------------------------------
A:jax.experimental.sparse.transform.self._buffers->list(bufs)
A:jax.experimental.sparse.transform.data_ref->self._push(data)
A:jax.experimental.sparse.transform.indices_ref->self._push(indices)
A:jax.experimental.sparse.transform.indptr_ref->self._push(indptr)
A:jax.experimental.sparse.transform.data->SparsifyEnv().data(spvalue)
A:jax.experimental.sparse.transform.val->getattr(obj, name)
A:jax.experimental.sparse.transform.(spvalue,)->arrays_to_spvalues(self.main.spenv, [val])
A:jax.experimental.sparse.transform.spenv->SparsifyEnv()
A:jax.experimental.sparse.transform.out_spvalues->arrays_to_spvalues(spenv, out_bufs if primitive.multiple_results else [out_bufs])
A:jax.experimental.sparse.transform.out_bufs->prim.bind(*(spenv.data(val) for val in invals), **eqn.params)
A:jax.experimental.sparse.transform.out_tracers->tuple((SparseTracer(self, spvalue=spvalue) for spvalue in out_spvalues))
A:jax.experimental.sparse.transform.spvalues->arrays_to_spvalues(spenv, args)
A:jax.experimental.sparse.transform.(fun, out_spvalues)->sparsify_subtrace(wrapped_fun, main, spvalues)
A:jax.experimental.sparse.transform.params->dict(params, donated_invars=tuple((False for buf in in_bufs)))
A:jax.experimental.sparse.transform.bufs_out->call_primitive.bind(fun, *in_bufs, **params)
A:jax.experimental.sparse.transform.trace->main.with_cur_sublevel()
A:jax.experimental.sparse.transform.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.sparse.transform.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(f, params), in_tree)
A:jax.experimental.sparse.transform.out->eval_sparse(call_jaxpr.jaxpr, call_jaxpr.consts, sparrs, spenv)
A:jax.experimental.sparse.transform.env[var]->SparsifyEnv().dense(a)
A:jax.experimental.sparse.transform.invals->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.(spvalues_flat, in_tree)->tree_flatten(spvalues, is_leaf=_is_spvalue)
A:jax.experimental.sparse.transform.in_avals_flat->spvalues_to_avals(spenv, spvalues_flat)
A:jax.experimental.sparse.transform.(jaxpr, out_avals_flat, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, in_avals_flat)
A:jax.experimental.sparse.transform.result->sparsify(functools.partial(lax_numpy._rewriting_take, idx=idx, indices_are_sorted=indices_are_sorted, mode=mode, unique_indices=unique_indices, fill_value=fill_value))(arr)
A:jax.experimental.sparse.transform.f_raw->sparsify_raw(f)
A:jax.experimental.sparse.transform.(spvalues_out, out_tree)->f_raw(spenv, *spvalues, **params)
A:jax.experimental.sparse.transform.arr->arr.sum_duplicates(nse=arr.nse, remove_zeros=False).sum_duplicates(nse=arr.nse, remove_zeros=False)
A:jax.experimental.sparse.transform.spvalue->SparsifyEnv().sparse(out_shape, **kwds)
A:jax.experimental.sparse.transform.buf->SparsifyEnv().data(spvalue)
A:jax.experimental.sparse.transform.buf_out->prim.bind(buf, **kwargs)
A:jax.experimental.sparse.transform.out_spvalue->SparsifyEnv().sparse(out_shape, mat.data, mat.indices)
A:jax.experimental.sparse.transform.sparse_rules_bcoo[_prim]->_zero_preserving_unary_op(_prim, linear=True)
A:jax.experimental.sparse.transform.sparse_rules_bcsr[_prim]->_zero_preserving_unary_op(_prim, linear=True)
A:jax.experimental.sparse.transform.sparse_rules_bcoo[prim]->_standard_sparse_rule(prim, bcoo_impl)
A:jax.experimental.sparse.transform.sparse_rules_bcsr[prim]->_standard_sparse_rule(prim, bcsr_impl)
A:jax.experimental.sparse.transform.permutation->tuple(permutation)
A:jax.experimental.sparse.transform.args->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.mat_transposed->jax.experimental.sparse.bcoo_transpose(args[0], permutation=permutation)
A:jax.experimental.sparse.transform.out_shape->jax.lax.broadcast_shapes(X.shape, Y.shape)
A:jax.experimental.sparse.transform.out_data->bcoo_multiply_dense(X_promoted, 1.0 / spenv.data(Y))
A:jax.experimental.sparse.transform.out_indices->jax.lax.concatenate([spenv.indices(X), spenv.indices(Y)], dimension=spenv.indices(X).ndim - 2)
A:jax.experimental.sparse.transform.(X_promoted, Y_promoted)->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.mat->jax.experimental.sparse.bcoo_reduce_sum(X_promoted, axes=axes)
A:jax.experimental.sparse.transform.X_promoted->spvalues_to_arrays(spenv, X)
A:jax.experimental.sparse.transform.(operand, start_indices)->spvalues_to_arrays(spenv, args)
A:jax.experimental.sparse.transform.(out_flat, out_tree)->tree_flatten(out)
A:jax.experimental.sparse.transform.(sp_jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped, avals_flat)
A:jax.experimental.sparse.transform.sp_jaxpr->jax._src.interpreters.partial_eval.ClosedJaxpr(sp_jaxpr, consts)
A:jax.experimental.sparse.transform.(cond_const_spvalues, body_const_spvalues, init_val_spvalues)->split_list(spvalues, [cond_nconsts, body_nconsts])
A:jax.experimental.sparse.transform.(cond_sp_jaxpr, _)->_sparsify_jaxpr(spenv, cond_jaxpr, *cond_const_spvalues, *init_val_spvalues)
A:jax.experimental.sparse.transform.(body_sp_jaxpr, out_tree)->_sparsify_jaxpr(spenv, body_jaxpr, *body_const_spvalues, *init_val_spvalues)
A:jax.experimental.sparse.transform.(cond_consts, _)->tree_flatten(spvalues_to_arrays(spenv, cond_const_spvalues))
A:jax.experimental.sparse.transform.(body_consts, _)->tree_flatten(spvalues_to_arrays(spenv, body_const_spvalues))
A:jax.experimental.sparse.transform.(init_vals, _)->tree_flatten(spvalues_to_arrays(spenv, init_val_spvalues))
A:jax.experimental.sparse.transform.out_flat->jax.lax.cond_p.bind(*args, branches=sp_branches, linear=sp_linear, **params)
A:jax.experimental.sparse.transform.(sp_call_jaxpr, out_tree)->_sparsify_jaxpr(spenv, call_jaxpr, *spvalues)
A:jax.experimental.sparse.transform.(args_flat, _)->tree_flatten(spvalues_to_arrays(spenv, spvalues))
A:jax.experimental.sparse.transform.donated_invars->tuple((False for arg in args_flat))
A:jax.experimental.sparse.transform.(const_spvalues, carry_spvalues, xs_spvalues)->split_list(spvalues, [num_consts, num_carry])
A:jax.experimental.sparse.transform.(sp_jaxpr, _)->_sparsify_jaxpr(spenv, jaxpr, *const_spvalues, *carry_spvalues, *xs_spvalues)
A:jax.experimental.sparse.transform.(consts, _)->tree_flatten(spvalues_to_arrays(spenv, const_spvalues))
A:jax.experimental.sparse.transform.(carry, carry_tree)->tree_flatten(spvalues_to_arrays(spenv, carry_spvalues))
A:jax.experimental.sparse.transform.(xs, xs_tree)->tree_flatten(spvalues_to_arrays(spenv, xs_spvalues))
A:jax.experimental.sparse.transform.(const_linear, carry_linear, xs_linear)->split_list(params.pop('linear'), [num_consts, num_carry])
A:jax.experimental.sparse.transform.carry_out->tree_unflatten(carry_tree, out[:len(carry)])
A:jax.experimental.sparse.transform.xs_out->tree_unflatten(xs_tree, out[len(carry):])
A:jax.experimental.sparse.transform.(sp_branches, treedefs)->zip(*(_sparsify_jaxpr(spenv, jaxpr, *operands) for jaxpr in branches))
A:jax.experimental.sparse.transform.sp_linear->tuple(_duplicate_for_sparse_spvalues(operands, linear))
A:jax.experimental.sparse.transform.(args, _)->tree_flatten(spvalues_to_arrays(spenv, (pred, *operands)))
A:jax.experimental.sparse.transform.call_jaxpr->dict(params, donated_invars=tuple((False for buf in in_bufs))).pop('call_jaxpr')
A:jax.experimental.sparse.transform.jvp_jaxpr_thunk->dict(params, donated_invars=tuple((False for buf in in_bufs))).pop('jvp_jaxpr_thunk')
A:jax.experimental.sparse.transform.num_consts->dict(params, donated_invars=tuple((False for buf in in_bufs))).pop('num_consts')
A:jax.experimental.sparse.transform.sparrs->arrays_to_spvalues(spenv, arrs)
A:jax.experimental.sparse.transform.jvp->lift_jvp(num_consts, jvp_jaxpr_thunk)
A:jax.experimental.sparse.transform.outvals->jax.custom_derivatives.custom_jvp_call_p.bind(fun, jvp, *invals, **params)
jax.experimental.sparse.SparseTracer(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.SparseTracer.aval(self)
jax.experimental.sparse.SparseTracer.full_lower(self)
jax.experimental.sparse.SparseTracer.spenv(self)
jax.experimental.sparse.sparsify(f,use_tracer=False)
jax.experimental.sparse.sparsify_fun(wrapped_fun,args:list[ArrayOrSparse])
jax.experimental.sparse.sparsify_raw(f)
jax.experimental.sparse.sparsify_subtrace(main,spvalues,*bufs)
jax.experimental.sparse.transform.SparseTrace(core.Trace)
jax.experimental.sparse.transform.SparseTrace.lift(self,val:core.Tracer)
jax.experimental.sparse.transform.SparseTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.sparse.transform.SparseTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers,*,symbolic_zeros)
jax.experimental.sparse.transform.SparseTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.sparse.transform.SparseTrace.pure(self,val:Any)
jax.experimental.sparse.transform.SparseTrace.sublift(self,val:SparseTracer)
jax.experimental.sparse.transform.SparseTracer(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.transform.SparseTracer.__init__(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.transform.SparseTracer.aval(self)
jax.experimental.sparse.transform.SparseTracer.full_lower(self)
jax.experimental.sparse.transform.SparseTracer.spenv(self)
jax.experimental.sparse.transform.SparsifyEnv(self,bufs=())
jax.experimental.sparse.transform.SparsifyEnv.__init__(self,bufs=())
jax.experimental.sparse.transform.SparsifyEnv._push(self,arr:Array)->int
jax.experimental.sparse.transform.SparsifyEnv.data(self,spvalue:SparsifyValue)->Array
jax.experimental.sparse.transform.SparsifyEnv.dense(self,data)
jax.experimental.sparse.transform.SparsifyEnv.indices(self,spvalue:SparsifyValue)->Array
jax.experimental.sparse.transform.SparsifyEnv.indptr(self,spvalue:SparsifyValue)->Array
jax.experimental.sparse.transform.SparsifyEnv.sparse(self,shape,data=None,indices=None,indptr=None,*,data_ref=None,indices_ref=None,indptr_ref=None,indices_sorted=False,unique_indices=False)
jax.experimental.sparse.transform.SparsifyValue(NamedTuple)
jax.experimental.sparse.transform.SparsifyValue.is_bcoo(self)
jax.experimental.sparse.transform.SparsifyValue.is_bcsr(self)
jax.experimental.sparse.transform.SparsifyValue.is_dense(self)
jax.experimental.sparse.transform.SparsifyValue.is_sparse(self)
jax.experimental.sparse.transform.SparsifyValue.ndim(self)
jax.experimental.sparse.transform._add_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._astype(self,*args,**kwargs)
jax.experimental.sparse.transform._bcoo_rewriting_take(arr,idx,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax.experimental.sparse.transform._bcsr_rewriting_take(arr,idx,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax.experimental.sparse.transform._cond_sparse(spenv,pred,*operands,branches,linear,**params)
jax.experimental.sparse.transform._custom_jvp_sparse_rule(spenv,*spvalues,**params)
jax.experimental.sparse.transform._div_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._duplicate_for_sparse_spvalues(spvalues,params)
jax.experimental.sparse.transform._ensure_unique_indices(spenv,spvalue)
jax.experimental.sparse.transform._gather_sparse_rule(spenv,*args,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax.experimental.sparse.transform._integer_pow_sparse(spenv,*spvalues,y)
jax.experimental.sparse.transform._mul_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._pjit_sparse(spenv,*spvalues,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax.experimental.sparse.transform._raise_unimplemented_primitive(primitive)
jax.experimental.sparse.transform._reduce_sum_sparse(spenv,*spvalues,axes)
jax.experimental.sparse.transform._reshape(self,*args,**kwargs)
jax.experimental.sparse.transform._scan_sparse(spenv,*spvalues,jaxpr,num_consts,num_carry,**params)
jax.experimental.sparse.transform._sparse_iter(arr)
jax.experimental.sparse.transform._sparsify_jaxpr(spenv,jaxpr,*spvalues)
jax.experimental.sparse.transform._sparsify_with_interpreter(f)
jax.experimental.sparse.transform._sparsify_with_tracer(fun)
jax.experimental.sparse.transform._standard_sparse_rule(prim,sparse_op)
jax.experimental.sparse.transform._sub_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._sum(self,*args,**kwargs)
jax.experimental.sparse.transform._todense_sparse_rule(spenv,spvalue,*,tree)
jax.experimental.sparse.transform._transpose_sparse(spenv,*spvalues,permutation)
jax.experimental.sparse.transform._while_sparse(spenv,*spvalues,cond_jaxpr,cond_nconsts,body_jaxpr,body_nconsts)
jax.experimental.sparse.transform._zero_preserving_unary_op(prim,linear)
jax.experimental.sparse.transform.arrays_to_spvalues(spenv:SparsifyEnv,args:Any)->Any
jax.experimental.sparse.transform.eval_sparse(jaxpr:core.Jaxpr,consts:Sequence[Array],spvalues:Sequence[SparsifyValue],spenv:SparsifyEnv)->Sequence[SparsifyValue]
jax.experimental.sparse.transform.popattr(obj:Any,name:str)->Any
jax.experimental.sparse.transform.setnewattr(obj:Any,name:str,val:Any)
jax.experimental.sparse.transform.sparsify(f,use_tracer=False)
jax.experimental.sparse.transform.sparsify_fun(wrapped_fun,args:list[ArrayOrSparse])
jax.experimental.sparse.transform.sparsify_raw(f)
jax.experimental.sparse.transform.sparsify_subtrace(main,spvalues,*bufs)
jax.experimental.sparse.transform.spvalues_to_arrays(spenv:SparsifyEnv,spvalues:Any)->Any
jax.experimental.sparse.transform.spvalues_to_avals(spenv:SparsifyEnv,spvalues:Any)->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/gpu.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/tpu.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/attention.py----------------------------------------
A:jax.experimental.pallas.ops.attention.start_q->jax.experimental.pallas.program_id(0)
A:jax.experimental.pallas.ops.attention.l_i->jax.numpy.zeros(block_q, dtype=jnp.float32)
A:jax.experimental.pallas.ops.attention.acc->acc.astype(o_ref.dtype).astype(o_ref.dtype)
A:jax.experimental.pallas.ops.attention.q->jax.experimental.pallas.load(q_ref, (pl.ds(start_q * block_q, block_q), slice(None)))
A:jax.experimental.pallas.ops.attention.k->jax.experimental.pallas.load(k_ref, (pl.ds(start_k * block_k, block_k), slice(None)))
A:jax.experimental.pallas.ops.attention.qk->jax.numpy.where(mask, qk, DEFAULT_MASK_VALUE)
A:jax.experimental.pallas.ops.attention.mask->jax.numpy.broadcast_to(mask, logits.shape)
A:jax.experimental.pallas.ops.attention.m_curr->jax.numpy.maximum(jnp.max(qk, axis=1), m_prev)
A:jax.experimental.pallas.ops.attention.p->jax.numpy.exp(qk - m[:, None])
A:jax.experimental.pallas.ops.attention.v->jax.experimental.pallas.load(v_ref, (pl.ds(start_k * block_k, block_k), slice(None)))
A:jax.experimental.pallas.ops.attention.upper_bound->jax.experimental.pallas.cdiv(seq_len, block_k)
A:jax.experimental.pallas.ops.attention.(acc, m_i, l_i)->jax.lax.fori_loop(0, upper_bound, body, (acc, m_i, l_i))
A:jax.experimental.pallas.ops.attention.q_segment_ids->jax.numpy.expand_dims(q_segment_ids, axis=-1)
A:jax.experimental.pallas.ops.attention.kv_segment_ids->jax.numpy.expand_dims(kv_segment_ids, axis=1)
A:jax.experimental.pallas.ops.attention.block_q->min(block_q, seq_len)
A:jax.experimental.pallas.ops.attention.block_k->min(block_k, seq_len)
A:jax.experimental.pallas.ops.attention.kernel->functools.partial(mha_forward_kernel, sm_scale=sm_scale, causal=causal, block_q=block_q, block_k=block_k, block_d=head_dim)
A:jax.experimental.pallas.ops.attention.out_shape->jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)
A:jax.experimental.pallas.ops.attention.(out, l, m)->jax.experimental.pallas.pallas_call(kernel, grid=grid_, in_specs=in_specs, out_specs=[pl.BlockSpec(lambda _, j, k: (j, 0, k, 0), (None, seq_len, None, head_dim)), pl.BlockSpec(lambda _, j, k: (j, k, 0), (None, None, seq_len)), pl.BlockSpec(lambda _, j, k: (j, k, 0), (None, None, seq_len))], num_warps=num_warps_, num_stages=num_stages, out_shape=out_shape, debug=debug, interpret=interpret, name='mha_forward')(q, k, v, segment_ids)
A:jax.experimental.pallas.ops.attention.pid_m->jax.experimental.pallas.program_id(0)
A:jax.experimental.pallas.ops.attention.off_m->jax.experimental.pallas.ds(pid_m * block_q, block_q)
A:jax.experimental.pallas.ops.attention.o->jax.experimental.pallas.load(out_ref, (off_m, slice(None))).astype(jnp.float32)
A:jax.experimental.pallas.ops.attention.do->jax.experimental.pallas.load(do_scaled_ref, (pl.ds(start_q * block_q, block_q), slice(None)))
A:jax.experimental.pallas.ops.attention.denom->jax.experimental.pallas.load(l_ref, (off_m,)).astype(jnp.float32)
A:jax.experimental.pallas.ops.attention.delta->jax.numpy.sum(o * do, axis=1)
A:jax.experimental.pallas.ops.attention.(do_scaled, delta)->_preprocess_backward(out, do, l, block_q, debug, interpret)
A:jax.experimental.pallas.ops.attention.dv->jax.numpy.zeros([block_k, block_d], dtype=jnp.float32)
A:jax.experimental.pallas.ops.attention.dk->jax.numpy.zeros([block_k, block_d], dtype=jnp.float32)
A:jax.experimental.pallas.ops.attention.m->jax.experimental.pallas.load(m_ref, (pl.ds(start_q * block_q, block_q),))
A:jax.experimental.pallas.ops.attention.di->jax.experimental.pallas.load(delta_ref, (pl.ds(start_q * block_q, block_q),))
A:jax.experimental.pallas.ops.attention.dq->jax.numpy.zeros(q.shape, jnp.float32)
A:jax.experimental.pallas.ops.attention.lower_bound->jax.lax.div(start_k * block_k, block_q)
A:jax.experimental.pallas.ops.attention.(dv, dk)->jax.lax.fori_loop(lower_bound, pl.cdiv(seq_len, block_q), inner_loop, (dv, dk))
A:jax.experimental.pallas.ops.attention.(dq, dk, dv)->jax.experimental.pallas.pallas_call(functools.partial(mha_backward_kernel, block_q=block_q, block_d=head_dim, block_k=block_k, sm_scale=sm_scale, causal=causal), grid=grid, out_shape=out_shapes, in_specs=in_specs, out_specs=[pl.BlockSpec(lambda j, k: (j, 0, k, 0), (None, seq_len, None, head_dim)), pl.BlockSpec(lambda j, k: (j, 0, k, 0), (None, seq_len, None, head_dim)), pl.BlockSpec(lambda j, k: (j, 0, k, 0), (None, seq_len, None, head_dim))], name='mha_backward', debug=debug, interpret=interpret, num_warps=num_warps, num_stages=1, input_output_aliases=input_output_aliases)(q, k, v, segment_ids, out, do_scaled, l, m, delta, dq)
A:jax.experimental.pallas.ops.attention.logits->jax.numpy.einsum('bqhc,bkhc->bhqk', q, k).astype(jnp.float32)
A:jax.experimental.pallas.ops.attention.causal_mask->jax.numpy.broadcast_to(causal_mask, logits.shape)
A:jax.experimental.pallas.ops.attention.weights->jax.nn.softmax(logits * sm_scale).astype(q.dtype)
jax.experimental.pallas.ops.attention._mha_backward(sm_scale:float,causal:bool,block_q:int,block_k:int,backward_pass_impl:str,num_warps:Optional[int],num_stages:int,grid:Any,interpret:bool,debug:bool,res,do)
jax.experimental.pallas.ops.attention._mha_forward(q,k,v,segment_ids:jax.Array|None,sm_scale:float,causal:bool,block_q:int,block_k:int,backward_pass_impl:str,num_warps:Optional[int],num_stages:int,grid:Any,interpret:bool,debug:bool)
jax.experimental.pallas.ops.attention._preprocess_backward(out,do,l,block_q:int,debug:bool,interpret:bool)
jax.experimental.pallas.ops.attention._preprocess_backward_kernel(out_ref,dout_ref,l_ref,new_dout_ref,delta_ref,*,block_q:int)
jax.experimental.pallas.ops.attention.mha(q,k,v,segment_ids:jnp.ndarray|None,sm_scale:float=1.0,causal:bool=False,block_q:int=128,block_k:int=128,backward_pass_impl:str='triton',num_warps:Optional[int]=None,num_stages:int=2,grid=None,interpret:bool=False,debug:bool=False)
jax.experimental.pallas.ops.attention.mha_backward_kernel(q_ref,k_ref,v_ref,segment_ids_ref:jax.Array|None,out_ref,do_scaled_ref,l_ref,m_ref,delta_ref,_,dq_ref,dk_ref,dv_ref,*,sm_scale:float,causal:bool,block_q:int,block_d:int,block_k:int)
jax.experimental.pallas.ops.attention.mha_forward_kernel(q_ref,k_ref,v_ref,segment_ids_ref:jax.Array|None,o_ref:Any,*residual_refs:Any,sm_scale:float,causal:bool,block_q:int,block_d:int,block_k:int)
jax.experimental.pallas.ops.attention.mha_reference(q,k,v,segment_ids:jnp.ndarray|None,sm_scale=1.0,causal:bool=False)
jax.experimental.pallas.ops.attention.segment_mask(q_segment_ids:jax.Array,kv_segment_ids:jax.Array)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/softmax.py----------------------------------------
A:jax.experimental.pallas.ops.softmax.row->jax.experimental.pallas.load(input_ref, (pl.dslice(0, block_row),), mask=mask, other=-float('inf'))
A:jax.experimental.pallas.ops.softmax.row_max->jax.numpy.max(row, axis=0)
A:jax.experimental.pallas.ops.softmax.numerator->jax.numpy.exp((row - row_max).astype(jnp.float32))
A:jax.experimental.pallas.ops.softmax.denominator->jax.numpy.sum(numerator, axis=0)
A:jax.experimental.pallas.ops.softmax.block_row->jax.experimental.pallas.next_power_of_2(row_len)
A:jax.experimental.pallas.ops.softmax.out_shape->jax.ShapeDtypeStruct(shape=(row_len,), dtype=x.dtype)
A:jax.experimental.pallas.ops.softmax.kernel->functools.partial(_vmappable_softmax_kernel, block_row=block_row)
A:jax.experimental.pallas.ops.softmax.f->jax.vmap(f)
jax.experimental.pallas.ops.softmax._vmappable_softmax_kernel(input_ref,probs_ref,*,block_row:int)
jax.experimental.pallas.ops.softmax.softmax(x:jax.Array,*,axis:int=-1,num_warps:int=4,interpret:bool=False,debug:bool=False)->jax.Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/rms_norm.py----------------------------------------
A:jax.experimental.pallas.ops.rms_norm.a->jax.experimental.pallas.load(x_ref, (row_idx[:, None], col_idx[None]), mask=mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.rms_norm.rstd_ref[...]->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32).astype(rstd_ref.dtype)
A:jax.experimental.pallas.ops.rms_norm.weight->jax.experimental.pallas.load(weight_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_last').astype(jnp.float32)
A:jax.experimental.pallas.ops.rms_norm.bias->jax.experimental.pallas.load(bias_ref, (col_idx,), mask=mask)
A:jax.experimental.pallas.ops.rms_norm.x->jax.experimental.pallas.load(x_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_first').astype(jnp.float32)
A:jax.experimental.pallas.ops.rms_norm.block_size->min(max(block_size, 128), 4096)
A:jax.experimental.pallas.ops.rms_norm.num_warps->min(max(block_size // 256, 1), 8)
A:jax.experimental.pallas.ops.rms_norm.kernel->functools.partial(rms_norm_forward_kernel, eps=eps, block_size=block_size)
A:jax.experimental.pallas.ops.rms_norm.method->jax.vmap(jax.vmap(method, in_axes=(0, None, None)), in_axes=(0, None, None))
A:jax.experimental.pallas.ops.rms_norm.(out, rstd)->method(x, weight, bias)
A:jax.experimental.pallas.ops.rms_norm.dout->jax.experimental.pallas.load(do_ref, (row_idx[:, None], col_idx[None]), mask=mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.rms_norm.c1->for_loop(pl.cdiv(n_col, block_size), mean_body, jnp.zeros(block_size))
A:jax.experimental.pallas.ops.rms_norm.j->jax.experimental.pallas.program_id(0)
A:jax.experimental.pallas.ops.rms_norm.rstd->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.rms_norm.(dw_acc, db_acc)->for_loop(pl.cdiv(m, block_m), body, (jnp.zeros(block_n), jnp.zeros(block_n)))
A:jax.experimental.pallas.ops.rms_norm.reshaped_x->jax.experimental.pallas.load(x_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_first').astype(jnp.float32).reshape((-1, n))
A:jax.experimental.pallas.ops.rms_norm.reshaped_rstd->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32).reshape((-1,))
A:jax.experimental.pallas.ops.rms_norm.reshaped_do->do.reshape((-1, n))
A:jax.experimental.pallas.ops.rms_norm.out_shape_dx->jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)
A:jax.experimental.pallas.ops.rms_norm.dx->dx.reshape((*shape_prefix, n)).reshape((*shape_prefix, n))
A:jax.experimental.pallas.ops.rms_norm.(dw, dbias)->method(reshaped_x, weight, bias, reshaped_do, reshaped_rstd)
A:jax.experimental.pallas.ops.rms_norm.out_shape->jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)
A:jax.experimental.pallas.ops.rms_norm.var->jax.numpy.mean(jnp.square(x), axis=1)
A:jax.experimental.pallas.ops.rms_norm.mul->jax.lax.rsqrt(var + eps)
jax.experimental.pallas.ops.rms_norm.rms_norm(x,weight,bias,num_warps:Optional[int]=None,num_stages:Optional[int]=3,eps:float=1e-05,backward_pass_impl:str='triton',interpret:bool=False)
jax.experimental.pallas.ops.rms_norm.rms_norm_backward(num_warps:Optional[int],num_stages:Optional[int],eps:float,backward_pass_impl:str,interpret:bool,res,do)
jax.experimental.pallas.ops.rms_norm.rms_norm_backward_kernel_dw_db(x_ref,weight_ref,bias_ref,do_ref,rstd_ref,dw_ref,db_ref,*,eps:float,block_m:int,block_n:int)
jax.experimental.pallas.ops.rms_norm.rms_norm_backward_kernel_dx(x_ref,weight_ref,bias_ref,do_ref,rstd_ref,dx_ref,*,eps:float,block_size:int)
jax.experimental.pallas.ops.rms_norm.rms_norm_forward(x,weight,bias,num_warps:Optional[int]=None,num_stages:Optional[int]=3,eps:float=1e-05,backward_pass_impl:str='triton',interpret:bool=False)
jax.experimental.pallas.ops.rms_norm.rms_norm_forward_kernel(x_ref,weight_ref,bias_ref,o_ref,rstd_ref=None,*,eps:float,block_size:int)
jax.experimental.pallas.ops.rms_norm.rms_norm_reference(x,weight,bias,*,eps:float=1e-05)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/layer_norm.py----------------------------------------
A:jax.experimental.pallas.ops.layer_norm.a->jax.experimental.pallas.load(x_ref, (row_idx[:, None], col_idx[None]), mask=mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.layer_norm.mean_ref[...]->jax.numpy.mean(x, axis=1).astype(mean_ref.dtype)
A:jax.experimental.pallas.ops.layer_norm.rstd_ref[...]->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32).astype(rstd_ref.dtype)
A:jax.experimental.pallas.ops.layer_norm.weight->jax.experimental.pallas.load(weight_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_last').astype(jnp.float32)
A:jax.experimental.pallas.ops.layer_norm.bias->jax.experimental.pallas.load(bias_ref, (col_idx,), mask=mask)
A:jax.experimental.pallas.ops.layer_norm.x->jax.experimental.pallas.load(x_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_first').astype(jnp.float32)
A:jax.experimental.pallas.ops.layer_norm.block_size->min(max(block_size, 128), 4096)
A:jax.experimental.pallas.ops.layer_norm.num_warps->min(max(block_size // 256, 1), 8)
A:jax.experimental.pallas.ops.layer_norm.kernel->functools.partial(layer_norm_forward_kernel, eps=eps, block_size=block_size)
A:jax.experimental.pallas.ops.layer_norm.method->jax.vmap(jax.vmap(method, in_axes=(0, None, None)), in_axes=(0, None, None))
A:jax.experimental.pallas.ops.layer_norm.(out, mean, rstd)->method(x, weight, bias)
A:jax.experimental.pallas.ops.layer_norm.dout->jax.experimental.pallas.load(do_ref, (row_idx[:, None], col_idx[None]), mask=mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.layer_norm.mean->jax.numpy.mean(x, axis=1)
A:jax.experimental.pallas.ops.layer_norm.j->jax.experimental.pallas.program_id(0)
A:jax.experimental.pallas.ops.layer_norm.rstd->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32)
A:jax.experimental.pallas.ops.layer_norm.(dw_acc, db_acc)->for_loop(pl.cdiv(m, block_m), body, (jnp.zeros(block_n), jnp.zeros(block_n)))
A:jax.experimental.pallas.ops.layer_norm.reshaped_x->jax.experimental.pallas.load(x_ref, (col_idx,), mask=mask, other=0.0, eviction_policy='evict_first').astype(jnp.float32).reshape((-1, n))
A:jax.experimental.pallas.ops.layer_norm.reshaped_mean->jax.numpy.mean(x, axis=1).reshape((-1,))
A:jax.experimental.pallas.ops.layer_norm.reshaped_rstd->jax.experimental.pallas.load(rstd_ref, (row_idx,), mask=row_mask, other=0.0).astype(jnp.float32).reshape((-1,))
A:jax.experimental.pallas.ops.layer_norm.reshaped_do->do.reshape((-1, n))
A:jax.experimental.pallas.ops.layer_norm.out_shape_dx->jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)
A:jax.experimental.pallas.ops.layer_norm.dx->dx.reshape((*shape_prefix, n)).reshape((*shape_prefix, n))
A:jax.experimental.pallas.ops.layer_norm.(dw, dbias)->method(reshaped_x, weight, bias, reshaped_do, reshaped_mean, reshaped_rstd)
A:jax.experimental.pallas.ops.layer_norm.out_shape->jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)
A:jax.experimental.pallas.ops.layer_norm.mean2->jax.numpy.mean(jnp.square(x), axis=1)
A:jax.experimental.pallas.ops.layer_norm.var->jax.numpy.maximum(0.0, mean2 - jnp.square(mean))
A:jax.experimental.pallas.ops.layer_norm.mul->jax.lax.rsqrt(var + eps)
jax.experimental.pallas.ops.layer_norm.layer_norm(x,weight,bias,num_warps:Optional[int]=None,num_stages:Optional[int]=3,eps:float=1e-05,backward_pass_impl:str='triton',interpret:bool=False)
jax.experimental.pallas.ops.layer_norm.layer_norm_backward(num_warps:Optional[int],num_stages:Optional[int],eps:float,backward_pass_impl:str,interpret:bool,res,do)
jax.experimental.pallas.ops.layer_norm.layer_norm_backward_kernel_dw_db(x_ref,weight_ref,bias_ref,do_ref,mean_ref,rstd_ref,dw_ref,db_ref,*,eps:float,block_m:int,block_n:int)
jax.experimental.pallas.ops.layer_norm.layer_norm_backward_kernel_dx(x_ref,weight_ref,bias_ref,do_ref,mean_ref,rstd_ref,dx_ref,*,eps:float,block_size:int)
jax.experimental.pallas.ops.layer_norm.layer_norm_forward(x,weight,bias,num_warps:Optional[int]=None,num_stages:Optional[int]=3,eps:float=1e-05,backward_pass_impl:str='triton',interpret:bool=False)
jax.experimental.pallas.ops.layer_norm.layer_norm_forward_kernel(x_ref,weight_ref,bias_ref,o_ref,mean_ref=None,rstd_ref=None,*,eps:float,block_size:int)
jax.experimental.pallas.ops.layer_norm.layer_norm_reference(x,weight,bias,*,eps:float=1e-05)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/tpu/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/pallas/ops/tpu/flash_attention.py----------------------------------------
A:jax.experimental.pallas.ops.tpu.flash_attention.block_sizes->BlockSizes.get_default(batch_size, num_heads, q_seq_len, kv_seq_len, d_model)
A:jax.experimental.pallas.ops.tpu.flash_attention.(o, l, m)->_flash_attention(q, k, v, ab, segment_ids, True, causal, sm_scale, block_sizes, debug)
A:jax.experimental.pallas.ops.tpu.flash_attention.di->jax.numpy.broadcast_to(di[..., None], (*di.shape, block_k_major))
A:jax.experimental.pallas.ops.tpu.flash_attention.(dk, dv)->_flash_attention_bwd_dkv(q, k, v, ab, segment_ids, l, m, do, di, block_q_major=block_sizes.block_q_major_dkv, block_k_major=block_sizes.block_k_major_dkv, block_k=block_sizes.block_k_dkv, block_q=block_sizes.block_q_dkv, sm_scale=sm_scale, causal=causal, mask_value=DEFAULT_MASK_VALUE, debug=debug)
A:jax.experimental.pallas.ops.tpu.flash_attention.(dq, ds)->_flash_attention_bwd_dq(q, k, v, ab, segment_ids, l, m, do, di, block_q_major=block_sizes.block_q_dq, block_k_major=block_sizes.block_k_major_dq, block_k=block_sizes.block_k_dq, sm_scale=sm_scale, causal=causal, mask_value=DEFAULT_MASK_VALUE, debug=debug)
A:jax.experimental.pallas.ops.tpu.flash_attention.kv_seq_idx->jax.experimental.pallas.program_id(3)
A:jax.experimental.pallas.ops.tpu.flash_attention.m_scratch_ref[batch_idx]->jax.numpy.full(m_scratch_ref.shape[2:], -jnp.inf, jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.l_scratch_ref[batch_idx]->jax.numpy.zeros(l_scratch_ref.shape[2:], jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.acc_scratch_ref[batch_idx]->jax.numpy.zeros(acc_scratch_ref.shape[2:], jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.q_seq_idx->jax.experimental.pallas.program_id(2)
A:jax.experimental.pallas.ops.tpu.flash_attention.should_run->below_or_on_diag(q_seq_index, block_q_major, kv_seq_index, block_k_major)
A:jax.experimental.pallas.ops.tpu.flash_attention.k->jax.experimental.pallas.load(k_tile_ref, (0, 0, k_slice, slice(None)))
A:jax.experimental.pallas.ops.tpu.flash_attention.s->jax.lax.dot_general(q, k, TRANS_B_DIM_NUMBERS, preferred_element_type=jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.ab->jax.experimental.pallas.load(ab_tile_ref, (0, 0, pl.dslice(None), pl.dslice(i * block_k, block_k))).astype(jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.(repeats, rem)->divmod(block_k, NUM_LANES)
A:jax.experimental.pallas.ops.tpu.flash_attention.q_segment_ids->jax.lax.broadcast_in_dim(segment_ids.q, (batch_size, q_seq_len, NUM_LANES), (0, 1))
A:jax.experimental.pallas.ops.tpu.flash_attention.kv_segment_ids->jax.lax.broadcast_in_dim(segment_ids.kv, (batch_size, NUM_SUBLANES, kv_seq_len), (0, 2))
A:jax.experimental.pallas.ops.tpu.flash_attention.mask->jax.numpy.equal(q_segment_ids, kv_segment_ids).astype(jnp.bool_)
A:jax.experimental.pallas.ops.tpu.flash_attention.row_ids->jax.lax.broadcasted_iota(jnp.int32, mask_shape, 0)
A:jax.experimental.pallas.ops.tpu.flash_attention.col_ids->jax.lax.broadcasted_iota(jnp.int32, mask_shape, 1)
A:jax.experimental.pallas.ops.tpu.flash_attention.m_next->jax.numpy.maximum(m_prev, m_curr)
A:jax.experimental.pallas.ops.tpu.flash_attention.(block_k_repeats, rem)->divmod(block_k, MIN_BLOCK_SIZE)
A:jax.experimental.pallas.ops.tpu.flash_attention.p->jax.numpy.exp(capped_logits - pltpu.repeat(m, block_k // MIN_BLOCK_SIZE, axis=1))
A:jax.experimental.pallas.ops.tpu.flash_attention.alpha->jax.numpy.exp(m_prev - m_next)
A:jax.experimental.pallas.ops.tpu.flash_attention.(head_dim_repeats, rem)->divmod(head_dim, MIN_BLOCK_SIZE)
A:jax.experimental.pallas.ops.tpu.flash_attention.l_next_inv_safe->jax.numpy.where(l_next == 0.0, 1.0, 1.0 / l_next)
A:jax.experimental.pallas.ops.tpu.flash_attention.v->jax.experimental.pallas.load(v_tile_ref, (0, 0, k_slice, slice(None)))
A:jax.experimental.pallas.ops.tpu.flash_attention.o_curr->jax.lax.dot(p.astype(v.dtype), v, preferred_element_type=jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.o_tile_ref[batch_idx]->jax.lax.dot(p.astype(v.dtype), v, preferred_element_type=jnp.float32).astype(o_tile_ref.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.l_ref[batch_idx]->jax.lax.broadcast_in_dim(l, l_ref.shape[2:], range(2))
A:jax.experimental.pallas.ops.tpu.flash_attention.m_ref[batch_idx]->jax.lax.broadcast_in_dim(m, m_ref.shape[2:], range(2))
A:jax.experimental.pallas.ops.tpu.flash_attention.next_kv_index->jax.lax.select(below_or_on_diag(q_seq_index, block_q_major, kv_seq_index, block_k_major), kv_seq_index, 0)
A:jax.experimental.pallas.ops.tpu.flash_attention.next_q_index->jax.lax.select(below_or_on_diag(q_seq_index, block_q_major, kv_seq_index, block_k_major), q_seq_index, 0)
A:jax.experimental.pallas.ops.tpu.flash_attention.kernel->functools.partial(_flash_attention_dq_kernel, sm_scale=sm_scale, causal=causal, mask_value=mask_value, block_k=block_k, kv_seq_len=kv_seq_len)
A:jax.experimental.pallas.ops.tpu.flash_attention.out_shape->jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.scratch_shape->functools.partial(jax.ShapeDtypeStruct, dtype=jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.m_scratch->scratch_shape((block_b, 1, block_q, MIN_BLOCK_SIZE))
A:jax.experimental.pallas.ops.tpu.flash_attention.l_scratch->scratch_shape((block_b, 1, block_q, MIN_BLOCK_SIZE))
A:jax.experimental.pallas.ops.tpu.flash_attention.acc_scratch->scratch_shape((block_b, 1, block_q, head_dim))
A:jax.experimental.pallas.ops.tpu.flash_attention.l->jax.numpy.exp(logits - m[..., None]).sum(axis=-1)
A:jax.experimental.pallas.ops.tpu.flash_attention.m->jax.numpy.einsum('bhqc,bhkc->bhqk', q.astype(jnp.float32), k.astype(jnp.float32)).max(axis=-1)
A:jax.experimental.pallas.ops.tpu.flash_attention.q_segment_ids_spec->jax.experimental.pallas.BlockSpec(q_segment_ids_index_map, (1, block_q_major, NUM_LANES))
A:jax.experimental.pallas.ops.tpu.flash_attention.kv_segment_ids_spec->jax.experimental.pallas.BlockSpec(kv_segment_ids_index_map, (1, NUM_SUBLANES, block_k_major))
A:jax.experimental.pallas.ops.tpu.flash_attention.(o, *aux)->jax.experimental.pallas.pallas_call(kernel, out_shape=out_shape, in_specs=in_specs, out_specs=out_specs, grid=grid, debug=debug, mosaic_params=dict(dimension_semantics=('parallel', 'parallel', 'parallel', 'arbitrary')))(q, k, v, ab, q_segment_ids, kv_segment_ids)
A:jax.experimental.pallas.ops.tpu.flash_attention.q_seq_index->jax.experimental.pallas.program_id(axis=2)
A:jax.experimental.pallas.ops.tpu.flash_attention.kv_seq_index->jax.experimental.pallas.program_id(axis=3)
A:jax.experimental.pallas.ops.tpu.flash_attention.dk_scratch_ref[:, :]->jax.numpy.zeros(dk_scratch_ref.shape, dk_scratch_ref.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.dv_scratch_ref[:, :]->jax.numpy.zeros(dv_scratch_ref.shape, dv_scratch_ref.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.q->jax.experimental.pallas.load(q_tile_ref, (0, 0, pl.ds(start_q, block_q), slice(None)))
A:jax.experimental.pallas.ops.tpu.flash_attention.do->jax.experimental.pallas.load(do_tile_ref, (0, 0, pl.ds(start_q, block_q), slice(None)))
A:jax.experimental.pallas.ops.tpu.flash_attention.capped_logits->jax.lax.dot_general(q, k, TRANS_B_DIM_NUMBERS, preferred_element_type=jnp.float32)
A:jax.experimental.pallas.ops.tpu.flash_attention.dv->jax.numpy.einsum('bhpt,bhpd->bhtd', p, do.astype(jnp.float32)).astype(v.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.dp->jax.numpy.einsum('bhpd,bhtd->bhpt', do.astype(jnp.float32), v.astype(jnp.float32))
A:jax.experimental.pallas.ops.tpu.flash_attention.dk->jax.numpy.einsum('bhsd,bhst->bhtd', q.astype(jnp.float32), ds).astype(k.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.dv_tile_ref[0, 0, :, :]->dv_scratch_ref[...].astype(dv_tile_ref)
A:jax.experimental.pallas.ops.tpu.flash_attention.dk_tile_ref[0, 0, :, :]->dk_scratch_ref[...].astype(dk_tile_ref)
A:jax.experimental.pallas.ops.tpu.flash_attention.qo_spec->jax.experimental.pallas.BlockSpec(qo_index_map, (1, 1, block_q_major, head_dim))
A:jax.experimental.pallas.ops.tpu.flash_attention.kv_spec->jax.experimental.pallas.BlockSpec(kv_index_map, (1, 1, block_k_major, head_dim))
A:jax.experimental.pallas.ops.tpu.flash_attention.lm_spec->jax.experimental.pallas.BlockSpec(lm_index_map, (1, 1, block_q_major, MIN_BLOCK_SIZE))
A:jax.experimental.pallas.ops.tpu.flash_attention.di_spec->jax.experimental.pallas.BlockSpec(qo_index_map, (1, 1, block_q_major, MIN_BLOCK_SIZE))
A:jax.experimental.pallas.ops.tpu.flash_attention.dkv_spec->jax.experimental.pallas.BlockSpec(dkv_index_map, (1, 1, block_k_major, head_dim))
A:jax.experimental.pallas.ops.tpu.flash_attention.(dk, dv, _, _)->jax.experimental.pallas.pallas_call(kernel, in_specs=in_specs, out_shape=out_shapes, out_specs=out_specs, grid=grid, debug=debug, mosaic_params=dict(dimension_semantics=('parallel', 'parallel', 'parallel', 'arbitrary')))(q, k, v, ab, q_segment_ids, kv_segment_ids, l, m, do, di)
A:jax.experimental.pallas.ops.tpu.flash_attention.dq_scratch_ref[:, :]->jax.numpy.zeros(dq_scratch_ref.shape, dq_scratch_ref.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.k_slice->jax.experimental.pallas.ds(i * block_k, block_k)
A:jax.experimental.pallas.ops.tpu.flash_attention.should_not_run->jax.lax.select(should_run, False, True)
A:jax.experimental.pallas.ops.tpu.flash_attention.ds_tile_ref[...]->jax.numpy.zeros_like(ds_tile_ref)
A:jax.experimental.pallas.ops.tpu.flash_attention.dq_tile_ref[0, 0, :, :]->dq_scratch_ref[...].astype(dq_tile_ref)
A:jax.experimental.pallas.ops.tpu.flash_attention.dq_scratch_ref[...]->jax.numpy.zeros_like(dq_scratch_ref)
A:jax.experimental.pallas.ops.tpu.flash_attention.dq_spec->jax.experimental.pallas.BlockSpec(qo_index_map, (1, 1, block_q_major, head_dim))
A:jax.experimental.pallas.ops.tpu.flash_attention.(dq, _, ds)->jax.experimental.pallas.pallas_call(kernel, in_specs=in_specs, out_shape=out_shapes, out_specs=out_specs, grid=grid, debug=debug, mosaic_params=dict(dimension_semantics=('parallel', 'parallel', 'parallel', 'arbitrary')))(q, k, v, ab, q_segment_ids, kv_segment_ids, l, m, do, di)
A:jax.experimental.pallas.ops.tpu.flash_attention.logits->jax.numpy.einsum('bhqc,bhkc->bhqk', q.astype(jnp.float32), k.astype(jnp.float32))
A:jax.experimental.pallas.ops.tpu.flash_attention.unnormalized->jax.numpy.exp(logits - m[..., None])
A:jax.experimental.pallas.ops.tpu.flash_attention.out->jax.numpy.einsum('bhqk,bhkc->bhqc', weights, v)
A:jax.experimental.pallas.ops.tpu.flash_attention.res->_mha_reference(q, k, v, ab, segment_ids, causal=causal, mask_value=mask_value, sm_scale=sm_scale, save_residuals=True)
A:jax.experimental.pallas.ops.tpu.flash_attention.dq->jax.numpy.einsum('bhst,bhtd->bhsd', ds, k.astype(jnp.float32)).astype(q.dtype)
A:jax.experimental.pallas.ops.tpu.flash_attention.(dq, dk, dv, dab)->mha_reference_bwd(q, k, v, ab, segment_ids, o, l, m, do, causal=causal, mask_value=mask_value, sm_scale=sm_scale)
jax.experimental.pallas.ops.tpu.flash_attention.BlockSizes
jax.experimental.pallas.ops.tpu.flash_attention.BlockSizes.__post_init__(self)
jax.experimental.pallas.ops.tpu.flash_attention.BlockSizes.get_default(cls,batch_size,num_heads,q_seq_len,kv_len,d_model)
jax.experimental.pallas.ops.tpu.flash_attention.BlockSizes.has_backward_blocks(self)->bool
jax.experimental.pallas.ops.tpu.flash_attention.SegmentIds(NamedTuple)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention(q,k,v,ab,segment_ids,save_residuals,causal,sm_scale,block_sizes,debug)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_bwd(save_residuals:bool,causal:bool,sm_scale:float,block_sizes:BlockSizes,debug:bool,residuals,do)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_bwd_dkv(q,k,v,ab,segment_ids,l,m,do,di,*,block_q_major:int|None,block_q:int|None,block_k_major:int|None,block_k:int|None,sm_scale:float,causal:bool=False,mask_value:float=DEFAULT_MASK_VALUE,debug:bool=False)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_bwd_dq(q,k,v,ab,segment_ids,l,m,do,di,*,block_q_major:int|None,block_k_major:int|None,block_k:int|None,sm_scale:float,causal:bool,mask_value:float,debug:bool)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_dkv_kernel(q_tile_ref,k_tile_ref,v_tile_ref,ab_tile_ref,q_segment_ids_tile_ref,kv_segment_ids_tile_ref,l_tile_ref,m_tile_ref,do_tile_ref,di_tile_ref,dk_tile_ref,dv_tile_ref,dk_scratch_ref,dv_scratch_ref,*,sm_scale:float,causal:bool,mask_value:float,q_seq_len:int,block_q:int,block_k:int)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_dq_kernel(q_tile_ref,k_tile_ref,v_tile_ref,ab_tile_ref,q_segment_ids_tile_ref,kv_segment_ids_tile_ref,l_tile_ref,m_tile_ref,do_tile_ref,di_tile_ref,dq_tile_ref,dq_scratch_ref,ds_tile_ref,*,sm_scale:float,causal:bool,mask_value:float,kv_seq_len:int,block_k:int)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_fwd(q,k,v,ab,segment_ids,save_residuals,causal,sm_scale,block_sizes,debug)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_impl(q,k,v,ab,segment_ids,save_residuals,causal,sm_scale,block_b,block_q,block_k_major,block_k,debug)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_kernel(q_tile_ref,*args,**kwargs)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_kernel_single_batch(batch_idx:tuple[int,...],q_tile_ref,k_tile_ref,v_tile_ref,ab_tile_ref,q_segment_ids_tile_ref,kv_segment_ids_tile_ref,o_tile_ref,m_scratch_ref,l_scratch_ref,acc_scratch_ref,l_ref:Any|None=None,m_ref:Any|None=None,*,causal,sm_scale,block_k,kv_seq_len,mask_value)
jax.experimental.pallas.ops.tpu.flash_attention._flash_attention_kernel_single_batch_single_step(batch_idx:tuple[int,...],q_tile_ref,k_tile_ref,v_tile_ref,ab_tile_ref,q_segment_ids_tile_ref,kv_segment_ids_tile_ref,o_tile_ref,m_scratch_ref,l_scratch_ref,acc_scratch_ref,l_ref:Any|None=None,m_ref:Any|None=None,*,causal,sm_scale,block_k,kv_seq_len,mask_value)
jax.experimental.pallas.ops.tpu.flash_attention._mha_reference(q,k,v,ab,segment_ids:SegmentIds|None,causal:bool,mask_value:float,sm_scale:float,save_residuals:bool)
jax.experimental.pallas.ops.tpu.flash_attention._mha_reference_bwd(causal:bool,mask_value:float,sm_scale:float,save_residuals:bool,residuals,do)
jax.experimental.pallas.ops.tpu.flash_attention._mha_reference_fwd(q,k,v,ab,segment_ids:SegmentIds|None,causal:bool,mask_value:float,sm_scale:float,save_residuals:bool)
jax.experimental.pallas.ops.tpu.flash_attention._verify_block(block_name,dim_name,block,dim,should_divide=True)
jax.experimental.pallas.ops.tpu.flash_attention.below_or_on_diag(r,r_blk_size,c,c_blk_size)
jax.experimental.pallas.ops.tpu.flash_attention.flash_attention(q,k,v,ab=None,segment_ids=None,*,causal:bool=False,sm_scale:float=1.0,block_sizes:BlockSizes|None=None,debug:bool=False)
jax.experimental.pallas.ops.tpu.flash_attention.mha_reference(q,k,v,ab,segment_ids:SegmentIds|None=None,causal:bool=False,mask_value:float=DEFAULT_MASK_VALUE,sm_scale=1.0)
jax.experimental.pallas.ops.tpu.flash_attention.mha_reference_bwd(q,k,v,ab,segment_ids:SegmentIds|None,o,l,m,do,causal:bool=False,mask_value:float=DEFAULT_MASK_VALUE,sm_scale:float=1.0)
jax.experimental.pallas.ops.tpu.flash_attention.mha_reference_no_custom_vjp(q,k,v,ab:jax.Array|None=None,segment_ids:SegmentIds|None=None,*,causal:bool=False,mask_value:float=DEFAULT_MASK_VALUE,sm_scale:float=1.0,save_residuals:bool=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/mosaic/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/mosaic/dialects.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/export/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/export/shape_poly.py----------------------------------------
A:jax.experimental.export.shape_poly.thread_local_state->_ShapePolyThreadLocalState()
A:jax.experimental.export.shape_poly.acc->set()
A:jax.experimental.export.shape_poly.opnd_str->', '.join([str(opnd) for opnd in self.operands])
A:jax.experimental.export.shape_poly.items->self.monomials()
A:jax.experimental.export.shape_poly.d->collections.Counter(self)
A:jax.experimental.export.shape_poly.(a_l, a_u)->a.bounds()
A:jax.experimental.export.shape_poly.old_c->self._coeffs.copy().get(mon)
A:jax.experimental.export.shape_poly.(m_trimmed, m_remainder)->divmod(dec.factor * dec.rest_monomial, dec.operands[1])
A:jax.experimental.export.shape_poly.(lb, ub)->_ensure_poly(self - other, 'ge').bounds()
A:jax.experimental.export.shape_poly.other->_ensure_poly(other, 'mul')
A:jax.experimental.export.shape_poly.coeffs->self._coeffs.copy()
A:jax.experimental.export.shape_poly.mon->mon1.mul(mon2)
A:jax.experimental.export.shape_poly.power->int(power)
A:jax.experimental.export.shape_poly.qmon->mon1.mul(mon2).divide(dmon)
A:jax.experimental.export.shape_poly.(qcount, rcount)->divmod(count, dcount)
A:jax.experimental.export.shape_poly.q->_DimExpr.from_monomial(qmon, qcount)
A:jax.experimental.export.shape_poly.dividend->int(dividend)
A:jax.experimental.export.shape_poly.(q, r)->divmod(dividend, int(divisor))
A:jax.experimental.export.shape_poly.lbub->self._coeffs.get(_DimMon(), 0)
A:jax.experimental.export.shape_poly.(m_l, m_u)->mon1.mul(mon2).bounds()
A:jax.experimental.export.shape_poly.e_minus_m_coeffs->e._coeffs.copy()
A:jax.experimental.export.shape_poly.shape->numpy.shape(operand)
A:jax.experimental.export.shape_poly.(contract_fake_ops, contractions)->opt_einsum.contract_path(*fake_ops, **kwargs)
A:jax.experimental.export.shape_poly.idx->tuple((i for (i, fake_op) in enumerate(fake_ops) if operand is fake_op))
A:jax.experimental.export.shape_poly.shape_assertion_p->jax._src.core.Primitive('shape_assertion')
A:jax.experimental.export.shape_poly.op->jax._src.interpreters.mlir.custom_call('shape_assertion', result_types=[], operands=[assert_what, *error_message_inputs], has_side_effect=True, extra_attributes=dict(error_message=mlir.ir.StringAttr.get(error_message)))
A:jax.experimental.export.shape_poly.shape_assertion_effect->ShapeAssertionEffect()
A:jax.experimental.export.shape_poly.dim_as_value_p->jax._src.core.Primitive('dim_as_value')
A:jax.experimental.export.shape_poly.(res,)->jax._src.interpreters.mlir.eval_dynamic_shape(ctx, (dim,))
A:jax.experimental.export.shape_poly.out_type->jax._src.interpreters.mlir.aval_to_ir_type(ctx.avals_out[0])
A:jax.experimental.export.shape_poly.shape_spec_repr->repr(shape_spec)
A:jax.experimental.export.shape_poly.shape_spec->str(shape_spec)
A:jax.experimental.export.shape_poly.self.tokstream->tokenize.tokenize(io.BytesIO(self.shape_spec.encode('utf-8')).readline)
A:jax.experimental.export.shape_poly.tok->self.consume_token(tok, tokenize.RPAR)
A:jax.experimental.export.shape_poly.(sh, tok)->self.shape(tok)
A:jax.experimental.export.shape_poly.t->next(self.tokstream)
A:jax.experimental.export.shape_poly.val->int(tok.string)
A:jax.experimental.export.shape_poly.(res, tok)->self.shape(self.next_tok())
A:jax.experimental.export.shape_poly.(e, tok)->self.expr(tok)
A:jax.experimental.export.shape_poly.(m, tok)->self.mon(tok)
A:jax.experimental.export.shape_poly.(a, tok)->self.atom(tok)
A:jax.experimental.export.shape_poly.(power, tok)->self.integer(tok)
A:jax.experimental.export.shape_poly.(v, tok)->self.integer(tok)
A:jax.experimental.export.shape_poly.(e1, tok)->self.expr(tok)
A:jax.experimental.export.shape_poly.(e2, tok)->self.expr(tok)
A:jax.experimental.export.shape_poly.dimension_size_p->jax._src.core.Primitive('dimension_size')
A:jax.experimental.export.shape_poly.dim_size->jax._src.interpreters.mlir.hlo.ConvertOp(dim_type, dim_size)
A:jax.experimental.export.shape_poly.dim_type->jax._src.interpreters.mlir.aval_to_ir_type(core.dim_value_aval())
A:jax.experimental.export.shape_poly.aval_shape->_parse_spec(polymorphic_shape, arg_shape)
A:jax.experimental.export.shape_poly.dim_vars->all_dim_vars(args_avals)
A:jax.experimental.export.shape_poly.res->e.evaluate(self.env)
A:jax.experimental.export.shape_poly.is_ok->constraint.compute(eval)
A:jax.experimental.export.shape_poly.cached_spec->format_specifiers.get(e)
A:jax.experimental.export.shape_poly.(error_message, error_message_inputs)->constraint.error_message_and_inputs(eval)
A:jax.experimental.export.shape_poly.c->ShapeConstraint(comp, left, right, error_message_pieces)
A:jax.experimental.export.shape_poly.(args_kwargs_with_paths, _)->jax._src.tree_util.tree_flatten_with_path(args_kwargs_tree.unflatten((0,) * args_kwargs_tree.num_leaves))
A:jax.experimental.export.shape_poly.arg_str->_cached_pretty_print_dimension_descriptor(args_kwargs_tree, flat_arg_idx)
A:jax.experimental.export.shape_poly.synth_dim_var->pretty_print_dimension_descriptor(args_kwargs_tree, arg_idx, dim_idx)
A:jax.experimental.export.shape_poly.(solution, shape_constraints)->_solve_dim_equations(dim_equations, polymorphic_shape_specs)
A:jax.experimental.export.shape_poly.(solution, shape_constraints, synth_dim_vars)->solve_dim_vars(tuple(args_avals), args_kwargs_tree=args_kwargs_tree)
A:jax.experimental.export.shape_poly.synthetic_eval->CachingShapeEvaluator(**synthetic_env)
A:jax.experimental.export.shape_poly.shape_constraints->ShapeConstraints()
A:jax.experimental.export.shape_poly.dim_value->_DimExpr.from_var(eqn.dim_name)
A:jax.experimental.export.shape_poly.mon_value->mon1.mul(mon2).evaluate(shapeenv)
A:jax.experimental.export.shape_poly.v->mon1.mul(mon2).to_var()
A:jax.experimental.export.shape_poly.(var_value, var_remainder)->divmod(dim_value, core.dim_constant(factor_var))
A:jax.experimental.export.shape_poly.nr_eqns->len(eqns)
A:jax.experimental.export.shape_poly.unsolved_vars->unsolved_vars.difference(shapeenv.keys()).difference(shapeenv.keys())
jax.experimental.export.shape_poly.CachingShapeEvaluator(self,**env)
jax.experimental.export.shape_poly.CachingShapeEvaluator.__init__(self,**env)
jax.experimental.export.shape_poly.CachingShapeEvaluator.evaluate(self,e:DimSize)
jax.experimental.export.shape_poly.InconclusiveDimensionOperation(self,message:str)
jax.experimental.export.shape_poly.InconclusiveDimensionOperation.__init__(self,message:str)
jax.experimental.export.shape_poly.PolyShape(self,*dim_specs)
jax.experimental.export.shape_poly.PolyShape.__init__(self,*dim_specs)
jax.experimental.export.shape_poly.PolyShape.__str__(self)
jax.experimental.export.shape_poly.ShapeAssertionEffect(effects.Effect)
jax.experimental.export.shape_poly.ShapeConstraint
jax.experimental.export.shape_poly.ShapeConstraint.Comparator(Enum)
jax.experimental.export.shape_poly.ShapeConstraint.__str__(self)
jax.experimental.export.shape_poly.ShapeConstraint.check_statically(self,eval:CachingShapeEvaluator)->None
jax.experimental.export.shape_poly.ShapeConstraint.compute(self,eval:CachingShapeEvaluator)->Optional[jax.Array]
jax.experimental.export.shape_poly.ShapeConstraint.error_message_and_inputs(self,eval:CachingShapeEvaluator)->tuple[str, Sequence[Any]]
jax.experimental.export.shape_poly.ShapeConstraint.make_error(self,eval:CachingShapeEvaluator)->Exception
jax.experimental.export.shape_poly.ShapeConstraints(self)
jax.experimental.export.shape_poly.ShapeConstraints.__init__(self)
jax.experimental.export.shape_poly.ShapeConstraints.add_constraint(self,comp:ShapeConstraint.Comparator,left:DimSize,right:DimSize,error_message_pieces:Sequence[Union[str,DimSize]])
jax.experimental.export.shape_poly.ShapeConstraints.check_statically(self,eval:CachingShapeEvaluator)->None
jax.experimental.export.shape_poly.ShapeConstraints.shape_assertions(self,eval:CachingShapeEvaluator)->None
jax.experimental.export.shape_poly._Decomposition
jax.experimental.export.shape_poly._DimAtom(self,*operands:'_DimExpr',var:Optional[str]=None,operation:Optional[str]=None)
jax.experimental.export.shape_poly._DimAtom.__eq__(self,other:Any)
jax.experimental.export.shape_poly._DimAtom.__hash__(self)
jax.experimental.export.shape_poly._DimAtom.__init__(self,*operands:'_DimExpr',var:Optional[str]=None,operation:Optional[str]=None)
jax.experimental.export.shape_poly._DimAtom.__lt__(self,other:'_DimAtom')
jax.experimental.export.shape_poly._DimAtom.__str__(self)
jax.experimental.export.shape_poly._DimAtom.bounds(self)->tuple[float, float]
jax.experimental.export.shape_poly._DimAtom.evaluate(self,env:DimVarEnv)
jax.experimental.export.shape_poly._DimAtom.from_operation(cls,operation:str,*operands:'_DimExpr')->'_DimAtom'
jax.experimental.export.shape_poly._DimAtom.from_var(cls,v:str)->'_DimAtom'
jax.experimental.export.shape_poly._DimAtom.get_vars(self)->set[str]
jax.experimental.export.shape_poly._DimAtom.to_var(self)->Optional[str]
jax.experimental.export.shape_poly._DimEquation
jax.experimental.export.shape_poly._DimEquation.__str__(self)
jax.experimental.export.shape_poly._DimExpr(self,coeffs:dict[_DimMon,int])
jax.experimental.export.shape_poly._DimExpr.__add__(self,other)
jax.experimental.export.shape_poly._DimExpr.__divmod__(self,divisor)
jax.experimental.export.shape_poly._DimExpr.__floordiv__(self,divisor)
jax.experimental.export.shape_poly._DimExpr.__gt__(self,other:DimSize)
jax.experimental.export.shape_poly._DimExpr.__hash__(self)
jax.experimental.export.shape_poly._DimExpr.__init__(self,coeffs:dict[_DimMon,int])
jax.experimental.export.shape_poly._DimExpr.__int__(self)
jax.experimental.export.shape_poly._DimExpr.__jax_array__(self)
jax.experimental.export.shape_poly._DimExpr.__le__(self,other:DimSize)
jax.experimental.export.shape_poly._DimExpr.__lt__(self,other:DimSize)
jax.experimental.export.shape_poly._DimExpr.__mod__(self,divisor)
jax.experimental.export.shape_poly._DimExpr.__mul__(self,other)
jax.experimental.export.shape_poly._DimExpr.__ne__(self,other:DimSize)->bool
jax.experimental.export.shape_poly._DimExpr.__neg__(self)->'_DimExpr'
jax.experimental.export.shape_poly._DimExpr.__pow__(self,power,modulo=None)
jax.experimental.export.shape_poly._DimExpr.__radd__(self,other)
jax.experimental.export.shape_poly._DimExpr.__rdivmod__(self,dividend)
jax.experimental.export.shape_poly._DimExpr.__repr__(self)
jax.experimental.export.shape_poly._DimExpr.__rfloordiv__(self,other)
jax.experimental.export.shape_poly._DimExpr.__rmod__(self,dividend)
jax.experimental.export.shape_poly._DimExpr.__rmul__(self,other)
jax.experimental.export.shape_poly._DimExpr.__rsub__(self,other)
jax.experimental.export.shape_poly._DimExpr.__rtruediv__(self,dividend)
jax.experimental.export.shape_poly._DimExpr.__str__(self)
jax.experimental.export.shape_poly._DimExpr.__sub__(self,other)
jax.experimental.export.shape_poly._DimExpr.__truediv__(self,divisor)
jax.experimental.export.shape_poly._DimExpr._add_coeffs(cls,coeffs:dict[_DimMon,int],mon:_DimMon,coeff:int)
jax.experimental.export.shape_poly._DimExpr.bounds(self)->tuple[float, float]
jax.experimental.export.shape_poly._DimExpr.dimension_as_value(self)
jax.experimental.export.shape_poly._DimExpr.divmod(self,divisor:'_DimExpr')->tuple[DimSize, int]
jax.experimental.export.shape_poly._DimExpr.eq(self,other:DimSize)->bool
jax.experimental.export.shape_poly._DimExpr.evaluate(self,env:DimVarEnv)
jax.experimental.export.shape_poly._DimExpr.from_monomial(cls,mon:_DimMon,exp:int)
jax.experimental.export.shape_poly._DimExpr.from_operation(cls,operation:str,*operands:'_DimExpr')->'_DimExpr'
jax.experimental.export.shape_poly._DimExpr.from_var(cls,v:str)->'_DimExpr'
jax.experimental.export.shape_poly._DimExpr.ge(self,other:DimSize)->bool
jax.experimental.export.shape_poly._DimExpr.get_aval(dim:'_DimExpr')
jax.experimental.export.shape_poly._DimExpr.get_vars(self)->set[str]
jax.experimental.export.shape_poly._DimExpr.inconclusive_comparison(self,operation:str,op:Any)->Exception
jax.experimental.export.shape_poly._DimExpr.is_constant(self)
jax.experimental.export.shape_poly._DimExpr.leading_term(self)->tuple[_DimMon, int]
jax.experimental.export.shape_poly._DimExpr.monomials(self)->Iterable[tuple[_DimMon, int]]
jax.experimental.export.shape_poly._DimExpr.non_negative(self)->'_DimExpr'
jax.experimental.export.shape_poly._DimExpr.normalize(cls,coeffs:dict[_DimMon,int])->DimSize
jax.experimental.export.shape_poly._DimExpr.normalize_floordiv_times_divisor(cls,coeffs:dict[_DimMon,int])->DimSize
jax.experimental.export.shape_poly._DimExpr.to_var(self)->Optional[str]
jax.experimental.export.shape_poly._DimMon(dict)
jax.experimental.export.shape_poly._DimMon.__hash__(self)
jax.experimental.export.shape_poly._DimMon.__lt__(self,other:'_DimMon')
jax.experimental.export.shape_poly._DimMon.__str__(self)
jax.experimental.export.shape_poly._DimMon.bounds(self)->tuple[float, float]
jax.experimental.export.shape_poly._DimMon.degree(self)
jax.experimental.export.shape_poly._DimMon.divide(self,divisor:'_DimMon')->'_DimMon'
jax.experimental.export.shape_poly._DimMon.evaluate(self,env:DimVarEnv)
jax.experimental.export.shape_poly._DimMon.from_atom(clscls,a:_DimAtom,aexp:int)
jax.experimental.export.shape_poly._DimMon.from_operation(cls,operation:str,*operands:'_DimExpr')->'_DimMon'
jax.experimental.export.shape_poly._DimMon.from_var(cls,v:str)->'_DimMon'
jax.experimental.export.shape_poly._DimMon.get_vars(self)->set[str]
jax.experimental.export.shape_poly._DimMon.mul(self,other:'_DimMon')->'_DimMon'
jax.experimental.export.shape_poly._DimMon.to_var(self)->Optional[str]
jax.experimental.export.shape_poly._Parser(self,shape_spec:str,arg_shape:Sequence[Optional[int]],shape_spec_repr:str)
jax.experimental.export.shape_poly._Parser.__init__(self,shape_spec:str,arg_shape:Sequence[Optional[int]],shape_spec_repr:str)
jax.experimental.export.shape_poly._Parser.add_dim(self,expr:Optional[DimSize],tok:tokenize.TokenInfo)
jax.experimental.export.shape_poly._Parser.atom(self,tok:tokenize.TokenInfo)->tuple[DimSize, tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.binary_op(self,op:str,tok)->tuple[DimSize, tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.consume_token(self,tok:tokenize.TokenInfo,expected:int)->tokenize.TokenInfo
jax.experimental.export.shape_poly._Parser.expect_token(self,tok:tokenize.TokenInfo,expected:Sequence[int])->None
jax.experimental.export.shape_poly._Parser.expr(self,tok:tokenize.TokenInfo)->tuple[DimSize, tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.integer(self,tok:tokenize.TokenInfo)->tuple[int, tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.mon(self,tok:tokenize.TokenInfo)->tuple[DimSize, tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.next_tok(self)->tokenize.TokenInfo
jax.experimental.export.shape_poly._Parser.parse(self)->Sequence[DimSize]
jax.experimental.export.shape_poly._Parser.parse_err(self,tok:Optional[tokenize.TokenInfo],detail:str)->Exception
jax.experimental.export.shape_poly._Parser.shape(self,tok:tokenize.TokenInfo)->tuple[Sequence[DimSize], tokenize.TokenInfo]
jax.experimental.export.shape_poly._Parser.unary_op(self,op:str,tok)->tuple[DimSize, tokenize.TokenInfo]
jax.experimental.export.shape_poly._ShapePolyThreadLocalState(self)
jax.experimental.export.shape_poly._ShapePolyThreadLocalState.__init__(self)
jax.experimental.export.shape_poly._cached_pretty_print_dimension_descriptor(args_kwargs_tree:tree_util.PyTreeDef,flat_arg_idx:int)->str
jax.experimental.export.shape_poly._convertible_to_int(p:DimSize)->bool
jax.experimental.export.shape_poly._convertible_to_poly(p:DimSize)->bool
jax.experimental.export.shape_poly._decompose_expr(e:_DimExpr,operation:str)->Iterable[_Decomposition]
jax.experimental.export.shape_poly._dim_as_value(dim:DimSize)
jax.experimental.export.shape_poly._dim_as_value_lowering(ctx:mlir.LoweringRuleContext,*,dim)
jax.experimental.export.shape_poly._dimension_size_abstract_eval(aval:core.AbstractValue,**_)->core.AbstractValue
jax.experimental.export.shape_poly._dimension_size_impl(arg,*,dimension)
jax.experimental.export.shape_poly._dimension_size_lowering_rule(ctx,arg,*,dimension)
jax.experimental.export.shape_poly._einsum_contract_path(*operands,**kwargs)
jax.experimental.export.shape_poly._ensure_poly(p:DimSize,operation_name:str)->_DimExpr
jax.experimental.export.shape_poly._evaluate_add(v1,v2)
jax.experimental.export.shape_poly._evaluate_multiply(v1,v2)
jax.experimental.export.shape_poly._parse_spec(shape_spec:Union[str,PolyShape,None],arg_shape:Sequence[Optional[int]])->Sequence[DimSize]
jax.experimental.export.shape_poly._shape_assertion_lowering_rule(ctx:mlir.LoweringRuleContext,assert_what:mlir.ir.Value,*error_message_inputs:mlir.ir.Value,error_message:str)
jax.experimental.export.shape_poly._solve_dim_equations(eqns:list[_DimEquation],polymorphic_shape_specs:Sequence[tuple[str,str]])->tuple[DimVarEnv, ShapeConstraints]
jax.experimental.export.shape_poly.all_dim_vars(args_avals:Sequence[core.AbstractValue])->Sequence[str]
jax.experimental.export.shape_poly.arg_aval(arg_shape:Sequence[Optional[int]],arg_jax_dtype:DType,polymorphic_shape:Optional[Union[str,PolyShape]])->core.ShapedArray
jax.experimental.export.shape_poly.args_kwargs_path_to_str(path:tree_util.KeyPath)->str
jax.experimental.export.shape_poly.compute_dim_vars_from_arg_shapes(args_avals:Sequence[core.AbstractValue],*actual_args:jax.Array,args_kwargs_tree:tree_util.PyTreeDef)->Sequence[jax.Array]
jax.experimental.export.shape_poly.dim_as_value_impl(dim:DimSize)
jax.experimental.export.shape_poly.is_poly_dim(p:DimSize)->bool
jax.experimental.export.shape_poly.pretty_print_dimension_descriptor(args_kwargs_tree:tree_util.PyTreeDef,flat_arg_idx:int,dim_idx:Optional[int])->str
jax.experimental.export.shape_poly.shape_assertion(assert_what:jax.Array,*error_message_inputs:jax.Array,error_message:str)->None
jax.experimental.export.shape_poly.solve_dim_vars(args_avals:Sequence[core.AbstractValue],args_kwargs_tree:tree_util.PyTreeDef)->tuple[DimVarEnv, ShapeConstraints, Sequence[tuple[str, int, int]]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/export/export.py----------------------------------------
A:jax.experimental.export.export.m->re.match('custom_call:(.+)$', self._impl)
A:jax.experimental.export.export.aval_shape->jax.experimental.export.shape_poly._parse_spec(polymorphic_shape, arg_shape)
A:jax.experimental.export.export.aval->jax._src.core.raise_to_shaped(core.get_aval(a))
A:jax.experimental.export.export.(args_flat, args_tree)->jax._src.tree_util.tree_flatten(args)
A:jax.experimental.export.export.shapes_and_dtypes->tuple(map(get_shape_and_dtype, args_flat))
A:jax.experimental.export.export.(shapes, dtypes)->jax._src.util.unzip2(shapes_and_dtypes)
A:jax.experimental.export.export.polymorphic_shapes_->tuple(polymorphic_shapes)
A:jax.experimental.export.export.polymorphic_shapes_flat->jax._src.tree_util.broadcast_prefix(polymorphic_shapes_, args, is_leaf=lambda x: x is None)
A:jax.experimental.export.export.(e, *_)->jax._src.tree_util.prefix_errors(polymorphic_shapes_, args, is_leaf=lambda x: x is None)
A:jax.experimental.export.export.args_specs_flat->tuple(map(poly_spec, shapes, dtypes, polymorphic_shapes_flat))
A:jax.experimental.export.export.fun_name->getattr(fun_jax, '__name__', 'unknown')
A:jax.experimental.export.export.wrapped_fun_jax->jax.jit(fun_jax)
A:jax.experimental.export.export.actual_lowering_platforms->tuple(lowering_platforms)
A:jax.experimental.export.export.lowered->jax.jit(fun_jax).lower(*args_specs, **kwargs_specs, _experimental_lowering_parameters=mlir.LoweringParameters(platforms=actual_lowering_platforms, replace_tokens_with_dummy=replace_tokens_with_dummy))
A:jax.experimental.export.export.mlir_module->_wrap_main_func(mlir_module, args_avals_flat, args_kwargs_tree=lowered.in_tree, has_platform_index_argument=shape_poly_state.has_platform_index_argument, module_kept_var_idx=module_kept_var_idx, serialization_version=version)
A:jax.experimental.export.export.(args_avals_flat, _)->jax._src.tree_util.tree_flatten(lowered.in_avals)
A:jax.experimental.export.export.module_kept_var_idx->tuple(range(len(args_avals_flat)))
A:jax.experimental.export.export.mlir_module_attrs['jax.uses_shape_polymorphism']->jax._src.interpreters.mlir.ir.BoolAttr.get(shape_poly_state.uses_dim_vars)
A:jax.experimental.export.export.mlir_module_serialized->_serialize_module(mlir_module)
A:jax.experimental.export.export.mlir_module_text->jax._src.interpreters.mlir.module_to_string(mlir_module)
A:jax.experimental.export.export.ordered_effects->tuple(lowering.compile_args['ordered_effects'])
A:jax.experimental.export.export.unordered_effects->tuple(lowering.compile_args['unordered_effects'])
A:jax.experimental.export.export.mlir_str->jax._src.interpreters.mlir.module_to_bytecode(module)
A:jax.experimental.export.export.target_version->jax._src.lib.mlir.dialects.hlo.get_minimum_version()
A:jax.experimental.export.export.module_serialized->jax._src.lib.xla_client._xla.mlir.serialize_portable_artifact(mlir_str, target_version)
A:jax.experimental.export.export.dim_vars->jax.experimental.export.shape_poly.all_dim_vars(args_avals_flat)
A:jax.experimental.export.export.context->jax._src.interpreters.mlir.make_ir_context()
A:jax.experimental.export.export.wrapped_module->jax._src.lib.mlir.ir.Module.parse(mlir.module_to_bytecode(module))
A:jax.experimental.export.export.symbol_table->jax._src.lib.mlir.ir.SymbolTable(wrapped_module.operation)
A:jax.experimental.export.export.orig_main.attributes['sym_visibility']->jax._src.lib.mlir.ir.StringAttr.get('private')
A:jax.experimental.export.export.arg_attrs->list(ir.ArrayAttr(orig_main.arg_attrs))
A:jax.experimental.export.export.nr_dim_args->len(dim_vars)
A:jax.experimental.export.export.nr_token_args->len(token_arg_idxs)
A:jax.experimental.export.export.(platform_input_types, dim_var_input_types, token_input_types, array_input_types)->jax._src.util.split_list(orig_input_types, [nr_platform_index_args, nr_dim_args, nr_token_args])
A:jax.experimental.export.export.result_attrs->list(ir.ArrayAttr(orig_main.result_attrs))
A:jax.experimental.export.export.nr_token_results->len(token_result_idxs)
A:jax.experimental.export.export.new_main_result_indices->tuple(range(nr_token_results, len(orig_output_types)))
A:jax.experimental.export.export.new_main_ftype->jax._src.lib.mlir.ir.FunctionType.get(new_main_input_types, new_main_output_types)
A:jax.experimental.export.export.new_main_op->jax._src.lib.mlir.dialects.func.FuncOp('main', new_main_ftype, ip=ir.InsertionPoint.at_block_begin(wrapped_module.body))
A:jax.experimental.export.export.new_main_op.attributes['sym_visibility']->jax._src.lib.mlir.ir.StringAttr.get('public')
A:jax.experimental.export.export.new_main_op.arg_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([arg_attrs[idx] for idx in new_main_arg_indices])
A:jax.experimental.export.export.new_main_op.result_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([result_attrs[idx] for idx in new_main_result_indices])
A:jax.experimental.export.export.entry_block->jax._src.lib.mlir.dialects.func.FuncOp('main', new_main_ftype, ip=ir.InsertionPoint.at_block_begin(wrapped_module.body)).add_entry_block()
A:jax.experimental.export.export.module_context->jax._src.interpreters.mlir.ModuleContext(backend_or_name='cpu', platforms=['cpu'], axis_context=sharding_impls.ShardingContext([]), name_stack=source_info_util.new_name_stack(), keepalives=[], channel_iterator=itertools.count(1), host_callbacks=[], module=wrapped_module, context=context, lowering_parameters=mlir.LoweringParameters(global_constant_computation=True))
A:jax.experimental.export.export.ctx->jax._src.interpreters.mlir.LoweringRuleContext(module_context=module_context, primitive=None, avals_in=args_avals_flat, avals_out=None, tokens_in=mlir.TokenSet(), tokens_out=None)
A:jax.experimental.export.export.dim_values->jax._src.interpreters.mlir.lower_fun(functools.partial(shape_poly.compute_dim_vars_from_arg_shapes, args_avals_flat, args_kwargs_tree=args_kwargs_tree), multiple_results=True)(ctx, *new_main_op_array_args)
A:jax.experimental.export.export.call->jax._src.lib.mlir.dialects.func.CallOp(callee_type.results, ir.FlatSymbolRefAttr.get(fn), submodule_args)
A:jax.experimental.export.export.sharding_attr->jax._src.lib.mlir.ir.StringAttr.get('Sharding', mod.context)
A:jax.experimental.export.export.shape_assertion_attr->jax._src.lib.mlir.ir.StringAttr.get('shape_assertion', mod.context)
A:jax.experimental.export.export.target->dc.is_custom_call()
A:jax.experimental.export.export.disallowed_custom_call_ops_str->'\n'.join(disallowed_custom_call_ops)
A:jax.experimental.export.export.replicated_s->jax.sharding.GSPMDSharding.get_replicated(in_s._device_assignment)
A:jax.experimental.export.export.(args, kwargs)->in_tree.unflatten(args_flat)
A:jax.experimental.export.export.res->primal_fun(*args, **kwargs)
A:jax.experimental.export.export.(res_flat, _)->jax._src.tree_util.tree_flatten(res)
A:jax.experimental.export.export.(args_flat_jax, out_cts_flat_jax)->jax._src.util.split_list(args_and_out_cts_flat_jax, [len(in_avals)])
A:jax.experimental.export.export.(_, pullback_jax)->jax.vjp(flattened_primal_fun_jax, *args_flat_jax)
A:jax.experimental.export.export.vjp_in_avals->list(itertools.chain(in_avals, map(lambda a: a.at_least_vspace(), out_avals)))
A:jax.experimental.export.export.all_in_shardings->expand_in_shardings(exported.in_shardings, exported.module_kept_var_idx, len(args))
A:jax.experimental.export.export.(vjp_in_shardings, vjp_out_shardings)->canonical_shardings(tuple(itertools.chain(all_in_shardings, out_shardings)), all_in_shardings)
A:jax.experimental.export.export.(fun_vjp_jax, vjp_in_avals)->_get_vjp_fun(primal_fun, in_tree=primal.in_tree, module_kept_var_idx=primal.module_kept_var_idx, in_avals=primal.in_avals, in_shardings=primal.in_shardings, out_avals=primal.out_avals, out_shardings=primal.out_shardings, apply_jit=True)
A:jax.experimental.export.export.exp_vjp->exported.vjp()
A:jax.experimental.export.export.in_ct_flat->call_exported(exp_vjp)(*args_flat, *ct_res_flat)
A:jax.experimental.export.export.(args_flat, in_tree)->jax._src.tree_util.tree_flatten((args, kwargs))
A:jax.experimental.export.export.in_args->in_tree.unflatten([0] * in_tree.num_leaves)
A:jax.experimental.export.export.exp_in_args->exported.in_tree.unflatten([0] * exported.in_tree.num_leaves)
A:jax.experimental.export.export.res_flat->f_flat(*args_flat)
A:jax.experimental.export.export.call_exported_p->jax._src.core.Primitive('call_exported')
A:jax.experimental.export.export.exported_dim_vars->jax.experimental.export.shape_poly.all_dim_vars(exported.in_avals)
A:jax.experimental.export.export.(solution, shape_constraints, synth_dim_vars)->jax.experimental.export.shape_poly.solve_dim_vars(exported.in_avals, args_kwargs_tree=exported.in_tree)
A:jax.experimental.export.export.synthetic_eval->jax.experimental.export.shape_poly.CachingShapeEvaluator(**synthetic_env)
A:jax.experimental.export.export.out_avals->tuple((core.ShapedArray(core.evaluate_shape(out_aval.shape, exported_dim_vars, *exported_dim_values), dtype=out_aval.dtype, weak_type=out_aval.weak_type, named_shape=out_aval.named_shape) for out_aval in exported.out_avals))
A:jax.experimental.export.export.args->tuple((wrap_with_sharding(ctx, exported, x, x_aval, x_sharding) for (x, x_aval, x_sharding) in zip(args, ctx.avals_in, all_in_shardings)))
A:jax.experimental.export.export.submodule->jax._src.lib.mlir.ir.Module.parse(exported.mlir_module())
A:jax.experimental.export.export.symtab->jax._src.lib.mlir.ir.SymbolTable(submodule.operation)
A:jax.experimental.export.export.new_ir_type->jax._src.interpreters.mlir.aval_to_ir_type(new_aval)
A:jax.experimental.export.export.fn->jax._src.interpreters.mlir.merge_mlir_modules(ctx.module_context.module, f'call_exported_{exported.fun_name}', submodule)
A:jax.experimental.export.export.current_platform_idx->jax._src.lib.mlir.dialects.hlo.ConvertOp(i32_type, current_platform_idx)
A:jax.experimental.export.export.callee_platform_idx->jax._src.lib.mlir.dialects.hlo.ConvertOp(callee_type.inputs[0], callee_platform_idx)
A:jax.experimental.export.export.branch->jax._src.lib.mlir.dialects.hlo.ConvertOp(callee_type.inputs[0], callee_platform_idx).regions[i].blocks.append()
A:jax.experimental.export.export.results->tuple((wrap_with_sharding(ctx, exported, x, x_aval, x_sharding) for (x, x_aval, x_sharding) in zip(results, ctx.avals_out, exported.out_shardings)))
A:jax.experimental.export.export.ctx_device_assignment->list(axis_context.mesh.devices.flat)
jax.experimental.export.export.DisabledSafetyCheck(self,_impl:str)
jax.experimental.export.export.DisabledSafetyCheck.__eq__(self,other)->bool
jax.experimental.export.export.DisabledSafetyCheck.__hash__(self)->int
jax.experimental.export.export.DisabledSafetyCheck.__init__(self,_impl:str)
jax.experimental.export.export.DisabledSafetyCheck.__str__(self)
jax.experimental.export.export.DisabledSafetyCheck.custom_call(cls,target_name:str)->'DisabledSafetyCheck'
jax.experimental.export.export.DisabledSafetyCheck.is_custom_call(self)->Optional[str]
jax.experimental.export.export.DisabledSafetyCheck.platform(cls)->'DisabledSafetyCheck'
jax.experimental.export.export.DisabledSafetyCheck.shape_assertions(cls)->'DisabledSafetyCheck'
jax.experimental.export.export.Exported
jax.experimental.export.export.Exported.__str__(self)
jax.experimental.export.export.Exported.mlir_module(self)->ir.Module
jax.experimental.export.export.Exported.vjp(self)->'Exported'
jax.experimental.export.export._call_exported_abstract_eval(*in_avals:core.AbstractValue,exported:Exported)->tuple[tuple[core.AbstractValue, ...], set[effects.Effect]]
jax.experimental.export.export._call_exported_impl(*args,exported:Exported)
jax.experimental.export.export._call_exported_lowering(ctx:mlir.LoweringRuleContext,*args,exported:Exported)
jax.experimental.export.export._check_lowering(lowering)->None
jax.experimental.export.export._check_module(mod:ir.Module,*,allow_non_replicated_sharding:bool,disabled_checks:Sequence[DisabledSafetyCheck])->None
jax.experimental.export.export._export_native_vjp(primal_fun,primal:Exported)->Exported
jax.experimental.export.export._get_vjp_fun(primal_fun:Callable,*,in_tree:tree_util.PyTreeDef,in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],module_kept_var_idx:tuple[int,...],in_shardings:tuple[Sharding,...],out_shardings:tuple[Sharding,...],apply_jit:bool)->tuple[Callable, Sequence[core.AbstractValue]]
jax.experimental.export.export._keep_main_tokens(serialization_version:int)->bool
jax.experimental.export.export._serialize_module(module:ir.Module)->bytes
jax.experimental.export.export._wrap_main_func(module:ir.Module,args_avals_flat:Sequence[core.ShapedArray],*,args_kwargs_tree:tree_util.PyTreeDef,has_platform_index_argument:bool,module_kept_var_idx:tuple[int,...],serialization_version:int)->ir.Module
jax.experimental.export.export.call_exported(exported:Exported)->Callable[..., jax.Array]
jax.experimental.export.export.canonical_shardings(in_shardings:Sequence[Sharding],out_shardings:Sequence[Sharding])->tuple[Union[pxla.UnspecifiedValue, Sequence[sharding.XLACompatibleSharding]], Union[pxla.UnspecifiedValue, Sequence[sharding.XLACompatibleSharding]]]
jax.experimental.export.export.default_lowering_platform()->str
jax.experimental.export.export.expand_in_shardings(in_shardings:tuple[Sharding,...],module_kept_var_idx:Sequence[int],nr_inputs:int)->tuple[Sharding, ...]
jax.experimental.export.export.export(fun_jax:Callable,*,lowering_platforms:Optional[Sequence[str]]=None,disabled_checks:Sequence[DisabledSafetyCheck]=())->Callable[..., Exported]
jax.experimental.export.export.poly_spec(arg_shape:Sequence[Optional[int]],arg_dtype:DType,polymorphic_shape:Optional[str])->jax.ShapeDtypeStruct
jax.experimental.export.export.poly_specs(args,polymorphic_shapes,get_shape_and_dtype=shape_and_dtype_jax_array)
jax.experimental.export.export.shape_and_dtype_jax_array(a)->tuple[Sequence[Optional[int]], DType]
jax.experimental.export.export.wrap_with_sharding(ctx:mlir.LoweringRuleContext,exported:Exported,x:ir.Value,x_aval:core.AbstractValue,x_sharding:Sharding)->ir.Value


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/array_serialization/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/array_serialization/serialization.py----------------------------------------
A:jax.experimental.array_serialization.serialization.TS_CONTEXT->tensorstore.Context({'file_io_concurrency': {'limit': 128}})
A:jax.experimental.array_serialization.serialization._module_unique_count->itertools.count()
A:jax.experimental.array_serialization.serialization.logger->logging.getLogger(__name__)
A:jax.experimental.array_serialization.serialization.device_to_index_map->inp_sharding.devices_indices_map(global_shape)
A:jax.experimental.array_serialization.serialization.m->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL)
A:jax.experimental.array_serialization.serialization.gcs_bucket->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL).group(1)
A:jax.experimental.array_serialization.serialization.path_without_bucket->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL).group(2)
A:jax.experimental.array_serialization.serialization.ckpt_path->os.path.normpath(ckpt_path).replace('gs:/', 'gs://')
A:jax.experimental.array_serialization.serialization.is_gcs_path->os.path.normpath(ckpt_path).replace('gs:/', 'gs://').startswith('gs://')
A:jax.experimental.array_serialization.serialization.base_path->os.path.dirname(ckpt_path)
A:jax.experimental.array_serialization.serialization.spec['kvstore']->_get_kvstore_for_gcs(ckpt_path)
A:jax.experimental.array_serialization.serialization.self._cv->asyncio.Condition(lock=asyncio.Lock())
A:jax.experimental.array_serialization.serialization.tensorstore_spec['metadata']->_get_metadata(arr_inp)
A:jax.experimental.array_serialization.serialization.open_future->tensorstore.open(ts.Spec(tensorstore_spec), create=True, open=True, context=context)
A:jax.experimental.array_serialization.serialization.write_future->t[shard.index].write(shard.data)
A:jax.experimental.array_serialization.serialization.future_write_state->jax.tree_util.tree_map(_write_array, local_shards)
A:jax.experimental.array_serialization.serialization.future_writer->jax.tree_util.tree_map(async_serialize, arrays, tensorstore_specs, commit_futures)
A:jax.experimental.array_serialization.serialization.new_shard_shape->in_sharding.shard_shape(tuple(shape))
A:jax.experimental.array_serialization.serialization.restricted_domain->t.domain.intersect(requested_domain)
A:jax.experimental.array_serialization.serialization.requested_bytes->estimate_read_memory_footprint(t, restricted_domain)
A:jax.experimental.array_serialization.serialization.out->out.astype(dtype).astype(dtype)
A:jax.experimental.array_serialization.serialization.result->jax.device_put(out, device)
A:jax.experimental.array_serialization.serialization.byte_limiter->_LimitInFlightBytes(concurrent_bytes)
A:jax.experimental.array_serialization.serialization.future_arrays->jax.tree_util.tree_map(partial(async_deserialize, byte_limiter=byte_limiter), shardings, tensorstore_specs, [None] * len(tensorstore_specs) if global_shapes is None else global_shapes, [None] * len(tensorstore_specs) if dtypes is None else dtypes)
A:jax.experimental.array_serialization.serialization.current_process->jax.process_index()
A:jax.experimental.array_serialization.serialization.process_count->jax.process_count()
A:jax.experimental.array_serialization.serialization.thread_start_time->time.time()
A:jax.experimental.array_serialization.serialization.key_for_barrier->_get_key(self._count)
A:jax.experimental.array_serialization.serialization.self._count->next(_module_unique_count)
A:jax.experimental.array_serialization.serialization.self._thread->threading.Thread(target=self._thread_func)
A:jax.experimental.array_serialization.serialization.get_key->_get_key(self._count)
A:jax.experimental.array_serialization.serialization.tspecs->jax.tree_map(get_tensorstore_spec, paths)
jax.experimental.array_serialization.serialization.AsyncManager(self,timeout_secs=300)
jax.experimental.array_serialization.serialization.AsyncManager.__del__(self)
jax.experimental.array_serialization.serialization.AsyncManager.__init__(self,timeout_secs=300)
jax.experimental.array_serialization.serialization.AsyncManager._add_futures(self,futures:Sequence[asyncio.Future])
jax.experimental.array_serialization.serialization.AsyncManager._start_async_commit(self,on_commit_callback)
jax.experimental.array_serialization.serialization.AsyncManager._thread_func(self)
jax.experimental.array_serialization.serialization.AsyncManager.check_for_errors(self)
jax.experimental.array_serialization.serialization.AsyncManager.wait_until_finished(self)
jax.experimental.array_serialization.serialization.BarrierTimeoutException(Exception)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager(AsyncManager,GlobalAsyncCheckpointManagerBase)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager.deserialize(self,shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None,concurrent_gb:int=32)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager.deserialize_with_paths(self,shardings:Sequence[sharding.Sharding],paths:Sequence[str],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None,concurrent_gb:int=32)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager.serialize(self,arrays,tensorstore_specs,*,on_commit_callback)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager.serialize_with_paths(self,arrays:Sequence[jax.Array],paths:Sequence[str],*,on_commit_callback)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManagerBase(metaclass=abc.ABCMeta)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManagerBase.check_for_errors(self)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManagerBase.deserialize(self,shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None)
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManagerBase.serialize(self,arrays,tensorstore_specs,*,on_commit_callback:Callable[[],None])
jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManagerBase.wait_until_finished(self)
jax.experimental.array_serialization.serialization._LimitInFlightBytes(self,num_bytes)
jax.experimental.array_serialization.serialization._LimitInFlightBytes.__init__(self,num_bytes)
jax.experimental.array_serialization.serialization._LimitInFlightBytes.release_bytes(self,requested_bytes)
jax.experimental.array_serialization.serialization._LimitInFlightBytes.wait_for_bytes(self,requested_bytes)
jax.experimental.array_serialization.serialization._get_key(key:int)
jax.experimental.array_serialization.serialization._get_kvstore_for_gcs(ckpt_path:str)
jax.experimental.array_serialization.serialization._get_metadata(arr)
jax.experimental.array_serialization.serialization._spec_has_metadata(tree)
jax.experimental.array_serialization.serialization.async_deserialize(in_sharding:sharding_impls.XLACompatibleSharding,tensorstore_spec:Union[ts.Spec,dict[str,Any]],global_shape:Optional[Sequence[int]]=None,dtype=None,byte_limiter:Optional[_LimitInFlightBytes]=None,context=TS_CONTEXT,assume_metadata:bool=False)
jax.experimental.array_serialization.serialization.async_serialize(arr_inp,tensorstore_spec,commit_future=None,context=TS_CONTEXT)
jax.experimental.array_serialization.serialization.create_async_array_from_callback(global_shape:array.Shape,inp_sharding:sharding_impls.XLACompatibleSharding,data_callback:Callable[[array.Index,jax.Device],Awaitable[jax.Array]])
jax.experimental.array_serialization.serialization.estimate_read_memory_footprint(t:ts.TensorStore,domain:ts.IndexDomain)->int
jax.experimental.array_serialization.serialization.get_tensorstore_spec(ckpt_path:str,ocdbt:bool=False)
jax.experimental.array_serialization.serialization.run_deserialization(shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None,concurrent_gb:int=32)
jax.experimental.array_serialization.serialization.run_serialization(arrays,tensorstore_specs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/array_serialization/serialization_test.py----------------------------------------
A:jax.experimental.array_serialization.serialization_test.prev_xla_flags->jax._src.test_util.set_host_platform_device_count(8)
A:jax.experimental.array_serialization.serialization_test.global_mesh->jax._src.test_util.create_global_mesh((2,), 'x')
A:jax.experimental.array_serialization.serialization_test.pspec->P('x', 'y')
A:jax.experimental.array_serialization.serialization_test.num->math.prod(global_input_shape)
A:jax.experimental.array_serialization.serialization_test.sharding->NamedSharding(global_mesh, pspec)
A:jax.experimental.array_serialization.serialization_test.src->jax.numpy.arange(num, dtype=np.int32).reshape(inp_shape)
A:jax.experimental.array_serialization.serialization_test.inp->jax._src.array.make_array_from_callback(inp_shape, sharding, lambda idx: src[idx])
A:jax.experimental.array_serialization.serialization_test.ckpt_dir->pathlib.Path(self.create_tempdir('first').full_path)
A:jax.experimental.array_serialization.serialization_test.tspec->tensorstore.array(data).spec()
A:jax.experimental.array_serialization.serialization_test.manager->jax.experimental.array_serialization.serialization.GlobalAsyncCheckpointManager()
A:jax.experimental.array_serialization.serialization_test.deserialize_with_byte_limit->jax.experimental.array_serialization.serialization.async_deserialize(sharding, tspec, inp_shape, byte_limiter=serialization._LimitInFlightBytes(4200000))
A:jax.experimental.array_serialization.serialization_test.(unused_current, peak)->tracemalloc.get_traced_memory()
A:jax.experimental.array_serialization.serialization_test.deserialize_wo_limit->jax.experimental.array_serialization.serialization.async_deserialize(sharding, tspec, inp_shape)
A:jax.experimental.array_serialization.serialization_test.global_input_data1->numpy.arange(num, dtype=np.int32).reshape(global_input_shape)
A:jax.experimental.array_serialization.serialization_test.a1->jax._src.array.make_array_from_callback(inp_shape, NamedSharding(global_mesh, pspec), lambda idx: global_input_data1[idx])
A:jax.experimental.array_serialization.serialization_test.ckpt_path1->pathlib.Path(self.create_tempdir(f'{ckpt_dir}/first').full_path)
A:jax.experimental.array_serialization.serialization_test.(m1,)->jax.experimental.array_serialization.serialization.run_deserialization([NamedSharding(global_mesh, P(None))], [tspec])
A:jax.experimental.array_serialization.serialization_test.global_input_data2->numpy.arange(num, num + num, dtype=np.int32).reshape(inp_shape)
A:jax.experimental.array_serialization.serialization_test.a2->jax._src.array.make_array_from_callback(inp_shape, NamedSharding(global_mesh, pspec), lambda idx: global_input_data2[idx])
A:jax.experimental.array_serialization.serialization_test.ckpt_path2->pathlib.Path(self.create_tempdir(f'{ckpt_dir}/second').full_path)
A:jax.experimental.array_serialization.serialization_test.global_mesh1d->jax._src.test_util.create_global_mesh((8,), ('x',))
A:jax.experimental.array_serialization.serialization_test.a3->jax._src.array.make_array_from_callback((0,), NamedSharding(global_mesh1d, P(None)), cb3)
A:jax.experimental.array_serialization.serialization_test.ckpt_path3->pathlib.Path(self.create_tempdir(f'{ckpt_dir}/third').full_path)
A:jax.experimental.array_serialization.serialization_test.tspecs->jax.tree_util.tree_map(serialization.get_tensorstore_spec, ckpt_paths)
A:jax.experimental.array_serialization.serialization_test.(m1, m2, m3)->jax.experimental.array_serialization.serialization.run_deserialization([NamedSharding(global_mesh, pspec), NamedSharding(global_mesh, P('x')), NamedSharding(global_mesh1d, P(None))], tspecs)
A:jax.experimental.array_serialization.serialization_test.arr->jax._src.array.make_array_from_callback(global_input_shape, NamedSharding(global_mesh, P('x', 'y')), cb1)
A:jax.experimental.array_serialization.serialization_test.ds->NamedSharding(jtu.create_global_mesh((2,), 'x'), P(None))
A:jax.experimental.array_serialization.serialization_test.new_ds->jax.sharding.GSPMDSharding.get_replicated(list(global_mesh.devices.flat))
A:jax.experimental.array_serialization.serialization_test.(m2,)->jax.experimental.array_serialization.serialization.run_deserialization([new_ds], tspecs, [(8, 2)], [np.float32])
A:jax.experimental.array_serialization.serialization_test.data->numpy.arange(1024)
A:jax.experimental.array_serialization.serialization_test.s->NamedSharding(global_mesh, P(None))
A:jax.experimental.array_serialization.serialization_test.array1->jax._src.array.make_array_from_callback(global_input_shape, s, lambda idx: data[idx])
A:jax.experimental.array_serialization.serialization_test.spec->jax.experimental.array_serialization.serialization.get_tensorstore_spec(path, ocdbt=True)
A:jax.experimental.array_serialization.serialization_test.is_gcs_path->path.startswith('gs://')
jax.experimental.array_serialization.serialization_test.CheckpointTest(jtu.JaxTestCase)
jax.experimental.array_serialization.serialization_test.CheckpointTest._on_commit_callback(self,temp_ckpt_dir,final_ckpt_dir)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_checkpointing_jax_array(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_checkpointing_scalar_jax_array(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_checkpointing_with_bigger_shape_jax_array(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_checkpointing_with_path_variant(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_deserialize_tensorstore_array_jax_array(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_empty_spec_has_no_metadata(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_get_tensorstore_spec_not_absolute_path(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_get_tensorstore_spec_ocdbt(self,path)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_memory_consumption(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_spec_has_metadata(self)
jax.experimental.array_serialization.serialization_test.CheckpointTest.test_spec_has_no_metadata(self)
jax.experimental.array_serialization.serialization_test.setUpModule()
jax.experimental.array_serialization.serialization_test.tearDownModule()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/compilation_cache/compilation_cache.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/experimental/compilation_cache/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/example_libraries/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/example_libraries/optimizers.py----------------------------------------
A:jax.example_libraries.optimizers.OptimizerState->namedtuple('OptimizerState', ['packed_state', 'tree_def', 'subtree_defs'])
A:jax.example_libraries.optimizers.(init, update, get_params)->opt_maker(*args, **kwargs)
A:jax.example_libraries.optimizers.(x0_flat, tree)->tree_flatten(x0_tree)
A:jax.example_libraries.optimizers.(states_flat, subtrees)->unzip2(map(tree_flatten, initial_states))
A:jax.example_libraries.optimizers.(grad_flat, tree2)->tree_flatten(grad_tree)
A:jax.example_libraries.optimizers.states->map(tree_unflatten, subtrees, states_flat)
A:jax.example_libraries.optimizers.new_states->map(partial(update, i), grad_flat, states)
A:jax.example_libraries.optimizers.(new_states_flat, subtrees2)->unzip2(map(tree_flatten, new_states))
A:jax.example_libraries.optimizers.params->map(get_params, states)
A:jax.example_libraries.optimizers.step_size->make_schedule(step_size)
A:jax.example_libraries.optimizers.v0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.g_sq->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.m->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.g_sq_inv_sqrt->jax.numpy.where(g_sq > 0, 1.0 / jnp.sqrt(g_sq), 0.0)
A:jax.example_libraries.optimizers.avg_sq_grad->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.mom->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.m0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.u0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.u->jax.numpy.maximum(b2 * u, jnp.abs(g))
A:jax.example_libraries.optimizers.lst->list(seq)
A:jax.example_libraries.optimizers.idx->splice([None] * ndim, axis, [slice(None)])
A:jax.example_libraries.optimizers.x0->jax.numpy.atleast_1d(x0)
A:jax.example_libraries.optimizers.accum_inv_sqrt->jax.numpy.where(accum > 0, 1.0 / jnp.sqrt(accum), 0)
A:jax.example_libraries.optimizers.step_num->jax.numpy.minimum(step_num, decay_steps)
A:jax.example_libraries.optimizers.boundaries->jax.numpy.array(boundaries)
A:jax.example_libraries.optimizers.values->jax.numpy.array(values)
A:jax.example_libraries.optimizers.(leaves, _)->tree_flatten(tree)
A:jax.example_libraries.optimizers.norm->l2_norm(grad_tree)
A:jax.example_libraries.optimizers.subtrees->map(tree_unflatten, subtree_defs, states_flat)
A:jax.example_libraries.optimizers.(sentinels, tree_def)->tree_flatten(marked_pytree)
A:jax.example_libraries.optimizers.(states_flat, subtree_defs)->unzip2(map(tree_flatten, subtrees))
jax.example_libraries.optimizers.JoinPoint(self,subtree)
jax.example_libraries.optimizers.JoinPoint.__init__(self,subtree)
jax.example_libraries.optimizers.JoinPoint.__iter__(self)
jax.example_libraries.optimizers.Optimizer(NamedTuple)
jax.example_libraries.optimizers.adagrad(step_size,momentum=0.9)
jax.example_libraries.optimizers.adam(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.example_libraries.optimizers.adamax(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.example_libraries.optimizers.clip_grads(grad_tree,max_norm)
jax.example_libraries.optimizers.constant(step_size)->Schedule
jax.example_libraries.optimizers.exponential_decay(step_size,decay_steps,decay_rate)
jax.example_libraries.optimizers.inverse_time_decay(step_size,decay_steps,decay_rate,staircase=False)
jax.example_libraries.optimizers.l2_norm(tree)
jax.example_libraries.optimizers.make_schedule(scalar_or_schedule:Union[float,Schedule])->Schedule
jax.example_libraries.optimizers.momentum(step_size:Schedule,mass:float)
jax.example_libraries.optimizers.nesterov(step_size:Schedule,mass:float)
jax.example_libraries.optimizers.optimizer(opt_maker:Callable[...,tuple[Callable[[Params],State],Callable[[Step,Updates,Params],Params],Callable[[State],Params]]])->Callable[..., Optimizer]
jax.example_libraries.optimizers.pack_optimizer_state(marked_pytree)
jax.example_libraries.optimizers.piecewise_constant(boundaries:Any,values:Any)
jax.example_libraries.optimizers.polynomial_decay(step_size,decay_steps,final_step_size,power=1.0)
jax.example_libraries.optimizers.rmsprop(step_size,gamma=0.9,eps=1e-08)
jax.example_libraries.optimizers.rmsprop_momentum(step_size,gamma=0.9,eps=1e-08,momentum=0.9)
jax.example_libraries.optimizers.sgd(step_size)
jax.example_libraries.optimizers.sm3(step_size,momentum=0.9)
jax.example_libraries.optimizers.unpack_optimizer_state(opt_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/example_libraries/stax.py----------------------------------------
A:jax.example_libraries.stax.(k1, k2)->jax.random.split(rng)
A:jax.example_libraries.stax.filter_shape_iter->iter(filter_shape)
A:jax.example_libraries.stax.output_shape->jax.lax.conv_transpose_shape_tuple(input_shape, kernel_shape, strides, padding, dimension_numbers)
A:jax.example_libraries.stax.Conv->functools.partial(GeneralConv, ('NHWC', 'HWIO', 'NHWC'))
A:jax.example_libraries.stax.Conv1DTranspose->functools.partial(GeneralConvTranspose, ('NHC', 'HIO', 'NHC'))
A:jax.example_libraries.stax.ConvTranspose->functools.partial(GeneralConvTranspose, ('NHWC', 'HWIO', 'NHWC'))
A:jax.example_libraries.stax.shape->tuple((d for (i, d) in enumerate(input_shape) if i not in axis))
A:jax.example_libraries.stax.ed->tuple((None if i in axis else slice(None) for i in range(jnp.ndim(x))))
A:jax.example_libraries.stax.z->standardize(x, axis, epsilon=epsilon)
A:jax.example_libraries.stax.Tanh->elementwise(jnp.tanh)
A:jax.example_libraries.stax.Relu->elementwise(relu)
A:jax.example_libraries.stax.Exp->elementwise(jnp.exp)
A:jax.example_libraries.stax.LogSoftmax->elementwise(log_softmax, axis=-1)
A:jax.example_libraries.stax.Softmax->elementwise(softmax, axis=-1)
A:jax.example_libraries.stax.Softplus->elementwise(softplus)
A:jax.example_libraries.stax.Sigmoid->elementwise(sigmoid)
A:jax.example_libraries.stax.Elu->elementwise(elu)
A:jax.example_libraries.stax.LeakyRelu->elementwise(leaky_relu)
A:jax.example_libraries.stax.Selu->elementwise(selu)
A:jax.example_libraries.stax.Gelu->elementwise(gelu)
A:jax.example_libraries.stax.padding_vals->jax.lax.padtype_to_pads(input_shape, window_shape, strides, padding)
A:jax.example_libraries.stax.out_shape->jax.lax.reduce_window_shape_tuple(input_shape, window_shape, strides, padding_vals, ones, ones)
A:jax.example_libraries.stax.out->jax.lax.reduce_window(inputs, init_val, reducer, window_shape, strides, padding)
A:jax.example_libraries.stax.MaxPool->_pooling_layer(lax.max, -jnp.inf)
A:jax.example_libraries.stax.SumPool->_pooling_layer(lax.add, 0.0)
A:jax.example_libraries.stax.spatial_shape->tuple((inputs.shape[i] for i in range(inputs.ndim) if i not in non_spatial_axes))
A:jax.example_libraries.stax.one->jax.numpy.ones(spatial_shape, dtype=inputs.dtype)
A:jax.example_libraries.stax.window_sizes->jax.numpy.expand_dims(window_sizes, i)
A:jax.example_libraries.stax.AvgPool->_pooling_layer(lax.add, 0.0, _normalize_by_window_size)
A:jax.example_libraries.stax.Flatten->Flatten()
A:jax.example_libraries.stax.Identity->Identity()
A:jax.example_libraries.stax.FanInSum->FanInSum()
A:jax.example_libraries.stax.concat_size->sum((shape[ax] for shape in input_shape))
A:jax.example_libraries.stax.rng->kwargs.pop('rng', None)
A:jax.example_libraries.stax.keep->jax.random.bernoulli(rng, rate, inputs.shape)
A:jax.example_libraries.stax.nlayers->len(layers)
A:jax.example_libraries.stax.(init_funs, apply_funs)->zip(*layers)
A:jax.example_libraries.stax.(rng, layer_rng)->jax.random.split(rng)
A:jax.example_libraries.stax.(input_shape, param)->init_fun(layer_rng, input_shape)
A:jax.example_libraries.stax.inputs->fun(param, inputs, rng=rng, **kwargs)
A:jax.example_libraries.stax.rngs->jax.random.split(rng, nlayers)
jax.example_libraries.stax.BatchNorm(axis=(0,1,2),epsilon=1e-05,center=True,scale=True,beta_init=zeros,gamma_init=ones)
jax.example_libraries.stax.Dense(out_dim,W_init=glorot_normal(),b_init=normal())
jax.example_libraries.stax.Dropout(rate,mode='train')
jax.example_libraries.stax.FanInConcat(axis=-1)
jax.example_libraries.stax.FanInSum()
jax.example_libraries.stax.FanOut(num)
jax.example_libraries.stax.Flatten()
jax.example_libraries.stax.GeneralConv(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.example_libraries.stax.GeneralConvTranspose(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.example_libraries.stax.Identity()
jax.example_libraries.stax._normalize_by_window_size(dims,strides,padding)
jax.example_libraries.stax._pooling_layer(reducer,init_val,rescaler=None)
jax.example_libraries.stax.elementwise(fun,**fun_kwargs)
jax.example_libraries.stax.parallel(*layers)
jax.example_libraries.stax.serial(*layers)
jax.example_libraries.stax.shape_dependent(make_layer)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/image/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/__init__.py----------------------------------------
A:jax.scipy.__init__.(__getattr__, __dir__, __all__)->jax._src.lazy_loader.attach(__name__, ['interpolate', 'linalg', 'ndimage', 'signal', 'sparse', 'special', 'stats', 'fft', 'cluster', 'integrate'])


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/fft.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/integrate.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/ndimage.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/special.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/linalg.py----------------------------------------
A:jax.scipy.linalg.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/signal.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/cluster/vq.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/sparse/linalg.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/spatial/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/spatial/transform.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/beta.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/multivariate_normal.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/chi2.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/cauchy.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/bernoulli.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/logistic.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/truncnorm.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/expon.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/vonmises.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/nbinom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/wrapcauchy.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/poisson.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/multinomial.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/betabinom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/t.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/laplace.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/geom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/gennorm.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/pareto.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/binom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/norm.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/dirichlet.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/gamma.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/stats/uniform.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/optimize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/scipy/interpolate/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/lib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/lib/xla_bridge.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/numpy/__init__.py----------------------------------------
A:jax.numpy.__init__.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/numpy/__init__.pyi----------------------------------------
jax.numpy.__init__.abs(x:ArrayLike,/)->Array
jax.numpy.__init__.absolute(x:ArrayLike,/)->Array
jax.numpy.__init__.add(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.all(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,*,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.allclose(a:ArrayLike,b:ArrayLike,rtol:ArrayLike=...,atol:ArrayLike=...,equal_nan:bool=...)->Array
jax.numpy.__init__.amax(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.amin(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.angle(z:ArrayLike,deg:bool=...)->Array
jax.numpy.__init__.any(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,*,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.append(arr:ArrayLike,values:ArrayLike,axis:Optional[int]=...)->Array
jax.numpy.__init__.apply_along_axis(func1d:Callable,axis:int,arr:ArrayLike,*args,**kwargs)->Array
jax.numpy.__init__.apply_over_axes(func:Callable,a:ArrayLike,axes:Sequence[int])->Array
jax.numpy.__init__.arange(start:DimSize,stop:Optional[DimSize]=...,step:Optional[DimSize]=...,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.arccos(x:ArrayLike,/)->Array
jax.numpy.__init__.arccosh(x:ArrayLike,/)->Array
jax.numpy.__init__.arcsin(x:ArrayLike,/)->Array
jax.numpy.__init__.arcsinh(x:ArrayLike,/)->Array
jax.numpy.__init__.arctan(x:ArrayLike,/)->Array
jax.numpy.__init__.arctan2(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.arctanh(x:ArrayLike,/)->Array
jax.numpy.__init__.argmax(a:ArrayLike,axis:Optional[int]=...,out:None=...,keepdims:Optional[bool]=...)->Array
jax.numpy.__init__.argmin(a:ArrayLike,axis:Optional[int]=...,out:None=...,keepdims:Optional[bool]=...)->Array
jax.numpy.__init__.argpartition(a:ArrayLike,kth:int,axis:int=...)->Array
jax.numpy.__init__.argsort(a:ArrayLike,axis:Optional[int]=-1,kind:str='stable',order:None=...)->Array
jax.numpy.__init__.argwhere(a:ArrayLike,*,size:Optional[int]=...,fill_value:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.array(object:Any,dtype:DTypeLike|None=...,copy:bool=True,order:str|None=...,ndmin:int=...)->Array
jax.numpy.__init__.array_equal(a1:ArrayLike,a2:ArrayLike,equal_nan:bool=...)->Array
jax.numpy.__init__.array_equiv(a1:ArrayLike,a2:ArrayLike)->Array
jax.numpy.__init__.array_split(ary:ArrayLike,indices_or_sections:Union[int,Sequence[int],ArrayLike],axis:int=...)->list[Array]
jax.numpy.__init__.asarray(a:Any,dtype:Optional[DTypeLike]=...,order:Optional[str]=...)->Array
jax.numpy.__init__.atleast_1d(*arys:ArrayLike)->Union[Array, list[Array]]
jax.numpy.__init__.atleast_1d(ary:ArrayLike)->Array
jax.numpy.__init__.atleast_2d(*arys:ArrayLike)->Union[Array, list[Array]]
jax.numpy.__init__.atleast_2d(ary:ArrayLike)->Array
jax.numpy.__init__.atleast_3d(*arys:ArrayLike)->Union[Array, list[Array]]
jax.numpy.__init__.atleast_3d(ary:ArrayLike)->Array
jax.numpy.__init__.average(a:ArrayLike,axis:_Axis=...,weights:Optional[ArrayLike]=...,*,returned:Literal[True],keepdims:bool=False)->tuple[Array, Array]
jax.numpy.__init__.average(a:ArrayLike,axis:_Axis=...,weights:Optional[ArrayLike]=...,returned:Literal[False]=False,keepdims:bool=False)->Array
jax.numpy.__init__.average(a:ArrayLike,axis:_Axis=...,weights:Optional[ArrayLike]=...,returned:bool=False,keepdims:bool=False)->Union[Array, tuple[Array, Array]]
jax.numpy.__init__.bartlett(M:int)->Array
jax.numpy.__init__.bincount(x:ArrayLike,weights:Optional[ArrayLike]=...,minlength:int=...,*,length:Optional[int]=...)->Array
jax.numpy.__init__.bitwise_and(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.bitwise_count(x:ArrayLike,/)->Array
jax.numpy.__init__.bitwise_not(x:ArrayLike,/)->Array
jax.numpy.__init__.bitwise_or(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.bitwise_xor(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.blackman(M:int)->Array
jax.numpy.__init__.block(arrays:Union[ArrayLike,Sequence[ArrayLike],Sequence[Sequence[ArrayLike]]])->Array
jax.numpy.__init__.broadcast_arrays(*args:ArrayLike)->list[Array]
jax.numpy.__init__.broadcast_shapes(*shapes:Sequence[Union[int,_core.Tracer]])->tuple[Union[int, _core.Tracer], ...]
jax.numpy.__init__.broadcast_shapes(*shapes:Sequence[int])->tuple[int, ...]
jax.numpy.__init__.broadcast_to(array:ArrayLike,shape:DimSize|Shape)->Array
jax.numpy.__init__.cbrt(x:ArrayLike,/)->Array
jax.numpy.__init__.ceil(x:ArrayLike,/)->Array
jax.numpy.__init__.choose(a:ArrayLike,choices:Sequence[ArrayLike],out:None=...,mode:str=...)->Array
jax.numpy.__init__.clip(a:ArrayLike,a_min:Optional[ArrayLike]=...,a_max:Optional[ArrayLike]=...,out:None=...)->Array
jax.numpy.__init__.column_stack(tup:Union[_np.ndarray,Array,Sequence[ArrayLike]])->Array
jax.numpy.__init__.compress(condition:ArrayLike,a:ArrayLike,axis:Optional[int]=...,out:None=...)->Array
jax.numpy.__init__.concatenate(arrays:Union[_np.ndarray,Array,Sequence[ArrayLike]],axis:Optional[int]=...,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.conjugate(x:ArrayLike,/)->Array
jax.numpy.__init__.convolve(a:ArrayLike,v:ArrayLike,mode:str=...,*,precision:PrecisionLike=...,preferred_element_type:Optional[dtype]=...)->Array
jax.numpy.__init__.copy(a:ArrayLike,order:Optional[str]=...)->Array
jax.numpy.__init__.copysign(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.corrcoef(x:ArrayLike,y:Optional[ArrayLike]=...,rowvar:bool=...)->Array
jax.numpy.__init__.correlate(a:ArrayLike,v:ArrayLike,mode:str=...,*,precision:PrecisionLike=...,preferred_element_type:Optional[dtype]=...)->Array
jax.numpy.__init__.cos(x:ArrayLike,/)->Array
jax.numpy.__init__.cosh(x:ArrayLike,/)->Array
jax.numpy.__init__.count_nonzero(a:ArrayLike,axis:_Axis=...,keepdims:bool=...)->Array
jax.numpy.__init__.cov(m:ArrayLike,y:Optional[ArrayLike]=...,rowvar:bool=...,bias:bool=...,ddof:Optional[int]=...,fweights:Optional[ArrayLike]=...,aweights:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.cross(a:ArrayLike,b:ArrayLike,axisa:int=-1,axisb:int=-1,axisc:int=-1,axis:Optional[int]=...)->Array
jax.numpy.__init__.cumprod(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...)->Array
jax.numpy.__init__.cumsum(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...)->Array
jax.numpy.__init__.deg2rad(x:ArrayLike,/)->Array
jax.numpy.__init__.delete(arr:ArrayLike,obj:Union[ArrayLike,slice],axis:Optional[int]=...,*,assume_unique_indices:bool=...)->Array
jax.numpy.__init__.diag(v:ArrayLike,k:int=0)->Array
jax.numpy.__init__.diag_indices(n:int,ndim:int=...)->tuple[Array, ...]
jax.numpy.__init__.diag_indices_from(arr:ArrayLike)->tuple[Array, ...]
jax.numpy.__init__.diagflat(v:ArrayLike,k:int=0)->Array
jax.numpy.__init__.diagonal(a:ArrayLike,offset:ArrayLike=...,axis1:int=...,axis2:int=...)
jax.numpy.__init__.diff(a:ArrayLike,n:int=...,axis:int=...,prepend:Optional[ArrayLike]=...,append:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.digitize(x:ArrayLike,bins:ArrayLike,right:bool=...)->Array
jax.numpy.__init__.divmod(x:ArrayLike,y:ArrayLike,/)->tuple[Array, Array]
jax.numpy.__init__.dot(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.dsplit(ary:ArrayLike,indices_or_sections:Union[int,ArrayLike])->list[Array]
jax.numpy.__init__.dstack(tup:Union[_np.ndarray,Array,Sequence[ArrayLike]],dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.ediff1d(ary:ArrayLike,to_end:Optional[ArrayLike]=...,to_begin:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.einsum(arr:ArrayLike,axes:Sequence[Any],/,*operands:Union[ArrayLike,Sequence[Any]],out:None=...,optimize:str='optimal',precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...,_use_xeinsum:bool=False,_dot_general:Callable[...,Array]=...)->Array
jax.numpy.__init__.einsum(subscript:str,/,*operands:ArrayLike,out:None=...,optimize:str='optimal',precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...,_use_xeinsum:bool=False,_dot_general:Callable[...,Array]=...)->Array
jax.numpy.__init__.einsum(subscripts,/,*operands,out:None=...,optimize:str=...,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...,_use_xeinsum:bool=...,_dot_general:Callable[...,Array]=...)->Array
jax.numpy.__init__.einsum_path(subscripts,*operands,optimize=...)
jax.numpy.__init__.empty(shape:Any,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.empty_like(prototype:Union[ArrayLike,DuckTypedArray],dtype:Optional[DTypeLike]=...,shape:Any=...)->Array
jax.numpy.__init__.equal(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.exp(x:ArrayLike,/)->Array
jax.numpy.__init__.exp2(x:ArrayLike,/)->Array
jax.numpy.__init__.expand_dims(a:ArrayLike,axis:Union[int,Sequence[int]])->Array
jax.numpy.__init__.expm1(x:ArrayLike,/)->Array
jax.numpy.__init__.extract(condition:ArrayLike,arr:ArrayLike)->Array
jax.numpy.__init__.eye(N:DimSize,M:Optional[DimSize]=...,k:int=...,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.fabs(x:ArrayLike,/)->Array
jax.numpy.__init__.fill_diagonal(a:ArrayLike,val:ArrayLike,wrap:bool=...,*,inplace:bool=...)->Array
jax.numpy.__init__.fix(x:ArrayLike,out:None=...)->Array
jax.numpy.__init__.flatnonzero(a:ArrayLike,*,size:Optional[int]=...,fill_value:Union[None,ArrayLike,tuple[ArrayLike]]=...)->Array
jax.numpy.__init__.flip(m:ArrayLike,axis:Optional[Union[int,Sequence[int]]]=...)->Array
jax.numpy.__init__.fliplr(m:ArrayLike)->Array
jax.numpy.__init__.flipud(m:ArrayLike)->Array
jax.numpy.__init__.float_power(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.floor(x:ArrayLike,/)->Array
jax.numpy.__init__.floor_divide(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.fmax(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.fmin(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.fmod(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.frexp(x:ArrayLike,/)->tuple[Array, Array]
jax.numpy.__init__.from_dlpack(x:Any)->Array
jax.numpy.__init__.frombuffer(buffer:Union[bytes,Any],dtype:DTypeLike=...,count:int=...,offset:int=...)->Array
jax.numpy.__init__.fromfile(*args,**kwargs)
jax.numpy.__init__.fromfunction(function:Callable[...,Array],shape:Any,*,dtype:DTypeLike=...,**kwargs)->Array
jax.numpy.__init__.fromiter(*args,**kwargs)
jax.numpy.__init__.fromstring(string:str,dtype:DTypeLike=...,count:int=...,*,sep:str)->Array
jax.numpy.__init__.full(shape:Any,fill_value:ArrayLike,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.full_like(a:Union[ArrayLike,DuckTypedArray],fill_value:ArrayLike,dtype:Optional[DTypeLike]=...,shape:Any=...)->Array
jax.numpy.__init__.gcd(x1:ArrayLike,x2:ArrayLike)->Array
jax.numpy.__init__.geomspace(start:ArrayLike,stop:ArrayLike,num:int=...,endpoint:bool=...,dtype:Optional[DTypeLike]=...,axis:int=...)->Array
jax.numpy.__init__.gradient(f:ArrayLike,*varargs:ArrayLike,axis:Optional[Union[int,Sequence[int]]]=...,edge_order:Optional[int]=...)->Union[Array, list[Array]]
jax.numpy.__init__.greater(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.greater_equal(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.hamming(M:int)->Array
jax.numpy.__init__.hanning(M:int)->Array
jax.numpy.__init__.heaviside(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.histogram(a:ArrayLike,bins:ArrayLike=...,range:Optional[Sequence[ArrayLike]]=...,weights:Optional[ArrayLike]=...,density:Optional[bool]=...)->tuple[Array, Array]
jax.numpy.__init__.histogram2d(x:ArrayLike,y:ArrayLike,bins:Union[ArrayLike,Sequence[ArrayLike]]=...,range:Optional[Sequence[Union[None,Array,Sequence[ArrayLike]]]]=...,weights:Optional[ArrayLike]=...,density:Optional[bool]=...)->tuple[Array, Array, Array]
jax.numpy.__init__.histogram_bin_edges(a:ArrayLike,bins:ArrayLike=...,range:Union[None,Array,Sequence[ArrayLike]]=...,weights:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.histogramdd(sample:ArrayLike,bins:Union[ArrayLike,Sequence[ArrayLike]]=...,range:Optional[Sequence[Union[None,Array,Sequence[ArrayLike]]]]=...,weights:Optional[ArrayLike]=...,density:Optional[bool]=...)->tuple[Array, list[Array]]
jax.numpy.__init__.hsplit(ary:ArrayLike,indices_or_sections:Union[int,ArrayLike])->list[Array]
jax.numpy.__init__.hstack(tup:Union[_np.ndarray,Array,Sequence[ArrayLike]],dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.hypot(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.i0(x:ArrayLike)->Array
jax.numpy.__init__.identity(n:DimSize,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.imag(x:ArrayLike,/)->Array
jax.numpy.__init__.in1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=...,invert:bool=...)->Array
jax.numpy.__init__.indices(dimensions:Sequence[int],dtype:DTypeLike=int32,*,sparse:Literal[True])->tuple[Array, ...]
jax.numpy.__init__.indices(dimensions:Sequence[int],dtype:DTypeLike=int32,sparse:Literal[False]=False)->Array
jax.numpy.__init__.indices(dimensions:Sequence[int],dtype:DTypeLike=int32,sparse:bool=False)->Union[Array, tuple[Array, ...]]
jax.numpy.__init__.inner(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.insert(arr:ArrayLike,obj:Union[ArrayLike,slice],values:ArrayLike,axis:Optional[int]=...)->Array
jax.numpy.__init__.interp(x:ArrayLike,xp:ArrayLike,fp:ArrayLike,left:Union[ArrayLike,str,None]=...,right:Union[ArrayLike,str,None]=...,period:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.intersect1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=...,return_indices:bool=...)->Union[Array, tuple[Array, Array, Array]]
jax.numpy.__init__.invert(x:ArrayLike,/)->Array
jax.numpy.__init__.isclose(a:ArrayLike,b:ArrayLike,rtol:ArrayLike=...,atol:ArrayLike=...,equal_nan:bool=...)->Array
jax.numpy.__init__.iscomplex(m:ArrayLike)->Array
jax.numpy.__init__.iscomplexobj(x:Any)->bool
jax.numpy.__init__.isfinite(x:ArrayLike,/)->Array
jax.numpy.__init__.isin(element:ArrayLike,test_elements:ArrayLike,assume_unique:bool=...,invert:bool=...)->Array
jax.numpy.__init__.isinf(x:ArrayLike,/)->Array
jax.numpy.__init__.isnan(x:ArrayLike,/)->Array
jax.numpy.__init__.isneginf(x:ArrayLike,/)->Array
jax.numpy.__init__.isposinf(x:ArrayLike,/)->Array
jax.numpy.__init__.isreal(m:ArrayLike)->Array
jax.numpy.__init__.isrealobj(x:Any)->bool
jax.numpy.__init__.isscalar(element:Any)->bool
jax.numpy.__init__.issubdtype(arg1:DTypeLike,arg2:DTypeLike)->bool
jax.numpy.__init__.ix_(*args:ArrayLike)->tuple[Array, ...]
jax.numpy.__init__.kaiser(M:int,beta:ArrayLike)->Array
jax.numpy.__init__.kron(a:ArrayLike,b:ArrayLike)->Array
jax.numpy.__init__.lcm(x1:ArrayLike,x2:ArrayLike)->Array
jax.numpy.__init__.ldexp(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.left_shift(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.less(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.less_equal(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.lexsort(keys:Sequence[ArrayLike],axis:int=...)->Array
jax.numpy.__init__.linspace(start:ArrayLike,stop:ArrayLike,num:int,endpoint:bool,retstep:Literal[True],dtype:Optional[DTypeLike]=...,axis:int=0)->tuple[Array, Array]
jax.numpy.__init__.linspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,*,retstep:Literal[True],dtype:Optional[DTypeLike]=...,axis:int=0)->tuple[Array, Array]
jax.numpy.__init__.linspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,retstep:Literal[False]=False,dtype:Optional[DTypeLike]=...,axis:int=0)->Array
jax.numpy.__init__.linspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,retstep:bool=False,dtype:Optional[DTypeLike]=...,axis:int=0)->Union[Array, tuple[Array, Array]]
jax.numpy.__init__.load(*args:Any,**kwargs:Any)->Array
jax.numpy.__init__.log(x:ArrayLike,/)->Array
jax.numpy.__init__.log10(x:ArrayLike,/)->Array
jax.numpy.__init__.log1p(x:ArrayLike,/)->Array
jax.numpy.__init__.log2(x:ArrayLike,/)->Array
jax.numpy.__init__.logaddexp(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.logaddexp2(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.logical_and(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.logical_not(x:ArrayLike,/)->Array
jax.numpy.__init__.logical_or(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.logical_xor(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.logspace(start:ArrayLike,stop:ArrayLike,num:int=...,endpoint:bool=...,base:ArrayLike=...,dtype:Optional[DTypeLike]=...,axis:int=...)->Array
jax.numpy.__init__.mask_indices(n:int,mask_func:Callable,k:int=...)->tuple[Array, ...]
jax.numpy.__init__.matmul(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.matrix_transpose(x:ArrayLike,/)->Array
jax.numpy.__init__.maximum(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.mean(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,*,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.median(a:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,keepdims:bool=...)->Array
jax.numpy.__init__.meshgrid(*xi:ArrayLike,copy:bool=...,sparse:bool=...,indexing:str=...)->list[Array]
jax.numpy.__init__.minimum(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.mod(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.modf(x:ArrayLike,/,out=None)->tuple[Array, Array]
jax.numpy.__init__.moveaxis(a:ArrayLike,source:Union[int,Sequence[int]],destination:Union[int,Sequence[int]])->Array
jax.numpy.__init__.multiply(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.nan_to_num(x:ArrayLike,copy:bool=...,nan:ArrayLike=...,posinf:Optional[ArrayLike]=...,neginf:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanargmax(a:ArrayLike,axis:Optional[int]=...,out:None=...,keepdims:Optional[bool]=...)->Array
jax.numpy.__init__.nanargmin(a:ArrayLike,axis:Optional[int]=...,out:None=...,keepdims:Optional[bool]=...)->Array
jax.numpy.__init__.nancumprod(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...)->Array
jax.numpy.__init__.nancumsum(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...)->Array
jax.numpy.__init__.nanmax(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanmean(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanmedian(a:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,keepdims:bool=...)->Array
jax.numpy.__init__.nanmin(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanpercentile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,method:str=...,keepdims:bool=...,interpolation:None=...)->Array
jax.numpy.__init__.nanprod(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanquantile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,method:str=...,keepdims:bool=...,interpolation:None=...)->Array
jax.numpy.__init__.nanstd(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,ddof:int=...,keepdims:bool=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nansum(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.nanvar(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,ddof:int=0,keepdims:bool=False,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.negative(x:ArrayLike,/)->Array
jax.numpy.__init__.nextafter(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.nonzero(a:ArrayLike,*,size:Optional[int]=...,fill_value:Union[None,ArrayLike,tuple[ArrayLike,...]]=...)->tuple[Array, ...]
jax.numpy.__init__.not_equal(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.ones(shape:Any,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.ones_like(a:Union[ArrayLike,DuckTypedArray],dtype:Optional[DTypeLike]=...,shape:Any=...)->Array
jax.numpy.__init__.outer(a:ArrayLike,b:Array,out:None=...)->Array
jax.numpy.__init__.packbits(a:ArrayLike,axis:Optional[int]=...,bitorder:str=...)->Array
jax.numpy.__init__.pad(array:ArrayLike,pad_width:PadValueLike[int|Array|_np.ndarray],mode:Union[str,Callable[...,Any]]=...,**kwargs)->Array
jax.numpy.__init__.partition(a:ArrayLike,kth:int,axis:int=...)->Array
jax.numpy.__init__.percentile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,method:str=...,keepdims:bool=...,interpolation:None=...)->Array
jax.numpy.__init__.piecewise(x:ArrayLike,condlist:Union[Array,Sequence[ArrayLike]],funclist:Sequence[Union[ArrayLike,Callable[...,Array]]],*args,**kw)->Array
jax.numpy.__init__.place(arr:ArrayLike,mask:ArrayLike,vals:ArrayLike,*,inplace:bool=...)->Array
jax.numpy.__init__.poly(seq_of_zeros:Array)->Array
jax.numpy.__init__.polyadd(a1:Array,a2:Array)->Array
jax.numpy.__init__.polyder(p:Array,m:int=...)->Array
jax.numpy.__init__.polydiv(u:ArrayLike,v:ArrayLike,*,trim_leading_zeros:bool=...)->tuple[Array, Array]
jax.numpy.__init__.polyfit(x:Array,y:Array,deg:int,rcond:Optional[float]=...,full:bool=...,w:Optional[Array]=...,cov:bool=...)->Union[Array, tuple[Array, ...]]
jax.numpy.__init__.polyint(p:Array,m:int=...,k:Optional[int]=...)->Array
jax.numpy.__init__.polymul(a1:ArrayLike,a2:ArrayLike,*,trim_leading_zeros:bool=...)->Array
jax.numpy.__init__.polysub(a1:Array,a2:Array)->Array
jax.numpy.__init__.polyval(p:Array,x:Array,*,unroll:int=...)->Array
jax.numpy.__init__.positive(x:ArrayLike,/)->Array
jax.numpy.__init__.power(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.prod(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...,promote_integers:bool=...)->Array
jax.numpy.__init__.ptp(a:ArrayLike,axis:_Axis=...,out:None=...,keepdims:bool=...)->Array
jax.numpy.__init__.put(a:ArrayLike,ind:ArrayLike,v:ArrayLike,mode:str|None=...,*,inplace:bool=...)->Array
jax.numpy.__init__.quantile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=...,out:None=...,overwrite_input:bool=...,method:str=...,keepdims:bool=...,interpolation:None=...)->Array
jax.numpy.__init__.rad2deg(x:ArrayLike,/)->Array
jax.numpy.__init__.ravel(a:ArrayLike,order:str=...)->Array
jax.numpy.__init__.ravel_multi_index(multi_index:Sequence[ArrayLike],dims:Sequence[int],mode:str=...,order:str=...)->Array
jax.numpy.__init__.real(x:ArrayLike,/)->Array
jax.numpy.__init__.reciprocal(x:ArrayLike,/)->Array
jax.numpy.__init__.remainder(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.repeat(a:ArrayLike,repeats:ArrayLike,axis:Optional[int]=...,*,total_repeat_length:Optional[int]=...)->Array
jax.numpy.__init__.reshape(a:ArrayLike,newshape:Union[DimSize,Shape],order:str=...)->Array
jax.numpy.__init__.resize(a:ArrayLike,new_shape:Shape)->Array
jax.numpy.__init__.result_type(*args:Any)->DType
jax.numpy.__init__.right_shift(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.rint(x:ArrayLike,/)->Array
jax.numpy.__init__.roll(a:ArrayLike,shift:Union[ArrayLike,Sequence[int]],axis:Optional[Union[int,Sequence[int]]]=...)->Array
jax.numpy.__init__.rollaxis(a:ArrayLike,axis:int,start:int=0)->Array
jax.numpy.__init__.roots(p:ArrayLike,*,strip_zeros:bool=...)->Array
jax.numpy.__init__.rot90(m:ArrayLike,k:int=...,axes:tuple[int,int]=...)->Array
jax.numpy.__init__.round(a:ArrayLike,decimals:int=...,out:None=...)->Array
jax.numpy.__init__.searchsorted(a:ArrayLike,v:ArrayLike,side:str=...,sorter:None=...,*,method:str=...)->Array
jax.numpy.__init__.select(condlist:Sequence[ArrayLike],choicelist:Sequence[ArrayLike],default:ArrayLike=...)->Array
jax.numpy.__init__.setdiff1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=...,*,size:Optional[int]=...,fill_value:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.setxor1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=...)->Array
jax.numpy.__init__.sign(x:ArrayLike,/)->Array
jax.numpy.__init__.signbit(x:ArrayLike,/)->Array
jax.numpy.__init__.sin(x:ArrayLike,/)->Array
jax.numpy.__init__.sinc(x:ArrayLike,/)->Array
jax.numpy.__init__.sinh(x:ArrayLike,/)->Array
jax.numpy.__init__.sort(a:ArrayLike,axis:Optional[int]=...,kind:str=...,order:None=...)->Array
jax.numpy.__init__.sort_complex(a:ArrayLike)->Array
jax.numpy.__init__.split(ary:ArrayLike,indices_or_sections:Union[int,Sequence[int],ArrayLike],axis:int=...)->list[Array]
jax.numpy.__init__.sqrt(x:ArrayLike,/)->Array
jax.numpy.__init__.square(x:ArrayLike,/)->Array
jax.numpy.__init__.squeeze(a:ArrayLike,axis:Optional[Union[int,Sequence[int]]]=...)->Array
jax.numpy.__init__.stack(arrays:Union[_np.ndarray,Array,Sequence[ArrayLike]],axis:int=...,out:None=...,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.std(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,ddof:int=...,keepdims:bool=...,*,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.subtract(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.sum(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,keepdims:bool=...,initial:Optional[ArrayLike]=...,where:Optional[ArrayLike]=...,promote_integers:bool=...)->Array
jax.numpy.__init__.swapaxes(a:ArrayLike,axis1:int,axis2:int)->Array
jax.numpy.__init__.take(a:ArrayLike,indices:ArrayLike,axis:Optional[int]=...,out:None=...,mode:Optional[str]=...,unique_indices:bool=...,indices_are_sorted:bool=...,fill_value:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.take_along_axis(arr:ArrayLike,indices:ArrayLike,axis:Optional[int],mode:Optional[Union[str,GatherScatterMode]]=...)->Array
jax.numpy.__init__.tan(x:ArrayLike,/)->Array
jax.numpy.__init__.tanh(x:ArrayLike,/)->Array
jax.numpy.__init__.tensordot(a:ArrayLike,b:ArrayLike,axes:Union[int,Sequence[int],Sequence[Sequence[int]]]=...,*,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.tile(A:ArrayLike,reps:Union[DimSize,Sequence[DimSize]])->Array
jax.numpy.__init__.trace(a:ArrayLike,offset:int=...,axis1:int=...,axis2:int=...,dtype:Optional[DTypeLike]=...,out:None=...)->Array
jax.numpy.__init__.transpose(a:ArrayLike,axes:Optional[Sequence[int]]=...)->Array
jax.numpy.__init__.trapz(y:ArrayLike,x:Optional[ArrayLike]=...,dx:ArrayLike=...,axis:int=...)->Array
jax.numpy.__init__.tri(N:int,M:Optional[int]=...,k:int=...,dtype:DTypeLike=...)->Array
jax.numpy.__init__.tril(m:ArrayLike,k:int=...)->Array
jax.numpy.__init__.tril_indices(n:int,k:int=...,m:Optional[int]=...)->tuple[Array, Array]
jax.numpy.__init__.tril_indices_from(arr:ArrayLike,k:int=...)->tuple[Array, Array]
jax.numpy.__init__.trim_zeros(filt:ArrayLike,trim:str=...)->Array
jax.numpy.__init__.triu(m:ArrayLike,k:int=...)->Array
jax.numpy.__init__.triu_indices(n:int,k:int=...,m:Optional[int]=...)->tuple[Array, Array]
jax.numpy.__init__.triu_indices_from(arr:ArrayLike,k:int=...)->tuple[Array, Array]
jax.numpy.__init__.true_divide(x:ArrayLike,y:ArrayLike,/)->Array
jax.numpy.__init__.trunc(x:ArrayLike,/)->Array
jax.numpy.__init__.union1d(ar1:ArrayLike,ar2:ArrayLike,*,size:Optional[int]=...,fill_value:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.unique(ar:ArrayLike,return_index:bool=...,return_inverse:bool=...,return_counts:bool=...,axis:Optional[int]=...,*,size:Optional[int]=...,fill_value:Optional[ArrayLike]=...)
jax.numpy.__init__.unpackbits(a:ArrayLike,axis:Optional[int]=...,count:Optional[ArrayLike]=...,bitorder:str=...)->Array
jax.numpy.__init__.unravel_index(indices:ArrayLike,shape:Shape)->tuple[Array, ...]
jax.numpy.__init__.unwrap(p:ArrayLike,discont:Optional[ArrayLike]=...,axis:int=...,period:ArrayLike=...)->Array
jax.numpy.__init__.vander(x:ArrayLike,N:Optional[int]=...,increasing:bool=...)->Array
jax.numpy.__init__.var(a:ArrayLike,axis:_Axis=...,dtype:DTypeLike=...,out:None=...,ddof:int=...,keepdims:bool=...,*,where:Optional[ArrayLike]=...)->Array
jax.numpy.__init__.vdot(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=...,preferred_element_type:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.vectorize(pyfunc,*,excluded=...,signature=...)->Callable
jax.numpy.__init__.vsplit(ary:ArrayLike,indices_or_sections:Union[int,ArrayLike])->list[Array]
jax.numpy.__init__.vstack(tup:Union[_np.ndarray,Array,Sequence[ArrayLike]],dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.where(condition:ArrayLike,x:ArrayLike,y:ArrayLike,*,size:Optional[int]=...,fill_value:Union[None,ArrayLike,tuple[ArrayLike,...]]=...)->Array
jax.numpy.__init__.where(condition:ArrayLike,x:Literal[None]=...,y:Literal[None]=...,*,size:Optional[int]=...,fill_value:Union[None,ArrayLike,tuple[ArrayLike,...]]=...)->tuple[Array, ...]
jax.numpy.__init__.where(condition:ArrayLike,x:Optional[ArrayLike]=...,y:Optional[ArrayLike]=...,*,size:Optional[int]=...,fill_value:Union[None,ArrayLike,tuple[ArrayLike,...]]=...)->Union[Array, tuple[Array, ...]]
jax.numpy.__init__.zeros(shape:Any,dtype:Optional[DTypeLike]=...)->Array
jax.numpy.__init__.zeros_like(a:Union[ArrayLike,DuckTypedArray],dtype:Optional[DTypeLike]=...,shape:Any=...)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/numpy/fft.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/numpy/linalg.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/tools/colab_tpu.py----------------------------------------
jax.tools.colab_tpu.setup_tpu(tpu_driver_version=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/tools/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/tools/build_utils.py----------------------------------------
A:jax.tools.build_utils.src_file_rloc->runfiles.Rlocation(src_file)
A:jax.tools.build_utils.src_filename->os.path.basename(src_file_rloc)
A:jax.tools.build_utils.dst_file->os.path.join(dst_dir, dst_filename or src_filename)
A:jax.tools.build_utils.output_file->os.path.join(output_path, os.path.basename(wheel))
jax.tools.build_utils.build_editable(sources_path:str,output_path:str,package_name:str)->None
jax.tools.build_utils.build_wheel(sources_path:str,output_path:str,package_name:str)->None
jax.tools.build_utils.copy_file(src_files:str|Sequence[str],dst_dir:pathlib.Path,dst_filename=None,runfiles=None)->None
jax.tools.build_utils.is_windows()->bool
jax.tools.build_utils.platform_tag(cpu:str)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/tools/jax_to_ir.py----------------------------------------
A:jax.tools.jax_to_ir._FN->absl.flags.DEFINE_string('fn', None, "Fully-qualified name of function that we're going to convert")
A:jax.tools.jax_to_ir._INPUT_SHAPES->absl.flags.DEFINE_string('input_shapes', None, 'Python dict indicating XLA shapes of params')
A:jax.tools.jax_to_ir._CONSTANTS->absl.flags.DEFINE_string('constants', '{}', 'Python dict giving constant values for some params')
A:jax.tools.jax_to_ir._EVALED_CONSTANTS->absl.flags.DEFINE_string('evaled_constants', '{}', 'Python dict giving constant values for some params.  Values in this dict that are of type str are evaluated using ast.literal_eval.')
A:jax.tools.jax_to_ir._IR_FORMAT->absl.flags.DEFINE_enum('ir_format', 'HLO', ('HLO', 'TF'), 'Output format.')
A:jax.tools.jax_to_ir._IR_DEST->absl.flags.DEFINE_string('ir_dest', None, 'File to write IR to')
A:jax.tools.jax_to_ir._IR_HUMAN_DEST->absl.flags.DEFINE_string('ir_human_dest', None, 'File to write human readable debug output')
A:jax.tools.jax_to_ir.fn_curried->functools.partial(fn, **constants)
A:jax.tools.jax_to_ir.comp->jax.xla_computation(ordered_wrapper)(*args)
A:jax.tools.jax_to_ir.serialized_proto->f.get_concrete_function(*args).graph.as_graph_def().SerializeToString()
A:jax.tools.jax_to_ir.debug_txt->str(g)
A:jax.tools.jax_to_ir.f->tensorflow.function(f, autograph=False)
A:jax.tools.jax_to_ir.g->tensorflow.function(f, autograph=False).get_concrete_function(*args).graph.as_graph_def()
A:jax.tools.jax_to_ir.args->tuple((tf.identity(a, name=name) for (a, (name, _)) in zip(args, input_shapes)))
A:jax.tools.jax_to_ir.jax_to_hlo->functools.partial(jax_to_ir, format='HLO')
A:jax.tools.jax_to_ir.jax_to_tf->functools.partial(jax_to_ir, format='TF')
A:jax.tools.jax_to_ir.(module_name, fn_name)->absl.flags.DEFINE_string('fn', None, "Fully-qualified name of function that we're going to convert").value.rsplit('.', 1)
A:jax.tools.jax_to_ir.module->importlib.import_module(module_name)
A:jax.tools.jax_to_ir.fn->getattr(module, fn_name)
A:jax.tools.jax_to_ir.v->jax.numpy.asarray(v)
A:jax.tools.jax_to_ir.(ir, debug_ir)->jax_to_ir(fn, input_shapes, constants=constants, format=_IR_FORMAT.value)
A:jax.tools.jax_to_ir.match->re.compile(f"^({'|'.join(_DT)})\\[\\s*(\\d*[\\s*,\\d+]*)\\s*\\]$").match(s)
A:jax.tools.jax_to_ir.shape->tuple((int(d.strip()) for d in match.group(2).split(',')))
A:jax.tools.jax_to_ir._SHAPE_RE->re.compile(f"^({'|'.join(_DT)})\\[\\s*(\\d*[\\s*,\\d+]*)\\s*\\]$")
jax.tools.jax_to_ir.jax_to_ir(fn,input_shapes,*,constants=None,format)
jax.tools.jax_to_ir.main(argv)
jax.tools.jax_to_ir.parse_shape_str(s)
jax.tools.jax_to_ir.set_up_flags()
jax.tools.jax_to_ir.tf_wrap_with_input_names(f,input_shapes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/extend/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/extend/core.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/extend/source_info_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/extend/linear_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/extend/random.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/nn/initializers.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/compilation_cache.py----------------------------------------
A:jax._src.compilation_cache.logger->logging.getLogger(__name__)
A:jax._src.compilation_cache._cache->GFileCache(path)
A:jax._src.compilation_cache.executable_and_time->zlib.compress(executable_and_time)
A:jax._src.compilation_cache.decompressor->zstandard.ZstdDecompressor()
A:jax._src.compilation_cache.(serialized_executable, compile_time)->extract_executable_and_time(executable_and_time)
A:jax._src.compilation_cache.xla_executable_deserialized->backend.deserialize_executable(serialized_executable, compile_options)
A:jax._src.compilation_cache.serialized_executable->backend.serialize_executable(executable)
A:jax._src.compilation_cache.compressor->zstandard.ZstdCompressor()
jax._src.compilation_cache.combine_executable_and_time(serialized_executable:bytes,compile_time:int)->bytes
jax._src.compilation_cache.extract_executable_and_time(exectuable_and_time:bytes)->tuple[bytes, int]
jax._src.compilation_cache.get_cache_key(module:ir.Module,devices:np.ndarray,compile_options,backend,produce_original_cache_key:bool=True)->str
jax._src.compilation_cache.get_executable_and_time(cache_key:str,compile_options,backend)->tuple[Optional[xla_client.LoadedExecutable], Optional[int]]
jax._src.compilation_cache.initialize_cache(path)
jax._src.compilation_cache.is_initialized()
jax._src.compilation_cache.put_executable_and_time(cache_key:str,module_name:str,executable:xla_client.LoadedExecutable,backend,compile_time:int)->None
jax._src.compilation_cache.reset_cache()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/ad_checkpoint.py----------------------------------------
A:jax._src.ad_checkpoint.logger->logging.getLogger(__name__)
A:jax._src.ad_checkpoint.name_p->jax._src.core.Primitive('name')
A:jax._src.ad_checkpoint.names_not_to_save->frozenset(names_not_to_save)
A:jax._src.ad_checkpoint.names_which_can_be_saved->set(names_which_can_be_saved)
A:jax._src.ad_checkpoint.checkpoint_policies->types.SimpleNamespace(everything_saveable=everything_saveable, nothing_saveable=nothing_saveable, dots_saveable=dots_saveable, checkpoint_dots=dots_saveable, dots_with_no_batch_dims_saveable=dot_with_no_batch_dims_saveable, checkpoint_dots_with_no_batch_dims=dot_with_no_batch_dims_saveable, save_anything_except_these_names=save_anything_except_these_names, save_any_names_but_these=save_any_names_but_these, save_only_these_names=save_only_these_names, save_from_both_policies=save_from_both_policies)
A:jax._src.ad_checkpoint.(fun_, args)->_remat_static_argnums(fun, static_argnums, args)
A:jax._src.ad_checkpoint.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax._src.ad_checkpoint.(jaxpr, consts, out_tree)->_trace_to_jaxpr(fun_, in_tree, tuple(in_avals))
A:jax._src.ad_checkpoint.out_flat->jax._src.core.Primitive('remat2').bind(*consts, *args_flat, jaxpr=jaxpr, prevent_cse=prevent_cse, differentiated=False, policy=policy)
A:jax._src.ad_checkpoint.nargs->len(args)
A:jax._src.ad_checkpoint.static_argnums_->frozenset((d % len(args) for d in static_argnums))
A:jax._src.ad_checkpoint.new_fun->_dyn_args_fun(fun, static_argnums_, tuple(static_args), nargs)
A:jax._src.ad_checkpoint.self.hash->id(val)
A:jax._src.ad_checkpoint._dyn_args_fun_cached->weakref_lru_cache(_dyn_args_fun_uncached)
A:jax._src.ad_checkpoint.(flat_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax._src.ad_checkpoint.debug->jax._src.interpreters.partial_eval.debug_info(fun, in_tree, out_tree, True, 'checkpoint')
A:jax._src.ad_checkpoint.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals, debug)
A:jax._src.ad_checkpoint.new_e->jax._src.core.ConcretizationTypeError.__new__(core.ConcretizationTypeError)
A:jax._src.ad_checkpoint.(in_leaves, in_tree)->tree_flatten((args, kwargs))
A:jax._src.ad_checkpoint.(args, kwargs)->tree_unflatten(in_tree, args)
A:jax._src.ad_checkpoint.out->jax._src.api.make_jaxpr(lambda *args: api.linearize(f_, *args)[1], return_shape=True)(*in_leaves)
A:jax._src.ad_checkpoint.dbg->jax._src.interpreters.partial_eval.debug_info(f, in_tree, out_tree, True, 'saved_residuals')
A:jax._src.ad_checkpoint.arg_info->jax._src.interpreters.partial_eval.arg_info_all(dbg)
A:jax._src.ad_checkpoint.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax._src.ad_checkpoint.remat_p->jax._src.core.Primitive('remat2')
A:jax._src.ad_checkpoint.(jaxpr_jvp_, out_nz)->jax._src.interpreters.ad.jvp_jaxpr(pe.close_jaxpr(jaxpr), in_nonzeros, False)
A:jax._src.ad_checkpoint.jaxpr_jvp->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_jvp_.jaxpr)
A:jax._src.ad_checkpoint.outs->jax._src.core.Primitive('remat2').bind(*jaxpr_jvp_.consts, *primals, *nonzero_tangents, jaxpr=jaxpr_jvp, prevent_cse=prevent_cse, differentiated=differentiated, policy=policy)
A:jax._src.ad_checkpoint.(out_primals, out_tangents_)->split_list(outs, [len(jaxpr.outvars)])
A:jax._src.ad_checkpoint.out_tangents_->iter(out_tangents_)
A:jax._src.ad_checkpoint.disallowed_effects->jax._src.effects.remat_allowed_effects.filter_not_in(jaxpr.effects)
A:jax._src.ad_checkpoint.(jaxpr_known, jaxpr_staged, out_unknowns, out_inst, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr, in_unknowns, [True] * len(in_unknowns), False, False, policy)
A:jax._src.ad_checkpoint.(_, out_inst_unknown)->partition_list(out_inst, out_unknowns)
A:jax._src.ad_checkpoint.(jaxpr_unknown, in_used_staged)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr_staged, out_inst_unknown)
A:jax._src.ad_checkpoint.(used_res, in_used_staged)->split_list(in_used_staged, [num_res])
A:jax._src.ad_checkpoint.(jaxpr_known, in_used_known)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr_known, out_used_known)
A:jax._src.ad_checkpoint.num_res->sum(used_res)
A:jax._src.ad_checkpoint.(_, in_consts_)->unzip2((t.pval for t in tracers if t.pval.is_known()))
A:jax._src.ad_checkpoint.(_, in_consts)->partition_list(in_used_known, in_consts_)
A:jax._src.ad_checkpoint.out_consts->jax._src.core.eval_jaxpr(jaxpr_known, (), *in_consts)
A:jax._src.ad_checkpoint.(out_knowns, residuals)->split_list(out_consts, [len(out_consts) - num_res])
A:jax._src.ad_checkpoint.res_tracers->map(trace.new_instantiated_const, residuals)
A:jax._src.ad_checkpoint.(_, tracers_staged)->partition_list(in_used_staged, tracers)
A:jax._src.ad_checkpoint.new_params->dict(eqn.params, jaxpr=new_jaxpr)
A:jax._src.ad_checkpoint.recipe->jax._src.interpreters.partial_eval.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p, new_params, jaxpr_unknown.effects, source_info_util.current())
A:jax._src.ad_checkpoint.(_, staged_unk)->partition_list(in_used_staged, in_unknowns)
A:jax._src.ad_checkpoint.(res_invars, _)->partition_list(staged_unk, jaxpr_unknown.invars[num_res:])
A:jax._src.ad_checkpoint.body_res->_saved_residuals(jaxpr_known.replace(outvars=res_outvars), None)
A:jax._src.ad_checkpoint.pe.partial_eval_jaxpr_custom_rules[remat_p]->partial(pe.call_partial_eval_custom_rule, 'jaxpr', remat_partial_eval_custom_params_updater)
A:jax._src.ad_checkpoint.(transposed_jaxpr_, in_zeros)->transpose_jaxpr(pe.close_jaxpr(jaxpr), in_linear, out_zeros, reduce_axes)
A:jax._src.ad_checkpoint.transposed_jaxpr->jax._src.core.ClosedJaxpr(transposed_jaxpr_, consts)
A:jax._src.ad_checkpoint.(args, _)->tree_flatten((in_primals, out_cts))
A:jax._src.ad_checkpoint.in_cts_nz->jax._src.core.Primitive('remat2').bind(*consts, *args, jaxpr=transposed_jaxpr, **params)
A:jax._src.ad_checkpoint.(ins_flat, out_cts_flat)->split_list(args_flat, [len(in_lin) - sum(in_lin)])
A:jax._src.ad_checkpoint.ins_iter->iter(ins_flat)
A:jax._src.ad_checkpoint.(lin_jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_nounits(lu.wrap_init(core.jaxpr_as_fun(jaxpr)), in_pvals, False)
A:jax._src.ad_checkpoint.out_cts_iter->iter(out_cts_flat)
A:jax._src.ad_checkpoint.in_cts->jax._src.interpreters.ad.backward_pass(lin_jaxpr, reduce_axes, False, consts, dummy_args, out_cts)
A:jax._src.ad_checkpoint.(in_cts_nz, _)->partition_list(in_zeros, in_cts)
A:jax._src.ad_checkpoint.(transposed_jaxpr_, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(transposed, in_avals)
A:jax._src.ad_checkpoint.(jaxpr_batched_, out_batched)->jax._src.interpreters.batching.batch_jaxpr_axes(pe.close_jaxpr(jaxpr), axis_size, dims, [batching.zero_if_mapped] * len(jaxpr.outvars), axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.ad_checkpoint.jaxpr_batched->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_batched)
A:jax._src.ad_checkpoint.batching.axis_primitive_batchers[remat_p]->partial(remat_vmap, None)
A:jax._src.ad_checkpoint.(new_jaxpr, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(eqn.params['jaxpr'], used_outputs)
A:jax._src.ad_checkpoint.new_eqn->jax._src.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax._src.ad_checkpoint.args->_optimization_barrier(args)
A:jax._src.ad_checkpoint.avals_out->tuple((v.aval for v in jaxpr.outvars))
A:jax._src.ad_checkpoint.unif->jax._src.lax.lax.rng_uniform(np.float32(0), np.float32(1), shape=())
A:jax._src.ad_checkpoint.results->jax._src.core.eval_jaxpr(jaxpr, (), *args)
A:jax._src.ad_checkpoint.carry_res->jax._src.lax.control_flow.while_loop(cond, body, carry_init)
A:jax._src.ad_checkpoint.barrier_types->map(mlir.aval_to_ir_types, ctx.avals_in)
A:jax._src.ad_checkpoint.flat_args->jax._src.interpreters.mlir.flatten_lowering_ir_args(args)
A:jax._src.ad_checkpoint.barrier_op->jax._src.lib.mlir.dialects.hlo.OptimizationBarrierOp(flat_args)
A:jax._src.ad_checkpoint.(flat_args, treedef)->tree_flatten(arg)
A:jax._src.ad_checkpoint.optimization_barrier_p->jax._src.core.Primitive('optimization_barrier')
jax._src.ad_checkpoint.WrapHashably(self,val)
jax._src.ad_checkpoint.WrapHashably.__eq__(self,other)
jax._src.ad_checkpoint.WrapHashably.__hash__(self)
jax._src.ad_checkpoint.WrapHashably.__init__(self,val)
jax._src.ad_checkpoint._dummy_like(aval:core.AbstractValue)->Any
jax._src.ad_checkpoint._dyn_args_fun(fun:Callable,static_argnums:frozenset[int],static_args:tuple[WrapHashably,...],nargs:int)
jax._src.ad_checkpoint._dyn_args_fun_uncached(fun:Callable,static_argnums:frozenset[int],static_args:tuple[WrapHashably,...],nargs:int)
jax._src.ad_checkpoint._optimization_barrier(arg)
jax._src.ad_checkpoint._optimization_barrier_abstract_eval(*args)
jax._src.ad_checkpoint._optimization_barrier_lowering_rule(ctx,*args)
jax._src.ad_checkpoint._remat_static_argnums(fun,static_argnums,args)
jax._src.ad_checkpoint._remat_translation_using_cond(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._remat_translation_using_opt_barrier(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._remat_translation_using_while(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._saved_residuals(jaxpr,arg_info)->list[tuple[core.AbstractValue, str]]
jax._src.ad_checkpoint._trace_to_jaxpr(fun,in_tree,in_avals)
jax._src.ad_checkpoint._transpose_jaxpr(jaxpr,in_lin,out_zeros,reduce_axes)
jax._src.ad_checkpoint.checkpoint(fun:Callable,*,prevent_cse:bool=True,policy:Optional[Callable[...,bool]]=None,static_argnums:Union[int,tuple[int,...]]=())->Callable
jax._src.ad_checkpoint.checkpoint_name(x,name)
jax._src.ad_checkpoint.checkpoint_wrapper(fun:Callable,*,concrete:bool=False,prevent_cse:bool=True,static_argnums:Union[int,tuple[int,...]]=(),policy:Optional[Callable[...,bool]]=None)->Callable
jax._src.ad_checkpoint.dot_with_no_batch_dims_saveable(prim,*_,**params)->bool
jax._src.ad_checkpoint.dots_saveable(prim,*_,**__)->bool
jax._src.ad_checkpoint.everything_saveable(*_,**__)->bool
jax._src.ad_checkpoint.name_batcher(args,dims,*,name)
jax._src.ad_checkpoint.name_jvp(primals,tangents,*,name)
jax._src.ad_checkpoint.nothing_saveable(*_,**__)->bool
jax._src.ad_checkpoint.print_saved_residuals(f,*args,**kwargs)
jax._src.ad_checkpoint.remat_abstract_eval(*args,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_dce(used_outputs:list[bool],eqn:core.JaxprEqn)->tuple[list[bool], Optional[core.JaxprEqn]]
jax._src.ad_checkpoint.remat_impl(*args,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_jvp(primals,tangents,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_lowering(*args,jaxpr:core.Jaxpr,prevent_cse:bool,differentiated:bool,is_gpu_platform:bool=False,**_)
jax._src.ad_checkpoint.remat_partial_eval(trace,*tracers,jaxpr,**params)
jax._src.ad_checkpoint.remat_partial_eval_custom_params_updater(*args)
jax._src.ad_checkpoint.remat_transpose(reduce_axes,out_cts,*in_primals,jaxpr,**params)
jax._src.ad_checkpoint.remat_vmap(spmd_axis_name,axis_size,axis_name,main_type,args,dims,*,jaxpr,**params)
jax._src.ad_checkpoint.save_any_names_but_these(*names_not_to_save)
jax._src.ad_checkpoint.save_anything_except_these_names(*names_not_to_save)
jax._src.ad_checkpoint.save_from_both_policies(policy_1,policy_2)
jax._src.ad_checkpoint.save_only_these_names(*names_which_can_be_saved)
jax._src.ad_checkpoint.saved_residuals(f,*args,**kwargs)->list[tuple[core.AbstractValue, str]]
jax._src.ad_checkpoint.transpose_jaxpr(jaxpr:core.ClosedJaxpr,in_linear:Union[bool,Sequence[bool]],out_zeros:Union[bool,Sequence[bool]],reduce_axes:Sequence[core.AxisName])->tuple[core.ClosedJaxpr, list[bool]]
jax.remat(fun:Callable,*,concrete:bool=False,prevent_cse:bool=True,static_argnums:Union[int,tuple[int,...]]=(),policy:Optional[Callable[...,bool]]=None)->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/tpu_custom_call.py----------------------------------------
A:jax._src.tpu_custom_call._MOSAIC_USE_CPP_PASSES->CustomCallBackendConfig(lowered_module_asm, has_communication, collective_id, device_type, cost_estimate, flags).define_bool_state(name='mosaic_use_cpp_passes', default=False, help='Use C++ implementation for apply-vector-layout and infer-memref-layout passes (still a WIP)')
A:jax._src.tpu_custom_call._MOSAIC_ALLOW_HLO->CustomCallBackendConfig(lowered_module_asm, has_communication, collective_id, device_type, cost_estimate, flags).define_bool_state(name='jax_mosaic_allow_hlo', default=False, help='Allow hlo dialects in Mosaic')
A:jax._src.tpu_custom_call._MOSAIC_DUMP_MLIR->CustomCallBackendConfig(lowered_module_asm, has_communication, collective_id, device_type, cost_estimate, flags).define_bool_state(name='jax_mosaic_dump_mlir', default=False, help='Print mlir module after each pass')
A:jax._src.tpu_custom_call.tpu_custom_call_p->jax.core.Primitive('tpu_custom_call')
A:jax._src.tpu_custom_call.config->CustomCallBackendConfig(lowered_module_asm, has_communication, collective_id, device_type, cost_estimate, flags)
A:jax._src.tpu_custom_call.arange->numpy.arange(aval.ndim, dtype=np.dtype(np.int64))[::-1].copy()
A:jax._src.tpu_custom_call.i32_type->jaxlib.mlir.ir.IntegerType.get_signless(32)
A:jax._src.tpu_custom_call.result_type->jax.interpreters.mlir.aval_to_ir_type(out_avals[0])
A:jax._src.tpu_custom_call.call->jaxlib.mlir.dialects.stablehlo.CustomCallOp([result_type], in_nodes, call_target_name=ir.StringAttr.get(b'tpu_custom_call'), has_side_effect=ir.BoolAttr.get(False), backend_config=ir.StringAttr.get(config.to_json()), api_version=ir.IntegerAttr.get(i32_type, 1), called_computations=ir.ArrayAttr.get([]), operand_layouts=_avals_to_layouts(ctx.avals_in), result_layouts=_avals_to_layouts(ctx.avals_out), output_operand_aliases=None)
A:jax._src.tpu_custom_call.call.attributes['kernel_name']->jaxlib.mlir.ir.StringAttr.get(kernel_name)
A:jax._src.tpu_custom_call.call.attributes['kernel_regeneration_metadata']->jaxlib.mlir.ir.StringAttr.get(base64.b64encode(kernel_regeneration_metadata))
A:jax._src.tpu_custom_call._LOCATION_REGEX->re.compile('loc\\("([a-zA-Z/]+)"\\("(.*)":([0-9]+):[0-9]+\\)\\)')
A:jax._src.tpu_custom_call._OP_ERROR_PATTERN->re.compile("'.*' op (.+)")
A:jax._src.tpu_custom_call.diag_msg->match.group(1)
A:jax._src.tpu_custom_call.module->jaxlib.mlir.ir.Module.parse(module.operation.get_asm(binary=True, enable_debug_info=True))
A:jax._src.tpu_custom_call.pipeline->jaxlib.mlir.passmanager.PassManager.parse('builtin.module(canonicalize)')
A:jax._src.tpu_custom_call.constant_attrs->jaxlib.mlir.ir.ArrayAttr(f.attributes['vector_constants'])
A:jax._src.tpu_custom_call.c->jaxlib.mlir.ir.DenseFPElementsAttr(c)
A:jax._src.tpu_custom_call.constant_type->jaxlib.mlir.ir.VectorType(c.type)
A:jax._src.tpu_custom_call.bytecode_buffer->io.BytesIO()
A:jax._src.tpu_custom_call.hardware_generation->int(device_kind[len('TPU v')])
A:jax._src.tpu_custom_call.(has_communication, has_custom_barrier)->tpu.private_has_communication(module.operation)
A:jax._src.tpu_custom_call.(lowered_module_asm, constants)->_lower_tpu_kernel(module, hardware_generation, device_type=device_type)
A:jax._src.tpu_custom_call.out_avals->tuple((core.ShapedArray(ty.shape, ty.dtype) for ty in out_type))
A:jax._src.tpu_custom_call.result->jax.core.Primitive('tpu_custom_call').bind(*args, *constants, config=config, kernel_name=kernel_name, kernel_regeneration_metadata=kernel_regeneration_metadata, out_avals=out_avals)
jax._src.tpu_custom_call.CostEstimate
jax._src.tpu_custom_call.CostEstimate.to_json(self)->bytes
jax._src.tpu_custom_call.CustomCallBackendConfig
jax._src.tpu_custom_call.CustomCallBackendConfig.__repr__(self)
jax._src.tpu_custom_call.CustomCallBackendConfig.to_json(self)->bytes
jax._src.tpu_custom_call._aval_to_layout(aval)
jax._src.tpu_custom_call._avals_to_layouts(avals)
jax._src.tpu_custom_call._lower_tpu_kernel(module:ir.Module,hardware_generation:int,device_type:str|None)->ir.Module
jax._src.tpu_custom_call._lowered_as_tpu_kernel(lowered_module_asm:bytes,out_type:Any,constants:Sequence[Any]=(),*,cost_estimate:CostEstimate|None=None,device_type:str|None=None,has_communication:bool=False,has_custom_barrier:bool=False,kernel_name:str|None=None,kernel_regeneration_metadata:bytes|None=None,flags:dict[str,bool|int|float]|None=None)
jax._src.tpu_custom_call._run_pass_pipeline(passes:PassManager,module:ir.Module,what:str)
jax._src.tpu_custom_call._tpu_custom_call_abstract_eval(*_,out_avals,**__)
jax._src.tpu_custom_call._tpu_custom_call_lowering(ctx:mlir.LoweringRuleContext,*in_nodes,config:CustomCallBackendConfig,kernel_name:str|None,kernel_regeneration_metadata:bytes|None,out_avals:Any)->...
jax._src.tpu_custom_call.as_tpu_kernel(module:ir.Module,out_type:Any,*,cost_estimate:CostEstimate|None=None,backend:str|xla_client.Client='tpu',device_type:str|None=None,kernel_name:str|None=None,kernel_regeneration_metadata:bytes|None=None,flags:dict[str,bool|int|float]|None=None)->Callable[..., Any]
jax._src.tpu_custom_call.dump_mlir(module:ir.Module,msg:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/stages.py----------------------------------------
A:jax._src.stages.xla_ext_exe->self.xla_extension_executable()
A:jax._src.stages.module_str->xla_extension.mlir.stablehlo_to_mhlo(mlir.module_to_bytecode(self.stablehlo()))
A:jax._src.stages.donate_argnums->frozenset(donate_argnums)
A:jax._src.stages.(flat_avals, _)->jax._src.tree_util.tree_flatten(in_avals)
A:jax._src.stages.self._params->CompiledCallParams(self._executable, self._no_kwargs, self.in_tree, self.out_tree)
A:jax._src.stages.shardings_flat->self._executable.output_shardings()
A:jax._src.stages.kws->', '.join(kwargs.keys())
A:jax._src.stages.(args_flat, in_tree)->jax._src.tree_util.tree_flatten((args, kwargs))
A:jax._src.stages.out_flat->params.executable.call(*args_flat)
A:jax._src.stages.outs->jax._src.tree_util.tree_unflatten(params.out_tree, out_flat)
A:jax._src.stages.self._call->self._executable.create_cpp_call(self._no_kwargs, self.in_tree, self.out_tree)
A:jax._src.stages.(outs, _, _)->Compiled.call(params, *args, **kwargs)
jax._src.stages.ArgInfo
jax._src.stages.Compiled(self,executable,args_info,out_tree,no_kwargs=False)
jax._src.stages.Compiled.__init__(self,executable,args_info,out_tree,no_kwargs=False)
jax._src.stages.Compiled.as_text(self)->str | None
jax._src.stages.Compiled.call(*args,**kwargs)
jax._src.stages.Compiled.cost_analysis(self)->Any | None
jax._src.stages.Compiled.input_shardings(self)
jax._src.stages.Compiled.memory_analysis(self)->Any | None
jax._src.stages.Compiled.output_shardings(self)
jax._src.stages.Compiled.runtime_executable(self)->Any | None
jax._src.stages.CompiledCallParams(NamedTuple)
jax._src.stages.Executable(Protocol)
jax._src.stages.Executable.as_text(self)->str
jax._src.stages.Executable.call(self,*args_flat)->Sequence[Any]
jax._src.stages.Executable.cost_analysis(self)->Any
jax._src.stages.Executable.create_cpp_call(self,no_kwargs,in_tree,out_tree)->Any
jax._src.stages.Executable.input_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.Executable.memory_analysis(self)->Any
jax._src.stages.Executable.output_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.Executable.runtime_executable(self)->Any
jax._src.stages.Lowered(self,lowering:XlaLowering,args_info,out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowered.__init__(self,lowering:XlaLowering,args_info,out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowered.as_text(self,dialect:str|None=None)->str
jax._src.stages.Lowered.compile(self,compiler_options:CompilerOptions|None=None)->Compiled
jax._src.stages.Lowered.compiler_ir(self,dialect:str|None=None)->Any | None
jax._src.stages.Lowered.cost_analysis(self)->Any | None
jax._src.stages.Lowered.from_flat_info(cls,lowering:XlaLowering,in_tree:tree_util.PyTreeDef,in_avals,donate_argnums:tuple[int,...],out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowering(Protocol)
jax._src.stages.Lowering.as_text(self,dialect:str|None=None)->str
jax._src.stages.Lowering.compile(self,compiler_options:CompilerOptions|None=None)->Executable
jax._src.stages.Lowering.compiler_ir(self,dialect:str|None=None)->Any
jax._src.stages.Lowering.cost_analysis(self)->Any
jax._src.stages.Stage
jax._src.stages.Stage.donate_argnums(self)
jax._src.stages.Stage.in_avals(self)
jax._src.stages.Stage.in_tree(self)->tree_util.PyTreeDef
jax._src.stages.Wrapped(self,*args,**kwargs)
jax._src.stages.Wrapped.__call__(self,*args,**kwargs)
jax._src.stages.Wrapped.lower(self,*args,**kwargs)->Lowered
jax._src.stages.XlaExecutable(Executable)
jax._src.stages.XlaExecutable.as_text(self)->str
jax._src.stages.XlaExecutable.call(self,*args_flat)->Sequence[Any]
jax._src.stages.XlaExecutable.cost_analysis(self)->list[dict[str, float]]
jax._src.stages.XlaExecutable.input_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.XlaExecutable.memory_analysis(self)->Any
jax._src.stages.XlaExecutable.output_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.XlaExecutable.runtime_executable(self)->Any
jax._src.stages.XlaExecutable.xla_extension_executable(self)->xc.LoadedExecutable
jax._src.stages.XlaLowering(Lowering)
jax._src.stages.XlaLowering.as_text(self,dialect:str|None=None)->str
jax._src.stages.XlaLowering.compile(self,compiler_options:CompilerOptions|None=None)->Executable
jax._src.stages.XlaLowering.compiler_ir(self,dialect:str|None=None)->Any
jax._src.stages.XlaLowering.cost_analysis(self)->dict[str, float]
jax._src.stages.XlaLowering.hlo(self)->xc.XlaComputation
jax._src.stages.XlaLowering.mhlo(self)->ir.Module
jax._src.stages.XlaLowering.stablehlo(self)->ir.Module
jax._src.stages.make_args_info(in_tree,in_avals,donate_argnums)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pjit.py----------------------------------------
A:jax._src.pjit.logger->logging.getLogger(__name__)
A:jax._src.pjit.dummy_args->tree_unflatten(tree, [False] * tree.num_leaves)
A:jax._src.pjit.sig->_try_infer_args(fun, in_tree)
A:jax._src.pjit.args_aug->generate_key_paths(tree_unflatten(in_tree, args_flat))
A:jax._src.pjit.loc->''.join((str(k) for k in rem_keys))
A:jax._src.pjit.mismatched_args_msg->_find_arg_mismatch(arg_list, fails, fun_name)
A:jax._src.pjit.(args_flat, _, params, in_tree, out_tree, _)->infer_params_fn(*args, **kwargs)
A:jax._src.pjit.out_flat->jax._src.core.AxisPrimitive('pjit').bind(*args_flat, **params)
A:jax._src.pjit.arg_names->_get_arg_names(fun, in_tree, args_flat)
A:jax._src.pjit.fun_name->getattr(fun, '__qualname__', getattr(fun, '__name__', str(fun)))
A:jax._src.pjit.msg->_device_assignment_mismatch_error(fun_name, fails, args_flat, api_name, arg_names)
A:jax._src.pjit.outs->tree_unflatten(out_tree, out_flat)
A:jax._src.pjit.(out_flat, out_tree)->jax._src.interpreters.pxla.reflatten_outputs_for_dispatch(out_tree, out_flat)
A:jax._src.pjit.fastpath_data->_get_fastpath_data(compiled, tree_structure(out_flat), args, out_flat)
A:jax._src.pjit.self.weak_key_dict->weakref.WeakKeyDictionary()
A:jax._src.pjit._most_recent_pjit_call_executable->_MostRecentPjitCallExecutable()
A:jax._src.pjit._cpp_pjit_cache->jax._src.lib.xla_client._xla.PjitFunctionCache(capacity=8192)
A:jax._src.pjit.(outs, out_flat, out_tree, args_flat, jaxpr)->_python_pjit_helper(fun, infer_params_fn, *args, **kwargs)
A:jax._src.pjit.executable->_read_most_recent_pjit_call_executable(jaxpr)
A:jax._src.pjit.cpp_pjit_f->jax._src.lib.xla_client._xla.pjit(getattr(fun, '__name__', '<unnamed function>'), fun, cache_miss, static_argnums, static_argnames, donate_argnums, tree_util.dispatch_registry, _get_cpp_global_cache(pjit_has_explicit_sharding))
A:jax._src.pjit.cpp_pjitted_f->wraps(fun)(cpp_pjit_f)
A:jax._src.pjit.in_shardings->tuple((_pjit_batcher_for_sharding(i, axis_in, new_parts, mesh, aval.ndim) if axis_in is not None else i for (axis_in, i, aval) in zip(dims_in, in_shardings, new_jaxpr.in_avals)))
A:jax._src.pjit.(in_shardings, _, _)->prepare_axis_resources(in_shardings, 'in_shardings')
A:jax._src.pjit.(out_shardings, _, _)->prepare_axis_resources(out_shardings, 'out_shardings')
A:jax._src.pjit.(donate_argnums, donate_argnames, static_argnums, static_argnames)->resolve_argnums(fun, donate_argnums, donate_argnames, static_argnums, static_argnames)
A:jax._src.pjit.wrapped->_python_pjit(fun, infer_params_fn)
A:jax._src.pjit.lowering_parameters->kwargs.pop('_experimental_lowering_parameters', mlir.LoweringParameters())
A:jax._src.pjit.(args_flat, flat_global_in_avals, params, in_tree, out_tree, donated_invars)->infer_params_fn(*args, **kwargs)
A:jax._src.pjit.lowering->_pjit_lower(params['jaxpr'], in_shardings, params['out_shardings'], params['resource_env'], params['donated_invars'], params['name'], params['keep_unused'], params['inline'], lowering_parameters=lowering_parameters)
A:jax._src.pjit.args_kwargs_in_tree->treedef_tuple([in_tree, tree_flatten({})[1]])
A:jax._src.pjit.donate_argnums->tuple((i for (i, d) in enumerate(donated_invars) if d))
A:jax._src.pjit.(in_shardings_flat, _)->tree_flatten(in_shardings)
A:jax._src.pjit.(out_shardings_flat, _)->tree_flatten(out_shardings)
A:jax._src.pjit.axes_specs->_flat_axes_specs(abstracted_axes, *args, **kwargs)
A:jax._src.pjit.dbg->debug_info(jit_name, fun, args, kwargs, static_argnums, static_argnames)
A:jax._src.pjit.f->_get_jaxpr_as_fun(jaxpr, tuple((getattr(i, '_original_sharding', i) for i in in_shardings)), tuple((getattr(o, '_original_sharding', o) for o in out_shardings)), resource_env, donated_invars, name, keep_unused, inline)
A:jax._src.pjit.(f, res_paths)->result_paths(f)
A:jax._src.pjit.(f, dyn_args)->argnums_partial_except(f, static_argnums, args, allow_invalid=True)
A:jax._src.pjit.(f, dyn_kwargs)->argnames_partial_except(f, static_argnames, kwargs)
A:jax._src.pjit.(explicit_args, in_tree)->tree_flatten(dyn_args)
A:jax._src.pjit.(flat_fun, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax._src.pjit.donated_invars->donation_vector(donate_argnums, donate_argnames, dyn_args, dyn_kwargs)
A:jax._src.pjit.in_shardingsout_shardings->_create_sharding_with_device_backend(device, backend)
A:jax._src.pjit.out_shardings->tuple((_pjit_batcher_for_sharding(o, axis_out, new_parts, mesh, aval.ndim) if axis_out is not None else o for (axis_out, o, aval) in zip(axes_out, out_shardings, new_jaxpr.out_avals)))
A:jax._src.pjit.in_type->jax._src.interpreters.partial_eval.infer_lambda_input_type(axes_specs, explicit_args)
A:jax._src.pjit.in_avals->tuple((a for (a, e) in in_type if e))
A:jax._src.pjit.in_typein_avals->tuple(avals)
A:jax._src.pjit.canonicalized_in_shardings_flat->_process_in_axis_resources(hashable_pytree(in_shardings), in_avals, in_tree, resource_env, dbg, device_or_backend_set)
A:jax._src.pjit.(jaxpr, consts, canonicalized_out_shardings_flat)->_pjit_jaxpr(flat_fun, hashable_pytree(out_shardings), in_type, dbg, device_or_backend_set, HashableFunction(out_tree, closure=()), HashableFunction(res_paths, closure=()))
A:jax._src.pjit.implicit_args->_extract_implicit_args(in_type, explicit_args)
A:jax._src.pjit.params->dict(eqn.params)
A:jax._src.pjit.explicit_args_->iter(explicit_args)
A:jax._src.pjit.(in_shardings, out_shardings, donate_argnums, donate_argnames, static_argnums, static_argnames)->pre_infer_params(fun, in_shardings, out_shardings, donate_argnums, donate_argnames, static_argnums, static_argnames, device, backend, abstracted_axes)
A:jax._src.pjit.pjit_info_args->PjitInfo(fun=fun, in_shardings=in_shardings, out_shardings=out_shardings, static_argnums=static_argnums, static_argnames=static_argnames, donate_argnums=donate_argnums, donate_argnames=donate_argnames, device=device, backend=backend, keep_unused=keep_unused, inline=inline, resource_env=resource_env, abstracted_axes=abstracted_axes)
A:jax._src.pjit.has_explicit_sharding->_pjit_explicit_sharding(in_shardings, out_shardings, None, None)
A:jax._src.pjit.(vals, treedef)->tree_flatten(pytree)
A:jax._src.pjit.vals->tuple(vals)
A:jax._src.pjit.out->SingleDeviceSharding(xb.get_backend(backend).local_devices()[0])
A:jax._src.pjit.dummy_tree->tree_unflatten(tree, [PytreeLeaf()] * tree.num_leaves)
A:jax._src.pjit.errors->prefix_errors(axis_tree, dummy_tree)
A:jax._src.pjit.orig_in_shardings->in_shardings_thunk()
A:jax._src.pjit.in_shardings_flat->flatten_axis_resources('pjit in_shardings', in_tree, orig_in_shardings, tupled_args=True)
A:jax._src.pjit.canonicalized_shardings->tuple((i if is_unspecified_or_auto(i) else to_gspmd_sharding(i, aval.ndim, device_or_backend_set) for (i, aval) in zip(in_shardings_flat, in_avals)))
A:jax._src.pjit.(jaxpr, global_out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_type, debug_info=pe_debug)
A:jax._src.pjit.jaxpr->weakref.ref(jaxpr)
A:jax._src.pjit.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.pjit.orig_out_shardings->out_shardings_thunk()
A:jax._src.pjit.out_shardings_flat->flatten_axis_resources('pjit out_shardings', out_tree(), orig_out_shardings, tupled_args=False)
A:jax._src.pjit.canonicalized_out_shardings_flat->_check_and_canonicalize_out_shardings(out_shardings_thunk, out_tree, tuple(out_type), jaxpr.jaxpr.debug_info, device_or_backend_set)
A:jax._src.pjit.(jaxpr, final_consts, out_type)->_create_pjit_jaxpr(fun, in_type, debug_info, result_paths)
A:jax._src.pjit.s->getattr(s, '_original_sharding', s)
A:jax._src.pjit.hlo_sharding->getattr(s, '_original_sharding', s)._to_xla_hlo_sharding(len(shape))
A:jax._src.pjit.(num_ways_dim_sharded, _)->jax._src.op_shardings.get_num_ways_dim_sharded(hlo_sharding)
A:jax._src.pjit.pjit_p->jax._src.core.AxisPrimitive('pjit')
A:jax._src.pjit.op->getattr(pjit_in_s, '_original_sharding', pjit_in_s)
A:jax._src.pjit.compiled->_pjit_lower(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, lowering_parameters=mlir.LoweringParameters()).compile()
A:jax._src.pjit.fingerprint->fingerprint.hex().hex()
A:jax._src.pjit._->jax._src.core.jaxpr_as_fun(jaxpr)(*args)
A:jax._src.pjit.(out_flat, compiled)->_pjit_call_impl_python(*args, jaxpr=jaxpr, in_shardings=in_shardings, out_shardings=out_shardings, resource_env=resource_env, donated_invars=donated_invars, name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.shardings_hash->tuple((s._hlo_sharding_hash if isinstance(s, GSPMDSharding) else s for s in self.shardings))
A:jax._src.pjit.o->getattr(o, '_original_sharding', o)
A:jax._src.pjit.da->tuple(axis_ctx.device_assignment)
A:jax._src.pjit.source_info->jax._src.source_info_util.current()
A:jax._src.pjit.aval->aval.update(shape=tuple(shape)).update(shape=tuple(shape))
A:jax._src.pjit.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe((*unknown_tracers_in, *residual_tracers), unknown_tracers_out, pjit_p, unknown_params, unknown_jaxpr.effects, source_info_util.current())
A:jax._src.pjit.func->_pjit_cached_lower_jaxpr_to_fun(ctx, name, jaxpr, tuple(effects), in_shardings, out_shardings, api_name='jit' if resource_env is None else 'pjit')
A:jax._src.pjit.effects->list(ctx.tokens_in.effects())
A:jax._src.pjit.output_types->map(mlir.aval_to_ir_types, ctx.avals_out)
A:jax._src.pjit.flat_output_types->flatten(output_types)
A:jax._src.pjit.call->jax._src.lib.mlir.dialects.func.CallOp(flat_output_types, ir.FlatSymbolRefAttr.get(func.name.value), mlir.flatten_lowering_ir_args(args))
A:jax._src.pjit.out_nodes->unflatten(call.results, map(len, output_types))
A:jax._src.pjit.(tokens, out_nodes)->split_list(out_nodes, [len(effects)])
A:jax._src.pjit.tokens_out->ctx.tokens_in.update_tokens(mlir.TokenSet(zip(effects, tokens)))
A:jax._src.pjit.(segment_lens, dims_in)->jax._src.interpreters.batching.indirectify_ragged_axes(dims_in)
A:jax._src.pjit.(new_jaxpr, axes_out)->jax._src.interpreters.batching.batch_jaxpr2(jaxpr, axis_size, dims_in, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.pjit.vals_out->jax._src.core.AxisPrimitive('pjit').bind(*vals_in, jaxpr=new_jaxpr, in_shardings=in_shardings, out_shardings=out_shardings, resource_env=resource_env, donated_invars=donated_invars, name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.resolved_axes_out->jax._src.interpreters.batching.resolve_ragged_axes_against_inputs_outputs(vals_in, vals_out, axes_out)
A:jax._src.pjit.batching.spmd_axis_primitive_batchers[pjit_p]->partial(_pjit_batcher, False)
A:jax._src.pjit.batching.axis_primitive_batchers[pjit_p]->partial(_pjit_batcher, False, None)
A:jax._src.pjit.pxla.spmd_primitive_batchers[pjit_p]->partial(_pjit_batcher, True, None)
A:jax._src.pjit.old_op->getattr(s, '_original_sharding', s)._hlo_sharding.to_proto()
A:jax._src.pjit.new_op->getattr(s, '_original_sharding', s)._hlo_sharding.to_proto().clone()
A:jax._src.pjit.tad->list(new_op.tile_assignment_dimensions)
A:jax._src.pjit.new_gs->to_gspmd_sharding(vmapped_s, ndim)
A:jax._src.pjit.parsed_pspec->parsed_pspec.insert_axis_partitions(dim, val).insert_axis_partitions(dim, val)
A:jax._src.pjit.mps->jax._src.sharding_impls.NamedSharding._from_parsed_pspec(mesh, parsed_pspec)
A:jax._src.pjit.(jaxpr_jvp, is_nz_tangents_out)->jax._src.interpreters.ad.jvp_jaxpr(jaxpr, is_nz_tangents_in, instantiate=False)
A:jax._src.pjit._filter_zeros_in->partial(_filter_zeros, is_nz_tangents_in)
A:jax._src.pjit._filter_zeros_out->partial(_filter_zeros, is_nz_tangents_out)
A:jax._src.pjit.outputs->jax._src.core.AxisPrimitive('pjit').bind(*primals_in, *_filter_zeros_in(tangents_in), jaxpr=jaxpr_jvp, in_shardings=(*in_shardings, *_filter_zeros_in(in_shardings)), out_shardings=(*out_shardings, *_filter_zeros_out(out_shardings)), resource_env=resource_env, donated_invars=(*donated_invars, *_filter_zeros_in(donated_invars)), name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.(primals_out, tangents_out)->split_list(outputs, [len(jaxpr.jaxpr.outvars)])
A:jax._src.pjit.tangents_out_it->iter(tangents_out)
A:jax._src.pjit.updated_jaxpr->jax._src.interpreters.partial_eval.prune_closed_jaxpr_outputs(known_jaxpr, keep).jaxpr.replace(outvars=[x for (x, i) in zip(known_jaxpr.jaxpr.outvars, in_fwd) if i is None])
A:jax._src.pjit.known_ins->tuple((pv.is_known() for pv in in_pvals))
A:jax._src.pjit.unknown_ins->tuple((not k for k in known_ins))
A:jax._src.pjit.(known_jaxpr, unknown_jaxpr, unknown_outs, res_avals)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)
A:jax._src.pjit.unknown_outs->tuple(unknown_outs)
A:jax._src.pjit.known_outs->tuple((not uk for uk in unknown_outs))
A:jax._src.pjit.num_residuals->len(res_avals)
A:jax._src.pjit.in_fwd->jax._src.interpreters.partial_eval._jaxpr_forwarding(known_jaxpr.jaxpr)
A:jax._src.pjit.(in_fwd_primal, in_fwd_res)->split_list(in_fwd, [num_out_primals])
A:jax._src.pjit.(out_vars, res_vars)->split_list(known_jaxpr.jaxpr.outvars, [num_out_primals])
A:jax._src.pjit.known_jaxpr->jax._src.interpreters.partial_eval.prune_closed_jaxpr_outputs(known_jaxpr, keep)
A:jax._src.pjit.known_out_shardings->keep_where(known_out_shardings, keep)
A:jax._src.pjit.known_params->dict(jaxpr=known_jaxpr, in_shardings=keep_where(in_shardings, known_ins), out_shardings=known_out_shardings, resource_env=resource_env, donated_invars=keep_where(donated_invars, known_ins), name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.all_known_outs->subs_list2(in_fwd, out_fwd, known_inputs, all_known_outs, all_known_outs)
A:jax._src.pjit.(known_out_vals, residual_vals)->split_list(all_known_outs, [len(all_known_outs) - num_residuals])
A:jax._src.pjit.residual_tracers->map(trace.new_instantiated_const, residual_vals)
A:jax._src.pjit.unknown_jaxpr->jax._src.interpreters.partial_eval.move_binders_to_back(unknown_jaxpr, [True] * num_residuals + [False] * sum(unknown_ins))
A:jax._src.pjit.unknown_params->dict(jaxpr=unknown_jaxpr, in_shardings=keep_where(in_shardings, unknown_ins) + res_shardings, out_shardings=keep_where(out_shardings, unknown_outs), resource_env=resource_env, donated_invars=keep_where(donated_invars, unknown_ins) + (False,) * num_residuals, name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.(donated_invars_known, _)->jax._src.interpreters.partial_eval.partition_list(unks_in, params_known['donated_invars'])
A:jax._src.pjit.(in_shardings_known, _)->jax._src.interpreters.partial_eval.partition_list(unks_in, params_known['in_shardings'])
A:jax._src.pjit.(_, out_shardings_known)->jax._src.interpreters.partial_eval.partition_list(kept_outs_known, params_known['out_shardings'])
A:jax._src.pjit.new_params_known->dict(params_known, in_shardings=tuple(in_shardings_known), out_shardings=(*out_shardings_known, *[UNSPECIFIED] * num_res_out), donated_invars=tuple(donated_invars_known))
A:jax._src.pjit.(_, donated_invars_staged)->jax._src.interpreters.partial_eval.partition_list(inst_in, params_staged['donated_invars'])
A:jax._src.pjit.(_, in_shardings_staged)->jax._src.interpreters.partial_eval.partition_list(inst_in, params_staged['in_shardings'])
A:jax._src.pjit.(_, out_shardings_staged)->jax._src.interpreters.partial_eval.partition_list(kept_outs_staged, params_staged['out_shardings'])
A:jax._src.pjit.new_params_staged->dict(params_staged, in_shardings=tuple(in_shardings_staged), out_shardings=tuple(out_shardings_staged), donated_invars=tuple(donated_invars_staged))
A:jax._src.pjit.pe.partial_eval_jaxpr_custom_rules[pjit_p]->partial(pe.closed_call_partial_eval_custom_rule, 'jaxpr', _pjit_partial_eval_custom_params_updater)
A:jax._src.pjit.(transpose_jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_avals)
A:jax._src.pjit.transpose_jaxpr->_pjit_transpose_trace(body, global_cts_in_avals)
A:jax._src.pjit.body->jax._src.linear_util.hashable_partial(body, jaxpr, reduce_axes, False)
A:jax._src.pjit.(primals_and_nz_cts_in, in_treedef)->tree_flatten((primals_in, cts_in))
A:jax._src.pjit.(body, cts_out_treedef_thunk)->flatten_fun_nokwargs(body, in_treedef)
A:jax._src.pjit.global_cts_in_avals->tuple((core.raise_to_shaped(core.get_aval(ct)) for ct in primals_and_nz_cts_in))
A:jax._src.pjit.cts_out_treedef->cts_out_treedef_thunk()
A:jax._src.pjit.transpose_out_shardings->prune_type(ad.Zero, in_shardings, tree_unflatten(cts_out_treedef, [object()] * cts_out_treedef.num_leaves))
A:jax._src.pjit.nz_cts_out->jax._src.core.AxisPrimitive('pjit').bind(*primals_and_nz_cts_in, jaxpr=transpose_jaxpr, in_shardings=transpose_in_shardings, out_shardings=transpose_out_shardings, resource_env=resource_env, donated_invars=(False,) * len(primals_and_nz_cts_in), name=name, keep_unused=keep_unused, inline=inline)
A:jax._src.pjit.(new_jaxpr, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr.jaxpr, used_outputs)
A:jax._src.pjit.(dced_jaxpr, used_inputs)->_dce_jaxpr_pjit(eqn.params['jaxpr'], tuple(used_outputs))
A:jax._src.pjit.new_params->dict(eqn_params, jaxpr=dced_jaxpr, in_shardings=keep_where(eqn_params['in_shardings'], used_inputs), out_shardings=keep_where(eqn_params['out_shardings'], used_outputs), donated_invars=keep_where(eqn_params['donated_invars'], used_inputs))
A:jax._src.pjit.new_eqn->jax._src.core.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, dced_jaxpr.effects, eqn.source_info)
A:jax._src.pjit.pjit_resources->set(it.chain.from_iterable([d for d in pos_axis_resources if d is not None]))
A:jax._src.pjit.aval_resources->set(it.chain.from_iterable((named_axis_resources[a] for a in aval.named_shape)))
A:jax._src.pjit.num_outs->len(jaxpr.outvars)
A:jax._src.pjit.(discharged_jaxpr, discharged_consts)->jax._src.state.discharge.discharge_state(jaxpr, consts)
A:jax._src.pjit.discharged_closed_jaxpr->jax._src.core.ClosedJaxpr(discharged_jaxpr, discharged_consts)
A:jax._src.pjit.out_and_ref_vals->jax._src.core.AxisPrimitive('pjit').bind(*args, jaxpr=discharged_closed_jaxpr, in_shardings=new_in_shardings, out_shardings=new_out_shardings, **params)
A:jax._src.pjit.(out_vals, ref_vals)->split_list(out_and_ref_vals, [num_outs])
A:jax._src.pjit.ref_vals_iter->iter(ref_vals)
A:jax._src.pjit.new_invals->tuple((next(ref_vals_iter) if isinstance(aval, state_discharge.AbstractRef) else None for aval in in_avals))
A:jax._src.pjit.sentinel->object()
A:jax._src.pjit.(x_flat, tree)->tree_flatten(x)
A:jax._src.pjit.(user_shardings, _, _)->prepare_axis_resources(shardings, 'shardings', allow_unconstrained_dims=True)
A:jax._src.pjit.user_shardings_flat->tuple(flatten_axes('with_sharding_constraint shardings', tree, user_shardings))
A:jax._src.pjit.sharding_constraint_p->jax._src.core.Primitive('sharding_constraint')
A:jax._src.pjit.sharding->GSPMDSharding(mps._device_assignment, mps._to_xla_hlo_sharding(aval.ndim, axis_ctx=axis_ctx))
A:jax._src.pjit.y->jax._src.core.Primitive('sharding_constraint').bind(x, sharding=_pjit_batcher_for_sharding(sharding, d, new_parts, resource_env.physical_mesh, x.ndim), resource_env=resource_env, unconstrained_dims=unconstrained_dims)
A:jax._src.pjit.batching.spmd_axis_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, False)
A:jax._src.pjit.batching.axis_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, False, None)
A:jax._src.pjit.pxla.spmd_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, True, None)
A:jax._src.pjit.gs->GSPMDSharding(s._device_assignment, s._to_xla_hlo_sharding(ndim), memory_kind=s.memory_kind)
A:jax._src.pjit.parameter_shardings_from_xla->_read_most_recent_pjit_call_executable(jaxpr).get_parameter_shardings()
A:jax._src.pjit.output_shardings_from_xla->_read_most_recent_pjit_call_executable(jaxpr).get_output_shardings()
A:jax._src.pjit.(input_op_shardings, output_op_sharding)->get_op_sharding_from_executable(executable)
A:jax._src.pjit.(in_ppspec, out_ppspec)->_get_ppspec_from_executable(executable, mesh)
A:jax._src.pjit.out_partition_spec->_get_partition_spec(out_ppspec)
A:jax._src.pjit.in_partition_spec->_get_partition_spec(in_ppspec)
jax._src.pjit.PjitInfo(NamedTuple)
jax._src.pjit.PytreeLeaf
jax._src.pjit.PytreeLeaf.__repr__(self)
jax._src.pjit.SameDeviceAssignmentTuple
jax._src.pjit.SameDeviceAssignmentTuple.__eq__(self,other)
jax._src.pjit.SameDeviceAssignmentTuple.__hash__(self)
jax._src.pjit._MostRecentPjitCallExecutable(self)
jax._src.pjit._MostRecentPjitCallExecutable.__init__(self)
jax._src.pjit._check_and_canonicalize_out_shardings(out_shardings_thunk,out_tree,out_type,debug_info,device_or_backend_set)
jax._src.pjit._check_resources_against_named_axes(what,aval,pos_axis_resources,named_axis_resources)
jax._src.pjit._cpp_pjit(fun:Callable,infer_params_fn,static_argnums,static_argnames,donate_argnums,pjit_has_explicit_sharding)
jax._src.pjit._cpp_pjit_evict_fn(self)
jax._src.pjit._create_pjit_jaxpr(fun,in_type,debug_info,out_paths)
jax._src.pjit._create_sharding_for_array(mesh,x,name,api_name)
jax._src.pjit._create_sharding_with_device_backend(device,backend)
jax._src.pjit._dce_jaxpr_pjit(jaxpr:core.ClosedJaxpr,used_outputs:tuple[bool])->tuple[core.ClosedJaxpr, list[bool]]
jax._src.pjit._device_assignment_mismatch_error(fun_name,fails,args_flat,api_name,arg_names)
jax._src.pjit._extract_implicit_args(in_type:Sequence[tuple[core.AbstractValue,bool]],explicit_args:Sequence[Any])->Sequence[core.Tracer]
jax._src.pjit._fast_path_get_device_assignment(shardings:Iterable[PjitSharding])->Optional[XLADeviceAssignment]
jax._src.pjit._find_arg_mismatch(arg_list,fails,fun_name)
jax._src.pjit._flat_axes_specs(abstracted_axes,*args,**kwargs)->Optional[list[pe.AbstractedAxesSpec]]
jax._src.pjit._get_arg_names(fun,in_tree,args_flat)
jax._src.pjit._get_cpp_global_cache(pjit_has_explicit_sharding)
jax._src.pjit._get_fastpath_data(executable,out_tree,args_flat,out_flat)
jax._src.pjit._get_jaxpr_as_fun(jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._get_partition_spec(ppspec:Sequence[ParsedPartitionSpec])->Sequence[PartitionSpec]
jax._src.pjit._get_ppspec_from_executable(executable,mesh)->tuple[Sequence[ParsedPartitionSpec], Sequence[ParsedPartitionSpec]]
jax._src.pjit._identity_fn(x)
jax._src.pjit._known_jaxpr_fwd(known_jaxpr:core.ClosedJaxpr,in_fwd:tuple[Optional[int]])->core.ClosedJaxpr
jax._src.pjit._out_type(jaxpr:core.ClosedJaxpr)->list[core.AbstractValue]
jax._src.pjit._pjit_abstract_eval(*args,jaxpr,out_shardings,resource_env,**_)
jax._src.pjit._pjit_batcher(insert_axis,spmd_axis_name,axis_size,axis_name,main_type,vals_in,dims_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_batcher_for_sharding(s:Union[GSPMDSharding,UnspecifiedValue],dim:int,val:tuple[str,...],mesh,ndim:int)
jax._src.pjit._pjit_cached_lower_jaxpr_to_fun(ctx,name,jaxpr,effects,in_shardings,out_shardings,api_name)
jax._src.pjit._pjit_call_impl(*args,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_call_impl_python(*args,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_explicit_sharding(in_shardings,out_shardings,device,backend)->bool
jax._src.pjit._pjit_jaxpr(fun,out_shardings_thunk,in_type,debug_info,device_or_backend_set,out_tree,result_paths)
jax._src.pjit._pjit_jvp(primals_in,tangents_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_lower(jaxpr:core.ClosedJaxpr,in_shardings,out_shardings,*args,**kwargs)
jax._src.pjit._pjit_lower_cached(jaxpr:core.ClosedJaxpr,sdat_in_shardings:SameDeviceAssignmentTuple,sdat_out_shardings:SameDeviceAssignmentTuple,resource_env,donated_invars,name:str,keep_unused:bool,inline:bool,*,lowering_parameters:mlir.LoweringParameters)
jax._src.pjit._pjit_lowering(ctx,*args,name,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,keep_unused,inline)
jax._src.pjit._pjit_partial_eval(trace,*in_tracers,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_partial_eval_custom_params_updater(unks_in:Sequence[bool],inst_in:Sequence[bool],kept_outs_known:Sequence[bool],kept_outs_staged:Sequence[bool],num_res_out:int,num_res_in:int,params_known:dict,params_staged:dict)->tuple[dict, dict]
jax._src.pjit._pjit_pp_rule(eqn,context,settings)
jax._src.pjit._pjit_state_discharge_rule(in_avals,out_avals,*args,jaxpr,in_shardings,out_shardings,**params)
jax._src.pjit._pjit_transpose(reduce_axes,cts_in,*primals_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,keep_unused,inline)
jax._src.pjit._pjit_transpose_trace(fun,in_avals)
jax._src.pjit._pjit_typecheck(ctx_factory,*in_atoms,jaxpr,**params)
jax._src.pjit._process_in_axis_resources(in_shardings_thunk,in_avals,in_tree,resource_env,debug_info,device_or_backend_set)
jax._src.pjit._python_pjit(fun:Callable,infer_params_fn)
jax._src.pjit._python_pjit_helper(fun,infer_params_fn,*args,**kwargs)
jax._src.pjit._read_most_recent_pjit_call_executable(jaxpr)
jax._src.pjit._resolve_in_shardings(args,pjit_in_shardings:Sequence[PjitSharding],out_shardings:Sequence[PjitSharding],pjit_mesh:Optional[pxla.Mesh])->Sequence[PjitSharding]
jax._src.pjit._resource_typing_pjit(avals,params,source_info,resource_env,named_axis_resources)
jax._src.pjit._resource_typing_sharding_constraint(avals,params,source_info,resource_env,named_axis_resources)
jax._src.pjit._sharding_constraint_batcher(insert_axis,spmd_axis_name,axis_size,axis_name,main_type,vals_in,dims_in,sharding,resource_env,unconstrained_dims)
jax._src.pjit._sharding_constraint_hlo_lowering(ctx,x_node,*,sharding,resource_env,unconstrained_dims)
jax._src.pjit._sharding_constraint_impl(x,sharding,resource_env,unconstrained_dims)
jax._src.pjit._try_infer_args(f,tree)
jax._src.pjit.common_infer_params(pjit_info_args,*args,**kwargs)
jax._src.pjit.dce_jaxpr_pjit_rule(used_outputs:list[bool],eqn:core.JaxprEqn)->tuple[list[bool], Optional[core.JaxprEqn]]
jax._src.pjit.flatten_axis_resources(what,tree,shardings,tupled_args)
jax._src.pjit.get_op_sharding_from_executable(executable)->tuple[Sequence[xc.OpSharding], Sequence[xc.OpSharding]]
jax._src.pjit.get_pspec_from_executable(executable,mesh:pxla.Mesh)->tuple[tuple[PartitionSpec, ...], tuple[PartitionSpec, ...]]
jax._src.pjit.get_unconstrained_dims(sharding:NamedSharding)
jax._src.pjit.hashable_pytree(pytree)
jax._src.pjit.pjit(fun:Callable,in_shardings=UNSPECIFIED,out_shardings=UNSPECIFIED,static_argnums:Union[int,Sequence[int],None]=None,static_argnames:Union[str,Iterable[str],None]=None,donate_argnums:Union[int,Sequence[int],None]=None,donate_argnames:Union[str,Iterable[str],None]=None,keep_unused:bool=False,device:Optional[xc.Device]=None,backend:Optional[str]=None,inline:bool=False,abstracted_axes:Optional[Any]=None)->stages.Wrapped
jax._src.pjit.pjit_check_aval_sharding(shardings,flat_avals,names:Optional[tuple[str,...]],what_aval:str,allow_uneven_sharding:bool)
jax._src.pjit.pjit_staging_rule(trace,*args,**params)
jax._src.pjit.post_infer_params(fun,infer_params_fn,static_argnums,static_argnames,donate_argnums,abstracted_axes,pjit_has_explicit_sharding)
jax._src.pjit.pre_infer_params(fun,in_shardings,out_shardings,donate_argnums,donate_argnames,static_argnums,static_argnames,device,backend,abstracted_axes)
jax._src.pjit.to_gspmd_sharding(s:XLACompatibleSharding,ndim:int,device_or_backend_set:bool=False)->GSPMDSharding
jax._src.pjit.with_sharding_constraint(x,shardings)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/basearray.py----------------------------------------
jax.Array(abc.ABC)
jax.Array.at(self)
jax._src.basearray.Array(abc.ABC)
jax._src.basearray.Array.at(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/basearray.pyi----------------------------------------
jax.Array.T(self)->Array
jax.Array.__abs__(self)->Array
jax.Array.__add__(self,other)->Array
jax.Array.__and__(self,other)->Array
jax.Array.__array__(self)->np.ndarray
jax.Array.__bool__(self)->bool
jax.Array.__complex__(self)->complex
jax.Array.__divmod__(self,other)->Array
jax.Array.__eq__(self,other)->Array
jax.Array.__float__(self)->float
jax.Array.__floordiv__(self,other)->Array
jax.Array.__ge__(self,other)->Array
jax.Array.__getitem__(self,key)->Array
jax.Array.__gt__(self,other)->Array
jax.Array.__index__(self)->int
jax.Array.__int__(self)->int
jax.Array.__invert__(self)->Array
jax.Array.__iter__(self)->Any
jax.Array.__le__(self,other)->Array
jax.Array.__len__(self)->int
jax.Array.__lshift__(self,other)->Array
jax.Array.__lt__(self,other)->Array
jax.Array.__matmul__(self,other)->Array
jax.Array.__mod__(self,other)->Array
jax.Array.__mul__(self,other)->Array
jax.Array.__ne__(self,other)->Array
jax.Array.__neg__(self)->Array
jax.Array.__or__(self,other)->Array
jax.Array.__pos__(self)->Array
jax.Array.__pow__(self,other)->Array
jax.Array.__radd__(self,other)->Array
jax.Array.__rand__(self,other)->Array
jax.Array.__rdivmod__(self,other)->Array
jax.Array.__reversed__(self)->Any
jax.Array.__rfloordiv__(self,other)->Array
jax.Array.__rlshift__(self,other)->Array
jax.Array.__rmatmul__(self,other)->Array
jax.Array.__rmod__(self,other)->Array
jax.Array.__rmul__(self,other)->Array
jax.Array.__ror__(self,other)->Array
jax.Array.__round__(self,ndigits=None)->Array
jax.Array.__rpow__(self,other)->Array
jax.Array.__rrshift__(self,other)->Array
jax.Array.__rshift__(self,other)->Array
jax.Array.__rsub__(self,other)->Array
jax.Array.__rtruediv__(self,other)->Array
jax.Array.__rxor__(self,other)->Array
jax.Array.__setitem__(self,key,value)->None
jax.Array.__sub__(self,other)->Array
jax.Array.__truediv__(self,other)->Array
jax.Array.__xor__(self,other)->Array
jax.Array.addressable_data(self,index:int)->Array
jax.Array.addressable_shards(self)->Sequence[Shard]
jax.Array.all(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None)->Array
jax.Array.any(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None)->Array
jax.Array.argmax(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax.Array.argmin(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax.Array.argpartition(self,kth,axis=-1,kind='introselect',order=None)->Array
jax.Array.argsort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax.Array.astype(self,dtype)->Array
jax.Array.block_until_ready(self)->Array
jax.Array.choose(self,choices,out=None,mode='raise')->Array
jax.Array.clip(self,min=None,max=None,out=None)->Array
jax.Array.compress(self,condition,axis:Optional[int]=None,out=None)->Array
jax.Array.conj(self)->Array
jax.Array.conjugate(self)->Array
jax.Array.copy(self)->Array
jax.Array.copy_to_host_async(self)->None
jax.Array.cumprod(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None)->Array
jax.Array.cumsum(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None)->Array
jax.Array.delete(self)->None
jax.Array.device(self)->Device
jax.Array.device_buffers(self)->Any
jax.Array.devices(self)->set[Device]
jax.Array.diagonal(self,offset=0,axis1:int=0,axis2:int=1)->Array
jax.Array.dot(self,b,*,precision=None)->Array
jax.Array.dtype(self)->np.dtype
jax.Array.flatten(self)->Array
jax.Array.global_shards(self)->Sequence[Shard]
jax.Array.imag(self)->Array
jax.Array.is_deleted(self)->bool
jax.Array.is_fully_addressable(self)->bool
jax.Array.is_fully_replicated(self)->bool
jax.Array.item(self,*args)->Any
jax.Array.itemsize(self)->int
jax.Array.mT(self)->Array
jax.Array.max(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.mean(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=False,*,where=None)->Array
jax.Array.min(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.nbytes(self)->int
jax.Array.ndim(self)->int
jax.Array.nonzero(self,*,size=None,fill_value=None)->Array
jax.Array.on_device_size_in_bytes(self)->int
jax.Array.prod(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.ptp(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=False)->Array
jax.Array.ravel(self,order='C')->Array
jax.Array.real(self)->Array
jax.Array.repeat(self,repeats,axis:Optional[int]=None,*,total_repeat_length=None)->Array
jax.Array.reshape(self,*args,order='C')->Array
jax.Array.round(self,decimals=0,out=None)->Array
jax.Array.searchsorted(self,v,side='left',sorter=None)->Array
jax.Array.shape(self)->tuple[int, ...]
jax.Array.size(self)->int
jax.Array.sort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax.Array.squeeze(self,axis:Optional[Union[int,Sequence[int]]]=None)->Array
jax.Array.std(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax.Array.sum(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.swapaxes(self,axis1:int,axis2:int)->Array
jax.Array.take(self,indices,axis:Optional[int]=None,out=None,mode=None)->Array
jax.Array.tobytes(self,order='C')->bytes
jax.Array.tolist(self)->list[Any]
jax.Array.trace(self,offset=0,axis1:int=0,axis2:int=1,dtype=None,out=None)->Array
jax.Array.traceback(self)->Traceback
jax.Array.transpose(self,*args)->Array
jax.Array.unsafe_buffer_pointer(self)->int
jax.Array.var(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax.Array.view(self,dtype=None,type=None)->Array
jax.Array.weak_type(self)->bool
jax._src.basearray.Array.T(self)->Array
jax._src.basearray.Array.__abs__(self)->Array
jax._src.basearray.Array.__add__(self,other)->Array
jax._src.basearray.Array.__and__(self,other)->Array
jax._src.basearray.Array.__array__(self)->np.ndarray
jax._src.basearray.Array.__bool__(self)->bool
jax._src.basearray.Array.__complex__(self)->complex
jax._src.basearray.Array.__divmod__(self,other)->Array
jax._src.basearray.Array.__dlpack__(self)->Any
jax._src.basearray.Array.__eq__(self,other)->Array
jax._src.basearray.Array.__float__(self)->float
jax._src.basearray.Array.__floordiv__(self,other)->Array
jax._src.basearray.Array.__ge__(self,other)->Array
jax._src.basearray.Array.__getitem__(self,key)->Array
jax._src.basearray.Array.__gt__(self,other)->Array
jax._src.basearray.Array.__index__(self)->int
jax._src.basearray.Array.__init__(self,shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax._src.basearray.Array.__int__(self)->int
jax._src.basearray.Array.__invert__(self)->Array
jax._src.basearray.Array.__iter__(self)->Any
jax._src.basearray.Array.__le__(self,other)->Array
jax._src.basearray.Array.__len__(self)->int
jax._src.basearray.Array.__lshift__(self,other)->Array
jax._src.basearray.Array.__lt__(self,other)->Array
jax._src.basearray.Array.__matmul__(self,other)->Array
jax._src.basearray.Array.__mod__(self,other)->Array
jax._src.basearray.Array.__mul__(self,other)->Array
jax._src.basearray.Array.__ne__(self,other)->Array
jax._src.basearray.Array.__neg__(self)->Array
jax._src.basearray.Array.__or__(self,other)->Array
jax._src.basearray.Array.__pos__(self)->Array
jax._src.basearray.Array.__pow__(self,other)->Array
jax._src.basearray.Array.__radd__(self,other)->Array
jax._src.basearray.Array.__rand__(self,other)->Array
jax._src.basearray.Array.__rdivmod__(self,other)->Array
jax._src.basearray.Array.__reversed__(self)->Any
jax._src.basearray.Array.__rfloordiv__(self,other)->Array
jax._src.basearray.Array.__rlshift__(self,other)->Array
jax._src.basearray.Array.__rmatmul__(self,other)->Array
jax._src.basearray.Array.__rmod__(self,other)->Array
jax._src.basearray.Array.__rmul__(self,other)->Array
jax._src.basearray.Array.__ror__(self,other)->Array
jax._src.basearray.Array.__round__(self,ndigits=None)->Array
jax._src.basearray.Array.__rpow__(self,other)->Array
jax._src.basearray.Array.__rrshift__(self,other)->Array
jax._src.basearray.Array.__rshift__(self,other)->Array
jax._src.basearray.Array.__rsub__(self,other)->Array
jax._src.basearray.Array.__rtruediv__(self,other)->Array
jax._src.basearray.Array.__rxor__(self,other)->Array
jax._src.basearray.Array.__setitem__(self,key,value)->None
jax._src.basearray.Array.__sub__(self,other)->Array
jax._src.basearray.Array.__truediv__(self,other)->Array
jax._src.basearray.Array.__xor__(self,other)->Array
jax._src.basearray.Array.addressable_data(self,index:int)->Array
jax._src.basearray.Array.addressable_shards(self)->Sequence[Shard]
jax._src.basearray.Array.all(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.any(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argmax(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argmin(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argpartition(self,kth,axis=-1,kind='introselect',order=None)->Array
jax._src.basearray.Array.argsort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax._src.basearray.Array.astype(self,dtype)->Array
jax._src.basearray.Array.block_until_ready(self)->Array
jax._src.basearray.Array.choose(self,choices,out=None,mode='raise')->Array
jax._src.basearray.Array.clip(self,min=None,max=None,out=None)->Array
jax._src.basearray.Array.compress(self,condition,axis:Optional[int]=None,out=None)->Array
jax._src.basearray.Array.conj(self)->Array
jax._src.basearray.Array.conjugate(self)->Array
jax._src.basearray.Array.copy(self)->Array
jax._src.basearray.Array.copy_to_host_async(self)->None
jax._src.basearray.Array.cumprod(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None)->Array
jax._src.basearray.Array.cumsum(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None)->Array
jax._src.basearray.Array.delete(self)->None
jax._src.basearray.Array.device(self)->Device
jax._src.basearray.Array.device_buffers(self)->Any
jax._src.basearray.Array.devices(self)->set[Device]
jax._src.basearray.Array.diagonal(self,offset=0,axis1:int=0,axis2:int=1)->Array
jax._src.basearray.Array.dot(self,b,*,precision=None)->Array
jax._src.basearray.Array.dtype(self)->np.dtype
jax._src.basearray.Array.flatten(self)->Array
jax._src.basearray.Array.global_shards(self)->Sequence[Shard]
jax._src.basearray.Array.imag(self)->Array
jax._src.basearray.Array.is_deleted(self)->bool
jax._src.basearray.Array.is_fully_addressable(self)->bool
jax._src.basearray.Array.is_fully_replicated(self)->bool
jax._src.basearray.Array.item(self,*args)->Any
jax._src.basearray.Array.itemsize(self)->int
jax._src.basearray.Array.mT(self)->Array
jax._src.basearray.Array.max(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.mean(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.min(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.nbytes(self)->int
jax._src.basearray.Array.ndim(self)->int
jax._src.basearray.Array.nonzero(self,*,size=None,fill_value=None)->Array
jax._src.basearray.Array.on_device_size_in_bytes(self)->int
jax._src.basearray.Array.prod(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.ptp(self,axis:Optional[Union[int,Sequence[int]]]=None,out=None,keepdims=False)->Array
jax._src.basearray.Array.ravel(self,order='C')->Array
jax._src.basearray.Array.real(self)->Array
jax._src.basearray.Array.repeat(self,repeats,axis:Optional[int]=None,*,total_repeat_length=None)->Array
jax._src.basearray.Array.reshape(self,*args,order='C')->Array
jax._src.basearray.Array.round(self,decimals=0,out=None)->Array
jax._src.basearray.Array.searchsorted(self,v,side='left',sorter=None)->Array
jax._src.basearray.Array.shape(self)->tuple[int, ...]
jax._src.basearray.Array.sharding(self)->Sharding
jax._src.basearray.Array.size(self)->int
jax._src.basearray.Array.sort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax._src.basearray.Array.squeeze(self,axis:Optional[Union[int,Sequence[int]]]=None)->Array
jax._src.basearray.Array.std(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.sum(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.swapaxes(self,axis1:int,axis2:int)->Array
jax._src.basearray.Array.take(self,indices,axis:Optional[int]=None,out=None,mode=None)->Array
jax._src.basearray.Array.tobytes(self,order='C')->bytes
jax._src.basearray.Array.tolist(self)->list[Any]
jax._src.basearray.Array.trace(self,offset=0,axis1:int=0,axis2:int=1,dtype=None,out=None)->Array
jax._src.basearray.Array.traceback(self)->Traceback
jax._src.basearray.Array.transpose(self,*args)->Array
jax._src.basearray.Array.unsafe_buffer_pointer(self)->int
jax._src.basearray.Array.var(self,axis:Optional[Union[int,Sequence[int]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.view(self,dtype=None,type=None)->Array
jax._src.basearray.Array.weak_type(self)->bool
jax._src.basearray._IndexUpdateHelper
jax._src.basearray._IndexUpdateHelper.__getitem__(self,index:Any)->_IndexUpdateRef
jax._src.basearray._IndexUpdateRef
jax._src.basearray._IndexUpdateRef.add(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.apply(self,func:Callable[[ArrayLike],ArrayLike],indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.divide(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.get(self,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None,fill_value:Optional[ArrayLike]=None)->Array
jax._src.basearray._IndexUpdateRef.max(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.min(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.mul(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.multiply(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.power(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None)->Array
jax._src.basearray._IndexUpdateRef.set(self,values:Any,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[str]=None,fill_value:Optional[ArrayLike]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/api_util.py----------------------------------------
A:jax._src.api_util.x->jax._src.core.concrete_or_error(None, x, 'expected a static index or sequence of indices.')
A:jax._src.api_util.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax._src.api_util.(args, in_tree)->tree_flatten(py_args)
A:jax._src.api_util.ans->fun(*args)
A:jax._src.api_util.py_args->tree_unflatten(in_tree, args_flat)
A:jax._src.api_util.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax._src.api_util.(aux_flat, aux_tree)->tree_flatten(aux)
A:jax._src.api_util.dyn_argnums->tuple((i for i in range(len(args)) if i not in static_argnums))
A:jax._src.api_util.dyn_args->tuple((args[i] for i in dyn_argnums))
A:jax._src.api_util.static_argnums->sorted(set(static_argnums))
A:jax._src.api_util.sentinel->object()
A:jax._src.api_util.fixed_args_->iter(fixed_args)
A:jax._src.api_util.fixed_kwargs[k]->Hashable(arg)
A:jax._src.api_util.kwargs->dict({k: v.val for (k, v) in fixed_kwargs.val.items()}, **dyn_kwargs)
A:jax._src.api_util.donate->bool(i in donate_argnums)
A:jax._src.api_util.donate_argnums->rebase_donate_argnums(donate_argnums, static_argnums)
A:jax._src.api_util.proxy->object()
A:jax._src.api_util.dummy->tree_unflatten(treedef, [object()] * treedef.num_leaves)
A:jax._src.api_util.(treedef, _)->treedef_children(treedef)
A:jax._src.api_util.(leaves, treedef)->tree_flatten(out_spec)
A:jax._src.api_util.(f, out_axes)->_flat_out_axes(f, tuple(leaves), treedef)
A:jax._src.api_util.spec->tree_unflatten(treedef, leaves)
A:jax._src.api_util.spec_flat->tuple(broadcast_prefix(spec, ans, is_leaf=lambda x: x is None))
A:jax._src.api_util.(e, *_)->prefix_errors(spec, ans)
A:jax._src.api_util.argnums->_ensure_index_tuple(argnums)
A:jax._src.api_util.argnames->tuple((k for (i, (k, param)) in enumerate(parameters.items()) if param.kind == _POSITIONAL_OR_KEYWORD and i in argnums))
A:jax._src.api_util.sig->inspect.signature(fun)
A:jax._src.api_util.(static_argnums, static_argnames)->infer_argnums_and_argnames(sig, static_argnums, static_argnames)
A:jax._src.api_util.(donate_argnums, donate_argnames)->infer_argnums_and_argnames(sig, donate_argnums, donate_argnames)
A:jax._src.api_util.out->set(static_argnames).intersection(set(donate_argnames))
A:jax._src.api_util.weak_type->getattr(x, 'weak_type', False)
A:jax._src.api_util.named_shape->getattr(x, 'named_shape', {})
A:jax._src.api_util.dtype->numpy.dtype(x)
A:jax._src.api_util.src->fun_sourceinfo(fun)
A:jax._src.api_util.arg_names->_arg_names(fun, args, kwargs, static_argnums, static_argnames)
A:jax._src.api_util.fun->inspect.unwrap(fun)
A:jax._src.api_util.static->object()
A:jax._src.api_util.static_argnums_->_ensure_inbounds(True, len(args), static_argnums)
A:jax._src.api_util.static_argnames_->set(static_argnames)
A:jax._src.api_util.ba->inspect.signature(fn).bind(*args_, **kwargs)
A:jax._src.api_util.result_paths->trace_debug.result_paths()
A:jax._src.api_util.debug_info->jax._src.core.JaxprDebugInfo(trace_debug.traced_for, trace_debug.func_src_info, trace_debug.arg_names, tuple(result_paths))
A:jax._src.api_util.res_paths_->HashableFunction(res_paths, closure=())
jax._src.api_util._HashableWithStrictTypeEquality(self,val)
jax._src.api_util._HashableWithStrictTypeEquality.__eq__(self,other)
jax._src.api_util._HashableWithStrictTypeEquality.__hash__(self)
jax._src.api_util._HashableWithStrictTypeEquality.__init__(self,val)
jax._src.api_util._arg_names(fn,args,kwargs,static_argnums,static_argnames)->Optional[tuple[str, ...]]
jax._src.api_util._argnames_partial(fixed_kwargs:WrapKwArgs,*args,**dyn_kwargs)
jax._src.api_util._argnums_partial(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax._src.api_util._dtype(x)
jax._src.api_util._ensure_inbounds(allow_invalid:bool,num_args:int,argnums:Sequence[int])->tuple[int, ...]
jax._src.api_util._ensure_index(x:Any)->Union[int, tuple[int, ...]]
jax._src.api_util._ensure_index_tuple(x:Any)->tuple[int, ...]
jax._src.api_util._ensure_str(x:str)->str
jax._src.api_util._ensure_str_tuple(x:Union[str,Iterable[str]])->tuple[str, ...]
jax._src.api_util._flat_out_axes(leaves,treedef,*args,**kwargs)
jax._src.api_util._np_scalar_abstractify(x:np.generic)->ShapedArray
jax._src.api_util._numpy_array_abstractify(x:np.ndarray)->ShapedArray
jax._src.api_util._shaped_abstractify_slow(x)
jax._src.api_util._str_abstractify(x)
jax._src.api_util.api_hook(fun,tag:str)
jax._src.api_util.apply_flat_fun(fun,io_tree,*py_args)
jax._src.api_util.apply_flat_fun_nokwargs(fun,io_tree,py_args)
jax._src.api_util.argnames_partial_except(f:lu.WrappedFun,static_argnames:tuple[str,...],kwargs:dict[str,Any])
jax._src.api_util.argnums_partial(f,dyn_argnums,args,require_static_args_hashable=True)
jax._src.api_util.argnums_partial_except(f:lu.WrappedFun,static_argnums:tuple[int,...],args:tuple[Any,...],*,allow_invalid:bool)
jax._src.api_util.assert_no_intersection(static_argnames,donate_argnames)
jax._src.api_util.check_callable(fun)
jax._src.api_util.debug_info(traced_for:str,fun:Callable,args:tuple[Any],kwargs:dict[str,Any],static_argnums:tuple[int,...],static_argnames:tuple[str,...])->Optional[TracingDebugInfo]
jax._src.api_util.debug_info_final(f:lu.WrappedFun,dbg:Optional[TracingDebugInfo],res_paths:Callable[[],tuple[str,...]])->lu.WrappedFun
jax._src.api_util.donation_vector(donate_argnums,donate_argnames,args,kwargs)->tuple[bool, ...]
jax._src.api_util.flat_out_axes(f:lu.WrappedFun,out_spec:Any)->tuple[lu.WrappedFun, Callable]
jax._src.api_util.flatten_axes(name,treedef,axis_tree,*,kws=False,tupled_args=False)
jax._src.api_util.flatten_fun(in_tree,*args_flat)
jax._src.api_util.flatten_fun_nokwargs(in_tree,*args_flat)
jax._src.api_util.flatten_fun_nokwargs2(in_tree,*args_flat)
jax._src.api_util.flattened_fun_in_tree(fn:lu.WrappedFun)->Optional[tuple[PyTreeDef, Callable[[], PyTreeDef], bool]]
jax._src.api_util.fun_sourceinfo(fun:Callable)->Optional[str]
jax._src.api_util.infer_argnums_and_argnames(sig:inspect.Signature,argnums:Union[int,Iterable[int],None],argnames:Union[str,Iterable[str],None])->tuple[tuple[int, ...], tuple[str, ...]]
jax._src.api_util.is_hashable(arg)
jax._src.api_util.jaxpr_debug_info(jaxpr:core.Jaxpr,trace_debug:Optional[TracingDebugInfo],result_paths:Optional[tuple[Optional[str],...]]=None)->core.Jaxpr
jax._src.api_util.rebase_donate_argnums(donate_argnums,static_argnums)->tuple[int, ...]
jax._src.api_util.resolve_argnums(fun,donate_argnums,donate_argnames,static_argnums,static_argnames)->tuple[tuple[int, ...], tuple[str, ...], tuple[int, ...], tuple[str, ...]]
jax._src.api_util.result_paths(*args,**kwargs)
jax._src.api_util.shaped_abstractify(x)
jax._src.api_util.validate_argnames(sig:inspect.Signature,argnames:tuple[str,...],argnames_name:str)->None
jax._src.api_util.validate_argnums(sig:inspect.Signature,argnums:tuple[int,...],argnums_name:str)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pickle_util.py----------------------------------------
A:jax._src.pickle_util.dispatch_table->cloudpickle.CloudPickler.dispatch_table.copy()
jax._src.pickle_util.dumps(obj:Any)->bytes
jax._src.pickle_util.loads(data:bytes)->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/environment_info.py----------------------------------------
A:jax._src.environment_info.python_version->sys.version.replace('\n', ' ')
A:jax._src.environment_info.devices_short->str(np.array(xla_bridge.devices())).replace('\n', '')
A:jax._src.environment_info.info->textwrap.dedent(f'  jax:    {version.__version__}\n  jaxlib: {lib.version_str}\n  numpy:  {np.__version__}\n  python: {python_version}\n  jax.devices ({xla_bridge.device_count()} total, {xla_bridge.local_device_count()} local): {devices_short}\n  process_count: {xla_bridge.process_count()}')
A:jax._src.environment_info.nvidia_smi->try_nvidia_smi()
jax._src.environment_info.print_environment_info(return_string:bool=False)->Union[None, str]
jax._src.environment_info.try_nvidia_smi()->Optional[str]
jax.print_environment_info(return_string:bool=False)->Union[None, str]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/sharding_specs.py----------------------------------------
A:jax._src.sharding_specs._UNSHARDED_INSTANCE->NoSharding()
A:jax._src.sharding_specs.mesh_shape->cast(tuple[int, ...], self.mesh_shape)
A:jax._src.sharding_specs.ty->special_axes_dict.get(axis, xc.OpSharding.Type.REPLICATED)
A:jax._src.sharding_specs.has_unstacked->any((isinstance(s, Unstacked) for s in self.sharding))
A:jax._src.sharding_specs.hlo_sharding->sharding_spec_sharding_proto(self)
A:jax._src.sharding_specs.total_chunks->math.prod(sharding.chunks)
A:jax._src.sharding_specs.(shard_size, ragged)->divmod(axis_size, total_chunks)
A:jax._src.sharding_specs.shard_indices->shard_indices.reshape(shard_indices_shape).reshape(shard_indices_shape)
A:jax._src.sharding_specs.num_sharded_dim->len(shard_indices_shape)
A:jax._src.sharding_specs.replica_sizes->tuple((a.replicas for a in self.mesh_mapping if isinstance(a, Replicated)))
A:jax._src.sharding_specs.ShardingSpec.mesh_shape->property(_sharding_spec_mesh_shape)
A:jax._src.sharding_specs.chunked->Chunked([])
A:jax._src.sharding_specs.sharding[axis]->Chunked(list(chunked.chunks) + [axis_sizes[name]])
A:jax._src.sharding_specs.mesh_mapping[mesh_axis_pos[name]]->ShardedAxis(next_sharded_axis)
A:jax._src.sharding_specs.(replication_factor, ragged)->divmod(nrep, axis_size)
A:jax._src.sharding_specs.pspec->ShardingSpec(sharding=[_UNSHARDED_INSTANCE] * len(sharded_shape), mesh_mapping=())
A:jax._src.sharding_specs.sharded_in_axis->sum((not isinstance(s, NoSharding) for s in pspec.sharding[:map_axis]))
jax._src.sharding_specs._sharding_spec_indices(self,shape:tuple[int,...])->np.ndarray
jax._src.sharding_specs._sharding_spec_mesh_shape(self)
jax._src.sharding_specs._sharding_spec_repr(self)
jax._src.sharding_specs.create_pmap_sharding_spec(shape:tuple[int,...],sharded_dim:int=0,sharded_dim_size:Optional[int]=None)
jax._src.sharding_specs.get_logical_mesh_ids(mesh_shape)
jax._src.sharding_specs.make_sharding_spec(axis_sizes,mesh_axis_pos,num_dimensions,aval_axes)
jax._src.sharding_specs.new_mesh_sharding_specs(axis_sizes,axis_names)
jax._src.sharding_specs.pmap_sharding_spec(nrep,axis_size,sharded_shape:Sequence[int],map_axis:Optional[int])->ShardingSpec
jax._src.sharding_specs.sharding_spec_sharding_proto(self,special_axes:Optional[Mapping[int,OpShardingType]]=None)->xc.HloSharding
jax._src.sharding_specs.spec_to_indices(shape:Sequence[int],spec:ShardingSpec)->tuple[Index, ...]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugging.py----------------------------------------
A:jax._src.debugging.debug_effect->DebugEffect()
A:jax._src.debugging.ordered_debug_effect->OrderedDebugEffect()
A:jax._src.debugging.debug_callback_p->jax._src.core.Primitive('debug_callback')
A:jax._src.debugging.axis_size->next((x.shape[i] for (x, i) in zip(args, dims) if i is not None))
A:jax._src.debugging.args_idx->map(functools.partial(get_arg_at_dim, i), dims, args)
A:jax._src.debugging.sharding->jax._src.lib.xla_client.OpSharding()
A:jax._src.debugging.(result, token, _)->jax._src.interpreters.mlir.emit_python_callback(ctx, _callback, None, list(args), ctx.avals_in, ctx.avals_out, True, sharding=sharding)
A:jax._src.debugging.(flat_args, in_tree)->jax._src.tree_util.tree_flatten((args, kwargs))
A:jax._src.debugging.(args, kwargs)->jax._src.tree_util.tree_unflatten(in_tree, flat_args)
A:jax._src.debugging.formatter->_DebugPrintFormatChecker()
A:jax._src.debugging.inspect_sharding_p->jax._src.core.Primitive('inspect_sharding')
A:jax._src.debugging.sharding_callbacks->weakref.WeakValueDictionary()
A:jax._src.debugging.devices->list(axis_context.mesh.devices.flat)
A:jax._src.debugging.pspec->parse_flatten_op_sharding(hlo_sharding, mesh)[0].get_partition_spec()
A:jax._src.debugging.key->jax._src.lib.xla_client.encode_inspect_sharding_callback(_hlo_sharding_callback)
A:jax._src.debugging.fun->jax._src.linear_util.wrap_init(lambda *args: [])
A:jax._src.debugging.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_avals)
A:jax._src.debugging.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.debugging.trivial_comp->jax._src.interpreters.mlir.build_xla_computation_helper(closed_jaxpr, name='tmp_xla_computation', platforms=module_context.platforms, backend_or_name=module_context.backend_or_name, axis_context=module_context.axis_context)
A:jax._src.debugging.empty_tuple_sharding->jax._src.lib.xla_client.OpSharding()
A:jax._src.debugging.result_sharding->jax._src.lib.xla_client.HloSharding.from_proto(empty_tuple_sharding)
A:jax._src.debugging.(r, g, b)->map(lambda x: int(x, 16), (color[1:3], color[3:5], color[5:7]))
A:jax._src.debugging.color_values->numpy.linspace(0, 1, num_colors)
A:jax._src.debugging.console->rich.console.Console(width=max_width)
A:jax._src.debugging.base_height->int(10 * scale)
A:jax._src.debugging.base_width->int(base_height * aspect_ratio)
A:jax._src.debugging.device_kind->next(iter(sharding.device_set)).platform.upper()
A:jax._src.debugging.device_indices_map->jax._src.lib.xla_client.OpSharding().devices_indices_map(tuple(shape))
A:jax._src.debugging.slcs->tuple(map(_raise_to_slice, slcs))
A:jax._src.debugging.chunk_idxs->tuple(map(_slice_to_chunk_idx, shape, slcs))
A:jax._src.debugging.vert->slice(0, 1, None)
A:jax._src.debugging.color_iter->make_color_iter(color_map, num_rows, num_cols)
A:jax._src.debugging.table->rich.table.Table(show_header=False, show_lines=not use_color, padding=0, highlight=not use_color, pad_edge=False, box=rich.box.SQUARE if not use_color else None)
A:jax._src.debugging.width->min(max(width, min_width), max_width)
A:jax._src.debugging.height->int(maybe_height * base_height)
A:jax._src.debugging.(left_padding, remainder)->divmod(width - len(entry) - 2, 2)
A:jax._src.debugging.(top_padding, remainder)->divmod(height - 2, 2)
A:jax._src.debugging.color->_canonicalize_color(next(color_iter)[:3])
A:jax._src.debugging.text_color->_get_text_color(color)
A:jax._src.debugging.padding->tuple((max(x, 0) for x in padding))
jax._src.debugging.DebugEffect(effects.Effect)
jax._src.debugging.OrderedDebugEffect(effects.Effect)
jax._src.debugging.ShardingCallbackInfo(self,callback,module_context)
jax._src.debugging.ShardingCallbackInfo.__init__(self,callback,module_context)
jax._src.debugging._DebugPrintFormatChecker(string.Formatter)
jax._src.debugging._DebugPrintFormatChecker.check_unused_args(self,used_args,args,kwargs)
jax._src.debugging._canonicalize_color(color:Color)->str
jax._src.debugging._debug_callback_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.debugging._format_print_callback(fmt:str,*args,**kwargs)
jax._src.debugging._get_text_color(color:str)->str
jax._src.debugging._inspect_sharding_abstract_eval(aval,**_)
jax._src.debugging._inspect_sharding_batching_rule(args,_,*,callback)
jax._src.debugging._inspect_sharding_impl(value,*,callback)
jax._src.debugging._inspect_sharding_jvp_rule(primals,_,**params)
jax._src.debugging._inspect_sharding_lowering_rule(ctx:mlir.LoweringRuleContext,value,*,callback)
jax._src.debugging._raise_to_slice(slc:Union[slice,int])
jax._src.debugging._slice_to_chunk_idx(size:int,slc:slice)->int
jax._src.debugging.debug_callback(callback:Callable[...,Any],*args:Any,ordered:bool=False,**kwargs:Any)->None
jax._src.debugging.debug_callback_abstract_eval(*flat_avals,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_callback_batching_rule(args,dims,**params)
jax._src.debugging.debug_callback_impl(*args,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_callback_jvp_rule(primals,tangents,**params)
jax._src.debugging.debug_callback_lowering(ctx,*args,effect,callback,**params)
jax._src.debugging.debug_callback_transpose_rule(*flat_args,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_print(fmt:str,*args,ordered:bool=False,**kwargs)->None
jax._src.debugging.inspect_array_sharding(value,*,callback:Callable[[Sharding],None])
jax._src.debugging.inspect_sharding_infer_sharding_from_operands(arg_shapes,arg_shardings,shape,backend_string)
jax._src.debugging.inspect_sharding_partition(shapes,arg_shardings,result_shape,result_sharding,backend_string)
jax._src.debugging.inspect_sharding_prop_user_sharding(sharding,backend_string)
jax._src.debugging.make_color_iter(color_map,num_rows,num_cols)
jax._src.debugging.visualize_array_sharding(arr,**kwargs)
jax._src.debugging.visualize_sharding(shape:Sequence[int],sharding:Sharding,*,use_color:bool=True,scale:float=1.0,min_width:int=9,max_width:int=80,color_map:Optional[ColorMap]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/dispatch.py----------------------------------------
A:jax._src.dispatch._JAX_FORCE_TUPLE_ARGS->jax._src.config.DEFINE_bool('jax_force_tuple_args', False, help='Force tuplization of arguments to lowered functions.')
A:jax._src.dispatch.logger->logging.getLogger(__name__)
A:jax._src.dispatch.aval->jax._src.interpreters.xla.abstractify(x)
A:jax._src.dispatch.(in_avals, in_shardings)->jax._src.util.unzip2([_arg_spec(a) for a in args])
A:jax._src.dispatch.in_tree->jax._src.tree_util.tree_structure(args)
A:jax._src.dispatch.compiled_fun->xla_primitive_callable(prim, in_avals, in_tree, OrigShardings(in_shardings), **params)
A:jax._src.dispatch.arg_names->jax._src.api_util._arg_names(prim.impl, args, {}, (), ())
A:jax._src.dispatch.msg->jax._src.pjit._device_assignment_mismatch_error(prim.name, fails, args, 'jit', arg_names)
A:jax._src.dispatch.out->prim.bind(*args, **params)
A:jax._src.dispatch.wrapped_fun->jax._src.linear_util.wrap_init(prim_fun)
A:jax._src.dispatch.(flat_fun, out_tree)->jax._src.api_util.flatten_fun_nokwargs(wrapped_fun, in_tree)
A:jax._src.dispatch.computation->sharded_lowering(flat_fun, prim.name, donated_invars, keep_unused=False, inline=True, in_avals=in_avals, in_shardings=orig_in_shardings.shardings, lowering_parameters=mlir.LoweringParameters())
A:jax._src.dispatch.compiled->sharded_lowering(flat_fun, prim.name, donated_invars, keep_unused=False, inline=True, in_avals=in_avals, in_shardings=orig_in_shardings.shardings, lowering_parameters=mlir.LoweringParameters()).compile()
A:jax._src.dispatch.call->sharded_lowering(flat_fun, prim.name, donated_invars, keep_unused=False, inline=True, in_avals=in_avals, in_shardings=orig_in_shardings.shardings, lowering_parameters=mlir.LoweringParameters()).compile().create_cpp_call_for_apply_primitive(out_tree())
A:jax._src.dispatch.tok->self.current_tokens.get(eff, np.zeros(0, np.bool_))
A:jax._src.dispatch.s->jax.sharding.GSPMDSharding.get_replicated(devices)
A:jax._src.dispatch.indices->tuple(s.addressable_devices_indices_map(tok.shape).values())
A:jax._src.dispatch.start_time->time.time()
A:jax._src.dispatch.source_info->SourceInfo(source_info_util.summarize(eqn.source_info), eqn.primitive.name)
A:jax._src.dispatch.call_jaxpr->eqn.params.get('call_jaxpr')
A:jax._src.dispatch.result_handler->jax._src.interpreters.pxla.global_aval_to_result_handler(aval, s, committed, False)
A:jax._src.dispatch.map_->jax.sharding.GSPMDSharding.get_replicated(devices).devices_indices_map(aval.shape)
A:jax._src.dispatch.inp_plat->inp_sharding._device_assignment[0].platform.upper()
A:jax._src.dispatch.target_plat->target_sharding._device_assignment[0].platform.upper()
A:jax._src.dispatch.old_hlo_sharding->inp_sharding._to_xla_hlo_sharding(x.ndim)
A:jax._src.dispatch.permute_order->numpy.vectorize(target_sharding._device_assignment.index, otypes=[int])(inp_sharding._device_assignment)
A:jax._src.dispatch.new_op_sharding->inp_sharding._to_xla_hlo_sharding(x.ndim).to_proto()
A:jax._src.dispatch.new_op_sharding.tile_assignment_devices->numpy.take(old_hlo_sharding.tile_assignment_devices(), permute_order)
A:jax._src.dispatch.new_hlo_sharding->jax._src.lib.xla_client.HloSharding.from_proto(new_op_sharding)
A:jax._src.dispatch.new_x->jax._src.array.make_array_from_single_device_arrays(x.shape, GSPMDSharding(target_sharding._device_assignment, new_hlo_sharding), x._arrays)
A:jax._src.dispatch.pxla._get_and_check_device_assignment.fn->partial(_override_get_device_assignment, target_sharding)
A:jax._src.dispatch.sh->SingleDeviceSharding(pxla._get_default_device() if device is None else device)
A:jax._src.dispatch.device_put_p->jax._src.core.Primitive('device_put')
A:jax._src.dispatch.x->jax._src.interpreters.mlir.wrap_with_sharding_op(ctx, x, out_aval, device._to_xla_hlo_sharding(aval.ndim).to_proto())
jax._src.dispatch.OrigShardings
jax._src.dispatch.OrigShardings.__eq__(self,other)
jax._src.dispatch.OrigShardings.__hash__(self)
jax._src.dispatch.RuntimeTokenSet(self)
jax._src.dispatch.RuntimeTokenSet.__init__(self)
jax._src.dispatch.RuntimeTokenSet.block_until_ready(self)
jax._src.dispatch.RuntimeTokenSet.clear(self)
jax._src.dispatch.RuntimeTokenSet.get_token_input(self,eff:core.Effect,devices:list[Device])->jax.Array
jax._src.dispatch.RuntimeTokenSet.set_output_runtime_token(self,device:Device,token:RuntimeToken)
jax._src.dispatch.RuntimeTokenSet.set_token_result(self,eff:core.Effect,token:jax.Array)
jax._src.dispatch.SourceInfo(NamedTuple)
jax._src.dispatch._ArgSpec(NamedTuple)
jax._src.dispatch._arg_spec(x:Any)->_ArgSpec
jax._src.dispatch._check_special(name:str,dtype:np.dtype,buf:basearray.Array)->None
jax._src.dispatch._common_device_put_lowering(ctx,x,*,device,src)
jax._src.dispatch._device_put_impl(x,device:Device|Sharding|None=None,src:Device|Sharding|None=None)
jax._src.dispatch._eqn_replicas(eqn:core.JaxprEqn)->int
jax._src.dispatch._identity_fn(x)
jax._src.dispatch._initial_style_primitive_replicas(params:dict[str,Any])->int
jax._src.dispatch._is_bint_axis_size(d:core.AxisSize)->bool
jax._src.dispatch._mcjax_reshard(x,target_sharding)
jax._src.dispatch._override_get_device_assignment(sharding,*args,**kwargs)
jax._src.dispatch._put_x(x,s:Sharding,aval:core.AbstractValue,committed:bool)
jax._src.dispatch._tpu_device_put_lowering(ctx,x,*,device,src)
jax._src.dispatch.apply_outfeed_rewriter(jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.dispatch.apply_primitive(prim,*args,**params)
jax._src.dispatch.check_arg(arg:Any)
jax._src.dispatch.check_special(name:str,bufs:Sequence[basearray.Array])->None
jax._src.dispatch.device_put_transpose_rule(ct,_,device,src)
jax._src.dispatch.is_single_device_sharding(sharding:Sharding)->bool
jax._src.dispatch.jaxpr_has_bints(jaxpr:core.Jaxpr)->bool
jax._src.dispatch.jaxpr_has_primitive(jaxpr:core.Jaxpr,prim_name:str)->bool
jax._src.dispatch.jaxpr_replicas(jaxpr:core.Jaxpr)->int
jax._src.dispatch.jaxpr_shardings(jaxpr:core.Jaxpr)->Iterator[tuple[XLACompatibleSharding, SourceInfo]]
jax._src.dispatch.log_elapsed_time(fmt:str,fun_name:str,event:str|None=None)
jax._src.dispatch.needs_check_special()->bool
jax._src.dispatch.sharded_lowering(fun:lu.WrappedFun,name:str,donated_invars:Sequence[bool],keep_unused:bool,inline:bool,in_avals:tuple[core.AbstractValue,...],in_shardings:Sequence[Sharding|None],lowering_parameters:mlir.LoweringParameters)->pxla.MeshComputation
jax._src.dispatch.should_tuple_args(num_args:int,platform:str)->bool
jax._src.dispatch.simple_impl(prim)
jax._src.dispatch.wait_for_tokens()
jax._src.dispatch.xla_primitive_callable(prim:core.Primitive,in_avals:tuple[core.AbstractValue,...],in_tree,orig_in_shardings:OrigShardings,**params)->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/traceback_util.py----------------------------------------
A:jax._src.traceback_util.C->TypeVar('C', bound=Callable[..., Any])
A:jax._src.traceback_util.path->os.path.abspath(path)
A:jax._src.traceback_util.path_prefix->os.path.abspath(path_prefix)
A:jax._src.traceback_util.common->os.path.commonpath([path, path_prefix])
A:jax._src.traceback_util.frames->list(traceback.walk_tb(tb))
A:jax._src.traceback_util.out->types.TracebackType(out, f, f.f_lasti, lineno)
A:jax._src.traceback_util.tb->traceback.extract_stack(e.__traceback__.tb_frame)
A:jax._src.traceback_util.mode->_filtering_mode()
A:jax._src.traceback_util.filtered_tb->filter_traceback(tb)
A:jax._src.traceback_util.jax_error->UnfilteredStackTrace(msg)
A:jax._src.traceback_util.msg->format_exception_only(e)
jax._src.traceback_util.SimplifiedTraceback(Exception)
jax._src.traceback_util.SimplifiedTraceback.__str__(self)
jax._src.traceback_util.UnfilteredStackTrace(Exception)
jax._src.traceback_util._add_call_stack_frames(tb:types.TracebackType)->types.TracebackType
jax._src.traceback_util._add_tracebackhide_to_hidden_frames(tb:types.TracebackType)
jax._src.traceback_util._filtering_mode()->str
jax._src.traceback_util._ignore_known_hidden_frame(f:types.FrameType)->bool
jax._src.traceback_util._ipython_supports_tracebackhide()->bool
jax._src.traceback_util._is_reraiser_frame(f:traceback.FrameSummary)->bool
jax._src.traceback_util._is_under_reraiser(e:BaseException)->bool
jax._src.traceback_util._path_starts_with(path:str,path_prefix:str)->bool
jax._src.traceback_util._running_under_ipython()->bool
jax._src.traceback_util.api_boundary(fun:C)->C
jax._src.traceback_util.filter_traceback(tb:types.TracebackType)->Optional[types.TracebackType]
jax._src.traceback_util.format_exception_only(e:BaseException)->str
jax._src.traceback_util.include_frame(f:types.FrameType)->bool
jax._src.traceback_util.register_exclusion(path:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/custom_transpose.py----------------------------------------
A:jax._src.custom_transpose.out_store->StoreEqual()
A:jax._src.custom_transpose.flatten_fun_nokwargs->transformation_with_aux(api_util.flatten_fun_nokwargs.args[0])
A:jax._src.custom_transpose.(_, res_tree)->tree_flatten(res_arg)
A:jax._src.custom_transpose.(_, lin_tree)->tree_flatten(lin_arg)
A:jax._src.custom_transpose.(args_flat, in_tree)->tree_flatten((res_arg, lin_arg))
A:jax._src.custom_transpose.(flat_fun, out_tree2)->flatten_fun_nokwargs(lu.wrap_init(self.fun), in_tree)
A:jax._src.custom_transpose.(out_types_flat, out_tree)->tree_flatten(out_types)
A:jax._src.custom_transpose.out_flat->CustomTransposePrimitive('custom_transpose_call').bind(flat_fun, *args_flat, transpose=self.transpose, out_types=out_types_flat, lin_tree=lin_tree, res_tree=res_tree, out_tree=out_tree)
A:jax._src.custom_transpose.full_tree->tree_fill(0, full_treedef)
A:jax._src.custom_transpose.entire->tree_fill(0, entire)
A:jax._src.custom_transpose.prefix->tree_fill(0, prefix)
A:jax._src.custom_transpose.(transpose_jaxpr, transpose_consts)->thunk()
A:jax._src.custom_transpose.transpose_jaxpr->jax._src.core.ClosedJaxpr(pe.convert_constvars_jaxpr(transpose_jaxpr), ())
A:jax._src.custom_transpose.args_flat->tree_leaves((res_arg, ct_out))
A:jax._src.custom_transpose.ct_ins->jax._src.core.jaxpr_as_fun(transpose_jaxpr)(*transpose_consts, *args_flat)
A:jax._src.custom_transpose.top_trace->jax._src.core.find_top_trace(args)
A:jax._src.custom_transpose.tracers->map(top_trace.full_raise, args)
A:jax._src.custom_transpose.outs->jax._src.core.find_top_trace(args).process_custom_transpose(self, call, tracers, **params)
A:jax._src.custom_transpose.new_params->dict(params)
A:jax._src.custom_transpose.new_params['transpose']->make_transpose_from_thunk(new_params.pop('transpose_jaxpr_thunk'), new_params['lin_tree'])
A:jax._src.custom_transpose.call->jax._src.linear_util.wrap_init(core.jaxpr_as_fun(new_params.pop('call_jaxpr')))
A:jax._src.custom_transpose.transpose->make_transpose_from_thunk(params['transpose_jaxpr_thunk'], lin_tree)
A:jax._src.custom_transpose.call_in_tree->treedef_tuple((res_tree, lin_tree))
A:jax._src.custom_transpose.(res_arg, lin_arg)->tree_unflatten(call_in_tree, args)
A:jax._src.custom_transpose.ct_out->tree_unflatten(out_tree, cts)
A:jax._src.custom_transpose.ct_lin->transpose(res_arg, ct_out)
A:jax._src.custom_transpose.(ct_lin_flat, _)->tree_flatten(tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None), is_leaf=lambda x: x is None)
A:jax._src.custom_transpose.custom_transpose_p->CustomTransposePrimitive('custom_transpose_call')
jax._src.custom_transpose.CustomTransposePrimitive(core.Primitive)
jax._src.custom_transpose.CustomTransposePrimitive.bind(self,call,*args,**params)
jax._src.custom_transpose.CustomTransposePrimitive.get_bind_params(self,params)
jax._src.custom_transpose.StoreEqual(lu.Store)
jax._src.custom_transpose.StoreEqual.store(self,val)
jax._src.custom_transpose.check_transpose_rule_trees(rule,lin_tree,rule_out_tree)
jax._src.custom_transpose.custom_transpose(self,fun:Callable)
jax._src.custom_transpose.custom_transpose.__init__(self,fun:Callable)
jax._src.custom_transpose.custom_transpose.def_transpose(self,transpose:Callable)
jax._src.custom_transpose.custom_transpose_lowering(*args,call_jaxpr,**params)
jax._src.custom_transpose.custom_transpose_transpose_rule(cts,*args,out_types,res_tree,lin_tree,out_tree,**params)
jax._src.custom_transpose.custom_transpose_typecheck(_,*in_atoms,out_types,**params)
jax._src.custom_transpose.is_treedef_prefix(entire,prefix)
jax._src.custom_transpose.make_transpose_from_thunk(thunk,lin_tree)
jax._src.custom_transpose.rule_name(rule)
jax._src.custom_transpose.transformation_with_aux(gen,fun:lu.WrappedFun,*gen_static_args)->tuple[lu.WrappedFun, Any]
jax._src.custom_transpose.tree_broadcast(full_treedef,tree,is_leaf=None)
jax._src.custom_transpose.tree_fill(x,treedef)
jax._src.custom_transpose.tree_fill_like(x,tree)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/custom_api_util.py----------------------------------------
A:jax._src.custom_api_util._custom_wrapper_types->set()
jax._src.custom_api_util.forward_attr(self_,name)
jax._src.custom_api_util.register_custom_decorator_type(cls)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/flatten_util.py----------------------------------------
A:jax._src.flatten_util.(leaves, treedef)->tree_flatten(pytree)
A:jax._src.flatten_util.(flat, unravel_list)->_ravel_list(leaves)
A:jax._src.flatten_util.from_dtypes->tuple((dtypes.dtype(l) for l in lst))
A:jax._src.flatten_util.to_dtype->jax._src.dtypes.result_type(*from_dtypes)
A:jax._src.flatten_util.(sizes, shapes)->unzip2(((jnp.size(x), jnp.shape(x)) for x in lst))
A:jax._src.flatten_util.indices->tuple(np.cumsum(sizes))
A:jax._src.flatten_util.raveled->jax.numpy.concatenate([ravel(e) for e in lst])
A:jax._src.flatten_util.unrav->HashablePartial(_unravel_list, indices, shapes, from_dtypes, to_dtype)
A:jax._src.flatten_util.chunks->jax.numpy.split(arr, indices[:-1])
A:jax._src.flatten_util.arr_dtype->jax._src.dtypes.dtype(arr)
jax._src.flatten_util._ravel_list(lst)
jax._src.flatten_util._unravel_list(indices,shapes,from_dtypes,to_dtype,arr)
jax._src.flatten_util._unravel_list_single_dtype(indices,shapes,arr)
jax._src.flatten_util.ravel_pytree(pytree)
jax._src.flatten_util.unravel_pytree(treedef,unravel_list,flat)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/core.py----------------------------------------
A:jax._src.core._TRACER_ERROR_NUM_TRACEBACK_FRAMES->jax._src.config.DEFINE_integer('jax_tracer_error_num_traceback_frames', config.int_env('JAX_TRACER_ERROR_NUM_TRACEBACK_FRAMES', 5), help='Set the number of stack frames in JAX tracer error messages.')
A:jax._src.core.constvars->pp_vars(jaxpr.constvars, context, print_shapes=settings.print_shapes)
A:jax._src.core.invars->pp_vars(jaxpr.invars, context, print_shapes=settings.print_shapes)
A:jax._src.core.outvars->jax._src.pretty_printer.concat([pp.text('('), pp_vars(jaxpr.outvars, context, separator=','), pp.text(')' if len(jaxpr.outvars) != 1 else ',)')])
A:jax._src.core.eqns->eqns_fn()
A:jax._src.core.effects->property(lambda self: self._effects)
A:jax._src.core.debug_info->property(lambda self: self._debug_info)
A:jax._src.core.self._constvars->list(constvars)
A:jax._src.core.self._invars->list(invars)
A:jax._src.core.self._outvars->list(outvars)
A:jax._src.core.self._eqns->list(eqns)
A:jax._src.core.doc->pp_jaxpr(self, JaxprPpContext(), JaxprPpSettings(source_info=source_info, print_shapes=print_shapes, custom_pp_eqn_rules=custom_pp_eqn_rules, name_stack=name_stack, print_effects=print_effects))
A:jax._src.core.jaxpr->dict(params).pop('call_jaxpr')
A:jax._src.core.consts->property(lambda self: self._consts)
A:jax._src.core.self._consts->list(consts)
A:jax._src.core.settings->JaxprPpSettings(source_info=source_info, print_shapes=print_shapes, name_stack=name_stack, custom_pp_eqn_rules=custom_pp_eqn_rules)
A:jax._src.core.self.aval->raise_to_shaped(aval)
A:jax._src.core.all_vars->itertools.chain.from_iterable((_jaxpr_vars(j) for j in jaxprs))
A:jax._src.core.counter->itertools.count(start=start)
A:jax._src.core.self.hash->hash((val.item(), val.dtype))
A:jax._src.core.out->ans._trace.main.with_cur_sublevel().process_primitive(self, map(trace.full_raise, args), params)
A:jax._src.core.self.abstract_eval->_effect_free_abstract_eval(abstract_eval)
A:jax._src.core.lu->last_used(jaxpr)
A:jax._src.core.(subfuns, bind_params)->eqn.primitive.get_bind_params(eqn.params)
A:jax._src.core.ans->max(tracers, key=operator.attrgetter('_trace.level'))
A:jax._src.core.TracerType->TypeVar('TracerType', bound='Tracer')
A:jax._src.core.val->val.dimension_as_value().dimension_as_value()
A:jax._src.core.idx->operator.index(idx)
A:jax._src.core.trace->max(tracers, key=operator.attrgetter('_trace.level'))._trace.main.with_cur_sublevel()
A:jax._src.core.tracer->max(tracers, key=operator.attrgetter('_trace.level'))._trace.main.with_cur_sublevel().full_raise(tracer)
A:jax._src.core.dbg->getattr(tracer, '_debug_info', None)
A:jax._src.core.line_info->getattr(tracer, '_line_info', None)
A:jax._src.core.attr->getattr(self.aval, name)
A:jax._src.core.t->ref(sublevel)
A:jax._src.core.base->jax._src.pretty_printer.group(pp.nest(2, pp.concat([base, pp.text(' with'), pp.brk(), pp.join(pp.brk(), [pp.text(f'{name} = ') + pp_payload for (name, pp_payload) in contents])])))
A:jax._src.core.aval_property->namedtuple('aval_property', ['fget'])
A:jax._src.core.aval_method->namedtuple('aval_method', ['fun'])
A:jax._src.core.eval_trace->MainTrace(0, EvalTrace)
A:jax._src.core.stack_str->map('  {}\n'.format, self.stack[::-1])
A:jax._src.core.new->self.__new__(TraceState)
A:jax._src.core.AxisEnvFrame->namedtuple('AxisEnvFrame', ['name', 'size', 'main_trace'])
A:jax._src.core.no_axis_name->object()
A:jax._src.core.self.trace_stack->TraceStack()
A:jax._src.core.new.trace_stack->self.trace_stack.copy()
A:jax._src.core.copy->MainTrace(dynamic.level, dynamic.trace_type, **dynamic.payload)
A:jax._src.core.self.trace_state->TraceState()
A:jax._src.core.thread_local_state->ThreadLocalState()
A:jax._src.core.tls->jax._src.lib.jax_jit.thread_local_state()
A:jax._src.core.traces->list(filter(lambda x: isinstance(x, Trace), gc.get_referrers(x)))
A:jax._src.core.tracers->map(top_trace.full_raise, args)
A:jax._src.core.why->partial(_why_alive, {id(tracers)})
A:jax._src.core.msgs->'\n\n'.join((f'{tracers[i]}{tracers[i]._origin_msg()}{why(tracers[i])}' for i in range(len(tracers))))
A:jax._src.core.name->getattr(container, '__name__', name)
A:jax._src.core.name_->getattr(container, '__name__', '<no-name>')
A:jax._src.core.closure->inspect.getclosurevars(container)
A:jax._src.core.level->stack.next_level()
A:jax._src.core.main->MainTrace(0, trace_type, **payload)
A:jax._src.core.leaked_tracers->maybe_find_leaked_tracers(t())
A:jax._src.core.sublevel->Sublevel(len(thread_local_state.trace_state.substack))
A:jax._src.core.top_tracer->max((x for x in xs if isinstance(x, Tracer)), default=None, key=attrgetter('_trace.level'))
A:jax._src.core.bot->Bot()
A:jax._src.core.handler->raise_to_shaped_mappings.get(typ)
A:jax._src.core.fname->getattr(fun, '__name__', fun)
A:jax._src.core.aval_dtype->getattr(aval, 'dtype', None)
A:jax._src.core.ctor->type(aval)
A:jax._src.core.aval_shape->getattr(aval, 'shape', None)
A:jax._src.core.elt_aval->getattr(aval, 'dtype', None)._rules.physical_element_aval(aval_dtype)
A:jax._src.core.self.dtype->_dtype_object(dtype)
A:jax._src.core._bool->partialmethod(_forward_to_value, bool)
A:jax._src.core._int->partialmethod(_forward_to_value, int)
A:jax._src.core._float->concretization_function_error(float, True)
A:jax._src.core._complex->concretization_function_error(complex, True)
A:jax._src.core._hex->partialmethod(_forward_to_value, hex)
A:jax._src.core._oct->partialmethod(_forward_to_value, oct)
A:jax._src.core._index->partialmethod(_forward_to_value, operator.index)
A:jax._src.core.self.shape->canonicalize_shape(shape)
A:jax._src.core.ndim->property(lambda self: len(self.shape))
A:jax._src.core.size->property(lambda self: 0 if any((type(d) is int and d == 0 for d in self.shape)) else math.prod(self.shape))
A:jax._src.core.named_shape->dict(aval.named_shape)
A:jax._src.core.shapestr->','.join(map(str, self.shape))
A:jax._src.core.named_shapestr->','.join((f'{k}:{v}' for (k, v) in self.named_shape.items()))
A:jax._src.core.dtype->_short_dtype_name(a.dtype)
A:jax._src.core.pad_shape->tuple((d.dtype.bound if type(d) is DArray and type(d.dtype) is bint else d for d in aval.shape))
A:jax._src.core.shape->property(lambda self: self._aval.shape)
A:jax._src.core.dtypestr->_short_dtype_name(self._aval.dtype)
A:jax._src.core.slices->tuple((slice(int(d._data)) if type(d) is DArray and type(d.dtype) is bint else slice(None) for d in self.shape))
A:jax._src.core.aval_type->type(aval)
A:jax._src.core.weak_type->getattr(aval, 'weak_type', False)
A:jax._src.core.sz1->math.prod(s1)
A:jax._src.core.sz2->math.prod(s2)
A:jax._src.core.(q, r)->divmod(sz1, sz2)
A:jax._src.core.(num, num_tracers)->partition(num)
A:jax._src.core.(denom, denom_tracers)->partition(denom)
A:jax._src.core.factor->_cancel_divide(num_tracers, denom_tracers)
A:jax._src.core.size1->math.prod(num)
A:jax._src.core.size2->math.prod(denom)
A:jax._src.core.num->list(num)
A:jax._src.core.i->next((i for (i, b) in enumerate(num) if definitely_equal(a, b)), None)
A:jax._src.core.env->dict(zip(dim_vars, dim_values))
A:jax._src.core.self.__positional->canonicalize_shape(args)
A:jax._src.core.self.__named->dict(kwargs)
A:jax._src.core.named->frozenset(self.__named.items())
A:jax._src.core.(call_bind_continuation, top_trace, fun_, tracers, params)->call_bind_with_continuation(self, fun, *args, **params)
A:jax._src.core.outs->map(trace.full_raise, outs)
A:jax._src.core.new_params->dict(params)
A:jax._src.core.subfun->last_used(jaxpr).hashable_partial(lu.wrap_init(eval_jaxpr), jaxpr, ())
A:jax._src.core.top_trace->find_top_trace(args)
A:jax._src.core.(fun_, env_trace_todo)->process_env_traces_call(fun, primitive, top_trace.level, tuple(params.items()))
A:jax._src.core.fun_->last_used(jaxpr).annotate(fun_, fun.in_type)
A:jax._src.core.params->subst_axis_names(eqn.primitive, eqn.params, subst)
A:jax._src.core.(outs, cur_todo)->max(tracers, key=operator.attrgetter('_trace.level'))._trace.main.with_cur_sublevel().post_process_call(primitive, outs, params)
A:jax._src.core.todos_list->list(todos)
A:jax._src.core.axes->dict(params).pop('out_axes')
A:jax._src.core.new_params['out_axes_thunk']->HashableFunction(lambda : axes, closure=axes)
A:jax._src.core.out_axes->t(out_axes)
A:jax._src.core.(_, out_axes_transforms)->todo_and_xforms()
A:jax._src.core.(fun, todo_and_xforms)->process_env_traces_map(fun, primitive, top_trace and top_trace.level, tuple(params.items()))
A:jax._src.core.(env_trace_todo, _)->todo_and_xforms()
A:jax._src.core.(map_bind_continuation, top_trace, fun, tracers, params)->map_bind_with_continuation(primitive, fun, *args, **params)
A:jax._src.core.(outs, (cur_todo, cur_xform))->primitive.post_process(trace, outs, params)
A:jax._src.core.(handler, _)->aval_mapping_handlers.get(type(aval), (None, None))
A:jax._src.core.(_, handler)->aval_mapping_handlers.get(type(aval), (None, None))
A:jax._src.core.frame->AxisEnvFrame(axis_name, size, tag)
A:jax._src.core.self.id->id(obj)
A:jax._src.core.self.axis_names->set()
A:jax._src.core.subst->NameGatheringSubst()
A:jax._src.core.new_params[name]->subst_axis_names_jaxpr(jaxpr, shadowed_subst)
A:jax._src.core.names->tuple(it.chain.from_iterable((subst(name) for name in v.aval.named_shape)))
A:jax._src.core.new_v->Var(v.count, v.suffix, v.aval.update(named_shape=named_shape))
A:jax._src.core.new_jaxpr->Jaxpr(constvars, invars, outvars, eqns, jaxpr.effects)
A:jax._src.core.axis_main->max((axis_frame(a).main_trace for a in used_axis_names(self, params)), default=None, key=lambda t: getattr(t, 'level', -1))
A:jax._src.core.ctx->JaxprPpContext()
A:jax._src.core.pp_settings->JaxprPpSettings()
A:jax._src.core.(ctx, pp_settings)->ctx_factory()
A:jax._src.core.jaxpr_str->str(pp_jaxpr_eqn_range(jaxpr, 0, 20, ctx, pp_settings))
A:jax._src.core.msg->'\n\n'.join([msg, 'in equation:', str(pp.nest(2, pp_eqn(eqn, ctx, settings))), f'from source: {src}'])
A:jax._src.core.(ctx, _)->ctx_factory()
A:jax._src.core.in_atoms->map(read, eqn.invars)
A:jax._src.core.(out_type, eqn_effects)->check_eqn(prim, in_avals, eqn.params)
A:jax._src.core.jaxpr_index->itertools.chain.from_iterable((_jaxpr_vars(j) for j in jaxprs)).index(eqn_invar)
A:jax._src.core.jaxpr_effect->eff.replace(input_index=jaxpr_index)
A:jax._src.core.out_type->substitute_vars_in_output_ty(out_type, eqn.invars, eqn.outvars)
A:jax._src.core.(ctx, settings)->ctx_factory()
A:jax._src.core.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax._src.core.aval->aval.update(shape=tuple((env.get(d, d) for d in aval.shape))).update(shape=tuple((env.get(d, d) for d in aval.shape)))
A:jax._src.core.(out_avals, effects)->prim.abstract_eval(*in_avals, **params)
A:jax._src.core.ordered_effects_->property(lambda self: self._effects).ordered_effects.filter_in(call_jaxpr.effects)
A:jax._src.core.self.var_ids->collections.defaultdict(it.count().__next__, {})
A:jax._src.core.pp_v->jax._src.pretty_printer.text(str(v))
A:jax._src.core.lhs->pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)
A:jax._src.core.kvs->' '.join((f'{k}={v}' for (k, v) in params.items() if _compact_eqn_should_include(k, v)))
A:jax._src.core.lo->max(lo, 0)
A:jax._src.core.hi->max(lo, min(hi, len(jaxpr.eqns)))
jax._src.core.AbstractToken(AbstractValue)
jax._src.core.AbstractToken.at_least_vspace(self)
jax._src.core.AbstractToken.join(self,other)
jax._src.core.AbstractToken.str_short(self,short_dtypes=False)
jax._src.core.AbstractValue
jax._src.core.AbstractValue.__repr__(self)
jax._src.core.AbstractValue.at_least_vspace(self)
jax._src.core.AbstractValue.join(self,other)
jax._src.core.AbstractValue.str_short(self,short_dtypes=False)
jax._src.core.AbstractValue.strip_named_shape(self)->AbstractValue
jax._src.core.AbstractValue.strip_weak_type(self)->AbstractValue
jax._src.core.AbstractValue.update(self,**kwargs)
jax._src.core.AxisPrimitive(Primitive)
jax._src.core.AxisPrimitive.bind(self,*args,**params)
jax._src.core.Bot(AbstractValue)
jax._src.core.CallPrimitive(Primitive)
jax._src.core.CallPrimitive.bind(self,fun,*args,**params)
jax._src.core.CallPrimitive.get_bind_params(self,params)
jax._src.core.ClosedCallPrimitive(CallPrimitive)
jax._src.core.ClosedCallPrimitive.get_bind_params(self,params)
jax._src.core.ClosedJaxpr(self,jaxpr:Jaxpr,consts:Sequence)
jax._src.core.ClosedJaxpr.__init__(self,jaxpr:Jaxpr,consts:Sequence)
jax._src.core.ClosedJaxpr.__repr__(self)
jax._src.core.ClosedJaxpr.__str__(self)
jax._src.core.ClosedJaxpr._repr_pretty_(self,p,cycle)
jax._src.core.ClosedJaxpr.effects(self)->Effects
jax._src.core.ClosedJaxpr.eqns(self)
jax._src.core.ClosedJaxpr.in_avals(self)
jax._src.core.ClosedJaxpr.literals(self)
jax._src.core.ClosedJaxpr.map_jaxpr(self,f)
jax._src.core.ClosedJaxpr.out_avals(self)
jax._src.core.ClosedJaxpr.pretty_print(self,*,source_info=False,print_shapes=True,name_stack=False,custom_pp_eqn_rules=True,**kw)
jax._src.core.ClosedJaxpr.replace(self,*,jaxpr=None,consts=None)
jax._src.core.ConcreteArray(self,dtype,val,weak_type=None)
jax._src.core.ConcreteArray.__eq__(self,other)
jax._src.core.ConcreteArray.__hash__(self)
jax._src.core.ConcreteArray.__init__(self,dtype,val,weak_type=None)
jax._src.core.ConcreteArray.join(self,other)->AbstractValue
jax._src.core.ConcreteArray.str_short(self,short_dtypes=False)->str
jax._src.core.ConcreteArray.update(self,dtype=None,val=None,weak_type=None)
jax._src.core.DArray(self,aval,data)
jax._src.core.DArray.__eq__(self,other)
jax._src.core.DArray.__hash__(self)->int
jax._src.core.DArray.__init__(self,aval,data)
jax._src.core.DArray.__len__(self)
jax._src.core.DArray.__repr__(self)->str
jax._src.core.DBIdx(NamedTuple)
jax._src.core.DConcreteArray(self,shape,dtype,weak_type,val)
jax._src.core.DConcreteArray.__init__(self,shape,dtype,weak_type,val)
jax._src.core.DShapedArray(self,shape,dtype,weak_type=False)
jax._src.core.DShapedArray.__eq__(self,other)
jax._src.core.DShapedArray.__hash__(self)
jax._src.core.DShapedArray.__init__(self,shape,dtype,weak_type=False)
jax._src.core.DShapedArray._len(self,tracer)
jax._src.core.DShapedArray.at_least_vspace(self)
jax._src.core.DShapedArray.join(self,other)
jax._src.core.DShapedArray.str_short(self,short_dtypes=False)->str
jax._src.core.DShapedArray.update(self,shape=None,dtype=None,weak_type=None)
jax._src.core.DropVar(self,aval:AbstractValue)
jax._src.core.DropVar.__init__(self,aval:AbstractValue)
jax._src.core.DropVar.__repr__(self)
jax._src.core.DuplicateAxisNameError(self,var)
jax._src.core.DuplicateAxisNameError.__init__(self,var)
jax._src.core.EvalTrace(Trace)
jax._src.core.EvalTrace.process_call(self,primitive,f,tracers,params)
jax._src.core.EvalTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers,**_)
jax._src.core.EvalTrace.process_custom_transpose(self,primitive,call,tracers,**_)
jax._src.core.EvalTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,**_)
jax._src.core.EvalTrace.process_primitive(self,primitive,tracers,params)
jax._src.core.EvalTrace.pure(self,x)
jax._src.core.InDBIdx
jax._src.core.InconclusiveDimensionOperation(Exception)
jax._src.core.Jaxpr(self,constvars:Sequence[Var],invars:Sequence[Var],outvars:Sequence[Atom],eqns:Sequence[JaxprEqn],effects:Effects=no_effects,debug_info:JaxprDebugInfo|None=None)
jax._src.core.Jaxpr.__init__(self,constvars:Sequence[Var],invars:Sequence[Var],outvars:Sequence[Atom],eqns:Sequence[JaxprEqn],effects:Effects=no_effects,debug_info:JaxprDebugInfo|None=None)
jax._src.core.Jaxpr.__str__(self)
jax._src.core.Jaxpr._repr_pretty_(self,p,cycle)
jax._src.core.Jaxpr.pretty_print(self,*,source_info=False,print_shapes=True,custom_pp_eqn_rules=True,name_stack=False,print_effects:bool=False,**kw)
jax._src.core.Jaxpr.replace(self,*,constvars=None,invars=None,outvars=None,eqns=None,effects=None,debug_info=None)
jax._src.core.JaxprDebugInfo(NamedTuple)
jax._src.core.JaxprEqn(NamedTuple)
jax._src.core.JaxprEqn.__repr__(self)
jax._src.core.JaxprEqn.replace(self,invars:list[Atom]|None=None,outvars:list[Var]|None=None,primitive:Primitive|None=None,params:dict[str,Any]|None=None,effects:Effects|None=None,source_info:source_info_util.SourceInfo|None=None)
jax._src.core.JaxprPpContext(self)
jax._src.core.JaxprPpContext.__init__(self)
jax._src.core.JaxprPpSettings(NamedTuple)
jax._src.core.JaxprTypeError(TypeError)
jax._src.core.Literal(self,val,aval)
jax._src.core.Literal.__init__(self,val,aval)
jax._src.core.Literal.__repr__(self)
jax._src.core.MainTrace(self,level,trace_type,**payload)
jax._src.core.MainTrace.__eq__(self,other:object)->bool
jax._src.core.MainTrace.__hash__(self)->int
jax._src.core.MainTrace.__init__(self,level,trace_type,**payload)
jax._src.core.MainTrace.__repr__(self)->str
jax._src.core.MainTrace.with_cur_sublevel(self)
jax._src.core.MapPrimitive(Primitive)
jax._src.core.MapPrimitive.bind(self,fun,*args,**params)
jax._src.core.MapPrimitive.get_bind_params(self,params)
jax._src.core.MapPrimitive.post_process(self,trace,out_tracers,params)
jax._src.core.MapPrimitive.process(self,trace,fun,tracers,params)
jax._src.core.NameGatheringSubst(self)
jax._src.core.NameGatheringSubst.__init__(self)
jax._src.core.NamedShape(self,*args,**kwargs)
jax._src.core.NamedShape.__eq__(self,other)
jax._src.core.NamedShape.__getitem__(self,idx)
jax._src.core.NamedShape.__hash__(self)
jax._src.core.NamedShape.__init__(self,*args,**kwargs)
jax._src.core.NamedShape.__str__(self)
jax._src.core.NamedShape.named_items(self)
jax._src.core.NamedShape.named_rank(self)
jax._src.core.NamedShape.named_sizes(self)
jax._src.core.NamedShape.names(self)
jax._src.core.NamedShape.positional(self)
jax._src.core.NamedShape.positional_rank(self)
jax._src.core.NamedShape.rank(self)
jax._src.core.NamedShape.total(self)
jax._src.core.OutDBIdx
jax._src.core.Primitive(self,name:str)
jax._src.core.Primitive.__init__(self,name:str)
jax._src.core.Primitive.__repr__(self)
jax._src.core.Primitive.abstract_eval(self,*args,**params)
jax._src.core.Primitive.bind(self,*args,**params)
jax._src.core.Primitive.bind_with_trace(self,trace,args,params)
jax._src.core.Primitive.def_abstract_eval(self,abstract_eval)
jax._src.core.Primitive.def_custom_bind(self,bind)
jax._src.core.Primitive.def_effectful_abstract_eval(self,effectful_abstract_eval)
jax._src.core.Primitive.def_impl(self,impl)
jax._src.core.Primitive.get_bind_params(self,params)
jax._src.core.Primitive.impl(self,*args,**params)
jax._src.core.ShapedArray(self,shape,dtype,weak_type=False,named_shape=None)
jax._src.core.ShapedArray.__eq__(self,other)
jax._src.core.ShapedArray.__hash__(self)
jax._src.core.ShapedArray.__init__(self,shape,dtype,weak_type=False,named_shape=None)
jax._src.core.ShapedArray._len(self,ignored_tracer)
jax._src.core.ShapedArray.at_least_vspace(self)
jax._src.core.ShapedArray.join(self,other)
jax._src.core.ShapedArray.str_short(self,short_dtypes=False)
jax._src.core.ShapedArray.strip_named_shape(self)
jax._src.core.ShapedArray.update(self,shape=None,dtype=None,weak_type=None,named_shape=None)
jax._src.core.SomeTracer
jax._src.core.SomeTracer.__repr__(self)
jax._src.core.Sublevel(self,level:int)
jax._src.core.Sublevel.__eq__(self,other)
jax._src.core.Sublevel.__init__(self,level:int)
jax._src.core.Sublevel.__lt__(self,other)
jax._src.core.Sublevel.__repr__(self)
jax._src.core.ThreadLocalState(self)
jax._src.core.ThreadLocalState.__init__(self)
jax._src.core.Token
jax._src.core.Trace(self,main:MainTrace,sublevel:Sublevel)
jax._src.core.Trace.__init__(self,main:MainTrace,sublevel:Sublevel)
jax._src.core.Trace.__repr__(self)
jax._src.core.Trace.full_raise(self,val)->TracerType
jax._src.core.Trace.lift(self,tracer)->TracerType
jax._src.core.Trace.process_call(self,call_primitive,f,tracers,params)
jax._src.core.Trace.process_custom_jvp_call(self,primitive,fun,jvp,tracers,*,symbolic_zeros)
jax._src.core.Trace.process_custom_transpose(self,prim,call,tracers,**params)
jax._src.core.Trace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.core.Trace.process_map(self,map_primitive,f,tracers,params)
jax._src.core.Trace.process_primitive(self,primitive,tracers,params)
jax._src.core.Trace.pure(self,val)->TracerType
jax._src.core.Trace.sublift(self,tracer)->TracerType
jax._src.core.TraceStack(self)
jax._src.core.TraceStack.__init__(self)
jax._src.core.TraceStack.__repr__(self)->str
jax._src.core.TraceStack.copy(self)
jax._src.core.TraceStack.next_level(self)->int
jax._src.core.TraceStack.pop(self)->None
jax._src.core.TraceStack.push(self,main_trace:MainTrace)->None
jax._src.core.TraceState(self)
jax._src.core.TraceState.__init__(self)
jax._src.core.TraceState.copy(self)
jax._src.core.Tracer(self,trace:Trace)
jax._src.core.Tracer.__array__(self,*args,**kw)
jax._src.core.Tracer.__array_module__(self,types)
jax._src.core.Tracer.__bool__(self)
jax._src.core.Tracer.__complex__(self)
jax._src.core.Tracer.__dlpack__(self,*args,**kw)
jax._src.core.Tracer.__float__(self)
jax._src.core.Tracer.__getattr__(self,name)
jax._src.core.Tracer.__hex__(self)
jax._src.core.Tracer.__index__(self)
jax._src.core.Tracer.__init__(self,trace:Trace)
jax._src.core.Tracer.__int__(self)
jax._src.core.Tracer.__iter__(self)
jax._src.core.Tracer.__len__(self)
jax._src.core.Tracer.__oct__(self)
jax._src.core.Tracer.__reduce__(self)
jax._src.core.Tracer.__repr__(self)
jax._src.core.Tracer.__reversed__(self)
jax._src.core.Tracer.__setitem__(self,idx,val)
jax._src.core.Tracer._assert_live(self)->None
jax._src.core.Tracer._contents(self)
jax._src.core.Tracer._error_repr(self)
jax._src.core.Tracer._origin_msg(self)->str
jax._src.core.Tracer._pretty_print(self)
jax._src.core.Tracer.addressable_data(self,index)
jax._src.core.Tracer.addressable_shards(self)
jax._src.core.Tracer.at(self)
jax._src.core.Tracer.aval(self)
jax._src.core.Tracer.block_until_ready(self)
jax._src.core.Tracer.copy_to_host_async(self)
jax._src.core.Tracer.delete(self)
jax._src.core.Tracer.device(self)
jax._src.core.Tracer.devices(self)
jax._src.core.Tracer.get_referent(self)->Any
jax._src.core.Tracer.global_shards(self)
jax._src.core.Tracer.is_deleted(self)
jax._src.core.Tracer.is_fully_addressable(self)
jax._src.core.Tracer.is_fully_replicated(self)
jax._src.core.Tracer.on_device_size_in_bytes(self)
jax._src.core.Tracer.sharding(self)
jax._src.core.Tracer.tobytes(self,order='C')
jax._src.core.Tracer.tolist(self)
jax._src.core.Tracer.traceback(self)
jax._src.core.Tracer.unsafe_buffer_pointer(self)
jax._src.core.UnshapedArray(self,dtype,weak_type=False)
jax._src.core.UnshapedArray.__eq__(self,other)
jax._src.core.UnshapedArray.__hash__(self)
jax._src.core.UnshapedArray.__init__(self,dtype,weak_type=False)
jax._src.core.UnshapedArray.__ne__(self,other)
jax._src.core.UnshapedArray.__repr__(self)
jax._src.core.UnshapedArray.at_least_vspace(self)->AbstractValue
jax._src.core.UnshapedArray.join(self,other)
jax._src.core.UnshapedArray.shape(self)
jax._src.core.UnshapedArray.str_short(self,short_dtypes=False)->str
jax._src.core.UnshapedArray.strip_weak_type(self)
jax._src.core.UnshapedArray.update(self,dtype=None,weak_type=None)
jax._src.core.Var(self,count:int,suffix:str,aval:AbstractValue)
jax._src.core.Var.__init__(self,count:int,suffix:str,aval:AbstractValue)
jax._src.core.Var.__lt__(self,other)
jax._src.core.Var.__repr__(self)
jax._src.core._TempAxisName(self,obj)
jax._src.core._TempAxisName.__eq__(self,other)
jax._src.core._TempAxisName.__hash__(self)
jax._src.core._TempAxisName.__init__(self,obj)
jax._src.core._TempAxisName.__lt__(self,other)
jax._src.core._TempAxisName.__repr__(self)
jax._src.core._cancel_divide(num,denom)
jax._src.core._canonicalize_dimension(dim:DimSize)->DimSize
jax._src.core._check_call(ctx_factory,prim,in_atoms,params)
jax._src.core._check_closed_call(_,*in_atoms,call_jaxpr)
jax._src.core._check_jaxpr(ctx_factory:Callable[[],tuple[JaxprPpContext,JaxprPpSettings]],jaxpr:Jaxpr)->None
jax._src.core._check_map(ctx_factory,prim,in_avals,params)
jax._src.core._compact_eqn_should_include(k:str,v:Any)->bool
jax._src.core._dtype_object(dtype)
jax._src.core._effect_free_abstract_eval(abstract_eval)
jax._src.core._encode_digits_alphabetic(n)
jax._src.core._forward_to_value(self,fun,ignored_tracer,*args)
jax._src.core._initialize_jax_jit_thread_local_state()
jax._src.core._invalid_shape_error(shape:Shape,context:str='')
jax._src.core._jaxpr_type_to_callable_annotation(jaxpr:Jaxpr)->InputType
jax._src.core._jaxpr_vars(jaxpr)
jax._src.core._map_dshaped_array(size:AxisSize,axis:int|None,aval:DShapedArray)->DShapedArray
jax._src.core._map_shaped_array(size:int,axis:int|None,aval:ShapedArray)->ShapedArray
jax._src.core._param_uses_outfeed(param)
jax._src.core._pp_eqn(eqn,context,settings)->pp.Doc
jax._src.core._replace_jaxpr_effects(jaxpr:ClosedJaxpr,effects:frozenset[Effect])
jax._src.core._short_dtype_name(dtype)->str
jax._src.core._unmap_dshaped_array(size:AxisSize,axis_name:AxisName,axis:int|None,aval:DShapedArray)->DShapedArray
jax._src.core._unmap_shaped_array(size:int,axis_name:AxisName,axis:int|None,aval:ShapedArray)->ShapedArray
jax._src.core._update_thread_local_jit_state(dynamic)
jax._src.core._why_alive(ignore_ids:set[int],x:Any)->str
jax._src.core._why_alive_container_info(container,obj_id)->str
jax._src.core.apply_todos(todos,outs)
jax._src.core.as_named_shape(shape)->NamedShape
jax._src.core.axis_frame(axis_name:AxisName,main_trace:MainTrace|None=None)->AxisEnvFrame
jax._src.core.bint(dtypes.ExtendedDType)
jax._src.core.bint.__str__(self)->str
jax._src.core.bint.name(self)->str
jax._src.core.bint.type(self)->type
jax._src.core.call_bind_with_continuation(primitive:CallPrimitive,fun,*args,**params)
jax._src.core.call_impl(f:lu.WrappedFun,*args,**params)
jax._src.core.cancel_divide_tracers(num,denom)
jax._src.core.canonicalize_dim(d:DimSize,context:str='')->DimSize
jax._src.core.canonicalize_shape(shape:Shape,context:str='')->tuple[Any, ...]
jax._src.core.check_bool_conversion(arr:Array,warn_on_empty=False)
jax._src.core.check_eqn(prim,in_avals,params)
jax._src.core.check_integer_conversion(arr:Array)
jax._src.core.check_jaxpr(jaxpr:Jaxpr)
jax._src.core.check_scalar_conversion(arr:Array)
jax._src.core.check_type(ctx_factory:Callable[[],tuple[JaxprPpContext,JaxprPpSettings]],env:set[Var],ty:AbstractValue)->None
jax._src.core.check_valid_jaxtype(x)
jax._src.core.clean_up_dead_vars(eqn:JaxprEqn,env:dict[Var,Any],last_used:dict[Var,JaxprEqn|None])
jax._src.core.concrete_aval(x)
jax._src.core.concrete_dim_or_error(val:Any,context='')
jax._src.core.concrete_or_error(force:Any,val:Any,context='')
jax._src.core.concretization_function_error(fun,suggest_astype=False)
jax._src.core.cur_sublevel()->Sublevel
jax._src.core.dedup_referents(itr:Iterable[Any])->list[Any]
jax._src.core.definitely_equal(x,y)
jax._src.core.definitely_equal_one_of_dim(d1:DimSize,dlist:Sequence[DimSize])->bool
jax._src.core.definitely_equal_shape(s1:Shape,s2:Shape)->bool
jax._src.core.dilate_dim(d:DimSize,dilation:DimSize)->DimSize
jax._src.core.dim_constant(ct:int)
jax._src.core.dim_value_aval()->AbstractValue
jax._src.core.dim_value_dtype()
jax._src.core.dimension_as_value(d:DimSize)
jax._src.core.divide_shape_sizes(s1:Shape,s2:Shape)->DimSize
jax._src.core.do_subst_axis_names_jaxpr(jaxpr:Jaxpr|ClosedJaxpr,subst:AxisSubst)
jax._src.core.dynamic_level()->int
jax._src.core.ensure_compile_time_eval()
jax._src.core.escaped_tracer_error(tracer,detail=None)
jax._src.core.eval_jaxpr(jaxpr:Jaxpr,consts,*args,propagate_source_info=True)
jax._src.core.evaluate_shape(shape:Shape,dim_vars:Sequence[str],*dim_values:Array)->Sequence[Array]
jax._src.core.extend_axis_env(axis_name:AxisName,size:int,tag:Any)
jax._src.core.extend_axis_env_nd(axes:Iterable[tuple[AxisName,int]],tag:Any=None)
jax._src.core.find_top_trace(xs)->Trace
jax._src.core.full_lower(val)
jax._src.core.gensym(jaxprs:Sequence[Jaxpr]|None=None,suffix:str='')->Callable[[AbstractValue], Var]
jax._src.core.get_aval(x)
jax._src.core.get_referent(x:Any)->Any
jax._src.core.is_constant_dim(d:DimSize)->bool
jax._src.core.is_constant_shape(s:Shape)->bool
jax._src.core.is_dim(v:Any)->bool
jax._src.core.is_empty_shape(s:Shape)->bool
jax._src.core.is_symbolic_dim(v:Any)->bool
jax._src.core.jaxpr_as_fun(closed_jaxpr:ClosedJaxpr,*args)
jax._src.core.jaxpr_uses_outfeed(jaxpr:Jaxpr)->bool
jax._src.core.jaxprs_in_params(params)->Iterator[Jaxpr]
jax._src.core.join_effects(*effects:Effects)->Effects
jax._src.core.join_named_shapes(*named_shapes)
jax._src.core.last_used(jaxpr:Jaxpr)->dict[Var, JaxprEqn | None]
jax._src.core.lattice_join(x:AbstractValue|None,y:AbstractValue|None)->AbstractValue
jax._src.core.leaked_tracer_error(name:str,t,tracers:list[Tracer])->Exception
jax._src.core.map_bind(primitive:MapPrimitive,fun,*args,**params)
jax._src.core.map_bind_with_continuation(primitive:MapPrimitive,fun,*args,out_axes_thunk,**params)
jax._src.core.mapped_aval(size:AxisSize,axis:int|None,aval:AbstractValue)->AbstractValue
jax._src.core.maybe_find_leaked_tracers(x:MainTrace|Sublevel|None)->list[Tracer]
jax._src.core.new_base_main(trace_type:type[Trace],**payload)->Generator[MainTrace, None, None]
jax._src.core.new_dynamic(level:int)->Generator[None, None, None]
jax._src.core.new_jaxpr_eqn(invars,outvars,primitive,params,effects,source_info=None)
jax._src.core.new_main(trace_type:type[Trace],dynamic:bool=False,**payload)->Generator[MainTrace, None, None]
jax._src.core.new_sublevel()->Generator[None, None, None]
jax._src.core.non_negative_dim(d:DimSize)->DimSize
jax._src.core.physical_aval(aval)
jax._src.core.pp_aval(a:AbstractValue,context:JaxprPpContext)->str
jax._src.core.pp_effect(effect:Effect,context:JaxprPpContext)->pp.Doc
jax._src.core.pp_eqn(eqn:JaxprEqn,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_eqns(eqns,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_jaxpr(jaxpr,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_jaxpr_eqn_range(jaxpr:Jaxpr,lo:int,hi:int,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_jaxpr_skeleton(jaxpr,eqns_fn,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_jaxprs(jaxprs,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_kv_pair(k:str,v:Any,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_kv_pairs(kv_pairs,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax._src.core.pp_var(v:Var,context:JaxprPpContext)->str
jax._src.core.pp_vars(vs:Sequence[Any],context:JaxprPpContext,*,separator='',print_shapes:bool=False)->pp.Doc
jax._src.core.primal_dtype_to_tangent_dtype(primal_dtype)
jax._src.core.primitive_uses_outfeed(prim:Primitive,params:dict)->bool
jax._src.core.process_env_traces_call(primitive:CallPrimitive,level:int,params_tuple:tuple,*args)
jax._src.core.process_env_traces_map(primitive:MapPrimitive,level:int,params_tuple:tuple,*args)
jax._src.core.raise_as_much_as_possible(tracer)->Tracer
jax._src.core.raise_to_shaped(aval:AbstractValue,weak_type=None)
jax._src.core.replace_jaxpr_effects(jaxpr:ClosedJaxpr,effects:Effects)
jax._src.core.replace_tracer_for_error_message(obj)
jax._src.core.reset_trace_state()->bool
jax._src.core.same_referent(x:Any,y:Any)->bool
jax._src.core.stash_axis_env()
jax._src.core.str_eqn_compact(primitive_name:str,params:dict)->str
jax._src.core.stride_dim(d:DimSize,window_size:DimSize,window_stride:DimSize)->DimSize
jax._src.core.subjaxprs(jaxpr:Jaxpr)->Iterator[Jaxpr]
jax._src.core.subst_axis_names(primitive:Primitive,params:ParamDict,subst:AxisSubst,traverse:bool=True)->ParamDict
jax._src.core.subst_axis_names_eqn(eqn:JaxprEqn,subst:AxisSubst,var_map:dict[Var,Var])->JaxprEqn
jax._src.core.subst_axis_names_jaxpr(jaxpr:Jaxpr|ClosedJaxpr,subst:AxisSubst)
jax._src.core.subst_axis_names_var(v:Var,subst:AxisSubst,var_map:dict[Var,Var])->Var
jax._src.core.substitute_vars_in_output_ty(out_type:Sequence[AbstractValue],in_atoms:Sequence[Atom],out_binders:Sequence[Var])->list[AbstractValue]
jax._src.core.trace_state_clean()->bool
jax._src.core.traverse_jaxpr_params(f,params)
jax._src.core.typecheck(aval:AbstractValue,x)->bool
jax._src.core.typecompat(aval_ref:AbstractValue,aval:AbstractValue)->bool
jax._src.core.typematch(aval1:AbstractValue,aval2:AbstractValue)->bool
jax._src.core.unmapped_aval(size:AxisSize,axis_name,axis:int|None,aval:AbstractValue)->AbstractValue
jax._src.core.used_axis_names(primitive:Primitive,params:ParamDict)->set[AxisName]
jax._src.core.used_axis_names_jaxpr(jaxpr:Jaxpr|ClosedJaxpr)
jax._src.core.valid_jaxtype(x)->bool
jax.ensure_compile_time_eval()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/source_info_util.py----------------------------------------
A:jax._src.source_info_util.scopes->tuple(map(Scope, name))
A:jax._src.source_info_util.scope->elem.wrap(scope)
A:jax._src.source_info_util.name_stack->name_stack.extend(name).extend(name)
A:jax._src.source_info_util.loc->jax._src.lib.xla_client.Traceback.code_addr2location(code, lasti)
A:jax._src.source_info_util.frames->itertools.islice(user_frames(source_info), num_frames)
A:jax._src.source_info_util.self.context->new_source_info()
A:jax._src.source_info_util._source_info_context->_SourceInfoContext()
A:jax._src.source_info_util.source_info->source_info.replace(traceback=xla_client.Traceback.get_traceback()).replace(traceback=xla_client.Traceback.get_traceback())
A:jax._src.source_info_util._source_info_context.context->_SourceInfoContext().context.replace(traceback=c, name_stack=name_stack)
A:jax._src.source_info_util.filtered_tb->jax._src.traceback_util.filter_traceback(c.as_python_traceback())
A:jax._src.source_info_util.msg->jax._src.traceback_util.format_exception_only(e)
A:jax._src.source_info_util.exp->JaxStackTraceBeforeTransformation(msg).with_traceback(filtered_tb)
A:jax._src.source_info_util.new_context->prev_context.replace(name_stack=curr_name_stack.transform(name))
jax._src.source_info_util.Frame(NamedTuple)
jax._src.source_info_util.JaxStackTraceBeforeTransformation(Exception)
jax._src.source_info_util.NameStack
jax._src.source_info_util.NameStack.__add__(self,other:'NameStack')->'NameStack'
jax._src.source_info_util.NameStack.__getitem__(self,idx:slice)->'NameStack'
jax._src.source_info_util.NameStack.__len__(self)
jax._src.source_info_util.NameStack.__radd__(self,other:'NameStack')->'NameStack'
jax._src.source_info_util.NameStack.__str__(self)->str
jax._src.source_info_util.NameStack.extend(self,name:Union[tuple[str,...],str])->'NameStack'
jax._src.source_info_util.NameStack.transform(self,transform_name:str)->'NameStack'
jax._src.source_info_util.NameStack.wrap_name(self,name:str)->str
jax._src.source_info_util.Scope(NamedTuple)
jax._src.source_info_util.Scope.wrap(self,stack:tuple[str,...])->tuple[str, ...]
jax._src.source_info_util.SourceInfo(NamedTuple)
jax._src.source_info_util.SourceInfo.replace(self,*,traceback:Optional[Traceback]=None,name_stack:Optional[NameStack]=None)->'SourceInfo'
jax._src.source_info_util.Transform(NamedTuple)
jax._src.source_info_util.Transform.wrap(self,stack:tuple[str,...])->tuple[str, ...]
jax._src.source_info_util._SourceInfoContext(self)
jax._src.source_info_util._SourceInfoContext.__init__(self)
jax._src.source_info_util._summarize_frame(frame:Frame)->str
jax._src.source_info_util.current()->SourceInfo
jax._src.source_info_util.current_name_stack()->NameStack
jax._src.source_info_util.extend_name_stack(name:str)->Iterator[NameStack]
jax._src.source_info_util.has_user_context(e)
jax._src.source_info_util.is_user_filename(filename:str)->bool
jax._src.source_info_util.new_name_stack(name:str='')->NameStack
jax._src.source_info_util.new_source_info()->SourceInfo
jax._src.source_info_util.register_exclusion(path:str)
jax._src.source_info_util.register_inclusion(path:str)
jax._src.source_info_util.reset_name_stack()->Iterator[None]
jax._src.source_info_util.set_name_stack(name_stack:NameStack)->Iterator[None]
jax._src.source_info_util.summarize(source_info:SourceInfo,num_frames=1)->str
jax._src.source_info_util.transform_name_stack(name:str)->Iterator[NameStack]
jax._src.source_info_util.user_context(c:Optional[Traceback],*,name_stack:Optional[NameStack]=None)
jax._src.source_info_util.user_frame(source_info:SourceInfo)->Optional[Frame]
jax._src.source_info_util.user_frames(source_info:SourceInfo)->Iterator[Frame]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/mesh.py----------------------------------------
A:jax._src.mesh.mesh_repr->', '.join((f"'{k}': {v}" for (k, v) in self.physical_mesh.shape.items()))
A:jax._src.mesh.is_local_device->numpy.vectorize(lambda d: d.process_index == process_index, otypes=[bool])(global_mesh.devices)
A:jax._src.mesh.other_axes->jax._src.util.tuple_delete(tuple(range(global_mesh.devices.ndim)), axis)
A:jax._src.mesh.local_slices->numpy.vectorize(lambda d: d.process_index == process_index, otypes=[bool])(global_mesh.devices).any(other_axes, keepdims=False)
A:jax._src.mesh.nonzero_indices->numpy.flatnonzero(local_slices)
A:jax._src.mesh.subcube_indices_tuple->tuple(subcube_indices)
A:jax._src.mesh.devices->numpy.array(devices)
A:jax._src.mesh.axis_names->tuple(axis_names)
A:jax._src.mesh.val->_mesh_object_dict.get(key, None)
A:jax._src.mesh.self->super(Mesh, cls).__new__(cls)
A:jax._src.mesh.self.devices->numpy.array(devices).copy()
A:jax._src.mesh.self._hash->hash((self.axis_names, tuple(self.devices.flat), self.devices.shape))
A:jax._src.mesh.new_env->_ThreadResourcesLocalState().stack[-1].with_mesh(self)
A:jax._src.mesh.EMPTY_ENV->ResourceEnv(Mesh(np.empty((), dtype=object), ()), ())
A:jax._src.mesh.thread_resources->_ThreadResourcesLocalState()
jax._src.mesh.Loop(NamedTuple)
jax._src.mesh.Mesh(cls,devices:np.ndarray|Sequence[xc.Device],axis_names:str|Sequence[MeshAxisName])
jax._src.mesh.Mesh.__enter__(self)
jax._src.mesh.Mesh.__eq__(self,other)
jax._src.mesh.Mesh.__exit__(self,exc_type,exc_value,traceback)
jax._src.mesh.Mesh.__hash__(self)
jax._src.mesh.Mesh.__new__(cls,devices:np.ndarray|Sequence[xc.Device],axis_names:str|Sequence[MeshAxisName])
jax._src.mesh.Mesh.__reduce__(self)
jax._src.mesh.Mesh.__repr__(self)
jax._src.mesh.Mesh.__setattr__(self,name,value)
jax._src.mesh.Mesh._flat_devices_set(self)
jax._src.mesh.Mesh._flat_devices_tuple(self)
jax._src.mesh.Mesh._internal_device_list(self)
jax._src.mesh.Mesh._local_devices_set(self)
jax._src.mesh.Mesh._local_mesh(self,process_index)
jax._src.mesh.Mesh._repr(self)
jax._src.mesh.Mesh.device_ids(self)
jax._src.mesh.Mesh.empty(self)
jax._src.mesh.Mesh.is_multi_process(self)
jax._src.mesh.Mesh.local_devices(self)
jax._src.mesh.Mesh.local_mesh(self)
jax._src.mesh.Mesh.shape(self)
jax._src.mesh.Mesh.size(self)
jax._src.mesh.ResourceEnv(NamedTuple)
jax._src.mesh.ResourceEnv.__repr__(self)
jax._src.mesh.ResourceEnv.local_shape(self)
jax._src.mesh.ResourceEnv.loop_resource_axes(self)->set[ResourceAxisName]
jax._src.mesh.ResourceEnv.physical_resource_axes(self)->set[ResourceAxisName]
jax._src.mesh.ResourceEnv.resource_axes(self)->set[ResourceAxisName]
jax._src.mesh.ResourceEnv.shape(self)
jax._src.mesh.ResourceEnv.with_extra_loop(self,loop:Loop)
jax._src.mesh.ResourceEnv.with_mesh(self,mesh:Mesh)
jax._src.mesh._ThreadResourcesLocalState(self)
jax._src.mesh._ThreadResourcesLocalState.__init__(self)
jax._src.mesh._get_local_mesh(global_mesh:Mesh,process_index:int)->Mesh
jax._src.mesh.show_axes(axes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/dtypes.py----------------------------------------
A:jax._src.dtypes._ml_dtypes_version->tuple(map(int, ml_dtypes.__version__.split('.')[:3]))
A:jax._src.dtypes.export->set_module('jax.dtypes')
A:jax._src.dtypes.dtype_->numpy.dtype(dtype)
A:jax._src.dtypes.ftype->to_inexact_dtype(dtype)
A:jax._src.dtypes.typ->dtype(x)
A:jax._src.dtypes.dtype->canonicalize_dtype(dtype, allow_extended_dtype=True)
A:jax._src.dtypes.lattice->_type_promotion_lattice(jax_numpy_dtype_promotion)
A:jax._src.dtypes.new_upper_bounds->set().union(*(lattice[b] for b in upper_bounds[n]))
A:jax._src.dtypes.N->set(nodes)
A:jax._src.dtypes.CUB->set.intersection(*bounds)
A:jax._src.dtypes.a_tp->cast(JAXType, a if any((a is t for t in _weak_types)) else np.dtype(a))
A:jax._src.dtypes.b_tp->cast(JAXType, b if any((b is t for t in _weak_types)) else np.dtype(b))
A:jax._src.dtypes.dt->numpy.result_type(x)
A:jax._src.dtypes.(dtypes, weak_types)->zip(*(_dtype_and_weaktype(arg) for arg in args))
A:jax._src.dtypes.result_type->_least_upper_bound(config.numpy_dtype_promotion.value, *{_jax_type(d, w) for (d, w) in zip(dtypes, weak_types)})
A:jax._src.dtypes.out_dtype->dtype(result_type)
A:jax._src.dtypes.out_weak_type->any((result_type is t for t in _weak_types))
A:jax._src.dtypes.(dtype, weak_type)->_lattice_result_type(*(float_ if arg is None else arg for arg in args))
A:jax._src.dtypes.np_dtype->numpy.dtype(dtype)
A:jax._src.dtypes.input_dtype->dtype(input_dtype_or_value, canonicalize=True)
A:jax._src.dtypes.output_dtype->dtype(output_dtype_or_value, canonicalize=True)
jax._src.dtypes.ExtendedDType(metaclass=abc.ABCMeta)
jax._src.dtypes.ExtendedDType.type(self)->type
jax._src.dtypes.TypePromotionError(ValueError)
jax._src.dtypes._canonicalize_dtype(x64_enabled:bool,allow_extended_dtype:bool,dtype:Any)->Union[DType, ExtendedDType]
jax._src.dtypes._dtype_and_weaktype(value:Any)->tuple[DType, bool]
jax._src.dtypes._issubclass(a:Any,b:Any)->bool
jax._src.dtypes._jax_type(dtype:DType,weak_type:bool)->JAXType
jax._src.dtypes._lattice_result_type(*args:Any)->tuple[DType, bool]
jax._src.dtypes._least_upper_bound(jax_numpy_dtype_promotion:str,*nodes:JAXType)->JAXType
jax._src.dtypes._make_lattice_upper_bounds(jax_numpy_dtype_promotion:str)->dict[JAXType, set[JAXType]]
jax._src.dtypes._scalar_type_to_dtype(typ:type,value:Any=None)->DType
jax._src.dtypes._type_promotion_lattice(jax_numpy_dtype_promotion:str)->dict[JAXType, list[JAXType]]
jax._src.dtypes.canonicalize_dtype(dtype:Any,allow_extended_dtype:bool=False,allow_opaque_dtype:Any=None)->Union[DType, ExtendedDType]
jax._src.dtypes.check_user_dtype_supported(dtype,fun_name=None)
jax._src.dtypes.check_valid_dtype(dtype:DType)->None
jax._src.dtypes.coerce_to_array(x:Any,dtype:Optional[DTypeLike]=None)->np.ndarray
jax._src.dtypes.dtype(x:Any,*,canonicalize:bool=False)->DType
jax._src.dtypes.extended(np.generic)
jax._src.dtypes.is_python_scalar(x:Any)->bool
jax._src.dtypes.is_weakly_typed(x:Any)->bool
jax._src.dtypes.issubdtype(a:DTypeLike|None,b:DTypeLike|None)->bool
jax._src.dtypes.prng_key(extended)
jax._src.dtypes.promote_types(a:DTypeLike,b:DTypeLike)->DType
jax._src.dtypes.result_type(*args:Any,return_weak_type_flag:bool=False)->Union[DType, tuple[DType, bool]]
jax._src.dtypes.safe_to_cast(input_dtype_or_value:Any,output_dtype_or_value:Any)->bool
jax._src.dtypes.scalar_type_of(x:Any)->type
jax._src.dtypes.to_complex_dtype(dtype:DTypeLike)->DType
jax._src.dtypes.to_inexact_dtype(dtype:DTypeLike)->DType
jax._src.dtypes.to_numeric_dtype(dtype:DTypeLike)->DType


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/api.py----------------------------------------
A:jax._src.api._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.api.F->TypeVar('F', bound=Callable)
A:jax._src.api.T->TypeVar('T')
A:jax._src.api.U->TypeVar('U')
A:jax._src.api.(in_shardings, out_shardings, donate_argnums, donate_argnames, static_argnums, static_argnames)->jax._src.pjit.pre_infer_params(fun, in_shardings, out_shardings, donate_argnums, donate_argnames, static_argnums, static_argnames, device, backend, abstracted_axes)
A:jax._src.api.pjit_info_args->jax._src.pjit.PjitInfo(fun=fun, in_shardings=in_shardings, out_shardings=out_shardings, static_argnums=static_argnums, static_argnames=static_argnames, donate_argnums=donate_argnums, donate_argnames=donate_argnames, device=device, backend=backend, keep_unused=keep_unused, inline=inline, resource_env=None, abstracted_axes=abstracted_axes)
A:jax._src.api.has_explicit_sharding->jax._src.pjit._pjit_explicit_sharding(in_shardings, out_shardings, device, backend)
A:jax._src.api.static_argnums->_ensure_index_tuple(static_argnums)
A:jax._src.api.donate_argnums->rebase_donate_argnums(donate_argnums, static_argnums)
A:jax._src.api.fun_name->getattr(fun, '__name__', 'unknown')
A:jax._src.api.(names, sizes)->unzip2(axis_env)
A:jax._src.api.f->jax._src.linear_util.annotate(f, in_type)
A:jax._src.api.(f, dyn_args)->argnums_partial(f, dyn_argnums, args)
A:jax._src.api.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax._src.api.donated_invars->donation_vector(donate_tuple, (), dyn_args, kwargs)
A:jax._src.api.(jaxtree_fun, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax._src.api.avals->map(shaped_abstractify, args_flat)
A:jax._src.api.(jaxpr, out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(jaxtree_fun, avals)
A:jax._src.api.jaxpr->jax._src.dispatch.apply_outfeed_rewriter(jaxpr)
A:jax._src.api.axis_env_->make_axis_env(dispatch.jaxpr_replicas(jaxpr))
A:jax._src.api.ordered_effects->list(effects.ordered_effects.filter_in(jaxpr.effects))
A:jax._src.api.lowering_result->jax._src.interpreters.mlir.lower_jaxpr_to_module(f'xla_computation_{fun_name}', core.ClosedJaxpr(jaxpr, consts), ordered_effects=ordered_effects, backend_or_name=backend, platforms=[platform], axis_context=sharding_impls.ReplicaAxisContext(axis_env_), name_stack=source_info_util.new_name_stack(wrap_name(fun_name, 'xla_computation')), donated_args=donated_invars, arg_shardings=None, result_shardings=None, lowering_parameters=mlir.LoweringParameters())
A:jax._src.api.built->jax._src.lib.xla_client._xla.mlir.mlir_module_to_xla_computation(mlir.module_to_string(lowering_result.module), use_tuple_args=tuple_args, return_tuple=True)
A:jax._src.api.out_shape->tree_unflatten(out_tree(), out_shapes_flat)
A:jax._src.api.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, holomorphic=holomorphic, allow_int=allow_int, reduce_axes=reduce_axes)
A:jax._src.api.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax._src.api.((_, aux), g)->value_and_grad_f(*args, **kwargs)
A:jax._src.api.argnums->_ensure_index(argnums)
A:jax._src.api.reduce_axes->_ensure_str_tuple(reduce_axes)
A:jax._src.api.(f_partial, dyn_args)->argnums_partial(f, argnums, args, require_static_args_hashable=False)
A:jax._src.api.(ans, vjp_py)->_vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)
A:jax._src.api.(ans, vjp_py, aux)->_vjp(f_partial, *dyn_args, has_aux=True, reduce_axes=reduce_axes)
A:jax._src.api.g->vjp_py(lax_internal._one(ans))
A:jax._src.api.aval->jax._src.core.unmapped_aval(len(devices), core.no_axis_name, 0, core.raise_to_shaped(core.get_aval(x)))
A:jax._src.api._check_input_dtype_grad->partial(_check_input_dtype_revderiv, 'grad')
A:jax._src.api._check_output_dtype_grad->partial(_check_output_dtype_revderiv, 'grad')
A:jax._src.api.(y, jac)->vmap(pushfwd, out_axes=(None, -1))(_std_basis(dyn_args))
A:jax._src.api.(y, jac, aux)->vmap(pushfwd, out_axes=(None, -1, None))(_std_basis(dyn_args))
A:jax._src.api.jac_tree->tree_transpose(tree_structure(example_args), tree_structure(y), jac_tree)
A:jax._src.api.(y, pullback)->_vjp(f_partial, *dyn_args)
A:jax._src.api.(y, pullback, aux)->_vjp(f_partial, *dyn_args, has_aux=True)
A:jax._src.api.jac->vmap(pullback)(_std_basis(y))
A:jax._src.api._check_input_dtype_jacrev->partial(_check_input_dtype_revderiv, 'jacrev')
A:jax._src.api._check_output_dtype_jacrev->partial(_check_output_dtype_revderiv, 'jacrev')
A:jax._src.api.(leaves, _)->tree_flatten(pytree)
A:jax._src.api.ndim->property(lambda self: len(self.shape))
A:jax._src.api.dtype->jax._src.dtypes.result_type(*leaves)
A:jax._src.api.flat_basis->jax.numpy.eye(ndim, dtype=dtype)
A:jax._src.api.(leaves, treedef)->tree_flatten(pytree)
A:jax._src.api.parts->_split(arr, np.cumsum(map(np.size, leaves[:-1])), axis)
A:jax._src.api.in_axes->tuple(in_axes)
A:jax._src.api.(flat_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.api.in_axes_flat->tuple(broadcast_prefix((dyn_in_axes, 0), (dyn_args, kwargs), is_leaf=lambda x: x is None))
A:jax._src.api.out_flat->jax._src.interpreters.batching.batch(flat_fun, axis_name, axis_size_, in_axes_flat, lambda : flatten_axes('vmap out_axes', out_tree(), out_axes), spmd_axis_name=spmd_axis_name).call_wrapped(*args_flat)
A:jax._src.api.(args, kwargs)->tree_unflatten(tree, vals)
A:jax._src.api.sizes->jax._src.core.dedup_referents((_get_axis_size(name, np.shape(x), d) for (x, d) in zip(vals, dims) if d is not None))
A:jax._src.api.ba->inspect.signature(fn).bind(*args, **kwargs)
A:jax._src.api.size_counts->collections.Counter((s for s in all_sizes if s is not None))
A:jax._src.api.((sz, ct), *other_counts)counts->collections.Counter((s for s in all_sizes if s is not None)).most_common()
A:jax._src.api.backend->jax._src.xla_bridge.get_backend(backend_name)
A:jax._src.api.global_axis_size->_get_global_axis_size(local_axis_size, in_devices, backend_name, axis_size)
A:jax._src.api.dbg->debug_info('pmap', fun, args, kwargs, static_broadcasted_tuple, ())
A:jax._src.api.dyn_in_axes->tuple((in_axes[i] for i in dyn_argnums))
A:jax._src.api.(args, in_tree)->tree_flatten(py_args)
A:jax._src.api.(e, *_)->prefix_errors((dyn_in_axes, 0), (dyn_args, kwargs))
A:jax._src.api.ex->e('pmap in_axes')
A:jax._src.api.local_axis_size->_mapped_axis_size(fun, in_tree, args, in_axes_flat, 'pmap')
A:jax._src.api.(f, res_paths)->result_paths(f)
A:jax._src.api.(f, out_axes_thunk)->flat_out_axes(f, out_axes)
A:jax._src.api.flat_fun->debug_info_final(flat_fun, dbg, res_paths)
A:jax._src.api.static_broadcasted_tuple->_ensure_index_tuple(static_broadcasted_argnums)
A:jax._src.api.donate_tuple->rebase_donate_argnums(_ensure_index_tuple(donate_argnums), static_broadcasted_tuple)
A:jax._src.api.(axis_name, static_broadcasted_tuple, donate_tuple)->_shared_code_pmap(fun, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)
A:jax._src.api.p->_prepare_pmap(fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, devices, backend, axis_size, args, kwargs)
A:jax._src.api.params->dict(backend=backend, axis_name=axis_name, axis_size=p.local_axis_size, global_axis_size=p.global_axis_size, devices=p.devices, in_axes=p.in_axes_flat, out_axes_thunk=p.out_axes_thunk, name=p.flat_fun.__name__, donated_invars=p.donated_invars, is_explicit_global_axis_size=p.is_explicit_global_axis_size)
A:jax._src.api.(map_bind_continuation, top_trace, fun_, tracers, params)->jax._src.core.map_bind_with_continuation(pxla.xla_pmap_p, p.flat_fun, *p.flat_args, **params)
A:jax._src.api.execute->jax._src.interpreters.pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)
A:jax._src.api.out->jax._src.interpreters.partial_eval.abstract_eval_fun(wrapped_fun.call_wrapped, *map(shaped_abstractify, args_flat), debug_info=debug_info)
A:jax._src.api.out_pytree_def->out_tree()
A:jax._src.api.execute_replicated->typing.cast(pxla.ExecuteReplicated, execute)
A:jax._src.api.fastpath_data->_PmapFastpathData(version=1, xla_executable=execute_replicated.xla_executable, in_handler=in_handler, out_handler=out_handler, out_pytree_def=out_pytree_def, input_sharding_specs=input_sharding_specs, input_devices=in_handler.local_devices, input_indices=in_handler.input_indices, input_array_shardings=in_handler.in_shardings, out_sharding_specs=out_sharding_specs, out_indices=out_indices, out_avals=out_handler.out_avals, out_array_shardings=out_array_shardings, out_committed=out_committed)
A:jax._src.api.cpp_mapped_f->jax._src.lib.pmap_lib.pmap(fun, cache_miss, static_broadcasted_tuple, pxla.shard_arg, pytree_registry=tree_util.default_registry)
A:jax._src.api.pmap_f->wraps(fun)(cpp_mapped_f)
A:jax._src.api.pmap_f.lower->_pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple, devices, backend, axis_size, donate_tuple)
A:jax._src.api._pmap_cache_clears->weakref.WeakSet()
A:jax._src.api.lowering_parameters->kwargs.pop('_experimental_lowering_parameters', mlir.LoweringParameters())
A:jax._src.api.abstract_args->list(map(shaped_abstractify, p.flat_args))
A:jax._src.api.computation->jax._src.interpreters.pxla.lower_parallel_callable(p.flat_fun, backend, axis_name, axis_size=p.local_axis_size, global_axis_size=p.global_axis_size, devices=p.devices, name=p.flat_fun.__name__, in_axes=p.in_axes_flat, out_axes_thunk=p.out_axes_thunk, donated_invars=p.donated_invars, is_explicit_global_axis_size=p.is_explicit_global_axis_size, avals=abstract_args, lowering_parameters=lowering_parameters)
A:jax._src.api.(ps_flat, tree_def)->tree_flatten(primals)
A:jax._src.api.(ts_flat, tree_def_2)->tree_flatten(tangents)
A:jax._src.api.(out_primals, out_tangents)->jvp_fun.call_wrapped(ps_flat, ts_flat)
A:jax._src.api.out_tree->out_tree()
A:jax._src.api.(flat_fun, out_aux_trees)->flatten_fun_nokwargs2(fun, in_tree)
A:jax._src.api.(jvp_fun, aux)->jax._src.interpreters.ad.jvp(flat_fun, has_aux=True)
A:jax._src.api.(out_tree, aux_tree)->out_aux_trees()
A:jax._src.api.(primals_flat, in_tree)->tree_flatten(primals)
A:jax._src.api.(out_primals, out_pvals, jaxpr, consts, *maybe_aux)->jax._src.interpreters.ad.linearize(jaxtree_fun, *primals_flat, has_aux=has_aux)
A:jax._src.api.out_primal_py->tree_unflatten(out_tree, out_primal)
A:jax._src.api.primal_avals->list(map(core.get_aval, primals_flat))
A:jax._src.api.lifted_jvp->Partial(partial(_lift_linearized, jaxpr, primal_avals, (in_tree, out_tree), out_pvals), consts)
A:jax._src.api.tangent_avals->list(map(core.get_aval, tangents))
A:jax._src.api.tangents_out->eval_jaxpr(jaxpr, consts, *tangents)
A:jax._src.api.tangents_out_->iter(tangents_out)
A:jax._src.api.expected_tangent_dtype->jax._src.core.primal_dtype_to_tangent_dtype(_dtype(arg))
A:jax._src.api.ans->fun(*args)
A:jax._src.api.(out_primal, out_vjp)->jax._src.interpreters.ad.vjp(flat_fun, primals_flat, reduce_axes=reduce_axes)
A:jax._src.api.(out_primal, out_vjp, aux)->jax._src.interpreters.ad.vjp(flat_fun, primals_flat, has_aux=True, reduce_axes=reduce_axes)
A:jax._src.api.vjp_py->Partial(partial(_vjp_pullback_wrapper, fun.__name__, ct_dtypes, ct_shapes, (out_tree, in_tree)), out_vjp)
A:jax._src.api.in_avals->map(shaped_abstractify, primals_flat)
A:jax._src.api.in_dtypes->map(dtypes.dtype, in_avals)
A:jax._src.api.in_pvals->map(pe.PartialVal.unknown, in_avals)
A:jax._src.api.(jaxpr, out_pvals, const)->jax._src.interpreters.partial_eval.trace_to_jaxpr_nounits(flat_fun, in_pvals, instantiate=True)
A:jax._src.api.(jaxpr, _)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr, [True] * len(jaxpr.outvars), True)
A:jax._src.api.(out_avals, _)->unzip2(out_type)
A:jax._src.api.out_dtypes->map(dtypes.dtype, out_avals)
A:jax._src.api.(out_cts, out_tree2)->tree_flatten(out_cotangent)
A:jax._src.api.in_cts->map(ad.instantiate_zeros, in_cts)
A:jax._src.api.(flat_args, in_tree)->tree_flatten((args, kwargs))
A:jax._src.api.axes_specs->_flat_axes_specs(abstracted_axes, *args, **kwargs)
A:jax._src.api.in_type->tuple(zip(in_avals, keep_inputs))
A:jax._src.api.(in_avals, keep_inputs)->unzip2(in_type)
A:jax._src.api.(f, args)->argnums_partial(f, dyn_argnums, args)
A:jax._src.api.(in_avals, in_tree, keep_inputs)->abstractify(args, kwargs)
A:jax._src.api.(f, out_tree)->flatten_fun(f, in_tree)
A:jax._src.api.debug_info->jax._src.interpreters.partial_eval.debug_info(fun, in_tree, out_tree, True, 'eval_shape')
A:jax._src.api.(jaxpr, out_type, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic2(f, debug_info=debug_info)
A:jax._src.api.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.api.(x_flat, treedef)->tree_flatten(x)
A:jax._src.api.device_flat->flatten_axes('device_put device', treedef, device)
A:jax._src.api.src_flat->flatten_axes('device_put source', treedef, src)
A:jax._src.api.(a1, a2)->next(((a1, a2) for (a1, a2) in zip(avals[:-1], avals[1:]) if a1 != a2))
A:jax._src.api.stacked_aval->avals[0].update(shape=(len(devices),) + avals[0].shape)
A:jax._src.api.sharding_spec->jax._src.sharding_specs.create_pmap_sharding_spec(aval.shape)
A:jax._src.api.sharding->PmapSharding(np.array(devices), sharding_spec)
A:jax._src.api.buf->device_put(x, devices[0])
A:jax._src.api.self.shape->tuple(shape)
A:jax._src.api.size->property(lambda self: math.prod(self.shape))
A:jax._src.api.named->frozenset(self.named_shape.items())
A:jax._src.api.(wrapped_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
jax.ShapeDtypeStruct(self,shape,dtype,named_shape=None,sharding=None)
jax.ShapeDtypeStruct.__eq__(self,other)
jax.ShapeDtypeStruct.__hash__(self)
jax.ShapeDtypeStruct.__len__(self)
jax.ShapeDtypeStruct.__repr__(self)
jax._src.api.PmapCallInfo(NamedTuple)
jax._src.api.ShapeDtypeStruct(self,shape,dtype,named_shape=None,sharding=None)
jax._src.api.ShapeDtypeStruct.__eq__(self,other)
jax._src.api.ShapeDtypeStruct.__hash__(self)
jax._src.api.ShapeDtypeStruct.__init__(self,shape,dtype,named_shape=None,sharding=None)
jax._src.api.ShapeDtypeStruct.__len__(self)
jax._src.api.ShapeDtypeStruct.__repr__(self)
jax._src.api._PmapFastpathData(NamedTuple)
jax._src.api._check_input_dtype_jacfwd(holomorphic:bool,x:Any)->None
jax._src.api._check_input_dtype_revderiv(name,holomorphic,allow_int,x)
jax._src.api._check_output_dtype_jacfwd(holomorphic,x)
jax._src.api._check_output_dtype_revderiv(name,holomorphic,x)
jax._src.api._check_scalar(x)
jax._src.api._check_sharding(x,s)
jax._src.api._cpp_pmap(fun:Callable,axis_name:AxisName|None=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:int|Iterable[int]=(),devices:Sequence[xc.Device]|None=None,backend:str|None=None,axis_size:int|None=None,donate_argnums:int|Iterable[int]=())->Any
jax._src.api._device_get(x)
jax._src.api._flat_axes_specs(abstracted_axes,*args,**kwargs)->list[pe.AbstractedAxesSpec]
jax._src.api._get_global_axis_size(local_axis_size:int,in_devices,backend_name:str,global_axis_size:int|None)
jax._src.api._infer_src_sharding(src,x)
jax._src.api._jacfwd_unravel(input_pytree,output_pytree_leaf,arr)
jax._src.api._jacrev_unravel(output_pytree,input_pytree_leaf,arr)
jax._src.api._jvp(fun:lu.WrappedFun,primals,tangents,has_aux=False)
jax._src.api._lift_linearized(jaxpr,primal_avals,io_tree,out_pvals,consts,*py_args)
jax._src.api._mapped_axis_size(fn,tree,vals,dims,name)
jax._src.api._nan_check_posthook(fun,args,kwargs,output)
jax._src.api._pmap_lower(fun,axis_name,in_axes,out_axes,static_broadcasted_tuple,devices,backend,axis_size,donate_tuple)
jax._src.api._possible_downcast(x,example)
jax._src.api._prepare_pmap(fun,in_axes,out_axes,static_broadcasted_tuple,donate_tuple,in_devices,backend_name,axis_size,args,kwargs)
jax._src.api._shared_code_pmap(fun,axis_name,static_broadcasted_argnums,donate_argnums,in_axes,out_axes)
jax._src.api._split(x,indices,axis)
jax._src.api._std_basis(pytree)
jax._src.api._unravel_array_into_pytree(pytree,axis,example,arr)
jax._src.api._update_debug_special_global(_)
jax._src.api._update_debug_special_thread_local(_)
jax._src.api._vjp(fun:lu.WrappedFun,*primals,has_aux=False,reduce_axes=())
jax._src.api._vjp_pullback_wrapper(name,cotangent_dtypes,cotangent_shapes,io_tree,fun,*py_args_)
jax._src.api.block_until_ready(x)
jax._src.api.clear_backends()
jax._src.api.clear_caches()
jax._src.api.device_get(x:Any)
jax._src.api.device_put(x,device:None|xc.Device|Sharding|Any|TransferToMemoryKind=None,*,src:None|xc.Device|Sharding|Any|TransferToMemoryKind=None)
jax._src.api.device_put_replicated(x:Any,devices:Sequence[xc.Device])
jax._src.api.device_put_sharded(shards:Sequence[Any],devices:Sequence[xc.Device])
jax._src.api.disable_jit(disable:bool=True)
jax._src.api.effects_barrier()
jax._src.api.eval_shape(fun:Callable,*args,**kwargs)
jax._src.api.grad(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable
jax._src.api.hessian(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax._src.api.jacfwd(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax._src.api.jacrev(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False)->Callable
jax._src.api.jit(fun:Callable,in_shardings=sharding_impls.UNSPECIFIED,out_shardings=sharding_impls.UNSPECIFIED,static_argnums:int|Sequence[int]|None=None,static_argnames:str|Iterable[str]|None=None,donate_argnums:int|Sequence[int]|None=None,donate_argnames:str|Iterable[str]|None=None,keep_unused:bool=False,device:xc.Device|None=None,backend:str|None=None,inline:bool=False,abstracted_axes:Any|None=None)->stages.Wrapped
jax._src.api.jvp(fun:Callable,primals,tangents,has_aux:bool=False)->tuple[Any, ...]
jax._src.api.linear_transpose(fun:Callable,*primals,reduce_axes=())->Callable
jax._src.api.linearize(fun:Callable,*primals,has_aux:bool=False)->tuple[Any, Callable] | tuple[Any, Callable, Any]
jax._src.api.live_arrays(platform=None)
jax._src.api.make_jaxpr(fun:Callable,static_argnums:int|Iterable[int]=(),axis_env:Sequence[tuple[AxisName,int]]|None=None,return_shape:bool=False,abstracted_axes:Any|None=None)->Callable[..., core.ClosedJaxpr | tuple[core.ClosedJaxpr, Any]]
jax._src.api.named_call(fun:F,*,name:str|None=None)->F
jax._src.api.named_scope(name:str)->Generator[None, None, None]
jax._src.api.pmap(fun:Callable,axis_name:AxisName|None=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:int|Iterable[int]=(),devices:Sequence[xc.Device]|None=None,backend:str|None=None,axis_size:int|None=None,donate_argnums:int|Iterable[int]=(),global_arg_shapes:tuple[tuple[int,...],...]|None=None)->Any
jax._src.api.value_and_grad(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable[..., tuple[Any, Any]]
jax._src.api.vjp(fun:Callable,*primals,has_aux:bool=False,reduce_axes=())->tuple[Any, Callable] | tuple[Any, Callable, Any]
jax._src.api.vmap(fun:F,in_axes:int|None|Sequence[Any]=0,out_axes:Any=0,axis_name:AxisName|None=None,axis_size:int|None=None,spmd_axis_name:AxisName|tuple[AxisName,...]|None=None)->F
jax._src.api.xla_computation(fun:Callable,static_argnums:int|Iterable[int]=(),axis_env:Sequence[tuple[AxisName,int]]|None=None,in_parts=None,out_parts=None,backend:str|None=None,tuple_args:bool=False,instantiate_const_outputs:bool|None=None,return_shape:bool=False,donate_argnums:int|Iterable[int]=())->Callable
jax.block_until_ready(x)
jax.clear_backends()
jax.clear_caches()
jax.device_get(x:Any)
jax.device_put(x,device:None|xc.Device|Sharding|Any|TransferToMemoryKind=None,*,src:None|xc.Device|Sharding|Any|TransferToMemoryKind=None)
jax.device_put_replicated(x:Any,devices:Sequence[xc.Device])
jax.device_put_sharded(shards:Sequence[Any],devices:Sequence[xc.Device])
jax.disable_jit(disable:bool=True)
jax.effects_barrier()
jax.eval_shape(fun:Callable,*args,**kwargs)
jax.grad(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable
jax.hessian(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.jacfwd(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.jacrev(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False)->Callable
jax.jit(fun:Callable,in_shardings=sharding_impls.UNSPECIFIED,out_shardings=sharding_impls.UNSPECIFIED,static_argnums:int|Sequence[int]|None=None,static_argnames:str|Iterable[str]|None=None,donate_argnums:int|Sequence[int]|None=None,donate_argnames:str|Iterable[str]|None=None,keep_unused:bool=False,device:xc.Device|None=None,backend:str|None=None,inline:bool=False,abstracted_axes:Any|None=None)->stages.Wrapped
jax.jvp(fun:Callable,primals,tangents,has_aux:bool=False)->tuple[Any, ...]
jax.linear_transpose(fun:Callable,*primals,reduce_axes=())->Callable
jax.linearize(fun:Callable,*primals,has_aux:bool=False)->tuple[Any, Callable] | tuple[Any, Callable, Any]
jax.live_arrays(platform=None)
jax.make_jaxpr(fun:Callable,static_argnums:int|Iterable[int]=(),axis_env:Sequence[tuple[AxisName,int]]|None=None,return_shape:bool=False,abstracted_axes:Any|None=None)->Callable[..., core.ClosedJaxpr | tuple[core.ClosedJaxpr, Any]]
jax.named_call(fun:F,*,name:str|None=None)->F
jax.named_scope(name:str)->Generator[None, None, None]
jax.pmap(fun:Callable,axis_name:AxisName|None=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:int|Iterable[int]=(),devices:Sequence[xc.Device]|None=None,backend:str|None=None,axis_size:int|None=None,donate_argnums:int|Iterable[int]=(),global_arg_shapes:tuple[tuple[int,...],...]|None=None)->Any
jax.value_and_grad(fun:Callable,argnums:int|Sequence[int]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable[..., tuple[Any, Any]]
jax.vjp(fun:Callable,*primals,has_aux:bool=False,reduce_axes=())->tuple[Any, Callable] | tuple[Any, Callable, Any]
jax.vmap(fun:F,in_axes:int|None|Sequence[Any]=0,out_axes:Any=0,axis_name:AxisName|None=None,axis_size:int|None=None,spmd_axis_name:AxisName|tuple[AxisName,...]|None=None)->F
jax.xla_computation(fun:Callable,static_argnums:int|Iterable[int]=(),axis_env:Sequence[tuple[AxisName,int]]|None=None,in_parts=None,out_parts=None,backend:str|None=None,tuple_args:bool=False,instantiate_const_outputs:bool|None=None,return_shape:bool=False,donate_argnums:int|Iterable[int]=())->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/custom_batching.py----------------------------------------
A:jax._src.custom_batching.(args_flat, in_tree)->tree_flatten(args)
A:jax._src.custom_batching.(flat_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(self.fun), in_tree)
A:jax._src.custom_batching.debug->jax._src.interpreters.partial_eval.debug_info(self.fun, in_tree, out_tree, False, 'custom_vmap')
A:jax._src.custom_batching.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals, debug)
A:jax._src.custom_batching.closed_call->jax._src.core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr), ())
A:jax._src.custom_batching.in_tree->treedef_tuple((tree_structure(consts), in_tree))
A:jax._src.custom_batching.out_flat->jax._src.core.Primitive('custom_vmap_call').bind(*consts, *args_flat, call=closed_call, rule=ClosedRule(self.vmap_rule), in_tree=in_tree, out_tree=out_tree())
A:jax._src.custom_batching.(f, out_axes)->jax._src.interpreters.batching.batch_subtrace(f)
A:jax._src.custom_batching.f->custom_vmap(f)
A:jax._src.custom_batching.outs->jax._src.core.Primitive('custom_vmap_call').bind(*primals, *tangents, call=jvp_call, rule=jvp_of_rule_rule, in_tree=jvp_in_tree, out_tree=jvp_out_tree)
A:jax._src.custom_batching.args_flat->map(maybe_bdim_at_front, args_flat, dims)
A:jax._src.custom_batching.args->tree_merge(in_batched, mapped_args, bcast_args)
A:jax._src.custom_batching.in_batched->tree_unflatten(in_tree, flat_in_batched)
A:jax._src.custom_batching.(out, out_batched)->call_rule(rule, axis_size, mutually_batched, primals)
A:jax._src.custom_batching.(flat_outs, tree1)->tree_flatten(out)
A:jax._src.custom_batching.(flat_out_batched, tree2)->tree_flatten(out_batched)
A:jax._src.custom_batching.mutually_batched->tree_map(operator.and_, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.extra_batched_ps->tree_map(lambda pb, tb: 0 if pb and (not tb) else None, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.extra_batched_ts->tree_map(lambda pb, tb: 0 if tb and (not pb) else None, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.out_mutually_batched->jax._src.linear_util.Store()
A:jax._src.custom_batching.(flat_ps_ts, tree_ps_ts)->tree_flatten((primals, tangents))
A:jax._src.custom_batching.(flat_extra_batched_ps_ts, tree_ps_ts2)->tree_flatten((extra_batched_ps, extra_batched_ts), is_leaf=lambda x: x is None)
A:jax._src.custom_batching.(to_vmap_over_extra_batched_dims_flat, out_tree2)->flatten_fun_nokwargs(lu.wrap_init(to_vmap_over_extra_batched_dims), tree_ps_ts)
A:jax._src.custom_batching.(flat_out_ps_ts, flat_out_axes)->vmap_unrestricted(to_vmap_over_extra_batched_dims_flat, *flat_ps_ts, in_axes=flat_extra_batched_ps_ts, axis_name=core.no_axis_name, axis_size=axis_size)
A:jax._src.custom_batching.(n, ragged)->divmod(len(flat_out_ps_ts), 2)
A:jax._src.custom_batching.flat_out_ps->map(maybe_bdim_at_front, flat_out_ps, flat_out_axes_p)
A:jax._src.custom_batching.flat_out_ts->map(maybe_bdim_at_front, flat_out_ts, flat_out_axes_t)
A:jax._src.custom_batching.(out_ps, out_ts)->tree_unflatten(out_tree2(), [*flat_out_ps, *flat_out_ts])
A:jax._src.custom_batching.(out_extra_batched_ps, out_extra_batched_ts)->tree_unflatten(out_tree2(), [*flat_out_extra_batched_ps, *flat_out_extra_batched_ts])
A:jax._src.custom_batching.out_batched_ps->tree_map(operator.or_, out_mutually_batched.val, out_extra_batched_ps)
A:jax._src.custom_batching.out_batched_ts->tree_map(operator.or_, out_mutually_batched.val, out_extra_batched_ts)
A:jax._src.custom_batching.tangents->map(ad.instantiate_zeros, tangents)
A:jax._src.custom_batching.(jvp_call, _)->jax._src.interpreters.ad.jvp_jaxpr(call, [True] * len(primals), True)
A:jax._src.custom_batching.jvp_in_tree->treedef_tuple((in_tree, in_tree))
A:jax._src.custom_batching.jvp_out_tree->treedef_tuple((out_tree, out_tree))
A:jax._src.custom_batching.(out_primals, out_tangents)->jax._src.util.split_list(outs, [len(outs) // 2])
A:jax._src.custom_batching.custom_vmap_p->jax._src.core.Primitive('custom_vmap_call')
A:jax._src.custom_batching.lhs->tree_map(lambda l, x: x if l else None, mask, tree)
A:jax._src.custom_batching.rhs->tree_map(lambda l, x: None if l else x, mask, tree)
A:jax._src.custom_batching.(mapped_args, bcast_args)->tree_split(in_batched, list(args))
A:jax._src.custom_batching.out->jax.lax.map(to_map, mapped_args)
A:jax._src.custom_batching.out_batched->tree_map(lambda _: True, out)
jax._src.custom_batching.ClosedRule(self,rule)
jax._src.custom_batching.ClosedRule.__init__(self,rule)
jax._src.custom_batching.ClosedRule.__str__(self)
jax._src.custom_batching.call_rule(rule,axis_size,in_batched,args)
jax._src.custom_batching.check_vmap_rule_trees(rule,original_out_tree,out_tree,out_batched_tree)
jax._src.custom_batching.custom_vmap(self,fun:Callable)
jax._src.custom_batching.custom_vmap.__init__(self,fun:Callable)
jax._src.custom_batching.custom_vmap.def_vmap(self,vmap_rule:Callable)->Callable
jax._src.custom_batching.custom_vmap_abstract_eval(*in_avals,call,**_)
jax._src.custom_batching.custom_vmap_batching(args_flat,dims,*,call,rule,in_tree,out_tree)
jax._src.custom_batching.custom_vmap_impl(*args,call,rule,in_tree,out_tree)
jax._src.custom_batching.custom_vmap_jvp(primals,tangents,*,call,rule,in_tree,out_tree)
jax._src.custom_batching.ensure_list(xs)
jax._src.custom_batching.maybe_bdim_at_front(x,bdim)
jax._src.custom_batching.rule_name(rule)
jax._src.custom_batching.sequential_vmap(f)
jax._src.custom_batching.tree_merge(mask,lhs_tree,rhs_tree)
jax._src.custom_batching.tree_split(mask,tree)
jax._src.custom_batching.vmap_unrestricted(f:lu.WrappedFun,*args,in_axes,axis_name,axis_size)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax_reference.py----------------------------------------
A:jax._src.lax_reference.quotient->numpy.floor_divide(lhs, rhs)
A:jax._src.lax_reference.select->numpy.logical_and(np.sign(lhs) != np.sign(rhs), np.remainder(lhs, rhs) != 0)
A:jax._src.lax_reference.iinfo->numpy.iinfo(x.dtype)
A:jax._src.lax_reference.x->x.view(f'uint{np.iinfo(x.dtype).bits}').view(f'uint{np.iinfo(x.dtype).bits}')
A:jax._src.lax_reference.m->list(map(np.uint32, m[:-1]))
A:jax._src.lax_reference.bits->(x[..., None] & mask).astype(np.bool_)
A:jax._src.lax_reference.out->numpy.full(operand.shape[:2] + tuple(outspace), fill_value, operand.dtype)
A:jax._src.lax_reference.pads->padtype_to_pads(op.shape, dims, strides, padding)
A:jax._src.lax_reference.(lhs_perm, rhs_perm, out_perm)->_conv_general_permutations(dimension_numbers)
A:jax._src.lax_reference.padding->padtype_to_pads(np.take(lhs.shape, lhs_perm)[2:], np.take(rhs.shape, rhs_perm)[2:], window_strides, padding)
A:jax._src.lax_reference.trans_lhs->transpose(lhs, lhs_perm)
A:jax._src.lax_reference.trans_rhs->transpose(rhs, rhs_perm)
A:jax._src.lax_reference.new_id->itertools.count()
A:jax._src.lax_reference.shared_id->next(new_id)
A:jax._src.lax_reference.out_axis_ids->filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)
A:jax._src.lax_reference.in_reshape->numpy.ones(len(shape), dtype=np.int32)
A:jax._src.lax_reference.dimensions->frozenset(dimensions)
A:jax._src.lax_reference.(lo, hi, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax_reference.outshape->numpy.add(np.add(np.add(lo_pos, hi_pos), operand.shape), np.multiply(interior, np.subtract(operand.shape, 1)))
A:jax._src.lax_reference.lhs_slices->tuple((_slice(None, None, step) for step in factors))
A:jax._src.lax_reference.trim_slices->tuple((_slice(-l if l < 0 else 0, h if h < 0 else None) for (l, h) in zip(lo, hi)))
A:jax._src.lax_reference.strides->numpy.ones(len(start_indices)).astype(int)
A:jax._src.lax_reference.slices->tuple((_slice(abs(lo) if lo < 0 else 0, hi % dim if hi < 0 else None) for ((lo, hi), dim) in zip(pads, np.shape(arr))))
A:jax._src.lax_reference.idx->tuple((_slice(start, start + size) for (start, size) in zip(start_indices, slice_sizes)))
A:jax._src.lax_reference.updated_operand->numpy.copy(operand)
A:jax._src.lax_reference.reducer->_make_reducer(computation, init_value)
A:jax._src.lax_reference.op->_dilate(op, base_dilation, init_value)
A:jax._src.lax_reference.view->numpy.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)
A:jax._src.lax_reference.idxs->list(np.ix_(*[np.arange(d) for d in keys.shape]))
A:jax._src.lax_reference.idxs[dimension]->numpy.argsort(keys, axis=dimension)
A:jax._src.lax_reference.(view, view_axes, rhs_axes, out_axes)->_conv_view(lhs, rhs.shape, window_strides, pads, 0.0)
A:jax._src.lax_reference.out_shape->numpy.ceil(np.true_divide(in_shape, window_strides)).astype(int)
A:jax._src.lax_reference.lhs->_pad(lhs, [(0, 0)] * 2 + list(pads), pad_value)
A:jax._src.lax_reference.dim->len(filter_shape)
A:jax._src.lax_reference.out_strides->numpy.multiply(window_strides, lhs.strides[2:])
A:jax._src.lax_reference.view_axes->list(range(view.ndim))
A:jax._src.lax_reference.outspace->numpy.add(operand.shape[2:], np.multiply(np.subtract(factors, 1), np.subtract(operand.shape[2:], 1)))
A:jax._src.lax_reference.monoid_record->_monoids.get(getattr(py_binop, '__name__'))
A:jax._src.lax_reference.MonoidRecord->collections.namedtuple('MonoidRecord', ['reducer', 'identity'])
A:jax._src.lax_reference.result->numpy.full(np.delete(np.shape(operand), axis), init_val, dtype=np.asarray(operand).dtype)
A:jax._src.lax_reference.out_idx->tuple(np.delete(idx, axis))
A:jax._src.lax_reference.result[out_idx]->py_binop(result[out_idx], operand[idx])
jax._src.lax_reference._conv(lhs,rhs,window_strides,pads)
jax._src.lax_reference._conv_general_permutations(dimension_numbers)
jax._src.lax_reference._conv_view(lhs,rhs_shape,window_strides,pads,pad_value)
jax._src.lax_reference._dilate(operand,factors,fill_value=0)
jax._src.lax_reference._get_max_identity(dt)
jax._src.lax_reference._get_min_identity(dt)
jax._src.lax_reference._identity_getter(op)
jax._src.lax_reference._make_reducer(py_binop,init_val)
jax._src.lax_reference._pad(arr,pads,pad_value)
jax._src.lax_reference._reducer_from_pyfunc(py_binop,init_val)
jax._src.lax_reference.bessel_i0e(x)
jax._src.lax_reference.bessel_i1e(x)
jax._src.lax_reference.betainc(a,b,x)
jax._src.lax_reference.bitcast_convert_type(operand,dtype)
jax._src.lax_reference.broadcast(operand,sizes)
jax._src.lax_reference.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax._src.lax_reference.clamp(min,operand,max)
jax._src.lax_reference.clz(x)
jax._src.lax_reference.complex(x,y)
jax._src.lax_reference.concatenate(operands,dimension)
jax._src.lax_reference.conj(x)
jax._src.lax_reference.conv(lhs,rhs,window_strides,padding)
jax._src.lax_reference.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers)
jax._src.lax_reference.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation)
jax._src.lax_reference.convert_element_type(operand,dtype)
jax._src.lax_reference.digamma(x)
jax._src.lax_reference.div(lhs,rhs)
jax._src.lax_reference.dot_general(lhs,rhs,dimension_numbers)
jax._src.lax_reference.dynamic_slice(operand,start_indices,slice_sizes)
jax._src.lax_reference.dynamic_update_slice(operand,update,start_indices)
jax._src.lax_reference.erf(x)
jax._src.lax_reference.erf_inv(x)
jax._src.lax_reference.erfc(x)
jax._src.lax_reference.lgamma(x)
jax._src.lax_reference.logistic(x)
jax._src.lax_reference.pad(operand,padding_value,padding_config)
jax._src.lax_reference.padtype_to_pads(in_shape,filter_shape,window_strides,padding)
jax._src.lax_reference.population_count(x)
jax._src.lax_reference.reduce(operand,init_value,computation,dimensions)
jax._src.lax_reference.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding,base_dilation)
jax._src.lax_reference.rem(lhs,rhs)
jax._src.lax_reference.reshape(operand,new_sizes,dimensions=None)
jax._src.lax_reference.rev(operand,dimensions)
jax._src.lax_reference.round(x)
jax._src.lax_reference.slice(operand,start_indices,limit_indices,strides=None)
jax._src.lax_reference.sort_key_val(keys,values,dimension=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/deprecations.py----------------------------------------
jax._deprecation_getattr(module,deprecations)
jax._src.deprecations.deprecation_getattr(module,deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/compiler.py----------------------------------------
A:jax._src.compiler._DISABLE_MOST_OPTIMIZATIONS->jax._src.config.DEFINE_bool('jax_disable_most_optimizations', config.bool_env('JAX_DISABLE_MOST_OPTIMIZATIONS', False), 'Try not to do much optimization work. This can be useful if the cost of optimization is greater than that of running a less-optimized program.')
A:jax._src.compiler._DUMP_IR_TO->jax._src.config.DEFINE_string('jax_dump_ir_to', os.getenv('JAX_DUMP_IR_TO', ''), help='Path to which the IR that is emitted by JAX as input to the compiler should be dumped as text files. Optional. If omitted, JAX will not dump IR.')
A:jax._src.compiler._COMPILER_DETAILED_LOGGING_MIN_OPS->jax._src.config.DEFINE_integer('jax_compiler_detailed_logging_min_ops', config.int_env('JAX_COMPILER_DETAILED_LOGGING_MIN_OPS', 10), help='How big should a module be in MLIR operations before JAX enables detailed compiler logging? The intent of this flag is to suppress detailed logging for small/uninteresting computations.')
A:jax._src.compiler.logger->logging.getLogger(__name__)
A:jax._src.compiler.k->_walk_operations(child_op, k)
A:jax._src.compiler.compile_options->jax._src.lib.xla_client.CompileOptions()
A:jax._src.compiler.device_assignment->jax._src.lib.xla_client.DeviceAssignment.create(device_assignment)
A:jax._src.compiler.compile_options.env_option_overrides->list(env_options_overrides.items())
A:jax._src.compiler.fdo_profile_version->get_latest_profile_version()
A:jax._src.compiler.output->io.BytesIO()
A:jax._src.compiler.built_c->_module_to_bytecode(module)
A:jax._src.compiler._ir_dump_counter->itertools.count()
A:jax._src.compiler.id->next(_ir_dump_counter)
A:jax._src.compiler.out_dir->jax._src.path.Path(_DUMP_IR_TO.value)
A:jax._src.compiler.cache_key->jax._src.compilation_cache.get_cache_key(computation, devices, compile_options, backend, config.use_original_compilation_cache_key_generation.value)
A:jax._src.compiler.cache_retrieval_start->time.monotonic()
A:jax._src.compiler.(retrieved_executable, retrieved_compile_time)->_cache_read(module_name, cache_key, compile_options, backend)
A:jax._src.compiler.start_time->time.monotonic()
A:jax._src.compiler.executable->backend_compile(backend, computation, compile_options, host_callbacks)
jax._src.compiler._cache_read(module_name:str,cache_key:str,compile_options:xc.CompileOptions,backend:xc.Client)->tuple[xc.LoadedExecutable | None, int | None]
jax._src.compiler._cache_write(cache_key:str,compile_time_secs:float,module_name:str,backend:xc.Client,executable:xc.LoadedExecutable,host_callbacks:Sequence[Any])->None
jax._src.compiler._dump_ir_to_file(name:str,ir:str)
jax._src.compiler._make_string_safe_for_filename(s:str)->str
jax._src.compiler._module_to_bytecode(module:ir.Module)->bytes
jax._src.compiler._module_to_string(module:ir.Module)->str
jax._src.compiler._walk_operations(op,k)
jax._src.compiler.backend_compile(backend:xc.Client,module:ir.Module,options:xc.CompileOptions,host_callbacks:Sequence[Any])->xc.LoadedExecutable
jax._src.compiler.compile_or_get_cached(backend:xc.Client,computation:ir.Module,devices:np.ndarray,compile_options:xc.CompileOptions,host_callbacks:Sequence[Any])->xc.LoadedExecutable
jax._src.compiler.get_compile_options(num_replicas:int,num_partitions:int,device_assignment=None,use_spmd_partitioning:bool=True,use_auto_spmd_partitioning:bool=False,auto_spmd_partitioning_mesh_shape:list[int]|None=None,auto_spmd_partitioning_mesh_ids:list[int]|None=None,env_options_overrides:dict[str,str]|None=None,fdo_profile:bytes|None=None,detailed_logging:bool=True)->xc.CompileOptions
jax._src.compiler.get_latest_profile_version()->int
jax._src.compiler.use_detailed_logging(module:ir.Module)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/compilation_cache_interface.py----------------------------------------
jax._src.compilation_cache_interface.CacheInterface(ABC)
jax._src.compilation_cache_interface.CacheInterface.get(self,key:str)
jax._src.compilation_cache_interface.CacheInterface.put(self,key:str,value:bytes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/partition_spec.py----------------------------------------
A:jax._src.partition_spec._UNCONSTRAINED_PARTITION->_UnconstrainedPartitionSingleton()
jax._src.partition_spec.PartitionSpec(self,*partitions)
jax._src.partition_spec.PartitionSpec.__init__(self,*partitions)
jax._src.partition_spec.PartitionSpec.__reduce__(self)
jax._src.partition_spec.PartitionSpec.__repr__(self)
jax._src.partition_spec._UnconstrainedPartitionSingleton
jax._src.partition_spec._UnconstrainedPartitionSingleton.__str__(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/prng.py----------------------------------------
A:jax._src.prng.ndim->len(impl.key_shape)
A:jax._src.prng._->self._base_array.copy_to_host_async()
A:jax._src.prng._device->property(op.attrgetter('_base_array._device'))
A:jax._src.prng._committed->property(op.attrgetter('_base_array._committed'))
A:jax._src.prng.device->property(op.attrgetter('_base_array.device'))
A:jax._src.prng.devices->property(op.attrgetter('_base_array.devices'))
A:jax._src.prng.is_fully_addressable->property(op.attrgetter('_base_array.is_fully_addressable'))
A:jax._src.prng.is_fully_replicated->property(op.attrgetter('_base_array.is_fully_replicated'))
A:jax._src.prng.delete->property(op.attrgetter('_base_array.delete'))
A:jax._src.prng.is_deleted->property(op.attrgetter('_base_array.is_deleted'))
A:jax._src.prng.on_device_size_in_bytes->property(op.attrgetter('_base_array.on_device_size_in_bytes'))
A:jax._src.prng.unsafe_buffer_pointer->property(op.attrgetter('_base_array.unsafe_buffer_pointer'))
A:jax._src.prng.base_ndim->len(impl.key_shape)
A:jax._src.prng.api_util._shaped_abstractify_handlers[PRNGKeyArrayImpl]->operator.attrgetter('aval')
A:jax._src.prng.phys_sharding_spec->jax._src.sharding_specs.ShardingSpec(sharding=(*sharding.sharding_spec.sharding, *trailing_sharding), mesh_mapping=sharding.sharding_spec.mesh_mapping)
A:jax._src.prng.hlos->sharding._to_xla_hlo_sharding(aval.ndim)
A:jax._src.prng.key_data->jax.lax.full(physical_shape, fill_value, dtype=np.dtype('uint32'))
A:jax._src.prng.op_sharding_proto->hlo_sharding.to_proto()
A:jax._src.prng.new_op_sharding->hlo_sharding.to_proto().clone()
A:jax._src.prng.tad->list(logical_op_sharding.tile_assignment_dimensions)
A:jax._src.prng.logical_sharding_spec->jax._src.sharding_specs.ShardingSpec(sharding=phys_sharding.sharding_spec.sharding[:-len(key_shape)], mesh_mapping=phys_sharding.sharding_spec.mesh_mapping)
A:jax._src.prng.phys_op_sharding->make_key_array_phys_sharding(aval, sharding, is_sharding_from_xla=False)._to_xla_hlo_sharding(aval.ndim + len(key_shape)).to_proto()
A:jax._src.prng.logical_op_sharding->make_key_array_phys_sharding(aval, sharding, is_sharding_from_xla=False)._to_xla_hlo_sharding(aval.ndim + len(key_shape)).to_proto().clone()
A:jax._src.prng.buf.aval->jax._src.core.ShapedArray(buf.shape, buf.dtype)
A:jax._src.prng.phys_aval->jax._src.core.physical_aval(aval)
A:jax._src.prng.phys_sharding->make_key_array_phys_sharding(aval, sharding, is_sharding_from_xla=False)
A:jax._src.prng.phys_handler->phys_handler_maker(phys_aval, phys_sharding, committed, False)
A:jax._src.prng.phys_result->phys_handler(phys_arrays)
A:jax._src.prng.physical_aval->jax._src.core.physical_aval(aval)
A:jax._src.prng.physical_buffers->jax.tree_util.tree_map(random_unwrap, vals)
A:jax._src.prng.physical_sharding->make_key_array_phys_sharding(aval, sharding, False)
A:jax._src.prng.physical_result->jax._src.interpreters.pxla.batched_device_put(physical_aval, physical_sharding, [physical_buf] * len(devices), devices)
A:jax._src.prng.physical_buf->random_unwrap(val)
A:jax._src.prng.f->squeeze_vmap(f, sz1 == 1)
A:jax._src.prng.x->jax._src.core.Primitive('threefry2x32').bind(key1, key2, x[0], x[1])
A:jax._src.prng.y->jax.numpy.squeeze(y, axis=0)
A:jax._src.prng.seeds_arr->jax.numpy.asarray(seeds)
A:jax._src.prng.random_seed_p->jax._src.core.Primitive('random_seed')
A:jax._src.prng.base_arr->random_fold_in_impl_base(keys._impl, keys._base_array, msgs, keys.shape)
A:jax._src.prng.seed->iterated_vmap_unary(aval.ndim, impl.seed)
A:jax._src.prng.seed_lowering->jax._src.interpreters.mlir.lower_fun(seed, multiple_results=False)
A:jax._src.prng.random_split_p->jax._src.core.Primitive('random_split')
A:jax._src.prng.split->iterated_vmap_unary(aval.ndim, lambda k: impl.split(k, shape))
A:jax._src.prng.split_lowering->jax._src.interpreters.mlir.lower_fun(split, multiple_results=False)
A:jax._src.prng.random_fold_in_p->jax._src.core.Primitive('random_fold_in')
A:jax._src.prng.shape->tuple(unsafe_map(core.concrete_dim_or_error, shape))
A:jax._src.prng.named_shape->jax._src.core.join_named_shapes(*(a.named_shape for a in args))
A:jax._src.prng.fold_in->iterated_vmap_binary_bcast(keys_aval.shape, msgs_aval.shape, impl.fold_in)
A:jax._src.prng.fold_in_lowering->jax._src.interpreters.mlir.lower_fun(fold_in, multiple_results=False)
A:jax._src.prng.real_size->jax.lax.psum(1, name)
A:jax._src.prng.axis_index->jax.lax.axis_index(name)
A:jax._src.prng.keys->threefry_split(key, (nblocks + 1,))
A:jax._src.prng.random_bits_p->jax._src.core.Primitive('random_bits')
A:jax._src.prng.out_dtype->jax._src.dtypes.dtype(f'uint{bit_width}')
A:jax._src.prng.bits->jax.lax.reshape(bits, (max_count * 32 // bit_width,), (1, 0))
A:jax._src.prng.bits_lowering->jax._src.interpreters.mlir.lower_fun(bits, multiple_results=False)
A:jax._src.prng.ctx_new->ctx.replace(avals_in=[core.physical_aval(aval)])
A:jax._src.prng.out->jax.numpy.concatenate(x)
A:jax._src.prng.random_wrap_p->jax._src.core.Primitive('random_wrap')
A:jax._src.prng.random_unwrap_p->jax._src.core.Primitive('random_unwrap')
A:jax._src.prng.k1->convert(lax.shift_right_logical(seed, lax_internal._const(seed, 32)))
A:jax._src.prng.k2->convert(jnp.bitwise_and(seed, np.uint32(4294967295)))
A:jax._src.prng.nbits->numpy.array(jnp.iinfo(dtype).bits, dtype)
A:jax._src.prng.d->jax.lax.convert_element_type(d, dtype)
A:jax._src.prng.aval->jax._src.core.UnshapedArray(jnp.dtype(jnp.uint32))
A:jax._src.prng.rotate_left->_make_rotate_left(np.uint32)
A:jax._src.prng.v[1]->rotate_left(v[1], rot)
A:jax._src.prng.(x, _, _)->jax.lax.fori_loop(0, 5, rolled_loop_step, (x, rotate_list(ks), rotations))
A:jax._src.prng.rank->len(aval_out.shape)
A:jax._src.prng.zeros->jax._src.interpreters.mlir.full_like_aval(ctx, 0, aval_out)
A:jax._src.prng.out_len->reduce(op.mul, aval_out.shape, 1)
A:jax._src.prng.length->int(out_len)
A:jax._src.prng.output_shape->jax._src.interpreters.mlir.eval_dynamic_shape_as_tensor(ctx, aval_out.shape)
A:jax._src.prng.threefry2x32_p->jax._src.core.Primitive('threefry2x32')
A:jax._src.prng.iota_2x32_shape_p->jax._src.core.Primitive('iota_2x32_shape')
A:jax._src.prng.aval_u64->jax._src.core.ShapedArray(shape, np.dtype('uint64'))
A:jax._src.prng.x_const->jax._src.interpreters.mlir.ir_constant(np.array(x, np.dtype('uint64')))
A:jax._src.prng.(x_const,)->jax._src.interpreters.mlir.eval_dynamic_shape(ctx, (x,))
A:jax._src.prng.x_bcast->jax._src.interpreters.mlir.broadcast_in_dim(ctx, x_const, aval_u64, broadcast_dimensions=[])
A:jax._src.prng.counts->jax.lax.iota(np.uint32, num * 2)
A:jax._src.prng.shift->jax._src.interpreters.mlir.broadcast_in_dim(ctx, shift, aval_u64, broadcast_dimensions=[])
A:jax._src.prng.num->math.prod(shape)
A:jax._src.prng.(counts1, counts2)->iota_2x32_shape(shape)
A:jax._src.prng.(bits1, bits2)->jax._src.core.Primitive('threefry2x32').bind(k1, k2, counts1, counts2)
A:jax._src.prng.bits_hi->jax.lax.convert_element_type(bits1, dtype)
A:jax._src.prng.bits_lo->jax.lax.convert_element_type(bits2, dtype)
A:jax._src.prng.size->math.prod(shape)
A:jax._src.prng.(max_count, r)->divmod(bit_width * size, 32)
A:jax._src.prng.(nblocks, rem)->divmod(max_count, jnp.iinfo(np.uint32).max)
A:jax._src.prng.blocks->vmap(threefry_2x32, in_axes=(0, None))(subkeys, lax.iota(np.uint32, jnp.iinfo(np.uint32).max))
A:jax._src.prng.last->threefry_2x32(last_key, lax.iota(np.uint32, rem))
A:jax._src.prng.threefry_prng_impl->PRNGImpl(key_shape=(2,), seed=threefry_seed, split=threefry_split, random_bits=threefry_random_bits, fold_in=threefry_fold_in, name='threefry2x32', tag='fry')
A:jax._src.prng.halfkey->threefry_seed(seed)
A:jax._src.prng.halfkeys->key.reshape(2, 2)
A:jax._src.prng.(_, bits)->jax.lax.rng_bit_generator(key, shape, dtype=UINT_DTYPES[bit_width])
A:jax._src.prng.rbg_prng_impl->PRNGImpl(key_shape=(4,), seed=_rbg_seed, split=_rbg_split, random_bits=_rbg_random_bits, fold_in=_rbg_fold_in, name='rbg', tag='rbg')
A:jax._src.prng.(_, keys)->jax.lax.rng_bit_generator(key, (10 * num, 4), dtype='uint32')
A:jax._src.prng.(_, random_bits)->jax.lax.rng_bit_generator(_rbg_seed(data), (10, 4), dtype='uint32')
A:jax._src.prng.unsafe_rbg_prng_impl->PRNGImpl(key_shape=(4,), seed=_rbg_seed, split=_unsafe_rbg_split, random_bits=_rbg_random_bits, fold_in=_unsafe_rbg_fold_in, name='unsafe_rbg', tag='urbg')
jax._src.prng.KeyTy(self,impl)
jax._src.prng.KeyTy.__eq__(self,other)
jax._src.prng.KeyTy.__hash__(self)->int
jax._src.prng.KeyTy.__init__(self,impl)
jax._src.prng.KeyTy.__repr__(self)->str
jax._src.prng.KeyTy.itemsize(self)->int
jax._src.prng.KeyTy.name(self)->str
jax._src.prng.KeyTyRules
jax._src.prng.KeyTyRules.device_put_replicated(val,aval,sharding,devices)
jax._src.prng.KeyTyRules.device_put_sharded(vals,aval,sharding,devices)
jax._src.prng.KeyTyRules.full(shape,fill_value,dtype)
jax._src.prng.KeyTyRules.global_sharded_result_handler(aval,out_sharding,committed,is_out_sharding_from_xla)
jax._src.prng.KeyTyRules.local_sharded_result_handler(aval,sharding,indices)
jax._src.prng.KeyTyRules.logical_op_sharding(aval,phys_sharding)->XLACompatibleSharding
jax._src.prng.KeyTyRules.make_sharded_array(aval,sharding,arrays,committed)
jax._src.prng.KeyTyRules.physical_const(val)->Array
jax._src.prng.KeyTyRules.physical_element_aval(dtype)->core.ShapedArray
jax._src.prng.KeyTyRules.physical_hlo_sharding(aval,hlo_sharding:xc.HloSharding)->xc.HloSharding
jax._src.prng.KeyTyRules.result_handler(sticky_device,aval)
jax._src.prng.PRNGImpl(NamedTuple)
jax._src.prng.PRNGImpl.__hash__(self)->int
jax._src.prng.PRNGImpl.__str__(self)->str
jax._src.prng.PRNGImpl.pprint(self)
jax._src.prng.PRNGKeyArray(jax.Array,metaclass=PRNGKeyArrayMeta)
jax._src.prng.PRNGKeyArray.T(self)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.__getitem__(self,_)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.__iter__(self)->Iterator[PRNGKeyArray]
jax._src.prng.PRNGKeyArray.__len__(self)->int
jax._src.prng.PRNGKeyArray.addressable_data(self,index:int)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.addressable_shards(self)->list[Shard]
jax._src.prng.PRNGKeyArray.at(self)->_IndexUpdateHelper
jax._src.prng.PRNGKeyArray.block_until_ready(self)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.copy_to_host_async(self)->None
jax._src.prng.PRNGKeyArray.delete(self)->None
jax._src.prng.PRNGKeyArray.device(self)->Device
jax._src.prng.PRNGKeyArray.devices(self)->set[Device]
jax._src.prng.PRNGKeyArray.dtype(self)
jax._src.prng.PRNGKeyArray.flatten(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.global_shards(self)->list[Shard]
jax._src.prng.PRNGKeyArray.is_deleted(self)->bool
jax._src.prng.PRNGKeyArray.is_fully_addressable(self)->bool
jax._src.prng.PRNGKeyArray.is_fully_replicated(self)->bool
jax._src.prng.PRNGKeyArray.itemsize(self)
jax._src.prng.PRNGKeyArray.ndim(self)->int
jax._src.prng.PRNGKeyArray.on_device_size_in_bytes(self)->int
jax._src.prng.PRNGKeyArray.ravel(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.reshape(self,*args,order='C')->PRNGKeyArray
jax._src.prng.PRNGKeyArray.shape(self)->tuple[int, ...]
jax._src.prng.PRNGKeyArray.sharding(self)
jax._src.prng.PRNGKeyArray.size(self)->int
jax._src.prng.PRNGKeyArray.squeeze(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.swapaxes(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.take(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.transpose(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArray.unsafe_buffer_pointer(self)->int
jax._src.prng.PRNGKeyArrayImpl(self,impl,key_data:Any)
jax._src.prng.PRNGKeyArrayImpl.T(self)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.__getitem__(self,_)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.__init__(self,impl,key_data:Any)
jax._src.prng.PRNGKeyArrayImpl.__iter__(self)->Iterator[PRNGKeyArrayImpl]
jax._src.prng.PRNGKeyArrayImpl.__len__(self)
jax._src.prng.PRNGKeyArrayImpl.__repr__(self)
jax._src.prng.PRNGKeyArrayImpl._is_scalar(self)
jax._src.prng.PRNGKeyArrayImpl.addressable_data(self,index:int)->PRNGKeyArrayImpl
jax._src.prng.PRNGKeyArrayImpl.addressable_shards(self)->list[Shard]
jax._src.prng.PRNGKeyArrayImpl.at(self)->_IndexUpdateHelper
jax._src.prng.PRNGKeyArrayImpl.aval(self)
jax._src.prng.PRNGKeyArrayImpl.block_until_ready(self)
jax._src.prng.PRNGKeyArrayImpl.copy(self)
jax._src.prng.PRNGKeyArrayImpl.copy_to_host_async(self)
jax._src.prng.PRNGKeyArrayImpl.dtype(self)
jax._src.prng.PRNGKeyArrayImpl.flatten(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.global_shards(self)->list[Shard]
jax._src.prng.PRNGKeyArrayImpl.itemsize(self)
jax._src.prng.PRNGKeyArrayImpl.ndim(self)
jax._src.prng.PRNGKeyArrayImpl.pprint(self)
jax._src.prng.PRNGKeyArrayImpl.ravel(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.reshape(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.shape(self)
jax._src.prng.PRNGKeyArrayImpl.sharding(self)
jax._src.prng.PRNGKeyArrayImpl.size(self)
jax._src.prng.PRNGKeyArrayImpl.squeeze(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.swapaxes(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.take(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.transpose(self,*_,**__)->PRNGKeyArray
jax._src.prng.PRNGKeyArrayImpl.unsafe_raw_array(self)
jax._src.prng.PRNGKeyArrayMeta(abc.ABCMeta)
jax._src.prng.PRNGKeyArrayMeta.__instancecheck__(cls,instance)
jax._src.prng._check_prng_key_data(impl,key_data:typing.Array)
jax._src.prng._is_threefry_prng_key(key:typing.Array)->bool
jax._src.prng._make_rotate_left(dtype)
jax._src.prng._rbg_fold_in(key:typing.Array,data:typing.Array)->typing.Array
jax._src.prng._rbg_random_bits(key:typing.Array,bit_width:int,shape:Sequence[int])->typing.Array
jax._src.prng._rbg_seed(seed:typing.Array)->typing.Array
jax._src.prng._rbg_split(key:typing.Array,shape:Shape)->typing.Array
jax._src.prng._threefry2x32_abstract_eval(*args)
jax._src.prng._threefry2x32_gpu_lowering(lowering_func,ctx,k1,k2,x1,x2)
jax._src.prng._threefry2x32_lowering(key1,key2,x1,x2,use_rolled_loops=True)
jax._src.prng._threefry_fold_in(key,data)
jax._src.prng._threefry_random_bits_original(key:typing.Array,bit_width,shape)
jax._src.prng._threefry_random_bits_partitionable(key:typing.Array,bit_width,shape)
jax._src.prng._threefry_seed(seed:typing.Array)->typing.Array
jax._src.prng._threefry_split(key,shape)->typing.Array
jax._src.prng._threefry_split_foldlike(key,shape)->typing.Array
jax._src.prng._threefry_split_original(key,shape)->typing.Array
jax._src.prng._unsafe_rbg_fold_in(key:typing.Array,data:typing.Array)->typing.Array
jax._src.prng._unsafe_rbg_split(key:typing.Array,shape:Shape)->typing.Array
jax._src.prng.apply_round(v,rot)
jax._src.prng.base_arr_shape_to_keys_shape(impl,base_arr_shape)
jax._src.prng.bcast_iotas_to_reshaped_iota(add:Callable[[ir.Value,ir.Value],ir.Value],mul:Callable[[core.DimSize,ir.Value],ir.Value],shape:core.Shape,iotas:Sequence[ir.Value])->ir.Value
jax._src.prng.iota_2x32_shape(shape)
jax._src.prng.iota_2x32_shape_abstract_eval(*,shape)
jax._src.prng.iota_2x32_shape_lowering(ctx,*,shape)
jax._src.prng.iterated_vmap_binary_bcast(shape1,shape2,f)
jax._src.prng.iterated_vmap_unary(n,f)
jax._src.prng.key_array_constant_handler(x)
jax._src.prng.key_array_shard_arg_handler(x:PRNGKeyArrayImpl,devices,indices,sharding)
jax._src.prng.keys_shaped_array(impl,shape)
jax._src.prng.make_key_array_phys_sharding(aval,sharding,is_sharding_from_xla)
jax._src.prng.prngkeyarrayimpl_flatten(x)
jax._src.prng.prngkeyarrayimpl_unflatten(impl,children)
jax._src.prng.random_bits(keys,bit_width,shape)
jax._src.prng.random_bits_abstract_eval(keys_aval,*,bit_width,shape)
jax._src.prng.random_bits_impl(keys,*,bit_width,shape)
jax._src.prng.random_bits_impl_base(impl,base_arr,keys_ndim,*,bit_width,shape)
jax._src.prng.random_bits_lowering(ctx,keys,*,bit_width,shape)
jax._src.prng.random_fold_in(keys,msgs)
jax._src.prng.random_fold_in_abstract_eval(keys_aval,msgs_aval)
jax._src.prng.random_fold_in_impl(keys,msgs)
jax._src.prng.random_fold_in_impl_base(impl,base_arr,msgs,keys_shape)
jax._src.prng.random_fold_in_lowering(ctx,keys,msgs)
jax._src.prng.random_seed(seeds:int|typing.ArrayLike,impl:PRNGImpl)->PRNGKeyArrayImpl
jax._src.prng.random_seed_abstract_eval(seeds_aval,*,impl)
jax._src.prng.random_seed_impl(seeds,*,impl)
jax._src.prng.random_seed_impl_base(seeds,*,impl)
jax._src.prng.random_seed_lowering(ctx,seeds,*,impl)
jax._src.prng.random_split(keys,shape:Shape)
jax._src.prng.random_split_abstract_eval(keys_aval,*,shape)
jax._src.prng.random_split_impl(keys,*,shape)
jax._src.prng.random_split_impl_base(impl,base_arr,keys_ndim,*,shape)
jax._src.prng.random_split_lowering(ctx,keys,*,shape)
jax._src.prng.random_unwrap(keys)
jax._src.prng.random_unwrap_abstract_eval(keys_aval)
jax._src.prng.random_unwrap_impl(keys)
jax._src.prng.random_unwrap_lowering(ctx,keys)
jax._src.prng.random_wrap(base_arr,*,impl)
jax._src.prng.random_wrap_abstract_eval(base_arr_aval,*,impl)
jax._src.prng.random_wrap_batch_rule(batched_args,batch_dims,*,impl)
jax._src.prng.random_wrap_impl(base_arr,*,impl)
jax._src.prng.random_wrap_lowering(ctx,base_arr,*,impl)
jax._src.prng.register_prng(impl:PRNGImpl)
jax._src.prng.rolled_loop_step(i,state)
jax._src.prng.rotate_list(xs)
jax._src.prng.seed_with_impl(impl:PRNGImpl,seed:int|typing.ArrayLike)->PRNGKeyArrayImpl
jax._src.prng.squeeze_vmap(f,left)
jax._src.prng.threefry_2x32(keypair,count)
jax._src.prng.threefry_fold_in(key:typing.Array,data:typing.Array)->typing.Array
jax._src.prng.threefry_random_bits(key:typing.Array,bit_width,shape)
jax._src.prng.threefry_seed(seed:typing.Array)->typing.Array
jax._src.prng.threefry_split(key:typing.Array,shape:Shape)->typing.Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/config.py----------------------------------------
A:jax._src.config.logger->logging.getLogger(__name__)
A:jax._src.config._T->TypeVar('_T')
A:jax._src.config.val->threading.local().__dict__.get(self._name, unset)
A:jax._src.config.self._contextmanager_flags->set()
A:jax._src.config.fn->globals().get(name, None)
A:jax._src.config.hook->self._update_hooks.get(name, None)
A:jax._src.config.jax_argv->itertools.takewhile(lambda a: a != '--', sys.argv)
A:jax._src.config.tls->jax._src.lib.jax_jit.thread_local_state()
A:jax._src.config.config->Config()
A:jax._src.config.no_default->NoDefault()
A:jax._src.config.unset->_Unset()
A:jax._src.config._thread_local_state->threading.local()
A:jax._src.config.prev_val->getattr(_thread_local_state, self._name, unset)
A:jax._src.config.name->name.lower().lower()
A:jax._src.config.s->_StateContextManager[Any](name, help, update_thread_local_hook, validate_new_val_hook)
A:jax._src.config.default->os.getenv(name.upper(), default)
A:jax._src.config.default_env->os.getenv(name.upper(), default)
A:jax._src.config.update_hook->kwargs.pop('update_hook', None)
A:jax._src.config.gs->jax._src.lib.jax_jit.global_state()
A:jax._src.config.gs.extra_jit_context->context._replace(**kw)
A:jax._src.config.self.canonicalize->functools.lru_cache(128)(lambda x: x)
A:jax._src.config._thread_local_state_cache->_ThreadLocalStateCache()
A:jax._src.config.tmp->context._replace(**kw)
A:jax._src.config.tls.extra_jit_context->_ThreadLocalStateCache().canonicalize(tmp)
A:jax._src.config.jax2tf_associative_scan_reductions->define_bool_state(name='jax2tf_associative_scan_reductions', default=False, help='JAX has two separate lowering rules for the cumulative reduction primitives (cumsum, cumprod, cummax, cummin). On CPUs and GPUs it uses a lax.associative_scan, while for TPUs it uses the HLO ReduceWindow. The latter has a slow implementation on CPUs and GPUs. By default, jax2tf uses the TPU lowering. Set this flag to True to use the associative scan lowering usage, and only if it makes a difference for your application. See the jax2tf README.md for more details.')
A:jax._src.config.jax2tf_default_native_serialization->define_bool_state(name='jax2tf_default_native_serialization', default=bool_env('JAX2TF_DEFAULT_NATIVE_SERIALIZATION', True), help='Sets the default value of the native_serialization parameter to jax2tf.convert. Prefer using the parameter instead of the flag, the flag may be removed in the future.')
A:jax._src.config.jax_serialization_version->define_int_state(name='jax_serialization_version', default=int_env('JAX_SERIALIZATION_VERSION', 8), help='The version number to use for native serialization. This must be within the range of versions supported by the tf.XlaCallModule used in your deployment environment. See https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md#native-serialization-versions.')
A:jax._src.config.jax_platforms->define_string_state(name='jax_platforms', default=None, help='Comma-separated list of platform names specifying which platforms jax should initialize. If any of the platforms in this list are not successfully initialized, an exception will be raised and the program will be aborted. The first platform in the list will be the default platform. For example, config.jax_platforms=cpu,tpu means that CPU and TPU backends will be initialized, and the CPU backend will be used unless otherwise specified. If TPU initialization fails, it will raise an exception. By default, jax will try to initialize all available platforms and will default to GPU or TPU if available, and fallback to CPU otherwise.')
A:jax._src.config.enable_checks->define_bool_state(name='jax_enable_checks', default=False, help='Turn on invariant checking for JAX internals. Makes things slower.')
A:jax._src.config.check_tracer_leaks->define_bool_state(name='jax_check_tracer_leaks', default=False, help='Turn on checking for leaked tracers as soon as a trace completes. Enabling leak checking may have performance impacts: some caching is disabled, and other overheads may be added. Additionally, be aware that some Python debuggers can cause false positives, so it is recommended to disable any debuggers while leak checking is enabled.')
A:jax._src.config.checking_leaks->functools.partial(check_tracer_leaks, True)
A:jax._src.config.debug_nans->define_bool_state(name='jax_debug_nans', default=False, help='Add nan checks to every operation. When a nan is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the nan.')
A:jax._src.config.debug_infs->define_bool_state(name='jax_debug_infs', default=False, help='Add inf checks to every operation. When an inf is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the inf.')
A:jax._src.config.log_compiles->define_bool_state(name='jax_log_compiles', default=False, help='Log a message each time every time `jit` or `pmap` compiles an XLA computation. Logging is performed with `logging`. When this option is set, the log level is WARNING; otherwise the level is DEBUG.')
A:jax._src.config.log_checkpoint_residuals->define_bool_state(name='jax_log_checkpoint_residuals', default=False, help='Log a message every time jax.checkpoint (aka jax.remat) is partially evaluated (e.g. for autodiff), printing what residuals are saved.')
A:jax._src.config.parallel_functions_output_gda->define_bool_state(name='jax_parallel_functions_output_gda', default=False, help='If True, pjit will output GDAs.')
A:jax._src.config.pmap_shmap_merge->define_bool_state(name='jax_pmap_shmap_merge', default=False, upgrade=True, help='If True, pmap and shard_map API will be merged.')
A:jax._src.config.enable_memories->define_bool_state('jax_enable_memories', default=True, upgrade=True, update_global_hook=_update_jax_memories_global, update_thread_local_hook=_update_jax_memories_thread_local, help='If True, will allow fetching memory kinds available on executable and annotate Shardings with it.')
A:jax._src.config.spmd_mode->define_enum_state(name='jax_spmd_mode', enum_values=['allow_all', 'allow_jit'], default='allow_jit', help="Decides whether Math on `jax.Array`'s that are not fully addressable (i.e. spans across multiple processes) is allowed. The options are: * allow_jit: Default, `pjit` and `jax.jit` computations are allowed     to execute on non-fully addressable `jax.Array`s\n* allow_all: `jnp`, normal math (like `a + b`, etc), `pjit`,     `jax.jit` and all other operations are allowed to     execute on non-fully addressable `jax.Array`s.")
A:jax._src.config.distributed_debug->define_bool_state(name='jax_distributed_debug', default=False, help='Enable logging useful for debugging multi-process distributed computations. Logging is performed with `logging` at WARNING level.')
A:jax._src.config.legacy_prng_key->define_enum_state(name='jax_legacy_prng_key', enum_values=['allow', 'warn', 'error'], default='allow', help='Specify the behavior when raw PRNG keys are passed to jax.random APIs.')
A:jax._src.config.enable_custom_prng->define_bool_state(name='jax_enable_custom_prng', default=False, upgrade=True, help='Enables an internal upgrade that allows one to define custom pseudo-random number generator implementations.')
A:jax._src.config.default_prng_impl->define_enum_state(name='jax_default_prng_impl', enum_values=['threefry2x32', 'rbg', 'unsafe_rbg'], default='threefry2x32', help='Select the default PRNG implementation, used when one is not explicitly provided at seeding time.')
A:jax._src.config.threefry_partitionable->define_bool_state(name='jax_threefry_partitionable', default=False, upgrade=True, help='Enables internal threefry PRNG implementation changes that render it automatically partitionable in some cases. Without this flag, using the standard jax.random pseudo-random number generation may result in extraneous communication and/or redundant distributed computation. With this flag, the communication overheads disappear in some cases.', update_global_hook=lambda val: _update_global_jit_state(threefry_partitionable=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(threefry_partitionable=val))
A:jax._src.config.softmax_custom_jvp->define_bool_state(name='jax_softmax_custom_jvp', default=False, upgrade=True, help='Use a new custom_jvp rule for jax.nn.softmax. The new rule should improve memory usage and stability. Set True to use new behavior. See https://github.com/google/jax/pull/15677', update_global_hook=lambda val: _update_global_jit_state(softmax_custom_jvp=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(softmax_custom_jvp=val))
A:jax._src.config.enable_custom_vjp_by_custom_transpose->define_bool_state(name='jax_enable_custom_vjp_by_custom_transpose', default=False, upgrade=True, help='Enables an internal upgrade that implements `jax.custom_vjp` by reduction to `jax.custom_jvp` and `jax.custom_transpose`.')
A:jax._src.config.raise_persistent_cache_errors->define_bool_state(name='jax_raise_persistent_cache_errors', default=False, help='If true, exceptions raised when reading or writing to the persistent compilation cache will be allowed through, halting program execution if not manually caught. If false, exceptions are caught and raised as warnings, allowing program execution to continue. Defaults to false so cache bugs or intermittent issues are non-fatal.')
A:jax._src.config.persistent_cache_min_compile_time_secs->define_float_state(name='jax_persistent_cache_min_compile_time_secs', default=1, help='The minimum compile time of a computation to be written to the persistent compilation cache. This threshold can be raised to decrease the number of entries written to the cache.')
A:jax._src.config.compilation_cache_include_metadata_in_key->define_bool_state(name='jax_compilation_cache_include_metadata_in_key', default=False, help='Include metadata, such as file names and line numbers, in the compilation cache key. If false, the cache will still get hits even if functions or files are moved, etc. However, it means that executables loaded from the cache may have stale metadata, which may show up in, e.g., profiles.')
A:jax._src.config.hlo_source_file_canonicalization_regex->define_string_state(name='jax_hlo_source_file_canonicalization_regex', default=None, help='Used to canonicalize the source_path metadata of HLO instructions by removing the given regex. If set, re.sub() is called on each source_file with the given regex, and all matches are removed. This can be used to avoid spurious cache misses when using the persistent compilation cache, which includes HLO metadata in the cache key.')
A:jax._src.config.include_full_tracebacks_in_locations->define_bool_state(name='jax_include_full_tracebacks_in_locations', default=False, help='Include full Python tracebacks in MLIR locations in IR emitted by JAX.')
A:jax._src.config.use_original_compilation_cache_key_generation->define_bool_state(name='jax_use_original_compilation_cache_key_generation', default=True, help='If true, use the original cache-key generation algorithm. This is a transient flag; once the new cache-key generation algorithm is deployed, this flag and the original cache-key generation algorithm will be removed.')
A:jax._src.config.default_dtype_bits->define_enum_state(name='jax_default_dtype_bits', enum_values=['32', '64'], default='64', help='Specify bit width of default dtypes, either 32-bit or 64-bit. This is a temporary flag that will be used during the process of deprecating the ``jax_enable_x64`` flag.')
A:jax._src.config.numpy_dtype_promotion->define_enum_state(name='jax_numpy_dtype_promotion', enum_values=['standard', 'strict'], default='standard', help='Specify the rules used for implicit type promotion in operations between arrays. Options are "standard" or "strict"; in strict-mode, binary operations between arrays of differing strongly-specified dtypes will result in an error.', update_global_hook=lambda val: _update_global_jit_state(numpy_dtype_promotion=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(numpy_dtype_promotion=val))
A:jax._src.config.enable_x64->define_bool_state(name='jax_enable_x64', default=False, help='Enable 64-bit types to be used', update_global_hook=_update_x64_global, update_thread_local_hook=_update_x64_thread_local)
A:jax._src.config.default_device->define_string_or_object_state(name='jax_default_device', default=None, help='Configure the default device for JAX operations. Set to a Device object (e.g. ``jax.devices("cpu")[0]``) to use that Device as the default device for JAX operations and jit\'d function calls (there is no effect on multi-device computations, e.g. pmapped function calls). Set to None to use the system default device. See :ref:`faq-data-placement` for more information on device placement.', update_global_hook=_update_default_device_global, update_thread_local_hook=_update_default_device_thread_local, validate_new_val_hook=_validate_default_device)
A:jax._src.config.disable_jit->define_bool_state(name='jax_disable_jit', default=False, help='Disable JIT compilation and just call original Python.', update_global_hook=_update_disable_jit_global, update_thread_local_hook=_update_disable_jit_thread_local)
A:jax._src.config.numpy_rank_promotion->define_enum_state(name='jax_numpy_rank_promotion', enum_values=['allow', 'warn', 'raise'], default='allow', help='Control NumPy-style automatic rank promotion broadcasting ("allow", "warn", or "raise").', update_global_hook=lambda val: _update_global_jit_state(numpy_rank_promotion=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(numpy_rank_promotion=val))
A:jax._src.config.default_matmul_precision->define_enum_state(name='jax_default_matmul_precision', enum_values=['bfloat16', 'tensorfloat32', 'float32'], default=None, help="Control the default matmul and conv precision for 32bit inputs.\n\nSome platforms, like TPU, offer configurable precision levels for matrix multiplication and convolution computations, trading off accuracy for speed. The precision can be controlled for each operation; for example, see the :func:`jax.lax.conv_general_dilated` and :func:`jax.lax.dot` docstrings. But it can be useful to control the default behavior obtained when an operation is not given a specific precision.\n\nThis option can be used to control the default precision level for computations involved in matrix multiplication and convolution on 32bit inputs. The levels roughly describe the precision at which scalar products are computed. The 'bfloat16' option is the fastest and least precise; 'float32' is similar to full float32 precision; 'tensorfloat32' is intermediate.\n\n", update_global_hook=lambda val: _update_global_jit_state(default_matmul_precision=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(default_matmul_precision=val))
A:jax._src.config.traceback_filtering->define_enum_state(name='jax_traceback_filtering', enum_values=['off', 'tracebackhide', 'remove_frames', 'quiet_remove_frames', 'auto'], default='auto', help='Controls how JAX filters internal frames out of tracebacks.\n\nValid values are:\n * "off": disables traceback filtering.\n * "auto": use "tracebackhide" if running under a sufficiently new IPython, or "remove_frames" otherwise.\n * "tracebackhide": adds "__tracebackhide__" annotations to hidden stack frames, which some traceback printers support.\n * "remove_frames": removes hidden frames from tracebacks, and adds the unfiltered traceback as a __cause__ of the exception.\n * "quiet_remove_frames": removes hidden frames from tracebacks, and adds a brief message (to the __cause__ of the exception) describing that this has happened.\n')
A:jax._src.config.bcoo_cusparse_lowering->define_bool_state(name='jax_bcoo_cusparse_lowering', default=False, help='Enables lowering BCOO ops to cuSparse.')
A:jax._src.config.dynamic_shapes->define_bool_state(name='jax_dynamic_shapes', default=bool(os.getenv('JAX_DYNAMIC_SHAPES', '')), help='Enables experimental features for staging out computations with dynamic shapes.', update_global_hook=lambda val: _update_global_jit_state(dynamic_shapes=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(dynamic_shapes=val))
A:jax._src.config.remat_opt_barrier->define_bool_state(name='jax_remat_opt_barrier', default=lib.version >= (0, 3, 6), help='Enables using optimization-barrier op for lowering remat.')
A:jax._src.config.eager_pmap->define_bool_state(name='jax_eager_pmap', default=True, upgrade=True, help='Enable eager-mode pmap when jax_disable_jit is activated.')
A:jax._src.config.xla_runtime_errors->define_bool_state(name='jax_experimental_unsafe_xla_runtime_errors', default=False, help='Enable XLA runtime errors for jax.experimental.checkify.checks on CPU and GPU. These errors are async, might get lost and are not very readable. But, they crash the computation and enable you to write jittable checks without needing to checkify. Does not work under pmap/pjit.')
A:jax._src.config.jax_xla_profile_version->define_int_state(name='jax_xla_profile_version', default=0, help='Optional profile version for XLA compilation. This is meaningful only when XLA is configured to support the remote compilation profile feature.', update_global_hook=lambda val: _update_global_jit_state(xla_profile_version=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(xla_profile_version=val))
A:jax._src.config.state->jax._src.lib.transfer_guard_lib.thread_local_state()
A:jax._src.config.transfer_guard_host_to_device->define_enum_state(name='jax_transfer_guard_host_to_device', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for host-to-device transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'host_to_device', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'host_to_device', val))
A:jax._src.config.transfer_guard_device_to_device->define_enum_state(name='jax_transfer_guard_device_to_device', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for device-to-device transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'device_to_device', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'device_to_device', val))
A:jax._src.config.transfer_guard_device_to_host->define_enum_state(name='jax_transfer_guard_device_to_host', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for device-to-host transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'device_to_host', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'device_to_host', val))
A:jax._src.config._transfer_guard->define_enum_state(name='jax_transfer_guard', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for all transfers. This option is set-only; the transfer guard level for a specific direction should be read using the per-transfer direction option. Default is "allow".', update_global_hook=_update_all_transfer_guard_global)
A:jax._src.config.module_names->module_names_str.split(',')
jax._src._config_module.Config(self)
jax._src._config_module.Config.__getattr__(self,name)
jax._src._config_module.Config._config_module_with_absl(self)
jax._src._config_module.Config._read(self,name)
jax._src._config_module.Config._trace_context(self)
jax._src._config_module.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs,update_hook:Optional[Callable[[Any],None]]=None)
jax._src._config_module.Config.complete_absl__config_module(self,absl_flags)
jax._src._config_module.Config.parse_flags_with_absl(self)
jax._src._config_module.Config.read(self,name)
jax._src._config_module.Config.update(self,name,val)
jax._src._config_module.DEFINE_bool(name,default,*args,**kwargs)->FlagHolder[bool]
jax._src._config_module.DEFINE_enum(name,default,*args,**kwargs)->FlagHolder[str]
jax._src._config_module.DEFINE_float(name,default,*args,**kwargs)->FlagHolder[float]
jax._src._config_module.DEFINE_integer(name,default,*args,**kwargs)->FlagHolder[int]
jax._src._config_module.DEFINE_string(name,default,*args,**kwargs)->FlagHolder[str]
jax._src._config_module.FlagHolder(self,name:str)
jax._src._config_module.FlagHolder.__bool__(self)->NoReturn
jax._src._config_module.FlagHolder.value(self)->_T
jax._src._config_module.NoDefault
jax._src._config_module._GlobalExtraJitContext(NamedTuple)
jax._src._config_module._StateContextManager(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src._config_module._StateContextManager.__bool__(self)->NoReturn
jax._src._config_module._StateContextManager._add_hooks(self,update_global_hook,update_thread_local_hook)
jax._src._config_module._StateContextManager.value(self)->_T
jax._src._config_module._ThreadLocalExtraJitContext(NamedTuple)
jax._src._config_module._ThreadLocalStateCache(self)
jax._src._config_module._Unset
jax._src._config_module._update_all_transfer_guard_global(val)
jax._src._config_module._update_default_device_global(val)
jax._src._config_module._update_default_device_thread_local(val)
jax._src._config_module._update_disable_jit_global(val)
jax._src._config_module._update_disable_jit_thread_local(val)
jax._src._config_module._update_global_jit_state(**kw)
jax._src._config_module._update_jax_memories_global(val)
jax._src._config_module._update_jax_memories_thread_local(val)
jax._src._config_module._update_transfer_guard(state,key,val)
jax._src._config_module._update_x64_global(val)
jax._src._config_module._update_x64_thread_local(val)
jax._src._config_module._validate_default_device(val)
jax._src._config_module.bool_env(varname:str,default:bool)->bool
jax._src._config_module.check_exists(name)
jax._src._config_module.define_bool_state(name:str,default:bool,help:str,*,update_global_hook:Optional[Callable[[bool],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[bool]],None]]=None,upgrade:bool=False,extra_description:str='')->_StateContextManager[bool]
jax._src._config_module.define_enum_state(name:str,enum_values:list[str],default:Optional[str],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[str]
jax._src._config_module.define_float_state(name:str,default:Optional[float],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[float]
jax._src._config_module.define_int_state(name:str,default:Optional[int],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[int]
jax._src._config_module.define_string_or_object_state(name:str,default:Any,help:str,*,update_global_hook:Optional[Callable[[Any],None]]=None,update_thread_local_hook:Optional[Callable[[Any],None]]=None,validate_new_val_hook:Optional[Callable[[Any],None]]=None)->_StateContextManager[Any]
jax._src._config_module.define_string_state(name:str,default:Optional[str],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[str]
jax._src._config_module.explicit_device_get_scope()->Iterator[None]
jax._src._config_module.explicit_device_put_scope()->Iterator[None]
jax._src._config_module.int_env(varname:str,default:int)->int
jax._src._config_module.update_thread_local_jit_state(**kw)
jax._src.config.Config(self)
jax._src.config.Config.__getattr__(self,name)
jax._src.config.Config.__init__(self)
jax._src.config.Config._read(self,name)
jax._src.config.Config._trace_context(self)
jax._src.config.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs,update_hook:Optional[Callable[[Any],None]]=None)
jax._src.config.Config.complete_absl_config(self,absl_flags)
jax._src.config.Config.config_with_absl(self)
jax._src.config.Config.parse_flags_with_absl(self)
jax._src.config.Config.read(self,name)
jax._src.config.Config.update(self,name,val)
jax._src.config.DEFINE_bool(name,default,*args,**kwargs)->FlagHolder[bool]
jax._src.config.DEFINE_enum(name,default,*args,**kwargs)->FlagHolder[str]
jax._src.config.DEFINE_float(name,default,*args,**kwargs)->FlagHolder[float]
jax._src.config.DEFINE_integer(name,default,*args,**kwargs)->FlagHolder[int]
jax._src.config.DEFINE_string(name,default,*args,**kwargs)->FlagHolder[str]
jax._src.config.FlagHolder(self,name:str)
jax._src.config.FlagHolder.__bool__(self)->NoReturn
jax._src.config.FlagHolder.__init__(self,name:str)
jax._src.config.FlagHolder.value(self)->_T
jax._src.config.NoDefault
jax._src.config._GlobalExtraJitContext(NamedTuple)
jax._src.config._StateContextManager(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src.config._StateContextManager.__bool__(self)->NoReturn
jax._src.config._StateContextManager.__init__(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src.config._StateContextManager._add_hooks(self,update_global_hook,update_thread_local_hook)
jax._src.config._StateContextManager.value(self)->_T
jax._src.config._ThreadLocalExtraJitContext(NamedTuple)
jax._src.config._ThreadLocalStateCache(self)
jax._src.config._ThreadLocalStateCache.__init__(self)
jax._src.config._Unset
jax._src.config._update_all_transfer_guard_global(val)
jax._src.config._update_debug_log_modules(module_names_str:Optional[str])
jax._src.config._update_default_device_global(val)
jax._src.config._update_default_device_thread_local(val)
jax._src.config._update_disable_jit_global(val)
jax._src.config._update_disable_jit_thread_local(val)
jax._src.config._update_global_jit_state(**kw)
jax._src.config._update_jax_memories_global(val)
jax._src.config._update_jax_memories_thread_local(val)
jax._src.config._update_transfer_guard(state,key,val)
jax._src.config._update_x64_global(val)
jax._src.config._update_x64_thread_local(val)
jax._src.config._validate_default_device(val)
jax._src.config.bool_env(varname:str,default:bool)->bool
jax._src.config.check_exists(name)
jax._src.config.define_bool_state(name:str,default:bool,help:str,*,update_global_hook:Optional[Callable[[bool],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[bool]],None]]=None,upgrade:bool=False,extra_description:str='')->_StateContextManager[bool]
jax._src.config.define_enum_state(name:str,enum_values:list[str],default:Optional[str],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[str]
jax._src.config.define_float_state(name:str,default:Optional[float],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[float]
jax._src.config.define_int_state(name:str,default:Optional[int],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[int]
jax._src.config.define_string_or_object_state(name:str,default:Any,help:str,*,update_global_hook:Optional[Callable[[Any],None]]=None,update_thread_local_hook:Optional[Callable[[Any],None]]=None,validate_new_val_hook:Optional[Callable[[Any],None]]=None)->_StateContextManager[Any]
jax._src.config.define_string_state(name:str,default:Optional[str],help:str,*,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)->_StateContextManager[str]
jax._src.config.explicit_device_get_scope()->Iterator[None]
jax._src.config.explicit_device_put_scope()->Iterator[None]
jax._src.config.int_env(varname:str,default:int)->int
jax._src.config.transfer_guard(new_val:str)->Iterator[None]
jax._src.config.update_thread_local_jit_state(**kw)
jax.transfer_guard(new_val:str)->Iterator[None]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/tree_util.py----------------------------------------
A:jax._src.tree_util.T->TypeVar('T')
A:jax._src.tree_util.U->TypeVar('U', bound=type[Any])
A:jax._src.tree_util.H->TypeVar('H', bound=Hashable)
A:jax._src.tree_util.default_registry->jax._src.lib.pytree.default_registry()
A:jax._src.tree_util.dispatch_registry->jax._src.lib.pytree.PyTreeRegistry(enable_none=True, enable_tuple=True, enable_namedtuple=True, enable_list=True, enable_dict=True)
A:jax._src.tree_util.lst->list(iterable)
A:jax._src.tree_util._Children->TypeVar('_Children', bound=Iterable[Any])
A:jax._src.tree_util._AuxData->TypeVar('_AuxData', bound=Hashable)
A:jax._src.tree_util._registry[nodetype]->_RegistryEntry(flatten_func, unflatten_func)
A:jax._src.tree_util.(leaves, treedef)->tree_flatten(tree, is_leaf)
A:jax._src.tree_util.(flat, treedef)->tree_flatten(pytree_to_transpose)
A:jax._src.tree_util.expected_treedef->outer_treedef.compose(inner_treedef)
A:jax._src.tree_util.iter_flat->iter(flat)
A:jax._src.tree_util.transposed_lol->zip(*lol)
A:jax._src.tree_util.subtrees->map(partial(tree_unflatten, outer_treedef), transposed_lol)
A:jax._src.tree_util._RegistryEntry->collections.namedtuple('_RegistryEntry', ['to_iter', 'from_iter'])
A:jax._src.tree_util.handler->_registry_with_keypaths.get(type(pytree))
A:jax._src.tree_util.(children, metadata)->_registry_with_keypaths.get(type(pytree)).to_iter(tree)
A:jax._src.tree_util.children->iter(tree)
A:jax._src.tree_util.no_initializer->object()
A:jax._src.tree_util.func->_HashableCallableShim(original_func)
A:jax._src.tree_util.out->super().__new__(klass, func, *args, **kw)
A:jax._src.tree_util.(children, meta)->_registry_with_keypaths.get(type(pytree)).to_iter(pytree)
A:jax._src.tree_util.(t1_children, t1_meta)->flatten_one_level(t1)
A:jax._src.tree_util.(t2_children, t2_meta)->flatten_one_level(t2)
A:jax._src.tree_util.diff->set(prefix_tree_keys).symmetric_difference(set(full_tree_keys))
A:jax._src.tree_util.KeyEntry->TypeVar('KeyEntry', bound=Hashable)
A:jax._src.tree_util.(children, treedef)->_registry[ty].to_iter(xs)
A:jax._src.tree_util._registry_with_keypaths[ty]->_RegistryWithKeypathsEntry(flatten_with_keys, _registry[ty].from_iter)
A:jax._src.tree_util.(key_children, treedef)->flatten_with_keys(tree)
A:jax._src.tree_util._registry_with_keypaths[nodetype]->_RegistryWithKeypathsEntry(flatten_with_keys, unflatten_func)
A:jax._src.tree_util.(_, tree_def)->tree_flatten(tree, is_leaf)
A:jax._src.tree_util.key_handler->_registry_with_keypaths.get(type(tree))
A:jax._src.tree_util.(key_children, _)->_registry_with_keypaths.get(type(tree)).flatten_with_keys(tree)
A:jax._src.tree_util.(children, _)->_registry_with_keypaths.get(type(pytree)).to_iter(tree)
A:jax._src.tree_util.k->FlattenedIndexKey(i)
A:jax._src.tree_util.(keypath_leaves, treedef)->tree_flatten_with_path(tree, is_leaf)
A:jax._src.tree_util.keypath_leaves->list(zip(*keypath_leaves))
A:jax._src.tree_util.num_children->len(treedef_children(tree_structure(pytree)))
A:jax._src.tree_util.(prefix_tree_children, prefix_tree_meta)->flatten_one_level(prefix_tree)
A:jax._src.tree_util.(full_tree_children, full_tree_meta)->flatten_one_level(full_tree)
A:jax._src.tree_util.prefix_tree_keys->_child_keys(prefix_tree)
A:jax._src.tree_util.full_tree_keys->_child_keys(full_tree)
A:jax._src.tree_util.ty->type(prefix_tree)
A:jax._src.tree_util.prefix_tree_meta_str->str(prefix_tree_meta)
A:jax._src.tree_util.full_tree_meta_str->str(full_tree_meta)
A:jax._src.tree_util.metadata_diff->textwrap.indent('\n'.join(difflib.ndiff(prefix_tree_meta_str.splitlines(), full_tree_meta_str.splitlines())), prefix='    ')
jax._src.tree_util.AttributeKeyPathEntry(_DeprecatedKeyPathEntry)
jax._src.tree_util.AttributeKeyPathEntry.__str__(self)
jax._src.tree_util.AttributeKeyPathEntry.pprint(self)->str
jax._src.tree_util.DictKey
jax._src.tree_util.DictKey.__str__(self)
jax._src.tree_util.FlattenedIndexKey
jax._src.tree_util.FlattenedIndexKey.__str__(self)
jax._src.tree_util.GetAttrKey
jax._src.tree_util.GetAttrKey.__str__(self)
jax._src.tree_util.GetitemKeyPathEntry(_DeprecatedKeyPathEntry)
jax._src.tree_util.GetitemKeyPathEntry.__str__(self)
jax._src.tree_util.GetitemKeyPathEntry.pprint(self)->str
jax._src.tree_util.Partial(klass,func,*args,**kw)
jax._src.tree_util.Partial.__new__(klass,func,*args,**kw)
jax._src.tree_util.SequenceKey
jax._src.tree_util.SequenceKey.__str__(self)
jax._src.tree_util._DeprecatedKeyPathEntry(NamedTuple)
jax._src.tree_util._DeprecatedKeyPathEntry.pprint(self)->str
jax._src.tree_util._HashableCallableShim(self,fun)
jax._src.tree_util._HashableCallableShim.__eq__(self,other)
jax._src.tree_util._HashableCallableShim.__hash__(self)
jax._src.tree_util._HashableCallableShim.__init__(self,fun)
jax._src.tree_util._HashableCallableShim.__repr__(self)
jax._src.tree_util._RegistryWithKeypathsEntry(NamedTuple)
jax._src.tree_util._child_keys(pytree:Any)->KeyPath
jax._src.tree_util._equality_errors(path,t1,t2,is_leaf)
jax._src.tree_util._generate_key_paths_(key_path:KeyPath,tree:Any,is_leaf:Callable[[Any],bool]|None=None)->Iterable[tuple[KeyPath, Any]]
jax._src.tree_util._prefix_error(key_path:KeyPath,prefix_tree:Any,full_tree:Any,is_leaf:Callable[[Any],bool]|None=None)->Iterable[Callable[[str], ValueError]]
jax._src.tree_util._register_keypaths(ty:type[T],handler:Callable[[T],tuple[KeyEntry,...]])->None
jax._src.tree_util._replace_nones(sentinel,tree)
jax._src.tree_util.all_leaves(iterable:Iterable[Any],is_leaf:Callable[[Any],bool]|None=None)->bool
jax._src.tree_util.broadcast_prefix(prefix_tree:Any,full_tree:Any,is_leaf:Callable[[Any],bool]|None=None)->list[Any]
jax._src.tree_util.build_tree(treedef:PyTreeDef,xs:Any)->Any
jax._src.tree_util.equality_errors(tree1:Any,tree2:Any,is_leaf:Callable[[Any],bool]|None=None)->Iterable[tuple[KeyPath, str, str, str]]
jax._src.tree_util.flatten_one_level(pytree:Any)->tuple[list[Any], Hashable]
jax._src.tree_util.generate_key_paths(tree:Any,is_leaf:Callable[[Any],bool]|None=None)->list[tuple[KeyPath, Any]]
jax._src.tree_util.keystr(keys:KeyPath)
jax._src.tree_util.prefix_errors(prefix_tree:Any,full_tree:Any,is_leaf:Callable[[Any],bool]|None=None)->list[Callable[[str], ValueError]]
jax._src.tree_util.register_keypaths(ty:type[T],handler:Callable[[T],tuple[KeyEntry,...]])->None
jax._src.tree_util.register_pytree_node(nodetype:type[T],flatten_func:Callable[[T],tuple[_Children,_AuxData]],unflatten_func:Callable[[_AuxData,_Children],T])
jax._src.tree_util.register_pytree_node_class(cls:U)->U
jax._src.tree_util.register_pytree_with_keys(nodetype:type[T],flatten_with_keys:Callable[[T],tuple[Iterable[tuple[KeyEntry,Any]],_AuxData]],unflatten_func:Callable[[_AuxData,Iterable[Any]],T],flatten_func:None|Callable[[T],tuple[Iterable[Any],_AuxData]]=None)
jax._src.tree_util.register_pytree_with_keys_class(cls:U)->U
jax._src.tree_util.register_static(cls:Type[H])->Type[H]
jax._src.tree_util.tree_all(tree:Any)->bool
jax._src.tree_util.tree_flatten(tree:Any,is_leaf:Callable[[Any],bool]|None=None)->tuple[list[Leaf], PyTreeDef]
jax._src.tree_util.tree_flatten_with_path(tree:Any,is_leaf:Callable[[Any],bool]|None=None)->tuple[list[tuple[KeyPath, Any]], PyTreeDef]
jax._src.tree_util.tree_leaves(tree:Any,is_leaf:Callable[[Any],bool]|None=None)->list[Leaf]
jax._src.tree_util.tree_leaves_with_path(tree:Any,is_leaf:Callable[[Any],bool]|None=None)->list[tuple[KeyPath, Any]]
jax._src.tree_util.tree_map(f:Callable[...,Any],tree:Any,*rest:Any,is_leaf:Callable[[Any],bool]|None=None)->Any
jax._src.tree_util.tree_map_with_path(f:Callable[...,Any],tree:Any,*rest:Any,is_leaf:Callable[[Any],bool]|None=None)->Any
jax._src.tree_util.tree_reduce(function:Callable[[T,Any],T],tree:Any,initializer:Any=no_initializer,is_leaf:Callable[[Any],bool]|None=None)->T
jax._src.tree_util.tree_structure(tree:Any,is_leaf:None|Callable[[Any],bool]=None)->PyTreeDef
jax._src.tree_util.tree_transpose(outer_treedef:PyTreeDef,inner_treedef:PyTreeDef,pytree_to_transpose:Any)->Any
jax._src.tree_util.tree_unflatten(treedef:PyTreeDef,leaves:Iterable[Leaf])->Any
jax._src.tree_util.treedef_children(treedef:PyTreeDef)->list[PyTreeDef]
jax._src.tree_util.treedef_is_leaf(treedef:PyTreeDef)->bool
jax._src.tree_util.treedef_is_strict_leaf(treedef:PyTreeDef)->bool
jax._src.tree_util.treedef_tuple(treedefs:Iterable[PyTreeDef])->PyTreeDef


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/effects.py----------------------------------------
jax._src.effects.Effect
jax._src.effects.EffectTypeSet(self)
jax._src.effects.EffectTypeSet.__init__(self)
jax._src.effects.EffectTypeSet.add_type(self,effect_type:type[Effect])
jax._src.effects.EffectTypeSet.contains(self,eff:Effect)->bool
jax._src.effects.EffectTypeSet.filter_in(self,effects:Iterable[Effect])->list[Effect]
jax._src.effects.EffectTypeSet.filter_not_in(self,effects:Iterable[Effect])->list[Effect]
jax._src.effects.JaxprInputEffect(self,input_index:Any)
jax._src.effects.JaxprInputEffect.__eq__(self,other)
jax._src.effects.JaxprInputEffect.__hash__(self)
jax._src.effects.JaxprInputEffect.__init__(self,input_index:Any)
jax._src.effects.JaxprInputEffect.__repr__(self)
jax._src.effects.JaxprInputEffect.replace(self,*,input_index:Any|None=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pretty_printer.py----------------------------------------
A:jax._src.pretty_printer._PPRINT_USE_COLOR->jax._src.config.DEFINE_bool('jax_pprint_use_color', config.bool_env('JAX_PPRINT_USE_COLOR', True), help='Enable jaxpr pretty-printing with colorful syntax highlighting.')
A:jax._src.pretty_printer.ipython->get_ipython()
A:jax._src.pretty_printer.CAN_USE_COLOR->_can_use_color()
A:jax._src.pretty_printer._nil->_NilDoc()
A:jax._src.pretty_printer.self.children->list(children)
A:jax._src.pretty_printer.Color->enum.Enum('_Color', ['BLACK', 'RED', 'GREEN', 'YELLOW', 'BLUE', 'MAGENTA', 'CYAN', 'WHITE', 'RESET'])
A:jax._src.pretty_printer.Intensity->enum.Enum('_Intensity', ['DIM', 'NORMAL', 'BRIGHT'])
A:jax._src.pretty_printer._BreakMode->enum.Enum('_BreakMode', ['FLAT', 'BREAK'])
A:jax._src.pretty_printer.(i, m, doc)->agenda.pop()
A:jax._src.pretty_printer.doc->agenda.pop()
A:jax._src.pretty_printer.maxlen->max((l.width for l in lines))
A:jax._src.pretty_printer.default_colors->_ColorState(Color.RESET, Color.RESET, Intensity.NORMAL)
A:jax._src.pretty_printer.annotation_colors->_ColorState(Color.RESET, Color.RESET, Intensity.DIM)
A:jax._src.pretty_printer.(i, m, doc, color)->agenda.pop()
A:jax._src.pretty_printer.(color_state, color_str)->_update_color(use_color, color_state, default_colors)
A:jax._src.pretty_printer.color->_ColorState(doc.foreground or color.foreground, doc.background or color.background, doc.intensity or color.intensity)
A:jax._src.pretty_printer.lines->_align_annotations(lines)
A:jax._src.pretty_printer.out->'\n'.join((l.text if l.annotations is None else f'{l.text}{annotation_prefix}{l.annotations}' for l in lines))
A:jax._src.pretty_printer.docs->list(docs)
A:jax._src.pretty_printer.type_annotation->partial(color, intensity=Intensity.NORMAL, foreground=Color.MAGENTA)
A:jax._src.pretty_printer.keyword->partial(color, intensity=Intensity.BRIGHT, foreground=Color.BLUE)
jax._src.pretty_printer.Doc(abc.ABC)
jax._src.pretty_printer.Doc.__add__(self,other:'Doc')->'Doc'
jax._src.pretty_printer.Doc.__str__(self)
jax._src.pretty_printer.Doc.format(self,width:int=80,use_color:Optional[bool]=None,annotation_prefix='#')->str
jax._src.pretty_printer._BreakDoc(self,text:str)
jax._src.pretty_printer._BreakDoc.__init__(self,text:str)
jax._src.pretty_printer._BreakDoc.__repr__(self)
jax._src.pretty_printer._ColorDoc(self,child:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer._ColorDoc.__init__(self,child:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer._ColorState(NamedTuple)
jax._src.pretty_printer._ConcatDoc(self,children:Sequence[Doc])
jax._src.pretty_printer._ConcatDoc.__init__(self,children:Sequence[Doc])
jax._src.pretty_printer._ConcatDoc.__repr__(self)
jax._src.pretty_printer._GroupDoc(self,child:Doc)
jax._src.pretty_printer._GroupDoc.__init__(self,child:Doc)
jax._src.pretty_printer._GroupDoc.__repr__(self)
jax._src.pretty_printer._Line(NamedTuple)
jax._src.pretty_printer._NestDoc(self,n:int,child:Doc)
jax._src.pretty_printer._NestDoc.__init__(self,n:int,child:Doc)
jax._src.pretty_printer._NestDoc.__repr__(self)
jax._src.pretty_printer._NilDoc(Doc)
jax._src.pretty_printer._NilDoc.__repr__(self)
jax._src.pretty_printer._State(NamedTuple)
jax._src.pretty_printer._TextDoc(self,text:str,annotation:Optional[str]=None)
jax._src.pretty_printer._TextDoc.__init__(self,text:str,annotation:Optional[str]=None)
jax._src.pretty_printer._TextDoc.__repr__(self)
jax._src.pretty_printer._align_annotations(lines)
jax._src.pretty_printer._can_use_color()->bool
jax._src.pretty_printer._fits(doc:Doc,width:int,agenda:list[tuple[int,_BreakMode,Doc]])->bool
jax._src.pretty_printer._format(doc:Doc,width:int,*,use_color,annotation_prefix)->str
jax._src.pretty_printer._sparse(doc:Doc)->bool
jax._src.pretty_printer._update_color(use_color:bool,state:_ColorState,update:_ColorState)->tuple[_ColorState, str]
jax._src.pretty_printer.brk(text:str='')->Doc
jax._src.pretty_printer.color(doc:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer.concat(docs:Sequence[Doc])->Doc
jax._src.pretty_printer.group(doc:Doc)->Doc
jax._src.pretty_printer.join(sep:Doc,docs:Sequence[Doc])->Doc
jax._src.pretty_printer.nest(n:int,doc:Doc)->Doc
jax._src.pretty_printer.nil()->Doc
jax._src.pretty_printer.text(s:str,annotation:Optional[str]=None)->Doc


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/dlpack.py----------------------------------------
A:jax._src.dlpack.SUPPORTED_DTYPES->frozenset({jnp.int8, jnp.int16, jnp.int32, jnp.int64, jnp.uint8, jnp.uint16, jnp.uint32, jnp.uint64, jnp.float16, jnp.bfloat16, jnp.float32, jnp.float64, jnp.complex64, jnp.complex128})
A:jax._src.dlpack.(dl_device_type, device_id)->external_array.__dlpack_device__()
A:jax._src.dlpack.backend->jax._src.xla_bridge.get_backend(device_platform)
A:jax._src.dlpack.device->jax._src.xla_bridge.get_backend(device_platform).device_from_local_hardware_id(device_id)
A:jax._src.dlpack.stream->jax._src.xla_bridge.get_backend(device_platform).device_from_local_hardware_id(device_id).get_stream_for_external_ready_events()
A:jax._src.dlpack.dlpack->external_array.__dlpack__(stream=stream)
A:jax._src.dlpack.cpu_backend->jax._src.xla_bridge.get_backend('cpu')
A:jax._src.dlpack.gpu_backend->jax._src.xla_bridge.get_backend('rocm')
jax._src.dlpack.DLDeviceType(enum.IntEnum)
jax._src.dlpack.from_dlpack(external_array)
jax._src.dlpack.to_dlpack(x:Array,take_ownership:bool=False,stream:int|Any|None=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/ad_util.py----------------------------------------
A:jax._src.ad_util.T->TypeVar('T')
A:jax._src.ad_util.attr->getattr(self.aval, name)
A:jax._src.ad_util.t->type(attr)
jax._src.ad_util.SymbolicZero(self,aval:core.AbstractValue)
jax._src.ad_util.SymbolicZero.__getattr__(self,name)
jax._src.ad_util.SymbolicZero.__init__(self,aval:core.AbstractValue)
jax._src.ad_util.SymbolicZero.__repr__(self)->str
jax._src.ad_util.Zero(self,aval:core.AbstractValue)
jax._src.ad_util.Zero.__init__(self,aval:core.AbstractValue)
jax._src.ad_util.Zero.__repr__(self)->str
jax._src.ad_util.Zero.from_value(val:Any)->Zero
jax._src.ad_util._stop_gradient_impl(x:T)->T
jax._src.ad_util.add_abstract(xs,ys)
jax._src.ad_util.add_impl(xs,ys)
jax._src.ad_util.add_jaxvals(x:ArrayLike,y:ArrayLike)->Array
jax._src.ad_util.instantiate(z:Zero|Array)->Array
jax._src.ad_util.replace_internal_symbolic_zeros(x:JaxTypeOrTracer|Zero)->JaxTypeOrTracer | SymbolicZero
jax._src.ad_util.replace_rule_output_symbolic_zeros(x:JaxTypeOrTracer|SymbolicZero)->JaxTypeOrTracer | Zero
jax._src.ad_util.zeros_like_aval(aval:core.AbstractValue)->Array
jax._src.ad_util.zeros_like_impl(example)
jax._src.ad_util.zeros_like_jaxval(val:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/profiler.py----------------------------------------
A:jax._src.profiler.logger->logging.getLogger(__name__)
A:jax._src.profiler._profiler_server->jax._src.lib.xla_client.profiler.start_server(port)
A:jax._src.profiler.self.lock->threading.Lock()
A:jax._src.profiler._profile_state->_ProfileState()
A:jax._src.profiler._profile_state.profile_session->jax._src.lib.xla_client.profiler.ProfilerSession()
A:jax._src.profiler.curr_path->os.path.abspath(log_dir)
A:jax._src.profiler.root_trace_folder->os.path.join(curr_path, 'plugins', 'profile')
A:jax._src.profiler.latest_folder->max(trace_folders, key=os.path.getmtime)
A:jax._src.profiler.trace_jsons->glob.glob(os.path.join(latest_folder, '*.trace.json.gz'))
A:jax._src.profiler.trace->json.load(fp)
A:jax._src.profiler.perfetto_trace->os.path.join(latest_folder, filename)
A:jax._src.profiler.orig_directory->os.path.abspath(os.getcwd())
A:jax._src.profiler.(directory, filename)->os.path.split(path)
A:jax._src.profiler.abs_filename->_write_perfetto_trace_file(_profile_state.log_dir)
A:jax._src.profiler.xspace->_ProfileState().profile_session.stop()
A:jax._src.profiler.fdo_profile->jax._src.lib.xla_client.profiler.get_fdo_profile(xspace)
A:jax._src.profiler.profile->device_memory_profile(backend)
jax._src.profiler.StepTraceAnnotation(self,name:str,**kwargs)
jax._src.profiler.StepTraceAnnotation.__init__(self,name:str,**kwargs)
jax._src.profiler.TraceAnnotation(xla_client.profiler.TraceMe)
jax._src.profiler._PerfettoServer(http.server.SimpleHTTPRequestHandler)
jax._src.profiler._PerfettoServer.do_GET(self)
jax._src.profiler._PerfettoServer.do_POST(self)
jax._src.profiler._PerfettoServer.end_headers(self)
jax._src.profiler._ProfileState(self)
jax._src.profiler._ProfileState.__init__(self)
jax._src.profiler._ProfileState.reset(self)
jax._src.profiler._host_perfetto_trace_file(path)
jax._src.profiler._write_perfetto_trace_file(log_dir)
jax._src.profiler.annotate_function(func:Callable,name:Optional[str]=None,**decorator_kwargs)
jax._src.profiler.device_memory_profile(backend:Optional[str]=None)->bytes
jax._src.profiler.save_device_memory_profile(filename,backend:Optional[str]=None)->None
jax._src.profiler.start_server(port:int)->xla_client.profiler.ProfilerServer
jax._src.profiler.start_trace(log_dir,create_perfetto_link:bool=False,create_perfetto_trace:bool=False)->None
jax._src.profiler.stop_and_get_fdo_profile()->bytes
jax._src.profiler.stop_server()
jax._src.profiler.stop_trace()
jax._src.profiler.trace(log_dir,create_perfetto_link=False,create_perfetto_trace=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/array.py----------------------------------------
A:jax._src.array.np_value->fun(*args)
A:jax._src.array.jnp_value->jax._src.api.device_put(np_value)
A:jax._src.array.jnp_value.aval->jax._src.api.device_put(np_value).aval.update(**aval_state)
A:jax._src.array.map_->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width).addressable_devices_indices_map(shape)
A:jax._src.array.seen_h_indices->set()
A:jax._src.array.h_index->hashed_index(index)
A:jax._src.array.di_map->_cached_index_calc(s, shape)
A:jax._src.array.ind->_cached_index_calc(s, shape).get(a.device(), None)
A:jax._src.array.num_unique_indices->len({hashed_index(v) for v in s.devices_indices_map(shape).values()})
A:jax._src.array.num_addressable_unique_indices->len({hashed_index(v) for v in s.addressable_devices_indices_map(shape).values()})
A:jax._src.array.array_device_ids->set(device_id_to_buffer.keys())
A:jax._src.array.ss->self.sharding.shard_shape(self.shape)
A:jax._src.array.indices->tuple(self.sharding.devices_indices_map(self.shape).values())
A:jax._src.array.arr_idx->tuple(self.sharding.devices_indices_map(self.shape).values()).index(cidx)
A:jax._src.array.s->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width)
A:jax._src.array.(fun, args, arr_state)->self._value.__reduce__()
A:jax._src.array.per_shard_size->arr.on_device_size_in_bytes()
A:jax._src.array.copy_plan->_create_copy_plan(self._arrays, self.sharding, self.shape)
A:jax._src.array.self._npy_value->self._single_device_array_to_np_array()
A:jax._src.array.npy_value->numpy.empty(self.shape, self.dtype)
A:jax._src.array.npy_value[ind]->arr._single_device_array_to_np_array()
A:jax._src.array.ArrayImpl->use_cpp_class(xc.ArrayImpl)(ArrayImpl)
A:jax._src.array.devices->list(device_to_index_map.keys())
A:jax._src.array.device_to_index_map->sharding.addressable_devices_indices_map(shape)
A:jax._src.array.first_value->jax._src.interpreters.xla.canonicalize_dtype(per_device_values[0])
A:jax._src.array.aval->jax._src.api_util.shaped_abstractify(x)
A:jax._src.array.arrays->jax._src.api.device_put(per_device_values, devices)
A:jax._src.array.xla.pytype_aval_mappings[ArrayImpl]->operator.attrgetter('aval')
A:jax._src.array.api_util._shaped_abstractify_handlers[ArrayImpl]->operator.attrgetter('aval')
A:jax._src.array.limit_indices->list(arr.shape)
A:jax._src.array.(start_indices, limit_indices, removed_dims)->unzip3((as_slice_indices(x, idx) for idx in indices))
A:jax._src.array.shards->x._multi_slice(start_indices, limit_indices, removed_dims)
A:jax._src.array.candidates->defaultdict(list)
A:jax._src.array.arr_indices->tuple(x.sharding.devices_indices_map(x.shape).values())
A:jax._src.array.x_indices->x.sharding.addressable_devices_indices_map(x.shape).values()
jax.Shard(self,device:Device,sharding:Sharding,global_shape:Shape,data:None|ArrayImpl|PRNGKeyArrayImpl=None)
jax.Shard.__repr__(self)
jax.Shard.data(self)
jax.Shard.device(self)
jax.Shard.index(self)->Index
jax.Shard.replica_id(self)->int
jax._src.array.ArrayImpl(self,aval:core.ShapedArray,sharding:Sharding,arrays:Sequence[ArrayImpl],committed:bool,_skip_checks:bool=False)
jax._src.array.ArrayImpl.__array__(self,dtype=None,context=None)
jax._src.array.ArrayImpl.__bool__(self)
jax._src.array.ArrayImpl.__complex__(self)
jax._src.array.ArrayImpl.__cuda_array_interface__(self)
jax._src.array.ArrayImpl.__dlpack__(self,*,stream:int|Any|None=None)
jax._src.array.ArrayImpl.__dlpack_device__(self)->tuple[enum.Enum, int]
jax._src.array.ArrayImpl.__float__(self)
jax._src.array.ArrayImpl.__format__(self,format_spec)
jax._src.array.ArrayImpl.__getitem__(self,idx)
jax._src.array.ArrayImpl.__hex__(self)
jax._src.array.ArrayImpl.__index__(self)
jax._src.array.ArrayImpl.__init__(self,aval:core.ShapedArray,sharding:Sharding,arrays:Sequence[ArrayImpl],committed:bool,_skip_checks:bool=False)
jax._src.array.ArrayImpl.__int__(self)
jax._src.array.ArrayImpl.__iter__(self)
jax._src.array.ArrayImpl.__len__(self)
jax._src.array.ArrayImpl.__oct__(self)
jax._src.array.ArrayImpl.__reduce__(self)
jax._src.array.ArrayImpl.__repr__(self)
jax._src.array.ArrayImpl.__str__(self)
jax._src.array.ArrayImpl._check_and_rearrange(self)
jax._src.array.ArrayImpl._check_if_deleted(self)
jax._src.array.ArrayImpl._copy_single_device_array_to_host_async(self)
jax._src.array.ArrayImpl._single_device_array_to_np_array(self)
jax._src.array.ArrayImpl._value(self)->np.ndarray
jax._src.array.ArrayImpl.addressable_data(self,index:int)->ArrayImpl
jax._src.array.ArrayImpl.addressable_shards(self)->Sequence[Shard]
jax._src.array.ArrayImpl.block_until_ready(self)
jax._src.array.ArrayImpl.copy_to_host_async(self)
jax._src.array.ArrayImpl.delete(self)
jax._src.array.ArrayImpl.device(self)->Device
jax._src.array.ArrayImpl.device_buffer(self)->ArrayImpl
jax._src.array.ArrayImpl.device_buffers(self)->Sequence[ArrayImpl]
jax._src.array.ArrayImpl.devices(self)->set[Device]
jax._src.array.ArrayImpl.dtype(self)
jax._src.array.ArrayImpl.global_shards(self)->Sequence[Shard]
jax._src.array.ArrayImpl.is_deleted(self)
jax._src.array.ArrayImpl.is_fully_addressable(self)->bool
jax._src.array.ArrayImpl.is_fully_replicated(self)->bool
jax._src.array.ArrayImpl.ndim(self)
jax._src.array.ArrayImpl.on_device_size_in_bytes(self)
jax._src.array.ArrayImpl.shape(self)->Shape
jax._src.array.ArrayImpl.sharding(self)
jax._src.array.ArrayImpl.size(self)
jax._src.array.ArrayImpl.tobytes(self,order='C')
jax._src.array.ArrayImpl.tolist(self)
jax._src.array.ArrayImpl.unsafe_buffer_pointer(self)
jax._src.array.ArrayImpl.weak_type(self)
jax._src.array.Shard(self,device:Device,sharding:Sharding,global_shape:Shape,data:None|ArrayImpl|PRNGKeyArrayImpl=None)
jax._src.array.Shard.__init__(self,device:Device,sharding:Sharding,global_shape:Shape,data:None|ArrayImpl|PRNGKeyArrayImpl=None)
jax._src.array.Shard.__repr__(self)
jax._src.array.Shard.data(self)
jax._src.array.Shard.device(self)
jax._src.array.Shard.index(self)->Index
jax._src.array.Shard.replica_id(self)->int
jax._src.array._array_global_result_handler(global_aval,out_sharding,committed,is_out_sharding_from_xla)
jax._src.array._array_local_result_handler(aval,sharding,indices)
jax._src.array._array_mlir_constant_handler(val)
jax._src.array._array_shard_arg(x,devices,indices,sharding)
jax._src.array._cached_index_calc(s,shape)
jax._src.array._create_copy_plan(arrays,s:Sharding,shape:Shape)
jax._src.array._hashable_index(idx)
jax._src.array._process_has_full_value_in_mcjax(s,shape)
jax._src.array._reconstruct_array(fun,args,arr_state,aval_state)
jax._src.array.as_slice_indices(arr:Any,idx:Index)->tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...]]
jax._src.array.make_array_from_callback(shape:Shape,sharding:Sharding,data_callback:Callable[[Index|None],ArrayLike])->ArrayImpl
jax._src.array.make_array_from_single_device_arrays(shape:Shape,sharding:Sharding,arrays:Sequence[basearray.Array])->ArrayImpl
jax._src.array.shard_device_array(x,devices,indices,sharding)
jax._src.array.shard_sharded_device_array_slow_path(x,devices,indices,sharding)
jax.make_array_from_callback(shape:Shape,sharding:Sharding,data_callback:Callable[[Index|None],ArrayLike])->ArrayImpl
jax.make_array_from_single_device_arrays(shape:Shape,sharding:Sharding,arrays:Sequence[basearray.Array])->ArrayImpl


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/cloud_tpu_init.py----------------------------------------
A:jax._src.cloud_tpu_init.libtpu_module->maybe_import_libtpu()
A:jax._src.cloud_tpu_init.use_pjrt_c_api->os.environ.get('JAX_USE_PJRT_C_API_ON_TPU', None)
jax._cloud_tpu_init()->None
jax._src.cloud_tpu_init.cloud_tpu_init()->None
jax._src.cloud_tpu_init.jax_force_tpu_init()->bool
jax._src.cloud_tpu_init.maybe_import_libtpu()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/sharding.py----------------------------------------
jax._src.sharding.Sharding
jax._src.sharding.Sharding.addressable_devices(self)->set[Device]
jax._src.sharding.Sharding.addressable_devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index | None]
jax._src.sharding.Sharding.device_set(self)->set[Device]
jax._src.sharding.Sharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index | None]
jax._src.sharding.Sharding.is_equivalent_to(self,other:Sharding,ndim:int)->bool
jax._src.sharding.Sharding.is_fully_addressable(self)->bool
jax._src.sharding.Sharding.is_fully_replicated(self)->bool
jax._src.sharding.Sharding.memory_kind(self)->str | None
jax._src.sharding.Sharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding.Sharding.with_memory_kind(self,kind:str)->Sharding
jax._src.sharding._addressable_devices_indices_map(sharding:Sharding,global_shape:Shape)->Mapping[Device, Index | None]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/distributed.py----------------------------------------
A:jax._src.distributed.logger->logging.getLogger(__name__)
A:jax._src.distributed.(coordinator_address, num_processes, process_id, local_device_ids)->jax._src.clusters.ClusterEnv.auto_detect_unset_distributed_params(coordinator_address, num_processes, process_id, local_device_ids)
A:jax._src.distributed.visible_devices->','.join((str(x) for x in local_device_ids))
A:jax._src.distributed.self.service->jax._src.lib.xla_extension.get_distributed_runtime_service(coordinator_address, num_processes, True)
A:jax._src.distributed.self.client->jax._src.lib.xla_extension.get_distributed_runtime_client(coordinator_address, process_id, True, init_timeout=initialization_timeout)
A:jax._src.distributed.self.preemption_sync_manager->jax._src.lib.xla_extension.create_preemption_sync_manager()
A:jax._src.distributed.global_state->State()
jax._src.distributed.State
jax._src.distributed.State.initialize(self,coordinator_address:Optional[str]=None,num_processes:Optional[int]=None,process_id:Optional[int]=None,local_device_ids:Optional[Union[int,Sequence[int]]]=None,initialization_timeout:int=300)
jax._src.distributed.State.initialize_preemption_sync_manager(self)
jax._src.distributed.State.shutdown(self)
jax._src.distributed.initialize(coordinator_address:Optional[str]=None,num_processes:Optional[int]=None,process_id:Optional[int]=None,local_device_ids:Optional[Union[int,Sequence[int]]]=None,initialization_timeout:int=300)
jax._src.distributed.shutdown()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/checkify.py----------------------------------------
A:jax._src.checkify.Out->TypeVar('Out')
A:jax._src.checkify.val->getattr(obj, attrname)
A:jax._src.checkify.sentinel->object()
A:jax._src.checkify.vals->jax._src.tree_util.tree_leaves((self.args, self.kwargs))
A:jax._src.checkify.exp->self.get_exception()
A:jax._src.checkify.shape->numpy.shape(list(self._pred.values())[0])
A:jax._src.checkify.payload->oob_payload(oob_mask, indices, dnums.scatter_dims_to_operand_dims, operand.shape)
A:jax._src.checkify.jax_error->tree_unflatten(self._metadata[int(min_code)], payload)
A:jax._src.checkify.new_err->self._pred.copy()
A:jax._src.checkify.new_code->self._code.copy()
A:jax._src.checkify.new_payload->self._payload.copy()
A:jax._src.checkify.new_payload[effect]->list(tree_map(lambda a: jnp.ones(a.shape, a.dtype), effect.shape_dtypes))
A:jax._src.checkify.init_error->Error({}, {}, {}, {})
A:jax._src.checkify.code->next_code()
A:jax._src.checkify.effect_type->FailedCheckError(get_traceback(), msg, *fmt_args, **fmt_kwargs).get_effect_type()
A:jax._src.checkify.(new_payload, new_metadata)->tree_flatten(new_error)
A:jax._src.checkify.err_of_type->_reduce_any_error(error)._pred.get(effect_type, False)
A:jax._src.checkify.out_code->jax.lax.select(err_of_type, error._code.get(effect_type, -1), code)
A:jax._src.checkify.cur_payload->_reduce_any_error(error)._payload.get(effect_type, None)
A:jax._src.checkify.out_payload->tree_map(functools.partial(lax.select, err_of_type), cur_payload, payload)
A:jax._src.checkify.(out_vals, out_tree)->jax._src.tree_util.tree_flatten((error, out))
A:jax._src.checkify.(err_vals, err_tree)->jax._src.tree_util.tree_flatten(in_err)
A:jax._src.checkify.num_error_vals->len(err_vals)
A:jax._src.checkify.params->dict(params, in_axes=(*(None,) * num_error_vals, *params['in_axes']), out_axes_thunk=out_axes_thunk)
A:jax._src.checkify.call_jaxpr->dict(params, in_axes=(*(None,) * num_error_vals, *params['in_axes']), out_axes_thunk=out_axes_thunk).pop('call_jaxpr')
A:jax._src.checkify.consts_->tuple((HashableWrapper(c) for c in consts))
A:jax._src.checkify.partial_checkify->jax._src.linear_util.wrap_init(functools.partial(checkify_jaxpr_flat, call_jaxpr.jaxpr, call_jaxpr.consts, enabled_errors, err_tree))
A:jax._src.checkify.(partial_checkify, metadata)->_flatten_and_get_error_metadata_thunk(partial_checkify)
A:jax._src.checkify.out_val_axes->dict(params, in_axes=(*(None,) * num_error_vals, *params['in_axes']), out_axes_thunk=out_axes_thunk).pop('out_axes')
A:jax._src.checkify.all_vals->primitive.bind(partial_checkify, *err_vals, *invals, **params)
A:jax._src.checkify.(out_tree, _)->metadata()
A:jax._src.checkify.(error, out_vals)->tree_unflatten(out_tree, all_vals)
A:jax._src.checkify.error->_reduce_any_error(error)
A:jax._src.checkify.(err_vals, in_args)->split_list(args, [err_tree.num_leaves])
A:jax._src.checkify.last_used->jax._src.core.last_used(jaxpr)
A:jax._src.checkify.invals->map(read_env, eqn.invars)
A:jax._src.checkify.checkify_rule->error_checks.get(eqn.primitive, functools.partial(default_checkify_rule, eqn.primitive))
A:jax._src.checkify.(error, outvals)->checkify_rule(error, enabled_errors, *invals, **eqn.params)
A:jax._src.checkify.consts->tuple((c.x for c in hashable_consts))
A:jax._src.checkify.(pred, code, payload)->tree_map(lambda x, idx=reduced_idx: x[idx], (errs, codes, payloads))
A:jax._src.checkify.out_error->out_error._replace(_metadata=error._metadata)._replace(_metadata=error._metadata)
A:jax._src.checkify.check_p->jax._src.core.Primitive('check')
A:jax._src.checkify.trimmed_params->sorted(((k, v) for (k, v) in eqn.params.items() if k != 'err_tree'))
A:jax._src.checkify.exc->_reduce_any_error(error).get_exception()
A:jax._src.checkify.filtered_tb->jax._src.traceback_util.filter_traceback(exc.traceback_info.as_python_traceback())
A:jax._src.checkify.functionalization_error->ValueError('Cannot abstractly evaluate a checkify.check which was not functionalized. This probably means you tried to stage (jit/scan/pmap/...) a `check` without functionalizing it through `checkify.checkify`.')
A:jax._src.checkify.(out_op, _, _)->jax._src.interpreters.mlir.emit_python_callback(ctx, callback=functools.partial(python_err, err_tree), token=None, operands=args, operand_avals=list(ctx.avals_in), result_avals=list(ctx.avals_out), has_side_effect=True)
A:jax._src.checkify.size->next((x.shape[dim] for (x, dim) in zip(batched_args, batch_dims) if dim is not batching.not_mapped))
A:jax._src.checkify.err->check_nans(prim, error, enabled_errors, out)
A:jax._src.checkify.out->jax._src.core.eval_jaxpr(jvp_jaxpr, jvp_consts, *primals, *nonzero_tangents)
A:jax._src.checkify.error_checks[_prim]->functools.partial(nan_error_check, _prim)
A:jax._src.checkify.operand_dims->numpy.array(operand.shape)
A:jax._src.checkify.slice_sizes->numpy.array(slice_sizes)
A:jax._src.checkify.start_indices->jax.numpy.array(start_indices)
A:jax._src.checkify.update_dims->numpy.array(update.shape)
A:jax._src.checkify.upper_bound->jax.lax.broadcast_in_dim(upper_bound, indices.shape, (len(indices.shape) - 1,))
A:jax._src.checkify.any_zero->jax.numpy.any(jnp.equal(y, 0))
A:jax._src.checkify.flat_idx->jax.numpy.argmin(jnp.logical_not(oob_mask))
A:jax._src.checkify.multi_idx->jax.numpy.unravel_index(flat_idx, indices.shape)
A:jax._src.checkify.lower_oob->jax.numpy.less(indices, 0)
A:jax._src.checkify.upper_oob->jax.numpy.greater(indices, upper_bound.astype(indices.dtype))
A:jax._src.checkify.oob_mask->jax.numpy.logical_or(lower_oob, upper_oob)
A:jax._src.checkify.(out_of_bounds, payload)->scatter_oob(operand, indices, updates, dimension_numbers)
A:jax._src.checkify.oob_error->OOBError(get_traceback(), prim.name, operand.shape, payload)
A:jax._src.checkify.error_checks[lax.scatter_p]->functools.partial(scatter_error_check, lax.scatter_p)
A:jax._src.checkify.error_checks[lax.scatter_add_p]->functools.partial(scatter_error_check, lax.scatter_add_p)
A:jax._src.checkify.error_checks[lax.scatter_mul_p]->functools.partial(scatter_error_check, lax.scatter_mul_p)
A:jax._src.checkify.error_checks[lax.scatter_min_p]->functools.partial(scatter_error_check, lax.scatter_min_p)
A:jax._src.checkify.error_checks[lax.scatter_max_p]->functools.partial(scatter_error_check, lax.scatter_max_p)
A:jax._src.checkify.checkify_jaxpr_partial->functools.partial(checkify_jaxpr_flat, jaxpr.jaxpr, jaxpr.consts, enabled_errors, err_tree)
A:jax._src.checkify.fun->jax._src.linear_util.wrap_init(checkify_jaxpr_partial)
A:jax._src.checkify.(fun, metadata)->_flatten_and_get_error_metadata_thunk(fun)
A:jax._src.checkify.(new_jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, flat_err_and_in_vals)
A:jax._src.checkify.checked_jaxpr->jax._src.interpreters.partial_eval.move_binders_to_front(checked_jaxpr_, tomove)
A:jax._src.checkify.(out_tree, error_effects)->metadata()
A:jax._src.checkify.in_avals->tuple(map(get_shaped_aval, new_vals_in))
A:jax._src.checkify.(_, _, effects)->jaxpr_to_checkify_jaxpr(jaxpr, enabled_errors, err_tree, *new_in_aval)
A:jax._src.checkify.merged_error->_reduce_any_error(error)._add_placeholder_effects(effects)
A:jax._src.checkify.(new_branches, out_trees, _)->unzip3((jaxpr_to_checkify_jaxpr(jxpr, enabled_errors, err_tree, *in_avals) for jxpr in branches))
A:jax._src.checkify.err_and_outs->jax.lax.cond_p.bind(index, *err_vals, *ops, branches=tuple(new_branches), linear=new_linear)
A:jax._src.checkify.(err0, out)->tree_unflatten(out_trees[0], err_and_outs)
A:jax._src.checkify.(err, _)->tree_unflatten(tr, err_and_outs)
A:jax._src.checkify.(consts, carry, xs)->split_list(in_flat, [num_consts, num_carry])
A:jax._src.checkify.(checked_jaxpr_, out_tree, _)->jaxpr_to_checkify_jaxpr(jaxpr, enabled_errors, err_tree, *new_in_aval)
A:jax._src.checkify.err_and_out->jax._src.pjit.pjit_p.bind(*new_vals_in, jaxpr=checked_jaxpr, in_shardings=new_in_shardings, out_shardings=new_out_shardings, resource_env=resource_env, donated_invars=new_donated_invars, name=name, inline=inline, keep_unused=keep_unused)
A:jax._src.checkify.(err, out)->tree_unflatten(out_tree, err_and_out)
A:jax._src.checkify.cond_f->jax._src.core.jaxpr_as_fun(cond_jaxpr)
A:jax._src.checkify.body_f->jax._src.core.jaxpr_as_fun(body_jaxpr)
A:jax._src.checkify.(c_consts, vals)->split_list(c_consts_and_vals, [c_consts_num])
A:jax._src.checkify._->cond_f(*c_consts, *out)
A:jax._src.checkify.new_body_f_->jax._src.linear_util.wrap_init(new_body_f)
A:jax._src.checkify.(jaxpr, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(new_body_f_, [*c_consts_avals, *body_jaxpr.in_avals])
A:jax._src.checkify.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, ())
A:jax._src.checkify.err_vals->map(get_shaped_aval, err_vals)
A:jax._src.checkify.(jaxpr, out_tree, error_effects)->jaxpr_to_checkify_jaxpr(closed_jaxpr, enabled_errors, err_tree, *flat_err_and_in_vals)
A:jax._src.checkify.new_jaxpr->jax._src.interpreters.partial_eval.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr_)).replace(outvars=jaxpr.outvars[num_error_vals:])
A:jax._src.checkify.(c_consts, b_consts, carry)->split_list(in_flat, [cond_nconsts, body_nconsts])
A:jax._src.checkify.(error, _)->checkify_jaxpr(cond_jaxpr, enabled_errors, error, *c_consts, *carry)
A:jax._src.checkify.(_, _, error_effects)->checkify_while_body_jaxpr(cond_jaxpr, body_jaxpr, enabled_errors, error, cond_nconsts)
A:jax._src.checkify.(checked_body_jaxpr_, body_out_tree, _)->checkify_while_body_jaxpr(cond_jaxpr, body_jaxpr, enabled_errors, error, cond_nconsts)
A:jax._src.checkify.checked_body_jaxpr->jax._src.interpreters.partial_eval.move_binders_to_front(checked_body_jaxpr_, to_move)
A:jax._src.checkify.cond_in_flat->map(get_shaped_aval, cond_in_flat)
A:jax._src.checkify.(checked_cond_jaxpr, _, _)->jaxpr_to_checkify_jaxpr(cond_jaxpr, enabled_errors, err_tree, *cond_in_flat)
A:jax._src.checkify.compat_cond_jaxpr_->ignore_error_output_jaxpr(checked_cond_jaxpr, num_error_vals)
A:jax._src.checkify.compat_cond_jaxpr->jax._src.interpreters.partial_eval.move_binders_to_front(compat_cond_jaxpr_, to_move)
A:jax._src.checkify.all_out_vals->jax.lax.while_p.bind(*new_in_flat, cond_nconsts=cond_nconsts, cond_jaxpr=compat_cond_jaxpr, body_nconsts=cond_nconsts + body_nconsts, body_jaxpr=checked_body_jaxpr)
A:jax._src.checkify.(error, out)->tree_unflatten(body_out_tree, all_out_vals)
A:jax._src.checkify.(checked_jaxpr, out_tree, _)->jaxpr_to_checkify_jaxpr(jaxpr, enabled_errors, err_tree, *in_avals)
A:jax._src.checkify.(partial_checkify, f_metadata)->_flatten_and_get_error_metadata_thunk(partial_checkify)
A:jax._src.checkify.jvp->lift_jvp(err_tree.num_leaves, num_consts, jvp_jaxpr_thunk)
A:jax._src.checkify.(jvp, jvp_out_tree)->flatten_fun_output(jvp)
A:jax._src.checkify.all_outs->jax._src.custom_derivatives.custom_vjp_call_p.bind(checkified_fun, checkified_fwd, bwd_, *err_vals, *in_vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)
A:jax._src.checkify.(fst, out_metadata)->jax._src.linear_util.merge_linear_aux(fun_metadata, fwd_out_tree)
A:jax._src.checkify.(out_err, out_vals)->tree_unflatten(err_and_out_tree, all_outs)
A:jax._src.checkify.(err_vals, out_vals)->split_list(all_outs, [len(err_vals)])
A:jax._src.checkify.out_err->jax._src.tree_util.tree_unflatten(err_tree, err_vals)
A:jax._src.checkify.(n, ragged)->divmod(len(xs), 2)
A:jax._src.checkify.(jvp_jaxpr, jvp_consts, out_zeros)->jvp_jaxpr_thunk(*zeros)
A:jax._src.checkify.(out_primals, nz_out_tangents)->split_list(out, [len(out_zeros)])
A:jax._src.checkify.nz_out_tangents_->iter(nz_out_tangents)
A:jax._src.checkify.checkified_fun->jax._src.linear_util.wrap_init(functools.partial(checkify_jaxpr_flat, fun_jaxpr.jaxpr, fun_jaxpr.consts, enabled_errors, err_tree))
A:jax._src.checkify.(checkified_fun, fun_metadata)->_flatten_and_get_error_metadata_thunk(checkified_fun)
A:jax._src.checkify.(fwd_jaxpr, fwd_consts)->fwd_jaxpr_thunk(*zeros)
A:jax._src.checkify.(checkified_fwd, fwd_out_tree)->flatten_fun_output(checkified_fwd)
A:jax._src.checkify.new_error->FailedCheckError(get_traceback(), msg, *fmt_args, **fmt_kwargs)
A:jax._src.checkify.discharged_error->discharged_error._replace(_metadata={**new_error._metadata, **discharged_error._metadata})._replace(_metadata={**new_error._metadata, **discharged_error._metadata})
A:jax._src.checkify.recharged_error->recharged_error._replace(_metadata=new_error._metadata)._replace(_metadata=new_error._metadata)
A:jax._src.checkify.user_checks->frozenset({FailedCheckError})
A:jax._src.checkify.nan_checks->frozenset({NaNError})
A:jax._src.checkify.index_checks->frozenset({OOBError})
A:jax._src.checkify.div_checks->frozenset({DivisionByZeroError})
A:jax._src.checkify.in_tree->jax._src.tree_util.tree_structure(((), {}))
A:jax._src.checkify.(fun_, out_tree)->flatten_fun(lu.wrap_init(closed_f), in_tree)
A:jax._src.checkify.debug->jax._src.interpreters.partial_eval.debug_info(closed_f, in_tree, out_tree, False, 'checkify')
A:jax._src.checkify.(jaxpr_, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun_, (), debug)
A:jax._src.checkify.jaxpr->jax._src.interpreters.partial_eval.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr_))
A:jax._src.checkify.(error, out_flat)->checkify_jaxpr(jaxpr, errors, init_error, *consts)
A:jax._src.checkify.(err_args, tree_def)->tree_flatten(error)
jax._src.checkify.BatchedError(JaxException)
jax._src.checkify.BatchedError.__post_init__(self)
jax._src.checkify.BatchedError.__str__(self)
jax._src.checkify.DivisionByZeroError(JaxException)
jax._src.checkify.DivisionByZeroError.__str__(self)
jax._src.checkify.DivisionByZeroError.get_effect_type(self)
jax._src.checkify.Error
jax._src.checkify.Error.__str__(self)
jax._src.checkify.Error._add_placeholder_effects(self,effects:set[ErrorEffect])
jax._src.checkify.Error._get_batched_exception(self)->BatchedError | None
jax._src.checkify.Error._replace(self,*args,**kwargs)
jax._src.checkify.Error._update(self,effect_type:ErrorEffect,pred,code,metadata,payload)
jax._src.checkify.Error.get(self)->str | None
jax._src.checkify.Error.get_exception(self)->JaxException | None
jax._src.checkify.Error.throw(self)
jax._src.checkify.Error.tree_flatten(self)
jax._src.checkify.Error.tree_unflatten(cls,metadata,data)
jax._src.checkify.ErrorEffect(effects.Effect)
jax._src.checkify.ErrorEffect.__lt__(self,other:ErrorEffect)
jax._src.checkify.FailedCheckError(self,traceback_info,fmt_string,*a,**k)
jax._src.checkify.FailedCheckError.__init__(self,traceback_info,fmt_string,*a,**k)
jax._src.checkify.FailedCheckError.__str__(self)
jax._src.checkify.FailedCheckError.get_effect_type(self)
jax._src.checkify.FailedCheckError.tree_flatten(self)
jax._src.checkify.FailedCheckError.tree_unflatten(cls,metadata,payload)
jax._src.checkify.JaxException(self,traceback_info)
jax._src.checkify.JaxException.__init__(self,traceback_info)
jax._src.checkify.JaxException.__init_subclass__(cls)
jax._src.checkify.JaxException.get_effect_type(self)->ErrorEffect
jax._src.checkify.JaxException.tree_flatten(self)
jax._src.checkify.JaxException.tree_unflatten(cls,metadata,payload)
jax._src.checkify.JaxRuntimeError(ValueError)
jax._src.checkify.NaNError(self,traceback_info,primitive_name)
jax._src.checkify.NaNError.__init__(self,traceback_info,primitive_name)
jax._src.checkify.NaNError.__str__(self)
jax._src.checkify.NaNError.get_effect_type(self)
jax._src.checkify.NaNError.tree_flatten(self)
jax._src.checkify.NaNError.tree_unflatten(cls,metadata,_)
jax._src.checkify.OOBError(self,traceback_info,primitive_name,operand_shape,payload)
jax._src.checkify.OOBError.__init__(self,traceback_info,primitive_name,operand_shape,payload)
jax._src.checkify.OOBError.__str__(self)
jax._src.checkify.OOBError.get_effect_type(self)
jax._src.checkify.OOBError.tree_flatten(self)
jax._src.checkify.OOBError.tree_unflatten(cls,metadata,payload)
jax._src.checkify._check(pred,msg,debug,*fmt_args,**fmt_kwargs)
jax._src.checkify._check_error(error,*,debug=False)
jax._src.checkify._flatten_and_get_error_metadata_thunk(*invals)
jax._src.checkify._pp_check(eqn,context,settings)->core.pp.Doc
jax._src.checkify._reduce_any_error(error:Error)
jax._src.checkify.assert_func(error:Error,pred:Bool,new_error:JaxException)->Error
jax._src.checkify.check(pred:Bool,msg:str,*fmt_args,**fmt_kwargs)->None
jax._src.checkify.check_abstract_eval(*args,err_tree,debug)
jax._src.checkify.check_batching_rule(batched_args,batch_dims,*,err_tree,debug)
jax._src.checkify.check_discharge_rule(error,enabled_errors,*args,err_tree,debug)
jax._src.checkify.check_error(error:Error)->None
jax._src.checkify.check_impl(*args,err_tree,debug)
jax._src.checkify.check_jvp_rule(primals,_,*,err_tree,debug)
jax._src.checkify.check_lowering_rule(ctx,*args,err_tree,debug)
jax._src.checkify.check_lowering_rule_unsupported(*a,debug,**k)
jax._src.checkify.check_nans(prim,error,enabled_errors,out)
jax._src.checkify.checkify(f:Callable[...,Out],errors:frozenset[ErrorCategory]=user_checks)->Callable[..., tuple[Error, Out]]
jax._src.checkify.checkify_jaxpr(jaxpr:core.ClosedJaxpr,enabled_errors,error:Error,*args)->tuple[Error, list[core.Value]]
jax._src.checkify.checkify_jaxpr_flat(jaxpr:core.Jaxpr,consts:Sequence[core.Value],enabled_errors,err_tree:PyTreeDef,*args:core.Value)->tuple[Error, list[Any]]
jax._src.checkify.checkify_jaxpr_flat_hashable(jaxpr,hashable_consts,enabled_errors,err_tree,*args)
jax._src.checkify.checkify_while_body_jaxpr(cond_jaxpr:core.ClosedJaxpr,body_jaxpr:core.ClosedJaxpr,enabled_errors,error:Error,c_consts_num:int)->tuple[core.ClosedJaxpr, PyTreeDef, set[ErrorEffect]]
jax._src.checkify.cond_error_check(error:Error,enabled_errors,index,*ops,branches,linear)
jax._src.checkify.custom_jvp_call_rule(in_err,enabled_errors,*in_vals,num_consts,jvp_jaxpr_thunk,call_jaxpr,**params)
jax._src.checkify.custom_vjp_call_jaxpr_rule(in_err,enabled_errors,*in_vals,fun_jaxpr,fwd_jaxpr_thunk,num_consts,bwd,out_trees,symbolic_zeros)
jax._src.checkify.debug_check(pred:Bool,msg:str,*fmt_args,**fmt_kwargs)->None
jax._src.checkify.default_checkify_rule(primitive:core.Primitive,error:Error,enabled_errors,*invals:core.Value,**params:Any)->tuple[Error, Sequence[core.Value]]
jax._src.checkify.div_error_check(error,enabled_errors,x,y)
jax._src.checkify.dynamic_slice_error_check(error,enabled_errors,operand,*start_indices,slice_sizes)
jax._src.checkify.dynamic_update_slice_error_check(error,enabled_errors,operand,update,*start_indices)
jax._src.checkify.flatten_fun_output(*args)
jax._src.checkify.gather_error_check(error,enabled_errors,operand,start_indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.checkify.get_shaped_aval(val)
jax._src.checkify.get_traceback()
jax._src.checkify.ignore_error_output_jaxpr(jaxpr,num_error_vals)
jax._src.checkify.is_scalar_pred(pred)->bool
jax._src.checkify.jaxpr_to_checkify_jaxpr(jaxpr:core.ClosedJaxpr,enabled_errors,err_tree:PyTreeDef,*flat_err_and_in_vals)->tuple[core.ClosedJaxpr, PyTreeDef, set[ErrorEffect]]
jax._src.checkify.lift_jvp(num_errs,num_consts,jvp_jaxpr_thunk)
jax._src.checkify.nan_error_check(prim,error,enabled_errors,*in_vals,**params)
jax._src.checkify.oob_payload(oob_mask,indices,dims_map,operand_shape)
jax._src.checkify.pjit_error_check(error,enabled_errors,*vals_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,inline,keep_unused)
jax._src.checkify.popattr(obj,attrname)
jax._src.checkify.python_err(err_tree,*args)
jax._src.checkify.scan_error_check(error,enabled_errors,*in_flat,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.checkify.scatter_error_check(prim,error,enabled_errors,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.checkify.scatter_oob(operand,indices,updates,dnums)
jax._src.checkify.setnewattr(obj,name,val)
jax._src.checkify.update_error(error,pred,code,metadata,payload,effect_type)
jax._src.checkify.while_loop_error_check(error,enabled_errors,*in_flat,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/linear_util.py----------------------------------------
A:jax._src.linear_util._EMPTY_STORE_VALUE->EmptyStoreValue()
A:jax._src.linear_util.self._store->Store()
A:jax._src.linear_util.val->property(operator.attrgetter('_store.val'))
A:jax._src.linear_util.okay->bool(self._store._val == val)
A:jax._src.linear_util.gen->gen(*gen_static_args + tuple(args), **kwargs)
A:jax._src.linear_util.(args, kwargs)->next(gen)
A:jax._src.linear_util.ans->call(fun, *args)
A:jax._src.linear_util.(gen, out_store)->stack.pop()
A:jax._src.linear_util.transformation_stack->map(transform_to_str, enumerate(self.transforms))
A:jax._src.linear_util.cache->fun_caches.setdefault(fun.f, {})
A:jax._src.linear_util.result->fun_caches.setdefault(fun.f, {}).get(key, None)
A:jax._src.linear_util.cache_clearing_funs->weakref.WeakSet()
A:jax._src.linear_util.out1->aux1()
A:jax._src.linear_util.out2->aux2()
jax._src.linear_util.EmptyStoreValue
jax._src.linear_util.EqualStore(self)
jax._src.linear_util.EqualStore.__init__(self)
jax._src.linear_util.EqualStore.reset(self)
jax._src.linear_util.EqualStore.store(self,val)
jax._src.linear_util.Store(self)
jax._src.linear_util.Store.__init__(self)
jax._src.linear_util.Store.__nonzero__(self)
jax._src.linear_util.Store.reset(self)
jax._src.linear_util.Store.store(self,val)
jax._src.linear_util.Store.val(self)
jax._src.linear_util.StoreException(Exception)
jax._src.linear_util.TracingDebugInfo(NamedTuple)
jax._src.linear_util.WrappedFun(self,f,transforms,stores,params,in_type,debug_info)
jax._src.linear_util.WrappedFun.__eq__(self,other)
jax._src.linear_util.WrappedFun.__hash__(self)
jax._src.linear_util.WrappedFun.__init__(self,f,transforms,stores,params,in_type,debug_info)
jax._src.linear_util.WrappedFun.__name__(self)
jax._src.linear_util.WrappedFun.__repr__(self)
jax._src.linear_util.WrappedFun.call_wrapped(self,*args,**kwargs)
jax._src.linear_util.WrappedFun.populate_stores(self,stores)
jax._src.linear_util.WrappedFun.wrap(self,gen,gen_static_args,out_store)->WrappedFun
jax._src.linear_util._check_input_type(in_type:core.InputType)->None
jax._src.linear_util._copy_main_traces(x)
jax._src.linear_util.add_debug_info(f:WrappedFun,debug_info:TracingDebugInfo|None)->WrappedFun
jax._src.linear_util.annotate(f:WrappedFun,in_type:core.InputType|None)->WrappedFun
jax._src.linear_util.cache(call:Callable)
jax._src.linear_util.clear_all_caches()
jax._src.linear_util.fun_name(f)
jax._src.linear_util.hashable_partial(*args)
jax._src.linear_util.merge_linear_aux(aux1,aux2)
jax._src.linear_util.transformation(gen,fun:WrappedFun,*gen_static_args)->WrappedFun
jax._src.linear_util.transformation_with_aux(gen,fun:WrappedFun,*gen_static_args,use_eq_store=False)->tuple[WrappedFun, Any]
jax._src.linear_util.wrap_init(f,params=None)->WrappedFun


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/errors.py----------------------------------------
A:jax._src.errors.export->set_module('jax.errors')
jax._src.errors.ConcretizationTypeError(self,tracer:core.Tracer,context:str='')
jax._src.errors.ConcretizationTypeError.__init__(self,tracer:core.Tracer,context:str='')
jax._src.errors.JAXIndexError(_JAXErrorMixin,IndexError)
jax._src.errors.JAXTypeError(_JAXErrorMixin,TypeError)
jax._src.errors.NonConcreteBooleanIndexError(self,tracer:core.Tracer)
jax._src.errors.NonConcreteBooleanIndexError.__init__(self,tracer:core.Tracer)
jax._src.errors.TracerArrayConversionError(self,tracer:core.Tracer)
jax._src.errors.TracerArrayConversionError.__init__(self,tracer:core.Tracer)
jax._src.errors.TracerBoolConversionError(self,tracer:core.Tracer)
jax._src.errors.TracerBoolConversionError.__init__(self,tracer:core.Tracer)
jax._src.errors.TracerIntegerConversionError(self,tracer:core.Tracer)
jax._src.errors.TracerIntegerConversionError.__init__(self,tracer:core.Tracer)
jax._src.errors.UnexpectedTracerError(self,msg:str)
jax._src.errors.UnexpectedTracerError.__init__(self,msg:str)
jax._src.errors._JAXErrorMixin(self,message:str)
jax._src.errors._JAXErrorMixin.__init__(self,message:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/cache_key.py----------------------------------------
A:jax._src.cache_key.logger->logging.getLogger(__name__)
A:jax._src.cache_key.hash_obj->hashlib.sha256()
A:jax._src.cache_key.fresh_hash_obj->hashlib.sha256()
A:jax._src.cache_key.output->io.BytesIO()
A:jax._src.cache_key.m->m_original.operation.clone()
A:jax._src.cache_key.passes->jax._src.lib.mlir.passmanager.PassManager.parse('builtin.module(strip-debuginfo)')
A:jax._src.cache_key.canonical_ir->_canonicalize_ir(module)
A:jax._src.cache_key.compile_options_copy->copy.deepcopy(compile_options_obj)
A:jax._src.cache_key.num_actual_options->len([x for x in dir(compile_options_obj) if not x.startswith('_')])
A:jax._src.cache_key.actual_options->len([x for x in dir(executable_obj) if not x.startswith('_')])
A:jax._src.cache_key.xla_flags_env_var->os.getenv('XLA_FLAGS')
jax._src.cache_key._canonicalize_ir(m_original:ir.Module)->bytes
jax._src.cache_key._hash_accelerator__config_module(hash_obj,accelerators:np.ndarray)
jax._src.cache_key._hash_accelerator_config(hash_obj,accelerators:np.ndarray)
jax._src.cache_key._hash_bool(hash_obj,bool_var)
jax._src.cache_key._hash_bool_list(hash_obj,bool_list)
jax._src.cache_key._hash_compile_options(hash_obj,compile_options_obj)
jax._src.cache_key._hash_computation(hash_obj,module)
jax._src.cache_key._hash_debug_options(hash_obj,debug_obj)
jax._src.cache_key._hash_devices(hash_obj,devices:np.ndarray)->None
jax._src.cache_key._hash_executable_build_options(hash_obj,executable_obj)
jax._src.cache_key._hash_float(hash_obj,float_var)
jax._src.cache_key._hash_int(hash_obj,int_var)
jax._src.cache_key._hash_int_list(hash_obj,int_list)
jax._src.cache_key._hash_platform(hash_obj,backend)
jax._src.cache_key._hash_serialized_compile_options(hash_obj,compile_options_obj)
jax._src.cache_key._hash_signed_int(hash_obj,int_var)
jax._src.cache_key._hash_string(hash_obj,str_var)
jax._src.cache_key._hash_xla_flags(hash_obj,extra_flag_prefixes:list[str])
jax._src.cache_key._log_cache_key_hash(hash_obj,last_serialized:str,hashfn)
jax._src.cache_key._serialize_ir(m:ir.Module)->bytes
jax._src.cache_key.add_flag_prefixes(flag_prefixes:list[str])->None
jax._src.cache_key.clear_flag_prefixes()->None
jax._src.cache_key.get(module:ir.Module,devices:np.ndarray,compile_options:xla_client.CompileOptions,backend:xla_client.Client,compression_algorithm:str='zstandard',produce_original_cache_key:bool=True)->str
jax._src.cache_key.get_flag_prefixes()->list[str]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/gfile_cache.py----------------------------------------
A:jax._src.gfile_cache.self._path->jax._src.path.Path(path)
jax._src.gfile_cache.GFileCache(self,path:str)
jax._src.gfile_cache.GFileCache.__init__(self,path:str)
jax._src.gfile_cache.GFileCache.get(self,key:str)
jax._src.gfile_cache.GFileCache.put(self,key:str,value:bytes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/monitoring.py----------------------------------------
A:jax._src.monitoring.size->len(_event_duration_secs_listeners)
jax._src.monitoring.EventDurationListenerWithMetadata(self,event:str,duration_secs:float,**kwargs:Union[str,int])
jax._src.monitoring.EventDurationListenerWithMetadata.__call__(self,event:str,duration_secs:float,**kwargs:Union[str,int])
jax._src.monitoring.EventListenerWithMetadata(self,event:str,**kwargs:Union[str,int])
jax._src.monitoring.EventListenerWithMetadata.__call__(self,event:str,**kwargs:Union[str,int])
jax._src.monitoring._unregister_event_duration_listener_by_callback(callback:EventDurationListenerWithMetadata)->None
jax._src.monitoring._unregister_event_duration_listener_by_index(index:int)->None
jax._src.monitoring._unregister_event_listener_by_callback(callback:EventListenerWithMetadata)->None
jax._src.monitoring.clear_event_listeners()
jax._src.monitoring.get_event_duration_listeners()->list[EventDurationListenerWithMetadata]
jax._src.monitoring.get_event_listeners()->list[EventListenerWithMetadata]
jax._src.monitoring.record_event(event:str,**kwargs:Union[str,int])->None
jax._src.monitoring.record_event_duration_secs(event:str,duration:float,**kwargs:Union[str,int])->None
jax._src.monitoring.register_event_duration_secs_listener(callback:EventDurationListenerWithMetadata)->None
jax._src.monitoring.register_event_listener(callback:EventListenerWithMetadata)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/op_shardings.py----------------------------------------
A:jax._src.op_shardings.partitions->hlo_sharding.tile_assignment_dimensions()
A:jax._src.op_shardings.subgroup_types->hlo_sharding.subgroup_types()
A:jax._src.op_shardings.replicate_on_last_tile_dim->hlo_sharding.replicate_on_last_tile_dim()
A:jax._src.op_shardings.op->jax._src.lib.xla_client.HloSharding.from_proto(op)
A:jax._src.op_shardings.indices->op_sharding_to_numpy_indices(op_sharding, shape, num_devices)
A:jax._src.op_shardings.(partitions, num_replicas)->get_num_ways_dim_sharded(hlo_sharding)
A:jax._src.op_shardings.(shard_size, ragged)->divmod(dim, n_shards)
A:jax._src.op_shardings.device_it->iter(hlo_sharding.tile_assignment_devices())
jax._src.op_shardings.are_op_shardings_equal(op1:Union[xc.OpSharding,xc.HloSharding],op2:Union[xc.OpSharding,xc.HloSharding])->bool
jax._src.op_shardings.get_num_ways_dim_sharded(hlo_sharding:xc.HloSharding)->tuple[Sequence[int], int]
jax._src.op_shardings.is_op_sharding_replicated(op:Union[xc.OpSharding,xc.HloSharding])->bool
jax._src.op_shardings.op_sharding_to_indices(op_sharding:xc.HloSharding,shape:Sequence[int],num_devices:int)->tuple[tuple[slice, ...], ...]
jax._src.op_shardings.op_sharding_to_numpy_indices(hlo_sharding:xc.HloSharding,shape:Sequence[int],num_devices:int)->np.ndarray


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/test_util.py----------------------------------------
A:jax._src.test_util._TEST_DUT->jax._src.config.DEFINE_string('jax_test_dut', '', help='Describes the device under test in case special consideration is required.')
A:jax._src.test_util.NUM_GENERATED_CASES->jax._src.config.DEFINE_integer('jax_num_generated_cases', int(os.getenv('JAX_NUM_GENERATED_CASES', '10')), help='Number of generated cases to test')
A:jax._src.test_util._MAX_CASES_SAMPLING_RETRIES->jax._src.config.DEFINE_integer('max_cases_sampling_retries', int(os.getenv('JAX_MAX_CASES_SAMPLING_RETRIES', '100')), 'Number of times a failed test sample should be retried. When an unseen case cannot be generated in this many trials, the sampling process is terminated.')
A:jax._src.test_util._SKIP_SLOW_TESTS->jax._src.config.DEFINE_bool('jax_skip_slow_tests', config.bool_env('JAX_SKIP_SLOW_TESTS', False), help='Skip tests marked as slow (> 5 sec).')
A:jax._src.test_util._TEST_TARGETS->jax._src.config.DEFINE_string('test_targets', os.getenv('JAX_TEST_TARGETS', ''), 'Regular expression specifying which tests to run, called via re.search on the test name. If empty or unspecified, run all tests.')
A:jax._src.test_util._EXCLUDE_TEST_TARGETS->jax._src.config.DEFINE_string('exclude_test_targets', os.getenv('JAX_EXCLUDE_TEST_TARGETS', ''), 'Regular expression specifying which tests NOT to run, called via re.search on the test name. If empty or unspecified, run all tests.')
A:jax._src.test_util.TEST_WITH_PERSISTENT_COMPILATION_CACHE->jax._src.config.DEFINE_bool('jax_test_with_persistent_compilation_cache', config.bool_env('JAX_TEST_WITH_PERSISTENT_COMPILATION_CACHE', False), help='If enabled, the persistent compilation cache will be enabled for all test cases. This can be used to increase compilation cache coverage.')
A:jax._src.test_util.kSanitizeNameRE->re.compile('[ \\"\'\\[\\](){}<>=,._]+')
A:jax._src.test_util.arr->numpy.asarray(arr)
A:jax._src.test_util.dtype->numpy.dtype(dtype)
A:jax._src.test_util.result->func(*args, **kwargs)
A:jax._src.test_util.tol1->_normalize_tolerance(tol1)
A:jax._src.test_util.tol2->_normalize_tolerance(tol2)
A:jax._src.test_util.out[k]->max(v, tol1.get(k, 0))
A:jax._src.test_util.assert_close->partial(_assert_numpy_allclose, err_msg=err_msg)
A:jax._src.test_util.batched_device_put_and_count->make_fn_and_count(batched_device_put)
A:jax._src.test_util.tags->_get_device_tags()
A:jax._src.test_util.device_tags->_get_device_tags()
A:jax._src.test_util.test_name->getattr(test_method, '__name__', '[unknown test]')
A:jax._src.test_util.prev_xla_flags->os.getenv('XLA_FLAGS')
A:jax._src.test_util.flag_value->jax._src.config._read(flag_name)
A:jax._src.test_util.NUMPY_SCALAR_SHAPE->_NumpyScalar()
A:jax._src.test_util.PYTHON_SCALAR_SHAPE->_PythonScalar()
A:jax._src.test_util.shape->tuple(shape)
A:jax._src.test_util.shapestr->','.join((str(dim) for dim in shape))
A:jax._src.test_util.vals->numpy.where(zeros, np.array(0, dtype=dtype), vals)
A:jax._src.test_util.x_ravel->numpy.asarray(x).ravel()
A:jax._src.test_util.base_rand->rand_default(rng)
A:jax._src.test_util.dims->_dims_of_shape(shape)
A:jax._src.test_util.r->numpy.random.RandomState(42).rand(*dims)
A:jax._src.test_util.jaxpr->jax._src.api.make_jaxpr(partial(fun, **kwargs))(*args)
A:jax._src.test_util.seen->set()
A:jax._src.test_util.rng->numpy.random.RandomState(42)
A:jax._src.test_util.x->numpy.asarray(x)
A:jax._src.test_util.cases->list(gen(choose_one))
A:jax._src.test_util.names->super().getTestCaseNames(testCaseClass)
A:jax._src.test_util.pattern->re.compile(_EXCLUDE_TEST_TARGETS.value)
A:jax._src.test_util.(flat_args, tree)->tree_flatten(args)
A:jax._src.test_util.args->args_maker()
A:jax._src.test_util.self._original_config[key]->jax._src.config._read(key)
A:jax._src.test_util.self._rng->numpy.random.RandomState(zlib.adler32(self._testMethodName.encode()))
A:jax._src.test_util.cls._compilation_cache_exit_stack->ExitStack()
A:jax._src.test_util.tmp_dir->stack.enter_context(tempfile.TemporaryDirectory())
A:jax._src.test_util.y->numpy.asarray(y)
A:jax._src.test_util.atol->max(tolerance(_dtype(x), atol), tolerance(_dtype(y), atol))
A:jax._src.test_util.rtol->max(tolerance(_dtype(x), rtol), tolerance(_dtype(y), rtol))
A:jax._src.test_util.expected->textwrap.dedent(expected)
A:jax._src.test_util.what->textwrap.dedent(what)
A:jax._src.test_util.ignore_space_re->re.compile('\\s*\\n\\s*')
A:jax._src.test_util.expected_clean->re.sub(ignore_space_re, '\n', expected.strip())
A:jax._src.test_util.what_clean->re.sub(ignore_space_re, '\n', what.strip())
A:jax._src.test_util.python_ans->fun(*args)
A:jax._src.test_util.python_shapes->tree_map(lambda x: np.shape(x), python_ans)
A:jax._src.test_util.np_shapes->tree_map(lambda x: np.shape(np.asarray(x)), python_ans)
A:jax._src.test_util.cfun->jax._src.api.jit(wrapped_fun)
A:jax._src.test_util.monitored_ans->cfun(*args)
A:jax._src.test_util.compiled_ans->cfun(*args)
A:jax._src.test_util.lax_ans->lax_op(*args)
A:jax._src.test_util.numpy_ans->numpy_reference_op(*args)
A:jax._src.test_util.(axis_names, shape)->unzip2(named_shape)
A:jax._src.test_util.size->math.prod(mesh_shape)
A:jax._src.test_util.local_devices->list(jax.local_devices())
A:jax._src.test_util.mesh_devices->numpy.array(devices[:size]).reshape(mesh_shape)
A:jax._src.test_util.devices->sorted(jax.devices(), key=lambda d: d.id)
A:jax._src.test_util.global_mesh->jax.sharding.Mesh(mesh_devices, axis_names)
A:jax._src.test_util.null->object()
A:jax._src.test_util.self._value->self._method(obj)
A:jax._src.test_util.supported->supported_dtypes()
A:jax._src.test_util.dtypes->_LazyDtypes()
A:jax._src.test_util._version_regex->re.compile('([0-9]+(?:\\.[0-9]+)*)(?:(rc|dev).*)?')
A:jax._src.test_util.m->re.compile('([0-9]+(?:\\.[0-9]+)*)(?:(rc|dev).*)?').match(v)
A:jax._src.test_util.testcase_name->'_'.join((f'{k}={kw[k]}' for k in sorted(kw.keys())))
A:jax._src.test_util.kw['testcase_name']->sanitize_test_name(testcase_name)
A:jax._src.test_util.filtered->tuple((kw for kw in kwargs_with_testcase_name if one_containing in kw['testcase_name']))
A:jax._src.test_util.(fwd_jaxpr, (y_shape, res_shape))->jax.make_jaxpr(lambda *args: jax.vjp(f, *args), return_shape=True)(*example_args)
A:jax._src.test_util.bwd_jaxpr->jax.make_jaxpr(lambda res, outs: res(outs))(res_shape, y_shape)
jax._src.test_util.BufferDonationTestCase(JaxTestCase)
jax._src.test_util.BufferDonationTestCase._assertDeleted(self,x,deleted)
jax._src.test_util.JaxTestCase(parameterized.TestCase)
jax._src.test_util.JaxTestCase._CheckAgainstNumpy(self,numpy_reference_op,lax_op,args_maker,check_dtypes=True,tol=None,atol=None,rtol=None,canonicalize_dtypes=True)
jax._src.test_util.JaxTestCase._CompileAndCheck(self,fun,args_maker,*,check_dtypes=True,tol=None,rtol=None,atol=None,check_cache_misses=True)
jax._src.test_util.JaxTestCase.assertAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None,canonicalize_dtypes=True,err_msg='')
jax._src.test_util.JaxTestCase.assertArraysAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None,err_msg='')
jax._src.test_util.JaxTestCase.assertArraysEqual(self,x,y,*,check_dtypes=True,err_msg='',allow_object_dtype=False)
jax._src.test_util.JaxTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax._src.test_util.JaxTestCase.assertMultiLineStrippedEqual(self,expected,what)
jax._src.test_util.JaxTestCase.assertNoWarnings(self)
jax._src.test_util.JaxTestCase.rng(self)
jax._src.test_util.JaxTestCase.setUp(self)
jax._src.test_util.JaxTestCase.setUpClass(cls)
jax._src.test_util.JaxTestCase.tearDown(self)
jax._src.test_util.JaxTestCase.tearDownClass(cls)
jax._src.test_util.JaxTestLoader(absltest.TestLoader)
jax._src.test_util.JaxTestLoader.getTestCaseNames(self,testCaseClass)
jax._src.test_util.ScalarShape
jax._src.test_util.ScalarShape.__getitem__(self,i)
jax._src.test_util.ScalarShape.__len__(self)
jax._src.test_util._LazyDtypes
jax._src.test_util._LazyDtypes.all(self)
jax._src.test_util._LazyDtypes.all_floating(self)
jax._src.test_util._LazyDtypes.all_inexact(self)
jax._src.test_util._LazyDtypes.all_integer(self)
jax._src.test_util._LazyDtypes.all_unsigned(self)
jax._src.test_util._LazyDtypes.boolean(self)
jax._src.test_util._LazyDtypes.complex(self)
jax._src.test_util._LazyDtypes.custom_floats(self)
jax._src.test_util._LazyDtypes.floating(self)
jax._src.test_util._LazyDtypes.inexact(self)
jax._src.test_util._LazyDtypes.integer(self)
jax._src.test_util._LazyDtypes.numeric(self)
jax._src.test_util._LazyDtypes.supported(self,dtypes)
jax._src.test_util._LazyDtypes.unsigned(self)
jax._src.test_util._NumpyScalar(ScalarShape)
jax._src.test_util._PythonScalar(ScalarShape)
jax._src.test_util._cached_property(self,method)
jax._src.test_util._cached_property.__get__(self,obj,cls)
jax._src.test_util._cached_property.__init__(self,method)
jax._src.test_util._cast_to_shape(value,shape,dtype)
jax._src.test_util._choice(n,m)
jax._src.test_util._device_filter(predicate)
jax._src.test_util._dims_of_shape(shape)
jax._src.test_util._format_shape_dtype_string(shape,dtype)
jax._src.test_util._get_device_tags()
jax._src.test_util._normalize_tolerance(tol)
jax._src.test_util._parse_version(v:str)->tuple[int, ...]
jax._src.test_util._rand_dtype(rand,shape,dtype,scale=1.0,post=lambdax:x)
jax._src.test_util.assert_dot_precision(expected_precision,fun,*args)
jax._src.test_util.assert_dot_preferred_element_type(expected,fun,*args,**kwargs)
jax._src.test_util.assert_num_jit_and_pmap_compilations(times)
jax._src.test_util.capture_stdout()->Generator[Callable[[], str], None, None]
jax._src.test_util.cases_from_gens(*gens)
jax._src.test_util.check_eq(xs,ys,err_msg='')
jax._src.test_util.check_raises(thunk,err_type,msg)
jax._src.test_util.check_raises_regexp(thunk,err_type,pattern)
jax._src.test_util.count_aot_jit_cpp_cache_miss()
jax._src.test_util.count_device_put()
jax._src.test_util.count_jit_and_pmap_compiles()
jax._src.test_util.count_pjit_cpp_cache_miss()
jax._src.test_util.count_primitive_compiles()
jax._src.test_util.count_subjaxpr_to_mhlo_conversion(fun_name:str)
jax._src.test_util.create_global_mesh(mesh_shape,axis_names)
jax._src.test_util.device_supports_buffer_donation()
jax._src.test_util.device_under_test()
jax._src.test_util.dtype_str(dtype)
jax._src.test_util.format_shape_dtype_string(shape,dtype)
jax._src.test_util.format_test_name_suffix(opname,shapes,dtypes)
jax._src.test_util.fwd_bwd_jaxprs(f,*example_args)
jax._src.test_util.ignore_warning(**kw)
jax._src.test_util.is_cloud_tpu()
jax._src.test_util.is_device_cuda()
jax._src.test_util.is_device_rocm()
jax._src.test_util.is_device_tpu_v4()
jax._src.test_util.is_sequence(x)
jax._src.test_util.is_valid_shape(shape,dtype)
jax._src.test_util.iter_eqns(jaxpr)
jax._src.test_util.join_tolerance(tol1,tol2)
jax._src.test_util.named_cases_from_sampler(gen)
jax._src.test_util.num_float_bits(dtype)
jax._src.test_util.numpy_version()
jax._src.test_util.parameterized_filterable(*,kwargs:Sequence[dict[str,Any]],testcase_name:Optional[Callable[[dict[str,Any]],str]]=None,one_containing:Optional[str]=None)
jax._src.test_util.promote_like_jnp(fun,inexact=False)
jax._src.test_util.pytest_mark_if_available(marker:str)
jax._src.test_util.rand_bool(rng)
jax._src.test_util.rand_default(rng,scale=3)
jax._src.test_util.rand_fullrange(rng,standardize_nans=False)
jax._src.test_util.rand_int(rng,low=0,high=None)
jax._src.test_util.rand_nonzero(rng)
jax._src.test_util.rand_not_small(rng,offset=10.0)
jax._src.test_util.rand_positive(rng)
jax._src.test_util.rand_small(rng)
jax._src.test_util.rand_small_positive(rng)
jax._src.test_util.rand_some_equal(rng)
jax._src.test_util.rand_some_inf(rng)
jax._src.test_util.rand_some_inf_and_nan(rng)
jax._src.test_util.rand_some_nan(rng)
jax._src.test_util.rand_some_zero(rng)
jax._src.test_util.rand_uniform(rng,low=0.0,high=1.0)
jax._src.test_util.rand_unique_int(rng,high=None)
jax._src.test_util.register_event_duration_listener(callback)
jax._src.test_util.run_on_devices(*enabled_devices)
jax._src.test_util.sample_product(*args,**kw)
jax._src.test_util.sample_product_testcases(*args,**kw)
jax._src.test_util.sanitize_test_name(s:str)->str
jax._src.test_util.set_env(**kwargs)
jax._src.test_util.set_host_platform_device_count(nr_devices:int)
jax._src.test_util.skip_on_devices(*disabled_devices)
jax._src.test_util.skip_on_flag(flag_name,skip_value)
jax._src.test_util.strict_promotion_if_dtypes_match(dtypes)
jax._src.test_util.supported_dtypes()
jax._src.test_util.test_device_matches(device_types:Iterable[str])->bool
jax._src.test_util.to_default_dtype(arr)
jax._src.test_util.with_and_without_mesh(f)
jax._src.test_util.with_config(**kwds)
jax._src.test_util.with_jax_dtype_defaults(func,use_defaults=True)
jax._src.test_util.with_mesh(named_shape:MeshSpec)->Generator[None, None, None]
jax._src.test_util.with_mesh_from_kwargs(f)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/logging_config.py----------------------------------------
A:jax._src.logging_config._debug_handler->logging.StreamHandler(sys.stderr)
A:jax._src.logging_config.logger->logging.getLogger(logger_name)
jax._src.logging_config.disable_all_debug_logging()
jax._src.logging_config.enable_debug_logging(logger_name)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/random.py----------------------------------------
A:jax._src.random.wrapped_key->jax._src.prng.random_wrap(key, impl=default_prng_impl())
A:jax._src.random.keys_fmt->', '.join((f'"{s}"' for s in prng.prngs.keys()))
A:jax._src.random.t->type(impl_spec)
A:jax._src.random.impl->resolve_prng_impl(impl_spec)
A:jax._src.random.default_impl->default_prng_impl()
A:jax._src.random.key->jax._src.prng.random_seed(seed, impl=impl)
A:jax._src.random.(key, wrapped)->_check_prng_key(key)
A:jax._src.random.(typed_key, wrapped)->_check_prng_key(key)
A:jax._src.random.keys_dtype->typing.cast(prng.KeyTy, key.dtype)
A:jax._src.random.(typed_keys, _)->_check_prng_key(keys)
A:jax._src.random.(keys, _)->_check_prng_key(keys)
A:jax._src.random.impl_obj->resolve_prng_impl(impl)
A:jax._src.random.shape->numpy.shape(sigma)
A:jax._src.random.shape_->jax.lax.broadcast_shapes(shape.positional, *param_shapes)
A:jax._src.random.(key, _)->_check_prng_key(key)
A:jax._src.random.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.random.minval->jax.lax.broadcast_to_rank(minval, len(shape))
A:jax._src.random.maxval->jax.lax.broadcast_to_rank(maxval, len(shape))
A:jax._src.random.finfo->jax.numpy.finfo(dtype)
A:jax._src.random.bits->jax.lax.convert_element_type(bits, uint_dtype)
A:jax._src.random.float_bits->jax.lax.bitwise_or(lax.shift_right_logical(bits, np.array(rng_bits - nmant, uint_dtype)), np.array(1.0, dtype).view(uint_dtype))
A:jax._src.random.maxval_out_of_range->jax.lax.gt(maxval, _convert_and_clip_integer(jnp.array(jnp.iinfo(dtype).max, dtype), maxval.dtype))
A:jax._src.random.(k1, k2)->_split(key, 2)
A:jax._src.random.span->jax.lax.select(maxval_out_of_range & (maxval > minval), lax.add(span, _lax_const(span, 1)), span)
A:jax._src.random.multiplier->jax.lax.rem(lax.mul(multiplier, multiplier), span)
A:jax._src.random.random_offset->jax.lax.rem(random_offset, span)
A:jax._src.random.axis->canonicalize_axis(axis, arr.ndim)
A:jax._src.random.r->rademacher(keys[1], shape, dtype)
A:jax._src.random.ind->jax.numpy.searchsorted(p_cuml, r).astype(int)
A:jax._src.random.num_rounds->int(np.ceil(exponent * np.log(max(1, x.size)) / np.log(uint32max)))
A:jax._src.random.(key, subkey)->_split(key)
A:jax._src.random.sort_keys->_random_bits(subkey, 32, x.shape)
A:jax._src.random.(_, x)->jax.lax.sort_key_val(sort_keys, x, axis)
A:jax._src.random.arr->jax.numpy.asarray(a)
A:jax._src.random.n_inputs->jax._src.core.concrete_or_error(int, a, 'The error occurred in jax.random.choice()')
A:jax._src.random.n_draws->math.prod(shape)
A:jax._src.random.(p_arr,)->promote_dtypes_inexact(p)
A:jax._src.random.p_cuml->jax.numpy.cumsum(p_arr)
A:jax._src.random.sqrt2->numpy.array(np.sqrt(2), dtype)
A:jax._src.random.(key_re, key_im)->_split(key)
A:jax._src.random._re->_normal_real(key_re, shape, real_dtype).astype(dtype)
A:jax._src.random._im->_normal_real(key_im, shape, real_dtype).astype(dtype)
A:jax._src.random.lo->numpy.nextafter(np.array(-1.0, dtype), np.array(0.0, dtype), dtype=dtype)
A:jax._src.random.hi->numpy.array(1.0, dtype)
A:jax._src.random.u->uniform(key, shape, dtype)
A:jax._src.random.(mean, cov)->promote_dtypes_inexact(mean, cov)
A:jax._src.random.(u, s, _)->svd(cov)
A:jax._src.random.(w, v)->eigh(cov)
A:jax._src.random.factor->cholesky(cov)
A:jax._src.random.normal_samples->normal(key, shape + mean.shape[-1:], dtype)
A:jax._src.random.lower->jax.lax.convert_element_type(lower, dtype)
A:jax._src.random.upper->jax.lax.convert_element_type(upper, dtype)
A:jax._src.random.a->jax.numpy.broadcast_to(a, shape)
A:jax._src.random.b->jax.lax.convert_element_type(b, dtype)
A:jax._src.random.p->jax.lax.convert_element_type(p, dtype)
A:jax._src.random.(key_a, key_b)->_split(key)
A:jax._src.random.log_gamma_a->loggamma(key_a, a, shape, dtype)
A:jax._src.random.log_gamma_b->loggamma(key_b, b, shape, dtype)
A:jax._src.random.log_max->jax.lax.max(log_gamma_a, log_gamma_b)
A:jax._src.random.gamma_a_scaled->jax.numpy.exp(log_gamma_a - log_max)
A:jax._src.random.gamma_b_scaled->jax.numpy.exp(log_gamma_b - log_max)
A:jax._src.random.pi->_lax_const(u, np.pi)
A:jax._src.random.alpha->jax.lax.select(boost_mask, alpha, lax.add(alpha, one))
A:jax._src.random.log_gamma_samples->loggamma(key, alpha, shape + np.shape(alpha)[-1:], dtype)
A:jax._src.random.x_max->jax.numpy.max(x, axis, keepdims=True)
A:jax._src.random.unnormalized->jax.numpy.exp(x - lax.stop_gradient(x_max))
A:jax._src.random.zero->jax._src.lax.lax._const(sample, 0)
A:jax._src.random.one->_lax_const(alpha, 1)
A:jax._src.random.minus_one->_lax_const(alpha, -1)
A:jax._src.random.one_over_two->_lax_const(alpha, 0.5)
A:jax._src.random.one_over_three->_lax_const(alpha, 1.0 / 3.0)
A:jax._src.random.squeeze_const->_lax_const(alpha, 0.0331)
A:jax._src.random.boost_mask->jax.lax.ge(alpha, one)
A:jax._src.random.d->jax._src.core.concrete_or_error(index, d, 'The error occurred in jax.random.ball()')
A:jax._src.random.c->jax.lax.div(one_over_three, lax.sqrt(d))
A:jax._src.random.cond->jax.lax.bitwise_and(lax.ge(U, lax.sub(one, lax.mul(squeeze_const, lax.mul(X, X)))), lax.ge(lax.log(U), lax.add(lax.mul(X, one_over_two), lax.mul(d, lax.add(lax.sub(one, V), lax.log(V))))))
A:jax._src.random.x->uniform(key, shape, dtype, minval=jnp.finfo(dtype).eps, maxval=1.0)
A:jax._src.random.v->normal(k1, shape, dtype)
A:jax._src.random.(key, x_key, U_key)->_split(key, 3)
A:jax._src.random.(_, x, v)->jax.lax.while_loop(lambda kxv: lax.le(kxv[2], zero), _next_kxv, (x_key, zero, minus_one))
A:jax._src.random.X->jax.lax.mul(x, x)
A:jax._src.random.V->jax.lax.mul(lax.mul(v, v), v)
A:jax._src.random.U->uniform(U_key, (), dtype=dtype)
A:jax._src.random.(_, _, V, _)->jax.lax.while_loop(_cond_fn, _body_fn, (key, zero, one, _lax_const(alpha, 2)))
A:jax._src.random.log_samples->jax.lax.neg(exponential(subkey, (), dtype=dtype))
A:jax._src.random.log_boost->jax.lax.select(boost_mask | (log_samples == 0), zero, lax.mul(log_samples, lax.div(one, alpha_orig)))
A:jax._src.random.boost->jax.lax.select(boost_mask, one, lax.pow(samples, lax.div(one, alpha_orig)))
A:jax._src.random.samples->jax.lax.map(lambda args: _gamma_one(*args, log_space=log_space), (keys, alphas))
A:jax._src.random.alphas->jax.numpy.broadcast_to(a, shape).flatten()
A:jax._src.random.tiny->jax.lax.full_like(samples, jnp.finfo(samples.dtype).tiny)
A:jax._src.random.grads->vmap(gamma_grad)(alphas, samples)
A:jax._src.random.a_shape->jax.numpy.shape(a)
A:jax._src.random.split_count->math.prod(a_shape[key.ndim:])
A:jax._src.random.keys->split(key)
A:jax._src.random.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax._src.random.k->jax.lax.floor((2 * a / u_shifted + b) * u + lam + 0.43)
A:jax._src.random.random_gamma_p->jax._src.core.Primitive('random_gamma')
A:jax._src.random.(rng, subkey)->_split(rng)
A:jax._src.random.k_init->jax.lax.full_like(lam, -1, lam.dtype, shape)
A:jax._src.random.log_rate_init->jax.lax.full_like(lam, 0, np.float32, shape)
A:jax._src.random.log_lam->jax.lax.log(lam)
A:jax._src.random.(key, subkey_0, subkey_1)->_split(key, 3)
A:jax._src.random.s->jax.lax.log(v * inv_alpha / (a / (u_shifted * u_shifted) + b))
A:jax._src.random.k_out->jax.lax.select(accept, k, k_out)
A:jax._src.random.accepted->jax.lax.full_like(lam, False, jnp.bool_, shape)
A:jax._src.random.lam_knuth->jax.lax.select(use_knuth, lam, lax.full_like(lam, 0.0))
A:jax._src.random.lam_rejection->jax.lax.select(use_knuth, lax.full_like(lam, 100000.0), lam)
A:jax._src.random.max_iters->jax._src.dtypes.canonicalize_dtype(dtype).type(jnp.iinfo(dtype).max)
A:jax._src.random.result->jax.lax.select(use_knuth, _poisson_knuth(key, lam_knuth, shape, dtype, max_iters), _poisson_rejection(key, lam_rejection, shape, dtype, max_iters))
A:jax._src.random.lam->jax.lax.convert_element_type(lam, np.float32)
A:jax._src.random.logits_arr->jax.numpy.asarray(logits)
A:jax._src.random.batch_shape->tuple(np.delete(logits_arr.shape, axis))
A:jax._src.random.logits_shape->list(shape[len(shape) - len(batch_shape):])
A:jax._src.random.e->exponential(k2, shape, dtype)
A:jax._src.random.df->jax.lax.convert_element_type(df, dtype)
A:jax._src.random.(key_n, key_g)->_split(key)
A:jax._src.random.n->jax._src.core.concrete_or_error(index, n, 'The error occurred in jax.random.orthogonal()')
A:jax._src.random.two->_lax_const(df, 2)
A:jax._src.random.half_df->jax.lax.div(df, two)
A:jax._src.random.g->generalized_normal(k1, p, (*shape, d), dtype)
A:jax._src.random.log_g->loggamma(key, a=half_df, shape=shape, dtype=dtype)
A:jax._src.random.chi2->jax.lax.mul(jnp.exp(log_g), two)
A:jax._src.random.dfden->jax.numpy.broadcast_to(dfden, shape)
A:jax._src.random.dfnum->jax.numpy.broadcast_to(dfnum, shape)
A:jax._src.random.(key_dfd, key_dfn)->_split(key)
A:jax._src.random.chi2_dfn->chisquare(key_dfn, dfnum, shape, dtype)
A:jax._src.random.chi2_dfd->chisquare(key_dfd, dfden, shape, dtype)
A:jax._src.random.num->jax.lax.div(chi2_dfn, dfnum)
A:jax._src.random.den->jax.lax.div(chi2_dfd, dfden)
A:jax._src.random.f->jax.lax.div(num, den)
A:jax._src.random.bernoulli_samples->bernoulli(key=key, p=0.5, shape=shape).astype(dtype)
A:jax._src.random.norm_rvs->normal(key=key, shape=shape, dtype=dtype)
A:jax._src.random.params_shapes->jax.lax.broadcast_shapes(np.shape(loc), np.shape(scale))
A:jax._src.random.(maxwell_key, rademacher_key)->_split(key)
A:jax._src.random.maxwell_rvs->maxwell(maxwell_key, shape=shape, dtype=dtype)
A:jax._src.random.random_sign->rademacher(rademacher_key, shape=shape, dtype=dtype)
A:jax._src.random.random_uniform->uniform(key=key, shape=shape, minval=0, maxval=1, dtype=dtype)
A:jax._src.random.z->uniform(k2, shape, dtype)
A:jax._src.random.(q, r)->jax.numpy.linalg.qr(z)
A:jax._src.random.scale->jax.numpy.broadcast_to(scale, shape)
A:jax._src.random.log_u->jax.lax.log(u)
A:jax._src.random.n_two->_lax_const(scale, -2)
A:jax._src.random.sqrt_u->jax.lax.sqrt(lax.mul(log_u, n_two))
A:jax._src.random.ray->jax.lax.mul(scale, sqrt_u)
A:jax._src.random.mean->jax.numpy.broadcast_to(mean, shape)
A:jax._src.random.y->jax.lax.integer_pow(v, 2)
A:jax._src.random.y_sq->jax.lax.integer_pow(y, 2)
A:jax._src.random.mean_sq->jax.lax.integer_pow(mean, 2)
A:jax._src.random.sqrt_term->jax.lax.sqrt(4 * mean * y + mean_sq * y_sq)
A:jax._src.random.w->jax.lax.select(lax.le(z, mean / (mean + x)), x, mean_sq / x)
A:jax._src.random.(p,)->promote_dtypes_inexact(p)
A:jax._src.random.log_one_minus_p->jax.numpy.broadcast_to(log_one_minus_p, shape)
A:jax._src.random.left->jax.numpy.broadcast_to(left, shape)
A:jax._src.random.mode->jax.numpy.broadcast_to(mode, shape)
A:jax._src.random.right->jax.numpy.broadcast_to(right, shape)
A:jax._src.random.tri->jax.lax.select(u < fc, out1, out2)
A:jax._src.random.sigma->jax.numpy.broadcast_to(sigma, shape)
jax._src.random.PRNGKey(seed:int|ArrayLike,*,impl:Optional[PRNGSpecDesc]=None)->KeyArray
jax._src.random.PRNGSpec(self,impl)
jax._src.random.PRNGSpec.__eq__(self,other)->bool
jax._src.random.PRNGSpec.__hash__(self)->int
jax._src.random.PRNGSpec.__init__(self,impl)
jax._src.random.PRNGSpec.__str__(self)->str
jax._src.random._bernoulli(key,p,shape)->Array
jax._src.random._beta(key,a,b,shape,dtype)->Array
jax._src.random._cauchy(key,shape,dtype)->Array
jax._src.random._check_default_impl_with_no_custom_prng(impl,name)
jax._src.random._check_prng_key(key:KeyArrayLike)->tuple[KeyArray, bool]
jax._src.random._check_shape(name:str,shape:Union[Shape,NamedShape],*param_shapes)->None
jax._src.random._chisquare(key,df,shape,dtype)->Array
jax._src.random._dirichlet(key,alpha,shape,dtype)->Array
jax._src.random._double_sided_maxwell(key,loc,scale,shape,dtype)->Array
jax._src.random._exponential(key,shape,dtype)->Array
jax._src.random._f(key,dfnum,dfden,shape,dtype)->Array
jax._src.random._fold_in(key:KeyArray,data:IntegerArray)->KeyArray
jax._src.random._gamma(key,a,shape,dtype,log_space=False)->Array
jax._src.random._gamma_batching_rule(batched_args,batch_dims,*,log_space)
jax._src.random._gamma_grad(sample,a,*,log_space)
jax._src.random._gamma_impl(key,a,*,log_space,use_vmap=False)
jax._src.random._gamma_one(key:KeyArray,alpha,log_space)->Array
jax._src.random._geometric(key,p,shape,dtype)->Array
jax._src.random._gumbel(key,shape,dtype)->Array
jax._src.random._isnan(x:ArrayLike)->Array
jax._src.random._key(ctor_name:str,seed:int|ArrayLike,impl_spec:Optional[PRNGSpecDesc])->KeyArray
jax._src.random._key_data(keys:KeyArray)->Array
jax._src.random._key_impl(keys:KeyArray)->PRNGImpl
jax._src.random._laplace(key,shape,dtype)->Array
jax._src.random._logistic(key,shape,dtype)
jax._src.random._lognormal(key,sigma,shape,dtype)->Array
jax._src.random._maxwell(key,shape,dtype)->Array
jax._src.random._multivariate_normal(key,mean,cov,shape,dtype,method)->Array
jax._src.random._normal(key,shape,dtype)->Array
jax._src.random._normal_real(key,shape,dtype)->Array
jax._src.random._pareto(key,b,shape,dtype)->Array
jax._src.random._poisson(key,lam,shape,dtype)->Array
jax._src.random._poisson_knuth(key,lam,shape,dtype,max_iters)->Array
jax._src.random._poisson_rejection(key,lam,shape,dtype,max_iters)->Array
jax._src.random._rademacher(key,shape,dtype)->Array
jax._src.random._randint(key,shape,minval,maxval,dtype)->Array
jax._src.random._random_bits(key:KeyArray,bit_width:int,shape:Shape)->Array
jax._src.random._rayleigh(key,scale,shape,dtype)->Array
jax._src.random._return_prng_keys(was_wrapped,key)
jax._src.random._shuffle(key,x,axis)->Array
jax._src.random._softmax(x,axis)->Array
jax._src.random._split(key:KeyArray,num:Union[int,tuple[int,...]]=2)->KeyArray
jax._src.random._t(key,df,shape,dtype)->Array
jax._src.random._triangular(key,left,mode,right,shape,dtype)->Array
jax._src.random._truncated_normal(key,lower,upper,shape,dtype)->Array
jax._src.random._uniform(key,shape,dtype,minval,maxval)->Array
jax._src.random._wald(key,mean,shape,dtype)->Array
jax._src.random._weibull_min(key,scale,concentration,shape,dtype)->Array
jax._src.random.ball(key:KeyArrayLike,d:int,p:float=2,shape:Shape=(),dtype:DTypeLikeFloat=float)
jax._src.random.bernoulli(key:KeyArrayLike,p:RealArray=np.float32(0.5),shape:Optional[Union[Shape,NamedShape]]=None)->Array
jax._src.random.beta(key:KeyArrayLike,a:RealArray,b:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.bits(key:KeyArrayLike,shape:Shape=(),dtype:Optional[DTypeLikeUInt]=None)->Array
jax._src.random.categorical(key:KeyArrayLike,logits:RealArray,axis:int=-1,shape:Optional[Shape]=None)->Array
jax._src.random.cauchy(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.chisquare(key:KeyArrayLike,df:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.choice(key:KeyArrayLike,a:Union[int,ArrayLike],shape:Shape=(),replace:bool=True,p:Optional[RealArray]=None,axis:int=0)->Array
jax._src.random.default_prng_impl()
jax._src.random.dirichlet(key:KeyArrayLike,alpha:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.double_sided_maxwell(key:KeyArrayLike,loc:RealArray,scale:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.exponential(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.f(key:KeyArrayLike,dfnum:RealArray,dfden:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.fold_in(key:KeyArrayLike,data:IntegerArray)->KeyArray
jax._src.random.gamma(key:KeyArrayLike,a:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.generalized_normal(key:KeyArrayLike,p:float,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.geometric(key:KeyArrayLike,p:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeInt=int)->Array
jax._src.random.gumbel(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.key(seed:int|ArrayLike,*,impl:Optional[PRNGSpecDesc]=None)->KeyArray
jax._src.random.key_data(keys:KeyArrayLike)->Array
jax._src.random.key_impl(keys:KeyArrayLike)->Hashable
jax._src.random.laplace(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.loggamma(key:KeyArrayLike,a:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.logistic(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.lognormal(key:KeyArrayLike,sigma:RealArray=np.float32(1),shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.maxwell(key:KeyArrayLike,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.multivariate_normal(key:KeyArrayLike,mean:RealArray,cov:RealArray,shape:Optional[Shape]=None,dtype:Optional[DTypeLikeFloat]=None,method:str='cholesky')->Array
jax._src.random.normal(key:KeyArrayLike,shape:Union[Shape,NamedShape]=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.orthogonal(key:KeyArrayLike,n:int,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.pareto(key:KeyArrayLike,b:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.permutation(key:KeyArrayLike,x:Union[int,ArrayLike],axis:int=0,independent:bool=False)->Array
jax._src.random.poisson(key:KeyArrayLike,lam:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeInt=int)->Array
jax._src.random.rademacher(key:KeyArrayLike,shape:Shape,dtype:DTypeLikeInt=int)->Array
jax._src.random.randint(key:KeyArrayLike,shape:Shape,minval:IntegerArray,maxval:IntegerArray,dtype:DTypeLikeInt=int)->Array
jax._src.random.rayleigh(key:KeyArrayLike,scale:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.rbg_key(seed:int|ArrayLike)->KeyArray
jax._src.random.resolve_prng_impl(impl_spec:Optional[PRNGSpecDesc])->PRNGImpl
jax._src.random.shuffle(key:KeyArrayLike,x:ArrayLike,axis:int=0)->Array
jax._src.random.split(key:KeyArrayLike,num:Union[int,tuple[int,...]]=2)->KeyArray
jax._src.random.t(key:KeyArrayLike,df:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.threefry2x32_key(seed:int|ArrayLike)->KeyArray
jax._src.random.triangular(key:KeyArrayLike,left:RealArray,mode:RealArray,right:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.truncated_normal(key:KeyArrayLike,lower:RealArray,upper:RealArray,shape:Optional[Union[Shape,NamedShape]]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.uniform(key:KeyArrayLike,shape:Union[Shape,NamedShape]=(),dtype:DTypeLikeFloat=float,minval:RealArray=0.0,maxval:RealArray=1.0)->Array
jax._src.random.unsafe_rbg_key(seed:int|ArrayLike)->KeyArray
jax._src.random.wald(key:KeyArrayLike,mean:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=float)->Array
jax._src.random.weibull_min(key:KeyArrayLike,scale:RealArray,concentration:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=float)->Array
jax._src.random.wrap_key_data(key_bits_array:Array,*,impl:Optional[PRNGSpecDesc]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/util.py----------------------------------------
A:jax._src.util.logger->logging.getLogger(__name__)
A:jax._src.util.T->TypeVar('T')
A:jax._src.util.T1->TypeVar('T1')
A:jax._src.util.T2->TypeVar('T2')
A:jax._src.util.T3->TypeVar('T3')
A:jax._src.util.args->list(args)
A:jax._src.util.n->len(args[0])
A:jax._src.util.lst->list(lst)
A:jax._src.util.sentinel->object()
A:jax._src.util.base_->iter(base)
A:jax._src.util.dct->dict(dct)
A:jax._src.util._unflatten_done->object()
A:jax._src.util.xs_iter->iter(xs)
A:jax._src.util.end_nodes->_remove_duplicates(end_nodes)
A:jax._src.util.stack->list(end_nodes)
A:jax._src.util.node->childless_nodes.pop()
A:jax._src.util.visited->set()
A:jax._src.util.seen->set()
A:jax._src.util.sides->list(map(predicate, xs))
A:jax._src.util.memoize->cache(max_size=None)
A:jax._src.util.cached_call->jax._src.lib.xla_client.weakref_lru_cache(config.config._trace_context, call, maxsize)
A:jax._src.util._weakref_lru_caches->weakref.WeakSet()
A:jax._src.util.axis->operator.index(axis)
A:jax._src.util.name->getattr(wrapped, '__name__', '<unnamed function>')
A:jax._src.util.fun.__annotations__->getattr(wrapped, '__annotations__', {})
A:jax._src.util.fun.__module__->getattr(wrapped, '__module__', '<unknown module>')
A:jax._src.util.fun.__qualname__->getattr(wrapped, '__qualname__', fun.__name__)
A:jax._src.util.pos->operator.index(axis)
A:jax._src.util.self.elts_set->set()
A:jax._src.util.self.hash->hash(x)
A:jax._src.util.original_func->_original_func(f)
jax._src.util.Hashable(self,val)
jax._src.util.Hashable.__eq__(self,other)
jax._src.util.Hashable.__hash__(self)
jax._src.util.Hashable.__init__(self,val)
jax._src.util.HashableFunction(self,f,closure)
jax._src.util.HashableFunction.__eq__(self,other)
jax._src.util.HashableFunction.__hash__(self)
jax._src.util.HashableFunction.__init__(self,f,closure)
jax._src.util.HashableFunction.__repr__(self)
jax._src.util.HashablePartial(self,f,*args,**kwargs)
jax._src.util.HashablePartial.__eq__(self,other)
jax._src.util.HashablePartial.__hash__(self)
jax._src.util.HashablePartial.__init__(self,f,*args,**kwargs)
jax._src.util.HashableWrapper(self,x)
jax._src.util.HashableWrapper.__eq__(self,other)
jax._src.util.HashableWrapper.__hash__(self)
jax._src.util.HashableWrapper.__init__(self,x)
jax._src.util.OrderedSet(self)
jax._src.util.OrderedSet.__contains__(self,elt:T)->bool
jax._src.util.OrderedSet.__init__(self)
jax._src.util.OrderedSet.__iter__(self)->Iterator[T]
jax._src.util.OrderedSet.__len__(self)->int
jax._src.util.OrderedSet.add(self,elt:T)->None
jax._src.util.OrderedSet.update(self,elts:Seq[T])->None
jax._src.util.Unhashable(self,val)
jax._src.util.Unhashable.__eq__(self,other)
jax._src.util.Unhashable.__init__(self,val)
jax._src.util.WrapKwArgs(self,val)
jax._src.util.WrapKwArgs.__eq__(self,other)
jax._src.util.WrapKwArgs.__hash__(self)
jax._src.util.WrapKwArgs.__init__(self,val)
jax._src.util._original_func(f)
jax._src.util._remove_duplicates(node_list)
jax._src.util.as_hashable_function(closure)
jax._src.util.assert_unreachable(x)
jax._src.util.cache(max_size=4096)
jax._src.util.canonicalize_axis(axis,num_dims)->int
jax._src.util.ceil_of_ratio(x,y)
jax._src.util.check_toposort(nodes)
jax._src.util.clear_all_weakref_lru_caches()
jax._src.util.concatenate(xs:Iterable[Sequence[T]])->list[T]
jax._src.util.curry(f)
jax._src.util.distributed_debug_log(*pairs)
jax._src.util.maybe_named_axis(axis,if_pos,if_named)
jax._src.util.merge_lists(bs:Sequence[bool],l0:Sequence[T],l1:Sequence[T])->list[T]
jax._src.util.moveaxis(x,src,dst)
jax._src.util.partition_list(bs:Sequence[bool],l:Sequence[T])->tuple[list[T], list[T]]
jax._src.util.set_module(module:str)->Callable[[T], T]
jax._src.util.split_dict(dct,names)
jax._src.util.split_list(args:Sequence[T],ns:Sequence[int])->list[list[T]]
jax._src.util.split_merge(predicate,xs)
jax._src.util.subs_list(subs:Sequence[Optional[int]],src:Sequence[T],base:Sequence[T])->list[T]
jax._src.util.subs_list2(subs1:Sequence[Optional[int]],subs2:Sequence[Optional[int]],src1:Sequence[T],src2:Sequence[T],base:Sequence[T])->list[T]
jax._src.util.subvals(lst,replace)
jax._src.util.toposort(end_nodes)
jax._src.util.tuple_delete(t,idx)
jax._src.util.tuple_insert(t,idx,val)
jax._src.util.unflatten(xs:Iterable[T],ns:Sequence[int])->list[list[T]]
jax._src.util.unzip2(xys:Iterable[tuple[T1,T2]])->tuple[tuple[T1, ...], tuple[T2, ...]]
jax._src.util.unzip3(xyzs:Iterable[tuple[T1,T2,T3]])->tuple[tuple[T1, ...], tuple[T2, ...], tuple[T3, ...]]
jax._src.util.weakref_lru_cache(call:Callable,maxsize=2048)
jax._src.util.wrap_name(name,transform_name)
jax._src.util.wraps(wrapped:Callable,namestr:Optional[str]=None,docstr:Optional[str]=None,**kwargs)->Callable[[T], T]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/xla_bridge.py----------------------------------------
A:jax._src.xla_bridge.logger->logging.getLogger(__name__)
A:jax._src.xla_bridge._XLA_BACKEND->json.load(f).DEFINE_string('jax_xla_backend', '', 'Deprecated, please use --jax_platforms instead.')
A:jax._src.xla_bridge.BACKEND_TARGET->json.load(f).DEFINE_string('jax_backend_target', os.getenv('JAX_BACKEND_TARGET', '').lower(), 'Either "local" or "rpc:address" to connect to a remote service target.')
A:jax._src.xla_bridge._PLATFORM_NAME->json.load(f).DEFINE_string('jax_platform_name', os.getenv('JAX_PLATFORM_NAME', '').lower(), 'Deprecated, please use --jax_platforms instead.')
A:jax._src.xla_bridge.CUDA_VISIBLE_DEVICES->json.load(f).DEFINE_string('jax_cuda_visible_devices', 'all', 'Restricts the set of CUDA devices that JAX will use. Either "all", or a comma-separate list of integer device IDs.')
A:jax._src.xla_bridge._ROCM_VISIBLE_DEVICES->json.load(f).DEFINE_string('jax_rocm_visible_devices', 'all', 'Restricts the set of ROCM devices that JAX will use. Either "all", or a comma-separate list of integer device IDs.')
A:jax._src.xla_bridge._USE_MOCK_GPU_CLIENT->json.load(f).DEFINE_bool(name='use_mock_gpu_client', default=False, help='If True, use a mock GPU client instead of a real one.')
A:jax._src.xla_bridge._MOCK_NUM_GPUS->json.load(f).DEFINE_integer(name='mock_num_gpus', default=1, help='Mock GPU client number of gpus.')
A:jax._src.xla_bridge.path_from_env->os.getenv('TPU_LIBRARY_PATH')
A:jax._src.xla_bridge.libtpu_module->maybe_import_libtpu()
A:jax._src.xla_bridge.t->threading.Timer(timer_secs, _log_warning)
A:jax._src.xla_bridge.client->jax._src.lib.xla_client.make_tpu_client()
A:jax._src.xla_bridge._backend_lock->threading.Lock()
A:jax._src.xla_bridge._plugin_lock->threading.Lock()
A:jax._src.xla_bridge._backend_factories[name]->BackendRegistration(factory, priority, fail_quietly, experimental)
A:jax._src.xla_bridge.build_version->get_build_version()
A:jax._src.xla_bridge.version->get_version()
A:jax._src.xla_bridge.(name, library_path)->plugin.split(os.path.pathsep)
A:jax._src.xla_bridge.config->json.load(f)
A:jax._src.xla_bridge.plugin_modules->set()
A:jax._src.xla_bridge.plugin_module->importlib.import_module(plugin_module_name)
A:jax._src.xla_bridge.c_api->jax._src.lib.xla_client.load_pjrt_plugin_dynamically(plugin_name, library_path)
A:jax._src.xla_bridge.pjrt_plugins->_get_pjrt_plugin_names_and_library_paths(os.getenv('PJRT_NAMES_AND_LIBRARY_PATHS', ''))
A:jax._src.xla_bridge.(library_path, options)->_get_pjrt_plugin_config(path)
A:jax._src.xla_bridge.platforms->_alias_to_platforms.get(platform, None)
A:jax._src.xla_bridge.b->backends()
A:jax._src.xla_bridge.priorities->range(len(platforms), 0, -1)
A:jax._src.xla_bridge.platform_registrations->list(((platform, registration.priority, registration.fail_quietly) for (platform, registration) in _backend_factories.items()))
A:jax._src.xla_bridge.backend->backends().get(platform, None)
A:jax._src.xla_bridge._backend_errors[platform]->str(err)
A:jax._src.xla_bridge.vendor_id->pathlib.Path(vendor_path).read_text().strip()
A:jax._src.xla_bridge.device_path->os.path.join(os.path.dirname(vendor_path), 'device')
A:jax._src.xla_bridge.device_id->pathlib.Path(device_path).read_text().strip()
A:jax._src.xla_bridge.registration->_backend_factories.get(platform, None)
A:jax._src.xla_bridge.bs->backends()
A:jax._src.xla_bridge.platform->canonicalize_platform(platform)
A:jax._src.xla_bridge.process_index->get_backend(backend).process_index()
A:jax._src.xla_bridge.library_path->_get_tpu_library_path()
jax._src.xla_bridge.BackendRegistration
jax._src.xla_bridge._check_cuda_versions()
jax._src.xla_bridge._clear_backends()->None
jax._src.xla_bridge._get_backend_uncached(platform:Union[None,str,xla_client.Client]=None)->xla_client.Client
jax._src.xla_bridge._get_pjrt_plugin__config_module(json_path:str)->tuple[str, Optional[Mapping[str, Union[str, int, list[int], float, bool]]]]
jax._src.xla_bridge._get_pjrt_plugin_config(json_path:str)->tuple[str, Optional[Mapping[str, Union[str, int, list[int], float, bool]]]]
jax._src.xla_bridge._get_pjrt_plugin_names_and_library_paths(plugins_from_env:str)->dict[str, str]
jax._src.xla_bridge._get_tpu_library_path()->Optional[str]
jax._src.xla_bridge._init_backend(platform:str)->xla_client.Client
jax._src.xla_bridge._num_available_tpu_chips()->int
jax._src.xla_bridge._suggest_missing_backends()
jax._src.xla_bridge.backends()->dict[str, xla_client.Client]
jax._src.xla_bridge.canonicalize_platform(platform:str)->str
jax._src.xla_bridge.default_backend()->str
jax._src.xla_bridge.device_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.devices(backend:Optional[Union[str,xla_client.Client]]=None)->list[xla_client.Device]
jax._src.xla_bridge.discover_pjrt_plugins()->None
jax._src.xla_bridge.expand_platform_alias(platform:str)->list[str]
jax._src.xla_bridge.get_backend(platform:Union[None,str,xla_client.Client]=None)->xla_client.Client
jax._src.xla_bridge.get_device_backend(device:Optional[xla_client.Device]=None)->xla_client.Client
jax._src.xla_bridge.host_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.host_id(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.host_ids(backend:Optional[Union[str,xla_client.Client]]=None)->list[int]
jax._src.xla_bridge.is_gpu(platform)
jax._src.xla_bridge.is_known_platform(platform:str)->bool
jax._src.xla_bridge.local_device_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.local_devices(process_index:Optional[int]=None,backend:Optional[Union[str,xla_client.Client]]=None,host_id:Optional[int]=None)->list[xla_client.Device]
jax._src.xla_bridge.make_gpu_client(*,platform_name:str,visible_devices_flag:config.FlagHolder[str])->xla_client.Client
jax._src.xla_bridge.make_pjrt_tpu_topology(topology_name='',**kwargs)
jax._src.xla_bridge.process_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.process_index(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax._src.xla_bridge.register_backend_factory(name:str,factory:BackendFactory,*,priority:int=0,fail_quietly:bool=True,experimental:bool=False)->None
jax._src.xla_bridge.register_pjrt_plugin_factories_from_env()->None
jax._src.xla_bridge.register_plugin(plugin_name:str,*,priority:int=400,library_path:Optional[str]=None,options:Optional[Mapping[str,Union[str,int,list[int],float,bool]]]=None)->None
jax._src.xla_bridge.tpu_client_timer_callback(timer_secs:float)->Optional[xla_client.Client]
jax._src.xla_bridge.using_pjrt_c_api(backend=None)
jax.default_backend()->str
jax.device_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax.devices(backend:Optional[Union[str,xla_client.Client]]=None)->list[xla_client.Device]
jax.host_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax.host_id(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax.host_ids(backend:Optional[Union[str,xla_client.Client]]=None)->list[int]
jax.local_device_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax.local_devices(process_index:Optional[int]=None,backend:Optional[Union[str,xla_client.Client]]=None,host_id:Optional[int]=None)->list[xla_client.Device]
jax.process_count(backend:Optional[Union[str,xla_client.Client]]=None)->int
jax.process_index(backend:Optional[Union[str,xla_client.Client]]=None)->int


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/jaxpr_util.py----------------------------------------
A:jax._src.jaxpr_util.d->collections.defaultdict(lambda : 0)
A:jax._src.jaxpr_util.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax._src.jaxpr_util.subs->map(var_defs_and_refs, core.subjaxprs(jaxpr))
A:jax._src.jaxpr_util.count_width->max((len(str(v)) for v in histogram.values()))
A:jax._src.jaxpr_util.s->collections.defaultdict(itertools.count(1).__next__)
A:jax._src.jaxpr_util.func->collections.defaultdict(itertools.count(1).__next__)
A:jax._src.jaxpr_util.loc->collections.defaultdict(itertools.count(1).__next__)
A:jax._src.jaxpr_util.raw_frames->zip(*tb.raw_frames())
A:jax._src.jaxpr_util.json_profile->json.dumps({'string_table': list(s.keys()), 'location': locations, 'function': functions, 'sample_type': sample_type, 'sample': samples})
jax._src.jaxpr_util._pprof_profile(profile:dict[tuple[Optional[xla_client.Traceback],core.Primitive],int])->bytes
jax._src.jaxpr_util.all_eqns(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.collect_eqns(jaxpr:core.Jaxpr,key:Callable)
jax._src.jaxpr_util.histogram(jaxpr:core.Jaxpr,key:Callable,key_fmt:Callable=lambdax:x)
jax._src.jaxpr_util.pprof_equation_profile(jaxpr:core.Jaxpr)->bytes
jax._src.jaxpr_util.primitives(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.primitives_by_shape(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.primitives_by_source(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.print_histogram(histogram:dict[Any,int])
jax._src.jaxpr_util.source_locations(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.var_defs_and_refs(jaxpr:core.Jaxpr)
jax._src.jaxpr_util.vars_by_fanout(jaxpr:core.Jaxpr)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/custom_derivatives.py----------------------------------------
A:jax._src.custom_derivatives.ba->inspect.signature(fun).bind(*args, **kwargs)
A:jax._src.custom_derivatives.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(rule, ans_avals)
A:jax._src.custom_derivatives.py_args->tree_unflatten(in_tree, args)
A:jax._src.custom_derivatives.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax._src.custom_derivatives.ReturnValue->TypeVar('ReturnValue')
A:jax._src.custom_derivatives.primal_out->self(*primals)
A:jax._src.custom_derivatives.zeros->_zeros_like_pytree(primal_out)
A:jax._src.custom_derivatives.tangent_out->tree_map(_sum_tangents, primal_out, *all_tangents_out)
A:jax._src.custom_derivatives.primal_name->getattr(self.fun, '__name__', str(self.fun))
A:jax._src.custom_derivatives.jvp_name->getattr(self.jvp, '__name__', str(self.jvp))
A:jax._src.custom_derivatives.args->map(core.full_lower, args)
A:jax._src.custom_derivatives.nondiff_argnums->set(self.nondiff_argnums)
A:jax._src.custom_derivatives.(f_, dyn_args)->argnums_partial(lu.wrap_init(self.fun), dyn_argnums, args, require_static_args_hashable=False)
A:jax._src.custom_derivatives.jvp->lift_jvp(num_consts, jvp_jaxpr_thunk)
A:jax._src.custom_derivatives.(args_flat, in_tree)->tree_flatten(args)
A:jax._src.custom_derivatives.(flat_fun, out_type1)->_flatten_fun_nokwargs(f_, in_tree)
A:jax._src.custom_derivatives.(flat_jvp, out_type2)->_flatten_jvp(jvp, primal_name, jvp_name, in_tree, out_type1)
A:jax._src.custom_derivatives.out_flat->jax._src.core.eval_jaxpr(jaxpr, consts, *all_args)
A:jax._src.custom_derivatives.(_, (out_tree, _))->jax._src.linear_util.merge_linear_aux(out_type, out_trees)
A:jax._src.custom_derivatives.extra_args->tuple((arg.val for arg in extra_args))
A:jax._src.custom_derivatives.(primals_in, tangents_in)->split_list(args, [len(args) // 2])
A:jax._src.custom_derivatives.py_primals->tree_unflatten(in_tree, primals_in)
A:jax._src.custom_derivatives.py_tangents->tree_unflatten(in_tree, tangents_in)
A:jax._src.custom_derivatives.(primals_out, out_tree)->tree_flatten(py_primals_out)
A:jax._src.custom_derivatives.(tangents_out, out_tree2)->tree_flatten(py_tangents_out)
A:jax._src.custom_derivatives.out_type_->maybe_out_type()
A:jax._src.custom_derivatives.ty_tree->tree_unflatten(out_tree, [a.str_short() for a in primal_avals])
A:jax._src.custom_derivatives.ty_tree_->tree_unflatten(out_tree_, [a.str_short() for a in primal_avals_])
A:jax._src.custom_derivatives.top_trace->jax._src.core.find_top_trace(args)
A:jax._src.custom_derivatives.(fun, env_trace_todo1)->process_env_traces(fun, self, top_trace and top_trace.level, False)
A:jax._src.custom_derivatives.(jvp, env_trace_todo2)->process_env_traces(jvp, self, top_trace and top_trace.level, True)
A:jax._src.custom_derivatives.tracers->map(top_trace.full_raise, args)
A:jax._src.custom_derivatives.outs->map(trace.full_raise, outs)
A:jax._src.custom_derivatives.(_, env_trace_todo)->jax._src.linear_util.merge_linear_aux(env_trace_todo1, env_trace_todo2)
A:jax._src.custom_derivatives.new_params->dict(params)
A:jax._src.custom_derivatives.call_jaxpr->dict(params).pop('call_jaxpr')
A:jax._src.custom_derivatives.num_consts->len(hoisted_consts)
A:jax._src.custom_derivatives.jvp_jaxpr_thunk->dict(params).pop('jvp_jaxpr_thunk')
A:jax._src.custom_derivatives.fun->custom_jvp(fun)
A:jax._src.custom_derivatives.(n, ragged)->divmod(len(xs), 2)
A:jax._src.custom_derivatives.(jvp_jaxpr, jvp_consts, out_zeros)->jvp_jaxpr_thunk(*zeros)
A:jax._src.custom_derivatives.out->unreachable_p.bind(*args_flat, out_avals=out_avals_flat, exc_type=exc_type, message=message)
A:jax._src.custom_derivatives.(out_primals, nz_out_tangents)->split_list(out, [len(out_zeros)])
A:jax._src.custom_derivatives.nz_out_tangents_->iter(nz_out_tangents)
A:jax._src.custom_derivatives.ans->max(tracers, key=lambda x: x._trace.level)
A:jax._src.custom_derivatives.trace->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel()
A:jax._src.custom_derivatives.(outs, cur_todo)->primitive.post_process(trace, outs, jvp_was_run)
A:jax._src.custom_derivatives.custom_jvp_call_p->CustomJVPCallPrimitive('custom_jvp_call')
A:jax._src.custom_derivatives.disallowed_effects->jax._src.effects.custom_derivatives_allowed_effects.filter_not_in(fun_jaxpr.effects)
A:jax._src.custom_derivatives.args_->map(mlir.wrap_singleton_ir_values, args)
A:jax._src.custom_derivatives.consts->merge(closure_consts, hoisted_consts)
A:jax._src.custom_derivatives.(out, tokens)->jax._src.interpreters.mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr, ctx.tokens_in, consts, *args_, dim_var_values=ctx.dim_var_values)
A:jax._src.custom_derivatives.fwd_name->getattr(self.fwd, '__name__', str(self.fwd))
A:jax._src.custom_derivatives.(fwd, _)->argnums_partial(lu.wrap_init(self.fwd), dyn_argnums, args, require_static_args_hashable=False)
A:jax._src.custom_derivatives.bwd->list(todos).pop()(bwd)
A:jax._src.custom_derivatives.(flat_fun, out_type)->_flatten_fun_nokwargs(f_, in_tree)
A:jax._src.custom_derivatives.(flat_fwd, out_trees)->_flatten_fwd(fwd, self.symbolic_zeros, primal_name, fwd_name, in_tree, out_type)
A:jax._src.custom_derivatives.(res, res_tree)->tree_flatten(res)
A:jax._src.custom_derivatives.(out_tree, res_tree)->out_trees()
A:jax._src.custom_derivatives.(res, cts_out)->split_list(args, [res_tree.num_leaves])
A:jax._src.custom_derivatives.py_res->tree_unflatten(res_tree, res)
A:jax._src.custom_derivatives.py_cts_out->tree_unflatten(out_tree, cts_out)
A:jax._src.custom_derivatives.zero->object()
A:jax._src.custom_derivatives.dummy->tree_unflatten(in_tree, [object()] * in_tree.num_leaves)
A:jax._src.custom_derivatives.(_, in_tree2)->tree_flatten(py_cts_in)
A:jax._src.custom_derivatives.(fwd, env_trace_todo2)->process_env_traces_fwd(fwd, top_trace and top_trace.level, out_trees)
A:jax._src.custom_derivatives.(fst, env_trace_todo)->jax._src.linear_util.merge_linear_aux(env_trace_todo1, env_trace_todo2)
A:jax._src.custom_derivatives.custom_vjp_call_p->CustomVJPCallPrimitive('custom_vjp_call')
A:jax._src.custom_derivatives.(outs, cur_todo, bwd_xform)->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel().post_process_custom_vjp_call_fwd(outs, out_trees)
A:jax._src.custom_derivatives.todos_list->list(todos)
A:jax._src.custom_derivatives.custom_vjp_call_jaxpr_p->jax._src.core.AxisPrimitive('custom_vjp_call_jaxpr')
A:jax._src.custom_derivatives.(_, args)->split_list(primals, [num_consts])
A:jax._src.custom_derivatives.(consts_dot, args_dot)->split_list(tangents, [num_consts])
A:jax._src.custom_derivatives.(fwd_jaxpr, fwd_consts)->fwd_jaxpr_thunk(*zeros)
A:jax._src.custom_derivatives.(_, res_tree)->out_trees()
A:jax._src.custom_derivatives.res_and_primals_out->jax._src.core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)
A:jax._src.custom_derivatives.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax._src.custom_derivatives.args_dot->map(ad.replace_float0s, args, args_dot)
A:jax._src.custom_derivatives.tangents_out->map(ad.recast_to_float0, primals_out, tangents_out)
A:jax._src.custom_derivatives.(_, args_batched)->split_list(in_batched, [num_consts])
A:jax._src.custom_derivatives.(batched_fun_jaxpr, out_batched)->jax._src.interpreters.batching.batch_jaxpr(fun_jaxpr, axis_size, in_batched, False, axis_name, spmd_axis_name, main_type)
A:jax._src.custom_derivatives.fwd_jaxpr->jax._src.core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))
A:jax._src.custom_derivatives.(batched_fwd_jaxpr, out_batched)->jax._src.interpreters.batching.batch_jaxpr(fwd_jaxpr, axis_size, args_batched, False, axis_name, spmd_axis_name, main_type)
A:jax._src.custom_derivatives.batched_bwd->jax._src.interpreters.batching.batch_custom_vjp_bwd(bwd, axis_name, axis_size, fwd_out_dims, fwd_args_batched, main_type, spmd_axis_name)
A:jax._src.custom_derivatives.batched_outs->jax._src.core.AxisPrimitive('custom_vjp_call_jaxpr').bind(*args, fun_jaxpr=batched_fun_jaxpr, fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd, num_consts=num_consts, out_trees=out_trees, symbolic_zeros=symbolic_zeros)
A:jax._src.custom_derivatives.batching.axis_primitive_batchers[custom_vjp_call_jaxpr_p]->partial(_custom_vjp_call_jaxpr_vmap, None)
A:jax._src.custom_derivatives.(ans, _)->fun(*args, **kwargs)
A:jax._src.custom_derivatives.(ans, rule)->fun(*args, **kwargs)
A:jax._src.custom_derivatives.(ans_flat, out_tree)->tree_flatten((ans,))
A:jax._src.custom_derivatives.(rule, in_tree)->flatten_fun_nokwargs(lu.wrap_init(rule), out_tree)
A:jax._src.custom_derivatives.(cts_flat, out_tree_)->tree_flatten((cts,))
A:jax._src.custom_derivatives.cts_out->jax._src.core.Primitive('linear_call').bind(*t_consts, *f_consts, *operands_res, *cts, callee=transpose, transpose=callee, num_callee_consts=len(t_consts), num_transpose_consts=len(f_consts), num_res=len(operands_res))
A:jax._src.custom_derivatives.(flat_args, in_tree)->tree_flatten(example_args)
A:jax._src.custom_derivatives.in_avals->tuple(map(abstractify, flat_args))
A:jax._src.custom_derivatives.x->jax._src.core.full_lower(x)
A:jax._src.custom_derivatives.vspace->jax._src.core.full_lower(x).aval.at_least_vspace()
A:jax._src.custom_derivatives.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.custom_derivatives.(jaxpr, out_pvals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, in_avals)
A:jax._src.custom_derivatives.out_tree->out_tree()
A:jax._src.custom_derivatives.((closure_consts, hoisted_consts), merge)->partition_list(_maybe_perturbed, consts)
A:jax._src.custom_derivatives.(args, hoisted_consts)->split_list(args_hconsts, [num_args])
A:jax._src.custom_derivatives.(all_args, in_tree2)->tree_flatten(tuple(args))
A:jax._src.custom_derivatives.(operands_res, res_tree)->tree_flatten(residual_args)
A:jax._src.custom_derivatives.(operands_lin, lin_tree)->tree_flatten(linear_args)
A:jax._src.custom_derivatives.f_in_tree->treedef_tuple((res_tree, lin_tree))
A:jax._src.custom_derivatives.(f, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), f_in_tree)
A:jax._src.custom_derivatives.res_avals->map(abstractify, operands_res)
A:jax._src.custom_derivatives.lin_avals->map(abstractify, operands_lin)
A:jax._src.custom_derivatives.(f_jaxpr, f_consts)->_initial_style_jaxpr(f, (*res_avals, *lin_avals))
A:jax._src.custom_derivatives.f_jaxpr->_close_jaxpr(f_jaxpr)
A:jax._src.custom_derivatives.out_avals->tree_map(core.get_aval, args)
A:jax._src.custom_derivatives.t_in_tree->treedef_tuple((res_tree, out_tree()))
A:jax._src.custom_derivatives.(t, t_out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun_transpose), t_in_tree)
A:jax._src.custom_derivatives.(t_jaxpr, t_consts)->_initial_style_jaxpr(t, (*res_avals, *out_avals))
A:jax._src.custom_derivatives.t_jaxpr->_close_jaxpr(t_jaxpr)
A:jax._src.custom_derivatives.(consts, _, operands_res, operands_lin)->split_list(args, [num_callee_consts, num_transpose_consts, num_res])
A:jax._src.custom_derivatives.(f_consts, t_consts, operands_res, operands_lin)->split_list(args, [num_callee_consts, num_transpose_consts, num_res])
A:jax._src.custom_derivatives.(_, _, cts_avals)->split_list(transpose.in_avals, [num_transpose_consts, num_res])
A:jax._src.custom_derivatives.linear_call_p->jax._src.core.Primitive('linear_call')
A:jax._src.custom_derivatives.(out_avals_flat, out_tree)->tree_flatten(out_avals)
A:jax._src.custom_derivatives.disallow_jvp->partial(unreachable, exc_type=TypeError, message="can't apply forward-mode autodiff (jvp) to a custom_vjp function.")
A:jax._src.custom_derivatives.(outs, residuals)->fwd(*primals)
A:jax._src.custom_derivatives.tan_out_types->tree_map(lambda o: core.get_aval(o).at_least_vspace(), outs)
A:jax._src.custom_derivatives.tan_fn->custom_transpose(partial(disallow_jvp, out_avals=tan_out_types))
A:jax._src.custom_derivatives.custom_jvp_call_jaxpr_p->jax._src.core.Primitive('custom_jvp_call_jaxpr')
jax._src.custom_derivatives.CustomJVPCallPrimitive(core.Primitive)
jax._src.custom_derivatives.CustomJVPCallPrimitive.bind(self,fun,jvp,*args,symbolic_zeros)
jax._src.custom_derivatives.CustomJVPCallPrimitive.get_bind_params(self,params)
jax._src.custom_derivatives.CustomJVPCallPrimitive.impl(self,fun,_,*args)
jax._src.custom_derivatives.CustomJVPCallPrimitive.post_process(self,trace,out_tracers,jvp_was_run:bool)
jax._src.custom_derivatives.CustomVJPCallPrimitive(core.CallPrimitive)
jax._src.custom_derivatives.CustomVJPCallPrimitive.bind(self,fun,fwd,bwd,*args,out_trees,symbolic_zeros)
jax._src.custom_derivatives.CustomVJPCallPrimitive.impl(self,fun,fwd,bwd,*args,out_trees)
jax._src.custom_derivatives.CustomVJPCallPrimitive.post_process(self,trace,out_tracers,params)
jax._src.custom_derivatives.CustomVJPPrimal
jax._src.custom_derivatives.Residuals(self,jaxpr,in_tree,out_tree,consts)
jax._src.custom_derivatives.Residuals.__init__(self,jaxpr,in_tree,out_tree,consts)
jax._src.custom_derivatives.Residuals.__iter__(self)
jax._src.custom_derivatives.Residuals.tree_flatten(self)
jax._src.custom_derivatives.Residuals.tree_unflatten(cls,aux,consts)
jax._src.custom_derivatives._add_args(f,extra_args)
jax._src.custom_derivatives._add_args_(extra_args,*args,**kwargs)
jax._src.custom_derivatives._apply_bwd_transform(todos,bwd)
jax._src.custom_derivatives._check_for_tracers(x)
jax._src.custom_derivatives._close_jaxpr(jaxpr)
jax._src.custom_derivatives._closure_convert_for_avals(fun,in_tree,in_avals)
jax._src.custom_derivatives._custom_jvp_call_mlir_translation(ctx,*args,call_jaxpr,jvp_jaxpr_thunk,num_consts,symbolic_zeros)
jax._src.custom_derivatives._custom_jvp_call_transpose(params,jaxpr,args,ct,_,reduce_axes)
jax._src.custom_derivatives._custom_jvp_call_typecheck(_,*in_avals,call_jaxpr,jvp_jaxpr_thunk,num_consts,symbolic_zeros)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_abstract_eval(*_,fun_jaxpr,**__)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_impl(*args,fun_jaxpr,**_)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_jvp(primals,tangents,*,fun_jaxpr:core.ClosedJaxpr,fwd_jaxpr_thunk:Callable[...,tuple[core.Jaxpr,Sequence[Any]]],num_consts:int,bwd:Callable,out_trees:Callable,symbolic_zeros:bool)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_vmap(spmd_axis_name,axis_size,axis_name,main_type,args,in_dims,*,fun_jaxpr:core.ClosedJaxpr,fwd_jaxpr_thunk:Callable[...,tuple[core.Jaxpr,Sequence[Any]]],num_consts:int,bwd:Callable,out_trees:Callable,symbolic_zeros:bool)
jax._src.custom_derivatives._flatten_bwd(in_tree,in_avals,out_trees,*args)
jax._src.custom_derivatives._flatten_fun_nokwargs(in_tree,*args_flat)
jax._src.custom_derivatives._flatten_fwd(symbolic_zeros,primal_name,fwd_name,in_tree,maybe_out_type,*args)
jax._src.custom_derivatives._flatten_jvp(primal_name,jvp_name,in_tree,maybe_out_type,*args)
jax._src.custom_derivatives._initial_style_jaxpr(fun,in_avals)
jax._src.custom_derivatives._linear_call_abstract_eval(*args,**kwargs)
jax._src.custom_derivatives._linear_call_impl(*args,callee,transpose,num_callee_consts,num_transpose_consts,num_res)
jax._src.custom_derivatives._linear_call_transpose_rule(cts,*args,callee,transpose,num_callee_consts,num_transpose_consts,num_res)
jax._src.custom_derivatives._maybe_perturbed(x:Any)->bool
jax._src.custom_derivatives._resolve_kwargs(fun,args,kwargs)
jax._src.custom_derivatives._stop_gradient(x)
jax._src.custom_derivatives._sum_tangents(_,x,*xs)
jax._src.custom_derivatives._zeros_like_pytree(x)
jax._src.custom_derivatives.abstractify(x)
jax._src.custom_derivatives.closure_convert(fun:Callable,*example_args)->tuple[Callable, list[Any]]
jax._src.custom_derivatives.custom_gradient(fun)
jax._src.custom_derivatives.custom_jvp(self,fun:Callable[...,ReturnValue],nondiff_argnums:tuple[int,...]=())
jax._src.custom_derivatives.custom_jvp.__init__(self,fun:Callable[...,ReturnValue],nondiff_argnums:tuple[int,...]=())
jax._src.custom_derivatives.custom_jvp.defjvp(self,jvp:Callable[...,tuple[ReturnValue,ReturnValue]],symbolic_zeros:bool=False)->Callable[..., tuple[ReturnValue, ReturnValue]]
jax._src.custom_derivatives.custom_jvp.defjvps(self,*jvps:Optional[Callable[...,ReturnValue]])
jax._src.custom_derivatives.custom_vjp(self,fun:Callable[...,ReturnValue],nondiff_argnums:tuple[int,...]=())
jax._src.custom_derivatives.custom_vjp.__init__(self,fun:Callable[...,ReturnValue],nondiff_argnums:tuple[int,...]=())
jax._src.custom_derivatives.custom_vjp.defvjp(self,fwd:Callable[...,tuple[ReturnValue,Any]],bwd:Callable[...,tuple[Any,...]],symbolic_zeros:bool=False)->None
jax._src.custom_derivatives.custom_vjp_by_custom_transpose(fun,fwd,bwd)
jax._src.custom_derivatives.custom_vjp_primal_tree_values(tree)
jax._src.custom_derivatives.lift_jvp(num_consts:int,jvp_jaxpr_thunk:Callable)->lu.WrappedFun
jax._src.custom_derivatives.linear_call(fun:Callable,fun_transpose:Callable,residual_args,linear_args)
jax._src.custom_derivatives.partition_list(choice,lst)
jax._src.custom_derivatives.process_env_traces(primitive,level:int,jvp_was_run:bool,*args)
jax._src.custom_derivatives.process_env_traces_fwd(level:int,out_trees,*args)
jax._src.custom_derivatives.unreachable(*args,out_avals=None,exc_type=TypeError,message='unreachable')
jax._src.custom_derivatives.unreachable_impl(*_,out_avals,exc_type,message)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/callback.py----------------------------------------
A:jax._src.callback.pure_callback_p->jax._src.core.Primitive('pure_callback')
A:jax._src.callback.axis_size->next((a.shape[0] for (a, d) in zip(args, dims) if d is not batching.not_mapped))
A:jax._src.callback.result_avals->jax._src.tree_util.tree_map(lambda x: core.ShapedArray(x.shape, x.dtype), result_shape_dtypes)
A:jax._src.callback.outvals->lax_map(_batch_fun, batched_args)
A:jax._src.callback.(unbatched_args, batched_args)->jax._src.util.partition_list(is_batched, new_args)
A:jax._src.callback.merged_args->jax._src.util.merge_lists(is_batched, unbatched_args, batched_args)
A:jax._src.callback.op_sharding->_callback_op_sharding(ctx.module_context.axis_context, sharding)
A:jax._src.callback.device->next(iter(sharding.device_set))
A:jax._src.callback.device_index->axis_context.device_assignment.index(device)
A:jax._src.callback.(result, _, _)->jax._src.interpreters.mlir.emit_python_callback(ctx, _callback, None, list(args), ctx.avals_in, ctx.avals_out, False, sharding=op_sharding)
A:jax._src.callback.dt->numpy.dtype(shape_dtype.dtype)
A:jax._src.callback.(args, kwargs)->jax._src.tree_util.tree_unflatten(in_tree, flat_args)
A:jax._src.callback.(flat_args, in_tree)->jax._src.tree_util.tree_flatten((args, kwargs))
A:jax._src.callback.(flat_result_avals, out_tree)->jax._src.tree_util.tree_flatten(result_avals)
A:jax._src.callback.out_flat->jax._src.core.Primitive('io_callback').bind(*flat_args, callback=_flat_callback, result_avals=tuple(flat_result_avals), sharding=sharding, ordered=ordered)
A:jax._src.callback.io_callback_p->jax._src.core.Primitive('io_callback')
A:jax._src.callback._IOEffect->IOEffect()
A:jax._src.callback._OrderedIOEffect->OrderedIOEffect()
A:jax._src.callback.(result, token, _)->jax._src.interpreters.mlir.emit_python_callback(ctx, _callback, None, list(args), ctx.avals_in, ctx.avals_out, True, sharding=op_sharding)
A:jax._src.callback.(flat_shape_dtypes, out_tree)->jax._src.tree_util.tree_flatten(result_shape_dtypes)
A:jax._src.callback.flat_result_avals->map(lambda x: core.ShapedArray(x.shape, x.dtype), flat_shape_dtypes)
A:jax._src.callback.flat_args->map(core.raise_as_much_as_possible, flat_args)
jax._src.callback.IOEffect(effects.Effect)
jax._src.callback.OrderedIOEffect(effects.Effect)
jax._src.callback._callback_op_sharding(axis_context,sharding:SingleDeviceSharding|None)
jax._src.callback._check_shape_dtype(shape_dtype)
jax._src.callback.io_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,sharding:SingleDeviceSharding|None=None,ordered:bool=False,**kwargs:Any)
jax._src.callback.io_callback_abstract_eval(*avals,callback:Callable[...,Any],result_avals,sharding:SingleDeviceSharding|None,ordered:bool)
jax._src.callback.io_callback_batching_rule(args,dims,callback,result_avals,sharding,ordered)
jax._src.callback.io_callback_impl(*args,result_avals,callback:Callable[...,Any],sharding:SingleDeviceSharding|None,ordered:bool)
jax._src.callback.io_callback_jvp_rule(*args,**kwargs)
jax._src.callback.io_callback_lowering(ctx,*args,callback,sharding,ordered,**params)
jax._src.callback.io_callback_transpose_rule(*args,**kwargs)
jax._src.callback.pure_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,sharding:SingleDeviceSharding|None=None,vectorized:bool=False,**kwargs:Any)
jax._src.callback.pure_callback_abstract_eval(*avals,callback:Callable[...,Any],result_avals,sharding:SingleDeviceSharding|None,vectorized:bool)
jax._src.callback.pure_callback_api(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,sharding:SingleDeviceSharding|None=None,vectorized:bool=False,**kwargs:Any)
jax._src.callback.pure_callback_batching_rule(args,dims,*,callback,sharding:SingleDeviceSharding|None,vectorized:bool,result_avals:Sequence[core.ShapedArray])
jax._src.callback.pure_callback_impl(*args,result_avals,callback:Callable[...,Any],sharding:SingleDeviceSharding|None,vectorized:bool)
jax._src.callback.pure_callback_jvp_rule(*args,**kwargs)
jax._src.callback.pure_callback_lowering(ctx,*args,callback,sharding:SingleDeviceSharding|None,**params)
jax._src.callback.pure_callback_transpose_rule(*args,**kwargs)
jax.pure_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,sharding:SingleDeviceSharding|None=None,vectorized:bool=False,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/maps.py----------------------------------------
A:jax._src.maps.self.contents->dict(*args, **kwargs)
A:jax._src.maps.thread_resources.env->old_env.with_extra_loop(mesh_lib.Loop(name, length))
A:jax._src.maps.result->jax._src.interpreters.pxla.vtile_manual(f, tuple(manual_mesh_axes), mesh, mesh_in_axes, mesh_out_axes).call_wrapped(*(_slice_tile(arg, spec.get(loop_name, None), i, loop_length) for (arg, spec) in zip(args, loop_in_axes)))
A:jax._src.maps.num_mapped_dims->sum((name is not None for name in entry))
A:jax._src.maps.user_repr->str(entry)
A:jax._src.maps.constr->partial(AxisNamePosWithRank, expected_rank=len(entry))
A:jax._src.maps.(entries, treedef)->tree_flatten(axes, is_leaf=_is_axes_leaf)
A:jax._src.maps.entries->map(partial(_parse_entry, arg_name), entries)
A:jax._src.maps.in_axes->tuple(in_axes)
A:jax._src.maps.(in_axes, in_axes_entries, _)->_prepare_axes(in_axes, 'in_axes')
A:jax._src.maps.(out_axes, out_axes_entries, out_axes_treedef)->_prepare_axes(out_axes, 'out_axes')
A:jax._src.maps.out_axes_entries->tuple(out_axes_entries)
A:jax._src.maps.axis_sizes_names->set(axis_sizes.keys())
A:jax._src.maps.in_axes_names->set(it.chain(*(spec.keys() for spec in in_axes_entries)))
A:jax._src.maps.out_axes_names->set(it.chain(*(spec.keys() for spec in out_axes_entries)))
A:jax._src.maps.name->fresh_resource_name()
A:jax._src.maps.axes_with_resources->set(axis_resources.keys())
A:jax._src.maps.resources->axis_resources.get(axis, ())
A:jax._src.maps.normalized_axis_resources[axis]->tuple(unsafe_map(normalize_resource, resources))
A:jax._src.maps.frozen_axis_resources->FrozenDict(normalized_axis_resources)
A:jax._src.maps.necessary_resources->set(it.chain(*frozen_axis_resources.values()))
A:jax._src.maps.donate_argnums->_ensure_index_tuple(donate_argnums)
A:jax._src.maps.has_input_rank_assertions->any((spec.expected_rank is not None for spec in in_axes_entries))
A:jax._src.maps.has_output_rank_assertions->any((spec.expected_rank is not None for spec in out_axes_entries))
A:jax._src.maps.available_resources->set(resource_env.shape.keys())
A:jax._src.maps.(args_flat, in_tree)->tree_flatten(args)
A:jax._src.maps.(fun_flat, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.maps.donated_invars->donation_vector(donate_argnums, (), args, {})
A:jax._src.maps.in_axes_flat->_flatten_axes('xmap in_axes', in_tree, in_axes, tupled_args=True)
A:jax._src.maps.out_axes_thunk->HashableFunction(lambda : tuple(_flatten_axes('xmap out_axes', out_tree(), out_axes, tupled_args=False)), closure=(out_axes_entries, out_axes_treedef))
A:jax._src.maps.axis_resource_count->_get_axis_resource_count(axis_resources, resource_env)
A:jax._src.maps.frozen_global_axis_sizes->_get_axis_sizes(args_flat, in_axes_flat, axis_sizes, axis_resource_count)
A:jax._src.maps.params->dict(name=getattr(fun, '__name__', '<unnamed function>'), in_axes=tuple(in_axes_flat), out_axes_thunk=out_axes_thunk, donated_invars=donated_invars, global_axis_sizes=frozen_global_axis_sizes, axis_resources=frozen_axis_resources, resource_env=resource_env, backend=backend, spmd_in_axes=None, spmd_out_axes_thunk=None)
A:jax._src.maps.f->jax._src.interpreters.pxla.vtile_manual(f, tuple(manual_mesh_axes), mesh, mesh_in_axes, mesh_out_axes)
A:jax._src.maps.(fun_flat, args_flat, params, _, out_tree)->infer_params(*args)
A:jax._src.maps.out_flat->XMapPrimitive().bind(fun, *all_args, **new_params)
A:jax._src.maps.lowering_parameters->kwargs.pop('_experimental_lowering_platform', mlir.LoweringParameters())
A:jax._src.maps.(fun_flat, args_flat, params, in_tree, out_tree)->infer_params(*args)
A:jax._src.maps.computation->make_xmap_callable(fun_flat, params['name'], params['in_axes'], params['out_axes_thunk'], params['donated_invars'], params['global_axis_sizes'], params['axis_resources'], params['resource_env'], params['backend'], params['spmd_in_axes'], params['spmd_out_axes_thunk'], lowering_parameters, *avals_flat)
A:jax._src.maps.in_tree->treedef_tuple([in_tree, tree_flatten({})[1]])
A:jax._src.maps.in_avals->treedef_tuple([in_tree, tree_flatten({})[1]]).unflatten(avals_flat)
A:jax._src.maps.plan->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes)
A:jax._src.maps.(jaxpr, out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_final(fun, mapped_in_avals)
A:jax._src.maps.out_axes->out_axes_thunk()
A:jax._src.maps.jaxpr->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('call_jaxpr')
A:jax._src.maps.(mesh_in_axes, mesh_out_axes)->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes).to_mesh_axes(in_axes, out_axes)
A:jax._src.maps.manual_mesh_axes->frozenset(it.chain.from_iterable(plan.physical_axis_resources.values()))
A:jax._src.maps.tiling_method->jax._src.interpreters.pxla.TileVectorize()
A:jax._src.maps.env->dict(self.resource_env.shape)
A:jax._src.maps.(physical_axis_resources, loop_axis_resources)->_unzip_axis_resources(axis_resources, resource_env)
A:jax._src.maps.axis_subst_dict->dict(axis_resources)
A:jax._src.maps.map_in_axes->tuple(unsafe_map(lambda spec: spec.get(naxis, None), in_axes))
A:jax._src.maps.map_out_axes->tuple(unsafe_map(lambda spec: spec.get(naxis, None), out_axes))
A:jax._src.maps.used_loops->set(it.chain.from_iterable(self.loop_axis_resources.values()))
A:jax._src.maps.loop_in_axes->_to_resource_axes(in_axes, self.loop_axis_resources)
A:jax._src.maps.loop_out_axes->_to_resource_axes(out_axes, self.loop_axis_resources)
A:jax._src.maps.(_, stacked_results)->jax.lax.scan(body, 0, (), length=loop_length)
A:jax._src.maps.post_process->getattr(trace, 'post_process_xmap', None)
A:jax._src.maps.new_params->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk)
A:jax._src.maps.subfun->jax._src.linear_util.hashable_partial(lu.wrap_init(core.eval_jaxpr), jaxpr, ())
A:jax._src.maps.axes->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('out_axes')
A:jax._src.maps.new_params['out_axes_thunk']->HashableFunction(lambda : axes, closure=axes)
A:jax._src.maps.spmd_axes->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('spmd_out_axes')
A:jax._src.maps.new_params['spmd_out_axes_thunk']->HashableFunction(lambda : spmd_axes, closure=spmd_axes)
A:jax._src.maps.xmap_p->XMapPrimitive()
A:jax._src.maps.new_jaxpr->jax._src.core.subst_axis_names_jaxpr(params['call_jaxpr'], shadowed_subst)
A:jax._src.maps.(all_args, in_tree_def)->tree_flatten(((), args, cts_in))
A:jax._src.maps.fun->jax._src.linear_util.hashable_partial(lu.wrap_init(ad.backward_pass), call_jaxpr, reduce_axes + tuple(params['global_axis_sizes'].keys()), False)
A:jax._src.maps.(fun, nz_arg_cts)->jax._src.interpreters.ad.nonzero_outputs(fun)
A:jax._src.maps.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax._src.maps.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax._src.maps.inner_axis_resources->dict(outer_axis_resources)
A:jax._src.maps.used_resources->set()
A:jax._src.maps.baxis_resources->set(inner_axis_resources[baxis])
A:jax._src.maps.(jaxpr, mapped_out_avals, consts)->trace_to_subjaxpr_dynamic(f, self.main, mapped_in_avals)
A:jax._src.maps.spmd_out_axes->spmd_out_axes_thunk()
A:jax._src.maps.source_info->jax._src.source_info_util.current()
A:jax._src.maps.invars->map(self.getvar, tracers)
A:jax._src.maps.constvars->map(self.getvar, map(self.instantiate_const, consts))
A:jax._src.maps.outvars->map(self.makevar, out_tracers)
A:jax._src.maps.call_jaxpr->convert_constvars_jaxpr(jaxpr)
A:jax._src.maps.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers), unknown_tracers_out, primitive, unknown_params, jaxpr_unknown.effects, source_info_util.current())
A:jax._src.maps.(donated_invars_known, _)->jax._src.interpreters.partial_eval.partition_list(unks_in, params_known['donated_invars'])
A:jax._src.maps.(in_axes_known, _)->jax._src.interpreters.partial_eval.partition_list(unks_in, params_known['in_axes'])
A:jax._src.maps.(_, out_axes_known)->partition_list(out_knowns, out_axes)
A:jax._src.maps.new_params_known->dict(params_known, in_axes=tuple(in_axes_known), out_axes=(*out_axes_known, *residual_axes), donated_invars=tuple(donated_invars_known))
A:jax._src.maps.(_, donated_invars_staged)->jax._src.interpreters.partial_eval.partition_list(inst_in, params_staged['donated_invars'])
A:jax._src.maps.(_, in_axes_staged)->jax._src.interpreters.partial_eval.partition_list(inst_in, params_staged['in_axes'])
A:jax._src.maps.(_, out_axes_staged)->jax._src.interpreters.partial_eval.partition_list(kept_outs_staged, params_staged['out_axes'])
A:jax._src.maps.new_params_staged->dict(params_staged, in_axes=tuple(in_axes_staged), out_axes=tuple(out_axes_staged), donated_invars=tuple(donated_invars_staged))
A:jax._src.maps.pe.partial_eval_jaxpr_custom_rules[xmap_p]->partial(pe.call_partial_eval_custom_rule, 'call_jaxpr', _xmap_partial_eval_custom_params_updater)
A:jax._src.maps.(in_knowns, in_avals, in_consts)->jax._src.interpreters.partial_eval.partition_pvals(in_pvals)
A:jax._src.maps.(f, aux)->jax._src.interpreters.partial_eval.partial_eval_wrapper_nounits(f, tuple(in_knowns), tuple(in_avals))
A:jax._src.maps.(f, out_named_shapes)->out_local_named_shapes(f, frozenset(global_axis_sizes))
A:jax._src.maps.(out_knowns, _, _, _)->aux()
A:jax._src.maps.(_, _, jaxpr_unknown, _)->aux()
A:jax._src.maps.num_res->len(jaxpr_unknown.constvars)
A:jax._src.maps.known_params->dict(params, in_axes=tuple((a for (a, k) in zip(in_axes, in_knowns) if k)), donated_invars=tuple((d for (d, k) in zip(donated_invars, in_knowns) if k)), out_axes_thunk=new_out_axes_thunk)
A:jax._src.maps.out->jax.lax.psum(padded, tuple(out_axes.keys()))
A:jax._src.maps.(out_knowns, out_avals, jaxpr_unknown, env)->aux()
A:jax._src.maps.(known_outvals, res)->split_list(out, [len(out) - len(jaxpr_unknown.constvars)])
A:jax._src.maps.jaxpr_unknown->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_unknown)
A:jax._src.maps.unknown_params->dict(params, call_jaxpr=jaxpr_unknown, out_axes=tuple(out_axes_unknown), spmd_out_axes=None, donated_invars=(*(False for _ in res), *(d for (d, k) in zip(donated_invars, in_knowns) if not k)), in_axes=(*res_axes(), *(None for _ in env), *(a for (a, k) in zip(in_axes, in_knowns) if not k)))
A:jax._src.maps.res_tracers->map(self.new_instantiated_const, res)
A:jax._src.maps.env_tracers->map(self.full_raise, env)
A:jax._src.maps.new_spmd_in_axes->tuple((spmd_axes if d is not_mapped else insert_spmd_axis(spmd_axes, d) for (spmd_axes, d) in zip(spmd_in_axes, dims)))
A:jax._src.maps.dims_out->dims_out_thunk()
A:jax._src.maps.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax._src.maps.new_in_axes->tuple((_fmap_dims(in_axes, lambda a: a + (d is not not_mapped and d <= a)) for (d, in_axes) in zip(dims, params['in_axes'])))
A:jax._src.maps.mapped_dims_in->tuple((d if d is not_mapped else d - sum((a < d for a in in_axis.values())) for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax._src.maps.(f, mapped_dims_out)->jax._src.interpreters.batching.batch_subtrace(f, self.main, mapped_dims_in)
A:jax._src.maps.(new_spmd_in_axes, new_spmd_out_axes_thunk)->_batch_trace_update_spmd_axes(params['spmd_in_axes'], params['spmd_out_axes_thunk'], self.axis_name, dims, dims_out_thunk)
A:jax._src.maps.vals_out->primitive.bind(f, *vals, **new_params)
A:jax._src.maps.batching.BatchTrace.process_xmap->partialmethod(_batch_trace_process_xmap, False)
A:jax._src.maps.pxla.SPMDBatchTrace.process_xmap->partialmethod(_batch_trace_process_xmap, True)
A:jax._src.maps.(vals, dims, srcs)->unzip3(((t.val, t.batch_dim, t.source_info) for t in out_tracers))
A:jax._src.maps.trace->main.with_cur_sublevel()
A:jax._src.maps.resource_call_jaxpr->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes).subst_axes_with_resources(call_jaxpr)
A:jax._src.maps.(vectorized_jaxpr, out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, local_avals)
A:jax._src.maps.sub_ctx->ctx.module_context.replace(name_stack=ctx.module_context.name_stack.extend(wrap_name(name, 'xmap')), axis_context=ctx.module_context.axis_context.extend_manual(manual_mesh_axes))
A:jax._src.maps.(tiled_outs, _)->jax._src.interpreters.mlir.jaxpr_subcomp(sub_ctx, vectorized_jaxpr, mlir.TokenSet(), const_nodes, *tiled_ins, dim_var_values=ctx.dim_var_values)
A:jax._src.maps.(vectorized_jaxpr, global_out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, global_in_avals)
A:jax._src.maps.global_sharding_spec->jax._src.interpreters.pxla.mesh_sharding_specs(mesh.shape, mesh.axis_names)
A:jax._src.maps.(global_out_nodes, _)->jax._src.interpreters.mlir.jaxpr_subcomp(sub_ctx, vectorized_jaxpr, mlir.TokenSet(), const_nodes, *([n] for n in global_in_nodes), dim_var_values=ctx.dim_var_values)
A:jax._src.maps.zero->numpy.zeros((), dtype=np.int32)
A:jax._src.maps.axis_index->jax.lax.axis_index(name)
A:jax._src.maps.stride_c->numpy.array(strides[axis], np.int32)
A:jax._src.maps.linear_idxs[axis]->jax.lax.add(linear_idxs[axis], lax.mul(axis_index, stride_c))
A:jax._src.maps.tile_shape->list(x.shape)
A:jax._src.maps.base_idxs->_tile_base_indices(tile_shape, out_axes, axis_sizes)
A:jax._src.maps.shape->list(x_moved.shape)
A:jax._src.maps.padded->jax.lax.dynamic_update_slice(padded, x, base_idxs)
A:jax._src.maps.named_shape->dict(aval.named_shape)
A:jax._src.maps.nlocal->math.prod(map(local_res_shape.get, resources))
A:jax._src.maps.resource_count_map[axis]->ResourceCount(math.prod(map(global_res_shape.get, resources)), nlocal, distributed)
A:jax._src.maps.global_axis_sizes->dict(global_axis_sizes)
A:jax._src.maps.arg->arg.squeeze(dim).squeeze(dim)
A:jax._src.maps.squeezed_args->map(_squeeze_mapped_axes, flat_args, flat_in_axes)
A:jax._src.maps.updates->jax._src.core.traverse_jaxpr_params(partial(_jaxpr_resources, resource_env=resource_env), eqn.params).values()
A:jax._src.maps.x_moved->moveaxis(x, 0, axis)
A:jax._src.maps.(tile_size, rem)->divmod(x.shape[dim], n)
A:jax._src.maps.defined_axes->set(global_axis_sizes)
A:jax._src.maps.undeclared_axes_str->sorted((str(axis) for axis in undeclared_axes))
A:jax._src.maps.mesh_in_axes->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes).to_mesh_axes(in_axes_flat)
A:jax._src.maps.xmap_sharding->jax._src.interpreters.pxla.create_mesh_pspec_sharding(mesh, array_mapping_to_axis_resources(xmap_array_mapping))
A:jax._src.maps.rec->partial(_check_no_loop_collectives, loop_axis_resources=loop_axis_resources)
A:jax._src.maps.gen_fresh_name->jax._src.core.gensym([jaxpr])
A:jax._src.maps.new_jaxpr_params->jax._src.core.traverse_jaxpr_params(rec, eqn.params)
A:jax._src.maps.mps->jax._src.sharding_impls.NamedSharding._from_parsed_pspec(resource_env.physical_mesh, ParsedPartitionSpec((), ()))
A:jax._src.maps.unconstrained_dims->get_unconstrained_dims(mps)
A:jax._src.maps.gspmd_sharding->jax._src.pjit.GSPMDSharding.get_replicated(mps._device_assignment)
A:jax._src.maps.SPMD_LOWERING->jax._src.config.define_bool_state(name='experimental_xmap_spmd_lowering', default=False, help='When set, multi-device xmap computations will be compiled through the XLA SPMD partitioner instead of explicit cross-replica collectives. Not supported on CPU!', update_global_hook=_clear_compilation_cache, update_thread_local_hook=_thread_local_flag_unsupported)
A:jax._src.maps.SPMD_LOWERING_MANUAL->jax._src.config.define_bool_state(name='experimental_xmap_spmd_lowering_manual', default=False, help='When set, multi-device xmap computations will be compiled using the MANUAL partitioning feature of the XLA SPMD partitioner instead of sharding constraints on vectorized code. Requires experimental_xmap_spmd_lowering!', update_global_hook=_ensure_spmd_and(_clear_compilation_cache), update_thread_local_hook=_thread_local_flag_unsupported)
A:jax._src.maps._ENSURE_FIXED_SHARDING->jax._src.config.define_bool_state(name='experimental_xmap_ensure_fixed_sharding', default=False, help='When set and `experimental_xmap_spmd_lowering` is enabled, the lowering will try to limit the flexibility of the automated SPMD partitioner heuristics by emitting additional sharding annotations for program intermediates.', update_global_hook=_ensure_spmd_and(_clear_compilation_cache), update_thread_local_hook=_thread_local_flag_unsupported)
jax._src.maps.AxisNamePos(self,*args,user_repr,**kwargs)
jax._src.maps.AxisNamePos.__init__(self,*args,user_repr,**kwargs)
jax._src.maps.AxisNamePosWithRank(self,*args,expected_rank,**kwargs)
jax._src.maps.AxisNamePosWithRank.__init__(self,*args,expected_rank,**kwargs)
jax._src.maps.DotDotDotRepr
jax._src.maps.DotDotDotRepr.__repr__(self)
jax._src.maps.EvaluationPlan(NamedTuple)
jax._src.maps.EvaluationPlan.axis_subst(self)->core.AxisSubst
jax._src.maps.EvaluationPlan.from_axis_resources(cls,axis_resources:dict[AxisName,tuple[ResourceAxisName,...]],resource_env:ResourceEnv,global_axis_sizes:dict[AxisName,int])
jax._src.maps.EvaluationPlan.resource_axis_env(self)
jax._src.maps.EvaluationPlan.subst_axes_with_resources(self,jaxpr)
jax._src.maps.EvaluationPlan.to_mesh_axes(self,in_axes,out_axes=None)
jax._src.maps.EvaluationPlan.vectorize_and_loop(self,f:lu.WrappedFun,in_axes,out_axes)->lu.WrappedFun
jax._src.maps.FrozenDict(self,*args,**kwargs)
jax._src.maps.FrozenDict.__eq__(self,other)
jax._src.maps.FrozenDict.__getitem__(self,name)
jax._src.maps.FrozenDict.__hash__(self)
jax._src.maps.FrozenDict.__init__(self,*args,**kwargs)
jax._src.maps.FrozenDict.__iter__(self)
jax._src.maps.FrozenDict.__len__(self)
jax._src.maps.FrozenDict.__repr__(self)
jax._src.maps.NoQuotesStr(str)
jax._src.maps.ResourceCount(NamedTuple)
jax._src.maps.ResourceCount.to_local(self,global_size)
jax._src.maps.SerialLoop(self,length)
jax._src.maps.SerialLoop.__eq__(self,other)
jax._src.maps.SerialLoop.__hash__(self)
jax._src.maps.SerialLoop.__init__(self,length)
jax._src.maps.XMapPrimitive(self)
jax._src.maps.XMapPrimitive.__init__(self)
jax._src.maps.XMapPrimitive.bind(self,fun,*args,in_axes,**params)
jax._src.maps.XMapPrimitive.get_bind_params(self,params)
jax._src.maps.XMapPrimitive.post_process(self,trace,out_tracers,params)
jax._src.maps.XMapPrimitive.process(self,trace,fun,tracers,params)
jax._src.maps._UniqueResourceName(self,uid,tag=None)
jax._src.maps._UniqueResourceName.__eq__(self,other)
jax._src.maps._UniqueResourceName.__hash__(self)
jax._src.maps._UniqueResourceName.__init__(self,uid,tag=None)
jax._src.maps._UniqueResourceName.__repr__(self)
jax._src.maps._axis_after_insertion(axis,inserted_named_axes)
jax._src.maps._batch_trace_post_process_xmap(self,primitive,out_tracers,params)
jax._src.maps._batch_trace_process_xmap(self,is_spmd,primitive,f:lu.WrappedFun,tracers,params)
jax._src.maps._batch_trace_update_spmd_axes(spmd_in_axes,spmd_out_axes_thunk,axis_name,dims,dims_out_thunk)
jax._src.maps._check_gda_or_array_xmap_partitioning(axis_resources,resource_env,global_axis_sizes,in_axes_flat,args_flat)
jax._src.maps._check_no_loop_collectives(jaxpr,loop_axis_resources)
jax._src.maps._check_out_avals_vs_out_axes(out_avals:Sequence[core.AbstractValue],out_axes:Sequence[AxisNamePos],global_axis_sizes:dict[AxisName,int])
jax._src.maps._clear_compilation_cache(_)
jax._src.maps._delete_aval_axes(aval,axes:AxisNamePos,global_axis_sizes)
jax._src.maps._dynamic_jaxpr_process_xmap(self,primitive,f,tracers,params)
jax._src.maps._ensure_spmd_and(f)
jax._src.maps._fix_inferred_spmd_sharding(jaxpr,resource_env,gen_fresh_name=None)
jax._src.maps._flatten_axes(what,tree,axes,tupled_args)
jax._src.maps._fmap_dims(axes,f)
jax._src.maps._get_axis_resource_count(axis_resources,resource_env)->dict[ResourceAxisName, ResourceCount]
jax._src.maps._get_axis_sizes(args_flat:Iterable[Any],in_axes_flat:Iterable[AxisNamePos],global_axis_sizes:dict[AxisName,int],axis_resource_count:dict[AxisName,ResourceCount])
jax._src.maps._insert_aval_axes(aval,axes:AxisNamePos,local_axis_sizes)
jax._src.maps._is_axes_leaf(entry)
jax._src.maps._jaxpr_resources(jaxpr,resource_env)->set[ResourceAxisName]
jax._src.maps._jaxpr_trace_process_xmap(self,primitive,f:lu.WrappedFun,tracers,params)
jax._src.maps._merge_leading_axis(x,axis:Optional[int])
jax._src.maps._parse_entry(arg_name,entry)
jax._src.maps._prepare_axes(axes,arg_name)
jax._src.maps._process_xmap_default(self,call_primitive,f,tracers,params)
jax._src.maps._resource_typing_xmap(avals,params,source_info:source_info_util.SourceInfo,resource_env,outer_axis_resources)
jax._src.maps._slice_tile(x,dim:Optional[int],i,n:int)
jax._src.maps._thread_local_flag_unsupported(_)
jax._src.maps._tile(x,in_axes,axis_sizes)
jax._src.maps._tile_base_indices(tile_shape,axes,axis_sizes)
jax._src.maps._to_resource_axes(axes_specs:Sequence[AxisNamePos],axis_resources:dict[AxisName,tuple[ResourceAxisName,...]])
jax._src.maps._typecheck_xmap(_,*in_atoms,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes)
jax._src.maps._untile(x,out_axes,axis_sizes)
jax._src.maps._unzip_axis_resources(axis_resources:dict[AxisName,tuple[ResourceAxisName,...]],resource_env:ResourceEnv)
jax._src.maps._xmap_axis_subst(params,subst,traverse)
jax._src.maps._xmap_lowering_rule(ctx,*args,**kwargs)
jax._src.maps._xmap_lowering_rule_replica(ctx,*in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,axis_resources,resource_env,backend)
jax._src.maps._xmap_lowering_rule_spmd(ctx,*global_in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,axis_resources,resource_env,backend)
jax._src.maps._xmap_lowering_rule_spmd_manual(ctx,*global_in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,axis_resources,resource_env,backend)
jax._src.maps._xmap_partial_eval_custom_params_updater(unks_in:Sequence[bool],inst_in:Sequence[bool],kept_outs_known:Sequence[bool],kept_outs_staged:Sequence[bool],num_res:int,params_known:dict,params_staged:dict)->tuple[dict, dict]
jax._src.maps._xmap_transpose(params,call_jaxpr,args,cts_in,cts_in_avals,reduce_axes)
jax._src.maps.fresh_resource_name(tag=None)
jax._src.maps.hide_mapped_axes(flat_in_axes,flat_out_axes,*flat_args)
jax._src.maps.make_xmap_callable(fun:lu.WrappedFun,name,in_axes,out_axes_thunk,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes_thunk,lowering_parameters:mlir.LoweringParameters,*in_avals)
jax._src.maps.out_local_named_shapes(local_axes,*args,**kwargs)
jax._src.maps.serial_loop(name:ResourceAxisName,length:int)
jax._src.maps.xmap(fun:Callable,in_axes,out_axes,*,axis_sizes:Optional[Mapping[AxisName,int]]=None,axis_resources:Optional[Mapping[AxisName,ResourceSet]]=None,donate_argnums:Union[int,Sequence[int]]=(),backend:Optional[str]=None)->stages.Wrapped
jax._src.maps.xmap_impl(fun:lu.WrappedFun,*args,name,in_axes,out_axes_thunk,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes_thunk)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/abstract_arrays.py----------------------------------------
A:jax._src.abstract_arrays.(dtype, weak_type)->jax._src.dtypes._lattice_result_type(x)
A:jax._src.abstract_arrays.dtype->jax._src.dtypes._scalar_type_to_dtype(t, x)
A:jax._src.abstract_arrays.aval->jax._src.core.ShapedArray((), dtype, weak_type=True)
A:jax._src.abstract_arrays.weak_type->jax._src.dtypes.is_weakly_typed(x)
A:jax._src.abstract_arrays.core.pytype_aval_mappings[t]->partial(_make_concrete_python_scalar, t)
A:jax._src.abstract_arrays.ad_util.jaxval_zeros_likers[t]->partial(_zeros_like_python_scalar, t)
jax._src.abstract_arrays._make_concrete_python_scalar(t,x)
jax._src.abstract_arrays._zeros_like_python_scalar(t,x)
jax._src.abstract_arrays.canonical_concrete_aval(val,weak_type=None)
jax._src.abstract_arrays.masked_array_error(*args,**kwargs)
jax._src.abstract_arrays.zeros_like_array(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/path.py----------------------------------------
A:jax._src.path.logger->logging.getLogger(__name__)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/public_test_util.py----------------------------------------
A:jax._src.public_test_util.a->maybe_upcast(a)
A:jax._src.public_test_util.b->maybe_upcast(b)
A:jax._src.public_test_util.dtype->_dtype(x)
A:jax._src.public_test_util.atol->_merge_tolerance(atol, default_gradient_tolerance)
A:jax._src.public_test_util.rtol->_merge_tolerance(rtol, default_gradient_tolerance)
A:jax._src.public_test_util.assert_close->partial(_assert_numpy_close, atol=atol, rtol=rtol, err_msg=err_msg)
A:jax._src.public_test_util.result->randn()
A:jax._src.public_test_util.add->partial(tree_map, _preserve_input_types(operator.add))
A:jax._src.public_test_util.sub->partial(tree_map, _preserve_input_types(operator.sub))
A:jax._src.public_test_util.safe_sub->partial(tree_map, lambda x, y: _safe_subtract(x, y, dtype=_dtype(x)))
A:jax._src.public_test_util.conj->partial(tree_map, _preserve_input_types(np.conj))
A:jax._src.public_test_util.shape->numpy.shape(x)
A:jax._src.public_test_util.delta->scalar_mul(tangents, eps)
A:jax._src.public_test_util.f_pos->f(*add(primals, delta))
A:jax._src.public_test_util.f_neg->f(*sub(primals, delta))
A:jax._src.public_test_util.out->default.copy()
A:jax._src.public_test_util.rng->numpy.random.RandomState(0)
A:jax._src.public_test_util.tangent->tree_map(_rand_like, args)
A:jax._src.public_test_util.(v_out, t_out)->f_jvp(args, tangent)
A:jax._src.public_test_util.v_out_expected->f(*args)
A:jax._src.public_test_util.t_out_expected->numerical_jvp(f, args, tangent, eps=eps)
A:jax._src.public_test_util._rand_like->partial(rand_like, np.random.RandomState(0))
A:jax._src.public_test_util.(v_out, vjpfun)->f_vjp(*args)
A:jax._src.public_test_util.tangent_out->numerical_jvp(f, args, tangent, eps=eps)
A:jax._src.public_test_util.cotangent->tree_map(_rand_like, v_out)
A:jax._src.public_test_util.cotangent_out->conj(vjpfun(conj(cotangent)))
A:jax._src.public_test_util.ip->inner_prod(tangent, cotangent_out)
A:jax._src.public_test_util.ip_expected->inner_prod(tangent_out, cotangent)
A:jax._src.public_test_util.args->tuple(args)
A:jax._src.public_test_util._check_jvp->partial(check_jvp, atol=atol, rtol=rtol, eps=eps)
A:jax._src.public_test_util._check_vjp->partial(check_vjp, atol=atol, rtol=rtol, eps=eps)
A:jax._src.public_test_util.(out_primal_py, vjp_py)->jax._src.api.vjp(f, *args)
jax._src.public_test_util._assert_numpy_allclose(a,b,atol=None,rtol=None,err_msg='')
jax._src.public_test_util._assert_numpy_close(a,b,atol=None,rtol=None,err_msg='')
jax._src.public_test_util._check_dtypes_match(xs,ys)
jax._src.public_test_util._dtype(x)
jax._src.public_test_util._merge_tolerance(tol,default)
jax._src.public_test_util._preserve_input_types(f)
jax._src.public_test_util._safe_subtract(x,y,*,dtype)
jax._src.public_test_util.check_close(xs,ys,atol=None,rtol=None,err_msg='')
jax._src.public_test_util.check_grads(f,args,order,modes=('fwd','rev'),atol=None,rtol=None,eps=None)
jax._src.public_test_util.check_jvp(f,f_jvp,args,atol=None,rtol=None,eps=EPS,err_msg='')
jax._src.public_test_util.check_vjp(f,f_vjp,args,atol=None,rtol=None,eps=EPS,err_msg='')
jax._src.public_test_util.default_tolerance()
jax._src.public_test_util.inner_prod(xs,ys)
jax._src.public_test_util.is_python_scalar(val)
jax._src.public_test_util.numerical_jvp(f,primals,tangents,eps=EPS)
jax._src.public_test_util.rand_like(rng,x)
jax._src.public_test_util.scalar_mul(xs,a)
jax._src.public_test_util.tolerance(dtype,tol=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lazy_loader.py----------------------------------------
jax._src.lazy_loader.attach(package_name:str,submodules:Sequence[str])->tuple[Callable[[str], Any], Callable[[], list[str]], list[str]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/sharding_impls.py----------------------------------------
A:jax._src.sharding_impls.hlo_sharding->jax._src.lib.xla_client.HloSharding.from_proto(hlo_sharding)
A:jax._src.sharding_impls.gspmd_sharding->GSPMDSharding(s._device_assignment, hlo_sharding)
A:jax._src.sharding_impls.(partitions, _)->get_num_ways_dim_sharded(hlo_sharding)
A:jax._src.sharding_impls.(quotient, remainder)->divmod(s, p)
A:jax._src.sharding_impls.h_index->hashed_index(index)
A:jax._src.sharding_impls.(sharding_spec, special_axes)->cls.__new__(cls)._get_sharding_spec(num_dimensions, manual_axes)
A:jax._src.sharding_impls._manual_axes->frozenset()
A:jax._src.sharding_impls.(self._parsed_pspec, _, _)->prepare_axis_resources(PartitionSpec() if self.spec is None else self.spec, 'NamedSharding spec', allow_unconstrained_dims=True)
A:jax._src.sharding_impls.mesh_repr->', '.join((f"'{k}': {v}" for (k, v) in self.mesh.shape.items()))
A:jax._src.sharding_impls.self._hash->hash((self._devices, self._hlo_sharding_hash, self.memory_kind))
A:jax._src.sharding_impls.array_mapping->get_array_mapping(self._parsed_pspec)
A:jax._src.sharding_impls.sharding_spec->jax._src.sharding_specs.create_pmap_sharding_spec(tuple(shape), sharded_dim)
A:jax._src.sharding_impls.indices->op_sharding_to_indices(self._hlo_sharding, global_shape, len(self._devices))
A:jax._src.sharding_impls.self.devices->numpy.asarray(devices)
A:jax._src.sharding_impls.pmap_devices->numpy.array(devices)
A:jax._src.sharding_impls.op_sharding->jax._src.lib.xla_client.HloSharding.from_proto(op_sharding)
A:jax._src.sharding_impls.name->cls.__new__(cls)._devices[0].platform.upper()
A:jax._src.sharding_impls.ids->', '.join(safe_map(str, sorted(self._ids)))
A:jax._src.sharding_impls.p->p.replicate(-1, keepdims=False).replicate(-1, keepdims=False)
A:jax._src.sharding_impls.pbuf->jax._src.lib.xla_client.OpSharding()
A:jax._src.sharding_impls.product_of_dims->math.prod(pbuf.tile_assignment_dimensions)
A:jax._src.sharding_impls.num_devices->len(pbuf.tile_assignment_devices)
A:jax._src.sharding_impls.devices->numpy.array(devices, dtype='object')
A:jax._src.sharding_impls.self._devices->tuple(devices)
A:jax._src.sharding_impls.self._ids->frozenset(ids)
A:jax._src.sharding_impls.self._internal_device_list->jax._src.lib.xla_client.DeviceList(self._devices)
A:jax._src.sharding_impls.self._memory_kind->jax._src.lib.xla_client.check_and_canonicalize_memory_kind(memory_kind, self._internal_device_list)
A:jax._src.sharding_impls.platform_name->cls.__new__(cls)._devices[0].platform.upper()
A:jax._src.sharding_impls.ids[idx]->DeviceIdSet(platform_name, *(self._devices[i].id for i in x))
A:jax._src.sharding_impls.body->numpy.array2string(ids, prefix=cls_name + '(', suffix=')', max_line_width=100)
A:jax._src.sharding_impls.T->property(transpose)
A:jax._src.sharding_impls.new_ids->cls.__new__(cls)._ids.sum(axis=axis, keepdims=keepdims)
A:jax._src.sharding_impls.self->cls.__new__(cls)
A:jax._src.sharding_impls.all_ids_equal->numpy.array_equal(self._ids, other._ids)
A:jax._src.sharding_impls.self._hlo_sharding->jax._src.lib.xla_client.HloSharding.from_proto(op_sharding)
A:jax._src.sharding_impls.(num_ways_dim_sharded, _)->get_num_ways_dim_sharded(self._hlo_sharding)
A:jax._src.sharding_impls.UNSPECIFIED->UnspecifiedValue()
A:jax._src.sharding_impls.reverse_map->collections.defaultdict(list)
A:jax._src.sharding_impls.self.partitions->tuple(partitions)
A:jax._src.sharding_impls.new_partitions->jax._src.util.tuple_insert(parts, dim, val)
A:jax._src.sharding_impls.axis_spec->tuple(axis_spec)
A:jax._src.sharding_impls.partitions->list(parsed_pspec.partitions)
A:jax._src.sharding_impls.unspecified->is_unspecified(axis_resources[0])
A:jax._src.sharding_impls.current_is_unspecified->is_unspecified(resource)
A:jax._src.sharding_impls.(entries, treedef)->jax._src.tree_util.tree_flatten(axis_resources, is_leaf=lambda x: x is None)
A:jax._src.sharding_impls.resource_counts->collections.Counter(itertools.chain.from_iterable(constrained_dims))
A:jax._src.sharding_impls.sizes->numpy.fromiter(named_sizes.values(), dtype=np.int64)
A:jax._src.sharding_impls.strides->strides_for_sizes(sizes)
A:jax._src.sharding_impls.dims->list(reversed(dims))
A:jax._src.sharding_impls.flat_assignment->numpy.asarray(assignment, dtype=np.int64)
A:jax._src.sharding_impls.mesh_axis_order->unflatten_array(mesh.shape, hlo_sharding.tile_assignment_devices())
A:jax._src.sharding_impls.mesh_axis->iter(mesh_axis_order)
A:jax._src.sharding_impls.shape->jax._src.lib.xla_client.HloSharding.from_proto(hlo_sharding).tile_assignment_dimensions()
A:jax._src.sharding_impls.axis->next(mesh_axis)
jax._src.sharding_impls.AUTO(self,mesh:mesh_lib.Mesh)
jax._src.sharding_impls.AUTO.__init__(self,mesh:mesh_lib.Mesh)
jax._src.sharding_impls.AxisEnv(NamedTuple)
jax._src.sharding_impls.CanonicalizedParsedPartitionSpec(self,parsed_pspec:ParsedPartitionSpec)
jax._src.sharding_impls.CanonicalizedParsedPartitionSpec.__init__(self,parsed_pspec:ParsedPartitionSpec)
jax._src.sharding_impls.CanonicalizedParsedPartitionSpec.__repr__(self)
jax._src.sharding_impls.DeviceIdSet(self,name,*ids)
jax._src.sharding_impls.DeviceIdSet.__add__(self,other)->DeviceIdSet
jax._src.sharding_impls.DeviceIdSet.__eq__(self,other)->bool
jax._src.sharding_impls.DeviceIdSet.__hash__(self)->int
jax._src.sharding_impls.DeviceIdSet.__init__(self,name,*ids)
jax._src.sharding_impls.DeviceIdSet.__iter__(self)
jax._src.sharding_impls.DeviceIdSet.__len__(self)->int
jax._src.sharding_impls.DeviceIdSet.__repr__(self)->str
jax._src.sharding_impls.GSPMDSharding(self,devices:Sequence[Device],op_sharding:xc.OpSharding|xc.HloSharding,*,memory_kind:str|None=None)
jax._src.sharding_impls.GSPMDSharding.__eq__(self,other)
jax._src.sharding_impls.GSPMDSharding.__hash__(self)
jax._src.sharding_impls.GSPMDSharding.__init__(self,devices:Sequence[Device],op_sharding:xc.OpSharding|xc.HloSharding,*,memory_kind:str|None=None)
jax._src.sharding_impls.GSPMDSharding.__reduce__(self)
jax._src.sharding_impls.GSPMDSharding.__repr__(self)
jax._src.sharding_impls.GSPMDSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.GSPMDSharding._hlo_sharding_hash(self)
jax._src.sharding_impls.GSPMDSharding._to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.GSPMDSharding.device_set(self)->set[Device]
jax._src.sharding_impls.GSPMDSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.GSPMDSharding.get_replicated(cls,device_assignment,*,memory_kind:str|None=None)
jax._src.sharding_impls.GSPMDSharding.is_compatible_aval(self,aval_shape:Shape)
jax._src.sharding_impls.GSPMDSharding.is_fully_addressable(self)->bool
jax._src.sharding_impls.GSPMDSharding.is_fully_replicated(self)->bool
jax._src.sharding_impls.GSPMDSharding.memory_kind(self)->str | None
jax._src.sharding_impls.GSPMDSharding.with_memory_kind(self,kind:str)->GSPMDSharding
jax._src.sharding_impls.NamedSharding(self,mesh:mesh_lib.Mesh,spec:PartitionSpec,*,memory_kind:str|None=None,_parsed_pspec=None,_manual_axes=frozenset())
jax._src.sharding_impls.NamedSharding.__eq__(self,other)
jax._src.sharding_impls.NamedSharding.__hash__(self)
jax._src.sharding_impls.NamedSharding.__init__(self,mesh:mesh_lib.Mesh,spec:PartitionSpec,*,memory_kind:str|None=None,_parsed_pspec=None,_manual_axes=frozenset())
jax._src.sharding_impls.NamedSharding.__reduce__(self)
jax._src.sharding_impls.NamedSharding.__repr__(self)
jax._src.sharding_impls.NamedSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.NamedSharding._from_parsed_pspec(cls,mesh,parsed_pspec,*,memory_kind=None,_manual_axes=frozenset())
jax._src.sharding_impls.NamedSharding._get_sharding_spec(self,num_dimensions,manual_axes)
jax._src.sharding_impls.NamedSharding._preprocess(self)
jax._src.sharding_impls.NamedSharding.addressable_devices(self)->set[Device]
jax._src.sharding_impls.NamedSharding.device_set(self)->set[Device]
jax._src.sharding_impls.NamedSharding.is_compatible_aval(self,aval_shape:Shape)
jax._src.sharding_impls.NamedSharding.is_fully_addressable(self)->bool
jax._src.sharding_impls.NamedSharding.is_fully_replicated(self)->bool
jax._src.sharding_impls.NamedSharding.with_memory_kind(self,kind:str)->NamedSharding
jax._src.sharding_impls.ParsedPartitionSpec(self,user_spec,partitions,sync=SpecSync.IN_SYNC)
jax._src.sharding_impls.ParsedPartitionSpec.__eq__(self,other)
jax._src.sharding_impls.ParsedPartitionSpec.__getitem__(self,i)
jax._src.sharding_impls.ParsedPartitionSpec.__hash__(self)
jax._src.sharding_impls.ParsedPartitionSpec.__init__(self,user_spec,partitions,sync=SpecSync.IN_SYNC)
jax._src.sharding_impls.ParsedPartitionSpec.__iter__(self)
jax._src.sharding_impls.ParsedPartitionSpec.__len__(self)
jax._src.sharding_impls.ParsedPartitionSpec.__repr__(self)
jax._src.sharding_impls.ParsedPartitionSpec.from_user_input(cls,entry,arg_name,allow_unconstrained_dims=False)
jax._src.sharding_impls.ParsedPartitionSpec.get_partition_spec(self)->PartitionSpec
jax._src.sharding_impls.ParsedPartitionSpec.insert_axis_partitions(self,dim,val)
jax._src.sharding_impls.ParsedPartitionSpec.unsynced_user_spec(self,min_sync)
jax._src.sharding_impls.ParsedPartitionSpec.user_spec(self)
jax._src.sharding_impls.PmapSharding(self,devices:Sequence[Device]|np.ndarray,sharding_spec:sharding_specs.ShardingSpec)
jax._src.sharding_impls.PmapSharding.__eq__(self,other)
jax._src.sharding_impls.PmapSharding.__hash__(self)
jax._src.sharding_impls.PmapSharding.__init__(self,devices:Sequence[Device]|np.ndarray,sharding_spec:sharding_specs.ShardingSpec)
jax._src.sharding_impls.PmapSharding.__reduce__(self)
jax._src.sharding_impls.PmapSharding.__repr__(self)
jax._src.sharding_impls.PmapSharding.__str__(self)
jax._src.sharding_impls.PmapSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.PmapSharding._to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.PmapSharding.default(cls,shape:Shape,sharded_dim:int=0,devices:Sequence[xc.Device]|None=None)->PmapSharding
jax._src.sharding_impls.PmapSharding.device_set(self)->set[Device]
jax._src.sharding_impls.PmapSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.PmapSharding.is_equivalent_to(self:PmapSharding,other:PmapSharding,ndim:int)->bool
jax._src.sharding_impls.PmapSharding.is_fully_addressable(self)->bool
jax._src.sharding_impls.PmapSharding.is_fully_replicated(self)->bool
jax._src.sharding_impls.PmapSharding.memory_kind(self)->str | None
jax._src.sharding_impls.PmapSharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding_impls.PmapSharding.with_memory_kind(self,kind:str)
jax._src.sharding_impls.PositionalSharding(self,devices:Sequence[xc.Device]|np.ndarray,*,memory_kind:str|None=None)
jax._src.sharding_impls.PositionalSharding.__eq__(self,other)->bool
jax._src.sharding_impls.PositionalSharding.__hash__(self)->int
jax._src.sharding_impls.PositionalSharding.__init__(self,devices:Sequence[xc.Device]|np.ndarray,*,memory_kind:str|None=None)
jax._src.sharding_impls.PositionalSharding.__repr__(self)->str
jax._src.sharding_impls.PositionalSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.PositionalSharding._remake(cls,devices:tuple[xc.Device,...],ids:np.ndarray,*,memory_kind:str|None=None)->PositionalSharding
jax._src.sharding_impls.PositionalSharding._to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.PositionalSharding.device_set(self)->set[xc.Device]
jax._src.sharding_impls.PositionalSharding.is_fully_addressable(self)->bool
jax._src.sharding_impls.PositionalSharding.is_fully_replicated(self)->bool
jax._src.sharding_impls.PositionalSharding.memory_kind(self)->str | None
jax._src.sharding_impls.PositionalSharding.ndim(self)
jax._src.sharding_impls.PositionalSharding.replicate(self,axis=None,keepdims=True)->PositionalSharding
jax._src.sharding_impls.PositionalSharding.reshape(self,*shape)->PositionalSharding
jax._src.sharding_impls.PositionalSharding.shape(self)
jax._src.sharding_impls.PositionalSharding.transpose(self,*axes)->PositionalSharding
jax._src.sharding_impls.PositionalSharding.with_memory_kind(self,kind:str)->PositionalSharding
jax._src.sharding_impls.ReplicaAxisContext
jax._src.sharding_impls.SPMDAxisContext
jax._src.sharding_impls.SPMDAxisContext.axis_env(self)
jax._src.sharding_impls.SPMDAxisContext.extend_manual(self,axes:frozenset[MeshAxisName])->SPMDAxisContext
jax._src.sharding_impls.SPMDAxisContext.unsafe_axis_env(self)
jax._src.sharding_impls.ShardingContext
jax._src.sharding_impls.ShardingContext.axis_env(self)
jax._src.sharding_impls.SingleDeviceSharding(self,device:Device,*,memory_kind:str|None=None)
jax._src.sharding_impls.SingleDeviceSharding.__eq__(self,other)
jax._src.sharding_impls.SingleDeviceSharding.__hash__(self)
jax._src.sharding_impls.SingleDeviceSharding.__init__(self,device:Device,*,memory_kind:str|None=None)
jax._src.sharding_impls.SingleDeviceSharding.__reduce__(self)
jax._src.sharding_impls.SingleDeviceSharding.__repr__(self)
jax._src.sharding_impls.SingleDeviceSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.SingleDeviceSharding._to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.SingleDeviceSharding.device_set(self)->set[Device]
jax._src.sharding_impls.SingleDeviceSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.SingleDeviceSharding.is_fully_addressable(self)->bool
jax._src.sharding_impls.SingleDeviceSharding.is_fully_replicated(self)->bool
jax._src.sharding_impls.SingleDeviceSharding.memory_kind(self)->str | None
jax._src.sharding_impls.SingleDeviceSharding.with_memory_kind(self,kind:str)->SingleDeviceSharding
jax._src.sharding_impls.SpecSync(enum.IntEnum)
jax._src.sharding_impls.TransferToMemoryKind
jax._src.sharding_impls.UnspecifiedValue
jax._src.sharding_impls.UnspecifiedValue.__repr__(self)
jax._src.sharding_impls.XLACompatibleSharding(sharding.Sharding)
jax._src.sharding_impls.XLACompatibleSharding._addressable_device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.XLACompatibleSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding_impls.XLACompatibleSharding._to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.XLACompatibleSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.XLACompatibleSharding.is_equivalent_to(self:XLACompatibleSharding,other:XLACompatibleSharding,ndim:int)->bool
jax._src.sharding_impls.XLACompatibleSharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding_impls._check_mesh_resource_axis(mesh,parsed_pspec)
jax._src.sharding_impls._check_unique_resources(axis_resources,arg_name)
jax._src.sharding_impls._common_shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding_impls._op_sharding_to_pos_sharding(op_sharding:Union[xc.OpSharding,xc.HloSharding],device_assignment:Sequence[xc.Device],memory_kind:Optional[str]=None)->PositionalSharding
jax._src.sharding_impls._positional_sharding_to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.array_mapping_to_axis_resources(array_mapping:ArrayMapping)
jax._src.sharding_impls.check_all_or_none_unspecified(axis_resources,name)
jax._src.sharding_impls.common_devices_indices_map(s,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.device_replica_id_map(sharding,global_shape:Shape)->Mapping[Device, int]
jax._src.sharding_impls.explode_superdims(sizes,dims)
jax._src.sharding_impls.get_array_mapping(axis_resources:ParsedPartitionSpec|AUTO|UnspecifiedValue)->ArrayMappingOrAutoOrUnspecified
jax._src.sharding_impls.get_replicated_hlo_sharding()
jax._src.sharding_impls.gspmd_sharding_devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.hashed_index(x)->int
jax._src.sharding_impls.is_auto(x)
jax._src.sharding_impls.is_unspecified(x)
jax._src.sharding_impls.is_unspecified_or_auto(x)
jax._src.sharding_impls.named_sharding_to_xla_hlo_sharding(self,num_dimensions:int)->xc.HloSharding
jax._src.sharding_impls.parse_flatten_op_sharding(hlo_sharding:xc.OpSharding|xc.HloSharding,mesh:mesh_lib.Mesh)->Sequence[ParsedPartitionSpec]
jax._src.sharding_impls.pmap_sharding_devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding_impls.prepare_axis_resources(axis_resources,arg_name,allow_unconstrained_dims=False)
jax._src.sharding_impls.strides_for_sizes(sizes)
jax._src.sharding_impls.unflatten_array(named_sizes,assignment)
jax._src.sharding_impls.unflatten_superdims(assignment)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/typing.py----------------------------------------
jax._src.typing.DuckTypedArray(Protocol)
jax._src.typing.DuckTypedArray.dtype(self)->DType
jax._src.typing.DuckTypedArray.shape(self)->Shape
jax._src.typing.SupportsDType(Protocol)
jax._src.typing.SupportsDType.dtype(self)->DType


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/image/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/image/scale.py----------------------------------------
A:jax._src.image.scale.out->jax.numpy.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)
A:jax._src.image.scale.dtype->jax.numpy.result_type(scale, translation)
A:jax._src.image.scale.weights->jax.numpy.where(jnp.abs(total_weight_sum) > 1000.0 * float(np.finfo(np.float32).eps), jnp.divide(weights, jnp.where(total_weight_sum != 0, total_weight_sum, 1)), 0)
A:jax._src.image.scale.total_weight_sum->jax.numpy.sum(weights, axis=0, keepdims=True)
A:jax._src.image.scale.in_indices->list(range(len(output_shape)))
A:jax._src.image.scale.out_indices->list(range(len(output_shape)))
A:jax._src.image.scale.d->canonicalize_axis(d, x.ndim)
A:jax._src.image.scale.w->compute_weight_mat(m, n, scale[i], translation[i], kernel, antialias).astype(x.dtype)
A:jax._src.image.scale.shape->jax._src.core.canonicalize_shape(shape)
A:jax._src.image.scale.method->ResizeMethod.from_string(method)
A:jax._src.image.scale.(image,)->promote_dtypes_inexact(image)
A:jax._src.image.scale.(scale, translation)->promote_dtypes_inexact(scale, translation)
A:jax._src.image.scale.spatial_dims->tuple((i for i in range(len(shape)) if not core.definitely_equal(image.shape[i], shape[i])))
A:jax._src.image.scale.offsets->jax.numpy.floor(offsets.astype(np.float32)).astype(np.int32)
jax._src.image.scale.ResizeMethod(enum.Enum)
jax._src.image.scale.ResizeMethod.from_string(s:str)
jax._src.image.scale._fill_keys_cubic_kernel(x)
jax._src.image.scale._fill_lanczos_kernel(radius,x)
jax._src.image.scale._fill_triangle_kernel(x)
jax._src.image.scale._resize(image,shape:core.Shape,method:Union[str,ResizeMethod],antialias:bool,precision)
jax._src.image.scale._resize_nearest(x,output_shape:core.Shape)
jax._src.image.scale._scale_and_translate(x,output_shape:core.Shape,spatial_dims:Sequence[int],scale,translation,kernel,antialias:bool,precision)
jax._src.image.scale.compute_weight_mat(input_size:core.DimSize,output_size:core.DimSize,scale,translation,kernel:Callable,antialias:bool)
jax._src.image.scale.resize(image,shape:core.Shape,method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)
jax._src.image.scale.scale_and_translate(image,shape:core.Shape,spatial_dims:Sequence[int],scale,translation,method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/clusters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/clusters/slurm_cluster.py----------------------------------------
A:jax._src.clusters.slurm_cluster.ind->next((i for (i, ch) in enumerate(node_list) if ch in delims), len(node_list))
A:jax._src.clusters.slurm_cluster.ind2->next((i for (i, ch) in enumerate(suffix) if ch in delims2), None)
jax._src.clusters.SlurmCluster(clusters.ClusterEnv)
jax._src.clusters.SlurmCluster.get_coordinator_address(cls)->str
jax._src.clusters.SlurmCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.SlurmCluster.get_process_count(cls)->int
jax._src.clusters.SlurmCluster.get_process_id(cls)->int
jax._src.clusters.SlurmCluster.is_env_present(cls)->bool
jax._src.clusters.slurm_cluster.SlurmCluster(clusters.ClusterEnv)
jax._src.clusters.slurm_cluster.SlurmCluster.get_coordinator_address(cls)->str
jax._src.clusters.slurm_cluster.SlurmCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.slurm_cluster.SlurmCluster.get_process_count(cls)->int
jax._src.clusters.slurm_cluster.SlurmCluster.get_process_id(cls)->int
jax._src.clusters.slurm_cluster.SlurmCluster.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/clusters/cluster.py----------------------------------------
A:jax._src.clusters.cluster.logger->logging.getLogger(__name__)
A:jax._src.clusters.cluster.env->next((env for env in cls._cluster_types if env.is_env_present()), None)
A:jax._src.clusters.cluster.coordinator_address->next((env for env in cls._cluster_types if env.is_env_present()), None).get_coordinator_address()
A:jax._src.clusters.cluster.num_processes->next((env for env in cls._cluster_types if env.is_env_present()), None).get_process_count()
A:jax._src.clusters.cluster.process_id->next((env for env in cls._cluster_types if env.is_env_present()), None).get_process_id()
jax._src.clusters.ClusterEnv
jax._src.clusters.ClusterEnv.__init_subclass__(cls,**kwargs)
jax._src.clusters.ClusterEnv.auto_detect_unset_distributed_params(cls,coordinator_address:Optional[str],num_processes:Optional[int],process_id:Optional[int],local_device_ids:Optional[Sequence[int]])->tuple[Optional[str], Optional[int], Optional[int], Optional[Sequence[int]]]
jax._src.clusters.ClusterEnv.get_coordinator_address(cls)->str
jax._src.clusters.ClusterEnv.get_local_process_id(cls)->Optional[int]
jax._src.clusters.ClusterEnv.get_process_count(cls)->int
jax._src.clusters.ClusterEnv.get_process_id(cls)->int
jax._src.clusters.ClusterEnv.is_env_present(cls)->bool
jax._src.clusters.cluster.ClusterEnv
jax._src.clusters.cluster.ClusterEnv.__init_subclass__(cls,**kwargs)
jax._src.clusters.cluster.ClusterEnv.auto_detect_unset_distributed_params(cls,coordinator_address:Optional[str],num_processes:Optional[int],process_id:Optional[int],local_device_ids:Optional[Sequence[int]])->tuple[Optional[str], Optional[int], Optional[int], Optional[Sequence[int]]]
jax._src.clusters.cluster.ClusterEnv.get_coordinator_address(cls)->str
jax._src.clusters.cluster.ClusterEnv.get_local_process_id(cls)->Optional[int]
jax._src.clusters.cluster.ClusterEnv.get_process_count(cls)->int
jax._src.clusters.cluster.ClusterEnv.get_process_id(cls)->int
jax._src.clusters.cluster.ClusterEnv.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/clusters/cloud_tpu_cluster.py----------------------------------------
A:jax._src.clusters.cloud_tpu_cluster.api_resp->requests.get(f'{gce_metadata_endpoint}/computeMetadata/v1/instance/attributes/{key}', headers={'Metadata-Flavor': 'Google'})
jax._src.clusters.TpuCluster(clusters.ClusterEnv)
jax._src.clusters.TpuCluster._get_worker_endpoints()->str
jax._src.clusters.TpuCluster.get_coordinator_address(cls)->str
jax._src.clusters.TpuCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.TpuCluster.get_process_count(cls)->int
jax._src.clusters.TpuCluster.get_process_id(cls)->int
jax._src.clusters.TpuCluster.is_env_present(cls)->bool
jax._src.clusters.cloud_tpu_cluster.TpuCluster(clusters.ClusterEnv)
jax._src.clusters.cloud_tpu_cluster.TpuCluster._get_worker_endpoints()->str
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_coordinator_address(cls)->str
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_process_count(cls)->int
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_process_id(cls)->int
jax._src.clusters.cloud_tpu_cluster.TpuCluster.is_env_present(cls)->bool
jax._src.clusters.cloud_tpu_cluster.get_metadata(key)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/clusters/ompi_cluster.py----------------------------------------
A:jax._src.clusters.ompi_cluster.launcher_ip_match->re.search('tcp://(.+?)[,:]|tcp6://\\[(.+?)[,\\]]', orte_uri)
A:jax._src.clusters.ompi_cluster.launcher_ip->next((i for i in launcher_ip_match.groups() if i is not None))
jax._src.clusters.OmpiCluster(clusters.ClusterEnv)
jax._src.clusters.OmpiCluster.get_coordinator_address(cls)->str
jax._src.clusters.OmpiCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.OmpiCluster.get_process_count(cls)->int
jax._src.clusters.OmpiCluster.get_process_id(cls)->int
jax._src.clusters.OmpiCluster.is_env_present(cls)->bool
jax._src.clusters.ompi_cluster.OmpiCluster(clusters.ClusterEnv)
jax._src.clusters.ompi_cluster.OmpiCluster.get_coordinator_address(cls)->str
jax._src.clusters.ompi_cluster.OmpiCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.ompi_cluster.OmpiCluster.get_process_count(cls)->int
jax._src.clusters.ompi_cluster.OmpiCluster.get_process_id(cls)->int
jax._src.clusters.ompi_cluster.OmpiCluster.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/fft.py----------------------------------------
A:jax._src.scipy.fft.(N_arr, k)->promote_dtypes_complex(N, k)
A:jax._src.scipy.fft.v0->jax.lax.slice_in_dim(x, None, None, 2, axis)
A:jax._src.scipy.fft.v1->jax.lax.rev(x[ix1], (axis,))
A:jax._src.scipy.fft.factor->jax.lax.expand_dims(factor, [a for a in range(out.ndim) if a != axis])
A:jax._src.scipy.fft.axis->canonicalize_axis(axis, x.ndim)
A:jax._src.scipy.fft.x->idct(x, axis=axis, norm=norm)
A:jax._src.scipy.fft.v->_dct_interleave(_dct_interleave(x, axis1), axis2)
A:jax._src.scipy.fft.V->jax.numpy.fft.fftn(v, axes=axes)
A:jax._src.scipy.fft.k->jax.lax.expand_dims(jnp.arange(N, dtype=jnp.float32), [a for a in range(x.ndim) if a != axis])
A:jax._src.scipy.fft.out->out.at[odds].set(v1).at[odds].set(v1)
A:jax._src.scipy.fft.(axis1, axis2)->map(partial(canonicalize_axis, num_dims=x.ndim), axes)
A:jax._src.scipy.fft.k1->jax.lax.expand_dims(jnp.arange(N1, dtype=V.dtype), [a for a in range(x.ndim) if a != axis1])
A:jax._src.scipy.fft.k2->jax.lax.expand_dims(jnp.arange(N2, dtype=V.dtype), [a for a in range(x.ndim) if a != axis2])
A:jax._src.scipy.fft.axes->range(x.ndim)
A:jax._src.scipy.fft.w4->_W4(N, k)
A:jax._src.scipy.fft.empty_slice->slice(None, None, None)
A:jax._src.scipy.fft.ix0->tuple((slice(None, math.ceil(x.shape[axis] / 2), 1) if i == axis else empty_slice for i in range(len(x.shape))))
A:jax._src.scipy.fft.ix1->tuple((slice(math.ceil(x.shape[axis] / 2), None, 1) if i == axis else empty_slice for i in range(len(x.shape))))
A:jax._src.scipy.fft.evens->tuple((slice(None, None, 2) if i == axis else empty_slice for i in range(len(x.shape))))
A:jax._src.scipy.fft.odds->tuple((slice(1, None, 2) if i == axis else empty_slice for i in range(len(x.shape))))
jax._src.scipy.fft._W4(N:int,k:Array)->Array
jax._src.scipy.fft._dct2(x:Array,axes:Sequence[int],norm:Optional[str])->Array
jax._src.scipy.fft._dct_deinterleave(x:Array,axis:int)->Array
jax._src.scipy.fft._dct_interleave(x:Array,axis:int)->Array
jax._src.scipy.fft._dct_ortho_norm(out:Array,axis:int)->Array
jax._src.scipy.fft.dct(x:Array,type:int=2,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.scipy.fft.dctn(x:Array,type:int=2,s:Optional[Sequence[int]]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.scipy.fft.idct(x:Array,type:int=2,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.scipy.fft.idctn(x:Array,type:int=2,s:Optional[Sequence[int]]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/integrate.py----------------------------------------
A:jax._src.scipy.integrate.(y_arr,)->jax._src.numpy.util.promote_dtypes_inexact(y)
A:jax._src.scipy.integrate.dx_array->jax.numpy.moveaxis(jnp.diff(x_arr, axis=axis), axis, -1)
A:jax._src.scipy.integrate.(y_arr, x_arr)->jax._src.numpy.util.promote_dtypes_inexact(y, x)
A:jax._src.scipy.integrate.y_arr->jax.numpy.moveaxis(y_arr, axis, -1)
jax._src.scipy.integrate.trapezoid(y:ArrayLike,x:ArrayLike|None=None,dx:ArrayLike=1.0,axis:int=-1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/ndimage.py----------------------------------------
A:jax._src.scipy.ndimage.index->jax.numpy.floor(coordinate).astype(jnp.int32)
A:jax._src.scipy.ndimage.weight->coordinate.dtype.type(1)
A:jax._src.scipy.ndimage.lower->jax.numpy.floor(coordinate)
A:jax._src.scipy.ndimage.input_arr->jax.numpy.asarray(input)
A:jax._src.scipy.ndimage.cval->jax.numpy.asarray(cval, input_arr.dtype)
A:jax._src.scipy.ndimage.index_fixer->_INDEX_FIXERS.get(mode)
A:jax._src.scipy.ndimage.interp_nodes->interp_fun(coordinate)
A:jax._src.scipy.ndimage.fixed_index->index_fixer(index, size)
A:jax._src.scipy.ndimage.valid->is_valid(index, size)
A:jax._src.scipy.ndimage.(indices, validities, weights)->jax._src.util.unzip3(items)
A:jax._src.scipy.ndimage.all_valid->functools.reduce(operator.and_, validities)
A:jax._src.scipy.ndimage.contribution->jax.numpy.where(all_valid, input_arr[indices], cval)
A:jax._src.scipy.ndimage.result->_round_half_away_from_zero(result)
jax._src.scipy.ndimage._linear_indices_and_weights(coordinate:Array)->list[tuple[Array, ArrayLike]]
jax._src.scipy.ndimage._map_coordinates(input:ArrayLike,coordinates:Sequence[ArrayLike],order:int,mode:str,cval:ArrayLike)->Array
jax._src.scipy.ndimage._mirror_index_fixer(index:Array,size:int)->Array
jax._src.scipy.ndimage._nearest_indices_and_weights(coordinate:Array)->list[tuple[Array, ArrayLike]]
jax._src.scipy.ndimage._nonempty_prod(arrs:Sequence[Array])->Array
jax._src.scipy.ndimage._nonempty_sum(arrs:Sequence[Array])->Array
jax._src.scipy.ndimage._reflect_index_fixer(index:Array,size:int)->Array
jax._src.scipy.ndimage._round_half_away_from_zero(a:Array)->Array
jax._src.scipy.ndimage.map_coordinates(input:ArrayLike,coordinates:Sequence[ArrayLike],order:int,mode:str='constant',cval:ArrayLike=0.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/special.py----------------------------------------
A:jax._src.scipy.special.(x,)->promote_args_inexact('exp1', x)
A:jax._src.scipy.special.betaln->_wraps(osp_special.betaln, module='scipy.special', update_doc=False)(_betaln_impl)
A:jax._src.scipy.special.(a, b, x)->promote_args_inexact('betainc', a, b, x)
A:jax._src.scipy.special.(a, x)->promote_args_inexact('gammaincc', a, x)
A:jax._src.scipy.special.logsumexp->_wraps(osp_special.logsumexp, module='scipy.special')(ops_special.logsumexp)
A:jax._src.scipy.special.(x, y)->promote_args_inexact('xlog1py', x, y)
A:jax._src.scipy.special.result->jax.numpy.where(both_gt_zero_mask, log_val, jnp.where(one_zero_mask, q, jnp.inf))
A:jax._src.scipy.special.d->jax.lax.while_loop(cond, body, init)
A:jax._src.scipy.special.(a, d_)->promote_args_inexact('multigammaln', a, d)
A:jax._src.scipy.special.constant->jax.lax.mul(lax.mul(lax.mul(_lax_const(a, 0.25), d_), lax.sub(d_, _lax_const(a, 1))), lax.log(_lax_const(a, np.pi)))
A:jax._src.scipy.special.b->jax.lax.div(jnp.arange(d, dtype=d_.dtype), _lax_const(a, 2))
A:jax._src.scipy.special.res->jax.numpy.sum(gammaln(jnp.expand_dims(a, axis=-1) - jnp.expand_dims(b, axis=tuple(range(a.ndim)))), axis=-1)
A:jax._src.scipy.special.(p, q)->promote_args_inexact('rel_entr', p, q)
A:jax._src.scipy.special.zero->_c(x, 0)
A:jax._src.scipy.special.both_gt_zero_mask->jax.lax.bitwise_and(lax.gt(p, zero), lax.gt(q, zero))
A:jax._src.scipy.special.one_zero_mask->jax.lax.bitwise_and(lax.eq(p, zero), lax.ge(q, zero))
A:jax._src.scipy.special.safe_p->jax.numpy.where(both_gt_zero_mask, p, 1)
A:jax._src.scipy.special.safe_q->jax.numpy.where(both_gt_zero_mask, q, 1)
A:jax._src.scipy.special.log_val->jax.lax.sub(_xlogx(safe_p), xlogy(safe_p, safe_q))
A:jax._src.scipy.special.(x, q)->promote_args_inexact('zeta', x, q)
A:jax._src.scipy.special.(s, a)->promote_args_inexact('zeta', x, q)
A:jax._src.scipy.special.k->jax.numpy.arange(2, 50, dtype=bn.dtype)
A:jax._src.scipy.special.S->jax.numpy.sum((a_ + k) ** (-s_), -1)
A:jax._src.scipy.special.I->jax.lax.div((a + N) ** (dtype(1) - s), s - dtype(1))
A:jax._src.scipy.special.m->jax.numpy.arange(4, n + 1, 2, dtype=bn.dtype)
A:jax._src.scipy.special.T1->jax.numpy.clip(T1, a_max=jnp.finfo(dtype).max)
A:jax._src.scipy.special.coefs->numpy.expand_dims(np.array(_BERNOULLI_COEFS[:T1.shape[-1]], dtype=dtype), tuple(range(a.ndim)))
A:jax._src.scipy.special.(n_arr, x_arr)->promote_args_inexact('polygamma', n, x)
A:jax._src.scipy.special._LOGNDTR_FLOAT64_LOWER->numpy.array(-20, np.float64)
A:jax._src.scipy.special._LOGNDTR_FLOAT32_LOWER->numpy.array(-10, np.float32)
A:jax._src.scipy.special._LOGNDTR_FLOAT64_UPPER->numpy.array(8, np.float64)
A:jax._src.scipy.special._LOGNDTR_FLOAT32_UPPER->numpy.array(5, np.float32)
A:jax._src.scipy.special.x->jax.numpy.asarray(x)
A:jax._src.scipy.special.dtype->jax.lax.dtype(x)
A:jax._src.scipy.special.z->jax.numpy.asarray(z)
A:jax._src.scipy.special.y->jax.numpy.where(x_5_bool, y_flag_one, y)
A:jax._src.scipy.special.p0->list(reversed([-59.96335010141079, 98.00107541859997, -56.67628574690703, 13.931260938727968, -1.2391658386738125]))
A:jax._src.scipy.special.q0->list(reversed([1.0, 1.9544885833814176, 4.676279128988815, 86.36024213908905, -225.46268785411937, 200.26021238006066, -82.03722561683334, 15.90562251262117, -1.1833162112133]))
A:jax._src.scipy.special.p1->list(reversed([4.0554489230596245, 31.525109459989388, 57.16281922464213, 44.08050738932008, 14.684956192885803, 2.1866330685079025, -0.1402560791713545, -0.03504246268278482, -0.0008574567851546854]))
A:jax._src.scipy.special.q1->list(reversed([1.0, 15.779988325646675, 45.39076351288792, 41.3172038254672, 15.04253856929075, 2.504649462083094, -0.14218292285478779, -0.03808064076915783, -0.0009332594808954574]))
A:jax._src.scipy.special.p2->list(reversed([3.2377489177694603, 6.915228890689842, 3.9388102529247444, 1.3330346081580755, 0.20148538954917908, 0.012371663481782003, 0.00030158155350823543, 2.6580697468673755e-06, 6.239745391849833e-09]))
A:jax._src.scipy.special.q2->jax.numpy.sum(k[:, None] ** (-m[None, :]), axis=0)
A:jax._src.scipy.special.shape->jax.numpy.shape(p)
A:jax._src.scipy.special.coeffs->numpy.array(coeffs, dtype)
A:jax._src.scipy.special.maybe_complement_p->jax.numpy.where(p > dtype(-np.expm1(-2.0)), dtype(1.0) - p, p)
A:jax._src.scipy.special.sanitized_mcp->jax.numpy.where(maybe_complement_p <= dtype(0.0), jnp.full(shape, dtype(0.5)), maybe_complement_p)
A:jax._src.scipy.special.ww->jax.lax.square(w)
A:jax._src.scipy.special.infinity->jax.numpy.full(shape, dtype(np.inf))
A:jax._src.scipy.special.x_nan_replaced->jax.numpy.where(p <= dtype(0.0), -infinity, jnp.where(p >= dtype(1.0), infinity, x))
A:jax._src.scipy.special.x_arr->jax.numpy.asarray(x)
A:jax._src.scipy.special.ans->log_ndtr(x, series_order=series_order)
A:jax._src.scipy.special.t_out->jax.lax.mul(t, lax.exp(lax.sub(_norm_logpdf(x), ans)))
A:jax._src.scipy.special.x_2->jax.lax.square(x)
A:jax._src.scipy.special.even_sum->jax.numpy.zeros_like(x)
A:jax._src.scipy.special.odd_sum->jax.numpy.zeros_like(x)
A:jax._src.scipy.special._norm_logpdf_constant->numpy.log(np.sqrt(2 * np.pi))
A:jax._src.scipy.special.neg_half->_lax_const(x, -0.5)
A:jax._src.scipy.special.log_normalizer->_lax_const(x, _norm_logpdf_constant)
A:jax._src.scipy.special.bs->_lax_const(z, 0.0)
A:jax._src.scipy.special.f0->_lax_const(z, 0.0)
A:jax._src.scipy.special.f1->_lax_const(z, 1e-16)
A:jax._src.scipy.special.f->_lax_const(z, 0.0)
A:jax._src.scipy.special.((_, _, bs, _), j_vals)->jax.lax.scan(f=_bessel_jn_scan_body_fun, init=(f0, f1, bs, z), xs=lax.iota(lax.dtype(z), n_iter + 1), reverse=True)
A:jax._src.scipy.special.(z,)->promote_dtypes_inexact(z)
A:jax._src.scipy.special.z_dtype->jax.lax.dtype(z)
A:jax._src.scipy.special.v->jax._src.core.concrete_or_error(operator.index, v, 'Argument v of bessel_jn.')
A:jax._src.scipy.special.n_iter->jax._src.core.concrete_or_error(int, n_iter, 'Argument n_iter of bessel_jn.')
A:jax._src.scipy.special.bessel_jn_fun->vmap(bessel_jn_fun)
A:jax._src.scipy.special.(m_mat, l_mat)->jax.numpy.meshgrid(jnp.arange(num_m, dtype=x.dtype), jnp.arange(num_l, dtype=x.dtype), indexing='ij')
A:jax._src.scipy.special.d0->jax.numpy.sqrt((4.0 * c0 - 1.0) / (c0 - c1))
A:jax._src.scipy.special.d1->jax.numpy.sqrt((c2 + 1.0) * (c3 - c1) / ((c2 - 3.0) * (c0 - c1)))
A:jax._src.scipy.special.d0_mask_indices->jax.numpy.triu_indices(l_max + 1, 1)
A:jax._src.scipy.special.d1_mask_indices->jax.numpy.triu_indices(l_max + 1, 2)
A:jax._src.scipy.special.d_zeros->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype)
A:jax._src.scipy.special.d0_mask->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype).at[d0_mask_indices].set(d0[d0_mask_indices])
A:jax._src.scipy.special.d1_mask->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype).at[d1_mask_indices].set(d1[d1_mask_indices])
A:jax._src.scipy.special.mask->(i + j - k == 0).astype(dtype)
A:jax._src.scipy.special.d0_mask_3d->jax.numpy.einsum('jk,ijk->ijk', d0_mask, mask)
A:jax._src.scipy.special.d1_mask_3d->jax.numpy.einsum('jk,ijk->ijk', d1_mask, mask)
A:jax._src.scipy.special.l_vec->jax.numpy.arange(num_l, dtype=p.dtype)
A:jax._src.scipy.special.update_p_p1->jax.numpy.einsum('i,ij->ij', coeff, p_p1)
A:jax._src.scipy.special.p_mm2_lm1->p_mm2_lm1.at[0, 3:num_l, :].set(update_p_p2).at[0, 3:num_l, :].set(update_p_p2)
A:jax._src.scipy.special.update_p_p2->jax.numpy.einsum('i,ij->ij', coeff, p_p2)
A:jax._src.scipy.special.coeff_zeros->jax.numpy.zeros((num_m, num_l), dtype=x.dtype)
A:jax._src.scipy.special.upper_0_indices->jax.numpy.triu_indices(num_m, 0, num_l)
A:jax._src.scipy.special.zero_vec->jax.numpy.zeros((num_l,), dtype=x.dtype)
A:jax._src.scipy.special.a0_masked->a0_masked.at[1, :].set(zero_vec).at[1, :].set(zero_vec)
A:jax._src.scipy.special.c0_masked->c0_masked.at[1, :].set(zero_vec).at[1, :].set(zero_vec)
A:jax._src.scipy.special.d0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(d0[upper_0_indices])
A:jax._src.scipy.special.e0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(e0[upper_0_indices])
A:jax._src.scipy.special.f0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(f0[upper_0_indices])
A:jax._src.scipy.special.g0->jax.numpy.einsum('i,ij->ij', (l_vec + 1) * l_vec, p[0, :, :])
A:jax._src.scipy.special.p_derivative_m0->jax.numpy.einsum('j,ij->ij', 0.5 / jnp.sqrt(1 - x * x), g0)
A:jax._src.scipy.special.p_derivative->p_derivative.at[1, 0, :].set(0).at[1, 0, :].set(0)
A:jax._src.scipy.special.p->jax.lax.fori_loop(lower=2, upper=l_max + 1, body_fun=body_fun, init_val=p)
A:jax._src.scipy.special.a_idx->jax.numpy.arange(1, l_max + 1, dtype=x.dtype)
A:jax._src.scipy.special.b_idx->jax.numpy.arange(l_max, dtype=x.dtype)
A:jax._src.scipy.special.f_a->jax.numpy.cumprod(1.0 - 2.0 * a_idx)
A:jax._src.scipy.special.f_b->jax.numpy.sqrt(2.0 * b_idx + 3.0)
A:jax._src.scipy.special.diag_indices->jax.numpy.diag_indices(l_max + 1)
A:jax._src.scipy.special.p_offdiag->jax.numpy.einsum('ij,ij->ij', jnp.einsum('i,j->ij', f_b, x), p[jnp.diag_indices(l_max)])
A:jax._src.scipy.special.(d0_mask_3d, d1_mask_3d)->_gen_recurrence_mask(l_max, is_normalized=is_normalized, dtype=x.dtype)
A:jax._src.scipy.special.n->jax._src.core.concrete_or_error(operator.index, n, 'Argument n of bernoulli')
A:jax._src.scipy.special.p_vals->_gen_associated_legendre(l_max, z, is_normalized)
A:jax._src.scipy.special.p_derivatives->_gen_derivatives(p_vals, z, is_normalized)
A:jax._src.scipy.special.cos_colatitude->jax.numpy.cos(phi)
A:jax._src.scipy.special.legendre->_gen_associated_legendre(n_max, cos_colatitude, True)
A:jax._src.scipy.special.legendre_val->_gen_associated_legendre(n_max, cos_colatitude, True).at[abs(m), n, jnp.arange(len(n))].get(mode='clip')
A:jax._src.scipy.special.vandermonde->jax.lax.complex(jnp.cos(angle), jnp.sin(angle))
A:jax._src.scipy.special.harmonics->jax.numpy.where(m < 0, (-1.0) ** abs(m) * jnp.conjugate(harmonics), harmonics)
A:jax._src.scipy.special.phi->jax.numpy.array([phi])
A:jax._src.scipy.special.n_max->jax._src.core.concrete_or_error(int, n_max, 'The `n_max` argument of `jnp.scipy.special.sph_harm` must be statically specified to use `sph_harm` within JAX transformations.')
A:jax._src.scipy.special.A_arr->jax.numpy.array(A, dtype=x.dtype)
A:jax._src.scipy.special.B_arr->jax.numpy.array(B, dtype=x.dtype)
A:jax._src.scipy.special.one->_c(x, 1)
A:jax._src.scipy.special.(x_arr,)->promote_args_inexact('expi', x)
A:jax._src.scipy.special.psi->jax.lax.fori_loop(_c(n, 1), n, lambda i, psi: psi + one / i, psi)
A:jax._src.scipy.special.n1->jax.numpy.where(n == _c(n, 1), n + n, n)
A:jax._src.scipy.special.init->dict(k=_c(n, 1), pkm2=one, qkm2=x, pkm1=one, qkm1=x + n, ans=one / (x + n), t=_c(x, jnp.inf), r=zero, x=x)
A:jax._src.scipy.special.d['t']->jax.numpy.where(nz, abs((d['ans'] - r) / r), one)
A:jax._src.scipy.special.BIG->_c(x, 1.4411518807585587e+17)
A:jax._src.scipy.special.yk->jax.numpy.where(odd, one, x)
A:jax._src.scipy.special.xk->jax.numpy.where(odd, n + (k - _c(k, 1)) / _c(k, 2), k / _c(k, 2))
A:jax._src.scipy.special.d['r']r->jax.numpy.where(nz, pk / qk, d['r'])
A:jax._src.scipy.special.d['ans']->jax.numpy.where(nz, r, d['ans'])
A:jax._src.scipy.special.d[key]->jax.numpy.where(is_big, d[key] / BIG, d[key])
A:jax._src.scipy.special.(n, x)->promote_args_inexact('expn', n, x)
A:jax._src.scipy.special.ret->jax.numpy.piecewise(x, conds, vals)
A:jax._src.scipy.special.A->jax.numpy.array([4.6512858607399003e-05, 0.007315890452380947, 0.13384763957830903, 0.8796913117545303, 2.7114985119655346, 4.256971560081218, 3.297713409852251, 1.0], dtype=w.dtype)
A:jax._src.scipy.special.B->jax.numpy.array([0.0006909904889125533, 0.02540437639325444, 0.2829748606025681, 1.4117259775183106, 3.6380053334513707, 5.03278880143317, 3.547713409852251, 1.0], dtype=w.dtype)
A:jax._src.scipy.special.w->jax.numpy.piecewise(x, [x1_5_bool, x_5_bool], [lambda x: 1.0 / x - 1.0, lambda x: -x, lambda x: x - 1.0])
A:jax._src.scipy.special.b3->jax.numpy.array([1, -1 / 2, 1 / 6])
A:jax._src.scipy.special.bn->jax.numpy.zeros(n + 1).at[:3].set(b3)
jax._src.scipy.special._bessel_jn(z:ArrayLike,*,v:int,n_iter:int=50)->Array
jax._src.scipy.special._bessel_jn_scan_body_fun(carry,k)
jax._src.scipy.special._double_factorial(n:int)->np.ndarray
jax._src.scipy.special._eval_expint_k(A:list[float],B:list[float],x:Array)->Array
jax._src.scipy.special._expi_neg(x:Array)->Array
jax._src.scipy.special._expi_pos(x:Array)->Array
jax._src.scipy.special._expint1(x:Array)->Array
jax._src.scipy.special._expint2(x:Array)->Array
jax._src.scipy.special._expint3(x:Array)->Array
jax._src.scipy.special._expint4(x:Array)->Array
jax._src.scipy.special._expint5(x)
jax._src.scipy.special._expint6(x)
jax._src.scipy.special._expint7(x)
jax._src.scipy.special._expn1(n:int,x_in:ArrayLike)->Array
jax._src.scipy.special._expn2(n:int,x:Array)->Array
jax._src.scipy.special._expn3(n:int,x:Array)->Array
jax._src.scipy.special._gen_associated_legendre(l_max:int,x:Array,is_normalized:bool)->Array
jax._src.scipy.special._gen_derivatives(p:Array,x:Array,is_normalized:bool)->Array
jax._src.scipy.special._gen_recurrence_mask(l_max:int,is_normalized:bool,dtype:Any)->tuple[Array, Array]
jax._src.scipy.special._log_ndtr_asymptotic_series(x,series_order)
jax._src.scipy.special._log_ndtr_jvp(series_order,primals,tangents)
jax._src.scipy.special._log_ndtr_lower(x,series_order)
jax._src.scipy.special._ndtr(x:ArrayLike)->Array
jax._src.scipy.special._ndtri(p:ArrayLike)->Array
jax._src.scipy.special._norm_logpdf(x)
jax._src.scipy.special._spence(x:Array)->Array
jax._src.scipy.special._spence_calc(x:Array)->Array
jax._src.scipy.special._spence_poly(w:Array)->Array
jax._src.scipy.special._sph_harm(m:Array,n:Array,theta:Array,phi:Array,n_max:int)->Array
jax._src.scipy.special._xlog1py_jvp(primals,tangents)
jax._src.scipy.special._xlogx(x)
jax._src.scipy.special._xlogx_jvp(primals,tangents)
jax._src.scipy.special._xlogy_jvp(primals,tangents)
jax._src.scipy.special._zeta_series_expansion(x:ArrayLike,q:Optional[ArrayLike]=None)->Array
jax._src.scipy.special.bernoulli(n:int)->Array
jax._src.scipy.special.bessel_jn(z:ArrayLike,*,v:int,n_iter:int=50)->Array
jax._src.scipy.special.betainc(a:ArrayLike,b:ArrayLike,x:ArrayLike)->Array
jax._src.scipy.special.digamma(x:ArrayLike)->Array
jax._src.scipy.special.entr(x:ArrayLike)->Array
jax._src.scipy.special.erf(x:ArrayLike)->Array
jax._src.scipy.special.erfc(x:ArrayLike)->Array
jax._src.scipy.special.erfinv(x:ArrayLike)->Array
jax._src.scipy.special.exp1(x:ArrayLike,module='scipy.special')->Array
jax._src.scipy.special.expi(x:ArrayLike)->Array
jax._src.scipy.special.expi_jvp(primals,tangents)
jax._src.scipy.special.expit(x:ArrayLike)->Array
jax._src.scipy.special.expn(n:ArrayLike,x:ArrayLike)->Array
jax._src.scipy.special.expn_jvp(n,primals,tangents)
jax._src.scipy.special.gamma(x:ArrayLike)->Array
jax._src.scipy.special.gammainc(a:ArrayLike,x:ArrayLike)->Array
jax._src.scipy.special.gammaincc(a:ArrayLike,x:ArrayLike)->Array
jax._src.scipy.special.gammaln(x:ArrayLike)->Array
jax._src.scipy.special.i0(x:ArrayLike)->Array
jax._src.scipy.special.i0e(x:ArrayLike)->Array
jax._src.scipy.special.i1(x:ArrayLike)->Array
jax._src.scipy.special.i1e(x:ArrayLike)->Array
jax._src.scipy.special.kl_div(p:ArrayLike,q:ArrayLike)->Array
jax._src.scipy.special.log_ndtr(x:ArrayLike,series_order:int=3)->Array
jax._src.scipy.special.logit(x:ArrayLike)->Array
jax._src.scipy.special.lpmn(m:int,n:int,z:Array)->tuple[Array, Array]
jax._src.scipy.special.lpmn_values(m:int,n:int,z:Array,is_normalized:bool)->Array
jax._src.scipy.special.multigammaln(a:ArrayLike,d:ArrayLike)->Array
jax._src.scipy.special.ndtr(x:ArrayLike)->Array
jax._src.scipy.special.ndtri(p:ArrayLike)->Array
jax._src.scipy.special.polygamma(n:ArrayLike,x:ArrayLike)->Array
jax._src.scipy.special.rel_entr(p:ArrayLike,q:ArrayLike)->Array
jax._src.scipy.special.spence(x:Array)->Array
jax._src.scipy.special.sph_harm(m:Array,n:Array,theta:Array,phi:Array,n_max:Optional[int]=None)->Array
jax._src.scipy.special.xlog1py(x:ArrayLike,y:ArrayLike)->Array
jax._src.scipy.special.xlogy(x:ArrayLike,y:ArrayLike)->Array
jax._src.scipy.special.zeta(x:ArrayLike,q:Optional[ArrayLike]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/linalg.py----------------------------------------
A:jax._src.scipy.linalg._no_chkfinite_doc->textwrap.dedent('\nDoes not support the Scipy argument ``check_finite=True``,\nbecause compiled JAX code cannot perform checks of array values at runtime.\n')
A:jax._src.scipy.linalg.(a,)->promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.scipy.linalg.l->jax._src.lax.linalg.cholesky(a if lower else jnp.conj(a.mT), symmetrize_input=False)
A:jax._src.scipy.linalg.(c, b)->promote_dtypes_inexact(jnp.asarray(c), jnp.asarray(b))
A:jax._src.scipy.linalg.b->jax._src.lax.linalg.triangular_solve(c, b, left_side=True, lower=lower, transpose_a=lower, conjugate_a=lower)
A:jax._src.scipy.linalg.(v, w)->jax._src.lax.linalg.eigh(a, lower=lower)
A:jax._src.scipy.linalg.a->jax.lax.pad(a, dtype.type(0), ((0, 0, 0), (acc.shape[-1], 0, 0)))
A:jax._src.scipy.linalg.(lu, pivots, _)->jax._src.lax.linalg.lu(a)
A:jax._src.scipy.linalg.perm->jax._src.lax.linalg.lu_pivots_to_permutation(pivots, m)
A:jax._src.scipy.linalg.(lu, _, permutation)->jax._src.lax.linalg.lu(a)
A:jax._src.scipy.linalg.dtype->jax.lax.dtype(acc)
A:jax._src.scipy.linalg.(m, n)->jax.numpy.shape(a)
A:jax._src.scipy.linalg.p->jax.numpy.real(jnp.array(permutation[None, :] == jnp.arange(m, dtype=permutation.dtype)[:, None], dtype=dtype))
A:jax._src.scipy.linalg.k->min(m, n)
A:jax._src.scipy.linalg.(q, r)->jax._src.lax.linalg.qr(a, full_matrices=full_matrices)
A:jax._src.scipy.linalg.(a, b)->promote_dtypes_inexact(jnp.asarray(a), jnp.asarray(b))
A:jax._src.scipy.linalg.factors->cho_factor(lax.stop_gradient(a), lower=lower)
A:jax._src.scipy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: lax_linalg._matvec_multiply(a, x), solve=lambda _, x: cho_solve(factors, x), symmetric=True)
A:jax._src.scipy.linalg.out->jax._src.lax.linalg.triangular_solve(a, b, left_side=True, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.scipy.linalg._expm_description->textwrap.dedent('\nIn addition to the original NumPy argument(s) listed below,\nalso supports the optional boolean argument ``upper_triangular``\nto specify whether the ``A`` matrix is upper triangular, and the optional\nargument ``max_squarings`` to specify the max number of squarings allowed\nin the scaling-and-squaring approximation method. Return nan if the actual\nnumber of squarings required is more than ``max_squarings``.\n\nThe number of required squarings = max(0, ceil(log2(norm(A)) - c)\nwhere norm() denotes the L1 norm, and\n\n- c=2.42 for float64 or complex128,\n- c=1.97 for float32 or complex64\n')
A:jax._src.scipy.linalg.(A,)->promote_dtypes_inexact(A)
A:jax._src.scipy.linalg.(P, Q, n_squarings)->_calc_P_Q(jnp.asarray(A))
A:jax._src.scipy.linalg.R->jax.lax.cond(n_squarings > max_squarings, _nan, _compute, (A, P, Q))
A:jax._src.scipy.linalg.A_L1->jax.numpy.linalg.norm(A, 1)
A:jax._src.scipy.linalg.n_squarings->jax.numpy.maximum(0, jnp.floor(jnp.log2(A_L1 / maxnorm)))
A:jax._src.scipy.linalg.conds->jax.numpy.array([0.4258730016922831, 1.880152677804762], dtype=A_L1.dtype)
A:jax._src.scipy.linalg.idx->jax.numpy.digitize(A_L1, conds)
A:jax._src.scipy.linalg.(U, V)->jax.lax.switch(idx, [_pade3, _pade5, _pade7], A)
A:jax._src.scipy.linalg.(res, _)->jax.lax.scan(_scan_f, R, jnp.arange(max_squarings, dtype=n_squarings.dtype))
A:jax._src.scipy.linalg.ident->jax.numpy.eye(M, N, dtype=A.dtype)
A:jax._src.scipy.linalg.A2->_precise_dot(A, A)
A:jax._src.scipy.linalg.U->jax.lax.fori_loop(0, n, j_loop, U)
A:jax._src.scipy.linalg.A4->_precise_dot(A2, A2)
A:jax._src.scipy.linalg.A6->_precise_dot(A4, A2)
A:jax._src.scipy.linalg.A8->_precise_dot(A6, A2)
A:jax._src.scipy.linalg._expm_frechet_description->textwrap.dedent("\nDoes not currently support the Scipy argument ``jax.numpy.asarray_chkfinite``,\nbecause `jax.numpy.asarray_chkfinite` does not exist at the moment. Does not\nsupport the ``method='blockEnlarge'`` argument.\n")
A:jax._src.scipy.linalg.A_arr->jax.numpy.asarray(A)
A:jax._src.scipy.linalg.E_arr->jax.numpy.asarray(E)
A:jax._src.scipy.linalg.bound_fun->partial(expm, upper_triangular=False, max_squarings=16)
A:jax._src.scipy.linalg.(expm_A, expm_frechet_AE)->jvp(bound_fun, (A_arr,), (E_arr,))
A:jax._src.scipy.linalg.arrs->cast(tuple[ArrayLike], promote_dtypes(*arrs))
A:jax._src.scipy.linalg.acc->jax.lax.concatenate([acc, a], dimension=0)
A:jax._src.scipy.linalg.zeros->jax.numpy.zeros(x.shape, dtype=jnp.int32)
A:jax._src.scipy.linalg.ones->jax.numpy.ones(x.shape, dtype=jnp.int32)
A:jax._src.scipy.linalg.count->jax.numpy.where(q <= pivmin, count + 1, count)
A:jax._src.scipy.linalg.q->jax.numpy.block([[jnp.ones(batch_dims + (1, 1), dtype=a_out.dtype), jnp.zeros(batch_dims + (1, n - 1), dtype=a_out.dtype)], [jnp.zeros(batch_dims + (n - 1, 1), dtype=a_out.dtype), q]])
A:jax._src.scipy.linalg.(q, count)->sturm_step(start + j, q, count)
A:jax._src.scipy.linalg.(i, q, count)->unrolled_steps((i, q, count))
A:jax._src.scipy.linalg.(_, _, count)->jax.lax.while_loop(cond, unrolled_steps, (i, q, count))
A:jax._src.scipy.linalg.alpha->jax.numpy.real(alpha)
A:jax._src.scipy.linalg.beta->jax.numpy.asarray(e)
A:jax._src.scipy.linalg.beta_sq->jax.numpy.square(beta)
A:jax._src.scipy.linalg.beta_abs->jax.numpy.abs(beta)
A:jax._src.scipy.linalg.off_diag_abs_row_sum->jax.numpy.concatenate([beta_abs[:1], beta_abs[:-1] + beta_abs[1:], beta_abs[-1:]], axis=0)
A:jax._src.scipy.linalg.lambda_est_max->jax.numpy.amax(alpha + off_diag_abs_row_sum)
A:jax._src.scipy.linalg.lambda_est_min->jax.numpy.amin(alpha - off_diag_abs_row_sum)
A:jax._src.scipy.linalg.t_norm->jax.numpy.maximum(jnp.abs(lambda_est_min), jnp.abs(lambda_est_max))
A:jax._src.scipy.linalg.finfo->numpy.finfo(alpha.dtype)
A:jax._src.scipy.linalg.one->numpy.ones([], dtype=alpha.dtype)
A:jax._src.scipy.linalg.safemin->numpy.maximum(one / finfo.max, (one + finfo.eps) * finfo.tiny)
A:jax._src.scipy.linalg.alpha0_perturbation->jax.numpy.broadcast_to(alpha0_perturbation, target_shape)
A:jax._src.scipy.linalg.abs_tol->jax.numpy.maximum(tol, abs_tol)
A:jax._src.scipy.linalg.target_counts->jax.numpy.arange(select_range[0], select_range[1] + 1, dtype=jnp.int32)
A:jax._src.scipy.linalg.target_shape->jax.numpy.shape(target_counts)
A:jax._src.scipy.linalg.lower->jax.numpy.where(counts <= target_counts, mid, lower)
A:jax._src.scipy.linalg.upper->jax.numpy.where(counts > target_counts, mid, upper)
A:jax._src.scipy.linalg.pivmin->jax.numpy.broadcast_to(pivmin, target_shape)
A:jax._src.scipy.linalg.counts->_sturm(alpha, beta_sq, pivmin, alpha0_perturbation, mid)
A:jax._src.scipy.linalg.(_, _, mid, _)->jax.lax.while_loop(cond, body, (0, lower, mid, upper))
A:jax._src.scipy.linalg.arr->arr.T.conj().T.conj()
A:jax._src.scipy.linalg.(unitary, posdef, _, _)->jax._src.lax.qdwh.qdwh(arr, is_hermitian=False, eps=eps)
A:jax._src.scipy.linalg.posdef->posdef.T.conj().T.conj()
A:jax._src.scipy.linalg.unitary->unitary.T.conj().T.conj()
A:jax._src.scipy.linalg.(u_svd, s_svd, vh_svd)->jax._src.lax.linalg.svd(arr, full_matrices=False)
A:jax._src.scipy.linalg.s_svd->s_svd.astype(u_svd.dtype).astype(u_svd.dtype)
A:jax._src.scipy.linalg.diag->jax.numpy.sqrt(jnp.diag(T))
A:jax._src.scipy.linalg.s->jax.lax.fori_loop(i + 1, j, lambda k, val: val + U[i, k] * U[k, j], 0.0)
A:jax._src.scipy.linalg.value->jax.numpy.where(T[i, j] == s, 0.0, (T[i, j] - s) / (diag[i] + diag[j]))
A:jax._src.scipy.linalg.(_, U)->jax.lax.fori_loop(0, j, i_loop, (j, U))
A:jax._src.scipy.linalg.(T, Z)->jax.lax.cond(jnp.abs(T[m, m - 1]) > eps * (jnp.abs(T[m - 1, m - 1]) + jnp.abs(T[m, m])), _update_T_Z, lambda m, T, Z: (T, Z), m, T, Z)
A:jax._src.scipy.linalg.sqrt_T->_sqrtm_triu(T)
A:jax._src.scipy.linalg.T_arr->jax.numpy.asarray(T)
A:jax._src.scipy.linalg.Z_arr->jax.numpy.asarray(Z)
A:jax._src.scipy.linalg.(T_arr, Z_arr)->promote_dtypes_complex(T_arr, Z_arr)
A:jax._src.scipy.linalg.r->jax.numpy.conjugate(jnp.asarray(c))
A:jax._src.scipy.linalg.G->jax.numpy.array([[c.conj(), s], [-s, c]], dtype=T.dtype)
A:jax._src.scipy.linalg.T_rows->jax.lax.dynamic_slice_in_dim(T, m - 1, 2, axis=0)
A:jax._src.scipy.linalg.T_rows_new->jax.numpy.where(~col_mask, T_rows, G_dot_T_zeroed_cols)
A:jax._src.scipy.linalg.T->T.at[m, m - 1].set(0.0).at[m, m - 1].set(0.0)
A:jax._src.scipy.linalg.T_cols->jax.lax.dynamic_slice_in_dim(T, m - 1, 2, axis=1)
A:jax._src.scipy.linalg.T_cols_new->jax.numpy.where(~row_mask, T_cols, T_zeroed_rows_dot_GH)
A:jax._src.scipy.linalg.Z_cols->jax.lax.dynamic_slice_in_dim(Z, m - 1, 2, axis=1)
A:jax._src.scipy.linalg.Z->jax.lax.dynamic_update_slice_in_dim(Z, Z_cols @ G.conj().T, m - 1, axis=1)
A:jax._src.scipy.linalg.(a_out, taus)->jax._src.lax.linalg.hessenberg(a)
A:jax._src.scipy.linalg.h->jax.numpy.triu(a_out, -1)
A:jax._src.scipy.linalg.c_arr->jax.numpy.asarray(c).flatten()
A:jax._src.scipy.linalg.r_arr->jax.numpy.asarray(r).flatten()
A:jax._src.scipy.linalg.elems->jax.numpy.concatenate((c_arr[::-1], r_arr[1:]))
jax._src.scipy.linalg._calc_P_Q(A:Array)->tuple[Array, Array, Array]
jax._src.scipy.linalg._cho_solve(c:ArrayLike,b:ArrayLike,lower:bool)->Array
jax._src.scipy.linalg._cholesky(a:ArrayLike,lower:bool)->Array
jax._src.scipy.linalg._eigh(a:ArrayLike,b:Optional[ArrayLike],lower:bool,eigvals_only:bool,eigvals:None,type:int)->Union[Array, tuple[Array, Array]]
jax._src.scipy.linalg._lu(a:ArrayLike,permute_l:bool)->Union[tuple[Array, Array], tuple[Array, Array, Array]]
jax._src.scipy.linalg._pade13(A:Array)->tuple[Array, Array]
jax._src.scipy.linalg._pade3(A:Array)->tuple[Array, Array]
jax._src.scipy.linalg._pade5(A:Array)->tuple[Array, Array]
jax._src.scipy.linalg._pade7(A:Array)->tuple[Array, Array]
jax._src.scipy.linalg._pade9(A:Array)->tuple[Array, Array]
jax._src.scipy.linalg._precise_dot(A:ArrayLike,B:ArrayLike)->Array
jax._src.scipy.linalg._qr(a:ArrayLike,mode:str,pivoting:bool)->Union[tuple[Array], tuple[Array, Array]]
jax._src.scipy.linalg._schur(a:Array,output:str)->tuple[Array, Array]
jax._src.scipy.linalg._solve(a:ArrayLike,b:ArrayLike,assume_a:str,lower:bool)->Array
jax._src.scipy.linalg._solve_P_Q(P:ArrayLike,Q:ArrayLike,upper_triangular:bool=False)->Array
jax._src.scipy.linalg._solve_triangular(a:ArrayLike,b:ArrayLike,trans:Union[int,str],lower:bool,unit_diagonal:bool)->Array
jax._src.scipy.linalg._sqrtm(A:ArrayLike)->Array
jax._src.scipy.linalg._sqrtm_triu(T:Array)->Array
jax._src.scipy.linalg._squaring(R:Array,n_squarings:Array,max_squarings:int)->Array
jax._src.scipy.linalg._svd(a:ArrayLike,*,full_matrices:bool,compute_uv:bool)->Union[Array, tuple[Array, Array, Array]]
jax._src.scipy.linalg.block_diag(*arrs:ArrayLike)->Array
jax._src.scipy.linalg.cho_factor(a:ArrayLike,lower:bool=False,overwrite_a:bool=False,check_finite:bool=True)->tuple[Array, bool]
jax._src.scipy.linalg.cho_solve(c_and_lower:tuple[ArrayLike,bool],b:ArrayLike,overwrite_b:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.cholesky(a:ArrayLike,lower:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.det(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.eigh(a:ArrayLike,b:Optional[ArrayLike]=None,lower:bool=True,eigvals_only:bool=False,overwrite_a:bool=False,overwrite_b:bool=False,turbo:bool=True,eigvals:None=None,type:int=1,check_finite:bool=True)->Union[Array, tuple[Array, Array]]
jax._src.scipy.linalg.eigh_tridiagonal(d:ArrayLike,e:ArrayLike,*,eigvals_only:bool=False,select:str='a',select_range:Optional[tuple[float,float]]=None,tol:Optional[float]=None)->Array
jax._src.scipy.linalg.expm(A:ArrayLike,*,upper_triangular:bool=False,max_squarings:int=16)->Array
jax._src.scipy.linalg.expm_frechet(A:ArrayLike,E:ArrayLike,*,method:Optional[str]=None,compute_expm:bool=True)->Union[Array, tuple[Array, Array]]
jax._src.scipy.linalg.hessenberg(a:ArrayLike,*,calc_q:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Union[Array, tuple[Array, Array]]
jax._src.scipy.linalg.inv(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.lu(a:ArrayLike,permute_l:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Union[tuple[Array, Array], tuple[Array, Array, Array]]
jax._src.scipy.linalg.lu_factor(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->tuple[Array, Array]
jax._src.scipy.linalg.lu_solve(lu_and_piv:tuple[Array,ArrayLike],b:ArrayLike,trans:int=0,overwrite_b:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.polar(a:ArrayLike,side:str='right',*,method:str='qdwh',eps:Optional[float]=None,max_iterations:Optional[int]=None)->tuple[Array, Array]
jax._src.scipy.linalg.qr(a:ArrayLike,overwrite_a:bool=False,lwork:Any=None,mode:str='full',pivoting:bool=False,check_finite:bool=True)->Union[tuple[Array], tuple[Array, Array]]
jax._src.scipy.linalg.rsf2csf(T:ArrayLike,Z:ArrayLike,check_finite:bool=True)->tuple[Array, Array]
jax._src.scipy.linalg.schur(a:ArrayLike,output:str='real')->tuple[Array, Array]
jax._src.scipy.linalg.solve(a:ArrayLike,b:ArrayLike,sym_pos:bool=False,lower:bool=False,overwrite_a:bool=False,overwrite_b:bool=False,debug:bool=False,check_finite:bool=True,assume_a:str='gen')->Array
jax._src.scipy.linalg.solve_triangular(a:ArrayLike,b:ArrayLike,trans:Union[int,str]=0,lower:bool=False,unit_diagonal:bool=False,overwrite_b:bool=False,debug:Any=None,check_finite:bool=True)->Array
jax._src.scipy.linalg.sqrtm(A:ArrayLike,blocksize:int=1)->Array
jax._src.scipy.linalg.svd(a:ArrayLike,full_matrices:bool=True,compute_uv:bool=True,overwrite_a:bool=False,check_finite:bool=True,lapack_driver:str='gesdd')->Union[Array, tuple[Array, Array, Array]]
jax._src.scipy.linalg.toeplitz(c:ArrayLike,r:Optional[ArrayLike]=None)->Array
jax._src.scipy.linalg.tril(m:ArrayLike,k:int=0)->Array
jax._src.scipy.linalg.triu(m:ArrayLike,k:int=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/signal.py----------------------------------------
A:jax._src.scipy.signal.(in1, in2)->promote_dtypes_inexact(in1, in2)
A:jax._src.scipy.signal._fftconvolve->jax.vmap(_fftconvolve, in_axes=ax, out_axes=ax)
A:jax._src.scipy.signal.axes->tuple((canonicalize_axis(ax, in1.ndim) for ax in axes))
A:jax._src.scipy.signal.full_shape->tuple((s1 + s2 - 1 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.fft_shape->tuple((osp_fft_next_fast_len(s) for s in full_shape))
A:jax._src.scipy.signal.no_swap->all((s1 >= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.swap->all((s1 <= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.sp1->fft(in1, fft_shape)
A:jax._src.scipy.signal.sp2->fft(in2, fft_shape)
A:jax._src.scipy.signal.conv->ifft(sp1 * sp2, fft_shape)
A:jax._src.scipy.signal.out_shape->tuple((s1 - s2 + 1 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.start_indices->tuple(((full_size - out_size) // 2 for (full_size, out_size) in zip(full_shape, out_shape)))
A:jax._src.scipy.signal.in2->jax.numpy.flip(in2)
A:jax._src.scipy.signal.strides->tuple((1 for s in shape))
A:jax._src.scipy.signal.result->jax.numpy.moveaxis(result, -1, axis)
A:jax._src.scipy.signal.same_shape->all((s1 == s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.(data_arr,)->promote_dtypes_inexact(jnp.asarray(data))
A:jax._src.scipy.signal.bp_arr->numpy.sort(np.unique(np.r_[0, bp, N]))
A:jax._src.scipy.signal.data_arr->data_arr.at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST)).at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST))
A:jax._src.scipy.signal.sl->slice(bp_arr[m], bp_arr[m + 1])
A:jax._src.scipy.signal.(coef, *_)->jax._src.numpy.linalg.lstsq(A, data_arr[sl])
A:jax._src.scipy.signal.x->jax.numpy.moveaxis(x, -1, time_axis)
A:jax._src.scipy.signal.batch_shape->list(batch_shape)
A:jax._src.scipy.signal.(result,)->promote_dtypes_complex(result)
A:jax._src.scipy.signal.left_end->jax.lax.slice_in_dim(x, 0, 1, axis=axis)
A:jax._src.scipy.signal.left_ext->jax.numpy.flip(lax.slice_in_dim(x, 1, n + 1, axis=axis), axis=axis)
A:jax._src.scipy.signal.right_end->jax.lax.slice_in_dim(x, -1, None, axis=axis)
A:jax._src.scipy.signal.right_ext->jax.numpy.flip(lax.slice_in_dim(x, -(n + 1), -1, axis=axis), axis=axis)
A:jax._src.scipy.signal.ext->jax.numpy.concatenate((2 * left_end - left_ext, x, 2 * right_end - right_ext), axis=axis)
A:jax._src.scipy.signal.axis->canonicalize_axis(axis, x.ndim)
A:jax._src.scipy.signal.(x,)->promote_dtypes_inexact(x)
A:jax._src.scipy.signal.outershape->jax.numpy.broadcast_shapes(tuple_delete(x.shape, axis), tuple_delete(y_arr.shape, axis))
A:jax._src.scipy.signal.(x, y_arr)->promote_dtypes_inexact(x, y)
A:jax._src.scipy.signal.result_dtype->jax._src.dtypes.to_complex_dtype(x.dtype)
A:jax._src.scipy.signal.nperseg_int->jax.core.concrete_or_error(int, nperseg or n_default, 'nperseg: segment length of STFT')
A:jax._src.scipy.signal.(win, nperseg_int)->jax._src.third_party.scipy.signal_helper._triage_segments(window, nperseg if nperseg is None else nperseg_int, input_length=x.shape[axis], dtype=x.dtype)
A:jax._src.scipy.signal.noverlap_int->jax.core.concrete_or_error(int, noverlap or nperseg_int // 2, 'noverlap of STFT')
A:jax._src.scipy.signal.nfft_int->jax.core.concrete_or_error(int, nfft, 'nfft of STFT')
A:jax._src.scipy.signal.shape->tuple_insert(outershape, min([x.shape[axis], y_arr.shape[axis]]), axis)
A:jax._src.scipy.signal.y_arr->jax.numpy.concatenate((y_arr, jnp.zeros_like(x, shape=(*y_arr.shape[:-1], nadd))), axis=-1)
A:jax._src.scipy.signal.pad_shape->list(y_arr.shape)
A:jax._src.scipy.signal.detrend_func->partial(detrend, type=detrend_type, axis=-1)
A:jax._src.scipy.signal.d->detrend_type(d)
A:jax._src.scipy.signal.scale->jax.numpy.sqrt(scale)
A:jax._src.scipy.signal.(scale,)->promote_dtypes_complex(scale)
A:jax._src.scipy.signal.freqs->jax.numpy.fft.rfftfreq(nfft_int, 1 / fs, dtype=freq_dtype)
A:jax._src.scipy.signal.result_y->_fft_helper(y_arr, win, detrend_func, nperseg_int, noverlap_int, nfft_int, sides)
A:jax._src.scipy.signal.(freqs, _, Pxy)->_spectral_helper(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode='psd')
A:jax._src.scipy.signal.bias->jax._src.third_party.scipy.signal_helper._median_bias(Pxy.shape[-1]).astype(Pxy.dtype)
A:jax._src.scipy.signal.Pxy->jax.numpy.reshape(Pxy, Pxy.shape[:-1])
A:jax._src.scipy.signal.(freqs, Pxx)->csd(x, None, fs=fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft, detrend=detrend, return_onesided=return_onesided, scaling=scaling, axis=axis, average=average)
A:jax._src.scipy.signal.step_size->jax.core.concrete_or_error(int, step_size, 'step_size for overlap_and_add')
A:jax._src.scipy.signal.flat_batchsize->math.prod(batch_shape)
A:jax._src.scipy.signal.freq_axis->canonicalize_axis(freq_axis, Zxx.ndim)
A:jax._src.scipy.signal.time_axis->canonicalize_axis(time_axis, Zxx.ndim)
A:jax._src.scipy.signal.Zxx->jax.numpy.transpose(Zxx, outer_idxs + (freq_axis, time_axis))
A:jax._src.scipy.signal.outer_idxs->tuple((idx for idx in range(Zxx.ndim) if idx not in {time_axis, freq_axis}))
A:jax._src.scipy.signal.win->win.reshape((1,) * (xsubs.ndim - 2) + win.shape + (1,)).reshape((1,) * (xsubs.ndim - 2) + win.shape + (1,))
A:jax._src.scipy.signal.win_squared->jax.numpy.repeat(win * win, xsubs.shape[-1], axis=-1)
A:jax._src.scipy.signal.norm->_overlap_and_add(win_squared.swapaxes(-2, -1), nstep)
jax._src.scipy.signal._convolve_nd(in1:Array,in2:Array,mode:str,*,precision:PrecisionLike)->Array
jax._src.scipy.signal._fft_helper(x:Array,win:Array,detrend_func:Callable[[Array],Array],nperseg:int,noverlap:int,nfft:Optional[int],sides:str)->Array
jax._src.scipy.signal._fftconvolve_unbatched(in1:Array,in2:Array,mode:str)->Array
jax._src.scipy.signal._overlap_and_add(x:Array,step_size:int)->Array
jax._src.scipy.signal._spectral_helper(x:Array,y:Optional[ArrayLike],fs:ArrayLike=1.0,window:str='hann',nperseg:Optional[int]=None,noverlap:Optional[int]=None,nfft:Optional[int]=None,detrend_type:Union[bool,str,Callable[[Array],Array]]='constant',return_onesided:bool=True,scaling:str='density',axis:int=-1,mode:str='psd',boundary:Optional[str]=None,padded:bool=False)->tuple[Array, Array, Array]
jax._src.scipy.signal.convolve(in1:Array,in2:Array,mode:str='full',method:str='auto',precision:PrecisionLike=None)->Array
jax._src.scipy.signal.convolve2d(in1:Array,in2:Array,mode:str='full',boundary:str='fill',fillvalue:float=0,precision:PrecisionLike=None)->Array
jax._src.scipy.signal.correlate(in1:Array,in2:Array,mode:str='full',method:str='auto',precision:PrecisionLike=None)->Array
jax._src.scipy.signal.correlate2d(in1:Array,in2:Array,mode:str='full',boundary:str='fill',fillvalue:float=0,precision:PrecisionLike=None)->Array
jax._src.scipy.signal.csd(x:Array,y:Optional[ArrayLike],fs:ArrayLike=1.0,window:str='hann',nperseg:Optional[int]=None,noverlap:Optional[int]=None,nfft:Optional[int]=None,detrend:str='constant',return_onesided:bool=True,scaling:str='density',axis:int=-1,average:str='mean')->tuple[Array, Array]
jax._src.scipy.signal.detrend(data:ArrayLike,axis:int=-1,type:str='linear',bp:int=0,overwrite_data:None=None)->Array
jax._src.scipy.signal.fftconvolve(in1:ArrayLike,in2:ArrayLike,mode:str='full',axes:Optional[Sequence[int]]=None)->Array
jax._src.scipy.signal.istft(Zxx:Array,fs:ArrayLike=1.0,window:str='hann',nperseg:Optional[int]=None,noverlap:Optional[int]=None,nfft:Optional[int]=None,input_onesided:bool=True,boundary:bool=True,time_axis:int=-1,freq_axis:int=-2)->tuple[Array, Array]
jax._src.scipy.signal.odd_ext(x:Array,n:int,axis:int=-1)->Array
jax._src.scipy.signal.stft(x:Array,fs:ArrayLike=1.0,window:str='hann',nperseg:int=256,noverlap:Optional[int]=None,nfft:Optional[int]=None,detrend:bool=False,return_onesided:bool=True,boundary:Optional[str]='zeros',padded:bool=True,axis:int=-1)->tuple[Array, Array, Array]
jax._src.scipy.signal.welch(x:Array,fs:ArrayLike=1.0,window:str='hann',nperseg:Optional[int]=None,noverlap:Optional[int]=None,nfft:Optional[int]=None,detrend:str='constant',return_onesided:bool=True,scaling:str='density',axis:int=-1,average:str='mean')->tuple[Array, Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/cluster/vq.py----------------------------------------
A:jax._src.scipy.cluster.vq._no_chkfinite_doc->textwrap.dedent('\nDoes not support the Scipy argument ``check_finite=True``,\nbecause compiled JAX code cannot perform checks of array values at runtime\n')
A:jax._src.scipy.cluster.vq.(obs, code_book)->promote_dtypes_inexact(obs, code_book)
A:jax._src.scipy.cluster.vq.dist->vmap(lambda ob: jnp.linalg.norm(ob[None] - code_book, axis=-1))(obs)
A:jax._src.scipy.cluster.vq.code->jax.numpy.argmin(dist, axis=-1)
A:jax._src.scipy.cluster.vq.dist_min->vmap(operator.getitem)(dist, code)
jax._src.scipy.cluster.vq.vq(obs,code_book,check_finite=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/sparse/linalg.py----------------------------------------
A:jax._src.scipy.sparse.linalg._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg._vdot->partial(jnp.vdot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg._einsum->partial(jnp.einsum, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg.result->_vdot(x.real, y.real)
A:jax._src.scipy.sparse.linalg.xs->tree_leaves(x)
A:jax._src.scipy.sparse.linalg._add->partial(tree_map, operator.add)
A:jax._src.scipy.sparse.linalg._sub->partial(tree_map, operator.sub)
A:jax._src.scipy.sparse.linalg._dot_tree->partial(tree_map, _dot)
A:jax._src.scipy.sparse.linalg.bs->_vdot_real_tree(b, b)
A:jax._src.scipy.sparse.linalg.atol2->jax.numpy.maximum(jnp.square(tol) * bs, jnp.square(atol))
A:jax._src.scipy.sparse.linalg.Ap->A(p)
A:jax._src.scipy.sparse.linalg.x_->tree_map(partial(jnp.where, exit_early), _add(x, _mul(alpha_, phat)), _add(x, _add(_mul(alpha_, phat), _mul(omega_, shat))))
A:jax._src.scipy.sparse.linalg.r_->tree_map(partial(jnp.where, exit_early), s, _sub(s, _mul(omega_, t)))
A:jax._src.scipy.sparse.linalg.z_->M(r_)
A:jax._src.scipy.sparse.linalg.gamma_->_vdot_real_tree(r_, z_).astype(dtype)
A:jax._src.scipy.sparse.linalg.p_->_add(r, _mul(beta, _sub(p, _mul(omega, q))))
A:jax._src.scipy.sparse.linalg.r0->_sub(b, A(x0))
A:jax._src.scipy.sparse.linalg.p0z0->M(r0)
A:jax._src.scipy.sparse.linalg.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.scipy.sparse.linalg.gamma0->_vdot_real_tree(r0, z0).astype(dtype)
A:jax._src.scipy.sparse.linalg.(x_final, *_)->jax.lax.while_loop(cond_fun, body_fun, initial_value)
A:jax._src.scipy.sparse.linalg.rs->_vdot_real_tree(r, r)
A:jax._src.scipy.sparse.linalg.rho_->_vdot_tree(rhat, r)
A:jax._src.scipy.sparse.linalg.phat->M(p_)
A:jax._src.scipy.sparse.linalg.q_->A(phat)
A:jax._src.scipy.sparse.linalg.s->_sub(r, _mul(alpha_, q_))
A:jax._src.scipy.sparse.linalg.shat->M(s)
A:jax._src.scipy.sparse.linalg.t->A(shat)
A:jax._src.scipy.sparse.linalg.k_->jax.numpy.where(rho_ == 0, -10, k_)
A:jax._src.scipy.sparse.linalg.rho0alpha0omega0->jax._src.lax.lax._convert_element_type(1, *dtypes._lattice_result_type(*tree_leaves(b)))
A:jax._src.scipy.sparse.linalg.x0->tree_map(jnp.zeros_like, b)
A:jax._src.scipy.sparse.linalg.(b, x0)->device_put((b, x0))
A:jax._src.scipy.sparse.linalg.size->sum((bi.size for bi in tree_leaves(b)))
A:jax._src.scipy.sparse.linalg.A->_normalize_matvec(A)
A:jax._src.scipy.sparse.linalg.M->_normalize_matvec(M)
A:jax._src.scipy.sparse.linalg.isolve_solve->partial(_isolve_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)
A:jax._src.scipy.sparse.linalg.x->jax.lax.custom_linear_solve(A, b, solve=_solve, transpose_solve=_solve)
A:jax._src.scipy.sparse.linalg.norm->jax.numpy.where(use_norm, norm, 0.0)
A:jax._src.scipy.sparse.linalg.(dtype, weak_type)->jax._src.dtypes._lattice_result_type(*tree_leaves(b))
A:jax._src.scipy.sparse.linalg.norm_cast->jax._src.lax.lax._convert_element_type(norm, dtype, weak_type)
A:jax._src.scipy.sparse.linalg.normalized_x->tree_map(lambda y: jnp.where(use_norm, y / norm_cast, 0.0), x)
A:jax._src.scipy.sparse.linalg.v_proj->tree_map(lambda X, y: _einsum('...n,...->n', X.conj(), y), A, v)
A:jax._src.scipy.sparse.linalg.r->jax.lax.rsqrt(1 + abs(t) ** 2).astype(t.dtype)
A:jax._src.scipy.sparse.linalg.h->h.at[k + 1].set(v_norm_1.astype(dtype)).at[k + 1].set(v_norm_1.astype(dtype))
A:jax._src.scipy.sparse.linalg.Qh->tree_map(lambda X: _dot(X, h), Q)
A:jax._src.scipy.sparse.linalg.q->_sub(q, Qh)
A:jax._src.scipy.sparse.linalg.(_, qnorm)->_safe_normalize(q)
A:jax._src.scipy.sparse.linalg.(_, _, q, qnorm_scaled)->jax.lax.while_loop(qnorm_cond, qnorm, init)
A:jax._src.scipy.sparse.linalg.(_, rnorm)->_safe_normalize(r)
A:jax._src.scipy.sparse.linalg.(k, q, r, qnorm_scaled)->body_function((0, q, r, xnorm_scaled))
A:jax._src.scipy.sparse.linalg.(k, q, r, _)->jax.lax.while_loop(cond_function, body_function, (k, q, r, qnorm_scaled))
A:jax._src.scipy.sparse.linalg.(dtype, _)->jax._src.dtypes._lattice_result_type(*tree_leaves(V))
A:jax._src.scipy.sparse.linalg.v->M(A(v))
A:jax._src.scipy.sparse.linalg.(_, v_norm_0)->_safe_normalize(v)
A:jax._src.scipy.sparse.linalg.(v, h)->_iterative_classical_gram_schmidt(V, v, v_norm_0, max_iterations=2)
A:jax._src.scipy.sparse.linalg.(unit_v, v_norm_1)->_safe_normalize(v, thresh=tol)
A:jax._src.scipy.sparse.linalg.V->tree_map(lambda x: jnp.pad(x[..., None], ((0, 0),) * x.ndim + ((0, restart),)), unit_residual)
A:jax._src.scipy.sparse.linalg.H->jax._src.lax.lax._convert_element_type(jnp.eye(restart, restart + 1, dtype=dtype), weak_type=weak_type)
A:jax._src.scipy.sparse.linalg.cs->jax.numpy.where(b_zero, 1, jnp.where(a_lt_b, r * t, r))
A:jax._src.scipy.sparse.linalg.sn->jax.numpy.where(b_zero, 0, jnp.where(a_lt_b, r, r * t))
A:jax._src.scipy.sparse.linalg.R_row->_rotate_vectors(R_row, k, *givens_factors)
A:jax._src.scipy.sparse.linalg.givens_factors->_givens_rotation(R_row[k], R_row[k + 1])
A:jax._src.scipy.sparse.linalg.givens->jax.numpy.zeros((restart, 2), dtype=dtype)
A:jax._src.scipy.sparse.linalg.R->R.at[k, :].set(R_row).at[k, :].set(R_row)
A:jax._src.scipy.sparse.linalg.beta_vec->jax.numpy.zeros_like(H, shape=(restart + 1,)).at[0].set(residual_norm.astype(dtype))
A:jax._src.scipy.sparse.linalg.(V, H, _)->_kth_arnoldi_iteration(k, A, M, V, R)
A:jax._src.scipy.sparse.linalg.(R_row, givens)->_apply_givens_rotations(H[k, :], givens, k)
A:jax._src.scipy.sparse.linalg.err->abs(beta_vec[k + 1])
A:jax._src.scipy.sparse.linalg.carry->jax.lax.while_loop(loop_cond, arnoldi_qr_step, carry)
A:jax._src.scipy.sparse.linalg.y->_lstsq(H.T, beta_vec)
A:jax._src.scipy.sparse.linalg.dx->tree_map(lambda X: _dot(X[..., :-1], y), V)
A:jax._src.scipy.sparse.linalg.residual->M(_sub(b, A(x0)))
A:jax._src.scipy.sparse.linalg.(unit_residual, residual_norm)->_safe_normalize(residual)
A:jax._src.scipy.sparse.linalg.a2->_dot(a.T.conj(), a)
A:jax._src.scipy.sparse.linalg.b2->_dot(a.T.conj(), b)
A:jax._src.scipy.sparse.linalg.(V, H, breakdown)->_kth_arnoldi_iteration(k, A, M, V, H)
A:jax._src.scipy.sparse.linalg.(V, H, _, _)->jax.lax.while_loop(loop_cond, arnoldi_process, carry)
A:jax._src.scipy.sparse.linalg.(x, unit_residual, residual_norm)->gmres_func(A, b, x, unit_residual, residual_norm, ptol, restart, M)
A:jax._src.scipy.sparse.linalg.(x_final, k, _, err)->jax.lax.while_loop(cond_fun, body_fun, initialization)
A:jax._src.scipy.sparse.linalg.restart->min(restart, size)
A:jax._src.scipy.sparse.linalg.b_norm->_norm(b)
A:jax._src.scipy.sparse.linalg.atol->jax.numpy.maximum(tol * b_norm, atol)
A:jax._src.scipy.sparse.linalg.Mb->M(b)
A:jax._src.scipy.sparse.linalg.Mb_norm->_norm(Mb)
A:jax._src.scipy.sparse.linalg.failed->jax.numpy.isnan(_norm(x))
A:jax._src.scipy.sparse.linalg.info->jax.numpy.where(failed, x=-1, y=0)
jax._src.scipy.sparse.linalg._apply_givens_rotations(H_row,givens,k)
jax._src.scipy.sparse.linalg._bicgstab_solve(A,b,x0=None,*,maxiter,tol=1e-05,atol=0.0,M=_identity)
jax._src.scipy.sparse.linalg._cg_solve(A,b,x0=None,*,maxiter,tol=1e-05,atol=0.0,M=_identity)
jax._src.scipy.sparse.linalg._div(tree,scalar)
jax._src.scipy.sparse.linalg._givens_rotation(a,b)
jax._src.scipy.sparse.linalg._gmres_batched(A,b,x0,unit_residual,residual_norm,ptol,restart,M)
jax._src.scipy.sparse.linalg._gmres_incremental(A,b,x0,unit_residual,residual_norm,ptol,restart,M)
jax._src.scipy.sparse.linalg._gmres_solve(A,b,x0,atol,ptol,restart,maxiter,M,gmres_func)
jax._src.scipy.sparse.linalg._identity(x)
jax._src.scipy.sparse.linalg._isolve(_isolve_solve,A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None,check_symmetric=False)
jax._src.scipy.sparse.linalg._iterative_classical_gram_schmidt(Q,x,xnorm,max_iterations=2)
jax._src.scipy.sparse.linalg._kth_arnoldi_iteration(k,A,M,V,H)
jax._src.scipy.sparse.linalg._lstsq(a,b)
jax._src.scipy.sparse.linalg._mul(scalar,tree)
jax._src.scipy.sparse.linalg._norm(x)
jax._src.scipy.sparse.linalg._normalize_matvec(f)
jax._src.scipy.sparse.linalg._project_on_columns(A,v)
jax._src.scipy.sparse.linalg._rotate_vectors(H,i,cs,sn)
jax._src.scipy.sparse.linalg._safe_normalize(x,thresh=None)
jax._src.scipy.sparse.linalg._shapes(pytree)
jax._src.scipy.sparse.linalg._vdot_real_part(x,y)
jax._src.scipy.sparse.linalg._vdot_real_tree(x,y)
jax._src.scipy.sparse.linalg._vdot_tree(x,y)
jax._src.scipy.sparse.linalg.bicgstab(A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None)
jax._src.scipy.sparse.linalg.cg(A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None)
jax._src.scipy.sparse.linalg.gmres(A,b,x0=None,*,tol=1e-05,atol=0.0,restart=20,maxiter=None,M=None,solve_method='batched')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/spatial/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/spatial/transform.py----------------------------------------
A:jax._src.scipy.spatial.transform.num_axes->len(seq)
A:jax._src.scipy.spatial.transform.angles->jax.numpy.where(degrees, jnp.deg2rad(angles), angles)
A:jax._src.scipy.spatial.transform.axes->jax.numpy.where(extrinsic, axes, axes[::-1])
A:jax._src.scipy.spatial.transform.quat->quat.at[axis].set(jnp.sin(angle / 2.0)).at[axis].set(jnp.sin(angle / 2.0))
A:jax._src.scipy.spatial.transform.K->jax.numpy.dot(w[jnp.newaxis, :] * self.quat.T, self.quat)
A:jax._src.scipy.spatial.transform.(_, v)->jax.numpy.linalg.eigh(K)
A:jax._src.scipy.spatial.transform.times->jax.numpy.asarray(times, dtype=rotations.quat.dtype)
A:jax._src.scipy.spatial.transform.timedelta->jax.numpy.diff(times)
A:jax._src.scipy.spatial.transform.new_rotations->Rotation(rotations.as_quat()[:-1])
A:jax._src.scipy.spatial.transform.compute_times->jax.numpy.atleast_1d(compute_times)
A:jax._src.scipy.spatial.transform.ind->jax.numpy.maximum(jnp.searchsorted(self.times, compute_times) - 1, 0)
A:jax._src.scipy.spatial.transform.sign->jax.numpy.array((i - j) * (j - k) * (k - i) // 2, dtype=quat.dtype)
A:jax._src.scipy.spatial.transform.scale->jax.numpy.where(angle <= 0.001, small_scale, large_scale)
A:jax._src.scipy.spatial.transform.cross->jax.numpy.cross(p[:3], q[:3])
A:jax._src.scipy.spatial.transform.angle_first->jax.numpy.where(extrinsic, 0, 2)
A:jax._src.scipy.spatial.transform.angle_third->jax.numpy.where(extrinsic, 2, 0)
A:jax._src.scipy.spatial.transform.k->jax.numpy.where(symmetric, 3 - i - j, k)
A:jax._src.scipy.spatial.transform.a->jax.numpy.where(symmetric, quat[3], quat[3] - quat[j])
A:jax._src.scipy.spatial.transform.b->jax.numpy.where(symmetric, quat[i], quat[i] + quat[k] * sign)
A:jax._src.scipy.spatial.transform.c->jax.numpy.where(symmetric, quat[j], quat[j] + quat[3])
A:jax._src.scipy.spatial.transform.d->jax.numpy.where(symmetric, quat[k] * sign, quat[k] * sign - quat[i])
A:jax._src.scipy.spatial.transform.case->jax.numpy.where(jnp.abs(angles[1]) <= eps, 1, case)
A:jax._src.scipy.spatial.transform.half_sum->jax.numpy.arctan2(b, a)
A:jax._src.scipy.spatial.transform.half_diff->jax.numpy.arctan2(d, c)
A:jax._src.scipy.spatial.transform.result->jax.numpy.where(intrinsic, _compose_quat(result, quat), _compose_quat(quat, result))
A:jax._src.scipy.spatial.transform.rotvec->jax.numpy.where(degrees, jnp.deg2rad(rotvec), rotvec)
A:jax._src.scipy.spatial.transform.angle->_vector_norm(rotvec)
A:jax._src.scipy.spatial.transform.decision->jax.numpy.array([matrix[0, 0], matrix[1, 1], matrix[2, 2], matrix_trace], dtype=matrix.dtype)
A:jax._src.scipy.spatial.transform.choice->jax.numpy.argmax(decision)
A:jax._src.scipy.spatial.transform.quat_012->quat_012.at[3].set(matrix[k, j] - matrix[j, k]).at[3].set(matrix[k, j] - matrix[j, k])
A:jax._src.scipy.spatial.transform.quat_3->quat_3.at[3].set(1 + decision[3]).at[3].set(1 + decision[3])
jax._src.scipy.spatial.transform.Rotation(typing.NamedTuple)
jax._src.scipy.spatial.transform.Rotation.__getitem__(self,indexer)
jax._src.scipy.spatial.transform.Rotation.__len__(self)
jax._src.scipy.spatial.transform.Rotation.__mul__(self,other)
jax._src.scipy.spatial.transform.Rotation.apply(self,vectors:jax.Array,inverse:bool=False)->jax.Array
jax._src.scipy.spatial.transform.Rotation.as_euler(self,seq:str,degrees:bool=False)
jax._src.scipy.spatial.transform.Rotation.as_matrix(self)->jax.Array
jax._src.scipy.spatial.transform.Rotation.as_mrp(self)->jax.Array
jax._src.scipy.spatial.transform.Rotation.as_quat(self)->jax.Array
jax._src.scipy.spatial.transform.Rotation.as_rotvec(self,degrees:bool=False)->jax.Array
jax._src.scipy.spatial.transform.Rotation.concatenate(cls,rotations:typing.Sequence)
jax._src.scipy.spatial.transform.Rotation.from_euler(cls,seq:str,angles:jax.Array,degrees:bool=False)
jax._src.scipy.spatial.transform.Rotation.from_matrix(cls,matrix:jax.Array)
jax._src.scipy.spatial.transform.Rotation.from_mrp(cls,mrp:jax.Array)
jax._src.scipy.spatial.transform.Rotation.from_quat(cls,quat:jax.Array)
jax._src.scipy.spatial.transform.Rotation.from_rotvec(cls,rotvec:jax.Array,degrees:bool=False)
jax._src.scipy.spatial.transform.Rotation.identity(cls,num:typing.Optional[int]=None,dtype=float)
jax._src.scipy.spatial.transform.Rotation.inv(self)
jax._src.scipy.spatial.transform.Rotation.magnitude(self)->jax.Array
jax._src.scipy.spatial.transform.Rotation.mean(self,weights:typing.Optional[jax.Array]=None)
jax._src.scipy.spatial.transform.Rotation.random(cls,random_key:jax.Array,num:typing.Optional[int]=None)
jax._src.scipy.spatial.transform.Rotation.single(self)->bool
jax._src.scipy.spatial.transform.Slerp(self,times:jax.Array)
jax._src.scipy.spatial.transform.Slerp.__call__(self,times:jax.Array)
jax._src.scipy.spatial.transform.Slerp.init(cls,times:jax.Array,rotations:Rotation)
jax._src.scipy.spatial.transform._apply(matrix:jax.Array,vector:jax.Array,inverse:bool)->jax.Array
jax._src.scipy.spatial.transform._as_matrix(quat:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._as_mrp(quat:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._as_rotvec(quat:jax.Array,degrees:bool)->jax.Array
jax._src.scipy.spatial.transform._compose_quat(p:jax.Array,q:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._compute_euler_from_quat(quat:jax.Array,axes:jax.Array,extrinsic:bool,degrees:bool)->jax.Array
jax._src.scipy.spatial.transform._elementary_basis_index(axis:str)->int
jax._src.scipy.spatial.transform._elementary_quat_compose(angles:jax.Array,axes:jax.Array,intrinsic:bool,degrees:bool)->jax.Array
jax._src.scipy.spatial.transform._from_matrix(matrix:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._from_mrp(mrp:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._from_rotvec(rotvec:jax.Array,degrees:bool)->jax.Array
jax._src.scipy.spatial.transform._inv(quat:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._magnitude(quat:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._make_elementary_quat(axis:int,angle:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._normalize_quaternion(quat:jax.Array)->jax.Array
jax._src.scipy.spatial.transform._vector_norm(vector:jax.Array)->jax.Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/beta.py----------------------------------------
A:jax._src.scipy.stats.beta.(x, a, b, loc, scale)->promote_args_inexact('beta.sf', x, a, b, loc, scale)
A:jax._src.scipy.stats.beta.one->_lax_const(x, 1)
A:jax._src.scipy.stats.beta.shape_term->jax.lax.neg(betaln(a, b))
A:jax._src.scipy.stats.beta.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.beta.log_linear_term->jax.lax.add(xlogy(lax.sub(a, one), y), xlog1py(lax.sub(b, one), lax.neg(y)))
A:jax._src.scipy.stats.beta.log_probs->jax.lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
jax._src.scipy.stats.beta.cdf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.beta.logcdf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.beta.logpdf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.beta.logsf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.beta.pdf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.beta.sf(x:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/multivariate_normal.py----------------------------------------
A:jax._src.scipy.stats.multivariate_normal.(x, mean, cov)->promote_dtypes_inexact(x, mean, cov)
A:jax._src.scipy.stats.multivariate_normal.L->jax.lax.linalg.cholesky(cov)
A:jax._src.scipy.stats.multivariate_normal.y->jax.numpy.vectorize(partial(lax.linalg.triangular_solve, lower=True, transpose_a=True), signature='(n,n),(n)->(n)')(L, x - mean)
jax._src.scipy.stats.multivariate_normal.logpdf(x:ArrayLike,mean:ArrayLike,cov:ArrayLike,allow_singular:None=None)->ArrayLike
jax._src.scipy.stats.multivariate_normal.pdf(x:ArrayLike,mean:ArrayLike,cov:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/chi2.py----------------------------------------
A:jax._src.scipy.stats.chi2.(x, df, loc, scale)->promote_args_inexact('chi2.sf', x, df, loc, scale)
A:jax._src.scipy.stats.chi2.one->_lax_const(x, 1)
A:jax._src.scipy.stats.chi2.two->_lax_const(scale, 2)
A:jax._src.scipy.stats.chi2.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.chi2.df_on_two->jax.lax.div(df, two)
A:jax._src.scipy.stats.chi2.kernel->jax.lax.sub(lax.mul(lax.sub(df_on_two, one), lax.log(y)), lax.div(y, two))
A:jax._src.scipy.stats.chi2.nrml_cnst->jax.lax.neg(lax.add(lax.lgamma(df_on_two), lax.div(lax.mul(lax.log(two), df), two)))
A:jax._src.scipy.stats.chi2.log_probs->jax.lax.add(lax.sub(nrml_cnst, lax.log(scale)), kernel)
jax._src.scipy.stats.chi2.cdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.chi2.logcdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.chi2.logpdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.chi2.logsf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.chi2.pdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.chi2.sf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/cauchy.py----------------------------------------
A:jax._src.scipy.stats.cauchy.(x, loc, scale)->promote_args_inexact('cauchy.logsf', x, loc, scale)
A:jax._src.scipy.stats.cauchy.pi->_lax_const(q, np.pi)
A:jax._src.scipy.stats.cauchy.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.cauchy.normalize_term->jax.lax.log(lax.mul(pi, scale))
A:jax._src.scipy.stats.cauchy.(q, loc, scale)->promote_args_inexact('cauchy.ppf', q, loc, scale)
A:jax._src.scipy.stats.cauchy.half_pi->_lax_const(q, np.pi / 2)
A:jax._src.scipy.stats.cauchy.unscaled->jax.lax.tan(lax.sub(lax.mul(pi, q), half_pi))
jax._src.scipy.stats.cauchy.cdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.isf(q:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.logcdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.logsf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.ppf(q:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.cauchy.sf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/kde.py----------------------------------------
A:jax._src.scipy.stats.kde.dataset->jax.numpy.atleast_2d(dataset)
A:jax._src.scipy.stats.kde.(dataset, weights)->promote_dtypes_inexact(dataset, weights)
A:jax._src.scipy.stats.kde.weights->jax.numpy.full(n, 1.0 / n, dtype=dataset.dtype)
A:jax._src.scipy.stats.kde.(dataset,)->promote_dtypes_inexact(dataset)
A:jax._src.scipy.stats.kde.neff->self._setattr('neff', 1 / jnp.sum(weights ** 2))
A:jax._src.scipy.stats.kde.factor->bw_method(self)
A:jax._src.scipy.stats.kde.data_covariance->jax.numpy.atleast_2d(jnp.cov(dataset, rowvar=1, bias=False, aweights=weights))
A:jax._src.scipy.stats.kde.data_inv_cov->jax.numpy.linalg.inv(data_covariance)
A:jax._src.scipy.stats.kde.kde->cls.__new__(cls)
A:jax._src.scipy.stats.kde.points->jax.numpy.dot(points, whitening)
A:jax._src.scipy.stats.kde.result->_gaussian_kernel_eval(True, self.dataset.T, self.weights[:, None], x.T, self.inv_cov)
A:jax._src.scipy.stats.kde.mean->jax.numpy.atleast_1d(jnp.squeeze(mean))
A:jax._src.scipy.stats.kde.cov->jax.numpy.atleast_2d(cov)
A:jax._src.scipy.stats.kde.chol->jax.scipy.linalg.cho_factor(self.covariance + other.covariance)
A:jax._src.scipy.stats.kde.sigma->jax.numpy.squeeze(jnp.sqrt(self.covariance))
A:jax._src.scipy.stats.kde.low->jax.numpy.squeeze((low - self.dataset) / sigma)
A:jax._src.scipy.stats.kde.high->jax.numpy.squeeze((high - self.dataset) / sigma)
A:jax._src.scipy.stats.kde.(ind_key, eps_key)->jax.random.split(key)
A:jax._src.scipy.stats.kde.ind->jax.random.choice(ind_key, self.n, shape=shape, p=self.weights)
A:jax._src.scipy.stats.kde.x->self._reshape_points(x)
A:jax._src.scipy.stats.kde.alpha->jax.scipy.linalg.cho_solve(chol, diff)
A:jax._src.scipy.stats.kde.(points, values, xi, precision)->promote_dtypes_inexact(points, values, xi, precision)
A:jax._src.scipy.stats.kde.whitening->jax.scipy.linalg.cholesky(precision, lower=True)
A:jax._src.scipy.stats.kde.xi->jax.numpy.dot(xi, whitening)
A:jax._src.scipy.stats.kde.mapped_kernel->vmap(reduced_kernel)
jax._src.scipy.stats.kde._gaussian_kernel_convolve(chol,norm,target,weights,mean)
jax._src.scipy.stats.kde._gaussian_kernel_eval(in_log,points,values,xi,precision)
jax._src.scipy.stats.kde.gaussian_kde(self,dataset,bw_method=None,weights=None)
jax._src.scipy.stats.kde.gaussian_kde.__init__(self,dataset,bw_method=None,weights=None)
jax._src.scipy.stats.kde.gaussian_kde._reshape_points(self,points)
jax._src.scipy.stats.kde.gaussian_kde._setattr(self,name,value)
jax._src.scipy.stats.kde.gaussian_kde.d(self)
jax._src.scipy.stats.kde.gaussian_kde.evaluate(self,points)
jax._src.scipy.stats.kde.gaussian_kde.integrate_box(self,low_bounds,high_bounds,maxpts=None)
jax._src.scipy.stats.kde.gaussian_kde.integrate_box_1d(self,low,high)
jax._src.scipy.stats.kde.gaussian_kde.integrate_gaussian(self,mean,cov)
jax._src.scipy.stats.kde.gaussian_kde.integrate_kde(self,other)
jax._src.scipy.stats.kde.gaussian_kde.logpdf(self,x)
jax._src.scipy.stats.kde.gaussian_kde.n(self)
jax._src.scipy.stats.kde.gaussian_kde.pdf(self,x)
jax._src.scipy.stats.kde.gaussian_kde.resample(self,key,shape=())
jax._src.scipy.stats.kde.gaussian_kde.set_bandwidth(self,bw_method=None)
jax._src.scipy.stats.kde.gaussian_kde.tree_flatten(self)
jax._src.scipy.stats.kde.gaussian_kde.tree_unflatten(cls,aux_data,children)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/bernoulli.py----------------------------------------
A:jax._src.scipy.stats.bernoulli.(k, p, loc)->promote_args_inexact('bernoulli.logpmf', k, p, loc)
A:jax._src.scipy.stats.bernoulli.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.bernoulli.one->_lax_const(k, 1)
A:jax._src.scipy.stats.bernoulli.x->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.bernoulli.(k, p)->promote_args_inexact('bernoulli.cdf', k, p)
A:jax._src.scipy.stats.bernoulli.(q, p)->promote_args_inexact('bernoulli.ppf', q, p)
jax._src.scipy.stats.bernoulli.cdf(k:ArrayLike,p:ArrayLike)->Array
jax._src.scipy.stats.bernoulli.logpmf(k:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.bernoulli.pmf(k:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.bernoulli.ppf(q:ArrayLike,p:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/logistic.py----------------------------------------
A:jax._src.scipy.stats.logistic.(x, loc, scale)->promote_args_inexact('logistic.cdf', x, loc, scale)
A:jax._src.scipy.stats.logistic.x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.logistic.two->_lax_const(x, 2)
A:jax._src.scipy.stats.logistic.half_x->jax.lax.div(x, two)
jax._src.scipy.stats.logistic.cdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.logistic.isf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.logistic.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.logistic.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.logistic.ppf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.logistic.sf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/truncnorm.py----------------------------------------
A:jax._src.scipy.stats.truncnorm.(a, b)->jax.numpy.broadcast_arrays(a, b)
A:jax._src.scipy.stats.truncnorm.out->jax.numpy.select([case_left, case_right, case_central], [mass_case_left(a, b), mass_case_right(a, b), mass_case_central(a, b)])
A:jax._src.scipy.stats.truncnorm.(x, a, b, loc, scale)->promote_args_inexact('truncnorm.logcdf', x, a, b, loc, scale)
A:jax._src.scipy.stats.truncnorm.val->jax.numpy.where(a >= b, jnp.nan, val)
A:jax._src.scipy.stats.truncnorm.x_scaled->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.truncnorm.(x, a, b)->jax.numpy.broadcast_arrays(x, a, b)
A:jax._src.scipy.stats.truncnorm.x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.truncnorm.logcdf->jax.numpy.where(a >= b, jnp.nan, logcdf)
jax._src.scipy.stats.truncnorm._log_diff(x,y)
jax._src.scipy.stats.truncnorm._log_gauss_mass(a,b)
jax._src.scipy.stats.truncnorm.cdf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.truncnorm.logcdf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.truncnorm.logpdf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.truncnorm.logsf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.truncnorm.pdf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.truncnorm.sf(x,a,b,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/expon.py----------------------------------------
A:jax._src.scipy.stats.expon.(x, loc, scale)->promote_args_inexact('expon.logpdf', x, loc, scale)
A:jax._src.scipy.stats.expon.log_scale->jax.lax.log(scale)
A:jax._src.scipy.stats.expon.linear_term->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.expon.log_probs->jax.lax.neg(lax.add(linear_term, log_scale))
jax._src.scipy.stats.expon.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.expon.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/_core.py----------------------------------------
A:jax._src.scipy.stats._core.ModeResult->namedtuple('ModeResult', ('mode', 'count'))
A:jax._src.scipy.stats._core.x->x.reshape(x.shape[0], math.prod(x.shape[1:])).reshape(x.shape[0], math.prod(x.shape[1:]))
A:jax._src.scipy.stats._core.output_shape->tuple((s for (i, s) in enumerate(input_shape) if i != axis))
A:jax._src.scipy.stats._core.(vals, counts)->vmap(_mode_helper, in_axes=1)(x)
A:jax._src.scipy.stats._core.axis->canonicalize_axis(axis, x.ndim)
A:jax._src.scipy.stats._core.a->jax.numpy.asarray(a)
A:jax._src.scipy.stats._core.arr->jax.numpy.ravel(a)
A:jax._src.scipy.stats._core.sorter->jax.numpy.argsort(arr)
A:jax._src.scipy.stats._core.inv->invert_permutation(sorter)
A:jax._src.scipy.stats._core.obs->jax.numpy.insert(arr[1:] != arr[:-1], 0, True)
jax._src.scipy.stats._core.invert_permutation(i:Array)->Array
jax._src.scipy.stats._core.mode(a:ArrayLike,axis:Optional[int]=0,nan_policy:str='propagate',keepdims:bool=False)->ModeResult
jax._src.scipy.stats._core.rankdata(a:ArrayLike,method:str='average',*,axis:Optional[int]=None,nan_policy:str='propagate')->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/vonmises.py----------------------------------------
A:jax._src.scipy.stats.vonmises.(x, kappa)->promote_args_inexact('vonmises.logpdf', x, kappa)
A:jax._src.scipy.stats.vonmises.zero->_lax_const(kappa, 0)
jax._src.scipy.stats.vonmises.logpdf(x:ArrayLike,kappa:ArrayLike)->Array
jax._src.scipy.stats.vonmises.pdf(x:ArrayLike,kappa:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/nbinom.py----------------------------------------
A:jax._src.scipy.stats.nbinom.(k, n, p, loc)->promote_args_inexact('nbinom.logpmf', k, n, p, loc)
A:jax._src.scipy.stats.nbinom.one->_lax_const(k, 1)
A:jax._src.scipy.stats.nbinom.y->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.nbinom.comb_term->jax.lax.sub(lax.sub(gammaln(lax.add(y, n)), gammaln(n)), gammaln(lax.add(y, one)))
A:jax._src.scipy.stats.nbinom.log_linear_term->jax.lax.add(xlogy(n, p), xlogy(y, lax.sub(one, p)))
A:jax._src.scipy.stats.nbinom.log_probs->jax.lax.add(comb_term, log_linear_term)
jax._src.scipy.stats.nbinom.logpmf(k:ArrayLike,n:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.nbinom.pmf(k:ArrayLike,n:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/wrapcauchy.py----------------------------------------
A:jax._src.scipy.stats.wrapcauchy.(x, c)->promote_args_inexact('wrapcauchy.logpdf', x, c)
jax._src.scipy.stats.wrapcauchy.logpdf(x:ArrayLike,c:ArrayLike)->Array
jax._src.scipy.stats.wrapcauchy.pdf(x:ArrayLike,c:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/poisson.py----------------------------------------
A:jax._src.scipy.stats.poisson.(k, mu, loc)->promote_args_inexact('poisson.logpmf', k, mu, loc)
A:jax._src.scipy.stats.poisson.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.poisson.x->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.poisson.p->gammaincc(jnp.floor(1 + x), mu)
jax._src.scipy.stats.poisson.cdf(k:ArrayLike,mu:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.poisson.logpmf(k:ArrayLike,mu:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.poisson.pmf(k:ArrayLike,mu:ArrayLike,loc:ArrayLike=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/multinomial.py----------------------------------------
A:jax._src.scipy.stats.multinomial.(p,)->promote_args_inexact('multinomial.logpmf', p)
A:jax._src.scipy.stats.multinomial.(x, n)->promote_args_numeric('multinomial.logpmf', x, n)
A:jax._src.scipy.stats.multinomial.x->x.astype(p.dtype).astype(p.dtype)
A:jax._src.scipy.stats.multinomial.n->n.astype(p.dtype).astype(p.dtype)
jax._src.scipy.stats.multinomial.logpmf(x:ArrayLike,n:ArrayLike,p:ArrayLike)->Array
jax._src.scipy.stats.multinomial.pmf(x:ArrayLike,n:ArrayLike,p:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/betabinom.py----------------------------------------
A:jax._src.scipy.stats.betabinom.(k, n, a, b, loc)->promote_args_inexact('betabinom.logpmf', k, n, a, b, loc)
A:jax._src.scipy.stats.betabinom.y->jax.lax.sub(lax.floor(k), loc)
A:jax._src.scipy.stats.betabinom.one->_lax_const(y, 1)
A:jax._src.scipy.stats.betabinom.zero->_lax_const(y, 0)
A:jax._src.scipy.stats.betabinom.combiln->jax.lax.neg(lax.add(lax.log1p(n), betaln(lax.add(lax.sub(n, y), one), lax.add(y, one))))
A:jax._src.scipy.stats.betabinom.beta_lns->jax.lax.sub(betaln(lax.add(y, a), lax.add(lax.sub(n, y), b)), betaln(a, b))
A:jax._src.scipy.stats.betabinom.log_probs->jax.numpy.where(y_cond, -jnp.inf, log_probs)
A:jax._src.scipy.stats.betabinom.y_cond->jax.numpy.logical_or(lax.lt(y, lax.neg(loc)), lax.gt(y, lax.sub(n, loc)))
A:jax._src.scipy.stats.betabinom.n_a_b_cond->jax.numpy.logical_or(jnp.logical_or(lax.lt(n, one), lax.lt(a, zero)), lax.lt(b, zero))
jax._src.scipy.stats.betabinom.logpmf(k:ArrayLike,n:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.betabinom.pmf(k:ArrayLike,n:ArrayLike,a:ArrayLike,b:ArrayLike,loc:ArrayLike=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/t.py----------------------------------------
A:jax._src.scipy.stats.t.(x, df, loc, scale)->promote_args_inexact('t.logpdf', x, df, loc, scale)
A:jax._src.scipy.stats.t.two->_lax_const(x, 2)
A:jax._src.scipy.stats.t.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.t.df_over_two->jax.lax.div(df, two)
A:jax._src.scipy.stats.t.df_plus_one_over_two->jax.lax.add(df_over_two, _lax_const(x, 0.5))
A:jax._src.scipy.stats.t.normalize_term_const->jax.lax.mul(lax.mul(scale, scale), _lax_const(x, np.pi))
A:jax._src.scipy.stats.t.normalize_term_tmp->jax.lax.div(lax.log(lax.mul(normalize_term_const, df)), two)
A:jax._src.scipy.stats.t.normalize_term->jax.lax.sub(lax.add(lax.lgamma(df_over_two), normalize_term_tmp), lax.lgamma(df_plus_one_over_two))
A:jax._src.scipy.stats.t.quadratic->jax.lax.div(lax.mul(scaled_x, scaled_x), df)
jax._src.scipy.stats.t.logpdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.t.pdf(x:ArrayLike,df:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/laplace.py----------------------------------------
A:jax._src.scipy.stats.laplace.(x, loc, scale)->promote_args_inexact('laplace.cdf', x, loc, scale)
A:jax._src.scipy.stats.laplace.two->_lax_const(x, 2)
A:jax._src.scipy.stats.laplace.linear_term->jax.lax.div(lax.abs(lax.sub(x, loc)), scale)
A:jax._src.scipy.stats.laplace.half->_lax_const(x, 0.5)
A:jax._src.scipy.stats.laplace.one->_lax_const(x, 1)
A:jax._src.scipy.stats.laplace.zero->_lax_const(x, 0)
A:jax._src.scipy.stats.laplace.diff->jax.lax.div(lax.sub(x, loc), scale)
jax._src.scipy.stats.laplace.cdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.laplace.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.laplace.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/geom.py----------------------------------------
A:jax._src.scipy.stats.geom.(k, p, loc)->promote_args_inexact('geom.logpmf', k, p, loc)
A:jax._src.scipy.stats.geom.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.geom.one->_lax_const(k, 1)
A:jax._src.scipy.stats.geom.x->jax.lax.sub(k, loc)
jax._src.scipy.stats.geom.logpmf(k:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.geom.pmf(k:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/gennorm.py----------------------------------------
A:jax._src.scipy.stats.gennorm.(x, p)->promote_args_inexact('gennorm.cdf', x, p)
jax._src.scipy.stats.gennorm.cdf(x:ArrayLike,p:ArrayLike)->Array
jax._src.scipy.stats.gennorm.logpdf(x:ArrayLike,p:ArrayLike)->Array
jax._src.scipy.stats.gennorm.pdf(x:ArrayLike,p:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/pareto.py----------------------------------------
A:jax._src.scipy.stats.pareto.(x, b, loc, scale)->promote_args_inexact('pareto.logpdf', x, b, loc, scale)
A:jax._src.scipy.stats.pareto.one->_lax_const(x, 1)
A:jax._src.scipy.stats.pareto.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.pareto.normalize_term->jax.lax.log(lax.div(scale, b))
A:jax._src.scipy.stats.pareto.log_probs->jax.lax.neg(lax.add(normalize_term, lax.mul(lax.add(b, one), lax.log(scaled_x))))
jax._src.scipy.stats.pareto.logpdf(x:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.pareto.pdf(x:ArrayLike,b:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/binom.py----------------------------------------
A:jax._src.scipy.stats.binom.(k, n, p, loc)->promote_args_inexact('binom.logpmf', k, n, p, loc)
A:jax._src.scipy.stats.binom.y->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.binom.comb_term->jax.lax.sub(gammaln(n + 1), lax.add(gammaln(y + 1), gammaln(n - y + 1)))
A:jax._src.scipy.stats.binom.log_linear_term->jax.lax.add(xlogy(y, p), xlog1py(lax.sub(n, y), lax.neg(p)))
A:jax._src.scipy.stats.binom.log_probs->jax.lax.add(comb_term, log_linear_term)
jax._src.scipy.stats.binom.logpmf(k:ArrayLike,n:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array
jax._src.scipy.stats.binom.pmf(k:ArrayLike,n:ArrayLike,p:ArrayLike,loc:ArrayLike=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/norm.py----------------------------------------
A:jax._src.scipy.stats.norm.(x, loc, scale)->promote_args_inexact('norm.sf', x, loc, scale)
A:jax._src.scipy.stats.norm.scale_sqrd->jax.lax.square(scale)
A:jax._src.scipy.stats.norm.log_normalizer->jax.lax.log(lax.mul(_lax_const(x, 2 * np.pi), scale_sqrd))
A:jax._src.scipy.stats.norm.quadratic->jax.lax.div(lax.square(lax.sub(x, loc)), scale_sqrd)
jax._src.scipy.stats.norm.cdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.isf(q:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.logcdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.logsf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.ppf(q:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.norm.sf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/dirichlet.py----------------------------------------
A:jax._src.scipy.stats.dirichlet.x_sum->jax.numpy.sum(x, axis=0)
A:jax._src.scipy.stats.dirichlet.one->_lax_const(x, 1)
A:jax._src.scipy.stats.dirichlet.x->jax.numpy.concatenate([x, lax.sub(one, x.sum(0, keepdims=True))], axis=0)
A:jax._src.scipy.stats.dirichlet.alpha->jax.lax.broadcast_in_dim(alpha, alpha.shape + (1,) * (x.ndim - 1), (0,))
A:jax._src.scipy.stats.dirichlet.log_probs->jax.lax.sub(jnp.sum(xlogy(lax.sub(alpha, one), x), axis=0), normalize_term)
jax._src.scipy.stats.dirichlet._is_simplex(x:Array)->Array
jax._src.scipy.stats.dirichlet._logpdf(x:Array,alpha:Array)->Array
jax._src.scipy.stats.dirichlet.logpdf(x:ArrayLike,alpha:ArrayLike)->Array
jax._src.scipy.stats.dirichlet.pdf(x:ArrayLike,alpha:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/gamma.py----------------------------------------
A:jax._src.scipy.stats.gamma.(x, a, loc, scale)->promote_args_inexact('gamma.sf', x, a, loc, scale)
A:jax._src.scipy.stats.gamma.one->_lax_const(x, 1)
A:jax._src.scipy.stats.gamma.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.gamma.log_linear_term->jax.lax.sub(xlogy(lax.sub(a, one), y), y)
A:jax._src.scipy.stats.gamma.shape_terms->jax.lax.add(gammaln(a), lax.log(scale))
A:jax._src.scipy.stats.gamma.log_probs->jax.lax.sub(log_linear_term, shape_terms)
jax._src.scipy.stats.gamma.cdf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.gamma.logcdf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.gamma.logpdf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.gamma.logsf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.gamma.pdf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.gamma.sf(x:ArrayLike,a:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/stats/uniform.py----------------------------------------
A:jax._src.scipy.stats.uniform.(x, loc, scale)->promote_args_inexact('uniform.logpdf', x, loc, scale)
A:jax._src.scipy.stats.uniform.log_probs->jax.lax.neg(lax.log(scale))
jax._src.scipy.stats.uniform.logpdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array
jax._src.scipy.stats.uniform.pdf(x:ArrayLike,loc:ArrayLike=0,scale:ArrayLike=1)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/optimize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/optimize/bfgs.py----------------------------------------
A:jax._src.scipy.optimize.bfgs._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.bfgs._einsum->partial(jnp.einsum, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.bfgs.initial_H->jax.numpy.eye(d, dtype=x0.dtype)
A:jax._src.scipy.optimize.bfgs.(f_0, g_0)->jax.value_and_grad(fun)(x0)
A:jax._src.scipy.optimize.bfgs.state->state._replace(status=status)._replace(status=status)
A:jax._src.scipy.optimize.bfgs.line_search_results->line_search(fun, state.x_k, p_k, old_fval=state.f_k, old_old_fval=state.old_old_fval, gfk=state.g_k, maxiter=line_search_maxiter)
A:jax._src.scipy.optimize.bfgs.rho_k->jax.numpy.reciprocal(_dot(y_k, s_k))
A:jax._src.scipy.optimize.bfgs.H_kp1->jax.numpy.where(jnp.isfinite(rho_k), H_kp1, state.H_k)
A:jax._src.scipy.optimize.bfgs.status->jax.numpy.where(state.converged, 0, jnp.where(state.k == maxiter, 1, jnp.where(state.failed, 2 + state.line_search_status, -1)))
jax._src.scipy.optimize.bfgs._BFGSResults(NamedTuple)
jax._src.scipy.optimize.bfgs.minimize_bfgs(fun:Callable,x0:jax.Array,maxiter:Optional[int]=None,norm=jnp.inf,gtol:float=1e-05,line_search_maxiter:int=10)->_BFGSResults


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/optimize/line_search.py----------------------------------------
A:jax._src.scipy.optimize.line_search._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.line_search.dtype->jax.numpy.result_type(a, fa, fpa, b, fb, c, fc)
A:jax._src.scipy.optimize.line_search.d1->jax.numpy.array([[dc ** 2, -db ** 2], [-dc ** 3, db ** 3]], dtype=dtype)
A:jax._src.scipy.optimize.line_search.d2->jax.numpy.array([fb - fa - C * db, fc - fa - C * dc], dtype=dtype)
A:jax._src.scipy.optimize.line_search.keys->new_dict.keys()
A:jax._src.scipy.optimize.line_search.state->jax.lax.while_loop(lambda state: ~state.done & (state.i <= maxiter) & ~state.failed, body, state)
A:jax._src.scipy.optimize.line_search.a->jax.numpy.minimum(state.a_hi, state.a_lo)
A:jax._src.scipy.optimize.line_search.b->jax.numpy.maximum(state.a_hi, state.a_lo)
A:jax._src.scipy.optimize.line_search.threshold->jax.numpy.where(jnp.finfo(dalpha).bits < 64, 1e-05, 1e-10)
A:jax._src.scipy.optimize.line_search.a_j_cubic->_cubicmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi, state.a_rec, state.phi_rec)
A:jax._src.scipy.optimize.line_search.a_j_quad->_quadmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi)
A:jax._src.scipy.optimize.line_search.a_j->jax.numpy.where(use_bisection, a_j_bisection, a_j)
A:jax._src.scipy.optimize.line_search.(phi_j, dphi_j, g_j)->restricted_func_and_grad(a_j)
A:jax._src.scipy.optimize.line_search.phi_j->phi_j.astype(state.phi_lo.dtype).astype(state.phi_lo.dtype)
A:jax._src.scipy.optimize.line_search.dphi_j->dphi_j.astype(state.dphi_lo.dtype).astype(state.dphi_lo.dtype)
A:jax._src.scipy.optimize.line_search.g_j->g_j.astype(state.g_star.dtype).astype(state.g_star.dtype)
A:jax._src.scipy.optimize.line_search.(xk, pk)->promote_dtypes_inexact(xk, pk)
A:jax._src.scipy.optimize.line_search.t->jax.numpy.array(t, dtype=pk.dtype)
A:jax._src.scipy.optimize.line_search.(phi, g)->jax.value_and_grad(f)(xk + t * pk)
A:jax._src.scipy.optimize.line_search.dphi->jax.numpy.real(_dot(g, pk))
A:jax._src.scipy.optimize.line_search.(phi_0, dphi_0, gfk)->restricted_func_and_grad(0)
A:jax._src.scipy.optimize.line_search.dphi_0->jax.numpy.real(_dot(gfk, pk))
A:jax._src.scipy.optimize.line_search.start_value->jax.numpy.where(candidate_start_value > 1, 1.0, candidate_start_value)
A:jax._src.scipy.optimize.line_search.a_i->jax.numpy.where(state.i == 1, start_value, state.a_i1 * 2.0)
A:jax._src.scipy.optimize.line_search.(phi_i, dphi_i, g_i)->restricted_func_and_grad(a_i)
A:jax._src.scipy.optimize.line_search.zoom1->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, state.a_i1, state.phi_i1, state.dphi_i1, a_i, phi_i, dphi_i, gfk, ~star_to_zoom1)
A:jax._src.scipy.optimize.line_search.zoom2->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, a_i, phi_i, dphi_i, state.a_i1, state.phi_i1, state.dphi_i1, gfk, ~star_to_zoom2)
A:jax._src.scipy.optimize.line_search.status->jax.numpy.where(state.failed, jnp.array(1), jnp.where(state.i > maxiter, jnp.array(3), jnp.array(0)))
A:jax._src.scipy.optimize.line_search.alpha_k->jax.numpy.where((jnp.finfo(alpha_k.dtype).bits != 64) & (jnp.abs(alpha_k) < 1e-08), jnp.sign(alpha_k) * 1e-08, alpha_k)
A:jax._src.scipy.optimize.line_search.results->_LineSearchResults(failed=state.failed | ~state.done, nit=state.i - 1, nfev=state.nfev, ngev=state.ngev, k=state.i, a_k=alpha_k, f_k=state.phi_star, g_k=state.g_star, status=status)
jax._src.scipy.optimize.line_search._LineSearchResults(NamedTuple)
jax._src.scipy.optimize.line_search._LineSearchState(NamedTuple)
jax._src.scipy.optimize.line_search._ZoomState(NamedTuple)
jax._src.scipy.optimize.line_search._binary_replace(replace_bit,original_dict,new_dict,keys=None)
jax._src.scipy.optimize.line_search._cubicmin(a,fa,fpa,b,fb,c,fc)
jax._src.scipy.optimize.line_search._quadmin(a,fa,fpa,b,fb)
jax._src.scipy.optimize.line_search._zoom(restricted_func_and_grad,wolfe_one,wolfe_two,a_lo,phi_lo,dphi_lo,a_hi,phi_hi,dphi_hi,g_0,pass_through)
jax._src.scipy.optimize.line_search.line_search(f,xk,pk,old_fval=None,old_old_fval=None,gfk=None,c1=0.0001,c2=0.9,maxiter=20)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/optimize/_lbfgs.py----------------------------------------
A:jax._src.scipy.optimize._lbfgs._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize._lbfgs.d->len(x0)
A:jax._src.scipy.optimize._lbfgs.dtype->jax.numpy.dtype(x0)
A:jax._src.scipy.optimize._lbfgs.(f_0, g_0)->jax.value_and_grad(fun)(x0)
A:jax._src.scipy.optimize._lbfgs.state_initial->LBFGSResults(converged=False, failed=False, k=0, nfev=1, ngev=1, x_k=x0, f_k=f_0, g_k=g_0, s_history=jnp.zeros((maxcor, d), dtype=dtype), y_history=jnp.zeros((maxcor, d), dtype=dtype), rho_history=jnp.zeros((maxcor,), dtype=dtype), gamma=1.0, status=0, ls_status=0)
A:jax._src.scipy.optimize._lbfgs.p_k->_two_loop_recursion(state)
A:jax._src.scipy.optimize._lbfgs.ls_results->line_search(f=fun, xk=state.x_k, pk=p_k, old_fval=state.f_k, gfk=state.g_k, maxiter=maxls)
A:jax._src.scipy.optimize._lbfgs.rho_k_inv->jax.numpy.real(_dot(y_k, s_k))
A:jax._src.scipy.optimize._lbfgs.rho_k->jax.numpy.reciprocal(rho_k_inv).astype(y_k.dtype)
A:jax._src.scipy.optimize._lbfgs.status->jax.numpy.where(ls_results.failed, 5, status)
A:jax._src.scipy.optimize._lbfgs.state->state._replace(converged=converged, failed=(status > 0) & ~converged, k=state.k + 1, nfev=state.nfev + ls_results.nfev, ngev=state.ngev + ls_results.ngev, x_k=x_kp1.astype(state.x_k.dtype), f_k=f_kp1.astype(state.f_k.dtype), g_k=g_kp1.astype(state.g_k.dtype), s_history=_update_history_vectors(history=state.s_history, new=s_k), y_history=_update_history_vectors(history=state.y_history, new=y_k), rho_history=_update_history_scalars(history=state.rho_history, new=rho_k), gamma=gamma.astype(state.g_k.dtype), status=jnp.where(converged, 0, status), ls_status=ls_results.status)._replace(converged=converged, failed=(status > 0) & ~converged, k=state.k + 1, nfev=state.nfev + ls_results.nfev, ngev=state.ngev + ls_results.ngev, x_k=x_kp1.astype(state.x_k.dtype), f_k=f_kp1.astype(state.f_k.dtype), g_k=g_kp1.astype(state.g_k.dtype), s_history=_update_history_vectors(history=state.s_history, new=s_k), y_history=_update_history_vectors(history=state.y_history, new=y_k), rho_history=_update_history_scalars(history=state.rho_history, new=rho_k), gamma=gamma.astype(state.g_k.dtype), status=jnp.where(converged, 0, status), ls_status=ls_results.status)
A:jax._src.scipy.optimize._lbfgs.his_size->len(state.rho_history)
A:jax._src.scipy.optimize._lbfgs.curr_size->jax.numpy.where(state.k < his_size, state.k, his_size)
A:jax._src.scipy.optimize._lbfgs.a_his->jax.numpy.zeros_like(state.rho_history)
A:jax._src.scipy.optimize._lbfgs._a_his->_a_his.at[i].set(a_i).at[i].set(a_i)
A:jax._src.scipy.optimize._lbfgs.(q, a_his)->jax.lax.fori_loop(0, curr_size, body_fun1, (q, a_his))
A:jax._src.scipy.optimize._lbfgs.q->jax.lax.fori_loop(0, curr_size, body_fun2, q)
jax._src.scipy.optimize._lbfgs.LBFGSResults(NamedTuple)
jax._src.scipy.optimize._lbfgs._minimize_lbfgs(fun:Callable,x0:Array,maxiter:Optional[float]=None,norm=jnp.inf,maxcor:int=10,ftol:float=2.220446049250313e-09,gtol:float=1e-05,maxfun:Optional[float]=None,maxgrad:Optional[float]=None,maxls:int=20)
jax._src.scipy.optimize._lbfgs._two_loop_recursion(state:LBFGSResults)
jax._src.scipy.optimize._lbfgs._update_history_scalars(history,new)
jax._src.scipy.optimize._lbfgs._update_history_vectors(history,new)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/optimize/minimize.py----------------------------------------
A:jax._src.scipy.optimize.minimize.results->_minimize_lbfgs(fun_with_args, x0, **options)
jax._src.scipy.optimize.minimize.OptimizeResults(NamedTuple)
jax._src.scipy.optimize.minimize.minimize(fun:Callable,x0:jax.Array,args:tuple=(),*,method:str,tol:Optional[float]=None,options:Optional[Mapping[str,Any]]=None)->OptimizeResults


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/scipy/interpolate/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lib/__init__.py----------------------------------------
A:jax._src.lib.__init__.version_regex->re.compile('[0-9]+(?:\\.[0-9]+)*')
A:jax._src.lib.__init__.m->re.compile('[0-9]+(?:\\.[0-9]+)*').match(v)
A:jax._src.lib.__init__._jax_version->_parse_version(jax_version)
A:jax._src.lib.__init__._minimum_jaxlib_version->_parse_version(minimum_jaxlib_version)
A:jax._src.lib.__init__._jaxlib_version->_parse_version(jaxlib_version)
A:jax._src.lib.__init__.version->check_jaxlib_version(jax_version=jax.version.__version__, jaxlib_version=jaxlib.version.__version__, minimum_jaxlib_version=jax.version._minimum_jaxlib_version)
A:jax._src.lib.__init__.cuda_path->_cuda_path()
jax._src.lib.__init__._cuda_path()->Optional[str]
jax._src.lib.__init__._xla_gc_callback(*args)
jax._src.lib.__init__.check_jaxlib_version(jax_version:str,jaxlib_version:str,minimum_jaxlib_version:str)->tuple[int, ...]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lib/mlir/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lib/mlir/dialects/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/setops.py----------------------------------------
A:jax._src.numpy.setops.ar1_flat->ravel(ar1)
A:jax._src.numpy.setops.ar2_flat->ravel(ar2)
A:jax._src.numpy.setops.ar1->ravel(ar1)
A:jax._src.numpy.setops.size->jax._src.core.concrete_or_error(operator.index, size, 'The error arose for the size argument of jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.arr1->cast(Array, unique(arr1, size=size and arr1.size))
A:jax._src.numpy.setops.fill_value->asarray(fill_value, dtype=result.dtype)
A:jax._src.numpy.setops.mask->zeros(size, dtype=bool)
A:jax._src.numpy.setops.ar2->ravel(ar2)
A:jax._src.numpy.setops.out->unique(concatenate((ar1, ar2), axis=None), size=size, fill_value=fill_value)
A:jax._src.numpy.setops.aux->where(isnan(aux), _lax_const(aux, np.nan), aux)
A:jax._src.numpy.setops.flag->concatenate((array([True]), aux[1:] != aux[:-1], array([True])))
A:jax._src.numpy.setops.ar->jax._src.core.concrete_or_error(None, ar, 'The error arose for the first argument of jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.iota->jax.lax.broadcasted_iota(np.int64, np.shape(ar), dimension=0)
A:jax._src.numpy.setops.(aux, indices)->jax.lax.sort_key_val(ar, iota)
A:jax._src.numpy.setops.(ar1, ind1)->unique(ar1, return_index=True)
A:jax._src.numpy.setops.(ar2, ind2)->unique(ar2, return_index=True)
A:jax._src.numpy.setops.(aux, mask, aux_sort_indices)->_intersect1d_sorted_mask(ar1, ar2, return_indices)
A:jax._src.numpy.setops.(aux, mask)->_intersect1d_sorted_mask(ar1, ar2, return_indices)
A:jax._src.numpy.setops.result->moveaxis(result, 0, axis)
A:jax._src.numpy.setops.perm->lexsort(aux.reshape(size, math.prod(out_shape)).T[::-1])
A:jax._src.numpy.setops.(aux, mask, perm)->_unique_sorted_mask(ar, axis)
A:jax._src.numpy.setops.ind->jax._src.core.concrete_or_error(None, mask, 'The error arose in jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.valid->jax.lax.expand_dims(arange(size) < mask.sum(), tuple(range(1, result.ndim)))
A:jax._src.numpy.setops.inv_idx->zeros(ar.shape[axis], dtype=int)
A:jax._src.numpy.setops.idx->idx.at[1:].set(where(idx[1:], idx[1:], mask.size)).at[1:].set(where(idx[1:], idx[1:], mask.size))
A:jax._src.numpy.setops.arr->arr.flatten().flatten()
jax._src.numpy.setops._in1d(ar1:ArrayLike,ar2:ArrayLike,invert:bool)->Array
jax._src.numpy.setops._intersect1d_sorted_mask(ar1:ArrayLike,ar2:ArrayLike,return_indices:bool=False)->tuple[Array, ...]
jax._src.numpy.setops._unique(ar:Array,axis:int,return_index:bool=False,return_inverse:bool=False,return_counts:bool=False,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None,return_true_size:bool=False)->Union[Array, tuple[Array, ...]]
jax._src.numpy.setops._unique_sorted_mask(ar:Array,axis:int)->tuple[Array, Array, Array]
jax._src.numpy.setops.in1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,invert:bool=False)->Array
jax._src.numpy.setops.intersect1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,return_indices:bool=False)->Union[Array, tuple[Array, Array, Array]]
jax._src.numpy.setops.isin(element:ArrayLike,test_elements:ArrayLike,assume_unique:bool=False,invert:bool=False)->Array
jax._src.numpy.setops.setdiff1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)->Array
jax._src.numpy.setops.setxor1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False)->Array
jax._src.numpy.setops.union1d(ar1:ArrayLike,ar2:ArrayLike,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)->Array
jax._src.numpy.setops.unique(ar:ArrayLike,return_index:bool=False,return_inverse:bool=False,return_counts:bool=False,axis:Optional[int]=None,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/fft.py----------------------------------------
A:jax._src.numpy.fft.arr->jax._src.numpy.lax_numpy.asarray(a)
A:jax._src.numpy.fft.s->tuple(map(operator.index, s))
A:jax._src.numpy.fft.axes->tuple(range(x.ndim))
A:jax._src.numpy.fft.in_s->list(arr.shape)
A:jax._src.numpy.fft.transformed->jax._src.numpy.lax_numpy.moveaxis(transformed, axes, orig_axes)
A:jax._src.numpy.fft.conj_a->jax._src.numpy.ufuncs.conj(a)
A:jax._src.numpy.fft.output->_fft_core_1d('ihfft', xla_client.FftType.RFFT, arr, n=n, axis=axis, norm=norm)
A:jax._src.numpy.fft.k->jax._src.numpy.lax_numpy.arange(0, (n - 1) // 2 + 1, dtype=dtype)
A:jax._src.numpy.fft.x->jax._src.numpy.lax_numpy.asarray(x)
jax._src.numpy.fft._axis_check_1d(func_name:str,axis:Optional[int])
jax._src.numpy.fft._fft_core(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,s:Optional[Shape],axes:Optional[Sequence[int]],norm:Optional[str])->Array
jax._src.numpy.fft._fft_core_1d(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,n:Optional[int],axis:Optional[int],norm:Optional[str])->Array
jax._src.numpy.fft._fft_core_2d(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,s:Optional[Shape],axes:Sequence[int],norm:Optional[str])->Array
jax._src.numpy.fft._fft_norm(s:Array,func_name:str,norm:str)->Array
jax._src.numpy.fft.fft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.fft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.fftfreq(n:int,d:ArrayLike=1.0,*,dtype=None)->Array
jax._src.numpy.fft.fftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.fftshift(x:ArrayLike,axes:Union[None,int,Sequence[int]]=None)->Array
jax._src.numpy.fft.hfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifftshift(x:ArrayLike,axes:Union[None,int,Sequence[int]]=None)->Array
jax._src.numpy.fft.ihfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfftfreq(n:int,d:ArrayLike=1.0,*,dtype=None)->Array
jax._src.numpy.fft.rfftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/polynomial.py----------------------------------------
A:jax._src.numpy.polynomial.A->A.at[0, :].set(-p[1:] / p[0]).at[0, :].set(-p[1:] / p[0])
A:jax._src.numpy.polynomial.p->_where(len(p) == num_leading_zeros, 1.0, p)
A:jax._src.numpy.polynomial.roots->_roots_no_zeros(roll(p, -num_leading_zeros))
A:jax._src.numpy.polynomial.p_arr->atleast_1d(*promote_dtypes_inexact(p))
A:jax._src.numpy.polynomial.num_leading_zeros->jax._src.core.concrete_or_error(int, num_leading_zeros, 'The error occurred in the jnp.roots() function. To use this within a JIT-compiled context, pass strip_zeros=False, but be aware that leading zeros will be result in some returned roots being set to NaN.')
A:jax._src.numpy.polynomial.deg->jax._src.core.concrete_or_error(int, deg, 'deg must be int')
A:jax._src.numpy.polynomial.rcond->jax._src.core.concrete_or_error(float, rcond, 'rcond must be float')
A:jax._src.numpy.polynomial.lhs->vander(x, order)
A:jax._src.numpy.polynomial.(w,)->promote_dtypes_inexact(w)
A:jax._src.numpy.polynomial.scale->sqrt((lhs * lhs).sum(axis=0))
A:jax._src.numpy.polynomial.(c, resids, rank, s)->jax._src.numpy.linalg.lstsq(lhs, rhs, rcond)
A:jax._src.numpy.polynomial.Vbase->jax._src.numpy.linalg.inv(dot(lhs.T, lhs))
A:jax._src.numpy.polynomial.(seq_of_zeros,)->promote_dtypes_inexact(seq_of_zeros)
A:jax._src.numpy.polynomial.seq_of_zeros->jax._src.numpy.linalg.eigvals(seq_of_zeros)
A:jax._src.numpy.polynomial.a->convolve(a, array([1, -seq_of_zeros[k]], dtype=dt), mode='full')
A:jax._src.numpy.polynomial.(p, x)->promote_dtypes_inexact(p, x)
A:jax._src.numpy.polynomial.shape->jax.lax.broadcast_shapes(p.shape[1:], x.shape)
A:jax._src.numpy.polynomial.y->jax.lax.full_like(x, 0, shape=shape, dtype=x.dtype)
A:jax._src.numpy.polynomial.(y, _)->jax.lax.scan(lambda y, p: (y * x + p, None), y, p, unroll=unroll)
A:jax._src.numpy.polynomial.(a1, a2)->promote_dtypes(a1, a2)
A:jax._src.numpy.polynomial.m->jax._src.core.concrete_or_error(operator.index, m, "'m' argument of jnp.polyder")
A:jax._src.numpy.polynomial.(p, k_arr)->promote_dtypes_inexact(p, k)
A:jax._src.numpy.polynomial.k_arr->full((m,), k_arr[0])
A:jax._src.numpy.polynomial.(p,)->promote_dtypes_inexact(p)
A:jax._src.numpy.polynomial.coeff->(arange(m, len(p), dtype=p.dtype)[np.newaxis] - arange(m, dtype=p.dtype)[:, np.newaxis]).prod(0)
A:jax._src.numpy.polynomial.(a1_arr, a2_arr)->promote_dtypes_inexact(a1, a2)
A:jax._src.numpy.polynomial.a1_arr->asarray([0], dtype=a2_arr.dtype)
A:jax._src.numpy.polynomial.a2_arr->asarray([0], dtype=a1_arr.dtype)
A:jax._src.numpy.polynomial.(u_arr, v_arr)->promote_dtypes_inexact(u, v)
A:jax._src.numpy.polynomial.q->q.at[k].set(d).at[k].set(d)
A:jax._src.numpy.polynomial.u_arr->trim_zeros_tol(u_arr, tol=sqrt(finfo(u_arr.dtype).eps), trim='f')
jax._src.numpy.polynomial._roots_no_zeros(p:Array)->Array
jax._src.numpy.polynomial._roots_with_zeros(p:Array,num_leading_zeros:int)->Array
jax._src.numpy.polynomial.poly(seq_of_zeros:Array)->Array
jax._src.numpy.polynomial.polyadd(a1:Array,a2:Array)->Array
jax._src.numpy.polynomial.polyder(p:Array,m:int=1)->Array
jax._src.numpy.polynomial.polydiv(u:ArrayLike,v:ArrayLike,*,trim_leading_zeros:bool=False)->tuple[Array, Array]
jax._src.numpy.polynomial.polyfit(x:Array,y:Array,deg:int,rcond:Optional[float]=None,full:bool=False,w:Optional[Array]=None,cov:bool=False)->Union[Array, tuple[Array, ...]]
jax._src.numpy.polynomial.polyint(p:Array,m:int=1,k:Optional[int]=None)->Array
jax._src.numpy.polynomial.polymul(a1:ArrayLike,a2:ArrayLike,*,trim_leading_zeros:bool=False)->Array
jax._src.numpy.polynomial.polysub(a1:Array,a2:Array)->Array
jax._src.numpy.polynomial.polyval(p:Array,x:Array,*,unroll:int=16)->Array
jax._src.numpy.polynomial.roots(p:ArrayLike,*,strip_zeros:bool=True)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/ufuncs.py----------------------------------------
A:jax._src.numpy.ufuncs.fn->jit(fn, inline=True)
A:jax._src.numpy.ufuncs.doc->dedent('\n\n'.join(lax_fn.__doc__.split('\n\n')[1:])).strip()
A:jax._src.numpy.ufuncs.(x1, x2)->promote_dtypes_inexact(x1, x2)
A:jax._src.numpy.ufuncs.rx->jax._src.lax.lax.real(x1)
A:jax._src.numpy.ufuncs.ry->jax._src.lax.lax.real(x2)
A:jax._src.numpy.ufuncs.fabs->_one_to_one_unop(np.fabs, lax.abs, True)
A:jax._src.numpy.ufuncs.bitwise_not->_one_to_one_unop(np.bitwise_not, lax.bitwise_not)
A:jax._src.numpy.ufuncs.invert->_one_to_one_unop(np.invert, lax.bitwise_not)
A:jax._src.numpy.ufuncs.negative->_one_to_one_unop(np.negative, lax.neg)
A:jax._src.numpy.ufuncs.positive->_one_to_one_unop(np.positive, lambda x: lax.asarray(x))
A:jax._src.numpy.ufuncs.floor->_one_to_one_unop(np.floor, lax.floor, True)
A:jax._src.numpy.ufuncs.ceil->_one_to_one_unop(np.ceil, lax.ceil, True)
A:jax._src.numpy.ufuncs.exp->_one_to_one_unop(np.exp, lax.exp, True)
A:jax._src.numpy.ufuncs.log->_one_to_one_unop(np.log, lax.log, True)
A:jax._src.numpy.ufuncs.expm1->_one_to_one_unop(np.expm1, lax.expm1, True)
A:jax._src.numpy.ufuncs.log1p->_one_to_one_unop(np.log1p, lax.log1p, True)
A:jax._src.numpy.ufuncs.sin->_one_to_one_unop(np.sin, lax.sin, True)
A:jax._src.numpy.ufuncs.cos->_one_to_one_unop(np.cos, lax.cos, True)
A:jax._src.numpy.ufuncs.tan->_one_to_one_unop(np.tan, lax.tan, True)
A:jax._src.numpy.ufuncs.arcsin->_one_to_one_unop(np.arcsin, lax.asin, True)
A:jax._src.numpy.ufuncs.arccos->_one_to_one_unop(np.arccos, lax.acos, True)
A:jax._src.numpy.ufuncs.arctan->_one_to_one_unop(np.arctan, lax.atan, True)
A:jax._src.numpy.ufuncs.sinh->_one_to_one_unop(np.sinh, lax.sinh, True)
A:jax._src.numpy.ufuncs.cosh->_one_to_one_unop(np.cosh, lax.cosh, True)
A:jax._src.numpy.ufuncs.arcsinh->_one_to_one_unop(np.arcsinh, lax.asinh, True)
A:jax._src.numpy.ufuncs.tanh->_one_to_one_unop(np.tanh, lax.tanh, True)
A:jax._src.numpy.ufuncs.arctanh->_one_to_one_unop(np.arctanh, lax.atanh, True)
A:jax._src.numpy.ufuncs.sqrt->_one_to_one_unop(np.sqrt, lax.sqrt, True)
A:jax._src.numpy.ufuncs.cbrt->_one_to_one_unop(np.cbrt, lax.cbrt, True)
A:jax._src.numpy.ufuncs.add->_maybe_bool_binop(np.add, lax.add, lax.bitwise_or)
A:jax._src.numpy.ufuncs.bitwise_and->_one_to_one_binop(np.bitwise_and, lax.bitwise_and)
A:jax._src.numpy.ufuncs.bitwise_or->_one_to_one_binop(np.bitwise_or, lax.bitwise_or)
A:jax._src.numpy.ufuncs.bitwise_xor->_one_to_one_binop(np.bitwise_xor, lax.bitwise_xor)
A:jax._src.numpy.ufuncs.left_shift->_one_to_one_binop(np.left_shift, lax.shift_left, promote_to_numeric=True)
A:jax._src.numpy.ufuncs.equal->_one_to_one_binop(np.equal, lax.eq)
A:jax._src.numpy.ufuncs.multiply->_maybe_bool_binop(np.multiply, lax.mul, lax.bitwise_and)
A:jax._src.numpy.ufuncs.not_equal->_one_to_one_binop(np.not_equal, lax.ne)
A:jax._src.numpy.ufuncs.subtract->_one_to_one_binop(np.subtract, lax.sub)
A:jax._src.numpy.ufuncs.arctan2->_one_to_one_binop(np.arctan2, lax.atan2, True)
A:jax._src.numpy.ufuncs.minimum->_one_to_one_binop(np.minimum, lax.min)
A:jax._src.numpy.ufuncs.maximum->_one_to_one_binop(np.maximum, lax.max)
A:jax._src.numpy.ufuncs.float_power->_one_to_one_binop(np.float_power, lax.pow, True)
A:jax._src.numpy.ufuncs.nextafter->_one_to_one_binop(np.nextafter, lax.nextafter, True, True)
A:jax._src.numpy.ufuncs.greater_equal->_comparison_op(np.greater_equal, lax.ge)
A:jax._src.numpy.ufuncs.greater->_comparison_op(np.greater, lax.gt)
A:jax._src.numpy.ufuncs.less_equal->_comparison_op(np.less_equal, lax.le)
A:jax._src.numpy.ufuncs.less->_comparison_op(np.less, lax.lt)
A:jax._src.numpy.ufuncs.out->jax._src.lax.lax.add(amax, lax.div(lax.log1p(exp2(delta)), _constant_like(x1, np.log(2))))
A:jax._src.numpy.ufuncs.dt->jax._src.dtypes.dtype(x)
A:jax._src.numpy.ufuncs.abs->_wraps(np.abs, module='numpy')(absolute)
A:jax._src.numpy.ufuncs.dtype->jax._src.dtypes.dtype(x)
A:jax._src.numpy.ufuncs.re->jax._src.lax.lax.real(x)
A:jax._src.numpy.ufuncs.quotient->jax._src.lax.lax.div(x1, x2)
A:jax._src.numpy.ufuncs.select->logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
A:jax._src.numpy.ufuncs.x1r->jax._src.lax.lax.real(x1)
A:jax._src.numpy.ufuncs.x1i->jax._src.lax.lax.imag(x1)
A:jax._src.numpy.ufuncs.x2r->jax._src.lax.lax.real(x2)
A:jax._src.numpy.ufuncs.x2i->jax._src.lax.lax.imag(x2)
A:jax._src.numpy.ufuncs.which->jax._src.lax.lax.ge(lax.abs(x2r), lax.abs(x2i))
A:jax._src.numpy.ufuncs.rat1->_where(which, lax.full_like(x2i, 1), lax.div(x2r, x2i))
A:jax._src.numpy.ufuncs.rat2->_where(which, lax.div(x2i, x2r), _lax_const(x2i, 1))
A:jax._src.numpy.ufuncs.mod->_wraps(np.mod, module='numpy')(remainder)
A:jax._src.numpy.ufuncs.div->jax._src.lax.lax.select(ind, div - _constant_like(div, 1), div)
A:jax._src.numpy.ufuncs.ind->jax._src.lax.lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
A:jax._src.numpy.ufuncs.x2->jax._src.lax.lax.abs(x2)
A:jax._src.numpy.ufuncs.(x1,)->promote_dtypes_numeric(x1)
A:jax._src.numpy.ufuncs.(x1_, x2_)->promote_args_numeric('power', x1, x2)
A:jax._src.numpy.ufuncs.zero->_lax_const(x1, 0)
A:jax._src.numpy.ufuncs.one->_constant_like(x2, 1)
A:jax._src.numpy.ufuncs.acc->_where(lax.bitwise_and(x2, one), lax.mul(acc, x1), acc)
A:jax._src.numpy.ufuncs.x1->jax._src.lax.lax.abs(x1)
A:jax._src.numpy.ufuncs.amax->jax._src.lax.lax.max(x1, x2)
A:jax._src.numpy.ufuncs.delta->jax._src.lax.lax.sub(lax.add(x1, x2), lax.mul(amax, _constant_like(amax, 2)))
A:jax._src.numpy.ufuncs.a->_constant_like(x, _a)
A:jax._src.numpy.ufuncs.two_a->_constant_like(x, 2 * _a)
A:jax._src.numpy.ufuncs.rem->jax._src.lax.lax.select(lax.lt(rem, zero), lax.add(rem, two_a), rem)
A:jax._src.numpy.ufuncs.(x1, x2, t1, t2)->promote_args_inexact('logaddexp2_jvp', x1, x2, t1, t2)
A:jax._src.numpy.ufuncs.primal_out->logaddexp2(x1, x2)
A:jax._src.numpy.ufuncs.tangent_out->jax._src.lax.lax.add(lax.mul(t1, exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))), lax.mul(t2, exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))
A:jax._src.numpy.ufuncs.(x,)->promote_dtypes_inexact(x)
A:jax._src.numpy.ufuncs.info->jax._src.dtypes.finfo(dtype)
A:jax._src.numpy.ufuncs.x->_where(overflow_cond, lax.sign(x1) * lax.full_like(x, np.inf), x)
A:jax._src.numpy.ufuncs.x1_dtype->jax._src.dtypes.dtype(x1)
A:jax._src.numpy.ufuncs.x2_dtype->jax._src.dtypes.dtype(x2)
A:jax._src.numpy.ufuncs.(x, e)->_normalize_float(x1)
A:jax._src.numpy.ufuncs.underflow_cond->less(x2, -(bias + info.nmant))
A:jax._src.numpy.ufuncs.overflow_cond->greater(x2, bias)
A:jax._src.numpy.ufuncs.m->_where(cond, m / (1 << info.nmant), m)
A:jax._src.numpy.ufuncs.cond->less(x2, -bias + 1)
A:jax._src.numpy.ufuncs.trunc_mod->jax._src.lax.lax.rem(x1, x2)
A:jax._src.numpy.ufuncs.trunc_mod_not_zero->jax._src.lax.lax.ne(trunc_mod, zero)
A:jax._src.numpy.ufuncs.do_plus->jax._src.lax.lax.bitwise_and(lax.ne(lax.lt(trunc_mod, zero), lax.lt(x2, zero)), trunc_mod_not_zero)
A:jax._src.numpy.ufuncs.whole->_where(lax.ge(x, lax._zero(x)), floor(x), ceil(x))
A:jax._src.numpy.ufuncs.im->jax._src.lax.lax.imag(x)
A:jax._src.numpy.ufuncs.eq_zero->jax._src.lax.lax.eq(x, _lax_const(x, 0))
A:jax._src.numpy.ufuncs.pi_x->jax._src.lax.lax.mul(_lax_const(x, np.pi), x)
A:jax._src.numpy.ufuncs.safe_pi_x->_where(eq_zero, _lax_const(x, 1), pi_x)
jax._src.numpy.ufuncs._comparison_op(numpy_fn:Callable[...,Any],lax_fn:BinOp)->BinOp
jax._src.numpy.ufuncs._constant_like(x,const)
jax._src.numpy.ufuncs._float_divmod(x1:ArrayLike,x2:ArrayLike)->tuple[Array, Array]
jax._src.numpy.ufuncs._isposneginf(infinity:float,x:ArrayLike,out)->Array
jax._src.numpy.ufuncs._logaddexp2_jvp(primals,tangents)
jax._src.numpy.ufuncs._logaddexp_jvp(primals,tangents)
jax._src.numpy.ufuncs._logical_op(np_op:Callable[...,Any],bitwise_op:Union[UnOp,BinOp])->Union[UnOp, BinOp]
jax._src.numpy.ufuncs._maybe_bool_binop(numpy_fn:Callable[...,Any],lax_fn:BinOp,bool_lax_fn:BinOp,lax_doc:bool=False)->BinOp
jax._src.numpy.ufuncs._normalize_float(x)
jax._src.numpy.ufuncs._one_to_one_binop(numpy_fn:Callable[...,Any],lax_fn:BinOp,promote_to_inexact:bool=False,lax_doc:bool=False,promote_to_numeric:bool=False)->BinOp
jax._src.numpy.ufuncs._one_to_one_unop(numpy_fn:Callable[...,Any],lax_fn:UnOp,promote_to_inexact:bool=False,lax_doc:bool=False)->UnOp
jax._src.numpy.ufuncs._pow_int_int(x1,x2)
jax._src.numpy.ufuncs._power(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.numpy.ufuncs._replace_inf(x:ArrayLike)->Array
jax._src.numpy.ufuncs._sinc_maclaurin(k,x)
jax._src.numpy.ufuncs._sinc_maclaurin_jvp(k,primals,tangents)
jax._src.numpy.ufuncs._wrap_between(x,_a)
jax._src.numpy.ufuncs.absolute(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.arccosh(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.bitwise_count(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.conjugate(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.copysign(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.deg2rad(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.divmod(x1:ArrayLike,x2:ArrayLike,/)->tuple[Array, Array]
jax._src.numpy.ufuncs.exp2(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.floor_divide(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.fmod(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.frexp(x:ArrayLike,/)->tuple[Array, Array]
jax._src.numpy.ufuncs.heaviside(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.hypot(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.imag(val:ArrayLike,/)->Array
jax._src.numpy.ufuncs.isfinite(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.isinf(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.isnan(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.ldexp(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.log10(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.log2(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.logaddexp(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.logaddexp2(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.modf(x:ArrayLike,/,out=None)->tuple[Array, Array]
jax._src.numpy.ufuncs.power(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.rad2deg(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.real(val:ArrayLike,/)->Array
jax._src.numpy.ufuncs.reciprocal(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.remainder(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.right_shift(x1:ArrayLike,x2:ArrayLike,/)->Array
jax._src.numpy.ufuncs.rint(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.sign(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.signbit(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.sinc(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.square(x:ArrayLike,/)->Array
jax._src.numpy.ufuncs.true_divide(x1:ArrayLike,x2:ArrayLike,/)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/linalg.py----------------------------------------
A:jax._src.numpy.linalg.(a,)->promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.numpy.linalg.(w, v)->jax._src.lax.linalg.eig(a, compute_left_eigenvectors=False)
A:jax._src.numpy.linalg.s->jax._src.numpy.lax_numpy.empty(0, dtype=a.dtype)
A:jax._src.numpy.linalg.sign->jax._src.numpy.lax_numpy.asarray(-2 * (parity % 2) + 1, dtype=dtype)
A:jax._src.numpy.linalg.idxs->jax.lax.rev(idxs, dimensions=[s.ndim - 1])
A:jax._src.numpy.linalg.(s, idxs, sign)->jax.lax.sort((s, idxs, sign), dimension=-1, num_keys=1)
A:jax._src.numpy.linalg.u->jax._src.numpy.lax_numpy.take_along_axis(w, idxs[..., None, :], axis=-1)
A:jax._src.numpy.linalg.vh->_H(u * sign[..., None, :].astype(u.dtype))
A:jax._src.numpy.linalg.(arr,)->promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.numpy.linalg.n->abs(n)
A:jax._src.numpy.linalg.arr->jax._src.numpy.lax_numpy.asarray(a)
A:jax._src.numpy.linalg.(n, bit)->divmod(n, 2)
A:jax._src.numpy.linalg.(M,)->promote_dtypes_inexact(jnp.asarray(M))
A:jax._src.numpy.linalg.S->svd(M, full_matrices=False, compute_uv=False)
A:jax._src.numpy.linalg.tol->jax._src.numpy.lax_numpy.expand_dims(tol, np.ndim(tol))
A:jax._src.numpy.linalg.dtype->jax.lax.dtype(a)
A:jax._src.numpy.linalg.(lu, pivot, _)->jax._src.lax.linalg.lu(a)
A:jax._src.numpy.linalg.diag->jax._src.numpy.lax_numpy.diagonal(lu, axis1=-2, axis2=-1)
A:jax._src.numpy.linalg.is_zero->jax._src.numpy.reductions.any(diag == jnp.array(0, dtype=dtype), axis=-1)
A:jax._src.numpy.linalg.iota->jax.lax.expand_dims(jnp.arange(a_shape[-1], dtype=pivots.dtype), range(pivots.ndim - 1))
A:jax._src.numpy.linalg.parity->jax._src.numpy.reductions.count_nonzero(pivots != iota, axis=-1)
A:jax._src.numpy.linalg.logdet->jax._src.numpy.lax_numpy.where(is_zero, jnp.array(-jnp.inf, dtype=dtype), reductions.sum(ufuncs.log(ufuncs.abs(diag)).astype(dtype), axis=-1))
A:jax._src.numpy.linalg.(a, taus)->jax._src.lax.linalg.geqrf(a)
A:jax._src.numpy.linalg.a_diag->jax._src.numpy.lax_numpy.diagonal(a, axis1=-2, axis2=-1)
A:jax._src.numpy.linalg.log_abs_det->jax._src.numpy.reductions.sum(ufuncs.log(ufuncs.abs(a_diag)), axis=-1)
A:jax._src.numpy.linalg.sign_diag->jax._src.numpy.reductions.prod(ufuncs.sign(a_diag), axis=-1)
A:jax._src.numpy.linalg.sign_taus->jax._src.numpy.reductions.prod(jnp.where(taus[..., :n - 1] != 0, -1, 1), axis=-1).astype(sign_diag.dtype)
A:jax._src.numpy.linalg.a_shape->jax._src.numpy.lax_numpy.shape(a)
A:jax._src.numpy.linalg.(sign, ans)->slogdet(x)
A:jax._src.numpy.linalg.ans_dot->jax._src.numpy.ufuncs.real(ans_dot)
A:jax._src.numpy.linalg.sign_dot->jax._src.numpy.lax_numpy.zeros_like(sign)
A:jax._src.numpy.linalg.(b,)->promote_dtypes_inexact(jnp.asarray(b))
A:jax._src.numpy.linalg.b_shape->jax._src.numpy.lax_numpy.shape(b)
A:jax._src.numpy.linalg.a_ndims->len(a_shape)
A:jax._src.numpy.linalg.(lu, pivots, permutation)->jax._src.lax.linalg.lu(a)
A:jax._src.numpy.linalg.batch_dims->jax.lax.broadcast_shapes(lu.shape[:-2], b.shape[:-2])
A:jax._src.numpy.linalg.x->x.ravel().ravel()
A:jax._src.numpy.linalg.lu->lu.at[..., -1, -1].set(1.0 / partial_det[..., -2]).at[..., -1, -1].set(1.0 / partial_det[..., -2])
A:jax._src.numpy.linalg.permutation->jax._src.numpy.lax_numpy.broadcast_to(permutation, (*batch_dims, a_shape[-1]))
A:jax._src.numpy.linalg.iotas->jax._src.numpy.lax_numpy.ix_(*(lax.iota(jnp.int32, b) for b in (*batch_dims, 1)))
A:jax._src.numpy.linalg.d->jax._src.numpy.lax_numpy.tile(d[..., None, None], d.ndim * (1,) + x.shape[-2:])
A:jax._src.numpy.linalg.(sign, logdet)->slogdet(a)
A:jax._src.numpy.linalg.(y, z)->_cofactor_solve(x, g)
A:jax._src.numpy.linalg.(v, w)->jax._src.lax.linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input)
A:jax._src.numpy.linalg.(w, _)->eigh(a, UPLO)
A:jax._src.numpy.linalg.max_rows_cols->max(arr.shape[-2:])
A:jax._src.numpy.linalg.rcond->jax._src.numpy.lax_numpy.where(rcond < 0, jnp.finfo(dtype).eps, rcond)
A:jax._src.numpy.linalg.(u, s, vh)->svd(arr, full_matrices=False, hermitian=hermitian)
A:jax._src.numpy.linalg.res->jax._src.numpy.lax_numpy.matmul(vh.mT, ufuncs.divide(u.mT, s[..., jnp.newaxis]), precision=lax.Precision.HIGHEST)
A:jax._src.numpy.linalg.p->pinv(a, rcond=rcond, hermitian=hermitian)
A:jax._src.numpy.linalg.a->_symmetrize(a)
A:jax._src.numpy.linalg.a_dot->_symmetrize(a_dot)
A:jax._src.numpy.linalg.(x,)->promote_dtypes_inexact(jnp.asarray(x))
A:jax._src.numpy.linalg.x_shape->jax._src.numpy.lax_numpy.shape(x)
A:jax._src.numpy.linalg.ndim->len(x_shape)
A:jax._src.numpy.linalg.axis->tuple((canonicalize_axis(x, ndim) for x in axis))
A:jax._src.numpy.linalg.num_axes->len(axis)
A:jax._src.numpy.linalg.abs_x->jax._src.numpy.ufuncs.abs(x)
A:jax._src.numpy.linalg.ord_arr->jax._src.lax.lax._const(abs_x, ord)
A:jax._src.numpy.linalg.ord_inv->jax._src.lax.lax._const(abs_x, 1.0 / ord_arr)
A:jax._src.numpy.linalg.out->jax._src.numpy.reductions.sum(abs_x ** ord_arr, axis=axis, keepdims=keepdims)
A:jax._src.numpy.linalg.(row_axis, col_axis)->cast(tuple[int, ...], axis)
A:jax._src.numpy.linalg.y->jax._src.numpy.lax_numpy.expand_dims(y, axis)
A:jax._src.numpy.linalg.(q, r)->jax._src.lax.linalg.qr(a, full_matrices=full_matrices)
A:jax._src.numpy.linalg.(a, b)->promote_dtypes_inexact(a, b)
A:jax._src.numpy.linalg.rank->mask.sum()
A:jax._src.numpy.linalg.(u, s, vt)->svd(a, full_matrices=False)
A:jax._src.numpy.linalg.safe_s->jax._src.numpy.lax_numpy.where(mask, s, 1).astype(a.dtype)
A:jax._src.numpy.linalg.uTb->jax._src.numpy.lax_numpy.matmul(u.conj().T, b, precision=lax.Precision.HIGHEST)
A:jax._src.numpy.linalg.resid->jax._src.numpy.lax_numpy.asarray([])
A:jax._src.numpy.linalg.b_estimate->jax._src.numpy.lax_numpy.matmul(a, x, precision=lax.Precision.HIGHEST)
A:jax._src.numpy.linalg._jit_lstsq->jit(partial(_lstsq, numpy_resid=False))
jax._src.numpy.linalg._H(x:ArrayLike)->Array
jax._src.numpy.linalg._cofactor_solve(a:ArrayLike,b:ArrayLike)->tuple[Array, Array]
jax._src.numpy.linalg._det_2x2(a:Array)->Array
jax._src.numpy.linalg._det_3x3(a:Array)->Array
jax._src.numpy.linalg._det_jvp(primals,tangents)
jax._src.numpy.linalg._lstsq(a:ArrayLike,b:ArrayLike,rcond:Optional[float],*,numpy_resid:bool=False)->tuple[Array, Array, Array, Array]
jax._src.numpy.linalg._pinv_jvp(rcond,hermitian,primals,tangents)
jax._src.numpy.linalg._slogdet_jvp(primals,tangents)
jax._src.numpy.linalg._slogdet_lu(a:Array)->tuple[Array, Array]
jax._src.numpy.linalg._slogdet_qr(a:Array)->tuple[Array, Array]
jax._src.numpy.linalg._symmetrize(x:Array)->Array
jax._src.numpy.linalg.cholesky(a:ArrayLike)->Array
jax._src.numpy.linalg.det(a:ArrayLike)->Array
jax._src.numpy.linalg.eig(a:ArrayLike)->tuple[Array, Array]
jax._src.numpy.linalg.eigh(a:ArrayLike,UPLO:Optional[str]=None,symmetrize_input:bool=True)->tuple[Array, Array]
jax._src.numpy.linalg.eigvals(a:ArrayLike)->Array
jax._src.numpy.linalg.eigvalsh(a:ArrayLike,UPLO:Optional[str]='L')->Array
jax._src.numpy.linalg.inv(a:ArrayLike)->Array
jax._src.numpy.linalg.lstsq(a:ArrayLike,b:ArrayLike,rcond:Optional[float]=None,*,numpy_resid:bool=False)->tuple[Array, Array, Array, Array]
jax._src.numpy.linalg.matrix_power(a:ArrayLike,n:int)->Array
jax._src.numpy.linalg.matrix_rank(M:ArrayLike,tol:Optional[ArrayLike]=None)->Array
jax._src.numpy.linalg.norm(x:ArrayLike,ord:Union[int,str,None]=None,axis:Union[None,tuple[int,...],int]=None,keepdims:bool=False)->Array
jax._src.numpy.linalg.pinv(a:ArrayLike,rcond:Optional[ArrayLike]=None,hermitian:bool=False)->Array
jax._src.numpy.linalg.qr(a:ArrayLike,mode:str='reduced')->Union[Array, tuple[Array, Array]]
jax._src.numpy.linalg.slogdet(a:ArrayLike,*,method:Optional[str]=None)->tuple[Array, Array]
jax._src.numpy.linalg.solve(a:ArrayLike,b:ArrayLike)->Array
jax._src.numpy.linalg.svd(a:ArrayLike,full_matrices:bool=True,compute_uv:bool=True,hermitian:bool=False)->Union[Array, tuple[Array, Array, Array]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py----------------------------------------
A:jax._src.numpy.array_methods.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.numpy.array_methods.axis->_ensure_index_tuple(args)
A:jax._src.numpy.array_methods.newshape->_compute_newshape(a, args[0] if len(args) == 1 else args)
A:jax._src.numpy.array_methods.sz->jax._src.core.cancel_divide_tracers(np.shape(a), (*newshape[:i], *newshape[i + 1:]))
A:jax._src.numpy.array_methods.dims->list(range(a.ndim)[::-1])
A:jax._src.numpy.array_methods.arr->arr.reshape(*arr.shape[:-1], arr.shape[-1] // factor, factor).reshape(*arr.shape[:-1], arr.shape[-1] // factor, factor)
A:jax._src.numpy.array_methods.out->jax._src.numpy.lax_numpy.round(number, decimals=ndigits or 0)
A:jax._src.numpy.array_methods.other->other.__jax_array__().__jax_array__()
A:jax._src.numpy.array_methods.sliced->jax.lax.squeeze(sliced, removed)
A:jax._src.numpy.array_methods.(num_chunks, tail)->jax._src.numpy.ufuncs.divmod(x.shape[0], size)
jax._src.numpy.array_methods._IndexUpdateHelper(self,array)
jax._src.numpy.array_methods._IndexUpdateHelper.__getitem__(self,index)
jax._src.numpy.array_methods._IndexUpdateHelper.__init__(self,array)
jax._src.numpy.array_methods._IndexUpdateHelper.__repr__(self)
jax._src.numpy.array_methods._IndexUpdateRef(self,array,index)
jax._src.numpy.array_methods._IndexUpdateRef.__init__(self,array,index)
jax._src.numpy.array_methods._IndexUpdateRef.__repr__(self)
jax._src.numpy.array_methods._IndexUpdateRef.add(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.apply(self,func,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.divide(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.get(self,*,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax._src.numpy.array_methods._IndexUpdateRef.max(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.min(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.multiply(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.power(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods._IndexUpdateRef.set(self,values,*,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.array_methods.__array_module__(self,types)
jax._src.numpy.array_methods._astype(arr:ArrayLike,dtype:DTypeLike)->Array
jax._src.numpy.array_methods._chunk_iter(x,size)
jax._src.numpy.array_methods._clip(number:ArrayLike,min:Optional[ArrayLike]=None,max:Optional[ArrayLike]=None,out:None=None)->Array
jax._src.numpy.array_methods._compress_method(a:ArrayLike,condition:ArrayLike,axis:Optional[int]=None,out:None=None)->Array
jax._src.numpy.array_methods._compute_newshape(a:ArrayLike,newshape:Union[DimSize,Shape])->Shape
jax._src.numpy.array_methods._copy(self:Array)->Array
jax._src.numpy.array_methods._deepcopy(self:Array,memo:Any)->Array
jax._src.numpy.array_methods._defer_to_unrecognized_arg(opchar,binary_op,swap=False)
jax._src.numpy.array_methods._forward_method_to_aval(name)
jax._src.numpy.array_methods._forward_operator_to_aval(name)
jax._src.numpy.array_methods._forward_property_to_aval(name)
jax._src.numpy.array_methods._getitem(self,item)
jax._src.numpy.array_methods._item(a:Array)->Any
jax._src.numpy.array_methods._itemsize(arr:ArrayLike)->int
jax._src.numpy.array_methods._make_abstract_method(name,func)
jax._src.numpy.array_methods._multi_slice(arr:ArrayLike,start_indices:tuple[tuple[int,...]],limit_indices:tuple[tuple[int,...]],removed_dims:tuple[tuple[int,...]])->list[Array]
jax._src.numpy.array_methods._nbytes(arr:ArrayLike)->int
jax._src.numpy.array_methods._notimplemented_flat(self)
jax._src.numpy.array_methods._operator_round(number:ArrayLike,ndigits:Optional[int]=None)->Array
jax._src.numpy.array_methods._reshape(a:Array,*args:Any,order:str='C')->Array
jax._src.numpy.array_methods._set_array_abstract_methods(basearray)
jax._src.numpy.array_methods._set_array_attributes(device_array)
jax._src.numpy.array_methods._set_array_base_attributes(device_array,include=None,exclude=None)
jax._src.numpy.array_methods._set_shaped_array_attributes(shaped_array)
jax._src.numpy.array_methods._set_tracer_aval_forwarding(tracer,exclude=())
jax._src.numpy.array_methods._transpose(a:Array,*args:Any)->Array
jax._src.numpy.array_methods._unimplemented_setitem(self,i,x)
jax._src.numpy.array_methods._unstack(x:Array)->list[Array]
jax._src.numpy.array_methods._view(arr:Array,dtype:Optional[DTypeLike]=None,type:None=None)->Array
jax._src.numpy.array_methods.register_jax_array_methods()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/util.py----------------------------------------
A:jax._src.numpy.util._T->TypeVar('_T')
A:jax._src.numpy.util._parameter_break->re.compile('\n(?=[A-Za-z_])')
A:jax._src.numpy.util._section_break->re.compile('\\n(?=[^\\n]{3,15}\\n-{3,15})', re.MULTILINE)
A:jax._src.numpy.util._numpy_signature_re->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE)
A:jax._src.numpy.util._versionadded->re.compile('^\\s+\\.\\.\\s+versionadded::', re.MULTILINE)
A:jax._src.numpy.util._docreference->re.compile(':doc:`(.*?)\\s*<.*?>`')
A:jax._src.numpy.util.docstr->getattr(fun, '__doc__', None)
A:jax._src.numpy.util.match->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE).match(body)
A:jax._src.numpy.util.signature->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE).match(body).group()
A:jax._src.numpy.util.(firstline, _, body)->textwrap.dedent(body.lstrip('\n')).partition('\n')
A:jax._src.numpy.util.body->textwrap.dedent(body.lstrip('\n'))
A:jax._src.numpy.util.(summary, _, body)->textwrap.dedent(body.lstrip('\n')).lstrip('\n').partition('\n')
A:jax._src.numpy.util.section_list->re.compile('\\n(?=[^\\n]{3,15}\\n-{3,15})', re.MULTILINE).split(body)
A:jax._src.numpy.util.(title, underline, content)->textwrap.dedent(body.lstrip('\n')).split('\n', 2)
A:jax._src.numpy.util.parameters->_parse_parameters(parsed.sections['Parameters'])
A:jax._src.numpy.util.name->getattr(fun, '__name__', getattr(op, '__name__', str(op)))
A:jax._src.numpy.util.parsed->_parse_numpydoc(docstr)
A:jax._src.numpy.util.code->getattr(getattr(op, '__wrapped__', op), '__code__', None)
A:jax._src.numpy.util.value->getattr(fun, attr)
A:jax._src.numpy.util._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.numpy.util.res_shape->jax._src.lax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.util.result_rank->len(lax.broadcast_shapes(*shapes))
A:jax._src.numpy.util.(to_dtype, weak_type)->jax._src.dtypes._lattice_result_type(*args)
A:jax._src.numpy.util.to_dtype->jax._src.dtypes.canonicalize_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_inexact->jax._src.dtypes.to_inexact_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_numeric->jax._src.dtypes.to_numeric_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_complex->jax._src.dtypes.to_complex_dtype(to_dtype)
A:jax._src.numpy.util.(pos, arg)->next(((i, arg) for (i, arg) in enumerate(args) if not (_arraylike(arg) or arg is None)))
A:jax._src.numpy.util.result_shape->jax._src.lax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.util.shape->jax._src.core.canonicalize_shape(shape)
A:jax._src.numpy.util.arr_shape->numpy.shape(arr)
A:jax._src.numpy.util.compatible->all((core.definitely_equal_one_of_dim(arr_d, [1, shape_d]) for (arr_d, shape_d) in safe_zip(arr_shape, shape_tail)))
A:jax._src.numpy.util.(diff,)->numpy.where(tuple((not core.definitely_equal(arr_d, shape_d) for (arr_d, shape_d) in safe_zip(arr_shape, shape_tail))))
A:jax._src.numpy.util.kept_dims->tuple(np.delete(np.arange(len(shape)), new_dims))
A:jax._src.numpy.util.condition->jax._src.lax.lax.ne(condition, lax._zero(condition))
A:jax._src.numpy.util.(x, y)->promote_dtypes(x, y)
A:jax._src.numpy.util.(condition_arr, x_arr, y_arr)->_broadcast_arrays(condition, x, y)
A:jax._src.numpy.util.is_always_empty->jax._src.core.is_empty_shape(x_arr.shape)
jax._src.numpy.util.ParsedDoc(NamedTuple)
jax._src.numpy.util._arraylike(x:ArrayLike)->bool
jax._src.numpy.util._broadcast_arrays(*args:ArrayLike)->list[Array]
jax._src.numpy.util._broadcast_to(arr:ArrayLike,shape:DimSize|Shape)->Array
jax._src.numpy.util._complex_elem_type(dtype:DTypeLike)->DType
jax._src.numpy.util._parse_extra_params(extra_params:str)->dict[str, str]
jax._src.numpy.util._parse_numpydoc(docstr:Optional[str])->ParsedDoc
jax._src.numpy.util._parse_parameters(body:str)->dict[str, str]
jax._src.numpy.util._rank_promotion_warning_or_error(fun_name:str,shapes:Sequence[Shape])
jax._src.numpy.util._where(condition:ArrayLike,x:ArrayLike,y:ArrayLike)->Array
jax._src.numpy.util._wraps(fun:Optional[Callable[...,Any]],update_doc:bool=True,lax_description:str='',sections:Sequence[str]=('Parameters','Returns','References'),skip_params:Sequence[str]=(),extra_params:Optional[str]=None,module:Optional[str]=None)->Callable[[_T], _T]
jax._src.numpy.util.check_arraylike(fun_name:str,*args:Any,emit_warning=False,stacklevel=3)
jax._src.numpy.util.check_arraylike_or_none(fun_name:str,*args:Any)
jax._src.numpy.util.check_no_float0s(fun_name:str,*args:Any)
jax._src.numpy.util.promote_args(fun_name:str,*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_args_inexact(fun_name:str,*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_args_numeric(fun_name:str,*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_dtypes(*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_dtypes_complex(*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_dtypes_inexact(*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_dtypes_numeric(*args:ArrayLike)->list[Array]
jax._src.numpy.util.promote_shapes(fun_name:str,*args:ArrayLike)->list[Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/ufunc_api.py----------------------------------------
A:jax._src.numpy.ufunc_api.jaxpr->jax.make_jaxpr(fun)(*args)
A:jax._src.numpy.ufunc_api._func->property(lambda self: self.__static_props['func'])
A:jax._src.numpy.ufunc_api._call->property(lambda self: self.__static_props['call'])
A:jax._src.numpy.ufunc_api.nin->property(lambda self: self.__static_props['nin'])
A:jax._src.numpy.ufunc_api.nout->property(lambda self: self.__static_props['nout'])
A:jax._src.numpy.ufunc_api.nargs->property(lambda self: self.__static_props['nargs'])
A:jax._src.numpy.ufunc_api.identity->property(lambda self: self.__static_props['identity'])
A:jax._src.numpy.ufunc_api.primitive->get_if_single_primitive(self._call, *self.nin * [lax_internal._one(a)])
A:jax._src.numpy.ufunc_api.reducer->_primitive_reducers.get(primitive, self._reduce_via_scan)
A:jax._src.numpy.ufunc_api.arr->_moveaxis(arr, axis, 0)
A:jax._src.numpy.ufunc_api.where->_moveaxis(where, axis, 0)
A:jax._src.numpy.ufunc_api.axis->canonicalize_axis(axis, a.ndim)
A:jax._src.numpy.ufunc_api.start_value->_broadcast_to(lax_internal.asarray(start_value).astype(dtype), arr.shape[1:])
A:jax._src.numpy.ufunc_api.result->jax.vmap(jax.vmap(partial(self._call, **kwargs), (None, 0)), (0, None))(_ravel(A), _ravel(B))
A:jax._src.numpy.ufunc_api.accumulator->_primitive_accumulators.get(primitive, self._accumulate_via_scan)
A:jax._src.numpy.ufunc_api.y->_where(i == 0, arr[0].astype(dtype), self._call(x.astype(dtype), arr[i].astype(dtype)))
A:jax._src.numpy.ufunc_api.(_, result)->jax.lax.scan(scan_fun, (0, arr[0].astype(dtype)), None, length=arr.shape[0])
A:jax._src.numpy.ufunc_api.a->jax._src.lax.lax.asarray(a)
A:jax._src.numpy.ufunc_api.args->tuple((lax_internal.asarray(arg).astype(dtype) for arg in args))
A:jax._src.numpy.ufunc_api.indices->_eliminate_deprecated_list_indexing(indices)
A:jax._src.numpy.ufunc_api.arg->_broadcast_to(args[0], (*shape, *args[0].shape[len(shape):]))
A:jax._src.numpy.ufunc_api.idx->tuple((ind if isinstance(ind, slice) else ind[i] for ind in indices))
A:jax._src.numpy.ufunc_api.(carry, _)->jax.lax.scan(scan_fun, (0, a), None, len(indices[0]))
A:jax._src.numpy.ufunc_api.idx_tuple->_eliminate_deprecated_list_indexing(indices)
A:jax._src.numpy.ufunc_api.out->take(a, indices, axis=axis)
A:jax._src.numpy.ufunc_api.ind->jax.lax.expand_dims(append(indices, a.shape[axis]), list(np.delete(np.arange(out.ndim), axis)))
A:jax._src.numpy.ufunc_api.ind_start->jax.lax.slice_in_dim(ind, 0, ind.shape[axis] - 1, axis=axis)
A:jax._src.numpy.ufunc_api.ind_end->jax.lax.slice_in_dim(ind, 1, ind.shape[axis], axis=axis)
jax._src.numpy.ufunc_api.frompyfunc(func:Callable[...,Any],/,nin:int,nout:int,*,identity:Any=None)->ufunc
jax._src.numpy.ufunc_api.get_if_single_primitive(fun:Callable[...,Any],*args:Any)->Optional[jax.core.Primitive]
jax._src.numpy.ufunc_api.ufunc(self,func:Callable[...,Any],/,nin:int,nout:int,*,name:Optional[str]=None,nargs:Optional[int]=None,identity:Any=None,update_doc=False)
jax._src.numpy.ufunc_api.ufunc.__eq__(self,other:Any)->bool
jax._src.numpy.ufunc_api.ufunc.__hash__(self)->int
jax._src.numpy.ufunc_api.ufunc.__init__(self,func:Callable[...,Any],/,nin:int,nout:int,*,name:Optional[str]=None,nargs:Optional[int]=None,identity:Any=None,update_doc=False)
jax._src.numpy.ufunc_api.ufunc.__repr__(self)->str
jax._src.numpy.ufunc_api.ufunc._accumulate_via_scan(self,arr:ArrayLike,axis:int=0,dtype:Optional[DTypeLike]=None)->Array
jax._src.numpy.ufunc_api.ufunc._at_via_scan(self,a:ArrayLike,indices:Any,*args:Any)->Array
jax._src.numpy.ufunc_api.ufunc._reduce_via_scan(self,arr:ArrayLike,axis:int=0,dtype:Optional[DTypeLike]=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.ufunc_api.ufunc._reduceat_via_scan(self,a:ArrayLike,indices:Any,axis:int=0,dtype:Optional[DTypeLike]=None)->Array
jax._src.numpy.ufunc_api.ufunc.accumulate(self,a:ArrayLike,axis:int=0,dtype:Optional[DTypeLike]=None,out:None=None)->Array
jax._src.numpy.ufunc_api.ufunc.at(self,a:ArrayLike,indices:Any,b:Optional[ArrayLike]=None,/,*,inplace:bool=True)->Array
jax._src.numpy.ufunc_api.ufunc.outer(self,A:ArrayLike,B:ArrayLike,/,**kwargs)->Array
jax._src.numpy.ufunc_api.ufunc.reduce(self,a:ArrayLike,axis:int=0,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.ufunc_api.ufunc.reduceat(self,a:ArrayLike,indices:Any,axis:int=0,dtype:Optional[DTypeLike]=None,out:None=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/reductions.py----------------------------------------
A:jax._src.numpy.reductions.element->element.__jax_array__().__jax_array__()
A:jax._src.numpy.reductions.a->jax.lax.sort(a, dimension=axis)
A:jax._src.numpy.reductions.source->_canonicalize_axis(source, np.ndim(a))
A:jax._src.numpy.reductions.destination->_canonicalize_axis(destination, np.ndim(a))
A:jax._src.numpy.reductions.axis->_canonicalize_axis(axis, a.ndim)
A:jax._src.numpy.reductions.(pos_dims, dims)->_reduction_dims(a, axis)
A:jax._src.numpy.reductions.shape->numpy.shape(a)
A:jax._src.numpy.reductions.result_dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.numpy.reductions.computation_dtype->_upcast_f16(result_dtype)
A:jax._src.numpy.reductions.init_val->_reduction_init_val(a, init_val)
A:jax._src.numpy.reductions.result->result.reshape(keepdim).reshape(keepdim)
A:jax._src.numpy.reductions.initial_arr->jax.lax.convert_element_type(initial, lax_internal.asarray(a).dtype)
A:jax._src.numpy.reductions.canon_axis->tuple((_canonicalize_axis_allow_named(x, np.ndim(a)) for x in axis))
A:jax._src.numpy.reductions.canon_pos_axis->tuple((x for x in canon_axis if isinstance(x, int)))
A:jax._src.numpy.reductions.a_dtype->jax._src.dtypes.canonicalize_dtype(dtypes.dtype(a))
A:jax._src.numpy.reductions.a_shape->list(np.shape(a))
A:jax._src.numpy.reductions.normalizer->sum(lax_internal.bitwise_not(lax_internal._isnan(a)), axis=axis, keepdims=keepdims, where=where)
A:jax._src.numpy.reductions.(a,)->promote_dtypes_inexact(a)
A:jax._src.numpy.reductions.avg->mean(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.weights_sum->_broadcast_to(weights_sum, avg.shape)
A:jax._src.numpy.reductions.(a, weights)->promote_dtypes_inexact(a, weights)
A:jax._src.numpy.reductions.a_ndim->len(a_shape)
A:jax._src.numpy.reductions.weights_shape->numpy.shape(weights)
A:jax._src.numpy.reductions.weights->_moveaxis(weights, -1, axis)
A:jax._src.numpy.reductions.(computation_dtype, dtype)->_var_promote_types(dtypes.dtype(a), dtype)
A:jax._src.numpy.reductions.a_mean->nanmean(a, axis, dtype=computation_dtype, keepdims=True, where=where)
A:jax._src.numpy.reductions.centered->jax.lax.square(centered)
A:jax._src.numpy.reductions.dtype->jax._src.dtypes.canonicalize_dtype(dtypes.int_)
A:jax._src.numpy.reductions.x->amax(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.y->amin(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.out->jnp_reduction(_where(lax_internal._isnan(a), _reduction_init_val(a, init_val), a), axis=axis, keepdims=keepdims, **kwargs)
A:jax._src.numpy.reductions.nansum.__doc__->nansum.__doc__.replace('\n\n\n', '\n\n')
A:jax._src.numpy.reductions.nan_mask->jax._src.lax.lax.bitwise_not(lax_internal._isnan(a))
A:jax._src.numpy.reductions.td->jax.lax.div(nansum(a, axis, dtype=dtype, keepdims=keepdims, where=where), normalizer)
A:jax._src.numpy.reductions.normalizer_mask->jax.lax.le(normalizer, lax_internal._zero(normalizer))
A:jax._src.numpy.reductions.divisor->_where(normalizer_mask, 1, normalizer)
A:jax._src.numpy.reductions.num_dims->len(a_shape)
A:jax._src.numpy.reductions.cumsum->_make_cumulative_reduction(np.cumsum, lax.cumsum, fill_nan=False)
A:jax._src.numpy.reductions.cumprod->_make_cumulative_reduction(np.cumprod, lax.cumprod, fill_nan=False)
A:jax._src.numpy.reductions.nancumsum->_make_cumulative_reduction(np.nancumsum, lax.cumsum, fill_nan=True, fill_value=0)
A:jax._src.numpy.reductions.nancumprod->_make_cumulative_reduction(np.nancumprod, lax.cumprod, fill_nan=True, fill_value=1)
A:jax._src.numpy.reductions.keepdim->list(a.shape)
A:jax._src.numpy.reductions.dimensions->list(range(nd))
A:jax._src.numpy.reductions.do_not_touch_shape->tuple((x for (idx, x) in enumerate(a.shape) if idx not in axis))
A:jax._src.numpy.reductions.touch_shape->tuple((x for (idx, x) in enumerate(a.shape) if idx in axis))
A:jax._src.numpy.reductions.counts->jax.lax.expand_dims(counts, tuple(range(q_ndim)))
A:jax._src.numpy.reductions.q->jax._src.numpy.ufuncs.true_divide(q, 100.0)
A:jax._src.numpy.reductions.low->jax.lax.convert_element_type(low, int)
A:jax._src.numpy.reductions.high->jax.lax.convert_element_type(high, int)
A:jax._src.numpy.reductions.high_weight->jax.lax.broadcast_in_dim(high_weight, high_value.shape, broadcast_dimensions=(0,))
A:jax._src.numpy.reductions.low_weight->jax.lax.broadcast_in_dim(low_weight, low_value.shape, broadcast_dimensions=(0,))
A:jax._src.numpy.reductions.n->jax.lax.convert_element_type(a_shape[axis], lax_internal._dtype(q))
A:jax._src.numpy.reductions.slice_sizes->list(a_shape)
A:jax._src.numpy.reductions.dnums->jax.lax.GatherDimensionNumbers(offset_dims=tuple(range(q_ndim, len(a_shape) + q_ndim if keepdims else len(a_shape) + q_ndim - 1)), collapsed_slice_dims=() if keepdims else (axis,), start_index_map=(axis,))
A:jax._src.numpy.reductions.low_value->jax.lax.gather(a, low[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax._src.numpy.reductions.high_value->jax.lax.gather(a, high[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax._src.numpy.reductions.pred->jax.lax.le(high_weight, _lax_const(high_weight, 0.5))
A:jax._src.numpy.reductions.(q,)->promote_dtypes_inexact(q)
jax._src.numpy.reductions.CumulativeReduction(self,a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None)
jax._src.numpy.reductions.CumulativeReduction.__call__(self,a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None)
jax._src.numpy.reductions._average(a:ArrayLike,axis:Axis=None,weights:Optional[ArrayLike]=None,returned:bool=False,keepdims:bool=False)->Union[Array, tuple[Array, Array]]
jax._src.numpy.reductions._axis_size(a:ArrayLike,axis:Union[int,Sequence[int]])
jax._src.numpy.reductions._canonicalize_axis_allow_named(x,rank)
jax._src.numpy.reductions._cast_to_bool(operand:ArrayLike)->Array
jax._src.numpy.reductions._cast_to_numeric(operand:ArrayLike)->Array
jax._src.numpy.reductions._ensure_optional_axes(x:Axis)->Axis
jax._src.numpy.reductions._isscalar(element:Any)->bool
jax._src.numpy.reductions._make_cumulative_reduction(np_reduction:Any,reduction:Callable[...,Array],fill_nan:bool=False,fill_value:ArrayLike=0)->CumulativeReduction
jax._src.numpy.reductions._mean(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,*,upcast_f16_for_computation:bool=True,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._moveaxis(a:ArrayLike,source:int,destination:int)->Array
jax._src.numpy.reductions._nan_reduction(a:ArrayLike,name:str,jnp_reduction:Callable[...,Array],init_val:ArrayLike,nan_if_all_nan:bool,axis:Axis=None,keepdims:bool=False,**kwargs)->Array
jax._src.numpy.reductions._ptp(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False)->Array
jax._src.numpy.reductions._quantile(a:Array,q:Array,axis:Optional[Union[int,tuple[int,...]]],interpolation:str,keepdims:bool,squash_nans:bool)->Array
jax._src.numpy.reductions._reduce_all(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._reduce_any(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._reduce_max(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._reduce_min(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._reduce_prod(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None,promote_integers:bool=True)->Array
jax._src.numpy.reductions._reduce_sum(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None,promote_integers:bool=True)->Array
jax._src.numpy.reductions._reduction(a:ArrayLike,name:str,np_fun:Any,op:ReductionOp,init_val:ArrayLike,*,has_identity:bool=True,preproc:Optional[Callable[[ArrayLike],ArrayLike]]=None,bool_op:Optional[ReductionOp]=None,upcast_f16_for_computation:bool=False,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where_:Optional[ArrayLike]=None,parallel_reduce:Optional[Callable[...,Array]]=None,promote_integers:bool=False)->Array
jax._src.numpy.reductions._reduction_dims(a:ArrayLike,axis:Axis)
jax._src.numpy.reductions._reduction_init_val(a:ArrayLike,init_val:Any)->np.ndarray
jax._src.numpy.reductions._std(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._upcast_f16(dtype:DTypeLike)->DType
jax._src.numpy.reductions._var(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions._var_promote_types(a_dtype:DTypeLike,dtype:Optional[DTypeLike])->tuple[DType, DType]
jax._src.numpy.reductions.all(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.any(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.average(a:ArrayLike,axis:Axis=None,weights:Optional[ArrayLike]=None,returned:bool=False,keepdims:bool=False)->Union[Array, tuple[Array, Array]]
jax._src.numpy.reductions.count_nonzero(a:ArrayLike,axis:Axis=None,keepdims:bool=False)->Array
jax._src.numpy.reductions.max(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.mean(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.median(a:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,keepdims:bool=False)->Array
jax._src.numpy.reductions.min(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanmax(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanmean(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanmedian(a:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,keepdims:bool=False)->Array
jax._src.numpy.reductions.nanmin(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanpercentile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,method:str='linear',keepdims:bool=False,interpolation:None=None)->Array
jax._src.numpy.reductions.nanprod(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanquantile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,method:str='linear',keepdims:bool=False,interpolation:None=None)->Array
jax._src.numpy.reductions.nanstd(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nansum(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.nanvar(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.percentile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,method:str='linear',keepdims:bool=False,interpolation:None=None)->Array
jax._src.numpy.reductions.prod(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None,promote_integers:bool=True)->Array
jax._src.numpy.reductions.ptp(a:ArrayLike,axis:Axis=None,out:None=None,keepdims:bool=False)->Array
jax._src.numpy.reductions.quantile(a:ArrayLike,q:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=None,out:None=None,overwrite_input:bool=False,method:str='linear',keepdims:bool=False,interpolation:None=None)->Array
jax._src.numpy.reductions.std(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array
jax._src.numpy.reductions.sum(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,keepdims:bool=False,initial:Optional[ArrayLike]=None,where:Optional[ArrayLike]=None,promote_integers:bool=True)->Array
jax._src.numpy.reductions.var(a:ArrayLike,axis:Axis=None,dtype:Optional[DTypeLike]=None,out:None=None,ddof:int=0,keepdims:bool=False,*,where:Optional[ArrayLike]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py----------------------------------------
A:jax._src.numpy.lax_numpy.T->TypeVar('T')
A:jax._src.numpy.lax_numpy.meta->_ScalarMeta(np_scalar_type.__name__, (object,), {'dtype': np.dtype(np_scalar_type)})
A:jax._src.numpy.lax_numpy.bool_->_make_scalar_type(np.bool_)
A:jax._src.numpy.lax_numpy.uint4->_make_scalar_type(dtypes.uint4)
A:jax._src.numpy.lax_numpy.uint8->_make_scalar_type(np.uint8)
A:jax._src.numpy.lax_numpy.uint16->_make_scalar_type(np.uint16)
A:jax._src.numpy.lax_numpy.uint32->_make_scalar_type(np.uint32)
A:jax._src.numpy.lax_numpy.uint64->_make_scalar_type(np.uint64)
A:jax._src.numpy.lax_numpy.int4->_make_scalar_type(dtypes.int4)
A:jax._src.numpy.lax_numpy.int8->_make_scalar_type(np.int8)
A:jax._src.numpy.lax_numpy.int16->_make_scalar_type(np.int16)
A:jax._src.numpy.lax_numpy.int32->_make_scalar_type(np.int32)
A:jax._src.numpy.lax_numpy.int64->_make_scalar_type(np.int64)
A:jax._src.numpy.lax_numpy.float8_e4m3fn->_make_scalar_type(dtypes.float8_e4m3fn)
A:jax._src.numpy.lax_numpy.float8_e4m3fnuz->_make_scalar_type(dtypes.float8_e4m3fnuz)
A:jax._src.numpy.lax_numpy.float8_e5m2->_make_scalar_type(dtypes.float8_e5m2)
A:jax._src.numpy.lax_numpy.float8_e5m2fnuz->_make_scalar_type(dtypes.float8_e5m2fnuz)
A:jax._src.numpy.lax_numpy.float8_e4m3b11fnuz->_make_scalar_type(dtypes.float8_e4m3b11fnuz)
A:jax._src.numpy.lax_numpy.bfloat16->_make_scalar_type(dtypes.bfloat16)
A:jax._src.numpy.lax_numpy.float16->_make_scalar_type(np.float16)
A:jax._src.numpy.lax_numpy.float32single->_make_scalar_type(np.float32)
A:jax._src.numpy.lax_numpy.float64double->_make_scalar_type(np.float64)
A:jax._src.numpy.lax_numpy.complex64csingle->_make_scalar_type(np.complex64)
A:jax._src.numpy.lax_numpy.complex128cdouble->_make_scalar_type(np.complex128)
A:jax._src.numpy.lax_numpy.dtype->_dtype(x)
A:jax._src.numpy.lax_numpy.val_dtype->jax._src.dtypes.canonicalize_dtype(val.dtype)
A:jax._src.numpy.lax_numpy.min_val->_lax_const(val, max(iinfo(dtype).min, iinfo(val_dtype).min))
A:jax._src.numpy.lax_numpy.max_val->_lax_const(val, min(iinfo(dtype).max, iinfo(val_dtype).max))
A:jax._src.numpy.lax_numpy.out->jax.lax.concatenate([bottom_ind, top_ind], dimension=arr.ndim - 1)
A:jax._src.numpy.lax_numpy.element->element.__jax_array__().__jax_array__()
A:jax._src.numpy.lax_numpy.(y_arr,)->jax._src.numpy.util.promote_dtypes_inexact(y)
A:jax._src.numpy.lax_numpy.(y_arr, x_arr)->jax._src.numpy.util.promote_dtypes_inexact(y, x)
A:jax._src.numpy.lax_numpy.dx->list(spacing)
A:jax._src.numpy.lax_numpy.y_arr->atleast_2d(y)
A:jax._src.numpy.lax_numpy.(x, y)->jax._src.numpy.util.promote_dtypes(x, y)
A:jax._src.numpy.lax_numpy.out_order->slice(None, None, -1)
A:jax._src.numpy.lax_numpy.y->jax.lax.rev(y, indexer.reversed_y_dims)
A:jax._src.numpy.lax_numpy.result->jax.lax.argmin(a, _canonicalize_axis(axis, a.ndim), dtypes.canonicalize_dtype(int_))
A:jax._src.numpy.lax_numpy.arr->tile(arr, int(np.ceil(size / arr.size)))
A:jax._src.numpy.lax_numpy.bins_int->jax._src.core.concrete_or_error(operator.index, bins, 'bins argument of histogram_bin_edges')
A:jax._src.numpy.lax_numpy.range->asarray(range, dtype=dtype)
A:jax._src.numpy.lax_numpy.a->asarray(arr)
A:jax._src.numpy.lax_numpy.weights->numpy.array(1, dtype=int_)
A:jax._src.numpy.lax_numpy.(a, weights)->map(ravel, util.promote_dtypes_inexact(a, weights))
A:jax._src.numpy.lax_numpy.bin_edges->histogram_bin_edges(sample[:, i], bins_per_dimension[i], range_i, weights)
A:jax._src.numpy.lax_numpy.bin_idx->where(sample[:, i] == bin_edges[-1], bin_idx - 1, bin_idx)
A:jax._src.numpy.lax_numpy.bin_widths->diff(bin_edges)
A:jax._src.numpy.lax_numpy.N->len(dimensions)
A:jax._src.numpy.lax_numpy.x_edgesy_edges->asarray(bins)
A:jax._src.numpy.lax_numpy.sample->transpose(asarray([x, y]))
A:jax._src.numpy.lax_numpy.(hist, edges)->histogramdd(sample, bins, range, weights, density)
A:jax._src.numpy.lax_numpy.(sample,)->jax._src.numpy.util.promote_dtypes_inexact(sample)
A:jax._src.numpy.lax_numpy.(sample, weights)->jax._src.numpy.util.promote_dtypes_inexact(sample, weights)
A:jax._src.numpy.lax_numpy.(N, D)->shape(sample)
A:jax._src.numpy.lax_numpy.num_bins->len(bins)
A:jax._src.numpy.lax_numpy.bins_per_dimension->list(bins)
A:jax._src.numpy.lax_numpy.nbins->tuple((len(bin_edges) + 1 for bin_edges in bin_edges_by_dim))
A:jax._src.numpy.lax_numpy.xy->ravel_multi_index(tuple(bin_idx_by_dim), nbins, mode='clip')
A:jax._src.numpy.lax_numpy.hist->hist.astype(sample.dtype).astype(sample.dtype)
A:jax._src.numpy.lax_numpy.ndim->len(shape)
A:jax._src.numpy.lax_numpy.ax1->_canonicalize_axis(ax1, ndim(m))
A:jax._src.numpy.lax_numpy.ax2->_canonicalize_axis(ax2, ndim(m))
A:jax._src.numpy.lax_numpy.perm->tuple((names.index(name) for name in result_names))
A:jax._src.numpy.lax_numpy.axis->_canonicalize_axis(axis, a_ndim)
A:jax._src.numpy.lax_numpy.i->array(i)
A:jax._src.numpy.lax_numpy.re->jax.lax.convert_element_type(re, dtype)
A:jax._src.numpy.lax_numpy.im->jax.lax.convert_element_type(im, dtype)
A:jax._src.numpy.lax_numpy.n->jax.lax.iota(dtype, M)
A:jax._src.numpy.lax_numpy.shape->jax._src.core.canonicalize_shape(shape, context='shape argument of jnp.fromfunction()')
A:jax._src.numpy.lax_numpy.prepend->broadcast_to(prepend, tuple(shape))
A:jax._src.numpy.lax_numpy.append->broadcast_to(append, tuple(shape))
A:jax._src.numpy.lax_numpy.slice1[axis]->slice(1, None)
A:jax._src.numpy.lax_numpy.slice2[axis]->slice(None, -1)
A:jax._src.numpy.lax_numpy.slice1_tuple->tuple(slice1)
A:jax._src.numpy.lax_numpy.slice2_tuple->tuple(slice2)
A:jax._src.numpy.lax_numpy.(a, *spacing)->jax._src.numpy.util.promote_args_inexact('gradient', f, *varargs)
A:jax._src.numpy.lax_numpy.sliced->partial(lax.slice_in_dim, a, axis=axis)
A:jax._src.numpy.lax_numpy.a_grad->concatenate((sliced(1, 2) - sliced(0, 1), (sliced(2, None) - sliced(None, -2)) * 0.5, sliced(-1, None) - sliced(-2, -1)), axis)
A:jax._src.numpy.lax_numpy.axis_tuple->tuple((_canonicalize_axis(i, a.ndim) for i in _ensure_index_tuple(axis)))
A:jax._src.numpy.lax_numpy.dims->list(range(ndim(a)))
A:jax._src.numpy.lax_numpy.strides->(np.cumprod(arr.shape[::-1])[::-1] // arr.shape).astype(int_)
A:jax._src.numpy.lax_numpy.indices_arr->asarray(indices)
A:jax._src.numpy.lax_numpy.(indices_arr, out_indices[i])->jax._src.numpy.ufuncs.divmod(indices_arr, s)
A:jax._src.numpy.lax_numpy.new_shape->_ensure_index_tuple(new_shape)
A:jax._src.numpy.lax_numpy.new_size->math.prod(new_shape)
A:jax._src.numpy.lax_numpy.repeats->broadcast_to(repeats, [shape(a)[axis]])
A:jax._src.numpy.lax_numpy.a_shape->shape(a)
A:jax._src.numpy.lax_numpy.source->tuple((_canonicalize_axis(i, ndim(a)) for i in source))
A:jax._src.numpy.lax_numpy.destination->tuple((_canonicalize_axis(i, ndim(a)) for i in destination))
A:jax._src.numpy.lax_numpy.(a, b)->jax._src.numpy.util.promote_dtypes(a, b)
A:jax._src.numpy.lax_numpy.rtol->jax.lax.convert_element_type(rtol, dtype)
A:jax._src.numpy.lax_numpy.atol->jax.lax.convert_element_type(atol, dtype)
A:jax._src.numpy.lax_numpy.a_inf->jax._src.numpy.ufuncs.isinf(a)
A:jax._src.numpy.lax_numpy.b_inf->jax._src.numpy.ufuncs.isinf(b)
A:jax._src.numpy.lax_numpy.any_inf->jax._src.numpy.ufuncs.logical_or(a_inf, b_inf)
A:jax._src.numpy.lax_numpy.both_inf->jax._src.numpy.ufuncs.logical_and(a_inf, b_inf)
A:jax._src.numpy.lax_numpy.same_value->jax.lax.eq(a, b)
A:jax._src.numpy.lax_numpy.same_inf->jax._src.numpy.ufuncs.logical_and(both_inf, same_value)
A:jax._src.numpy.lax_numpy.a_nan->jax._src.numpy.ufuncs.isnan(a)
A:jax._src.numpy.lax_numpy.b_nan->jax._src.numpy.ufuncs.isnan(b)
A:jax._src.numpy.lax_numpy.any_nan->jax._src.numpy.ufuncs.logical_or(a_nan, b_nan)
A:jax._src.numpy.lax_numpy.both_nan->jax._src.numpy.ufuncs.logical_and(a_nan, b_nan)
A:jax._src.numpy.lax_numpy.(x_arr, xp_arr)->jax._src.numpy.util.promote_dtypes_inexact(x, xp)
A:jax._src.numpy.lax_numpy.(fp_arr,)->jax._src.numpy.util.promote_dtypes_inexact(fp)
A:jax._src.numpy.lax_numpy.period->jax._src.numpy.ufuncs.abs(period)
A:jax._src.numpy.lax_numpy.(xp_arr, fp_arr)->jax.lax.sort_key_val(xp_arr, fp_arr)
A:jax._src.numpy.lax_numpy.xp_arr->concatenate([xp_arr[-1:] - period, xp_arr, xp_arr[:1] + period])
A:jax._src.numpy.lax_numpy.fp_arr->concatenate([fp_arr[-1:], fp_arr, fp_arr[:1]])
A:jax._src.numpy.lax_numpy.epsilon->numpy.spacing(np.finfo(xp_arr.dtype).eps)
A:jax._src.numpy.lax_numpy.f->where(x_arr > xp_arr[-1], right_arr, f)
A:jax._src.numpy.lax_numpy.jitted_interp->jit(_interp, static_argnames=static_argnames)
A:jax._src.numpy.lax_numpy.choices->jax._src.numpy.util.promote_dtypes(default, *choicelist)
A:jax._src.numpy.lax_numpy.output->where(cond, choice, output)
A:jax._src.numpy.lax_numpy.minlength->jax._src.core.concrete_or_error(operator.index, minlength, "The error occurred because of argument 'minlength' of jnp.bincount.")
A:jax._src.numpy.lax_numpy.x_arr->jax.lax.abs(x_arr)
A:jax._src.numpy.lax_numpy.length->jax._src.core.concrete_dim_or_error(length, "The error occurred because of argument 'length' of jnp.bincount.")
A:jax._src.numpy.lax_numpy.ary->asarray(ary)
A:jax._src.numpy.lax_numpy.indices_or_sections->numpy.array([core.concrete_or_error(np.int64, i_s, f'in jax.numpy.{op} argument 1') for i_s in indices_or_sections], np.int64)
A:jax._src.numpy.lax_numpy.split_indices->numpy.array([np.int64(i) * (part_size + 1) for i in range(r + 1)] + [np.int64(i) * part_size + ((r + 1) * (part_size + 1) - 1) for i in range(num_sections - r)])
A:jax._src.numpy.lax_numpy.(part_size, r)->divmod(size, num_sections)
A:jax._src.numpy.lax_numpy.vsplit->_split_on_axis('vsplit', axis=0)
A:jax._src.numpy.lax_numpy.hsplit->_split_on_axis('hsplit', axis=1)
A:jax._src.numpy.lax_numpy.dsplit->_split_on_axis('dsplit', axis=2)
A:jax._src.numpy.lax_numpy.decimals->jax._src.core.concrete_or_error(operator.index, decimals, "'decimals' argument of jnp.round")
A:jax._src.numpy.lax_numpy.factor->_lax_const(x, 10 ** decimals)
A:jax._src.numpy.lax_numpy.zero->_lax_const(x, 0)
A:jax._src.numpy.lax_numpy.info->finfo(dtypes.canonicalize_dtype(dtype))
A:jax._src.numpy.lax_numpy.size->jax._src.core.concrete_dim_or_error(size, 'The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.')
A:jax._src.numpy.lax_numpy.flat_indices->jax._src.numpy.reductions.cumsum(bincount(reductions.cumsum(mask), length=size))
A:jax._src.numpy.lax_numpy.p->asarray(p)
A:jax._src.numpy.lax_numpy.dd->diff(p, axis=axis)
A:jax._src.numpy.lax_numpy.ddmod->where((ddmod == -interval) & (dd > 0), interval, ddmod)
A:jax._src.numpy.lax_numpy.ph_correct->where(ufuncs.abs(dd) < discont, 0, ddmod - dd)
A:jax._src.numpy.lax_numpy.up->concatenate((lax.slice_in_dim(p, 0, 1, axis=axis), lax.slice_in_dim(p, 1, None, axis=axis) + reductions.cumsum(ph_correct, axis=axis)), axis=axis)
A:jax._src.numpy.lax_numpy.nvals->numpy.asarray(tree_map(lambda x: core.concrete_or_error(None, x, context=f'{name} argument of jnp.pad'), nvals))
A:jax._src.numpy.lax_numpy.v->jax.lax.pad(v, zero(v), ((max(0, k), max(0, -k), 0),))
A:jax._src.numpy.lax_numpy.nd->ndim(arr)
A:jax._src.numpy.lax_numpy.constant_values->kwargs.get('constant_values', 0)
A:jax._src.numpy.lax_numpy.array->asarray(array)
A:jax._src.numpy.lax_numpy.(repeats, (left_remainder, right_remainder))->numpy.divmod(pad_width[i], size)
A:jax._src.numpy.lax_numpy.edge->jax.lax.slice_in_dim(x, -1, None, axis=i)
A:jax._src.numpy.lax_numpy.curr_pad->min(padding, n - offset)
A:jax._src.numpy.lax_numpy.x->jax._src.numpy.ufuncs.remainder(lax.convert_element_type(x, np.int32), lax.max(a_shape_i, np.int32(1)))
A:jax._src.numpy.lax_numpy.edge_before->jax.lax.slice_in_dim(array, 0, 1, axis=axis)
A:jax._src.numpy.lax_numpy.pad_before->empty_like(array, shape=shape_before)
A:jax._src.numpy.lax_numpy.edge_after->jax.lax.slice_in_dim(array, -1, None, axis=axis)
A:jax._src.numpy.lax_numpy.pad_after->empty_like(array, shape=shape_after)
A:jax._src.numpy.lax_numpy.ramp_before->jax._src.lax.lax._convert_element_type(ramp_before, weak_type=dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.ramp_after->flip(ramp_after, axis)
A:jax._src.numpy.lax_numpy.stat_before->jax._src.lax.lax._convert_element_type(stat_before, array.dtype, dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.length_before->min(length_before, array_length)
A:jax._src.numpy.lax_numpy.length_after->min(length_after, array_length)
A:jax._src.numpy.lax_numpy.slice_before->jax.lax.slice_in_dim(array, 0, length_before, axis=i)
A:jax._src.numpy.lax_numpy.slice_after->jax.lax.slice_in_dim(array, -length_after, None, axis=i)
A:jax._src.numpy.lax_numpy.stat_after->jax._src.lax.lax._convert_element_type(stat_after, array.dtype, dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.pad_width->_broadcast_to_pairs(pad_width, ndim(array), 'pad_width')
A:jax._src.numpy.lax_numpy.padded->apply_along_axis(func, axis, padded, pad_width[axis], axis, kwargs)
A:jax._src.numpy.lax_numpy.pad_width_arr->numpy.array(pad_width)
A:jax._src.numpy.lax_numpy.end_values->kwargs.get('end_values', 0)
A:jax._src.numpy.lax_numpy.stat_length->kwargs.get('stat_length', None)
A:jax._src.numpy.lax_numpy.reflect_type->kwargs.get('reflect_type', 'even')
A:jax._src.numpy.lax_numpy.shape0->shape(arrays[0])
A:jax._src.numpy.lax_numpy.reps_tup->tuple((operator.index(rep) if core.is_constant_dim(rep) else rep for rep in reps_tup))
A:jax._src.numpy.lax_numpy.arrays_out->jax._src.numpy.util.promote_dtypes(*arrays)
A:jax._src.numpy.lax_numpy.arrs->jax.vmap(atleast_3d)(tup)
A:jax._src.numpy.lax_numpy.(arr, *choices)->broadcast_arrays(arr, *choices)
A:jax._src.numpy.lax_numpy.m->ndim(x)
A:jax._src.numpy.lax_numpy.(xs_tup, depths)->unzip2([_block(x) for x in xs])
A:jax._src.numpy.lax_numpy.rank->ndim(arr)
A:jax._src.numpy.lax_numpy.xs_tup->tuple((_atleast_nd(x, rank) for x in xs_tup))
A:jax._src.numpy.lax_numpy.(out, _)->_block(arrays)
A:jax._src.numpy.lax_numpy._->jax._src.dtypes.coerce_to_array(object, dtype)
A:jax._src.numpy.lax_numpy.object->tree_map(lambda leaf: leaf.__jax_array__() if hasattr(leaf, '__jax_array__') else leaf, object)
A:jax._src.numpy.lax_numpy.leaves->tree_leaves(object)
A:jax._src.numpy.lax_numpy.view->memoryview(object)
A:jax._src.numpy.lax_numpy.out_array->jax.lax.expand_dims(out_array, range(ndmin - ndim(out_array)))
A:jax._src.numpy.lax_numpy.eq->jax._src.numpy.ufuncs.equal(a1, a2)
A:jax._src.numpy.lax_numpy.function->jax.vmap(function, in_axes=tuple(in_axes[::-1]))
A:jax._src.numpy.lax_numpy.N_int->jax._src.core.canonicalize_dim(N, "'N' argument of jnp.eye()")
A:jax._src.numpy.lax_numpy.k->jax._src.core.concrete_or_error(operator.index, k, 'k argument of jnp.triu_indices')
A:jax._src.numpy.lax_numpy.start->jax._src.core.concrete_or_error(operator.index, start, "'start' argument of jnp.rollaxis()")
A:jax._src.numpy.lax_numpy.stop->asarray(stop, dtype=computation_dtype)
A:jax._src.numpy.lax_numpy.step->step.astype(computation_dtype).astype(computation_dtype)
A:jax._src.numpy.lax_numpy.start_dtype->_dtype(start)
A:jax._src.numpy.lax_numpy.num->jax._src.core.concrete_or_error(operator.index, num, "'num' argument of jnp.geomspace")
A:jax._src.numpy.lax_numpy.computation_dtype->jax._src.dtypes.to_inexact_dtype(dtype)
A:jax._src.numpy.lax_numpy.bounds_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax._src.numpy.lax_numpy.broadcast_start->broadcast_to(start, bounds_shape)
A:jax._src.numpy.lax_numpy.broadcast_stop->broadcast_to(stop, bounds_shape)
A:jax._src.numpy.lax_numpy.delta->asarray(nan, dtype=computation_dtype)
A:jax._src.numpy.lax_numpy.empty_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax._src.numpy.lax_numpy.lin->linspace(start, stop, num, endpoint=endpoint, retstep=False, dtype=None, axis=axis)
A:jax._src.numpy.lax_numpy.signflip->signflip.astype(computation_dtype).astype(computation_dtype)
A:jax._src.numpy.lax_numpy.res->argmin(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.lax_numpy.(x_arr,)->jax._src.numpy.util.promote_args_inexact('i0', x)
A:jax._src.numpy.lax_numpy.dimensions->tuple((core.concrete_or_error(operator.index, d, 'dimensions argument of jnp.indices') for d in dimensions))
A:jax._src.numpy.lax_numpy.idx->jax.lax.iota(working_dtype, len(x))
A:jax._src.numpy.lax_numpy.input_shape->shape(a)
A:jax._src.numpy.lax_numpy.total_repeat_length->numpy.sum(repeats)
A:jax._src.numpy.lax_numpy.result_shape->list(shape(a))
A:jax._src.numpy.lax_numpy.exclusive_repeats->roll(repeats, shift=1).at[0].set(0)
A:jax._src.numpy.lax_numpy.scatter_indices->jax._src.numpy.reductions.cumsum(exclusive_repeats)
A:jax._src.numpy.lax_numpy.block_split_indicators->block_split_indicators.at[scatter_indices].add(1).at[scatter_indices].add(1)
A:jax._src.numpy.lax_numpy.m_shape->shape(m)
A:jax._src.numpy.lax_numpy.mask->numpy.ones(a.shape[axis], dtype=bool)
A:jax._src.numpy.lax_numpy.default_int->jax._src.dtypes.canonicalize_dtype(int)
A:jax._src.numpy.lax_numpy.mask_indices->_wrap_indices_function(np.mask_indices)
A:jax._src.numpy.lax_numpy.mk->min(n, m - k)
A:jax._src.numpy.lax_numpy.(i, j)->nonzero(tril(ones((n, m)), k=k), size=_triu_size(m, n, -k))
A:jax._src.numpy.lax_numpy.arr_shape->replace(a.shape, 1)
A:jax._src.numpy.lax_numpy.val->asarray(val)
A:jax._src.numpy.lax_numpy.s->shape(arr)
A:jax._src.numpy.lax_numpy.offset->jax._src.core.concrete_or_error(operator.index, offset, "'offset' argument of jnp.diagonal()")
A:jax._src.numpy.lax_numpy.diag_size->max(0, min(a_shape[axis1] + min(offset, 0), a_shape[axis2] - max(offset, 0)))
A:jax._src.numpy.lax_numpy.j->arange(abs(offset), abs(offset) + diag_size)
A:jax._src.numpy.lax_numpy.v_shape->shape(v)
A:jax._src.numpy.lax_numpy.v_ravel->ravel(v)
A:jax._src.numpy.lax_numpy.v_length->len(v_ravel)
A:jax._src.numpy.lax_numpy.filt->jax._src.core.concrete_or_error(asarray, filt, 'Error arose in the `filt` argument of trim_zeros_tol()')
A:jax._src.numpy.lax_numpy.obj->sort(obj)
A:jax._src.numpy.lax_numpy.obj_array->jax._src.core.concrete_or_error(np.asarray, obj, "'obj' array argument of jnp.delete()")
A:jax._src.numpy.lax_numpy.values_arr->moveaxis(values_arr, 0, axis)
A:jax._src.numpy.lax_numpy.indices->argmax(reductions.cumsum(concatenate([zeros_like(condlist[:1]), condlist], 0), 0), 0)
A:jax._src.numpy.lax_numpy.out_shape->jax.lax.broadcast_shapes(idx_shape, arr_shape)
A:jax._src.numpy.lax_numpy.values_ind->argmax(reductions.cumsum(concatenate([zeros_like(condlist[:1]), condlist], 0), 0), 0).at[argsort(indices)].add(arange(n_insert, dtype=indices.dtype))
A:jax._src.numpy.lax_numpy.arr_mask->ones(n_input + n_insert, dtype=bool).at[values_ind].set(False)
A:jax._src.numpy.lax_numpy.num_dims->ndim(arr)
A:jax._src.numpy.lax_numpy.func->jax.vmap(func, in_axes=0, out_axes=0)
A:jax._src.numpy.lax_numpy.a_arr->expand_dims(b, axis)
A:jax._src.numpy.lax_numpy.b->expand_dims(b, range(ndim(a) - ndim(b)))
A:jax._src.numpy.lax_numpy.(preferred_element_type, output_weak_type)->jax._src.dtypes.result_type(*operands, return_weak_type_flag=True)
A:jax._src.numpy.lax_numpy.num_batch_dims->max(len(a_batch_dims), len(b_batch_dims))
A:jax._src.numpy.lax_numpy.a_ndim->ndim(a)
A:jax._src.numpy.lax_numpy.b_ndim->ndim(b)
A:jax._src.numpy.lax_numpy.ty->next(iter(non_constant_dim_types))
A:jax._src.numpy.lax_numpy.contract_path->_poly_einsum_handlers.get(ty, _default_poly_einsum_handler)
A:jax._src.numpy.lax_numpy.(operands, contractions)->contract_path(*operands, einsum_call=True, use_blas=True, optimize=optimize)
A:jax._src.numpy.lax_numpy.contractions->tuple(((a, frozenset(b), c) for (a, b, c, *_) in contractions))
A:jax._src.numpy.lax_numpy.dummy->collections.namedtuple('dummy', ['shape', 'dtype'])
A:jax._src.numpy.lax_numpy.(out_dummies, contractions)->opt_einsum.contract_path(*dummies, **kwargs)
A:jax._src.numpy.lax_numpy.operands->list(map(asarray, operands))
A:jax._src.numpy.lax_numpy.operand->jax.lax.transpose(operand, perm)
A:jax._src.numpy.lax_numpy.names->names.replace(name, '', count - 1).replace(name, '', count - 1)
A:jax._src.numpy.lax_numpy.eye->jax._src.lax.lax._delta(np.dtype('bool'), operand.shape, axes)
A:jax._src.numpy.lax_numpy.(sqez_axes, keep_axes)->partition_list(keep, list(range(operand.ndim)))
A:jax._src.numpy.lax_numpy.contracted_names->sorted(contracted_names_set)
A:jax._src.numpy.lax_numpy.(input_str, result_names)->einstr.split('->')
A:jax._src.numpy.lax_numpy.input_names->input_str.split(',')
A:jax._src.numpy.lax_numpy.counts->collections.Counter(names)
A:jax._src.numpy.lax_numpy.(operand, names)->sum_repeats(operand, names, counts, result_names)
A:jax._src.numpy.lax_numpy.(lhs, rhs)->map(operands.pop, operand_indices)
A:jax._src.numpy.lax_numpy.(lhs, lhs_names)->sum_repeats(lhs, lhs_names, lhs_counts, result_names + rhs_names)
A:jax._src.numpy.lax_numpy.(rhs, rhs_names)->sum_repeats(rhs, rhs_names, rhs_counts, result_names + lhs_names)
A:jax._src.numpy.lax_numpy.lhs_counts->collections.Counter(lhs_names)
A:jax._src.numpy.lax_numpy.rhs_counts->collections.Counter(rhs_names)
A:jax._src.numpy.lax_numpy.(lhs_batch, rhs_batch)->unzip2(((lhs_names.find(n), rhs_names.find(n)) for n in batch_names))
A:jax._src.numpy.lax_numpy.batch_names_str->''.join(batch_names)
A:jax._src.numpy.lax_numpy.(lhs_cont, rhs_cont)->unzip2(((lhs_names.index(n), rhs_names.index(n)) for n in contracted_names))
A:jax._src.numpy.lax_numpy.remaining_lhs_names->_removechars(lhs_names, deleted_names)
A:jax._src.numpy.lax_numpy.remaining_rhs_names->_removechars(rhs_names, deleted_names)
A:jax._src.numpy.lax_numpy.c->jax.lax.complex(real_part, complex_part)
A:jax._src.numpy.lax_numpy.a_reshaped->expand_dims(a, range(1, 2 * ndim(a), 2))
A:jax._src.numpy.lax_numpy.b_reshaped->expand_dims(b, range(0, 2 * ndim(b), 2))
A:jax._src.numpy.lax_numpy.iota->jax.lax.broadcasted_iota(index_dtype, gather_index_shape, j)
A:jax._src.numpy.lax_numpy.nan_mask->jax._src.numpy.ufuncs.isnan(a)
A:jax._src.numpy.lax_numpy.key_tuple->tuple(keys)
A:jax._src.numpy.lax_numpy.key_arrays->tuple((asarray(k) for k in key_tuple))
A:jax._src.numpy.lax_numpy.axis_num->_canonicalize_axis(axis, arr.ndim)
A:jax._src.numpy.lax_numpy.(_, perm)->jax.lax.sort_key_val(arr, iota, dimension=axis_num)
A:jax._src.numpy.lax_numpy.kth->_canonicalize_axis(kth, arr.shape[axis])
A:jax._src.numpy.lax_numpy.set_to_zero->jax.vmap(set_to_zero)
A:jax._src.numpy.lax_numpy.proxy->set_to_zero(ones(arr.shape), bottom_ind)
A:jax._src.numpy.lax_numpy.b_shape->jax.lax.broadcast_shapes(shift.shape, np.shape(axis))
A:jax._src.numpy.lax_numpy.a_shape_i->array(a.shape[i], dtype=np.int32)
A:jax._src.numpy.lax_numpy.a_concat->jax.lax.concatenate((a, a), i)
A:jax._src.numpy.lax_numpy.shift->_ensure_index_tuple(shift)
A:jax._src.numpy.lax_numpy.bits->expand_dims(bits, tuple(range(arr.ndim - 1)))
A:jax._src.numpy.lax_numpy.packed->(arr << bits).sum(-1).astype('uint8')
A:jax._src.numpy.lax_numpy.unpacked->pad(unpacked, [(0, 0)] * (unpacked.ndim - 1) + [(0, count - unpacked.shape[-1])])
A:jax._src.numpy.lax_numpy.axis_idx->_canonicalize_axis(axis, ndim(a))
A:jax._src.numpy.lax_numpy.index_dims->len(shape(indices))
A:jax._src.numpy.lax_numpy.slice_sizes->list(shape(a))
A:jax._src.numpy.lax_numpy.dnums->jax.lax.GatherDimensionNumbers(offset_dims=tuple(offset_dims), collapsed_slice_dims=tuple(sorted(collapsed_slice_dims)), start_index_map=tuple(start_index_map))
A:jax._src.numpy.lax_numpy.axis_size_val->jax.lax.convert_element_type(core.dimension_as_value(axis_size), _dtype(index))
A:jax._src.numpy.lax_numpy.index_dtype->dtype(int64 if use_64bit_index else int32)
A:jax._src.numpy.lax_numpy.idx_shape->shape(indices)
A:jax._src.numpy.lax_numpy.axis_int->_canonicalize_axis(axis, rank)
A:jax._src.numpy.lax_numpy.lst->list(tup)
A:jax._src.numpy.lax_numpy.use_64bit_index->any((not core.is_constant_dim(d) or d >= 1 << 31 for d in x_shape))
A:jax._src.numpy.lax_numpy.gather_indices_arr->jax.lax.concatenate(gather_indices, dimension=j)
A:jax._src.numpy.lax_numpy.has_partial_slices->any((idx[i].indices(arr.shape[i]) != (0, arr.shape[i], 1) for i in contiguous_slices))
A:jax._src.numpy.lax_numpy.(start, stop, step)->jax.lax.iota(working_dtype, len(x)).indices(size)
A:jax._src.numpy.lax_numpy.start_indices->jax._src.numpy.util.promote_dtypes(*start_indices)
A:jax._src.numpy.lax_numpy.aval->jax._src.core.get_aval(idx)
A:jax._src.numpy.lax_numpy.(treedef, static_idx, dynamic_idx)->_split_index_for_jit(idx, arr.shape)
A:jax._src.numpy.lax_numpy.indexer->_index_to_gather(shape(arr), idx)
A:jax._src.numpy.lax_numpy.fill_value->fill_value.item().item()
A:jax._src.numpy.lax_numpy.(leaves, treedef)->tree_flatten(idx)
A:jax._src.numpy.lax_numpy.(advanced_indexes, idx_advanced_axes, x_advanced_axes)->zip(*advanced_pairs)
A:jax._src.numpy.lax_numpy.advanced_axes_are_contiguous->bool(np.all(np.diff(idx_advanced_axes) == 1))
A:jax._src.numpy.lax_numpy.advanced_indexes->broadcast_arrays(*advanced_indexes)
A:jax._src.numpy.lax_numpy.start_dim->len(gather_indices_shape)
A:jax._src.numpy.lax_numpy.abstract_i->jax._src.core.get_aval(i)
A:jax._src.numpy.lax_numpy.(start, limit, stride, needs_rev)->_static_idx(slice(start, stop, step), x_shape[x_axis])
A:jax._src.numpy.lax_numpy.gather_indices_array->jax.lax.concatenate([lax.broadcast_in_dim(g, gather_indices_shape, tuple(range(i, i + g.ndim))) for (g, i) in gather_indices], last_dim)
A:jax._src.numpy.lax_numpy.last_dim->len(gather_indices_shape)
A:jax._src.numpy.lax_numpy.total_dims->sum((_ndim(e) if _is_boolean_index(e) else 1 for e in idx if e is not None and e is not Ellipsis))
A:jax._src.numpy.lax_numpy.num_ellipsis->sum((e is Ellipsis for e in idx))
A:jax._src.numpy.lax_numpy.i_shape->_shape(i)
A:jax._src.numpy.lax_numpy.len_without_none->sum((1 for e in idx if e is not None and e is not Ellipsis))
A:jax._src.numpy.lax_numpy.ellipsis_index->next(ellipses, None)
A:jax._src.numpy.lax_numpy.M->jax._src.core.concrete_or_error(int, M, 'M argument of jnp.kaiser')
A:jax._src.numpy.lax_numpy.(x1, x2)->jax._src.numpy.util.promote_dtypes(x1, x2)
A:jax._src.numpy.lax_numpy.(gcd, _)->jax.lax.while_loop(_gcd_cond_fn, _gcd_body_fn, (ufuncs.abs(x1), ufuncs.abs(x2)))
A:jax._src.numpy.lax_numpy.d->diag(c)
A:jax._src.numpy.lax_numpy.condition_arr->asarray(condition).astype(bool)
A:jax._src.numpy.lax_numpy.(m, y)->jax._src.numpy.util.promote_args_inexact('cov', m, y)
A:jax._src.numpy.lax_numpy.(m,)->jax._src.numpy.util.promote_args_inexact('cov', m)
A:jax._src.numpy.lax_numpy.X->concatenate((X, y_arr), axis=0)
A:jax._src.numpy.lax_numpy.w->asarray(ufuncs.abs(fweights))
A:jax._src.numpy.lax_numpy.aweights->jax._src.numpy.ufuncs.abs(aweights)
A:jax._src.numpy.lax_numpy.(avg, w_sum)->jax._src.numpy.reductions.average(X, axis=1, weights=w, returned=True)
A:jax._src.numpy.lax_numpy.stddev->jax._src.numpy.ufuncs.sqrt(ufuncs.real(d)).astype(c.dtype)
A:jax._src.numpy.lax_numpy.real_part->clip(ufuncs.real(c), -1, 1)
A:jax._src.numpy.lax_numpy.complex_part->clip(ufuncs.imag(c), -1, 1)
A:jax._src.numpy.lax_numpy.go_left->op(query, sorted_arr[mid])
A:jax._src.numpy.lax_numpy.n_levels->int(np.ceil(np.log2(len(sorted_arr) + 1)))
A:jax._src.numpy.lax_numpy.(carry, _)->jax.lax.scan(body_fun, init, (), length=n_levels, unroll=n_levels if unrolled else 1)
A:jax._src.numpy.lax_numpy.query_flat->query.ravel()
A:jax._src.numpy.lax_numpy.comparisons->jax.vmap(op, in_axes=(0, None))(sorted_arr, query)
A:jax._src.numpy.lax_numpy.(a, v)->jax._src.numpy.util.promote_dtypes(a, v)
A:jax._src.numpy.lax_numpy.right->jax._src.core.concrete_or_error(bool, right, 'right argument of jnp.digitize()')
A:jax._src.numpy.lax_numpy.bins_arr->asarray(bins)
A:jax._src.numpy.lax_numpy.funcdict->dict(funcs)
A:jax._src.numpy.lax_numpy.vals_arr->_tile_to_size(vals_arr, len(indices))
A:jax._src.numpy.lax_numpy.v_arr->_tile_to_size(v_arr, len(ind_arr))
A:jax._src.numpy.lax_numpy.ind_arr->clip(ind_arr, 0, arr.size - 1)
jax._src.numpy.lax_numpy.PadStatFunc(self,array:ArrayLike,/,*,axis:int|None=None,keepdims:bool=False)
jax._src.numpy.lax_numpy.PadStatFunc.__call__(self,array:ArrayLike,/,*,axis:int|None=None,keepdims:bool=False)
jax._src.numpy.lax_numpy._Indexer(NamedTuple)
jax._src.numpy.lax_numpy._ScalarMeta(self,x:Any)
jax._src.numpy.lax_numpy._ScalarMeta.__call__(self,x:Any)
jax._src.numpy.lax_numpy._ScalarMeta.__eq__(self,other:Any)->bool
jax._src.numpy.lax_numpy._ScalarMeta.__hash__(self)->int
jax._src.numpy.lax_numpy._ScalarMeta.__instancecheck__(self,instance:Any)->bool
jax._src.numpy.lax_numpy._ScalarMeta.__ne__(self,other:Any)->bool
jax._src.numpy.lax_numpy._abstractify_scalar_meta(x)
jax._src.numpy.lax_numpy._arange_dynamic(start:DimSize,stop:DimSize,step:DimSize,dtype:DTypeLike)->Array
jax._src.numpy.lax_numpy._argmax(a:Array,axis:int|None=None,keepdims:bool=False)->Array
jax._src.numpy.lax_numpy._argmin(a:Array,axis:int|None=None,keepdims:bool=False)->Array
jax._src.numpy.lax_numpy._atleast_nd(x:ArrayLike,n:int)->Array
jax._src.numpy.lax_numpy._attempt_rewriting_take_via_slice(arr:Array,idx:Any,mode:str|None)->Array | None
jax._src.numpy.lax_numpy._block(xs:ArrayLike|list[ArrayLike])->tuple[Array, int]
jax._src.numpy.lax_numpy._broadcast_to_pairs(nvals:PadValueLike,nd:int,name:str)->PadValue
jax._src.numpy.lax_numpy._canonicalize_tuple_index(arr_ndim,idx,array_name='array')
jax._src.numpy.lax_numpy._check_no_padding(axis_padding:tuple[Any,Any],mode:str)
jax._src.numpy.lax_numpy._concatenate_array(arr:ArrayLike,axis:int|None,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy._conv(x:Array,y:Array,mode:str,op:str,precision:PrecisionLike,preferred_element_type:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy._convert_and_clip_integer(val:ArrayLike,dtype:DType)->Array
jax._src.numpy.lax_numpy._convert_to_array_if_dtype_fails(x:ArrayLike)->ArrayLike
jax._src.numpy.lax_numpy._default_poly_einsum_handler(*operands,**kwargs)
jax._src.numpy.lax_numpy._diag(v,k)
jax._src.numpy.lax_numpy._dtype(x:Any)->DType
jax._src.numpy.lax_numpy._einsum(operands:Sequence,contractions:Sequence[tuple[tuple[int,...],frozenset[str],str]],precision,preferred_element_type,_dot_general=lax.dot_general)
jax._src.numpy.lax_numpy._eliminate_deprecated_list_indexing(idx)
jax._src.numpy.lax_numpy._expand_bool_indices(idx,shape)
jax._src.numpy.lax_numpy._flip(m:Array,axis:int|tuple[int,...]|None=None)->Array
jax._src.numpy.lax_numpy._gather(arr,treedef,static_idx,dynamic_idx,indices_are_sorted,unique_indices,mode,fill_value)
jax._src.numpy.lax_numpy._gcd_body_fn(xs:tuple[Array,Array])->tuple[Array, Array]
jax._src.numpy.lax_numpy._gcd_cond_fn(xs:tuple[Array,Array])->Array
jax._src.numpy.lax_numpy._geomspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,dtype:DTypeLike|None=None,axis:int=0)->Array
jax._src.numpy.lax_numpy._index_to_gather(x_shape:Sequence[int],idx:Sequence[Any],normalize_indices:bool=True)->_Indexer
jax._src.numpy.lax_numpy._int(aval)
jax._src.numpy.lax_numpy._interp(x:ArrayLike,xp:ArrayLike,fp:ArrayLike,left:ArrayLike|str|None=None,right:ArrayLike|str|None=None,period:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy._is_advanced_int_indexer(idx)
jax._src.numpy.lax_numpy._is_boolean_index(i)
jax._src.numpy.lax_numpy._is_contiguous_slice(idx)
jax._src.numpy.lax_numpy._is_int_arraylike(x)
jax._src.numpy.lax_numpy._is_integer_index(idx:Any)->bool
jax._src.numpy.lax_numpy._is_scalar(x)
jax._src.numpy.lax_numpy._is_simple_reverse_slice(idx:Any)->bool
jax._src.numpy.lax_numpy._is_slice_element_none_or_constant(elt)
jax._src.numpy.lax_numpy._is_valid_integer_index_for_slice(idx,size,mode)
jax._src.numpy.lax_numpy._jnp_dtype(obj:DTypeLike|None,*,align:bool=False,copy:bool=False)->DType
jax._src.numpy.lax_numpy._linspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,retstep:bool=False,dtype:DTypeLike|None=None,axis:int=0)->Array | tuple[Array, Array]
jax._src.numpy.lax_numpy._logspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,base:ArrayLike=10.0,dtype:DTypeLike|None=None,axis:int=0)->Array
jax._src.numpy.lax_numpy._make_scalar_type(np_scalar_type:type)->_ScalarMeta
jax._src.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax._src.numpy.lax_numpy._moveaxis(a:Array,source:tuple[int,...],destination:tuple[int,...])->Array
jax._src.numpy.lax_numpy._nanargmax(a,axis:int|None=None,keepdims:bool=False)
jax._src.numpy.lax_numpy._nanargmin(a,axis:int|None=None,keepdims:bool=False)
jax._src.numpy.lax_numpy._normalize_index(index,axis_size)
jax._src.numpy.lax_numpy._pad(array:ArrayLike,pad_width:PadValueLike[int],mode:str,constant_values:ArrayLike,stat_length:PadValueLike[int],end_values:PadValueLike[ArrayLike],reflect_type:str)
jax._src.numpy.lax_numpy._pad_constant(array:Array,pad_width:PadValue[int],constant_values:Array)->Array
jax._src.numpy.lax_numpy._pad_edge(array:Array,pad_width:PadValue[int])->Array
jax._src.numpy.lax_numpy._pad_empty(array:Array,pad_width:PadValue[int])->Array
jax._src.numpy.lax_numpy._pad_func(array:Array,pad_width:PadValue[int],func:Callable[...,Any],**kwargs)->Array
jax._src.numpy.lax_numpy._pad_linear_ramp(array:Array,pad_width:PadValue[int],end_values:PadValue[ArrayLike])->Array
jax._src.numpy.lax_numpy._pad_stats(array:Array,pad_width:PadValue[int],stat_length:PadValue[int]|None,stat_func:PadStatFunc)->Array
jax._src.numpy.lax_numpy._pad_symmetric_or_reflect(array:Array,pad_width:PadValue[int],mode:str,reflect_type:str)->Array
jax._src.numpy.lax_numpy._pad_wrap(array:Array,pad_width:PadValue[int])->Array
jax._src.numpy.lax_numpy._piecewise(x:Array,condlist:Array,consts:dict[int,ArrayLike],funcs:frozenset[tuple[int,Callable[...,Array]]],*args,**kw)->Array
jax._src.numpy.lax_numpy._removechars(s,chars)
jax._src.numpy.lax_numpy._rewriting_take(arr,idx,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax._src.numpy.lax_numpy._roll_dynamic(a:Array,shift:Array,axis:Sequence[int])->Array
jax._src.numpy.lax_numpy._roll_static(a:Array,shift:Sequence[int],axis:Sequence[int])->Array
jax._src.numpy.lax_numpy._searchsorted_via_compare_all(sorted_arr:Array,query:Array,side:str,dtype:type)->Array
jax._src.numpy.lax_numpy._searchsorted_via_scan(unrolled:bool,sorted_arr:Array,query:Array,side:str,dtype:type)->Array
jax._src.numpy.lax_numpy._searchsorted_via_sort(sorted_arr:Array,query:Array,side:str,dtype:type)->Array
jax._src.numpy.lax_numpy._should_unpack_list_index(x)
jax._src.numpy.lax_numpy._split(op:str,ary:ArrayLike,indices_or_sections:int|Sequence[int]|ArrayLike,axis:int=0)->list[Array]
jax._src.numpy.lax_numpy._split_index_for_jit(idx,shape)
jax._src.numpy.lax_numpy._split_on_axis(op:str,axis:int)->Callable[[ArrayLike, int | ArrayLike], list[Array]]
jax._src.numpy.lax_numpy._squeeze(a:Array,axis:tuple[int])->Array
jax._src.numpy.lax_numpy._static_idx(idx:slice,size:DimSize)
jax._src.numpy.lax_numpy._take(a,indices,axis:int|None=None,out=None,mode=None,unique_indices=False,indices_are_sorted=False,fill_value=None)
jax._src.numpy.lax_numpy._tile_to_size(arr:Array,size:int)->Array
jax._src.numpy.lax_numpy._triu_size(n,m,k)
jax._src.numpy.lax_numpy._wrap_indices_function(f)
jax._src.numpy.lax_numpy.allclose(a:ArrayLike,b:ArrayLike,rtol:ArrayLike=1e-05,atol:ArrayLike=1e-08,equal_nan:bool=False)->Array
jax._src.numpy.lax_numpy.angle(z:ArrayLike,deg:bool=False)->Array
jax._src.numpy.lax_numpy.append(arr:ArrayLike,values:ArrayLike,axis:int|None=None)->Array
jax._src.numpy.lax_numpy.apply_along_axis(func1d:Callable,axis:int,arr:ArrayLike,*args,**kwargs)->Array
jax._src.numpy.lax_numpy.apply_over_axes(func:Callable[[ArrayLike,int],Array],a:ArrayLike,axes:Sequence[int])->Array
jax._src.numpy.lax_numpy.arange(start:DimSize,stop:Optional[DimSize]=None,step:DimSize|None=None,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.argmax(a:ArrayLike,axis:int|None=None,out:None=None,keepdims:bool|None=None)->Array
jax._src.numpy.lax_numpy.argmin(a:ArrayLike,axis:int|None=None,out:None=None,keepdims:bool|None=None)->Array
jax._src.numpy.lax_numpy.argpartition(a:ArrayLike,kth:int,axis:int=-1)->Array
jax._src.numpy.lax_numpy.argsort(a:ArrayLike,axis:int|None=-1,kind:str='stable',order:None=None)->Array
jax._src.numpy.lax_numpy.argwhere(a:ArrayLike,*,size:int|None=None,fill_value:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.array(object:Any,dtype:DTypeLike|None=None,copy:bool=True,order:str|None='K',ndmin:int=0)->Array
jax._src.numpy.lax_numpy.array_equal(a1:ArrayLike,a2:ArrayLike,equal_nan:bool=False)->Array
jax._src.numpy.lax_numpy.array_equiv(a1:ArrayLike,a2:ArrayLike)->Array
jax._src.numpy.lax_numpy.array_split(ary:ArrayLike,indices_or_sections:int|Sequence[int]|ArrayLike,axis:int=0)->list[Array]
jax._src.numpy.lax_numpy.asarray(a:Any,dtype:DTypeLike|None=None,order:str|None=None)->Array
jax._src.numpy.lax_numpy.atleast_1d(*arys:ArrayLike)->Array | list[Array]
jax._src.numpy.lax_numpy.atleast_2d(*arys:ArrayLike)->Array | list[Array]
jax._src.numpy.lax_numpy.atleast_3d(*arys:ArrayLike)->Array | list[Array]
jax._src.numpy.lax_numpy.bartlett(M:int)->Array
jax._src.numpy.lax_numpy.bincount(x:ArrayLike,weights:ArrayLike|None=None,minlength:int=0,*,length:int|None=None)->Array
jax._src.numpy.lax_numpy.blackman(M:int)->Array
jax._src.numpy.lax_numpy.block(arrays:ArrayLike|list[ArrayLike])->Array
jax._src.numpy.lax_numpy.broadcast_arrays(*args:ArrayLike)->list[Array]
jax._src.numpy.lax_numpy.broadcast_shapes(*shapes)
jax._src.numpy.lax_numpy.broadcast_to(array:ArrayLike,shape:DimSize|Shape)->Array
jax._src.numpy.lax_numpy.canonicalize_shape(shape:Any,context:str='')->core.Shape
jax._src.numpy.lax_numpy.choose(a:ArrayLike,choices:Sequence[ArrayLike],out:None=None,mode:str='raise')->Array
jax._src.numpy.lax_numpy.clip(a:ArrayLike,a_min:ArrayLike|None=None,a_max:ArrayLike|None=None,out:None=None)->Array
jax._src.numpy.lax_numpy.column_stack(tup:np.ndarray|Array|Sequence[ArrayLike])->Array
jax._src.numpy.lax_numpy.compress(condition:ArrayLike,a:ArrayLike,axis:int|None=None,out:None=None)->Array
jax._src.numpy.lax_numpy.concatenate(arrays:np.ndarray|Array|Sequence[ArrayLike],axis:int|None=0,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.convolve(a:ArrayLike,v:ArrayLike,mode:str='full',*,precision:PrecisionLike=None,preferred_element_type:dtype|None=None)->Array
jax._src.numpy.lax_numpy.copy(a:ArrayLike,order:str|None=None)->Array
jax._src.numpy.lax_numpy.corrcoef(x:ArrayLike,y:ArrayLike|None=None,rowvar:bool=True)->Array
jax._src.numpy.lax_numpy.correlate(a:ArrayLike,v:ArrayLike,mode:str='valid',*,precision:PrecisionLike=None,preferred_element_type:dtype|None=None)->Array
jax._src.numpy.lax_numpy.cov(m:ArrayLike,y:ArrayLike|None=None,rowvar:bool=True,bias:bool=False,ddof:int|None=None,fweights:ArrayLike|None=None,aweights:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.cross(a,b,axisa:int=-1,axisb:int=-1,axisc:int=-1,axis:int|None=None)
jax._src.numpy.lax_numpy.delete(arr:ArrayLike,obj:ArrayLike|slice,axis:int|None=None,*,assume_unique_indices:bool=False)->Array
jax._src.numpy.lax_numpy.diag(v:ArrayLike,k:int=0)->Array
jax._src.numpy.lax_numpy.diag_indices(n:int,ndim:int=2)->tuple[Array, ...]
jax._src.numpy.lax_numpy.diag_indices_from(arr:ArrayLike)->tuple[Array, ...]
jax._src.numpy.lax_numpy.diagflat(v:ArrayLike,k:int=0)->Array
jax._src.numpy.lax_numpy.diagonal(a:ArrayLike,offset:int=0,axis1:int=0,axis2:int=1)->Array
jax._src.numpy.lax_numpy.diff(a:ArrayLike,n:int=1,axis:int=-1,prepend:ArrayLike|None=None,append:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.digitize(x:ArrayLike,bins:ArrayLike,right:bool=False)->Array
jax._src.numpy.lax_numpy.dot(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=None,preferred_element_type:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.dstack(tup:np.ndarray|Array|Sequence[ArrayLike],dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.ediff1d(ary:ArrayLike,to_end:ArrayLike|None=None,to_begin:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.einsum(subscripts,/,*operands,out:None=None,optimize:str='optimal',precision:PrecisionLike=None,preferred_element_type:DTypeLike|None=None,_use_xeinsum:bool=False,_dot_general:Callable[...,Array]=lax.dot_general)->Array
jax._src.numpy.lax_numpy.einsum_path(subscripts,*operands,optimize='greedy')
jax._src.numpy.lax_numpy.empty(shape:Any,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.empty_like(prototype:ArrayLike|DuckTypedArray,dtype:DTypeLike|None=None,shape:Any=None)->Array
jax._src.numpy.lax_numpy.expand_dims(a:ArrayLike,axis:int|Sequence[int])->Array
jax._src.numpy.lax_numpy.extract(condition:ArrayLike,arr:ArrayLike)->Array
jax._src.numpy.lax_numpy.eye(N:DimSize,M:DimSize|None=None,k:int=0,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.fill_diagonal(a:ArrayLike,val:ArrayLike,wrap:bool=False,*,inplace:bool=True)->Array
jax._src.numpy.lax_numpy.fix(x:ArrayLike,out:None=None)->Array
jax._src.numpy.lax_numpy.flatnonzero(a:ArrayLike,*,size:int|None=None,fill_value:None|ArrayLike|tuple[ArrayLike]=None)->Array
jax._src.numpy.lax_numpy.flip(m:ArrayLike,axis:int|Sequence[int]|None=None)->Array
jax._src.numpy.lax_numpy.fliplr(m:ArrayLike)->Array
jax._src.numpy.lax_numpy.flipud(m:ArrayLike)->Array
jax._src.numpy.lax_numpy.fmax(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.numpy.lax_numpy.fmin(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.numpy.lax_numpy.from_dlpack(x:Any)->Array
jax._src.numpy.lax_numpy.frombuffer(buffer:bytes|Any,dtype:DTypeLike=float,count:int=-1,offset:int=0)->Array
jax._src.numpy.lax_numpy.fromfile(*args,**kwargs)
jax._src.numpy.lax_numpy.fromfunction(function:Callable[...,Array],shape:Any,*,dtype:DTypeLike=float,**kwargs)->Array
jax._src.numpy.lax_numpy.fromiter(*args,**kwargs)
jax._src.numpy.lax_numpy.fromstring(string:str,dtype:DTypeLike=float,count:int=-1,*,sep:str)->Array
jax._src.numpy.lax_numpy.full(shape:Any,fill_value:ArrayLike,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.full_like(a:ArrayLike|DuckTypedArray,fill_value:ArrayLike,dtype:DTypeLike|None=None,shape:Any=None)->Array
jax._src.numpy.lax_numpy.gcd(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.numpy.lax_numpy.geomspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,dtype:DTypeLike|None=None,axis:int=0)->Array
jax._src.numpy.lax_numpy.gradient(f:ArrayLike,*varargs:ArrayLike,axis:int|Sequence[int]|None=None,edge_order:int|None=None)->Array | list[Array]
jax._src.numpy.lax_numpy.hamming(M:int)->Array
jax._src.numpy.lax_numpy.hanning(M:int)->Array
jax._src.numpy.lax_numpy.histogram(a:ArrayLike,bins:ArrayLike=10,range:Sequence[ArrayLike]|None=None,weights:ArrayLike|None=None,density:bool|None=None)->tuple[Array, Array]
jax._src.numpy.lax_numpy.histogram2d(x:ArrayLike,y:ArrayLike,bins:ArrayLike|list[ArrayLike]=10,range:Sequence[None|Array|Sequence[ArrayLike]]|None=None,weights:ArrayLike|None=None,density:bool|None=None)->tuple[Array, Array, Array]
jax._src.numpy.lax_numpy.histogram_bin_edges(a:ArrayLike,bins:ArrayLike=10,range:None|Array|Sequence[ArrayLike]=None,weights:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.histogramdd(sample:ArrayLike,bins:ArrayLike|list[ArrayLike]=10,range:Sequence[None|Array|Sequence[ArrayLike]]|None=None,weights:ArrayLike|None=None,density:bool|None=None)->tuple[Array, list[Array]]
jax._src.numpy.lax_numpy.hstack(tup:np.ndarray|Array|Sequence[ArrayLike],dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.i0(x:ArrayLike)->Array
jax._src.numpy.lax_numpy.identity(n:DimSize,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.indices(dimensions:Sequence[int],dtype:DTypeLike=int32,sparse:bool=False)->Array | tuple[Array, ...]
jax._src.numpy.lax_numpy.inner(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=None,preferred_element_type:DType|None=None)->Array
jax._src.numpy.lax_numpy.insert(arr:ArrayLike,obj:ArrayLike|slice,values:ArrayLike,axis:int|None=None)->Array
jax._src.numpy.lax_numpy.interp(x:ArrayLike,xp:ArrayLike,fp:ArrayLike,left:ArrayLike|str|None=None,right:ArrayLike|str|None=None,period:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.isclose(a:ArrayLike,b:ArrayLike,rtol:ArrayLike=1e-05,atol:ArrayLike=1e-08,equal_nan:bool=False)->Array
jax._src.numpy.lax_numpy.iscomplex(x:ArrayLike)->Array
jax._src.numpy.lax_numpy.iscomplexobj(x:Any)->bool
jax._src.numpy.lax_numpy.isreal(x:ArrayLike)->Array
jax._src.numpy.lax_numpy.isrealobj(x:Any)->bool
jax._src.numpy.lax_numpy.isscalar(element:Any)->bool
jax._src.numpy.lax_numpy.issubdtype(arg1:DTypeLike,arg2:DTypeLike)->bool
jax._src.numpy.lax_numpy.ix_(*args:ArrayLike)->tuple[Array, ...]
jax._src.numpy.lax_numpy.kaiser(M:int,beta:ArrayLike)->Array
jax._src.numpy.lax_numpy.kron(a:ArrayLike,b:ArrayLike)->Array
jax._src.numpy.lax_numpy.lcm(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.numpy.lax_numpy.lexsort(keys:Union[Array,np.ndarray,Sequence[ArrayLike]],axis:int=-1)->Array
jax._src.numpy.lax_numpy.linspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,retstep:bool=False,dtype:DTypeLike|None=None,axis:int=0)->Array | tuple[Array, Array]
jax._src.numpy.lax_numpy.load(*args:Any,**kwargs:Any)->Array
jax._src.numpy.lax_numpy.logspace(start:ArrayLike,stop:ArrayLike,num:int=50,endpoint:bool=True,base:ArrayLike=10.0,dtype:DTypeLike|None=None,axis:int=0)->Array
jax._src.numpy.lax_numpy.matmul(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=None,preferred_element_type:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.matrix_transpose(x:ArrayLike,/)->Array
jax._src.numpy.lax_numpy.meshgrid(*xi:ArrayLike,copy:bool=True,sparse:bool=False,indexing:str='xy')->list[Array]
jax._src.numpy.lax_numpy.moveaxis(a:ArrayLike,source:int|Sequence[int],destination:int|Sequence[int])->Array
jax._src.numpy.lax_numpy.nan_to_num(x:ArrayLike,copy:bool=True,nan:ArrayLike=0.0,posinf:ArrayLike|None=None,neginf:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.nanargmax(a:ArrayLike,axis:int|None=None,out:None=None,keepdims:bool|None=None)->Array
jax._src.numpy.lax_numpy.nanargmin(a:ArrayLike,axis:int|None=None,out:None=None,keepdims:bool|None=None)->Array
jax._src.numpy.lax_numpy.nonzero(a:ArrayLike,*,size:int|None=None,fill_value:None|ArrayLike|tuple[ArrayLike,...]=None)->tuple[Array, ...]
jax._src.numpy.lax_numpy.ones(shape:Any,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.ones_like(a:ArrayLike|DuckTypedArray,dtype:DTypeLike|None=None,shape:Any=None)->Array
jax._src.numpy.lax_numpy.outer(a:ArrayLike,b:ArrayLike,out:None=None)->Array
jax._src.numpy.lax_numpy.packbits(a:ArrayLike,axis:int|None=None,bitorder:str='big')->Array
jax._src.numpy.lax_numpy.pad(array:ArrayLike,pad_width:PadValueLike[int|Array|np.ndarray],mode:str|Callable[...,Any]='constant',**kwargs)->Array
jax._src.numpy.lax_numpy.partition(a:ArrayLike,kth:int,axis:int=-1)->Array
jax._src.numpy.lax_numpy.piecewise(x:ArrayLike,condlist:Array|Sequence[ArrayLike],funclist:list[ArrayLike|Callable[...,Array]],*args,**kw)->Array
jax._src.numpy.lax_numpy.place(arr:ArrayLike,mask:ArrayLike,vals:ArrayLike,*,inplace:bool=True)->Array
jax._src.numpy.lax_numpy.put(a:ArrayLike,ind:ArrayLike,v:ArrayLike,mode:str|None=None,*,inplace:bool=True)->Array
jax._src.numpy.lax_numpy.ravel(a:ArrayLike,order:str='C')->Array
jax._src.numpy.lax_numpy.ravel_multi_index(multi_index:Sequence[ArrayLike],dims:Sequence[int],mode:str='raise',order:str='C')->Array
jax._src.numpy.lax_numpy.repeat(a:ArrayLike,repeats:ArrayLike,axis:int|None=None,*,total_repeat_length:int|None=None)->Array
jax._src.numpy.lax_numpy.reshape(a:ArrayLike,newshape:DimSize|Shape,order:str='C')->Array
jax._src.numpy.lax_numpy.resize(a:ArrayLike,new_shape:Shape)->Array
jax._src.numpy.lax_numpy.result_type(*args:Any)->DType
jax._src.numpy.lax_numpy.roll(a:ArrayLike,shift:ArrayLike|Sequence[int],axis:int|Sequence[int]|None=None)->Array
jax._src.numpy.lax_numpy.rollaxis(a:ArrayLike,axis:int,start:int=0)->Array
jax._src.numpy.lax_numpy.rot90(m:ArrayLike,k:int=1,axes:tuple[int,int]=(0,1))->Array
jax._src.numpy.lax_numpy.round(a:ArrayLike,decimals:int=0,out:None=None)->Array
jax._src.numpy.lax_numpy.searchsorted(a:ArrayLike,v:ArrayLike,side:str='left',sorter:None=None,*,method:str='scan')->Array
jax._src.numpy.lax_numpy.select(condlist:Sequence[ArrayLike],choicelist:Sequence[ArrayLike],default:ArrayLike=0)->Array
jax._src.numpy.lax_numpy.sort(a:ArrayLike,axis:int|None=-1,kind:str='quicksort',order:None=None)->Array
jax._src.numpy.lax_numpy.sort_complex(a:ArrayLike)->Array
jax._src.numpy.lax_numpy.split(ary:ArrayLike,indices_or_sections:int|Sequence[int]|ArrayLike,axis:int=0)->list[Array]
jax._src.numpy.lax_numpy.squeeze(a:ArrayLike,axis:int|Sequence[int]|None=None)->Array
jax._src.numpy.lax_numpy.stack(arrays:np.ndarray|Array|Sequence[ArrayLike],axis:int=0,out:None=None,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.swapaxes(a:ArrayLike,axis1:int,axis2:int)->Array
jax._src.numpy.lax_numpy.take(a:ArrayLike,indices:ArrayLike,axis:int|None=None,out:None=None,mode:str|None=None,unique_indices:bool=False,indices_are_sorted:bool=False,fill_value:ArrayLike|None=None)->Array
jax._src.numpy.lax_numpy.take_along_axis(arr:ArrayLike,indices:ArrayLike,axis:int|None,mode:str|lax.GatherScatterMode|None=None)->Array
jax._src.numpy.lax_numpy.tensordot(a:ArrayLike,b:ArrayLike,axes:int|Sequence[int]|Sequence[Sequence[int]]=2,*,precision:PrecisionLike=None,preferred_element_type:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.tile(A:ArrayLike,reps:DimSize|Sequence[DimSize])->Array
jax._src.numpy.lax_numpy.trace(a:ArrayLike,offset:int=0,axis1:int=0,axis2:int=1,dtype:DTypeLike|None=None,out:None=None)->Array
jax._src.numpy.lax_numpy.transpose(a:ArrayLike,axes:Sequence[int]|None=None)->Array
jax._src.numpy.lax_numpy.trapz(y:ArrayLike,x:ArrayLike|None=None,dx:ArrayLike=1.0,axis:int=-1)->Array
jax._src.numpy.lax_numpy.tri(N:int,M:int|None=None,k:int=0,dtype:Optional[DTypeLike]=None)->Array
jax._src.numpy.lax_numpy.tril(m:ArrayLike,k:int=0)->Array
jax._src.numpy.lax_numpy.tril_indices(n:int,k:int=0,m:int|None=None)->tuple[Array, Array]
jax._src.numpy.lax_numpy.tril_indices_from(arr:ArrayLike,k:int=0)->tuple[Array, Array]
jax._src.numpy.lax_numpy.trim_zeros(filt,trim='fb')
jax._src.numpy.lax_numpy.trim_zeros_tol(filt,tol,trim='fb')
jax._src.numpy.lax_numpy.triu(m:ArrayLike,k:int=0)->Array
jax._src.numpy.lax_numpy.triu_indices(n:int,k:int=0,m:int|None=None)->tuple[Array, Array]
jax._src.numpy.lax_numpy.triu_indices_from(arr:ArrayLike,k:int=0)->tuple[Array, Array]
jax._src.numpy.lax_numpy.trunc(x:ArrayLike)->Array
jax._src.numpy.lax_numpy.unpackbits(a:ArrayLike,axis:int|None=None,count:int|None=None,bitorder:str='big')->Array
jax._src.numpy.lax_numpy.unravel_index(indices:ArrayLike,shape:Shape)->tuple[Array, ...]
jax._src.numpy.lax_numpy.unwrap(p:ArrayLike,discont:ArrayLike|None=None,axis:int=-1,period:ArrayLike=2*pi)->Array
jax._src.numpy.lax_numpy.vander(x:ArrayLike,N:int|None=None,increasing:bool=False)->Array
jax._src.numpy.lax_numpy.vdot(a:ArrayLike,b:ArrayLike,*,precision:PrecisionLike=None,preferred_element_type:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.vstack(tup:np.ndarray|Array|Sequence[ArrayLike],dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.where(condition:ArrayLike,x:ArrayLike|None=None,y:ArrayLike|None=None,*,size:int|None=None,fill_value:None|ArrayLike|tuple[ArrayLike,...]=None)->Array | tuple[Array, ...]
jax._src.numpy.lax_numpy.zeros(shape:Any,dtype:DTypeLike|None=None)->Array
jax._src.numpy.lax_numpy.zeros_like(a:ArrayLike|DuckTypedArray,dtype:DTypeLike|None=None,shape:Any=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/vectorize.py----------------------------------------
A:jax._src.numpy.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax._src.numpy.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax._src.numpy.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax._src.numpy.vectorize.num_core_dims->len(core_dims)
A:jax._src.numpy.vectorize.broadcast_shape->jax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.vectorize.out->func(*args)
A:jax._src.numpy.vectorize.out_shapes->map(jnp.shape, out if isinstance(out, tuple) else [out])
A:jax._src.numpy.vectorize.sizes->dict(dim_sizes)
A:jax._src.numpy.vectorize.args->tuple(map(jnp.asarray, args))
A:jax._src.numpy.vectorize.error_context->'on vectorized function with excluded={!r} and signature={!r}'.format(excluded, signature)
A:jax._src.numpy.vectorize.(excluded_func, args)->_apply_excluded(pyfunc, excluded, args)
A:jax._src.numpy.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax._src.numpy.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims, error_context)
A:jax._src.numpy.vectorize.checked_func->_check_output_dims(excluded_func, dim_sizes, output_core_dims, error_context)
A:jax._src.numpy.vectorize.squeeze_indices->tuple((i for (i, size) in enumerate(noncore_shape) if size == 1))
A:jax._src.numpy.vectorize.squeezed_arg->jax._src.numpy.lax_numpy.squeeze(arg, axis=squeeze_indices)
A:jax._src.numpy.vectorize.in_axes->tuple((None if size == 1 else 0 for size in axis_sizes))
A:jax._src.numpy.vectorize.vectorized_func->jax._src.api.vmap(vectorized_func, in_axes)
A:jax._src.numpy.vectorize.result->vectorized_func(*squeezed_args)
jax._src.numpy.vectorize._apply_excluded(func,excluded,args)
jax._src.numpy.vectorize._check_output_dims(func:Callable,dim_sizes:dict[str,int],expected_output_core_dims:list[CoreDims],error_context:str='')->Callable
jax._src.numpy.vectorize._parse_gufunc_signature(signature:str)->tuple[list[CoreDims], list[CoreDims]]
jax._src.numpy.vectorize._parse_input_dimensions(args:tuple[NDArray,...],input_core_dims:list[CoreDims],error_context:str='')->tuple[tuple[int, ...], dict[str, int]]
jax._src.numpy.vectorize._update_dim_sizes(dim_sizes:dict[str,int],shape:tuple[int,...],core_dims:CoreDims,error_context:str='',*,is_input:bool)
jax._src.numpy.vectorize.vectorize(pyfunc,*,excluded=frozenset(),signature=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/numpy/index_tricks.py----------------------------------------
A:jax._src.numpy.index_tricks.stop->jax._src.core.concrete_or_error(None, s.stop, f'slice stop of jnp.{op_name}')
A:jax._src.numpy.index_tricks.newobj->transpose(newobj, shape_obj)
A:jax._src.numpy.index_tricks.output->promote_dtypes(*output)
A:jax._src.numpy.index_tricks.output_arr->meshgrid(*output, indexing='ij', sparse=False)
A:jax._src.numpy.index_tricks.mgrid->_Mgrid()
A:jax._src.numpy.index_tricks.ogrid->_Ogrid()
A:jax._src.numpy.index_tricks.k->len(vec)
A:jax._src.numpy.index_tricks.params->list(map(int, vec))
A:jax._src.numpy.index_tricks.shape_obj->tuple(shape_obj[num_lshifts:] + shape_obj[:num_lshifts])
A:jax._src.numpy.index_tricks.res->expand_dims(res, matrix)
A:jax._src.numpy.index_tricks.r_->RClass()
A:jax._src.numpy.index_tricks.c_->CClass()
jax._src.numpy.index_tricks.CClass(_AxisConcat)
jax._src.numpy.index_tricks.RClass(_AxisConcat)
jax._src.numpy.index_tricks._AxisConcat(abc.ABC)
jax._src.numpy.index_tricks._AxisConcat.__getitem__(self,key:Union[_IndexType,tuple[_IndexType,...]])->Array
jax._src.numpy.index_tricks._AxisConcat.__len__(self)->int
jax._src.numpy.index_tricks._Mgrid
jax._src.numpy.index_tricks._Mgrid.__getitem__(self,key:Union[slice,tuple[slice,...]])->Array
jax._src.numpy.index_tricks._Ogrid
jax._src.numpy.index_tricks._Ogrid.__getitem__(self,key:Union[slice,tuple[slice,...]])->Union[Array, list[Array]]
jax._src.numpy.index_tricks._make_1d_grid_from_slice(s:slice,op_name:str)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/extend/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/extend/random.py----------------------------------------
jax._src.extend.random.define_prng_impl(*,key_shape:Shape,seed:Callable[[Array],Array],split:Callable[[Array,Shape],Array],random_bits:Callable[[Array,int,Shape],Array],fold_in:Callable[[Array,int],Array],name:str='<unnamed>',tag:str='?')->Hashable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/colab_debugger.py----------------------------------------
A:jax._src.debugger.colab_debugger.self._view->jax._src.debugger.colab_lib.dynamic(colab_lib.div())
A:jax._src.debugger.colab_debugger.is_dark_mode->google.colab.output.eval_js('document.documentElement.matches("[theme=dark]");')
A:jax._src.debugger.colab_debugger.lexer->pygments.lexers.get_lexer_by_name('python')
A:jax._src.debugger.colab_debugger.formatter->pygments.formatters.HtmlFormatter(full=False, hl_lines=highlights, linenos=True, linenostart=linenostart, style=code_style)
A:jax._src.debugger.colab_debugger.css_->pygments.formatters.HtmlFormatter(full=False, hl_lines=highlights, linenos=True, linenostart=linenostart, style=code_style).get_style_defs()
A:jax._src.debugger.colab_debugger.code->pygments.highlight(code, lexer, formatter)
A:jax._src.debugger.colab_debugger.(code_, css_)->self._highlight_code(self._code, highlights, linenostart)
A:jax._src.debugger.colab_debugger.uuid_->uuid.uuid4()
A:jax._src.debugger.colab_debugger.code_div->jax._src.debugger.colab_lib.div(colab_lib.css(css_), code_, id=f'code-{uuid_}', style=colab_lib.style({'max-height': '500px', 'overflow-y': 'scroll', 'background-color': 'var(--colab-border-color)', 'padding': '5px 5px 5px 5px'}))
A:jax._src.debugger.colab_debugger.self._header->jax._src.debugger.colab_lib.dynamic(colab_lib.div(colab_lib.span('Breakpoint'), style=colab_lib.style({'background-color': 'var(--colab-secondary-surface-color)', 'color': 'var(--colab-primary-text-color)', 'padding': '5px 5px 5px 5px', 'font-weight': 'bold'})))
A:jax._src.debugger.colab_debugger.self._code_view->CodeViewer('', highlights=[])
A:jax._src.debugger.colab_debugger.filename->self.frame.filename.strip()
A:jax._src.debugger.colab_debugger.self._file_cache[filename]->fp.read()
A:jax._src.debugger.colab_debugger.source->'\n'.join(frame.source)
A:jax._src.debugger.colab_debugger.highlight->min(frame.offset + 1, len(frame.source) - 1)
A:jax._src.debugger.colab_debugger.self._interaction_log->jax._src.debugger.colab_lib.dynamic(colab_lib.div())
A:jax._src.debugger.colab_debugger.self._frame_preview->FramePreview(frame)
A:jax._src.debugger.colab_debugger.self._debugger_view->DebuggerView(self.current_frame())
jax._src.debugger.colab_debugger.CodeViewer(self,code_:str,highlights:list[int],linenostart:int=1)
jax._src.debugger.colab_debugger.CodeViewer.__init__(self,code_:str,highlights:list[int],linenostart:int=1)
jax._src.debugger.colab_debugger.CodeViewer._highlight_code(self,code:str,highlights,linenostart:int)
jax._src.debugger.colab_debugger.CodeViewer.append(self,child)
jax._src.debugger.colab_debugger.CodeViewer.clear(self)
jax._src.debugger.colab_debugger.CodeViewer.render(self)
jax._src.debugger.colab_debugger.CodeViewer.update(self,elem)
jax._src.debugger.colab_debugger.CodeViewer.update_code(self,code_,highlights,*,linenostart:int=1)
jax._src.debugger.colab_debugger.ColabDebugger(self,frames:list[debugger_core.DebuggerFrame],thread_id:int)
jax._src.debugger.colab_debugger.ColabDebugger.__init__(self,frames:list[debugger_core.DebuggerFrame],thread_id:int)
jax._src.debugger.colab_debugger.ColabDebugger.do_down(self,arg)
jax._src.debugger.colab_debugger.ColabDebugger.do_up(self,arg)
jax._src.debugger.colab_debugger.ColabDebugger.run(self)
jax._src.debugger.colab_debugger.DebuggerView(self,frame,*,log_color='')
jax._src.debugger.colab_debugger.DebuggerView.__init__(self,frame,*,log_color='')
jax._src.debugger.colab_debugger.DebuggerView.append(self,child)
jax._src.debugger.colab_debugger.DebuggerView.clear(self)
jax._src.debugger.colab_debugger.DebuggerView.flush(self)
jax._src.debugger.colab_debugger.DebuggerView.isatty(self)
jax._src.debugger.colab_debugger.DebuggerView.read(self)
jax._src.debugger.colab_debugger.DebuggerView.readline(self)
jax._src.debugger.colab_debugger.DebuggerView.render(self)
jax._src.debugger.colab_debugger.DebuggerView.update(self,elem)
jax._src.debugger.colab_debugger.DebuggerView.update_frame(self,frame)
jax._src.debugger.colab_debugger.DebuggerView.write(self,text)
jax._src.debugger.colab_debugger.FramePreview(self,frame)
jax._src.debugger.colab_debugger.FramePreview.__init__(self,frame)
jax._src.debugger.colab_debugger.FramePreview.append(self,child)
jax._src.debugger.colab_debugger.FramePreview.clear(self)
jax._src.debugger.colab_debugger.FramePreview.render(self)
jax._src.debugger.colab_debugger.FramePreview.update(self,elem)
jax._src.debugger.colab_debugger.FramePreview.update_frame(self,frame)
jax._src.debugger.colab_debugger._run_debugger(frames,thread_id,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/web_debugger.py----------------------------------------
A:jax._src.debugger.web_debugger.web_pdb_version->tuple(map(int, web_pdb.__version__.split('.')))
A:jax._src.debugger.web_debugger._web_consoles[host, port]->web_pdb.WebConsole(host, port, self)
A:jax._src.debugger.web_debugger._web_console._debugger->weakref.proxy(self)
A:jax._src.debugger.web_debugger.current_frame->self.current_frame()
A:jax._src.debugger.web_debugger.globals->'\n'.join([f'{key} = {value}' for (key, value) in sorted(current_frame.globals.items())])
A:jax._src.debugger.web_debugger.locals->'\n'.join([f'{key} = {value}' for (key, value) in sorted(current_frame.locals.items())])
jax._src.debugger.web_debugger.WebDebugger(self,frames:list[debugger_core.DebuggerFrame],thread_id,completekey:str='tab',host:str='',port:int=5555)
jax._src.debugger.web_debugger.WebDebugger.__init__(self,frames:list[debugger_core.DebuggerFrame],thread_id,completekey:str='tab',host:str='',port:int=5555)
jax._src.debugger.web_debugger.WebDebugger.get_current_frame_data(self)
jax._src.debugger.web_debugger.WebDebugger.get_globals(self)
jax._src.debugger.web_debugger.WebDebugger.get_locals(self)
jax._src.debugger.web_debugger.WebDebugger.run(self)
jax._src.debugger.web_debugger.run_debugger(frames:list[debugger_core.DebuggerFrame],thread_id:int|None,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/cli_debugger.py----------------------------------------
A:jax._src.debugger.cli_debugger.msg->traceback.format_exception_only(*exc_info)[-1].strip()
jax._src.debugger.cli_debugger.CliDebugger(self,frames:list[DebuggerFrame],thread_id,stdin:IO[str]|None=None,stdout:IO[str]|None=None,completekey:str='tab')
jax._src.debugger.cli_debugger.CliDebugger.__init__(self,frames:list[DebuggerFrame],thread_id,stdin:IO[str]|None=None,stdout:IO[str]|None=None,completekey:str='tab')
jax._src.debugger.cli_debugger.CliDebugger._error_message(self)
jax._src.debugger.cli_debugger.CliDebugger.current_frame(self)
jax._src.debugger.cli_debugger.CliDebugger.default(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_continue(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_down(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_list(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_p(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_pp(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_quit(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_up(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_where(self,_)
jax._src.debugger.cli_debugger.CliDebugger.evaluate(self,expr)
jax._src.debugger.cli_debugger.CliDebugger.print_backtrace(self)
jax._src.debugger.cli_debugger.CliDebugger.print_context(self,num_lines=2)
jax._src.debugger.cli_debugger.CliDebugger.run(self)
jax._src.debugger.cli_debugger.run_debugger(frames:list[DebuggerFrame],thread_id:int|None,**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/colab_lib.py----------------------------------------
A:jax._src.debugger.colab_lib.self._uuid->str(uuid.uuid4())
A:jax._src.debugger.colab_lib.self._root_elem->div(id=self.tag)
A:jax._src.debugger.colab_lib.children->'\n'.join([str(c) for c in self.children])
A:jax._src.debugger.colab_lib.code->functools.partial(_make_elem, 'code')
A:jax._src.debugger.colab_lib.div->functools.partial(_make_elem, 'div')
A:jax._src.debugger.colab_lib.li->functools.partial(_make_elem, 'li')
A:jax._src.debugger.colab_lib.ol->functools.partial(_make_elem, 'ol')
A:jax._src.debugger.colab_lib.pre->functools.partial(_make_elem, 'pre')
A:jax._src.debugger.colab_lib.progress->functools.partial(_make_elem, 'progress')
A:jax._src.debugger.colab_lib.span->functools.partial(_make_elem, 'span')
jax._src.debugger.colab_lib.DOMElement(metaclass=abc.ABCMeta)
jax._src.debugger.colab_lib.DOMElement.render(self)
jax._src.debugger.colab_lib.DynamicDOMElement(DOMElement)
jax._src.debugger.colab_lib.DynamicDOMElement.append(self,child:DOMElement)
jax._src.debugger.colab_lib.DynamicDOMElement.clear(self)
jax._src.debugger.colab_lib.DynamicDOMElement.render(self)
jax._src.debugger.colab_lib.DynamicDOMElement.update(self,elem:DOMElement)
jax._src.debugger.colab_lib.DynamicDiv(DynamicDOMElement)
jax._src.debugger.colab_lib.DynamicDiv.__post_init__(self)
jax._src.debugger.colab_lib.DynamicDiv.append(self,child:DOMElement)
jax._src.debugger.colab_lib.DynamicDiv.clear(self)
jax._src.debugger.colab_lib.DynamicDiv.render(self)
jax._src.debugger.colab_lib.DynamicDiv.tag(self)
jax._src.debugger.colab_lib.DynamicDiv.update(self,elem:DOMElement)
jax._src.debugger.colab_lib.StaticDOMElement(DOMElement)
jax._src.debugger.colab_lib.StaticDOMElement.__repr__(self)
jax._src.debugger.colab_lib.StaticDOMElement.__str__(self)
jax._src.debugger.colab_lib.StaticDOMElement.append(self,child:DOMElement)->DOMElement
jax._src.debugger.colab_lib.StaticDOMElement.attr(self,key:str)->str
jax._src.debugger.colab_lib.StaticDOMElement.html(self)
jax._src.debugger.colab_lib.StaticDOMElement.render(self)
jax._src.debugger.colab_lib.StaticDOMElement.replace(self,**kwargs)->DOMElement
jax._src.debugger.colab_lib._make_elem(tag:str,*children:Element,**attrs)->StaticDOMElement
jax._src.debugger.colab_lib._style_dict_to_str(style_dict:dict[str,Any])->str
jax._src.debugger.colab_lib.css(text:str)->StaticDOMElement
jax._src.debugger.colab_lib.dynamic(elem:StaticDOMElement)->DynamicDiv
jax._src.debugger.colab_lib.style(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/debugger/core.py----------------------------------------
A:jax._src.debugger.core.cant_flatten->_CantFlatten()
A:jax._src.debugger.core.(flat_locals, locals_tree)->_safe_flatten_dict(self.locals)
A:jax._src.debugger.core.(flat_globals, globals_tree)->_safe_flatten_dict(self.globals)
A:jax._src.debugger.core.(invalid_vars, valid_vars)->jax._src.util.partition_list(is_valid, flat_vars)
A:jax._src.debugger.core.flat_vars->jax._src.util.merge_lists(is_valid, invalid_vars, valid_vars)
A:jax._src.debugger.core.(flat_locals, flat_globals)->jax._src.util.split_list(flat_vars, [num_locals])
A:jax._src.debugger.core.locals_->jax.tree_util.tree_unflatten(locals_tree, flat_locals).to_dict()
A:jax._src.debugger.core.globals_->jax.tree_util.tree_unflatten(globals_tree, flat_globals).to_dict()
A:jax._src.debugger.core.(_, start)->inspect.getsourcelines(frame_info.frame)
A:jax._src.debugger.core.source->inspect.getsource(frame_info.frame).split('\n')
A:jax._src.debugger.core.debuggers->sorted(_debugger_registry.values(), key=lambda x: -x[0])
A:jax._src.debugger.core.debug_lock->threading.Lock()
A:jax._src.debugger.core.frame_infos->inspect.stack()
A:jax._src.debugger.core.(flat_args, frames_tree)->jax.tree_util.tree_flatten(frames)
A:jax._src.debugger.core.frames->jax.tree_util.tree_unflatten(frames_tree, flat_args)
A:jax._src.debugger.core.thread_id->threading.get_ident()
A:jax._src.debugger.core.debugger->get_debugger(backend=backend)
A:jax._src.debugger.core.(token, flat_args)->jax.lax.stop_gradient((token, flat_args))
jax._src.debugger.breakpoint(*,backend:str|None=None,filter_frames:bool=True,num_frames:int|None=None,ordered:bool=False,token=None,**kwargs)
jax._src.debugger.core.Debugger(self,frames:list[DebuggerFrame],thread_id:int|None,**kwargs:Any)
jax._src.debugger.core.Debugger.__call__(self,frames:list[DebuggerFrame],thread_id:int|None,**kwargs:Any)
jax._src.debugger.core.DebuggerFrame
jax._src.debugger.core.DebuggerFrame.from_frameinfo(cls,frame_info)->DebuggerFrame
jax._src.debugger.core.DebuggerFrame.tree_flatten(self)
jax._src.debugger.core.DebuggerFrame.tree_unflatten(cls,info,valid_vars)
jax._src.debugger.core._CantFlatten
jax._src.debugger.core._DictWrapper(self,keys,values)
jax._src.debugger.core._DictWrapper.__init__(self,keys,values)
jax._src.debugger.core._DictWrapper.to_dict(self)
jax._src.debugger.core._DictWrapper.tree_flatten(self)
jax._src.debugger.core._DictWrapper.tree_unflatten(cls,keys,values)
jax._src.debugger.core._safe_flatten_dict(dct:dict[Any,Any])->tuple[list[Any], tree_util.PyTreeDef]
jax._src.debugger.core.breakpoint(*,backend:str|None=None,filter_frames:bool=True,num_frames:int|None=None,ordered:bool=False,token=None,**kwargs)
jax._src.debugger.core.get_debugger(backend:str|None=None)->Debugger
jax._src.debugger.core.register_debugger(name:str,debugger:Debugger,priority:int)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/internal_test_util/deprecation_module.py----------------------------------------
A:jax._src.internal_test_util.deprecation_module.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/internal_test_util/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/internal_test_util/lax_test_util.py----------------------------------------
A:jax._src.internal_test_util.lax_test_util.OpRecord->collections.namedtuple('OpRecord', ['op', 'nargs', 'dtypes', 'rng_factory', 'tol'])
A:jax._src.internal_test_util.lax_test_util.ReducerOpRecord->collections.namedtuple('ReducerOpRecord', ['op', 'reference_op', 'init_val', 'dtypes', 'primitive'])
A:jax._src.internal_test_util.lax_test_util.shape->list(shape)
A:jax._src.internal_test_util.lax_test_util.slicers->map(slicer, args, bdims)
jax._src.internal_test_util.lax_test_util.add_bdim(bdim_size,bdim,shape)
jax._src.internal_test_util.lax_test_util.all_bdims(*shapes)
jax._src.internal_test_util.lax_test_util.args_slicer(args,bdims)
jax._src.internal_test_util.lax_test_util.lax_ops()
jax._src.internal_test_util.lax_test_util.lax_reduce_ops()
jax._src.internal_test_util.lax_test_util.op_record(op,nargs,dtypes,rng_factory,tol=None)
jax._src.internal_test_util.lax_test_util.slicer(x,bdim)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/internal_test_util/lazy_loader_module/__init__.py----------------------------------------
A:jax._src.internal_test_util.lazy_loader_module.__init__.(__getattr__, __dir__, __all__)->jax._src.lazy_loader.attach(__name__, ['lazy_test_submodule'])


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/internal_test_util/lazy_loader_module/lazy_test_submodule.py----------------------------------------
jax._src.internal_test_util.lazy_loader_module.lazy_test_submodule.a_function()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/core.py----------------------------------------
A:jax._src.pallas.core.mapped->Mapped()
A:jax._src.pallas.core.block_shape->tuple((mapped if s is None else s for s in block_shape))
A:jax._src.pallas.core.out->self.index_map(*args)
A:jax._src.pallas.core.(discharged_jaxpr, discharged_consts)->jax._src.state.discharge.discharge_state(self.index_map_jaxpr.jaxpr, self.index_map_jaxpr.consts)
A:jax._src.pallas.core.jaxpr->jax._src.core.ClosedJaxpr(discharged_jaxpr, discharged_consts)
A:jax._src.pallas.core.block_indices_and_rest->jax._src.core.jaxpr_as_fun(jaxpr)(*loop_idx, *args)
A:jax._src.pallas.core.(block_indices, _)->split_list(block_indices_and_rest, [len(self.block_shape)])
A:jax._src.pallas.core.(flat_fun, _)->jax._src.api_util.flatten_fun(lu.wrap_init(compute_index), in_tree)
A:jax._src.pallas.core.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals)
A:jax._src.pallas.core.shape->tuple((s for s in block_shape if s is not None))
A:jax._src.pallas.core.in_ref_avals->map(state.AbstractRef, in_avals)
A:jax._src.pallas.core.out_ref_avals->map(state.AbstractRef, out_avals)
A:jax._src.pallas.core.no_block_spec->NoBlockSpec()
A:jax._src.pallas.core.in_specs->tuple(in_specs)
A:jax._src.pallas.core.out_specs->tuple(out_specs)
A:jax._src.pallas.core.self.grid->_preprocess_grid(grid)
A:jax._src.pallas.core.(flat_in_specs, self.in_specs_tree)->jax._src.tree_util.tree_flatten(in_specs)
A:jax._src.pallas.core.self.in_specs->tuple(flat_in_specs)
A:jax._src.pallas.core.(flat_out_specs, self.out_specs_tree)->jax._src.tree_util.tree_flatten(out_specs)
A:jax._src.pallas.core.self.out_specs->tuple(flat_out_specs)
A:jax._src.pallas.core.(flat_in_specs, flat_out_specs)->self._get_in_out_specs(in_avals, in_tree, out_avals, out_tree)
A:jax._src.pallas.core.(in_specs, in_ref_avals, out_specs, out_ref_avals)->_get_ref_avals(self.grid, in_avals, flat_in_specs, out_avals, flat_out_specs)
A:jax._src.pallas.core.grid_tree->jax._src.tree_util.tree_structure((tuple(grid_avals), {}))
A:jax._src.pallas.core.in_block_mappings->map(partial(_convert_block_spec_to_block_mapping, grid_avals, in_tree=grid_tree), in_specs, in_ref_avals)
A:jax._src.pallas.core.out_block_mappings->map(partial(_convert_block_spec_to_block_mapping, grid_avals, in_tree=grid_tree), out_specs, out_ref_avals)
A:jax._src.pallas.core.grid_mapping->GridMapping(self.grid, (*in_block_mappings, *out_block_mappings), (), num_index_operands=0, num_scratch_operands=0)
A:jax._src.pallas.core.jaxpr_in_avals->jax._src.tree_util.tree_unflatten(in_tree, in_ref_avals)
A:jax._src.pallas.core.jaxpr_out_avals->jax._src.tree_util.tree_unflatten(out_tree, out_ref_avals)
jax._src.pallas.core.BlockMapping
jax._src.pallas.core.BlockMapping.compute_start_indices(self,loop_idx,*args)
jax._src.pallas.core.BlockSpec(self,index_map:Callable[...,Any]|None=None,block_shape:tuple[int|None,...]|None=None,memory_space:Any=None)
jax._src.pallas.core.BlockSpec.__init__(self,index_map:Callable[...,Any]|None=None,block_shape:tuple[int|None,...]|None=None,memory_space:Any=None)
jax._src.pallas.core.BlockSpec.compute_index(self,*args)
jax._src.pallas.core.GridEnv
jax._src.pallas.core.GridMapping
jax._src.pallas.core.GridSpec(self,grid:Grid|None=None,in_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec)
jax._src.pallas.core.GridSpec.__init__(self,grid:Grid|None=None,in_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec)
jax._src.pallas.core.GridSpec._get_in_out_specs(self,in_avals,in_tree,out_avals,out_tree)
jax._src.pallas.core.GridSpec.get_grid_mapping(self,in_avals,in_tree,out_avals,out_tree)->tuple[tuple[jax_core.AbstractValue, ...], GridMapping]
jax._src.pallas.core.Mapped
jax._src.pallas.core.NoBlockSpec
jax._src.pallas.core._convert_block_spec_to_block_mapping(in_avals:list[jax_core.ShapedArray],block_spec:BlockSpec|None,aval:jax_core.ShapedArray,in_tree:Any)->BlockSpec | None
jax._src.pallas.core._get_ref_avals(grid,in_avals,in_specs,out_avals,out_specs)
jax._src.pallas.core._preprocess_grid(grid:Grid|int|None)->Grid
jax._src.pallas.core._tile_ref(ref:state.AbstractRef,block_shape:tuple[int,...]|None)->state.AbstractRef
jax._src.pallas.core.current_grid_env()->tuple[GridEnv, ...] | None
jax._src.pallas.core.grid_env(env:tuple[tuple[Any,int],...])->Iterator[None]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/utils.py----------------------------------------
A:jax._src.pallas.utils.size->numpy.prod(shape)
A:jax._src.pallas.utils.jaxpr->jaxpr.replace(eqns=jaxpr.eqns[:eqn_index] + jaxpr.eqns[eqn_index + 1:], outvars=jaxpr.outvars[1:]).replace(eqns=jaxpr.eqns[:eqn_index] + jaxpr.eqns[eqn_index + 1:], outvars=jaxpr.outvars[1:])
jax._src.pallas.utils.cdiv(a:int,b:int)->int
jax._src.pallas.utils.next_power_of_2(x:int)->int
jax._src.pallas.utils.pattern_match_scan_to_fori_loop(jaxpr:jax_core.Jaxpr,num_consts:int,num_carry:int)->tuple[jax_core.Jaxpr, bool]
jax._src.pallas.utils.strides_from_shape(shape:Tuple[int,...])->Tuple[int, ...]
jax._src.pallas.utils.when(condition)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/indexing.py----------------------------------------
A:jax._src.pallas.indexing.broadcast_to_p->jax.core.Primitive('broadcast_to')
A:jax._src.pallas.indexing.(start, stop, step)->slc.indices(size)
A:jax._src.pallas.indexing.(slice_idx, non_slice_idx)->partition_list(indexed_dims, self.indices)
A:jax._src.pallas.indexing.(flat_idx, idx_tree)->jax.tree_util.tree_flatten(non_slice_idx)
A:jax._src.pallas.indexing.non_slice_idx->jax.tree_util.tree_unflatten(idx_tree, flat_idx)
A:jax._src.pallas.indexing.indices->merge_lists(is_int_indexing, other_indexers, int_indexers)
A:jax._src.pallas.indexing.(other_indexers, int_indexers)->partition_list(is_int_indexing, indices)
A:jax._src.pallas.indexing.bcast_shape->numpy.broadcast_shapes(*indexer_shapes)
A:jax._src.pallas.indexing.(other_indexers, _)->partition_list(is_int_indexing, self.indices)
jax._src.pallas.indexing.NDIndexer
jax._src.pallas.indexing.NDIndexer.__post_init__(self)
jax._src.pallas.indexing.NDIndexer.from_indices_shape(cls,indices,shape)->NDIndexer
jax._src.pallas.indexing.NDIndexer.get_indexer_shape(self)->Tuple[int, ...]
jax._src.pallas.indexing.NDIndexer.tree_flatten(self)
jax._src.pallas.indexing.NDIndexer.tree_unflatten(cls,data,flat_idx)
jax._src.pallas.indexing.Slice
jax._src.pallas.indexing.Slice.__post_init__(self)
jax._src.pallas.indexing.Slice.from_slice(cls,slc:slice,size:int)->Slice
jax._src.pallas.indexing.Slice.tree_flatten(self)
jax._src.pallas.indexing.Slice.tree_unflatten(cls,aux_data,children)->Slice
jax._src.pallas.indexing._broadcast_to_abstract_eval(aval,*,shape)
jax._src.pallas.indexing._broadcast_to_impl(a,*,shape)
jax._src.pallas.indexing.broadcast_to(a:jax.Array,shape:Tuple[int,...])->jax.Array
jax._src.pallas.indexing.dslice(start:int|jax.Array|None,size:int|None=None)->slice | Slice


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/pallas_call.py----------------------------------------
A:jax._src.pallas.pallas_call.pallas_call_p->jax._src.core.Primitive('pallas_call')
A:jax._src.pallas.pallas_call.output->jax.lax.dynamic_slice(value, start_idx, slice_sizes=block_shape)
A:jax._src.pallas.pallas_call.squeeze_dims->tuple(np.arange(len(is_indexing))[np.array(is_indexing, dtype=np.bool_)])
A:jax._src.pallas.pallas_call.broadcast_dims->tuple((i for (i, b) in enumerate(is_indexing) if not b))
A:jax._src.pallas.pallas_call.update->jax.lax.broadcast_in_dim(update, block_shape, broadcast_dims)
A:jax._src.pallas.pallas_call.(discharged_jaxpr, consts)->jax._src.state.discharge.discharge_state(jaxpr, ())
A:jax._src.pallas.pallas_call.loop_indices->jax.numpy.array(list(it.product(*(range(g) for g in grid))))
A:jax._src.pallas.pallas_call.(scalars, args)->split_list(args, [grid_mapping.num_index_operands])
A:jax._src.pallas.pallas_call.blocks->jax.core.eval_jaxpr(discharged_jaxpr, consts, *scalars, *blocks)
A:jax._src.pallas.pallas_call.(local_grid_env, _)->partition_list(is_mapped_grid_dim, zip(loop_idx, grid_mapping.grid))
A:jax._src.pallas.pallas_call.carry->map(_maybe_dynamic_update_slice, start_indices, block_shapes, carry, blocks, is_indexing_dim)
A:jax._src.pallas.pallas_call.(_, *carry)->jax.lax.while_loop(cond, body, (0, *carry))
A:jax._src.pallas.pallas_call.(_, out)->split_list(carry, [len(args)])
A:jax._src.pallas.pallas_call.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, ())
A:jax._src.pallas.pallas_call.(jvp_jaxpr_, _)->jax.interpreters.ad.jvp_jaxpr(closed_jaxpr, nonzero_tangents_with_outputs, [])
A:jax._src.pallas.pallas_call.(primal_refs, primal_out_refs, tangent_refs, tangent_out_refs)->split_list(jvp_jaxpr.invars, [len(primals), len(out_shapes), len(tangents)])
A:jax._src.pallas.pallas_call.jvp_jaxpr->jvp_jaxpr.replace(invars=invars).replace(invars=invars)
A:jax._src.pallas.pallas_call.(in_bms, out_bms)->split_list(grid_mapping.block_mappings, [len(primals)])
A:jax._src.pallas.pallas_call.out_flat->jax._src.core.Primitive('pallas_call').bind(*consts, *flat_args, jaxpr=jaxpr, name=name, which_linear=which_linear, in_shapes=tuple((jax.ShapeDtypeStruct(a.shape, a.dtype) for a in flat_args)), out_shapes=tuple(flat_out_shapes), debug=debug, interpret=interpret, grid_mapping=grid_mapping, input_output_aliases=tuple(input_output_aliases.items()), **compiler_params)
A:jax._src.pallas.pallas_call.(out_primals, out_tangents)->split_list(out_flat, [len(out_flat) // 2])
A:jax._src.pallas.pallas_call.indices->jax._src.core.eval_jaxpr(block_mapping.index_map_jaxpr.jaxpr, block_mapping.index_map_jaxpr.consts, *args)
A:jax._src.pallas.pallas_call.i32_aval->jax._src.core.ShapedArray((), jnp.int32)
A:jax._src.pallas.pallas_call.(block_mapping_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(_block_map_function), idx_avals)
A:jax._src.pallas.pallas_call.new_block_shape->tuple_insert(shape, dim, pallas_core.mapped)
A:jax._src.pallas.pallas_call.jaxpr->_hoist_consts_to_refs(jaxpr)
A:jax._src.pallas.pallas_call.dims_->list(dims)
A:jax._src.pallas.pallas_call.args_->list(args)
A:jax._src.pallas.pallas_call.args_[input_index]->jax.interpreters.batching.broadcast(args_[input_index], axis_size, 0)
A:jax._src.pallas.pallas_call.args->tuple(args_)
A:jax._src.pallas.pallas_call.dims->tuple(dims_)
A:jax._src.pallas.pallas_call.batched_block_mappings->map(partial(_batch_block_mapping, grid_mapping.grid), avals[num_index_operands:], all_dims[num_index_operands:], block_mappings)
A:jax._src.pallas.pallas_call.batched_in_shapes->tuple((jax.ShapeDtypeStruct(x.shape if dim is batching.not_mapped else tuple_insert(x.shape, dim, axis_size), x.dtype) for (x, dim) in zip(in_shapes, dims)))
A:jax._src.pallas.pallas_call.batched_out_shapes->tuple((jax.ShapeDtypeStruct(tuple_insert(x.shape, 0, axis_size), x.dtype) for x in out_shapes))
A:jax._src.pallas.pallas_call.batched_grid_mapping->grid_mapping.replace(grid=(axis_size, *grid_mapping.grid), block_mappings=tuple(batched_block_mappings), mapped_dims=(0,) + tuple((a + 1 for a in grid_mapping.mapped_dims)))
A:jax._src.pallas.pallas_call.out->jax.tree_util.tree_unflatten(out_tree, out_flat)
A:jax._src.pallas.pallas_call.(const_avals, const_ref_avals)->partition_list(is_const_ref, all_const_avals)
A:jax._src.pallas.pallas_call.const_avals->map(state.AbstractRef, const_avals)
A:jax._src.pallas.pallas_call.merged_const_avals->merge_lists(is_const_ref, const_avals, const_ref_avals)
A:jax._src.pallas.pallas_call.arg_avals->list((var.aval for var in jaxpr.invars))
A:jax._src.pallas.pallas_call.num_consts->len(merged_const_avals)
A:jax._src.pallas.pallas_call.(all_consts, args)->split_list(consts_args, [num_consts])
A:jax._src.pallas.pallas_call.(consts, const_refs)->partition_list(is_const_ref, all_consts)
A:jax._src.pallas.pallas_call.consts->map(lambda x: sp.ref_get(x, ()), consts)
A:jax._src.pallas.pallas_call.all_consts->merge_lists(is_const_ref, consts, const_refs)
A:jax._src.pallas.pallas_call.(hoisted_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(_hoist), in_avals)
A:jax._src.pallas.pallas_call.(avals, grid_mapping)->GridSpec(grid, in_specs, out_specs).get_grid_mapping(flat_in_avals, in_tree, flat_out_avals, out_tree)
A:jax._src.pallas.pallas_call.(jaxpr_flat_avals, jaxpr_in_tree)->jax.tree_util.tree_flatten(avals)
A:jax._src.pallas.pallas_call.(wrapped_fun, out_tree_thunk)->jax.api_util.flatten_fun_nokwargs(lu.wrap_init(fun), jaxpr_in_tree)
A:jax._src.pallas.pallas_call.debug->jax.interpreters.partial_eval.debug_info(fun, jaxpr_in_tree, out_tree_thunk, False, 'pallas_call')
A:jax._src.pallas.pallas_call.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, jaxpr_flat_avals, debug)
A:jax._src.pallas.pallas_call.name->_extract_function_name(f, name)
A:jax._src.pallas.pallas_call.grid_spec->GridSpec(grid, in_specs, out_specs)
A:jax._src.pallas.pallas_call.out_shape->tuple(out_shape)
A:jax._src.pallas.pallas_call.(flat_out_shapes, out_tree)->jax.tree_util.tree_flatten(out_shape)
A:jax._src.pallas.pallas_call.(flat_args, in_tree)->jax.tree_util.tree_flatten(args)
A:jax._src.pallas.pallas_call.flat_in_avals->tuple((jax_core.raise_to_shaped(jax_core.get_aval(a)) for a in flat_args))
A:jax._src.pallas.pallas_call.flat_out_avals->tuple((jax_core.ShapedArray(v.shape, v.dtype) for v in flat_out_shapes))
A:jax._src.pallas.pallas_call.(grid_mapping, jaxpr, consts, _)->_trace_to_jaxpr(f, grid_spec, flat_in_avals, flat_out_avals, in_tree, out_tree)
jax._src.pallas.pallas_call._batch_block_mapping(grid:Tuple[int,...],aval:jax_core.ShapedArray,dim:int|batching.NotMapped,block_mapping:BlockMapping|None)->BlockMapping
jax._src.pallas.pallas_call._extract_function_name(f:Callable,name:str|None)->str
jax._src.pallas.pallas_call._hoist_consts_to_refs(jaxpr:jax_core.Jaxpr)->jax_core.Jaxpr
jax._src.pallas.pallas_call._maybe_dynamic_slice(start_idx,block_shape,value,is_indexing)
jax._src.pallas.pallas_call._maybe_dynamic_update_slice(start_idx,block_shape,value,update,is_indexing)
jax._src.pallas.pallas_call._pallas_call_abstract_eval(*avals,out_shapes,**_)
jax._src.pallas.pallas_call._pallas_call_batching_rule(args,dims,*,jaxpr:jax_core.Jaxpr,name:str,in_shapes:Tuple[jax.ShapeDtypeStruct,...],out_shapes:Tuple[jax.ShapeDtypeStruct,...],grid_mapping:GridMapping,input_output_aliases:Tuple[Tuple[int,int],...],debug:bool,interpret:bool,which_linear:Tuple[bool,...],**compiler_params:Any)
jax._src.pallas.pallas_call._pallas_call_impl(*args,jaxpr,name,out_shapes,which_linear,interpret,debug:bool,in_shapes,input_output_aliases:Tuple[Tuple[int,int],...],grid_mapping:GridMapping,**compiler_params:Any)
jax._src.pallas.pallas_call._pallas_call_jvp_rule(primals,tangents,*,jaxpr,name,which_linear,input_output_aliases:Tuple[Tuple[int,int],...],in_shapes,out_shapes,grid_mapping,debug,interpret,**compiler_params:Any)
jax._src.pallas.pallas_call._trace_to_jaxpr(fun:Callable,grid_spec:GridSpec,flat_in_avals,flat_out_avals,in_tree,out_tree)
jax._src.pallas.pallas_call.pallas_call(f:Callable[...,None],out_shape:Any,*,grid_spec:GridSpec|None=None,debug:bool=False,grid:Grid|None=None,in_specs:Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|NoBlockSpec|Sequence[BlockSpec|NoBlockSpec]=no_block_spec,input_output_aliases:Dict[int,int]={},interpret:bool=False,name:str|None=None,**compiler_params:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/primitives.py----------------------------------------
A:jax._src.pallas.primitives.program_id_p->jax._src.core.Primitive('program_id')
A:jax._src.pallas.primitives.grid_env->jax._src.pallas.core.current_grid_env()
A:jax._src.pallas.primitives.atomic_rmw_p->jax._src.core.Primitive('atomic_rmw')
A:jax._src.pallas.primitives.(ref, idx, val, mask)->args_tree.unflatten(args_flat)
A:jax._src.pallas.primitives.slice_sizes->tuple((s.size if isinstance(s, Slice) else 1 for s in indices))
A:jax._src.pallas.primitives.out_ones->jax.lax.dynamic_slice(ref, slice_starts, slice_sizes=slice_sizes)
A:jax._src.pallas.primitives.val_indexer->tuple((None if scalar else slice(None) for scalar in scalar_dims))
A:jax._src.pallas.primitives.val->jax.numpy.where(mask, val, out_)
A:jax._src.pallas.primitives.x_new->ref.at[idx.indices].set(val)
A:jax._src.pallas.primitives.out_indexer->tuple((0 if scalar else slice(None) for scalar in scalar_dims))
A:jax._src.pallas.primitives.(ref, _, _, _)->args_tree.unflatten(avals_flat)
A:jax._src.pallas.primitives.idx->NDIndexer.from_indices_shape(idx, x_ref.shape)
A:jax._src.pallas.primitives.(args_flat, args_tree)->jax.tree_util.tree_flatten((x_ref, idx, val, mask))
A:jax._src.pallas.primitives.atomic_xchg->functools.partial(atomic_rmw, atomic_type=AtomicOpType.XCHG)
A:jax._src.pallas.primitives.atomic_add->functools.partial(atomic_rmw, atomic_type=AtomicOpType.ADD)
A:jax._src.pallas.primitives.atomic_max->functools.partial(atomic_rmw, atomic_type=AtomicOpType.MAX)
A:jax._src.pallas.primitives.atomic_min->functools.partial(atomic_rmw, atomic_type=AtomicOpType.MIN)
A:jax._src.pallas.primitives.atomic_and->functools.partial(atomic_rmw, atomic_type=AtomicOpType.AND)
A:jax._src.pallas.primitives.atomic_or->functools.partial(atomic_rmw, atomic_type=AtomicOpType.OR)
A:jax._src.pallas.primitives.atomic_xor->functools.partial(atomic_rmw, atomic_type=AtomicOpType.XOR)
A:jax._src.pallas.primitives.atomic_cas_p->jax._src.core.Primitive('atomic_cas')
A:jax._src.pallas.primitives.new_val->jax.numpy.where(ref == cmp, val, ref)
A:jax._src.pallas.primitives.max_contiguous_p->jax._src.core.Primitive('max_contiguous')
A:jax._src.pallas.primitives.multiple_of_p->jax._src.core.Primitive('multiple_of')
A:jax._src.pallas.primitives.load_p->jax._src.core.Primitive('masked_load')
A:jax._src.pallas.primitives.(ref, idx, _, _)->args_tree.unflatten(avals_flat)
A:jax._src.pallas.primitives.size->jax._src.pretty_printer.text(str(slice.size))
A:jax._src.pallas.primitives.start->jax._src.pretty_printer.text(jax_core.pp_var(slice.start, context))
A:jax._src.pallas.primitives.end->jax._src.pretty_printer.concat([start, pp.text('+'), size])
A:jax._src.pallas.primitives.(x, idx, _, _)->eqn.params['args_tree'].unflatten(eqn.invars)
A:jax._src.pallas.primitives.lhs->jax._src.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.pallas.primitives.(ref_primal, idx, mask, other_primal)->args_tree.unflatten(primals)
A:jax._src.pallas.primitives.(ref_tangent, _, _, other_tangent)->args_tree.unflatten(tangents)
A:jax._src.pallas.primitives.other_tangent->jax._src.ad_util.instantiate(other_tangent)
A:jax._src.pallas.primitives.(ref, idx, mask, other)->args_tree.unflatten(args_flat)
A:jax._src.pallas.primitives.out->jax.numpy.where(mask, out, val)
A:jax._src.pallas.primitives.swap_p->jax._src.core.Primitive('masked_swap')
A:jax._src.pallas.primitives.(ref, idx, val, _)->args_tree.unflatten(avals_flat)
A:jax._src.pallas.primitives.expected_output_shape->NDIndexer.from_indices_shape(idx, x_ref.shape).get_indexer_shape()
A:jax._src.pallas.primitives.(x, idx, val, _)->eqn.params['args_tree'].unflatten(eqn.invars)
A:jax._src.pallas.primitives.x_i->jax._src.pretty_printer.concat([pp.text(jax_core.pp_var(x, context)), pp.text('['), idx, pp.text(']')])
A:jax._src.pallas.primitives.y->jax._src.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.pallas.primitives.(ref_primal, idx, val_primal, mask)->args_tree.unflatten(primals)
A:jax._src.pallas.primitives.(ref_tangent, _, val_tangent, _)->args_tree.unflatten(tangents)
A:jax._src.pallas.primitives.val_tangent->jax._src.ad_util.instantiate(val_tangent)
A:jax._src.pallas.primitives._->swap(x_ref, idx, val, mask=mask, eviction_policy=eviction_policy)
jax._src.pallas.primitives.AtomicOpType(enum.Enum)
jax._src.pallas.primitives._atomic_abstract_eval(*avals_flat,args_tree,atomic_type:AtomicOpType)
jax._src.pallas.primitives._atomic_cas_abstract_eval(ref_aval,cmp_aval,val_aval)
jax._src.pallas.primitives._atomic_cas_discharge_rule(in_avals,out_avals,ref,cmp,val)
jax._src.pallas.primitives._atomic_rmw_discharge_rule(in_avals,out_avals,*args_flat,args_tree,atomic_type:AtomicOpType)
jax._src.pallas.primitives._load_abstract_eval(*avals_flat,args_tree,**_)
jax._src.pallas.primitives._load_discharge_rule(in_avals,out_avals,*args_flat,args_tree,**_)
jax._src.pallas.primitives._load_jvp(primals,tangents,args_tree,**params)
jax._src.pallas.primitives._load_pp_rule(eqn,context,settings)
jax._src.pallas.primitives._max_contiguous_abstract_eval(aval,**_)
jax._src.pallas.primitives._multiple_of_abstract_eval(aval,**_)
jax._src.pallas.primitives._pp_dslice(dim:int,slice:Slice,context)
jax._src.pallas.primitives._pp_idx(ref_aval,idx:NDIndexer,context)
jax._src.pallas.primitives._program_id_abstract_eval(**_)
jax._src.pallas.primitives._program_id_impl(*,axis:int)
jax._src.pallas.primitives._swap_abstract_eval(*avals_flat,args_tree,**_)
jax._src.pallas.primitives._swap_discharge_rule(in_avals,out_avals,*args_flat,args_tree,**_)
jax._src.pallas.primitives._swap_jvp(primals,tangents,*,args_tree,**params)
jax._src.pallas.primitives._swap_pp_rule(eqn,context,settings)
jax._src.pallas.primitives.atomic_cas(ref,cmp,val)
jax._src.pallas.primitives.atomic_rmw(x_ref,idx,val,*,mask:Any|None=None,atomic_type:AtomicOpType)
jax._src.pallas.primitives.dot(a,b,trans_a:bool=False,trans_b:bool=False,allow_tf32:bool|None=None,precision=None)
jax._src.pallas.primitives.load(x_ref,idx,*,mask=None,other=None,cache_modifier='',eviction_policy='',volatile=False)
jax._src.pallas.primitives.max_contiguous(x,values)
jax._src.pallas.primitives.multiple_of(x,values)
jax._src.pallas.primitives.program_id(axis)
jax._src.pallas.primitives.program_id_bind(*,axis:int)
jax._src.pallas.primitives.store(x_ref,idx,val,*,mask=None,eviction_policy='')->None
jax._src.pallas.primitives.swap(x_ref,idx,val,*,mask=None,eviction_policy='')->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/triton/lowering.py----------------------------------------
A:jax._src.pallas.triton.lowering.block_indices->tuple(lower_jaxpr_to_triton_ir(ctx, block_mapping.index_map_jaxpr.jaxpr, None, *idx))
A:jax._src.pallas.triton.lowering.total_axis_size->numpy.prod(grid_prefix)
A:jax._src.pallas.triton.lowering.grid0->triton.language.program_id(0, _builder=builder)
A:jax._src.pallas.triton.lowering.(jaxpr, _)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr, [True] * len(jaxpr.outvars), instantiate=True)
A:jax._src.pallas.triton.lowering.ir_context->triton._C.libtriton.triton.ir.context()
A:jax._src.pallas.triton.lowering.builder->triton._C.libtriton.triton.ir.builder(ir_context)
A:jax._src.pallas.triton.lowering.builder.target->triton.compiler.compiler.CudaTargetDescriptor(capability=triton_kernel_call_lib.get_compute_capability(device), num_warps=num_warps, enable_fp_fusion=True)
A:jax._src.pallas.triton.lowering.module->triton._C.libtriton.triton.ir.builder(ir_context).create_module()
A:jax._src.pallas.triton.lowering.prototype->triton.language.function_type([], arg_types)
A:jax._src.pallas.triton.lowering.out->_closed_call_lowering_rule(ctx, *args, call_jaxpr=jaxpr)
A:jax._src.pallas.triton.lowering.fn->triton._C.libtriton.triton.ir.builder(ir_context).get_or_insert_function(module, name, out, 'public', False)
A:jax._src.pallas.triton.lowering.entry->triton._C.libtriton.triton.ir.builder(ir_context).get_or_insert_function(module, name, out, 'public', False).add_entry_block()
A:jax._src.pallas.triton.lowering.ptr->_compute_pointers_from_indices(ptr, ref_block_info, idx, avals_in[0].shape, ctx.builder)
A:jax._src.pallas.triton.lowering.(new_grid, program_ids)->_process_grid_to_3d_grid(builder, grid_mapping)
A:jax._src.pallas.triton.lowering.ctx->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]])
A:jax._src.pallas.triton.lowering.start_indices->map(partial(_eval_index_map, ctx, program_ids), grid_mapping.block_mappings)
A:jax._src.pallas.triton.lowering.()->lower_jaxpr_to_triton_ir(ctx, jaxpr, block_infos, *args)
A:jax._src.pallas.triton.lowering.t->triton.language.semantic.cast(t, dst_ty, ctx.builder)
A:jax._src.pallas.triton.lowering.invals->map(read_env, eqn.invars)
A:jax._src.pallas.triton.lowering.eqn_block_infos->map(read_block_info_env, eqn.invars)
A:jax._src.pallas.triton.lowering.rule_ctx->TritonLoweringRuleContext(ctx, avals_in, avals_out, eqn_block_infos)
A:jax._src.pallas.triton.lowering.outvals->rule(rule_ctx, *invals, **eqn.params)
A:jax._src.pallas.triton.lowering.(ptr, idx, value, mask)->args_tree.unflatten(args_flat)
A:jax._src.pallas.triton.lowering.op->_ATOMIC_OP_MAPPING.get(atomic_type)
A:jax._src.pallas.triton.lowering.kwargs->jax.tree_util.tree_map(tl.constexpr, kwargs)
A:jax._src.pallas.triton.lowering.wrapped_fun->jax._src.linear_util.wrap_init(fn, params)
A:jax._src.pallas.triton.lowering.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)
A:jax._src.pallas.triton.lowering.jaxpr->jaxpr.replace(eqns=jaxpr.eqns[:eqn_index] + jaxpr.eqns[eqn_index + 1:], invars=new_invars, outvars=new_outvars).replace(eqns=jaxpr.eqns[:eqn_index] + jaxpr.eqns[eqn_index + 1:], invars=new_invars, outvars=new_outvars)
A:jax._src.pallas.triton.lowering.triton_lowering_rules[primitive]->lower_fun(fn, multiple_results=False)
A:jax._src.pallas.triton.lowering.shape->map(tl.constexpr, shape)
A:jax._src.pallas.triton.lowering.a->a.__neg__(_builder=ctx.builder).__neg__(_builder=ctx.builder).__neg__(_builder=ctx.builder).__neg__(_builder=ctx.builder)
A:jax._src.pallas.triton.lowering.dst_shape->map(tl.constexpr, ctx.avals_out[0].shape)
A:jax._src.pallas.triton.lowering.in_shape->tuple((d.value for d in a.shape))
A:jax._src.pallas.triton.lowering.out_shape->tuple((d.value for (di, d) in enumerate(a.shape) if di != i))
A:jax._src.pallas.triton.lowering.reduce_ctx->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(avals_in=[ctx.avals_in[0].update(shape=in_shape)], avals_out=[ctx.avals_in[0].update(shape=out_shape)])
A:jax._src.pallas.triton.lowering.num_mapped_dims->sum((b is pallas_core.mapped for b in block_info.block_shape))
A:jax._src.pallas.triton.lowering.strides->jax._src.pallas.utils.strides_from_shape(full_shape)
A:jax._src.pallas.triton.lowering.indexer_shape->nd_indexer.get_indexer_shape()
A:jax._src.pallas.triton.lowering.indexer_iter->iter(indices)
A:jax._src.pallas.triton.lowering.index->triton.language.core.broadcast_to(index, a.shape, _builder=ctx.builder)
A:jax._src.pallas.triton.lowering.ptr_dim_offset->ptr_dim_offset.__add__(start_offset, _builder=builder).__add__(start_offset, _builder=builder)
A:jax._src.pallas.triton.lowering.num_right_expand_dims->len(other_shape)
A:jax._src.pallas.triton.lowering.num_left_expand_dims->max(len(indexer_shape) - 1, 0)
A:jax._src.pallas.triton.lowering.ndim->len(ptr_dim_offset.shape)
A:jax._src.pallas.triton.lowering.stride_size->triton.language.core._to_tensor(int(dim_stride), builder)
A:jax._src.pallas.triton.lowering.non_slice_idx_iter->iter(non_slice_idx)
A:jax._src.pallas.triton.lowering.idx_avals->_pack_indices(idx_avals, indexed_dims)
A:jax._src.pallas.triton.lowering.idx->jax._src.pallas.primitives.NDIndexer(idx, avals_in[0].shape, int_indexer_shape)
A:jax._src.pallas.triton.lowering.(args_flat, args_tree)->jax.tree_util.tree_flatten((ptr, idx, value, None))
A:jax._src.pallas.triton.lowering.(ptr, idx, mask, other)->args_tree.unflatten(args_flat)
A:jax._src.pallas.triton.lowering.val->triton.language.load(ptr, mask=mask, other=other, cache_modifier=cache_modifier, volatile=is_volatile, eviction_policy=eviction_policy, _builder=ctx.builder)
A:jax._src.pallas.triton.lowering.old_value->triton.language.load(ptr, mask=mask, other=other, _builder=ctx.builder)
A:jax._src.pallas.triton.lowering.b->triton.language.trans(b, _builder=ctx.builder)
A:jax._src.pallas.triton.lowering.out_dtypeacc_dtype->_convert_dtype(ctx.avals_out[0].dtype)
A:jax._src.pallas.triton.lowering.flat_args->jax.tree_util.tree_leaves(args)
A:jax._src.pallas.triton.lowering.in_tree->jax.tree_util.tree_structure((args, args))
A:jax._src.pallas.triton.lowering.(flat_fun, out_tree_thunk)->jax._src.api_util.flatten_fun_nokwargs(lu.wrap_init(body), in_tree)
A:jax._src.pallas.triton.lowering.(combine_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, [*mapped_avals, *mapped_avals])
A:jax._src.pallas.triton.lowering.out_tree->out_tree_thunk()
A:jax._src.pallas.triton.lowering.reduce_op->triton._C.libtriton.triton.ir.builder(ir_context).create_reduce([t.handle for t in flat_args], axis)
A:jax._src.pallas.triton.lowering.region->triton._C.libtriton.triton.ir.builder(ir_context).create_reduce([t.handle for t in flat_args], axis).get_region(0)
A:jax._src.pallas.triton.lowering.old_ip->triton._C.libtriton.triton.ir.builder(ir_context).get_insertion_point()
A:jax._src.pallas.triton.lowering.block->triton._C.libtriton.triton.ir.builder(ir_context).create_block_with_parent(region, ir_param_types)
A:jax._src.pallas.triton.lowering.results->lower_jaxpr_to_triton_ir(ctx.context, combine_jaxpr, None, *combine_args)
A:jax._src.pallas.triton.lowering.res_ty->triton.language.block_type(scalar_ty, ctx.avals_out[0].shape)
A:jax._src.pallas.triton.lowering.axis->max(axes)
A:jax._src.pallas.triton.lowering.dst_avals->tuple((v.update(shape=v.shape[:axis] + v.shape[axis + 1:]) for v in ctx.avals_in))
A:jax._src.pallas.triton.lowering.axes->tuple((ax for ax in axes if ax != axis))
A:jax._src.pallas.triton.lowering.triton_lowering_rules[lax.reduce_max_p]->functools.partial(_reduce_lowering, jnp.maximum)
A:jax._src.pallas.triton.lowering.triton_lowering_rules[lax.reduce_min_p]->functools.partial(_reduce_lowering, jnp.minimum)
A:jax._src.pallas.triton.lowering.triton_lowering_rules[lax.reduce_sum_p]->functools.partial(_reduce_lowering, jnp.add)
A:jax._src.pallas.triton.lowering.expand_dims_index[axis]->slice(None)
A:jax._src.pallas.triton.lowering.(_, indices)->_reduction_lowering(body, ctx, (a, index), axes=axes)
A:jax._src.pallas.triton.lowering.index_min->jax.numpy.minimum(index1, index2)
A:jax._src.pallas.triton.lowering.index_ret->jax.numpy.where(lt, index1, jnp.where(gt, index2, index_min))
A:jax._src.pallas.triton.lowering.value_ret->jax.numpy.minimum(value1, value2)
A:jax._src.pallas.triton.lowering.triton_lowering_rules[lax.argmax_p]->functools.partial(_argreduce_lowering, _reduce_argmax_combine)
A:jax._src.pallas.triton.lowering.triton_lowering_rules[lax.argmin_p]->functools.partial(_argreduce_lowering, _reduce_argmin_combine)
A:jax._src.pallas.triton.lowering.lower_bound->triton._C.libtriton.triton.ir.builder(ir_context).get_int32(0)
A:jax._src.pallas.triton.lowering.upper_bound->triton._C.libtriton.triton.ir.builder(ir_context).get_int32(length)
A:jax._src.pallas.triton.lowering.step->triton._C.libtriton.triton.ir.builder(ir_context).get_int32(step)
A:jax._src.pallas.triton.lowering.current_block->triton._C.libtriton.triton.ir.builder(ir_context).get_insertion_block()
A:jax._src.pallas.triton.lowering.(discharged_jaxpr, ())->jax._src.state.discharge.discharge_state(jaxpr, (), should_discharge=[True, *should_discharge])
A:jax._src.pallas.triton.lowering.read_only->map(_is_read_only, state_effects)
A:jax._src.pallas.triton.lowering.is_loop_arg->map(operator.and_, map(operator.not_, read_only), should_discharge)
A:jax._src.pallas.triton.lowering.(ptrs, _)->partition_list(should_discharge, init_args)
A:jax._src.pallas.triton.lowering.(non_loop_args, loop_args)->partition_list(is_loop_arg, init_args)
A:jax._src.pallas.triton.lowering.for_op->triton._C.libtriton.triton.ir.builder(ir_context).create_for_op(lower_bound, upper_bound, step, [arg.handle for arg in args])
A:jax._src.pallas.triton.lowering.loop_block->triton._C.libtriton.triton.ir.builder(ir_context).create_block()
A:jax._src.pallas.triton.lowering.loop_index->triton.language.core.tensor(for_op.get_induction_var(), tl.core.int32)
A:jax._src.pallas.triton.lowering.loop_body_args->merge_lists(is_loop_arg, non_loop_args, for_body_args)
A:jax._src.pallas.triton.lowering.out_discharged->lower_jaxpr_to_triton_ir(ctx.context, discharged_jaxpr, [None, *ctx.block_infos], loop_index, *loop_body_args)
A:jax._src.pallas.triton.lowering.all_out->lower_jaxpr_to_triton_ir(ctx.context, jaxpr, ctx.block_infos, *jaxpr_args)
A:jax._src.pallas.triton.lowering.(_, loop_out)->partition_list(is_loop_arg, all_out)
A:jax._src.pallas.triton.lowering.(jaxpr, has_loop_index)->jax._src.pallas.utils.pattern_match_scan_to_fori_loop(jaxpr, num_consts, num_carry)
A:jax._src.pallas.triton.lowering.(consts, args)->jax._src.util.split_list(args, [num_consts])
A:jax._src.pallas.triton.lowering.ub->lb.__add__(tl.constexpr(length), _builder=builder)
A:jax._src.pallas.triton.lowering.for_out->_lower_jaxpr_to_for_loop(ctx, jaxpr, lb.handle, ub.handle, body_consts, *args, has_loop_index=True, step=1, bound_type=lb.type)
A:jax._src.pallas.triton.lowering.(_, cond_invars)->split_list(cond_jaxpr.jaxpr.invars, [cond_nconsts])
A:jax._src.pallas.triton.lowering.(_, body_invars)->split_list(body_jaxpr.jaxpr.invars, [body_nconsts])
A:jax._src.pallas.triton.lowering.new_invars->tuple((*jaxpr.invars[:body_nconsts], jaxpr.invars[body_nconsts], *jaxpr.invars[body_nconsts + 2:]))
A:jax._src.pallas.triton.lowering.new_outvars->tuple(jaxpr.outvars[2:])
A:jax._src.pallas.triton.lowering.(_, body_consts, carry)->split_list(args, [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.(const_block_infos, args_block_infos)->split_list(ctx.block_infos, [body_nconsts])
A:jax._src.pallas.triton.lowering.result->_maybe_pattern_match_fori_loop(ctx, *args, cond_nconsts=cond_nconsts, body_nconsts=body_nconsts, cond_jaxpr=cond_jaxpr, body_jaxpr=body_jaxpr)
A:jax._src.pallas.triton.lowering.num_args->len(args)
A:jax._src.pallas.triton.lowering.(cond_consts, body_consts, carry)->jax._src.util.split_list(args, [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.(cond_const_block_infos, body_const_block_infos, carry_block_infos)->jax._src.util.split_list(ctx.block_infos, [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.current_bb->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).builder.get_insertion_block()
A:jax._src.pallas.triton.lowering.while_op->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).builder.create_while_op([*cond_const_types, *body_const_types, *carry_types], [arg.handle for arg in args])
A:jax._src.pallas.triton.lowering.before_block->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).builder.create_block_with_parent(while_op.get_before(), all_types)
A:jax._src.pallas.triton.lowering.(cond_consts_, _, carry_)->jax._src.util.split_list([before_block.arg(i) for i in range(num_args)], [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.(cond,)->lower_jaxpr_to_triton_ir(ctx.context, cond_jaxpr.jaxpr, [*cond_const_block_infos, *carry_block_infos], *cond_args)
A:jax._src.pallas.triton.lowering.after_block->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).builder.create_block_with_parent(while_op.get_after(), all_types)
A:jax._src.pallas.triton.lowering.(cond_consts_, body_consts_, carry_)->jax._src.util.split_list([after_block.arg(i) for i in range(num_args)], [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.(cond_const_args, body_const_args, carry_args)->jax._src.util.split_list(all_args, [cond_nconsts, body_nconsts])
A:jax._src.pallas.triton.lowering.loop_out->lower_jaxpr_to_triton_ir(ctx.context, body_jaxpr.jaxpr, [*body_const_block_infos, *carry_block_infos], *body_const_args, *carry_args)
A:jax._src.pallas.triton.lowering.use_branch0->triton.language.core.broadcast_to(index, a.shape, _builder=ctx.builder).__eq__(0, _builder=ctx.builder)
A:jax._src.pallas.triton.lowering.if_op->ctx.replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).replace(block_infos=[*const_block_infos, None, *args_block_infos[2:]]).builder.create_if_op(out_ir_types, use_branch0.handle, True)
A:jax._src.pallas.triton.lowering.outs0->lower_jaxpr_to_triton_ir(ctx.context, branches[0].jaxpr, block_infos[1:], *args)
A:jax._src.pallas.triton.lowering.outs1->lower_jaxpr_to_triton_ir(ctx.context, branches[1].jaxpr, block_infos[1:], *args)
A:jax._src.pallas.triton.lowering.lowering_result->lower_jaxpr_to_triton_module(jaxpr, in_shapes, grid_mapping, name, num_warps)
A:jax._src.pallas.triton.lowering.ttir->str(lowering_result.module)
A:jax._src.pallas.triton.lowering.(ptx, name, shared_mem_bytes, compute_capability)->compile_ttir_to_ptx_inplace(lowering_result.module, device=device, num_warps=num_warps, num_stages=num_stages, dump=debug)
A:jax._src.pallas.triton.lowering.num_warps->compiler_params.get('num_warps', 4)
A:jax._src.pallas.triton.lowering.num_stages->compiler_params.get('num_stages', 3)
A:jax._src.pallas.triton.lowering.compilation_result->compile_jaxpr(jaxpr, tuple((*in_shapes, *out_shapes)), grid_mapping, name, num_warps, num_stages, debug=debug)
A:jax._src.pallas.triton.lowering.kernel->jax._src.lib.gpu_triton.TritonKernel(compilation_result.kernel_name, num_warps, compilation_result.shared_mem_bytes, compilation_result.ptx, compilation_result.ttir, compilation_result.compute_capability)
A:jax._src.pallas.triton.lowering.grid->jax_triton.triton_lib.normalize_grid(compilation_result.lowering_result.grid, metaparams={})
A:jax._src.pallas.triton.lowering.kernel_call->jax._src.lib.gpu_triton.TritonKernelCall(kernel, grid[0], grid[1], grid[2], kernel_params)
A:jax._src.pallas.triton.lowering.serialized_metadata->triton_params.get('serialized_metadata', b'')
A:jax._src.pallas.triton.lowering.kernel_call_proto->jax._src.lib.gpu_triton.TritonKernelCall(kernel, grid[0], grid[1], grid[2], kernel_params).to_proto(serialized_metadata)
jax._src.pallas.triton.lowering.BlockInfo
jax._src.pallas.triton.lowering.TritonCompilationResult
jax._src.pallas.triton.lowering.TritonLoweringException(Exception)
jax._src.pallas.triton.lowering.TritonLoweringResult
jax._src.pallas.triton.lowering.TritonLoweringRuleContext
jax._src.pallas.triton.lowering.TritonLoweringRuleContext.__post_init__(self)
jax._src.pallas.triton.lowering.TritonModuleContext
jax._src.pallas.triton.lowering._addupdate_lowering_rule(ctx:TritonLoweringRuleContext,ptr,value,*non_slice_idx,indexed_dims)
jax._src.pallas.triton.lowering._argreduce_lowering(body,ctx:TritonLoweringRuleContext,a,*,axes,index_dtype)
jax._src.pallas.triton.lowering._atomic_lowering_rule(ctx:TritonLoweringRuleContext,*args_flat,args_tree,atomic_type:primitives.AtomicOpType)
jax._src.pallas.triton.lowering._broadcast_in_dim_lowering_rule(ctx:TritonLoweringRuleContext,a,*,broadcast_dimensions,shape)
jax._src.pallas.triton.lowering._closed_call_lowering_rule(ctx:TritonLoweringRuleContext,*args,call_jaxpr,**_)
jax._src.pallas.triton.lowering._compute_pointers_from_indices(root_ptr:tl.core.tensor,block_info:BlockInfo|None,nd_indexer:NDIndexer,array_shape:Tuple[int,...],builder:tl_ir.builder)->tl.core.tensor
jax._src.pallas.triton.lowering._cond_lowering_rule(ctx:TritonLoweringRuleContext,index,*args,branches,linear)
jax._src.pallas.triton.lowering._convert_dtype(dtype:jnp.dtype)->tl.dtype
jax._src.pallas.triton.lowering._convert_element_type_lowering_rule(ctx:TritonLoweringRuleContext,a,*,new_dtype,weak_type)
jax._src.pallas.triton.lowering._div_lowering_rule(ctx:TritonLoweringRuleContext,a,b)
jax._src.pallas.triton.lowering._dot_general_lowering(ctx:TritonLoweringRuleContext,a,b,*,dimension_numbers,precision,preferred_element_type)
jax._src.pallas.triton.lowering._eval_index_map(ctx:TritonModuleContext,idx,block_mapping:BlockMapping|None)
jax._src.pallas.triton.lowering._for_lowering_rule(ctx:TritonLoweringRuleContext,*args,jaxpr,which_linear,nsteps,reverse,unroll)
jax._src.pallas.triton.lowering._get_lowering_rule(ctx:TritonLoweringRuleContext,ptr,*non_slice_idx,indexed_dims)
jax._src.pallas.triton.lowering._integer_pow(a,*,y)
jax._src.pallas.triton.lowering._iota_lowering_rule(ctx:TritonLoweringRuleContext,*,dtype,shape,dimension)
jax._src.pallas.triton.lowering._is_read_only(ref_effects)->bool
jax._src.pallas.triton.lowering._lower_jaxpr_to_for_loop(ctx:TritonLoweringRuleContext,jaxpr:jax_core.Jaxpr,lower_bound,upper_bound,consts,*args,has_loop_index:bool,step:int=1,bound_type:tl.dtype=tl.int32)
jax._src.pallas.triton.lowering._masked_load_lowering_rule(ctx:TritonLoweringRuleContext,*args_flat,args_tree,eviction_policy,cache_modifier,is_volatile)
jax._src.pallas.triton.lowering._masked_swap_lowering_rule(ctx:TritonLoweringRuleContext,*args_flat,args_tree,eviction_policy)
jax._src.pallas.triton.lowering._maybe_pattern_match_fori_loop(ctx:TritonLoweringRuleContext,*args,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.pallas.triton.lowering._pack_indices(non_slice_idx,indexed_dims)
jax._src.pallas.triton.lowering._pjit_lowering_rule(ctx:TritonLoweringRuleContext,*args,jaxpr,**_)
jax._src.pallas.triton.lowering._process_grid_to_3d_grid(builder,grid_mapping:GridMapping)
jax._src.pallas.triton.lowering._program_id_lowering_rule(ctx:TritonLoweringRuleContext,*,axis)
jax._src.pallas.triton.lowering._reduce_argmax_combine(left,right)
jax._src.pallas.triton.lowering._reduce_argmin_combine(left,right)
jax._src.pallas.triton.lowering._reduce_lowering(body,ctx:TritonLoweringRuleContext,a,*,axes)
jax._src.pallas.triton.lowering._reduction_lowering(body,ctx:TritonLoweringRuleContext,args,axes)
jax._src.pallas.triton.lowering._remat_lowering_rule(ctx:TritonLoweringRuleContext,*args,jaxpr,**_)
jax._src.pallas.triton.lowering._reshape_lowering_rule(ctx:TritonLoweringRuleContext,a,*,new_sizes,dimensions)
jax._src.pallas.triton.lowering._scan_lowering_rule(ctx:TritonLoweringRuleContext,*args,jaxpr,linear,length,reverse,unroll,num_consts,num_carry)
jax._src.pallas.triton.lowering._squeeze_lowering_rule(ctx:TritonLoweringRuleContext,a,*,dimensions)
jax._src.pallas.triton.lowering._swap_lowering_rule(ctx:TritonLoweringRuleContext,ptr,value,*non_slice_idx,indexed_dims)
jax._src.pallas.triton.lowering._transpose_lowering(ctx:TritonLoweringRuleContext,a,*,permutation)
jax._src.pallas.triton.lowering._while_lowering_rule(ctx:TritonLoweringRuleContext,*args,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.pallas.triton.lowering.compile_jaxpr(jaxpr:jax_core.Jaxpr,in_shapes,grid_mapping:GridMapping,name:str,num_warps:int,num_stages:int,debug:bool)->TritonCompilationResult
jax._src.pallas.triton.lowering.lower_fun(fun:Callable[...,Any],*,multiple_results:bool)->Callable[..., Any]
jax._src.pallas.triton.lowering.lower_jaxpr_to_triton_ir(ctx:TritonModuleContext,jaxpr:jax_core.Jaxpr,block_infos:Sequence[BlockInfo|None]|None,*args)->Sequence[Any]
jax._src.pallas.triton.lowering.lower_jaxpr_to_triton_module(jaxpr:jax_core.Jaxpr,in_shapes,grid_mapping:GridMapping,name:str,num_warps:int)->tl_ir.module
jax._src.pallas.triton.lowering.pallas_call_lowering(ctx:mlir.LoweringRuleContext,*in_nodes,jaxpr:jax_core.Jaxpr,name:str,in_shapes:Tuple[jax.ShapeDtypeStruct,...],out_shapes:Tuple[jax.ShapeDtypeStruct,...],which_linear:Tuple[bool,...],interpret:bool,debug:bool,input_output_aliases:Tuple[Tuple[int,int],...],grid_mapping:GridMapping,triton_params:Dict[str,Any]|None=None,**compiler_params:Any)
jax._src.pallas.triton.lowering.select_n_lowering_rule(ctx:TritonLoweringRuleContext,pred,a,b)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/triton/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/lowering.py----------------------------------------
A:jax._src.pallas.mosaic.lowering.memspace->jax._src.lib.mlir.ir.Attribute.parse(f'#tpu.memory_space<{aval.memory_space}>')
A:jax._src.pallas.mosaic.lowering.x->jax._src.lib.mlir.dialects.vector.BroadcastOp(x_ty, x)
A:jax._src.pallas.mosaic.lowering.mlir_type->jax._src.interpreters.mlir.dtype_to_ir_type(y_aval.dtype)
A:jax._src.pallas.mosaic.lowering.skip_mlir_conversions->set()
A:jax._src.pallas.mosaic.lowering.m->jax._src.lib.mlir.ir.Module.create()
A:jax._src.pallas.mosaic.lowering.sym_tab->jax._src.lib.mlir.ir.SymbolTable(m.operation)
A:jax._src.pallas.mosaic.lowering.used_axis_names->jax.core.used_axis_names_jaxpr(jaxpr)
A:jax._src.pallas.mosaic.lowering.axis_names->list(used_axis_names)
A:jax._src.pallas.mosaic.lowering.mesh_strides->jax._src.pallas.utils.strides_from_shape(tuple((mesh.shape[a] for a in axis_names)))
A:jax._src.pallas.mosaic.lowering.logical_to_mesh->numpy.empty((mesh.size, len(axis_names)), dtype=np.int32)
A:jax._src.pallas.mosaic.lowering.logical_to_mesh[i]->numpy.array(idx)
A:jax._src.pallas.mosaic.lowering.func_op->lower_jaxpr_to_func(ctx, jaxpr, grid_mapping=grid_mapping, name='main', mesh_info=mesh_info)
A:jax._src.pallas.mosaic.lowering.mlir_func->lower_jaxpr_to_transform_func(ctx, bm.index_map_jaxpr.jaxpr, [*[None] * len(grid), *[SMEM] * num_smem_inputs], name=func_name)
A:jax._src.pallas.mosaic.lowering.window_shape->jax._src.lib.mlir.ir.DenseI64ArrayAttr.get(block_shape)
A:jax._src.pallas.mosaic.lowering.func_op.attributes['window_params']->jax._src.lib.mlir.ir.ArrayAttr.get(window_params)
A:jax._src.pallas.mosaic.lowering.func_op.attributes['iteration_bounds']->jax._src.lib.mlir.ir.DenseI64ArrayAttr.get(grid)
A:jax._src.pallas.mosaic.lowering.func_op.attributes['scalar_prefetch']->jax._src.lib.mlir.ir.IntegerAttr.get(ir.IntegerType.get_signless(64), num_smem_inputs)
A:jax._src.pallas.mosaic.lowering.func_op.attributes['scratch_operands']->jax._src.lib.mlir.ir.IntegerAttr.get(ir.IntegerType.get_signless(64), num_scratch_inputs)
A:jax._src.pallas.mosaic.lowering.dimension_semantics_iter->iter(dimension_semantics)
A:jax._src.pallas.mosaic.lowering.func_op.attributes['dimension_semantics']->jax._src.lib.mlir.ir.ArrayAttr.get(map(ir.Attribute.parse, func_dimension_semantics))
A:jax._src.pallas.mosaic.lowering.lowering_context->ctx.lowering_context.replace(block_shapes=(*ctx.block_shapes, *block_shapes)).lowering_context.replace(block_shapes=ctx.block_shapes)
A:jax._src.pallas.mosaic.lowering.body_func->functools.partial(jaxpr_subcomp, lowering_context, jaxpr)
A:jax._src.pallas.mosaic.lowering.body->jax._src.lib.mlir.dialects.func.FuncOp.from_py_func(*arg_types, name=name)(body_func)
A:jax._src.pallas.mosaic.lowering.wrapped_fun->jax._src.linear_util.wrap_init(f, params)
A:jax._src.pallas.mosaic.lowering.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)
A:jax._src.pallas.mosaic.lowering.jaxpr->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr)
A:jax._src.pallas.mosaic.lowering.out->jaxpr_subcomp(lowering_context, branches[0].jaxpr, *args)
A:jax._src.pallas.mosaic.lowering.arg_types->map(aval_to_ir_type, [jax_core.ShapedArray((), jnp.int32) for _ in grid_mapping.grid])
A:jax._src.pallas.mosaic.lowering.shape->tuple((1 if b is core.mapped else b for b in block_mapping.block_shape))
A:jax._src.pallas.mosaic.lowering.l_to_m_aval->jax._src.state.AbstractRef(jax_core.raise_to_shaped(jax_core.get_aval(l_to_m)))
A:jax._src.pallas.mosaic.lowering.(invar_arg_types, block_shapes)->unzip2(map(_get_arg_type, [invar.aval for invar in jaxpr.invars], block_mappings, memory_spaces))
A:jax._src.pallas.mosaic.lowering.(grid_indices, args)->split_list(args, [len(grid_mapping.grid)])
A:jax._src.pallas.mosaic.lowering.((l_to_m,), args)->split_list(args, [1])
A:jax._src.pallas.mosaic.lowering.mesh_context->MeshContext(l_to_m, axis_names, mesh_strides)
A:jax._src.pallas.mosaic.lowering.invals->map(read_env, eqn.invars)
A:jax._src.pallas.mosaic.lowering.source_info->eqn.source_info.replace(name_stack=ctx.name_stack + eqn.source_info.name_stack)
A:jax._src.pallas.mosaic.lowering.loc->jax._src.interpreters.mlir._source_info_to_location(eqn.primitive, eqn.params, source_info, ctx.name_stack)
A:jax._src.pallas.mosaic.lowering.block_shapes->tuple((a.shape if isinstance(a, state.AbstractRef) else None for a in in_avals))
A:jax._src.pallas.mosaic.lowering.rule_context->LoweringRuleContext(ctx, [v.aval for v in eqn.invars], [v.aval for v in eqn.outvars], block_shapes)
A:jax._src.pallas.mosaic.lowering.ans->lowering_rules[eqn.primitive](rule_context, *invals, **eqn.params)
A:jax._src.pallas.mosaic.lowering.outvals->map(read_env, jaxpr.outvars)
A:jax._src.pallas.mosaic.lowering.non_slice_idx_iter->iter(zip(non_slice_idx, non_slice_idx_avals))
A:jax._src.pallas.mosaic.lowering.splatted_idx_idx_avals->tuple((next(non_slice_idx_iter) if indexed else (primitives.Slice(0, s), primitives.Slice(0, s)) for (s, indexed) in zip(ref_aval.shape, indexed_dims)))
A:jax._src.pallas.mosaic.lowering.(splatted_idx, splatted_idx_avals)->unzip2(splatted_idx_idx_avals)
A:jax._src.pallas.mosaic.lowering.nd_indexer->NDIndexer(splatted_idx, ref_aval.shape, int_indexer_shape)
A:jax._src.pallas.mosaic.lowering.nd_indexer_avals->NDIndexer(splatted_idx_avals, ref_aval.shape, int_indexer_shape)
A:jax._src.pallas.mosaic.lowering.(nd_indexer, nd_indexer_avals)->_convert_flat_indexing_to_indexer(ref_aval, non_slice_idx, non_slice_idx_avals, indexed_dims)
A:jax._src.pallas.mosaic.lowering.(args_flat, args_tree)->jax.tree_util.tree_flatten((ref, nd_indexer, val, None))
A:jax._src.pallas.mosaic.lowering.avals_flat->jax.tree_util.tree_leaves((ref_aval, nd_indexer_avals, val_aval, None))
A:jax._src.pallas.mosaic.lowering.ctx->ctx.lowering_context.replace(block_shapes=(*ctx.block_shapes, *block_shapes)).lowering_context.replace(block_shapes=(*ctx.block_shapes, *block_shapes))
A:jax._src.pallas.mosaic.lowering.(ref, idx, mask, _)->args_tree.unflatten(args_flat)
A:jax._src.pallas.mosaic.lowering.(_, idx_aval, _, _)->args_tree.unflatten(ctx.avals_in)
A:jax._src.pallas.mosaic.lowering.ref_type->jax._src.lib.mlir.ir.MemRefType(ref.type)
A:jax._src.pallas.mosaic.lowering.starts->map(partial(_ensure_mlir_value, aval=jax_core.ShapedArray((), jnp.int32)), starts)
A:jax._src.pallas.mosaic.lowering.idx_iter->iter(mlir_indices)
A:jax._src.pallas.mosaic.lowering.load_shape->list(aval_out.shape)
A:jax._src.pallas.mosaic.lowering.load_shape_iter->iter(load_shape)
A:jax._src.pallas.mosaic.lowering.load_aval->aval_out.update(shape=tuple(load_shape))
A:jax._src.pallas.mosaic.lowering.vec_type->jax._src.lib.mlir.ir.VectorType.get(aval_out.shape, mlir.dtype_to_ir_type(aval_out.dtype))
A:jax._src.pallas.mosaic.lowering.(ref, idx, val, mask)->args_tree.unflatten(args_flat)
A:jax._src.pallas.mosaic.lowering.val->jax._src.lib.mlir.ir.FloatAttr.get(ir.F16Type.get(), 0.0)
A:jax._src.pallas.mosaic.lowering.mem_slice_shape->list(aval_out.shape)
A:jax._src.pallas.mosaic.lowering.mem_slice_shape_iter->iter(mem_slice_shape)
A:jax._src.pallas.mosaic.lowering.mem_aval->aval_out.update(shape=tuple(mem_slice_shape))
A:jax._src.pallas.mosaic.lowering.mem_aval_vec_type->jax._src.lib.mlir.ir.VectorType.get(mem_aval.shape, mlir.dtype_to_ir_type(mem_aval.dtype))
A:jax._src.pallas.mosaic.lowering.result_vec_type->jax._src.lib.mlir.ir.VectorType.get(aval_out.shape, mlir.dtype_to_ir_type(aval_out.dtype))
A:jax._src.pallas.mosaic.lowering.val_vec_type->jax._src.lib.mlir.ir.VectorType.get(mem_aval.shape, mlir.dtype_to_ir_type(mem_aval.dtype))
A:jax._src.pallas.mosaic.lowering.out_type->jax._src.lib.mlir.ir.MemRefType.get(aval.shape, mlir.dtype_to_ir_type(aval.dtype), memory_space=memspace)
A:jax._src.pallas.mosaic.lowering.kind->jax._src.lib.mlir.ir.Attribute.parse('#vector.kind<add>')
A:jax._src.pallas.mosaic.lowering.identity->jax._src.lib.mlir.ir.DenseElementsAttr.get_splat(out_type, val)
A:jax._src.pallas.mosaic.lowering.acc->jax._src.lib.mlir.dialects.arith.ConstantOp(red_type, ir.DenseElementsAttr.get_splat(red_type, val))
A:jax._src.pallas.mosaic.lowering.op->jax._src.lib.mlir.dialects.vector.ExtractStridedSliceOp(aval_to_ir_type(aval_out), x, start_indices, sizes, strides)
A:jax._src.pallas.mosaic.lowering.out_shape->list(out_aval.shape)
A:jax._src.pallas.mosaic.lowering.bcast_shape->jax._src.lib.mlir.ir.VectorType.get(list(bcast_shape), mlir.dtype_to_ir_type(ctx.avals_out[0].dtype))
A:jax._src.pallas.mosaic.lowering.y->jax._src.lib.mlir.dialects.vector.BroadcastOp(y_ty, y)
A:jax._src.pallas.mosaic.lowering.red_type->aval_to_ir_type(lhs_aval.update(shape=(lhs_aval.shape[0],)))
A:jax._src.pallas.mosaic.lowering.red->jax._src.lib.mlir.dialects.vector.MultiDimReductionOp(ir.Attribute.parse('#vector.kind<add>'), arith.MulFOp(x, y), acc, ir.ArrayAttr.get([ir.IntegerAttr.get(ir.IntegerType.get_signless(64), 1)]))
A:jax._src.pallas.mosaic.lowering.precision_attr->jax._src.lib.mlir.ir.Attribute.parse('#tpu.contract_precision<fp32>')
A:jax._src.pallas.mosaic.lowering.out_tile->jax._src.lib.mlir.dialects.arith.ConstantOp(out_type, ir.DenseElementsAttr.get_splat(out_type, val))
A:jax._src.pallas.mosaic.lowering.ext_type->aval_to_ir_type(out_aval.update(dtype=ext_dtype))
A:jax._src.pallas.mosaic.lowering.i64_type->jax._src.lib.mlir.ir.IntegerType.get_signless(64)
A:jax._src.pallas.mosaic.lowering.transp->jax._src.lib.mlir.ir.ArrayAttr.get([ir.IntegerAttr.get(i64_type, i) for i in permutation])
A:jax._src.pallas.mosaic.lowering.x_ty->jax._src.lib.mlir.ir.VectorType.get(out_shape, mlir.dtype_to_ir_type(x_aval.dtype))
A:jax._src.pallas.mosaic.lowering.y_ty->jax._src.lib.mlir.ir.VectorType.get(out_shape, mlir.dtype_to_ir_type(y_aval.dtype))
A:jax._src.pallas.mosaic.lowering.(x, y)->_bcast(x, y, ctx.avals_in[0], ctx.avals_in[1], ctx.avals_out[0])
A:jax._src.pallas.mosaic.lowering.new_ctx->ctx.lowering_context.replace(block_shapes=(*ctx.block_shapes, *block_shapes)).lowering_context.replace(block_shapes=(*ctx.block_shapes, *block_shapes)).replace(avals_in=(jax_core.ShapedArray((), x_aval.dtype), x_aval), block_shapes=((), *ctx.block_shapes))
A:jax._src.pallas.mosaic.lowering.one->jax._src.lib.mlir.dialects.vector.BroadcastOp(out_type, ir_constant(1.0))
A:jax._src.pallas.mosaic.lowering.predicate->jax._src.lib.mlir.ir.IntegerAttr.get(ir.IntegerType.get_signless(64), pred)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.eq_p]->functools.partial(_cmp_lowering_rule, lax.eq_p)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.ne_p]->functools.partial(_cmp_lowering_rule, lax.ne_p)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.lt_p]->functools.partial(_cmp_lowering_rule, lax.lt_p)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.le_p]->functools.partial(_cmp_lowering_rule, lax.le_p)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.gt_p]->functools.partial(_cmp_lowering_rule, lax.gt_p)
A:jax._src.pallas.mosaic.lowering.lowering_rules[lax.ge_p]->functools.partial(_cmp_lowering_rule, lax.ge_p)
A:jax._src.pallas.mosaic.lowering.lower_ctx->LoweringRuleContext(lowering_context=ctx.lowering_context, avals_in=[jax_core.ShapedArray((), jnp.int32)] * len(device_ids), avals_out=[jax_core.ShapedArray((), jnp.int32)], block_shapes=(None,) * len(device_ids))
A:jax._src.pallas.mosaic.lowering.pred->lower_fun(lambda x: x != 0, multiple_results=False)(lower_ctx, pred)
A:jax._src.pallas.mosaic.lowering.res->jax.numpy.maximum(operand, min)
A:jax._src.pallas.mosaic.lowering.(jaxpr, ())->jax._src.state.discharge.discharge_state(jaxpr, (), should_discharge=[False, *should_discharge])
A:jax._src.pallas.mosaic.lowering.i->ir_constant(i)
A:jax._src.pallas.mosaic.lowering.non_ref_args->jaxpr_subcomp(lowering_context, jaxpr, i, *args)
A:jax._src.pallas.mosaic.lowering.non_ref_args_iter->iter(non_ref_args)
A:jax._src.pallas.mosaic.lowering.args->map(_alloc_value, in_avals)
A:jax._src.pallas.mosaic.lowering.(jaxpr, has_loop_index)->jax._src.pallas.utils.pattern_match_scan_to_fori_loop(jaxpr, num_consts, num_carry)
A:jax._src.pallas.mosaic.lowering.(consts, args)->split_list(args, [num_consts])
A:jax._src.pallas.mosaic.lowering.out_types->map(aval_to_ir_type, ctx.avals_out)
A:jax._src.pallas.mosaic.lowering.if_op->jax._src.lib.mlir.dialects.scf.IfOp(pred, out_types, hasElse=True)
A:jax._src.pallas.mosaic.lowering.length->len(ctx.lowering_context.grid_indices)
A:jax._src.pallas.mosaic.lowering.sem_type->jax._src.lib.mlir.ir.Type.parse('!tpu.semaphore')
A:jax._src.pallas.mosaic.lowering.region->jax.experimental.mosaic.dialects.tpu.RegionOp()
A:jax._src.pallas.mosaic.lowering.device_ids->jax.tree_util.tree_leaves(device_id)
A:jax._src.pallas.mosaic.lowering.device_id->_make_index(tpu.DeviceIdOp().result)
A:jax._src.pallas.mosaic.lowering.target_shape->indexer.get_indexer_shape()
A:jax._src.pallas.mosaic.lowering.(starts, sizes)->_indexer_to_start_size(indexer)
A:jax._src.pallas.mosaic.lowering.target_ref_ty->jax._src.lib.mlir.ir.MemRefType.get(tuple(sizes), mlir.dtype_to_ir_type(ref_aval.dtype), memory_space=ref.type.memory_space)
A:jax._src.pallas.mosaic.lowering.squeezed_ref_ty->jax._src.lib.mlir.ir.MemRefType.get(tuple(target_shape), mlir.dtype_to_ir_type(ref_aval.dtype), memory_space=ref.type.memory_space)
A:jax._src.pallas.mosaic.lowering.(src_ref, src_idx, dst_ref, dst_idx, sem, src_sem, device_id)->jax.tree_util.tree_unflatten(tree, args)
A:jax._src.pallas.mosaic.lowering.(src_ref_aval, src_idx_aval, dst_ref_aval, *_)->jax.tree_util.tree_unflatten(tree, ctx.avals_in)
A:jax._src.pallas.mosaic.lowering.src->_slice_memref(src_ref, src_ref_aval, src_idx)
A:jax._src.pallas.mosaic.lowering.dst->_slice_memref(dst_ref, dst_ref_aval, dst_idx)
A:jax._src.pallas.mosaic.lowering.(sem, ref, idx)->jax.tree_util.tree_unflatten(tree, args)
A:jax._src.pallas.mosaic.lowering.(sem_aval, ref_aval, idx_aval)->jax.tree_util.tree_unflatten(tree, ctx.avals_in)
A:jax._src.pallas.mosaic.lowering.ref_slc->_slice_memref(ref, ref_aval, idx)
A:jax._src.pallas.mosaic.lowering.col->_make_index(axis_names.index(axis_name))
jax._src.pallas.mosaic.LoweringException(Exception)
jax._src.pallas.mosaic.lowering.LoweringContext
jax._src.pallas.mosaic.lowering.LoweringException(Exception)
jax._src.pallas.mosaic.lowering.LoweringRuleContext
jax._src.pallas.mosaic.lowering.MeshContext
jax._src.pallas.mosaic.lowering._abs_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._add_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._alloc_value(aval:jax_core.AbstractValue)->ir.Value
jax._src.pallas.mosaic.lowering._and_lowering_rule(ctx:LoweringRuleContext,lhs,rhs)
jax._src.pallas.mosaic.lowering._axis_index_rule(ctx:LoweringRuleContext,*,axis_name:str)
jax._src.pallas.mosaic.lowering._bcast(x,y,x_aval,y_aval,out_aval)
jax._src.pallas.mosaic.lowering._broadcast_in_dim_lowering_rule(ctx:LoweringRuleContext,val,*,shape,broadcast_dimensions)
jax._src.pallas.mosaic.lowering._clamp(min,operand,max)
jax._src.pallas.mosaic.lowering._clamp_lowering_rule(ctx:LoweringRuleContext,min,operand,max)
jax._src.pallas.mosaic.lowering._cmp_lowering_rule(prim,ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._concatenate_lowering_rule(ctx:LoweringRuleContext,*xs,dimension)
jax._src.pallas.mosaic.lowering._cond_lowering_rule(ctx:LoweringRuleContext,*args,branches,linear)
jax._src.pallas.mosaic.lowering._convert_element_type_lowering_rule(ctx:LoweringRuleContext,x,*,new_dtype,weak_type)
jax._src.pallas.mosaic.lowering._convert_flat_indexing_to_indexer(ref_aval,non_slice_idx,non_slice_idx_avals,indexed_dims)
jax._src.pallas.mosaic.lowering._custom_jvp_call_lowering_rule(ctx:LoweringRuleContext,*args,call_jaxpr:jax_core.Jaxpr,jvp_jaxpr_thunk:Callable,num_consts:int,symbolic_zeros:bool)
jax._src.pallas.mosaic.lowering._debug_callback_lowering_rule(ctx:LoweringRuleContext,*args,**kwargs)
jax._src.pallas.mosaic.lowering._device_id_lowering_rule(ctx:LoweringRuleContext)
jax._src.pallas.mosaic.lowering._device_id_to_logical(ctx:LoweringRuleContext,device_id,device_id_type:tpu_primitives.DeviceIdType)
jax._src.pallas.mosaic.lowering._div_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._dma_start_lowering_rule(ctx:LoweringRuleContext,*args,tree,device_id_type:tpu_primitives.DeviceIdType)
jax._src.pallas.mosaic.lowering._dma_wait_lowering_rule(ctx:LoweringRuleContext,*args,tree,device_id_type:tpu_primitives.DeviceIdType)
jax._src.pallas.mosaic.lowering._dot_general_lowering_rule(ctx:LoweringRuleContext,x,y,dimension_numbers,precision,**_)
jax._src.pallas.mosaic.lowering._ensure_mlir_value(val,aval)
jax._src.pallas.mosaic.lowering._exp2_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._exp_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._for_lowering_rule(ctx:LoweringRuleContext,*args,jaxpr,nsteps,reverse,unroll,which_linear)
jax._src.pallas.mosaic.lowering._get_lowering_rule(ctx:LoweringRuleContext,ref,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.pallas.mosaic.lowering._indexer_to_start_size(indexer:NDIndexer)
jax._src.pallas.mosaic.lowering._integer_pow_lowering_rule(ctx:LoweringRuleContext,x,*,y)
jax._src.pallas.mosaic.lowering._iota_lowering_rule(ctx:LoweringRuleContext,dtype,shape,dimension)
jax._src.pallas.mosaic.lowering._load_lowering_rule(ctx:LoweringRuleContext,*args_flat,args_tree,**_)
jax._src.pallas.mosaic.lowering._log1p_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._log_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._logistic_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._lower_jaxpr_to_unrolled_for_loop(ctx:LoweringRuleContext,jaxpr:jax_core.Jaxpr,start:int,num_steps:int,consts,*args,has_loop_index:bool)
jax._src.pallas.mosaic.lowering._make_index(s)
jax._src.pallas.mosaic.lowering._masked_swap_lowering_rule(ctx:LoweringRuleContext,*args_flat,args_tree,**_)
jax._src.pallas.mosaic.lowering._max_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._memory_space_to_tpu_memspace(memory_space:TPUMemorySpace|None)->ir.Attribute
jax._src.pallas.mosaic.lowering._min_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._mul_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._multiple_of_lowering_rule(ctx:LoweringRuleContext,val,*,values)
jax._src.pallas.mosaic.lowering._neg_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._or_lowering_rule(ctx:LoweringRuleContext,lhs,rhs)
jax._src.pallas.mosaic.lowering._pjit_lowering_rule(ctx:LoweringRuleContext,*args,jaxpr,**_)
jax._src.pallas.mosaic.lowering._pow_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._program_id_lowering_rule(ctx:LoweringRuleContext,*,axis:int)
jax._src.pallas.mosaic.lowering._reduce_max_lowering_rule(ctx:LoweringRuleContext,x,*,axes)
jax._src.pallas.mosaic.lowering._reduce_sum_lowering_rule(ctx:LoweringRuleContext,x,*,axes)
jax._src.pallas.mosaic.lowering._rem_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._repeat_lowering_rule(ctx:LoweringRuleContext,x,*,repeats,axis)
jax._src.pallas.mosaic.lowering._reshape_lowering_rule(ctx:LoweringRuleContext,x,new_sizes,dimensions)
jax._src.pallas.mosaic.lowering._rsqrt_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._run_scoped_lowering_rule(ctx:LoweringRuleContext,*consts,jaxpr)
jax._src.pallas.mosaic.lowering._scan_lowering_rule(ctx:LoweringRuleContext,*args,jaxpr:jax_core.Jaxpr,linear:tuple[bool,...],length:int,reverse:bool,unroll:bool,num_consts:int,num_carry:int)
jax._src.pallas.mosaic.lowering._select_n_lowering_rule(ctx:LoweringRuleContext,pred,x,*args)
jax._src.pallas.mosaic.lowering._semaphore_signal_lowering_rule(ctx:LoweringRuleContext,semaphore,value,*args,has_device_id:bool,device_id_type:tpu_primitives.DeviceIdType)
jax._src.pallas.mosaic.lowering._semaphore_wait_lowering_rule(ctx:LoweringRuleContext,semaphore,value)
jax._src.pallas.mosaic.lowering._shift_left_lowering_rule(ctx:LoweringRuleContext,x,d)
jax._src.pallas.mosaic.lowering._shift_right_logical_lowering_rules(ctx:LoweringRuleContext,x,d)
jax._src.pallas.mosaic.lowering._sin_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._slice_lowering_rule(ctx:LoweringRuleContext,x,limit_indices,start_indices,strides)
jax._src.pallas.mosaic.lowering._slice_memref(ref:ir.Value,ref_aval:state.AbstractRef,indexer:NDIndexer)->ir.Value
jax._src.pallas.mosaic.lowering._sqrt_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._squeeze_lowering_rule(ctx:LoweringRuleContext,x,dimensions)
jax._src.pallas.mosaic.lowering._sub_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering._swap_lowering_rule(ctx:LoweringRuleContext,ref,val,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.pallas.mosaic.lowering._tanh_lowering_rule(ctx:LoweringRuleContext,x)
jax._src.pallas.mosaic.lowering._trace_start_lowering_rule(ctx:LoweringRuleContext,*,message:str,level:int)
jax._src.pallas.mosaic.lowering._trace_stop_lowering_rule(ctx:LoweringRuleContext)
jax._src.pallas.mosaic.lowering._transpose_lowering_rule(ctx:LoweringRuleContext,x,*,permutation)
jax._src.pallas.mosaic.lowering._xor_lowering_rule(ctx:LoweringRuleContext,x,y)
jax._src.pallas.mosaic.lowering.aval_to_ir_type(aval,shape=None,memory_space:TPUMemorySpace|None=None)
jax._src.pallas.mosaic.lowering.ir_constant(x,mlir_type=None)
jax._src.pallas.mosaic.lowering.jaxpr_subcomp(ctx:LoweringContext,jaxpr:jax_core.Jaxpr,*args:ir.Value)->Sequence[ir.Value]
jax._src.pallas.mosaic.lowering.lower_fun(fun:Callable,*,multiple_results:bool)->Callable
jax._src.pallas.mosaic.lowering.lower_jaxpr_to_func(ctx:ir.Context,jaxpr:jax_core.Jaxpr,*,grid_mapping:core.GridMapping|None,name:str,mesh_info:Any)->func.FuncOp
jax._src.pallas.mosaic.lowering.lower_jaxpr_to_module(ctx:ir.Context,grid_mapping:core.GridMapping,jaxpr:jax_core.Jaxpr,dimension_semantics:tuple[str|None,...]|None,mesh:mesh_lib.Mesh|None=None)->ir.Module
jax._src.pallas.mosaic.lowering.lower_jaxpr_to_transform_func(ctx:ir.Context,jaxpr:jax_core.Jaxpr,memspaces:Sequence[Any],*,name:str)->func.FuncOp


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/pallas_call_registration.py----------------------------------------
A:jax._src.pallas.mosaic.pallas_call_registration.dimension_semantics->mosaic_params.get('dimension_semantics', None)
A:jax._src.pallas.mosaic.pallas_call_registration.kernel_regeneration_metadata->mosaic_params.get('kernel_regeneration_metadata')
A:jax._src.pallas.mosaic.pallas_call_registration.(mosaic_module, extra_args)->jax._src.pallas.mosaic.lowering.lower_jaxpr_to_module(mlir_ctx, grid_mapping, jaxpr, dimension_semantics=dimension_semantics, mesh=mesh)
jax._src.pallas.mosaic.pallas_call_registration.pallas_call_tpu_lowering_rule(ctx:mlir.LoweringRuleContext,*in_nodes,jaxpr:jax_core.Jaxpr,name:str,which_linear:tuple[bool,...],grid_mapping:core.GridMapping,input_output_aliases:tuple[tuple[int,int],...],in_shapes:tuple[jax.ShapeDtypeStruct,...],out_shapes:tuple[jax.ShapeDtypeStruct,...],debug:bool,interpret:bool,mosaic_params:dict[str,Any]|None=None,**compiler_params:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/core.py----------------------------------------
A:jax._src.pallas.mosaic.core.self.scratch_shapes->tuple(scratch_shapes)
A:jax._src.pallas.mosaic.core.all_avals->jax._src.tree_util.tree_unflatten(in_tree, in_avals)
A:jax._src.pallas.mosaic.core.(flat_scratch_shapes, scratch_tree)->jax._src.tree_util.tree_flatten(self.scratch_shapes)
A:jax._src.pallas.mosaic.core.flat_scratch_avals->map(_make_aval, flat_scratch_shapes)
A:jax._src.pallas.mosaic.core.(scalar_avals, unflat_in_avals)->split_list(all_avals, [self.num_scalar_prefetch])
A:jax._src.pallas.mosaic.core.(flat_scalar_avals, scalar_tree)->jax._src.tree_util.tree_flatten(scalar_avals)
A:jax._src.pallas.mosaic.core.num_flat_scalar_prefetch->len(flat_scalar_avals)
A:jax._src.pallas.mosaic.core.(in_avals, in_avals_tree)->jax._src.tree_util.tree_flatten(tuple(unflat_in_avals))
A:jax._src.pallas.mosaic.core.(flat_in_specs, flat_out_specs)->self._get_in_out_specs(in_avals, in_avals_tree, out_avals, out_tree)
A:jax._src.pallas.mosaic.core.(in_specs, in_ref_avals, out_specs, out_ref_avals)->jax._src.pallas.core._get_ref_avals(self.grid, in_avals, flat_in_specs, out_avals, flat_out_specs)
A:jax._src.pallas.mosaic.core.index_map_in_tree->jax._src.tree_util.tree_structure(((*grid_avals, *scalar_avals), {}))
A:jax._src.pallas.mosaic.core.in_block_mappings->map(partial(_convert_block_spec_to_block_mapping, (*grid_avals, *scalar_ref_avals), in_tree=index_map_in_tree), in_specs, in_ref_avals)
A:jax._src.pallas.mosaic.core.out_block_mappings->map(partial(_convert_block_spec_to_block_mapping, (*grid_avals, *scalar_ref_avals), in_tree=index_map_in_tree), out_specs, out_ref_avals)
A:jax._src.pallas.mosaic.core.grid_mapping->GridMapping(grid=self.grid, block_mappings=(*in_block_mappings, *out_block_mappings), mapped_dims=(), num_index_operands=num_flat_scalar_prefetch, num_scratch_operands=len(flat_scratch_avals))
A:jax._src.pallas.mosaic.core.jaxpr_scalar_ref_avals->jax._src.tree_util.tree_unflatten(scalar_tree, scalar_ref_avals)
A:jax._src.pallas.mosaic.core.jaxpr_in_ref_avals->jax._src.tree_util.tree_unflatten(in_avals_tree, in_ref_avals)
A:jax._src.pallas.mosaic.core.jaxpr_scratch_avals->jax._src.tree_util.tree_unflatten(scratch_tree, flat_scratch_avals)
A:jax._src.pallas.mosaic.core.jaxpr_out_avals->jax._src.tree_util.tree_unflatten(out_tree, out_ref_avals)
jax._src.pallas.mosaic.PrefetchScalarGridSpec(self,num_scalar_prefetch:int,grid:Grid|None=None,in_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,scratch_shapes:Any|Sequence[Any]=())
jax._src.pallas.mosaic.PrefetchScalarGridSpec.get_grid_mapping(self,in_avals,in_tree,out_avals,out_tree)->tuple[tuple[jax_core.AbstractValue, ...], GridMapping]
jax._src.pallas.mosaic.SemaphoreType(enum.Enum)
jax._src.pallas.mosaic.SemaphoreType.get_aval(self)->AbstractSemaphore
jax._src.pallas.mosaic.TPUMemorySpace(self,shape:tuple[int,...],dtype:jnp.dtype)
jax._src.pallas.mosaic.TPUMemorySpace.__str__(self)->str
jax._src.pallas.mosaic.core.AbstractMemoryRef(self,inner_aval:jax_core.AbstractValue,memory_space:TPUMemorySpace)
jax._src.pallas.mosaic.core.AbstractMemoryRef.__eq__(self,other)
jax._src.pallas.mosaic.core.AbstractMemoryRef.__hash__(self)
jax._src.pallas.mosaic.core.AbstractMemoryRef.__init__(self,inner_aval:jax_core.AbstractValue,memory_space:TPUMemorySpace)
jax._src.pallas.mosaic.core.AbstractMemoryRef.__repr__(self)->str
jax._src.pallas.mosaic.core.AbstractMemoryRef.at_least_vspace(self)
jax._src.pallas.mosaic.core.AbstractSemaphore(jax_core.AbstractValue)
jax._src.pallas.mosaic.core.MemoryRef
jax._src.pallas.mosaic.core.MemoryRef.get_aval(self)->AbstractMemoryRef
jax._src.pallas.mosaic.core.PrefetchScalarGridSpec(self,num_scalar_prefetch:int,grid:Grid|None=None,in_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,scratch_shapes:Any|Sequence[Any]=())
jax._src.pallas.mosaic.core.PrefetchScalarGridSpec.__init__(self,num_scalar_prefetch:int,grid:Grid|None=None,in_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,out_specs:BlockSpec|Sequence[BlockSpec|NoBlockSpec]|NoBlockSpec=no_block_spec,scratch_shapes:Any|Sequence[Any]=())
jax._src.pallas.mosaic.core.PrefetchScalarGridSpec.get_grid_mapping(self,in_avals,in_tree,out_avals,out_tree)->tuple[tuple[jax_core.AbstractValue, ...], GridMapping]
jax._src.pallas.mosaic.core.SemaphoreType(enum.Enum)
jax._src.pallas.mosaic.core.SemaphoreType.get_aval(self)->AbstractSemaphore
jax._src.pallas.mosaic.core.TPUMemorySpace(self,shape:tuple[int,...],dtype:jnp.dtype)
jax._src.pallas.mosaic.core.TPUMemorySpace.__call__(self,shape:tuple[int,...],dtype:jnp.dtype)
jax._src.pallas.mosaic.core.TPUMemorySpace.__str__(self)->str
jax._src.pallas.mosaic.core._make_aval(obj:object)->jax_core.AbstractValue
jax._src.pallas.mosaic.core._ref_raise_to_shaped(ref_aval:AbstractMemoryRef,weak_type)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/kernel_regeneration_util.py----------------------------------------
A:jax._src.pallas.mosaic.kernel_regeneration_util.serialized_metadata->bytes(json.dumps(metadata), encoding='utf-8')
jax._src.pallas.mosaic.encode_kernel_regeneration_metadata(metadata:dict[str,Any])->dict[str, bytes]
jax._src.pallas.mosaic.extract_kernel_regeneration_metadata(op:ir.Operation)->dict[str, Any]
jax._src.pallas.mosaic.kernel_regeneration_util.encode_kernel_regeneration_metadata(metadata:dict[str,Any])->dict[str, bytes]
jax._src.pallas.mosaic.kernel_regeneration_util.extract_kernel_regeneration_metadata(op:ir.Operation)->dict[str, Any]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/pallas/mosaic/primitives.py----------------------------------------
A:jax._src.pallas.mosaic.primitives.repeat_p->jax._src.core.Primitive('repeat')
A:jax._src.pallas.mosaic.primitives.shape->list(x.shape)
A:jax._src.pallas.mosaic.primitives.trace_start_p->jax._src.core.Primitive('trace_start')
A:jax._src.pallas.mosaic.primitives.trace_stop_p->jax._src.core.Primitive('trace_stop')
A:jax._src.pallas.mosaic.primitives.run_scoped_p->jax._src.core.Primitive('run_scoped')
A:jax._src.pallas.mosaic.primitives.(flat_types, in_tree)->jax._src.tree_util.tree_flatten((types, kw_types))
A:jax._src.pallas.mosaic.primitives.(flat_fun, _)->jax._src.api_util.flatten_fun(lu.wrap_init(f), in_tree)
A:jax._src.pallas.mosaic.primitives.avals->map(lambda t: t.get_aval(), flat_types)
A:jax._src.pallas.mosaic.primitives.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, avals)
A:jax._src.pallas.mosaic.primitives.semaphore_signal_p->jax._src.core.Primitive('semaphore_signal')
A:jax._src.pallas.mosaic.primitives.inc->jax.numpy.asarray(inc, dtype=jnp.int32)
A:jax._src.pallas.mosaic.primitives.semaphore_wait_p->jax._src.core.Primitive('semaphore_wait')
A:jax._src.pallas.mosaic.primitives.dec->jax.numpy.asarray(dec, dtype=jnp.int32)
A:jax._src.pallas.mosaic.primitives.(flat_args, tree)->jax._src.tree_util.tree_flatten((self.src_ref, self.src_indexer, self.dst_ref, self.dst_indexer, self.dst_sem, self.src_sem, self.device_id))
A:jax._src.pallas.mosaic.primitives.(wait_args, tree)->jax._src.tree_util.tree_flatten((self.src_sem, self.src_ref, self.src_indexer))
A:jax._src.pallas.mosaic.primitives.dma_start_p->jax._src.core.Primitive('dma_start')
A:jax._src.pallas.mosaic.primitives.start_str->jax._src.core.pp_var(start, context)
A:jax._src.pallas.mosaic.primitives.(src_ref, src_indexer, dst_ref, dst_indexer, dst_sem, src_sem, device_id)->jax._src.tree_util.tree_unflatten(tree, invars)
A:jax._src.pallas.mosaic.primitives.src_indexer->jax._src.pallas.indexing.NDIndexer.from_indices_shape(src_indices, src_ref.shape)
A:jax._src.pallas.mosaic.primitives.dst_indexer->jax._src.pallas.indexing.NDIndexer.from_indices_shape(dst_indices, dst_ref.shape)
A:jax._src.pallas.mosaic.primitives.dma_wait_p->jax._src.core.Primitive('dma_wait')
A:jax._src.pallas.mosaic.primitives.(sem, ref, indexer)->jax._src.tree_util.tree_unflatten(tree, invars)
A:jax._src.pallas.mosaic.primitives.(src_ref, src_indices)->_get_ref_and_indexer(src_ref)
A:jax._src.pallas.mosaic.primitives.(dst_ref, dst_indices)->_get_ref_and_indexer(dst_ref)
A:jax._src.pallas.mosaic.primitives.copy_descriptor->make_async_remote_copy(src_ref, dst_ref, send_sem, recv_sem, device_id, device_id_type)
A:jax._src.pallas.mosaic.primitives.device_id_p->jax._src.core.Primitive('device_id')
jax._src.pallas.mosaic.DeviceIdType(enum.Enum)
jax._src.pallas.mosaic.async_copy(src_ref,dst_ref,sem)
jax._src.pallas.mosaic.async_remote_copy(src_ref,dst_ref,send_sem,recv_sem,device_id,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.make_async_copy(src_ref,dst_ref,sem)
jax._src.pallas.mosaic.make_async_remote_copy(src_ref,dst_ref,send_sem,recv_sem,device_id,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.__post_init__(self)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.is_remote(self)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.start(self)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.wait(self)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.wait_recv(self)
jax._src.pallas.mosaic.primitives.AsyncCopyDescriptor.wait_send(self)
jax._src.pallas.mosaic.primitives.DeviceIdType(enum.Enum)
jax._src.pallas.mosaic.primitives._device_id_abstract_eval()
jax._src.pallas.mosaic.primitives._dma_start_abstract_eval(*args,tree,device_id_type)
jax._src.pallas.mosaic.primitives._dma_start_pp_eqn(eqn:jax_core.JaxprEqn,context:jax_core.JaxprPpContext,settings:jax_core.JaxprPpSettings)
jax._src.pallas.mosaic.primitives._dma_wait_abstract_eval(*args,tree,device_id_type)
jax._src.pallas.mosaic.primitives._dma_wait_pp_eqn(eqn:jax_core.JaxprEqn,context:jax_core.JaxprPpContext,settings:jax_core.JaxprPpSettings)
jax._src.pallas.mosaic.primitives._get_ref_and_indexer(ref)
jax._src.pallas.mosaic.primitives._make_copy_descriptor(src_ref,src_indices,dst_ref,dst_indices,dst_sem,src_sem,device_id,device_id_type)->AsyncCopyDescriptor
jax._src.pallas.mosaic.primitives._pp_indexer(indexer:indexing.NDIndexer,context:jax_core.JaxprPpContext)->pp.Doc
jax._src.pallas.mosaic.primitives._pp_ref(ref,indexer,context)
jax._src.pallas.mosaic.primitives._pp_slice(slc:indexing.Slice,dim:int,context:jax_core.JaxprPpContext)->str
jax._src.pallas.mosaic.primitives._repeat_abstract_eval(x,*,repeats,axis)
jax._src.pallas.mosaic.primitives._repeat_lowering_rule(ctx:mlir.LoweringRuleContext,x,*,repeats,axis)
jax._src.pallas.mosaic.primitives._run_scoped_abstract_eval(*args,jaxpr)
jax._src.pallas.mosaic.primitives._semaphore_signal_abstract_eval(sem_aval:tpu_core.AbstractSemaphore,value,*args,has_device_id:bool,device_id_type:DeviceIdType)
jax._src.pallas.mosaic.primitives._semaphore_wait_abstract_eval(sem_aval:tpu_core.AbstractSemaphore,value)
jax._src.pallas.mosaic.primitives._trace_start_abstract_eval(*,message:str,level:int)
jax._src.pallas.mosaic.primitives._trace_stop_abstract_eval()
jax._src.pallas.mosaic.primitives.async_copy(src_ref,dst_ref,sem)
jax._src.pallas.mosaic.primitives.async_remote_copy(src_ref,dst_ref,send_sem,recv_sem,device_id,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.primitives.make_async_copy(src_ref,dst_ref,sem)
jax._src.pallas.mosaic.primitives.make_async_remote_copy(src_ref,dst_ref,send_sem,recv_sem,device_id,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.primitives.repeat(x,repeats,axis)
jax._src.pallas.mosaic.primitives.run_scoped(f:Callable[...,None],*types,**kw_types)->None
jax._src.pallas.mosaic.primitives.semaphore_signal(sem,inc:int|jax.Array=1,*,device_id:int|jax.Array|None=None,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.primitives.semaphore_wait(sem,dec:int|jax.Array=1)
jax._src.pallas.mosaic.primitives.trace(message:str,level:int=10)
jax._src.pallas.mosaic.repeat(x,repeats,axis)
jax._src.pallas.mosaic.run_scoped(f:Callable[...,None],*types,**kw_types)->None
jax._src.pallas.mosaic.semaphore_signal(sem,inc:int|jax.Array=1,*,device_id:int|jax.Array|None=None,device_id_type:DeviceIdType=DeviceIdType.MESH)
jax._src.pallas.mosaic.semaphore_wait(sem,dec:int|jax.Array=1)
jax._src.pallas.mosaic.trace(message:str,level:int=10)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/scipy/signal_helper.py----------------------------------------
A:jax._src.third_party.scipy.signal_helper.win->jax.numpy.asarray(window)
A:jax._src.third_party.scipy.signal_helper.ii_2->jax.numpy.arange(2.0, n, 2)
jax._src.third_party.scipy.signal_helper._median_bias(n:int)->Array
jax._src.third_party.scipy.signal_helper._triage_segments(window:Union[ArrayLike,str,tuple[Any,...]],nperseg:Optional[int],input_length:int,dtype:DTypeLike)->tuple[Array, int]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/scipy/linalg.py----------------------------------------
A:jax._src.third_party.scipy.linalg.minden->jax.numpy.where(minden == 0.0, tol, minden)
A:jax._src.third_party.scipy.linalg.(_, F, minden)->jax.lax.fori_loop(1, N - p + 1, _inner_loop, (p, *F_minden))
A:jax._src.third_party.scipy.linalg.s->jax.numpy.where(den != 0, s / den, s)
A:jax._src.third_party.scipy.linalg.F->F.astype(T.dtype.char).astype(T.dtype.char)
A:jax._src.third_party.scipy.linalg.A_arr->jax.numpy.asarray(A)
A:jax._src.third_party.scipy.linalg.(T, Z)->rsf2csf(T, Z)
A:jax._src.third_party.scipy.linalg.(F, minden)->_algorithm_11_1_1(F, T)
A:jax._src.third_party.scipy.linalg.err->jax.numpy.where(jnp.any(jnp.isinf(F)), jnp.inf, jnp.minimum(1, jnp.maximum(tol, tol / minden * norm(jnp.triu(T, 1), 1))))
jax._src.third_party.scipy.linalg._algorithm_11_1_1(F:Array,T:Array)->tuple[Array, Array]
jax._src.third_party.scipy.linalg.funm(A:ArrayLike,func:Callable[[Array],Array],disp:bool=True)->Array | tuple[Array, Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/scipy/interpolate.py----------------------------------------
A:jax._src.third_party.scipy.interpolate.p->broadcast_arrays(*points)
A:jax._src.third_party.scipy.interpolate.points->points.reshape(-1, ndim).reshape(-1, ndim)
A:jax._src.third_party.scipy.interpolate.(values,)->promote_dtypes_inexact(values)
A:jax._src.third_party.scipy.interpolate.fill_value->asarray(fill_value)
A:jax._src.third_party.scipy.interpolate.self.grid->tuple((asarray(p) for p in points))
A:jax._src.third_party.scipy.interpolate.ndim->len(self.grid)
A:jax._src.third_party.scipy.interpolate.xi->xi.reshape(-1, xi_shape[-1]).reshape(-1, xi_shape[-1])
A:jax._src.third_party.scipy.interpolate.(indices, norm_distances, out_of_bounds)->self._find_indices(xi.T)
A:jax._src.third_party.scipy.interpolate.result->where(out_of_bounds.reshape(bc_shp), self.fill_value, result)
A:jax._src.third_party.scipy.interpolate.edges->product(*[[i, i + 1] for i in indices])
A:jax._src.third_party.scipy.interpolate.values->asarray(0.0)
A:jax._src.third_party.scipy.interpolate.weight->asarray(1.0)
A:jax._src.third_party.scipy.interpolate.out_of_bounds->zeros((xi.shape[1],), dtype=bool)
A:jax._src.third_party.scipy.interpolate.i->where(i > g.size - 2, g.size - 2, i)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator(self,points,values,method='linear',bounds_error=False,fill_value=nan)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator.__init__(self,points,values,method='linear',bounds_error=False,fill_value=nan)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._evaluate_linear(self,indices,norm_distances)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._evaluate_nearest(self,indices,norm_distances)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._find_indices(self,xi)
jax._src.third_party.scipy.interpolate._ndim_coords_from_arrays(points,ndim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/scipy/betaln.py----------------------------------------
A:jax._src.third_party.scipy.betaln.(a, b)->promote_args_inexact('betaln', a, b)
jax._src.third_party.scipy.betaln.algdiv(a,b)
jax._src.third_party.scipy.betaln.betaln(a:ArrayLike,b:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/numpy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/third_party/numpy/linalg.py----------------------------------------
A:jax._src.third_party.numpy.linalg.s->numpy.empty((n, n), dtype=np.intp)
A:jax._src.third_party.numpy.linalg.invx->jax.numpy.linalg.inv(x)
A:jax._src.third_party.numpy.linalg.orig_nan_check->jax.numpy.full_like(r, ~jnp.isnan(r).any())
A:jax._src.third_party.numpy.linalg.nan_mask->jax.numpy.logical_and(jnp.isnan(r), ~jnp.isnan(x).any(axis=(-2, -1)))
A:jax._src.third_party.numpy.linalg.r->jax.numpy.where(orig_nan_check, jnp.where(nan_mask, jnp.inf, r), r)
A:jax._src.third_party.numpy.linalg.a->a.reshape(-1, prod).reshape(-1, prod)
A:jax._src.third_party.numpy.linalg.ia->jax.numpy.linalg.inv(a)
A:jax._src.third_party.numpy.linalg.b->b.ravel().ravel()
A:jax._src.third_party.numpy.linalg.allaxes->list(range(0, an))
A:jax._src.third_party.numpy.linalg.res->res.reshape(Q).reshape(Q)
A:jax._src.third_party.numpy.linalg.n->len(arrays)
A:jax._src.third_party.numpy.linalg.arrays[0]->jax.numpy.atleast_2d(arrays[0])
A:jax._src.third_party.numpy.linalg.result->_multi_dot(arrays, order, 0, n - 1, precision)
A:jax._src.third_party.numpy.linalg.order->_multi_dot_matrix_chain_order(arrays)
A:jax._src.third_party.numpy.linalg.m->numpy.zeros((n, n), dtype=np.double)
jax._src.third_party.numpy.linalg._assert2d(*arrays)
jax._src.third_party.numpy.linalg._assertNdSquareness(*arrays)
jax._src.third_party.numpy.linalg._assertNoEmpty2d(*arrays)
jax._src.third_party.numpy.linalg._assertRankAtLeast2(*arrays)
jax._src.third_party.numpy.linalg._isEmpty2d(arr)
jax._src.third_party.numpy.linalg._multi_dot(arrays,order,i,j,precision)
jax._src.third_party.numpy.linalg._multi_dot_matrix_chain_order(arrays,return_costs=False)
jax._src.third_party.numpy.linalg._multi_dot_three(A,B,C,precision)
jax._src.third_party.numpy.linalg.cond(x,p=None)
jax._src.third_party.numpy.linalg.multi_dot(arrays,*,precision=None)
jax._src.third_party.numpy.linalg.tensorinv(a,ind=2)
jax._src.third_party.numpy.linalg.tensorsolve(a,b,axes=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/nn/functions.py----------------------------------------
A:jax._src.nn.functions.x_arr->jax.numpy.asarray(x)
A:jax._src.nn.functions.[x_arr]->jax._src.numpy.util.promote_args_inexact('gelu', x)
A:jax._src.nn.functions.sqrt_2_over_pi->numpy.sqrt(2 / np.pi).astype(x_arr.dtype)
A:jax._src.nn.functions.sqrt_2->numpy.sqrt(2).astype(x_arr.dtype)
A:jax._src.nn.functions.(x1, x2)->jax.numpy.split(x_arr, 2, axis)
A:jax._src.nn.functions.x_max->jax.numpy.max(x, axis, where=where, initial=initial, keepdims=True)
A:jax._src.nn.functions.shifted_logsumexp->jax.numpy.log(jnp.sum(jnp.exp(shifted), axis, where=where, keepdims=True))
A:jax._src.nn.functions.unnormalized->jax.numpy.exp(x - lax.stop_gradient(x_max))
A:jax._src.nn.functions.result->jax.numpy.where(where, result, 0)
A:jax._src.nn.functions.y->_softmax(x, axis, where, initial)
A:jax._src.nn.functions.mean->jax.numpy.mean(x, axis, keepdims=True, where=where)
A:jax._src.nn.functions.num_classes->jax._src.core.concrete_dim_or_error(num_classes, 'The error arose in jax.nn.one_hot argument `num_classes`.')
A:jax._src.nn.functions.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.nn.functions.output_pos_axis->jax._src.util.canonicalize_axis(axis, x_arr.ndim + 1)
A:jax._src.nn.functions.axis_size->jax.lax.psum(1, axis)
A:jax._src.nn.functions.axis_idx->jax.lax.axis_index(axis)
A:jax._src.nn.functions.axis->operator.index(axis)
A:jax._src.nn.functions.lhs->jax.lax.expand_dims(x_arr, (axis,))
A:jax._src.nn.functions.rhs->jax.lax.broadcasted_iota(x_arr.dtype, rhs_shape, output_pos_axis)
jax._src.nn.functions._one_hot(x:Any,num_classes:int,*,dtype:Any,axis:Union[int,AxisName])->Array
jax._src.nn.functions._softmax(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,where:Optional[ArrayLike]=None,initial:Optional[ArrayLike]=None)->Array
jax._src.nn.functions._softmax_deprecated(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,where:Optional[ArrayLike]=None,initial:Optional[ArrayLike]=None)->Array
jax._src.nn.functions._softmax_jvp(axis,primals,tangents)
jax._src.nn.functions.celu(x:ArrayLike,alpha:ArrayLike=1.0)->Array
jax._src.nn.functions.elu(x:ArrayLike,alpha:ArrayLike=1.0)->Array
jax._src.nn.functions.gelu(x:ArrayLike,approximate:bool=True)->Array
jax._src.nn.functions.glu(x:ArrayLike,axis:int=-1)->Array
jax._src.nn.functions.hard_sigmoid(x:ArrayLike)->Array
jax._src.nn.functions.hard_silu(x:ArrayLike)->Array
jax._src.nn.functions.hard_tanh(x:ArrayLike)->Array
jax._src.nn.functions.leaky_relu(x:ArrayLike,negative_slope:ArrayLike=0.01)->Array
jax._src.nn.functions.log_sigmoid(x:ArrayLike)->Array
jax._src.nn.functions.log_softmax(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,where:Optional[ArrayLike]=None,initial:Optional[ArrayLike]=None)->Array
jax._src.nn.functions.normalize(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,mean:Optional[ArrayLike]=None,variance:Optional[ArrayLike]=None,epsilon:ArrayLike=1e-05,where:Optional[ArrayLike]=None)->Array
jax._src.nn.functions.one_hot(x:Any,num_classes:int,*,dtype:Any=jnp.float_,axis:Union[int,AxisName]=-1)->Array
jax._src.nn.functions.relu(x:ArrayLike)->Array
jax._src.nn.functions.relu6(x:ArrayLike)->Array
jax._src.nn.functions.selu(x:ArrayLike)->Array
jax._src.nn.functions.sigmoid(x:ArrayLike)->Array
jax._src.nn.functions.silu(x:ArrayLike)->Array
jax._src.nn.functions.soft_sign(x:ArrayLike)->Array
jax._src.nn.functions.softmax(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,where:Optional[ArrayLike]=None,initial:Optional[ArrayLike]=None)->Array
jax._src.nn.functions.softplus(x:ArrayLike)->Array
jax._src.nn.functions.standardize(x:ArrayLike,axis:Optional[Union[int,tuple[int,...]]]=-1,mean:Optional[ArrayLike]=None,variance:Optional[ArrayLike]=None,epsilon:ArrayLike=1e-05,where:Optional[ArrayLike]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/nn/initializers.py----------------------------------------
A:jax._src.nn.initializers.export->set_module('jax.nn.initializers')
A:jax._src.nn.initializers.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.nn.initializers.in_size->math.prod([shape[i] for i in in_axis])
A:jax._src.nn.initializers.out_size->math.prod([shape[i] for i in out_axis])
A:jax._src.nn.initializers.batch_size->math.prod([shape[i] for i in batch_axis])
A:jax._src.nn.initializers.(key_r, key_theta)->jax.random.split(key)
A:jax._src.nn.initializers.r->jax.numpy.sqrt(-jnp.log(1 - t))
A:jax._src.nn.initializers.named_shape->jax._src.core.as_named_shape(shape)
A:jax._src.nn.initializers.(fan_in, fan_out)->_compute_fans(named_shape, in_axis, out_axis, batch_axis)
A:jax._src.nn.initializers.variance->jax.numpy.array(scale / denominator, dtype=dtype)
A:jax._src.nn.initializers.A->jax.random.normal(key, matrix_shape, dtype)
A:jax._src.nn.initializers.(Q, R)->jax.numpy.linalg.qr(A)
A:jax._src.nn.initializers.diag_sign->jax.lax.broadcast_to_rank(jnp.sign(jnp.diag(R)), rank=Q.ndim)
A:jax._src.nn.initializers.Q->jax.numpy.moveaxis(Q, -1, column_axis)
A:jax._src.nn.initializers.ortho_init->orthogonal(scale=scale, column_axis=column_axis, dtype=dtype)
A:jax._src.nn.initializers.ortho_matrix->ortho_init(key, shape[-2:])
A:jax._src.nn.initializers.W->jax.numpy.zeros(shape, dtype=dtype)
jax._src.nn.initializers.Initializer(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)
jax._src.nn.initializers.Initializer.__call__(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)
jax._src.nn.initializers._complex_truncated_normal(key:KeyArray,upper:ArrayLike,shape:Union[Sequence[int],core.NamedShape],dtype:DTypeLikeInexact)->Array
jax._src.nn.initializers._complex_uniform(key:KeyArray,shape:Union[Sequence[int],core.NamedShape],dtype:DTypeLikeInexact)->Array
jax._src.nn.initializers._compute_fans(shape:core.NamedShape,in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Union[int,Sequence[int]]=())->tuple[Array, Array]
jax._src.nn.initializers.constant(value:ArrayLike,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.delta_orthogonal(scale:RealNumeric=1.0,column_axis:int=-1,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.glorot_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.glorot_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.he_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.he_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.lecun_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.lecun_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.normal(stddev:RealNumeric=0.01,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.ones(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)->Array
jax._src.nn.initializers.orthogonal(scale:RealNumeric=1.0,column_axis:int=-1,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.truncated_normal(stddev:RealNumeric=0.01,dtype:DTypeLikeInexact=jnp.float_,lower:RealNumeric=-2.0,upper:RealNumeric=2.0)->Initializer
jax._src.nn.initializers.uniform(scale:RealNumeric=0.01,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.variance_scaling(scale:RealNumeric,mode:Union[Literal['fan_in'],Literal['fan_out'],Literal['fan_avg']],distribution:Union[Literal['truncated_normal'],Literal['normal'],Literal['uniform']],in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.zeros(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/ops/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/ops/scatter.py----------------------------------------
A:jax._src.ops.scatter.x->jax._src.numpy.lax_numpy.asarray(x)
A:jax._src.ops.scatter.y->jax.lax.rev(y, indexer.reversed_y_dims)
A:jax._src.ops.scatter.(treedef, static_idx, dynamic_idx)->jax._src.numpy.lax_numpy._split_index_for_jit(idx, x.shape)
A:jax._src.ops.scatter.dtype->jax.lax.dtype(x)
A:jax._src.ops.scatter.weak_type->jax._src.dtypes.is_weakly_typed(x)
A:jax._src.ops.scatter.idx->jax._src.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)
A:jax._src.ops.scatter.indexer->jax._src.numpy.lax_numpy._index_to_gather(jnp.shape(x), idx, normalize_indices=normalize_indices)
A:jax._src.ops.scatter.(x, y)->promote_dtypes(x, y)
A:jax._src.ops.scatter.dnums->jax.lax.ScatterDimensionNumbers(update_window_dims=indexer.dnums.offset_dims, inserted_window_dims=indexer.dnums.collapsed_slice_dims, scatter_dims_to_operand_dims=indexer.dnums.start_index_map)
A:jax._src.ops.scatter.out->_scatter_update(out, np.index_exp[jnp.arange(segment_ids.shape[0]) // bucket_size, segment_ids[None, :]], data, scatter_op, indices_are_sorted, unique_indices, normalize_indices=False, mode=mode)
A:jax._src.ops.scatter.data->jax._src.numpy.lax_numpy.asarray(data)
A:jax._src.ops.scatter.segment_ids->jax._src.numpy.lax_numpy.asarray(segment_ids)
A:jax._src.ops.scatter.num_segments->jax._src.core.concrete_or_error(int, num_segments, 'segment_sum() `num_segments` argument.')
A:jax._src.ops.scatter.num_buckets->jax._src.util.ceil_of_ratio(segment_ids.size, bucket_size)
jax._src.ops.scatter._get_identity(op,dtype)
jax._src.ops.scatter._scatter_impl(x,y,scatter_op,treedef,static_idx,dynamic_idx,indices_are_sorted,unique_indices,mode,normalize_indices)
jax._src.ops.scatter._scatter_update(x,idx,y,scatter_op,indices_are_sorted,unique_indices,mode=None,normalize_indices=True)
jax._src.ops.scatter._segment_update(name:str,data:ArrayLike,segment_ids:ArrayLike,scatter_op:Callable,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,reducer:Optional[Callable]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_max(data:ArrayLike,segment_ids:ArrayLike,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_min(data:ArrayLike,segment_ids:ArrayLike,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_prod(data:ArrayLike,segment_ids:ArrayLike,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_sum(data:ArrayLike,segment_ids:ArrayLike,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/ops/special.py----------------------------------------
A:jax._src.ops.special.(a_arr, b_arr)->promote_args_inexact('logsumexp', a, b)
A:jax._src.ops.special.a_arr->jax.numpy.where(b_arr != 0, a_arr, -jnp.inf)
A:jax._src.ops.special.(a_arr,)->promote_args_inexact('logsumexp', a)
A:jax._src.ops.special.(pos_dims, dims)->_reduction_dims(a_arr, axis)
A:jax._src.ops.special.amax->jax.lax.stop_gradient(lax.select(jnp.isfinite(amax), amax, lax.full_like(amax, 0)))
A:jax._src.ops.special.out->jax.numpy.where(sign < 0, jnp.array(np.nan, dtype=out.dtype), out)
A:jax._src.ops.special.sign->jax.lax.stop_gradient(jnp.sign(sumexp))
A:jax._src.ops.special.expsub->jax.lax.mul(expsub, b_arr)
A:jax._src.ops.special.sumexp->jax.numpy.sum(expsub, axis=dims, keepdims=keepdims)
jax._src.ops.special.logsumexp(a:ArrayLike,axis:Axis=None,b:Optional[ArrayLike]=None,keepdims:bool=False,return_sign:bool=False)->Union[Array, tuple[Array, Array]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/ad.py----------------------------------------
A:jax._src.interpreters.ad.(fun, aux)->jvp_subtrace_aux(fun)
A:jax._src.interpreters.ad.trace->JVPTrace(main, core.cur_sublevel())
A:jax._src.interpreters.ad.out_tracers->map(trace.full_raise, ans)
A:jax._src.interpreters.ad.ans_tracers->map(trace.full_raise, ans)
A:jax._src.interpreters.ad.(out_primals, out_tangents)->unzip2(((t.primal, t.tangent) for t in ans_tracers))
A:jax._src.interpreters.ad.has_aux->kwargs.pop('has_aux', False)
A:jax._src.interpreters.ad.jvpfun->jvp(traceable)
A:jax._src.interpreters.ad.(jvpfun, aux)->jvp(traceable, has_aux=True)
A:jax._src.interpreters.ad.(_, in_tree)->tree_flatten(((primals, primals), {}))
A:jax._src.interpreters.ad.(jvpfun_flat, out_tree)->flatten_fun(jvpfun, in_tree)
A:jax._src.interpreters.ad.(jaxpr, out_pvals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)
A:jax._src.interpreters.ad.(out_primals_pvals, out_tangents_pvals)->tree_unflatten(out_tree(), out_pvals)
A:jax._src.interpreters.ad.(out_primals, pvals, jaxpr, consts)->linearize(traceable, *primals)
A:jax._src.interpreters.ad.(out_primals, pvals, jaxpr, consts, aux)->linearize(traceable, *primals, has_aux=True)
A:jax._src.interpreters.ad.cts->tuple((ct for (ct, pval) in zip(cts, pvals) if not pval.is_known()))
A:jax._src.interpreters.ad.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax._src.interpreters.ad.vjp_->Partial(partial(unbound_vjp, pvals, jaxpr), consts)
A:jax._src.interpreters.ad.axes_to_reduce->tuple((axis_name for axis_name in reduce_axes if axis_name in core.get_aval(ct).named_shape and axis_name not in v.aval.named_shape))
A:jax._src.interpreters.ad.ct->jax.lax.psum(ct, axis_name=axes_to_reduce)
A:jax._src.interpreters.ad.a->a.update(shape=tuple(shape)).update(shape=tuple(shape))
A:jax._src.interpreters.ad.invals->map(read_primal, eqn.invars)
A:jax._src.interpreters.ad.cts_in->map(replace_rule_output_symbolic_zeros, cts_in)
A:jax._src.interpreters.ad.(cts_in,)->map(read_cotangent, eqn.outvars)
A:jax._src.interpreters.ad.params->update_params(params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct])
A:jax._src.interpreters.ad.call_jaxpr->update_params(params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct]).pop('call_jaxpr')
A:jax._src.interpreters.ad.cts_out->map(instantiate_zeros_aval, out_avals, cts_out)
A:jax._src.interpreters.ad.cotangents_out->map(read_cotangent, jaxpr.invars)
A:jax._src.interpreters.ad.tangent_zero->Zero(get_aval(val).at_least_vspace())
A:jax._src.interpreters.ad.(primals_in, tangents_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax._src.interpreters.ad.jvp->primitive_jvps.get(primitive)
A:jax._src.interpreters.ad.(primal_out, tangent_out)->tree_unflatten(out_tree(), result)
A:jax._src.interpreters.ad.(primals, tangents)->tree_unflatten(in_tree, primals_and_tangents)
A:jax._src.interpreters.ad.(args, in_tree)->tree_flatten((primals, tangents))
A:jax._src.interpreters.ad.f_jvp->jvp_subtrace(f, self.main)
A:jax._src.interpreters.ad.(f_jvp, which_nz_out)->nonzero_tangent_outputs(f_jvp)
A:jax._src.interpreters.ad.out_ax->out_axes_thunk()
A:jax._src.interpreters.ad.(f_jvp, out_tree)->traceable(f_jvp, in_tree)
A:jax._src.interpreters.ad.update_params->call_transpose_param_updaters.get(primitive)
A:jax._src.interpreters.ad.result->call_primitive.bind(_update_annotation(f_jvp, f.in_type, which_nz), *args, **new_params)
A:jax._src.interpreters.ad.(out, treedef)->tree_flatten((primals, tangents))
A:jax._src.interpreters.ad.primals_in->map(core.full_lower, primals_in)
A:jax._src.interpreters.ad.tangents_in->map(instantiate_zeros, tangents_in)
A:jax._src.interpreters.ad.outs->jvp_subtrace(f, self.main).call_wrapped(*it.chain(primals_in, tangents_in))
A:jax._src.interpreters.ad.(primals_out, tangents_out)->split_list(outs, [len(outs) // 2])
A:jax._src.interpreters.ad.tangents_out->list(tangents_out)
A:jax._src.interpreters.ad.res_and_primals_out->fwd.call_wrapped(*fwd_in)
A:jax._src.interpreters.ad.(_, res_tree)->out_trees()
A:jax._src.interpreters.ad.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax._src.interpreters.ad.(ps_in, ts_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax._src.interpreters.ad.(res_ps_in, lin_ps_in)->split_list(ps_in, [params['res_tree'].num_leaves])
A:jax._src.interpreters.ad.(res_ts_in, lin_ts_in)->split_list(ts_in, [params['res_tree'].num_leaves])
A:jax._src.interpreters.ad.ps_out->prim.bind(call, *ps_in, **params)
A:jax._src.interpreters.ad.lin_ts_in->map(instantiate_zeros, lin_ts_in)
A:jax._src.interpreters.ad.ts_out->prim.bind(call, *res_ps_in, *lin_ts_in, **params)
A:jax._src.interpreters.ad.primal_aval->raise_to_shaped(get_aval(primal), weak_type=False)
A:jax._src.interpreters.ad.tangent_aval->raise_to_shaped(get_aval(tangent), weak_type=False)
A:jax._src.interpreters.ad.expected_tangent_dtype->jax._src.core.primal_dtype_to_tangent_dtype(primal_aval.dtype)
A:jax._src.interpreters.ad.primitive_jvps[primitive]->partial(zero_jvp, primitive)
A:jax._src.interpreters.ad.primitive_transposes[primitive]->partial(linear_transpose2, transpose_rule)
A:jax._src.interpreters.ad.val_out->primitive.bind(*primals, **params)
A:jax._src.interpreters.ad.tangents->map(instantiate_zeros, tangents)
A:jax._src.interpreters.ad.primitive_transposes[prim]->partial(bilinear_transpose, lhs_rule, rhs_rule)
A:jax._src.interpreters.ad.out->rhs_rule(cotangent, x, y, **kwargs)
A:jax._src.interpreters.ad.r->primitive.bind(*primals, **params)
A:jax._src.interpreters.ad.(out_flat, out_tree)->tree_flatten((primals_out, tangents_out))
A:jax._src.interpreters.ad.(all_args, in_tree_def)->tree_flatten(((), args, ct))
A:jax._src.interpreters.ad.fun->jax._src.linear_util.hashable_partial(lu.wrap_init(backward_pass), call_jaxpr, reduce_axes, False)
A:jax._src.interpreters.ad.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax._src.interpreters.ad.(res_invars, _)->partition_list(which_lin, call_jaxpr.invars)
A:jax._src.interpreters.ad.out_flat->primitive.bind(fun, *all_args, **new_params)
A:jax._src.interpreters.ad.primitive_transposes[core.call_p]->partial(call_transpose, call_p)
A:jax._src.interpreters.ad.jaxpr_->jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_)
A:jax._src.interpreters.ad.(fun, nz_arg_cts)->nonzero_outputs(fun)
A:jax._src.interpreters.ad.new_params->update_params(new_params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct])
A:jax._src.interpreters.ad.f->jax._src.linear_util.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax._src.interpreters.ad.(f_jvp, out_nonzeros)->f_jvp_traceable(jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)
A:jax._src.interpreters.ad.avals_in->list(it.chain(jaxpr.in_avals, tangent_avals))
A:jax._src.interpreters.ad.(jaxpr_out, avals_out, literals_out)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f_jvp, avals_in)
A:jax._src.interpreters.ad.num_primals->len(nonzeros)
A:jax._src.interpreters.ad.primals->list(primals_and_nztangents[:num_primals])
A:jax._src.interpreters.ad.nonzero_tangents->iter(primals_and_nztangents[num_primals:])
A:jax._src.interpreters.ad.new_invars->_perm(primals_in, tangents_in, jaxpr.jaxpr.invars)
A:jax._src.interpreters.ad.new_outvars->_perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)
A:jax._src.interpreters.ad.new_jaxpr->jax._src.core.Jaxpr(jaxpr.jaxpr.constvars, new_invars, new_outvars, jaxpr.jaxpr.eqns, jaxpr.jaxpr.effects)
A:jax._src.interpreters.ad.n->sum(primal_counts)
A:jax._src.interpreters.ad.primal_groups->split_list(primals, primal_counts[:-1])
A:jax._src.interpreters.ad.tangent_groups->split_list(tangents, tangent_counts[:-1])
A:jax._src.interpreters.ad.(res, _)->split_list(invals, [num_res])
jax._src.ad.CustomJVPException(self)
jax._src.ad.CustomVJPException(self)
jax._src.ad.JVPTrace(Trace)
jax._src.ad.JVPTrace.join(self,xt,yt)
jax._src.ad.JVPTrace.lift(self,val)
jax._src.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.ad.JVPTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.ad.JVPTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.ad.JVPTrace.process_call(self,call_primitive,f,tracers,params)
jax._src.ad.JVPTrace.process_custom_jvp_call(self,_,__,f_jvp,tracers,*,symbolic_zeros)
jax._src.ad.JVPTrace.process_custom_vjp_call(self,_,__,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax._src.ad.JVPTrace.pure(self,val)
jax._src.ad.JVPTrace.sublift(self,val)
jax._src.ad.JVPTracer(self,trace,primal,tangent)
jax._src.ad.JVPTracer.aval(self)
jax._src.ad.JVPTracer.full_lower(self)
jax._src.ad.UndefinedPrimal(self,aval)
jax._src.ad.UndefinedPrimal.__repr__(self)
jax._src.ad._closed_call_transpose(params,jaxpr,args,ct,cts_in_avals,reduce_axes)
jax._src.ad._custom_lin_transpose(cts_out,*invals,num_res,bwd,out_avals,symbolic_zeros)
jax._src.ad._interleave(xs,ys)
jax._src.ad._jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax._src.ad._perm(primal_counts,tangent_counts,lst)
jax._src.ad._primal_tangent_shapes_match(primal,tangent)
jax._src.ad.add_tangents(x,y)
jax._src.ad.backward_pass(jaxpr:core.Jaxpr,reduce_axes,transform_stack,consts,primals_in,cotangents_in)
jax._src.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax._src.ad.call_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax._src.ad.closed_backward_pass(jaxpr:core.ClosedJaxpr,reduce_axes,transform_stack,primals_in,cotangents_in)
jax._src.ad.defbilinear(prim,lhs_rule,rhs_rule)
jax._src.ad.defjvp(primitive,*jvprules)
jax._src.ad.defjvp2(primitive,*jvprules)
jax._src.ad.defjvp_zero(primitive)
jax._src.ad.deflinear(primitive,transpose_rule)
jax._src.ad.deflinear2(primitive,transpose_rule)
jax._src.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax._src.ad.get_primitive_transpose(p)
jax._src.ad.identity(x)
jax._src.ad.instantiate_zeros(tangent)
jax._src.ad.instantiate_zeros_aval(aval,tangent)
jax._src.ad.is_undefined_primal(x)
jax._src.ad.jvp(fun:lu.WrappedFun,has_aux=False,instantiate=True,transform_stack=True)->Any
jax._src.ad.jvp_jaxpr(jaxpr:core.ClosedJaxpr,nonzeros:Sequence[bool],instantiate:Union[bool,Sequence[bool]])->tuple[core.ClosedJaxpr, list[bool]]
jax._src.ad.jvp_subtrace(main,primals,tangents)
jax._src.ad.jvp_subtrace_aux(main,primals,tangents)
jax._src.ad.jvpfun(instantiate,transform_stack,primals,tangents)
jax._src.ad.linear_jvp(primitive,primals,tangents,**params)
jax._src.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax._src.ad.linear_transpose2(transpose_rule,cotangent,*args,**kwargs)
jax._src.ad.linearize(traceable,*primals,**kwargs)
jax._src.ad.map_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax._src.ad.nonzero_outputs(*args,**kwargs)
jax._src.ad.nonzero_tangent_outputs(*args,**kwargs)
jax._src.ad.raise_custom_vjp_error_on_jvp(*_,**__)
jax._src.ad.rearrange_binders(jaxpr:core.ClosedJaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax._src.ad.recast_to_float0(primal,tangent)
jax._src.ad.replace_float0s(primal,tangent)
jax._src.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax._src.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax._src.ad.traceable(in_tree,*primals_and_tangents)
jax._src.ad.unpair_pval(pval)
jax._src.ad.vjp(traceable,primals,has_aux=False,reduce_axes=())
jax._src.ad.zero_jvp(primitive,primals,tangents,**params)
jax._src.interpreters.ad.CustomJVPException(self)
jax._src.interpreters.ad.CustomJVPException.__init__(self)
jax._src.interpreters.ad.CustomVJPException(self)
jax._src.interpreters.ad.CustomVJPException.__init__(self)
jax._src.interpreters.ad.JVPTrace(Trace)
jax._src.interpreters.ad.JVPTrace.join(self,xt,yt)
jax._src.interpreters.ad.JVPTrace.lift(self,val)
jax._src.interpreters.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.interpreters.ad.JVPTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.interpreters.ad.JVPTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.interpreters.ad.JVPTrace.process_call(self,call_primitive,f,tracers,params)
jax._src.interpreters.ad.JVPTrace.process_custom_jvp_call(self,_,__,f_jvp,tracers,*,symbolic_zeros)
jax._src.interpreters.ad.JVPTrace.process_custom_transpose(self,prim,call,tracers,**params)
jax._src.interpreters.ad.JVPTrace.process_custom_vjp_call(self,_,__,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.interpreters.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax._src.interpreters.ad.JVPTrace.pure(self,val)
jax._src.interpreters.ad.JVPTrace.sublift(self,val)
jax._src.interpreters.ad.JVPTracer(self,trace,primal,tangent)
jax._src.interpreters.ad.JVPTracer.__init__(self,trace,primal,tangent)
jax._src.interpreters.ad.JVPTracer.aval(self)
jax._src.interpreters.ad.JVPTracer.full_lower(self)
jax._src.interpreters.ad.UndefinedPrimal(self,aval)
jax._src.interpreters.ad.UndefinedPrimal.__init__(self,aval)
jax._src.interpreters.ad.UndefinedPrimal.__repr__(self)
jax._src.interpreters.ad._closed_call_transpose(params,jaxpr,args,ct,cts_in_avals,reduce_axes)
jax._src.interpreters.ad._custom_lin_transpose(cts_out,*invals,num_res,bwd,out_avals,symbolic_zeros)
jax._src.interpreters.ad._interleave(xs,ys)
jax._src.interpreters.ad._jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax._src.interpreters.ad._perm(primal_counts,tangent_counts,lst)
jax._src.interpreters.ad._primal_tangent_shapes_match(primal,tangent)
jax._src.interpreters.ad._update_annotation(f:lu.WrappedFun,orig_type:Optional[tuple[tuple[core.AbstractValue,bool],...]],explicit_nonzeros:list[bool])->lu.WrappedFun
jax._src.interpreters.ad.add_tangents(x,y)
jax._src.interpreters.ad.backward_pass(jaxpr:core.Jaxpr,reduce_axes,transform_stack,consts,primals_in,cotangents_in)
jax._src.interpreters.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax._src.interpreters.ad.call_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax._src.interpreters.ad.closed_backward_pass(jaxpr:core.ClosedJaxpr,reduce_axes,transform_stack,primals_in,cotangents_in)
jax._src.interpreters.ad.defbilinear(prim,lhs_rule,rhs_rule)
jax._src.interpreters.ad.defjvp(primitive,*jvprules)
jax._src.interpreters.ad.defjvp2(primitive,*jvprules)
jax._src.interpreters.ad.defjvp_zero(primitive)
jax._src.interpreters.ad.deflinear(primitive,transpose_rule)
jax._src.interpreters.ad.deflinear2(primitive,transpose_rule)
jax._src.interpreters.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax._src.interpreters.ad.get_primitive_transpose(p)
jax._src.interpreters.ad.identity(x)
jax._src.interpreters.ad.instantiate_zeros(tangent)
jax._src.interpreters.ad.instantiate_zeros_aval(aval,tangent)
jax._src.interpreters.ad.is_undefined_primal(x)
jax._src.interpreters.ad.jvp(fun:lu.WrappedFun,has_aux=False,instantiate=True,transform_stack=True)->Any
jax._src.interpreters.ad.jvp_jaxpr(jaxpr:core.ClosedJaxpr,nonzeros:Sequence[bool],instantiate:Union[bool,Sequence[bool]])->tuple[core.ClosedJaxpr, list[bool]]
jax._src.interpreters.ad.jvp_subtrace(main,primals,tangents)
jax._src.interpreters.ad.jvp_subtrace_aux(main,primals,tangents)
jax._src.interpreters.ad.jvpfun(instantiate,transform_stack,primals,tangents)
jax._src.interpreters.ad.linear_jvp(primitive,primals,tangents,**params)
jax._src.interpreters.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax._src.interpreters.ad.linear_transpose2(transpose_rule,cotangent,*args,**kwargs)
jax._src.interpreters.ad.linearize(traceable,*primals,**kwargs)
jax._src.interpreters.ad.map_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax._src.interpreters.ad.nonzero_outputs(*args,**kwargs)
jax._src.interpreters.ad.nonzero_tangent_outputs(*args,**kwargs)
jax._src.interpreters.ad.raise_custom_vjp_error_on_jvp(*_,**__)
jax._src.interpreters.ad.rearrange_binders(jaxpr:core.ClosedJaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax._src.interpreters.ad.recast_to_float0(primal,tangent)
jax._src.interpreters.ad.replace_float0s(primal,tangent)
jax._src.interpreters.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax._src.interpreters.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax._src.interpreters.ad.traceable(in_tree,*primals_and_tangents)
jax._src.interpreters.ad.unpair_pval(pval)
jax._src.interpreters.ad.vjp(traceable,primals,has_aux=False,reduce_axes=())
jax._src.interpreters.ad.zero_jvp(primitive,primals,tangents,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/batching.py----------------------------------------
A:jax._src.interpreters.batching.jumble_axis->JumbleAxis()
A:jax._src.interpreters.batching.elt_ty->moveaxis(x, bdx, bdy).aval.update(shape=x.shape[1:])
A:jax._src.interpreters.batching.aval->JumbleTy(core.Var(0, '', core.ShapedArray((), np.dtype('int32'))), x.shape[0], elt_ty)
A:jax._src.interpreters.batching.binder->jax._src.core.Var(0, '', core.ShapedArray((), np.dtype('int32')))
A:jax._src.interpreters.batching.shape->list(np.shape(x))
A:jax._src.interpreters.batching.shape[ragged_axis - 1]->IndexedAxisSize(binder, segment_lens)
A:jax._src.interpreters.batching.new_axes->tuple(((move_axis(ax), sizes) for (ax, sizes) in self.ragged_axes))
A:jax._src.interpreters.batching.result->moveaxis(operand, bdim.stacked_axis, dst)
A:jax._src.interpreters.batching.result[ragged_axis]->IndexedAxisSize(binder, segment_lens)
A:jax._src.interpreters.batching.expl_names->set(map(Name, new_avals))
A:jax._src.interpreters.batching.handler->make_iota_handlers.get(type(axis_size))
A:jax._src.interpreters.batching.batch_axis->make_batch_axis(x.data.ndim, 0, [(d + 1, ias.lengths)])
A:jax._src.interpreters.batching.x_->main.with_cur_sublevel().full_raise(x)
A:jax._src.interpreters.batching.(spec_type, axis_size_type)->vmappables.pop(data_type)
A:jax._src.interpreters.batching.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax._src.interpreters.batching.NotMapped->type(None)
A:jax._src.interpreters.batching.new_aval->JumbleTy(core.Var(0, '', core.ShapedArray((), np.dtype('int32'))), x.shape[0], elt_ty).update(shape=tuple(new_shape))
A:jax._src.interpreters.batching.size_tracer->BatchTracer(self._trace, segment_lengths, 0)
A:jax._src.interpreters.batching.(axis_size,)->jax._src.core.dedup_referents(sizes)
A:jax._src.interpreters.batching.frame->self.get_frame(vals_in, dims_in)
A:jax._src.interpreters.batching.(vals_in, dims_in)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax._src.interpreters.batching.used_names->jax._src.core.used_axis_names(primitive, params)
A:jax._src.interpreters.batching.batcher_primitive->self.get_axis_primitive_batcher(primitive, frame)
A:jax._src.interpreters.batching.(val_out, dim_out)->batched_primitive(vals_in, dims_in, **params)
A:jax._src.interpreters.batching.batched_primitive->self.get_primitive_batcher(primitive, frame)
A:jax._src.interpreters.batching.src->jax._src.source_info_util.current()
A:jax._src.interpreters.batching.params->dict(params, input_shape=operand.shape)
A:jax._src.interpreters.batching.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax._src.interpreters.batching.(segment_lens, dims)->indirectify_ragged_axes(dims)
A:jax._src.interpreters.batching.(f_, dims_out)->batch_subtrace(f, self.main, tuple(dims))
A:jax._src.interpreters.batching.f_->_update_annotation(f_, f.in_type, axis_size, self.axis_name, dims, segment_lens)
A:jax._src.interpreters.batching.vals_out->map_primitive.bind(f, *vals, **new_params)
A:jax._src.interpreters.batching.(vals_out, dims_out)->resolve_ragged_axes(vals_out, dims_out())
A:jax._src.interpreters.batching.(vals, dims, srcs)->unzip3(((t.val, t.batch_dim, t.source_info) for t in out_tracers))
A:jax._src.interpreters.batching.trace->main.with_cur_sublevel()
A:jax._src.interpreters.batching.new_in_axes->tuple((in_axis + 1 if both_mapped(in_axis, d) and d <= in_axis else in_axis for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax._src.interpreters.batching.new_dims->tuple((d - 1 if both_mapped(in_axis, d) and in_axis < d else d for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax._src.interpreters.batching.(f, dims_out)->batch_subtrace(f, self.main, new_dims)
A:jax._src.interpreters.batching.new_params->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk)
A:jax._src.interpreters.batching.(in_vals, in_dims)->resolve_ragged_axes(in_vals, in_dims)
A:jax._src.interpreters.batching.(fun, out_dims1)->batch_subtrace(fun, self.main, in_dims)
A:jax._src.interpreters.batching.(jvp, out_dims2)->batch_custom_jvp_subtrace(jvp, self.main, in_dims)
A:jax._src.interpreters.batching.out_vals->map(partial(matchaxis, trace.axis_name, axis_size), out_axes, out_axes_dest, out_vals)
A:jax._src.interpreters.batching.(fst, out_dims)->jax._src.linear_util.merge_linear_aux(out_dims1, out_dims2)
A:jax._src.interpreters.batching.(fwd, out_dims2)->batch_subtrace(fwd, self.main, fwd_in_dims)
A:jax._src.interpreters.batching.bwd->batch_custom_vjp_bwd(bwd, self.axis_name, axis_size, out_dims2, in_dims, self.main.trace_type, self.spmd_axis_name)
A:jax._src.interpreters.batching.(_, res_tree)->out_trees()
A:jax._src.interpreters.batching.(_, out_dims)->split_list(out_dims, [res_tree.num_leaves])
A:jax._src.interpreters.batching.(res_dims, primal_dims)->split_list(dims, [num_res])
A:jax._src.interpreters.batching.(_, primal_srcs)->split_list(srcs, [num_res])
A:jax._src.interpreters.batching.f->_batch_jaxpr_outer(f, axis_name, spmd_axis_name, axis_size, in_axes, main_type)
A:jax._src.interpreters.batching.idx->memoize(lambda : BatchTracer(trace, make_iota(axis_size), 0, source_info_util.current()))
A:jax._src.interpreters.batching.in_tracers->map(partial(to_elt, trace, idx), in_vals, in_dims)
A:jax._src.interpreters.batching.out_tracers->map(trace.full_raise, outs)
A:jax._src.interpreters.batching.(out_vals, out_dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax._src.interpreters.batching.(segment_lens, out_dims)->indirectify_ragged_axes(out_dims)
A:jax._src.interpreters.batching.(_, dbidx)->axis_map.setdefault(id(core.get_referent(segment_lengths)), (segment_lengths, pe.DBIdx(len(axis_map))))
A:jax._src.interpreters.batching.key->id(core.get_referent(segment_lengths))
A:jax._src.interpreters.batching.value->ident(operand.dtype)
A:jax._src.interpreters.batching.(f, out_axes)->_batch_jaxpr_inner(f, axis_size)
A:jax._src.interpreters.batching.(in_axes2, avals_in)->unzip2([handle_ragged(closed_jaxpr.in_avals, dim, aval) if isinstance(dim, RaggedAxis) else (dim, aval) for (dim, aval) in zip(in_axes, closed_jaxpr.in_avals)])
A:jax._src.interpreters.batching.(jaxpr_out, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, avals_in)
A:jax._src.interpreters.batching.new_shape->list(aval.shape)
A:jax._src.interpreters.batching.(f, out_batched)->_match_axes_jaxpr(f, axis_size, out_axes_dest, out_axes)
A:jax._src.interpreters.batching.(_, in_axes)->resolve_ragged_axes(in_vals, in_axes)
A:jax._src.interpreters.batching.(out_vals, out_axes)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax._src.interpreters.batching.new_out_axes->indirectify_ragged_axes_against_inputs_outputs(out_axes, in_vals, out_vals)
A:jax._src.interpreters.batching.out_axes->out_axes()
A:jax._src.interpreters.batching.zero_if_mapped->ZeroIfMapped()
A:jax._src.interpreters.batching.(out_primals, out_tangents)->split_list(out_vals, [len(out_vals) // 2])
A:jax._src.interpreters.batching.(out_primal_bds, out_tangent_bds)->split_list(out_dims, [len(out_vals) // 2])
A:jax._src.interpreters.batching.out_dims->map(_merge_bdims, out_primal_bds, out_tangent_bds)
A:jax._src.interpreters.batching.out_primals->map(partial(matchaxis, trace.axis_name, size), out_primal_bds, out_dims, out_primals)
A:jax._src.interpreters.batching.out_tangents->map(partial(matchaxis, trace.axis_name, size), out_tangent_bds, out_dims, out_tangents)
A:jax._src.interpreters.batching.(bwd_, out_dims_thunk)->batch_subtrace(lu.wrap_init(bwd))
A:jax._src.interpreters.batching.bwd_->_match_axes_and_sum(bwd_, axis_size, axis_name, out_dims_thunk, out_dim_dests)
A:jax._src.interpreters.batching.primitive_batchers[prim]->partial(reducer_batcher, prim, ident)
A:jax._src.interpreters.batching.(shape, dim)->next(((x.shape, d) for (x, d) in zip(args, dims) if d is not not_mapped))
A:jax._src.interpreters.batching.out->prim.bind(*args, **params)
A:jax._src.interpreters.batching.ndim->max((np.ndim(x) for x in args))
A:jax._src.interpreters.batching.axes->tuple(np.where(np.less(axes, bdim.stacked_axis), axes, np.add(axes, 1)))
A:jax._src.interpreters.batching.bdim_out->out_axis(axes, bdim.stacked_axis)
A:jax._src.interpreters.batching.operand->_mask_one_ragged_axis(operand, ident, this_axis_spec)
A:jax._src.interpreters.batching.this_axis_spec->RaggedAxis(axis_spec.stacked_axis, ((ragged_axis, segment_lengths),))
A:jax._src.interpreters.batching.positions->jax.lax.broadcasted_iota('int32', operand.shape, ragged_axis)
A:jax._src.interpreters.batching.lengths->jax.lax.convert_element_type(segment_lengths, 'int32')
A:jax._src.interpreters.batching.limits->jax.lax.broadcast_in_dim(lengths, operand.shape, [axis_spec.stacked_axis])
A:jax._src.interpreters.batching.dst->canonicalize_axis(dst, operand.ndim)
A:jax._src.interpreters.batching.broadcast_dims->tuple(np.delete(np.arange(len(shape)), axis))
A:jax._src.interpreters.batching.x->moveaxis(x, bdx, bdy)
A:jax._src.interpreters.batching._->jax._src.core.get_aval(x)
A:jax._src.interpreters.batching.y->broadcast(y, x.shape[bdx], bdx)
jax._src.batching.BatchTrace(self,*args,axis_name,spmd_axis_name=None)
jax._src.batching.BatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax._src.batching.BatchTrace.get_frame(self,vals,dims)->core.AxisEnvFrame
jax._src.batching.BatchTrace.get_primitive_batcher(self,primitive,frame)
jax._src.batching.BatchTrace.lift(self,val)
jax._src.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.batching.BatchTrace.post_process_custom_jvp_call(self,out_tracers,jvp_was_run)
jax._src.batching.BatchTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.batching.BatchTrace.post_process_custom_vjp_call_fwd(self,out_tracers,out_trees)
jax._src.batching.BatchTrace.post_process_map(self,call_primitive,out_tracers,params)
jax._src.batching.BatchTrace.process_call(self,call_primitive,f,tracers,params)
jax._src.batching.BatchTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.batching.BatchTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,*,out_trees,symbolic_zeros)
jax._src.batching.BatchTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax._src.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax._src.batching.BatchTrace.pure(self,val)
jax._src.batching.BatchTrace.sublift(self,val)
jax._src.batching.BatchTracer(self,trace,val,batch_dim:NotMapped|int|RaggedAxis,source_info:source_info_util.SourceInfo|None=None)
jax._src.batching.BatchTracer._contents(self)
jax._src.batching.BatchTracer._origin_msg(self)
jax._src.batching.BatchTracer.aval(self)
jax._src.batching.BatchTracer.full_lower(self)
jax._src.batching.BatchTracer.get_referent(self)
jax._src.batching.IndexedAxisSize
jax._src.batching.IndexedAxisSize.__repr__(self)->str
jax._src.batching.Jumble
jax._src.batching.JumbleAxis
jax._src.batching.JumbleTy
jax._src.batching.JumbleTy.__repr__(self)->str
jax._src.batching.RaggedAxis
jax._src.batching.RaggedAxis.move_stacked_axis(self:RaggedAxis,dst:int)->RaggedAxis
jax._src.batching.RaggedAxis.size(self)
jax._src.batching.ZeroIfMapped
jax._src.batching._batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,spmd_axis_name,main_type)
jax._src.batching._batch_jaxpr2(closed_jaxpr:core.ClosedJaxpr,axis_size:core.AxisSize,in_axes:tuple[int|NotMapped|RaggedAxis,...],axis_name:AxisName,spmd_axis_name:AxisName,main_type:type[BatchTrace])->tuple[core.ClosedJaxpr, tuple[int | NotMapped, ...]]
jax._src.batching._batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,spmd_axis_name,main_type)
jax._src.batching._batch_jaxpr_outer(axis_name,spmd_axis_name,axis_size,in_dims,main_type,*in_vals)
jax._src.batching._batch_outer(axis_name,axis_size,in_dims,main_type,spmd_axis_name,*in_vals)
jax._src.batching._handle_scalar_broadcasting(nd,x,d)
jax._src.batching._jumble_flatten(jumble)
jax._src.batching._jumble_result(axis_size,stacked_axis,ragged_axes,x)
jax._src.batching._jumble_unflatten(aval,x)
jax._src.batching._locate_value(key,in_vals,out_vals)
jax._src.batching._main_trace_for_axis_names(main_trace:core.MainTrace,axis_name:Iterable[AxisName])->bool
jax._src.batching._mask_one_ragged_axis(operand:Array,ident,axis_spec:RaggedAxis)->Array
jax._src.batching._match_axes_and_sum(axis_size,axis_name,out_dims_thunk,out_dim_dests,*in_vals)
jax._src.batching._match_axes_jaxpr(axis_size,out_axes_dest,out_axes,main,in_axes,*in_vals)
jax._src.batching._matchaxis_symbolic_zeros(axis_name,sz,name,src,dst,x,sum_match=False)
jax._src.batching._merge_bdims(x,y)
jax._src.batching._sorted_ragged_axis(stacked_axis,ragged_axes)
jax._src.batching.add_batched(batched_args,batch_dims)
jax._src.batching.batch(fun:lu.WrappedFun,axis_name:AxisName,axis_size,in_dims,out_dim_dests,main_type:type[BatchTrace]=BatchTrace,spmd_axis_name:tuple[AxisName,...]|None=None)->lu.WrappedFun
jax._src.batching.batch_custom_jvp_subtrace(main,in_dims,*in_vals)
jax._src.batching.batch_custom_vjp_bwd(bwd,axis_name,axis_size,in_dims,out_dim_dests,main_type,spmd_axis_name)
jax._src.batching.batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,spmd_axis_name,main_type)
jax._src.batching.batch_jaxpr2(closed_jaxpr:core.ClosedJaxpr,axis_size:core.AxisSize,in_axes:tuple[int|NotMapped|RaggedAxis,...],axis_name:AxisName,spmd_axis_name:AxisName,main_type:type[BatchTrace])->tuple[core.ClosedJaxpr, tuple[int | NotMapped | RaggedAxis, ...]]
jax._src.batching.batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,spmd_axis_name,main_type)
jax._src.batching.batch_subtrace(main,in_dims,*in_vals)
jax._src.batching.bdim_as_shape(bdim:int|RaggedAxis,data_shape:core.Shape)->core.Shape
jax._src.batching.bdim_at_front(x,bdim,size)
jax._src.batching.broadcast(x,sz,axis)
jax._src.batching.broadcast_batcher(prim,args,dims,**params)
jax._src.batching.defbroadcasting(prim)
jax._src.batching.defreducer(prim,ident)
jax._src.batching.defvectorized(prim)
jax._src.batching.flatten_fun_for_vmap(in_tree,*args_flat)
jax._src.batching.from_elt(trace:BatchTrace,axis_size:AxisSize,x:Elt,spec:MapSpec)->Vmappable
jax._src.batching.handle_ragged(in_avals:list[core.AbstractValue],dim:RaggedAxis,aval:core.ShapedArray)->tuple[int, core.ShapedArray]
jax._src.batching.indirectify_ragged_axes(dims)
jax._src.batching.indirectify_ragged_axes_against_inputs_outputs(dims,in_vals,out_vals)
jax._src.batching.is_vmappable(x:Any)->bool
jax._src.batching.make_batch_axis(ndim:int,stacked_axis:int,ragged_axes:list[tuple[int,Array|core.Var]])->int | RaggedAxis
jax._src.batching.make_iota(axis_size:AxisSize)->Array
jax._src.batching.mask_ragged_axes(operand:Array,ident,axis_spec:RaggedAxis)->Array
jax._src.batching.matchaxis(axis_name,sz,src,dst,x,sum_match=False)
jax._src.batching.move_stacked_axis(operand,bdim,dst)
jax._src.batching.reducer_batcher(prim,ident,batched_args,batch_dims,axes,**params)
jax._src.batching.register_vmappable(data_type:type,spec_type:type,axis_size_type:type,to_elt:Callable,from_elt:Callable,make_iota:Callable|None)
jax._src.batching.resolve_ragged_axes(vals,dims)
jax._src.batching.resolve_ragged_axes_against_inputs_outputs(in_vals,out_vals,dims)
jax._src.batching.shape_as_bdim(stacked_axis:int,data_shape:core.Shape)->int | RaggedAxis
jax._src.batching.to_elt(trace:Trace,get_idx:GetIdx,x:Vmappable,spec:MapSpec)->Elt
jax._src.batching.transpose_ragged_axes(dim:RaggedAxis,perm:tuple[int,...])->RaggedAxis
jax._src.batching.unregister_vmappable(data_type:type)->None
jax._src.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax._src.batching.vtile(f_flat:lu.WrappedFun,in_axes_flat:tuple[int|None,...],out_axes_flat:tuple[int|None,...],tile_size:int|None,axis_name:AxisName,main_type:type[BatchTrace]=BatchTrace)
jax._src.batching.zeros_like_batched(batched_args,batch_dims)
jax._src.interpreters.batching.BatchTrace(self,*args,axis_name,spmd_axis_name=None)
jax._src.interpreters.batching.BatchTrace.__init__(self,*args,axis_name,spmd_axis_name=None)
jax._src.interpreters.batching.BatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax._src.interpreters.batching.BatchTrace.get_frame(self,vals,dims)->core.AxisEnvFrame
jax._src.interpreters.batching.BatchTrace.get_primitive_batcher(self,primitive,frame)
jax._src.interpreters.batching.BatchTrace.lift(self,val)
jax._src.interpreters.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.interpreters.batching.BatchTrace.post_process_custom_jvp_call(self,out_tracers,jvp_was_run)
jax._src.interpreters.batching.BatchTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.interpreters.batching.BatchTrace.post_process_custom_vjp_call_fwd(self,out_tracers,out_trees)
jax._src.interpreters.batching.BatchTrace.post_process_map(self,call_primitive,out_tracers,params)
jax._src.interpreters.batching.BatchTrace.process_call(self,call_primitive,f,tracers,params)
jax._src.interpreters.batching.BatchTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.interpreters.batching.BatchTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,*,out_trees,symbolic_zeros)
jax._src.interpreters.batching.BatchTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax._src.interpreters.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax._src.interpreters.batching.BatchTrace.pure(self,val)
jax._src.interpreters.batching.BatchTrace.sublift(self,val)
jax._src.interpreters.batching.BatchTracer(self,trace,val,batch_dim:NotMapped|int|RaggedAxis,source_info:source_info_util.SourceInfo|None=None)
jax._src.interpreters.batching.BatchTracer.__init__(self,trace,val,batch_dim:NotMapped|int|RaggedAxis,source_info:source_info_util.SourceInfo|None=None)
jax._src.interpreters.batching.BatchTracer._contents(self)
jax._src.interpreters.batching.BatchTracer._origin_msg(self)
jax._src.interpreters.batching.BatchTracer.aval(self)
jax._src.interpreters.batching.BatchTracer.full_lower(self)
jax._src.interpreters.batching.BatchTracer.get_referent(self)
jax._src.interpreters.batching.IndexedAxisSize
jax._src.interpreters.batching.IndexedAxisSize.__repr__(self)->str
jax._src.interpreters.batching.Jumble
jax._src.interpreters.batching.JumbleAxis
jax._src.interpreters.batching.JumbleTy
jax._src.interpreters.batching.JumbleTy.__repr__(self)->str
jax._src.interpreters.batching.RaggedAxis
jax._src.interpreters.batching.RaggedAxis.move_stacked_axis(self:RaggedAxis,dst:int)->RaggedAxis
jax._src.interpreters.batching.RaggedAxis.size(self)
jax._src.interpreters.batching.ZeroIfMapped
jax._src.interpreters.batching._batch_inner(axis_size,out_dim_dests,main,in_dims,*in_vals)
jax._src.interpreters.batching._batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,spmd_axis_name,main_type)
jax._src.interpreters.batching._batch_jaxpr2(closed_jaxpr:core.ClosedJaxpr,axis_size:core.AxisSize,in_axes:tuple[int|NotMapped|RaggedAxis,...],axis_name:AxisName,spmd_axis_name:AxisName,main_type:type[BatchTrace])->tuple[core.ClosedJaxpr, tuple[int | NotMapped, ...]]
jax._src.interpreters.batching._batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,spmd_axis_name,main_type)
jax._src.interpreters.batching._batch_jaxpr_inner(axis_size,main,in_axes,*in_vals)
jax._src.interpreters.batching._batch_jaxpr_outer(axis_name,spmd_axis_name,axis_size,in_dims,main_type,*in_vals)
jax._src.interpreters.batching._batch_outer(axis_name,axis_size,in_dims,main_type,spmd_axis_name,*in_vals)
jax._src.interpreters.batching._handle_scalar_broadcasting(nd,x,d)
jax._src.interpreters.batching._jumble_flatten(jumble)
jax._src.interpreters.batching._jumble_result(axis_size,stacked_axis,ragged_axes,x)
jax._src.interpreters.batching._jumble_unflatten(aval,x)
jax._src.interpreters.batching._locate_value(key,in_vals,out_vals)
jax._src.interpreters.batching._main_trace_for_axis_names(main_trace:core.MainTrace,axis_name:Iterable[AxisName])->bool
jax._src.interpreters.batching._mask_one_ragged_axis(operand:Array,ident,axis_spec:RaggedAxis)->Array
jax._src.interpreters.batching._match_axes_and_sum(axis_size,axis_name,out_dims_thunk,out_dim_dests,*in_vals)
jax._src.interpreters.batching._match_axes_jaxpr(axis_size,out_axes_dest,out_axes,main,in_axes,*in_vals)
jax._src.interpreters.batching._matchaxis_symbolic_zeros(axis_name,sz,name,src,dst,x,sum_match=False)
jax._src.interpreters.batching._merge_bdims(x,y)
jax._src.interpreters.batching._sorted_ragged_axis(stacked_axis,ragged_axes)
jax._src.interpreters.batching._update_annotation(f:lu.WrappedFun,orig_type:core.InputType|None,axis_size:core.AxisSize,axis_name:AxisName,explicit_in_dims:Sequence[int|RaggedAxis|None],segment_lens:Sequence[Array])->lu.WrappedFun
jax._src.interpreters.batching.add_batched(batched_args,batch_dims)
jax._src.interpreters.batching.batch(fun:lu.WrappedFun,axis_name:AxisName,axis_size,in_dims,out_dim_dests,main_type:type[BatchTrace]=BatchTrace,spmd_axis_name:tuple[AxisName,...]|None=None)->lu.WrappedFun
jax._src.interpreters.batching.batch_custom_jvp_subtrace(main,in_dims,*in_vals)
jax._src.interpreters.batching.batch_custom_vjp_bwd(bwd,axis_name,axis_size,in_dims,out_dim_dests,main_type,spmd_axis_name)
jax._src.interpreters.batching.batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,spmd_axis_name,main_type)
jax._src.interpreters.batching.batch_jaxpr2(closed_jaxpr:core.ClosedJaxpr,axis_size:core.AxisSize,in_axes:tuple[int|NotMapped|RaggedAxis,...],axis_name:AxisName,spmd_axis_name:AxisName,main_type:type[BatchTrace])->tuple[core.ClosedJaxpr, tuple[int | NotMapped | RaggedAxis, ...]]
jax._src.interpreters.batching.batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,spmd_axis_name,main_type)
jax._src.interpreters.batching.batch_subtrace(main,in_dims,*in_vals)
jax._src.interpreters.batching.bdim_as_shape(bdim:int|RaggedAxis,data_shape:core.Shape)->core.Shape
jax._src.interpreters.batching.bdim_at_front(x,bdim,size)
jax._src.interpreters.batching.broadcast(x,sz,axis)
jax._src.interpreters.batching.broadcast_batcher(prim,args,dims,**params)
jax._src.interpreters.batching.defbroadcasting(prim)
jax._src.interpreters.batching.defreducer(prim,ident)
jax._src.interpreters.batching.defvectorized(prim)
jax._src.interpreters.batching.flatten_fun_for_vmap(in_tree,*args_flat)
jax._src.interpreters.batching.from_elt(trace:BatchTrace,axis_size:AxisSize,x:Elt,spec:MapSpec)->Vmappable
jax._src.interpreters.batching.handle_ragged(in_avals:list[core.AbstractValue],dim:RaggedAxis,aval:core.ShapedArray)->tuple[int, core.ShapedArray]
jax._src.interpreters.batching.indirectify_ragged_axes(dims)
jax._src.interpreters.batching.indirectify_ragged_axes_against_inputs_outputs(dims,in_vals,out_vals)
jax._src.interpreters.batching.is_vmappable(x:Any)->bool
jax._src.interpreters.batching.make_batch_axis(ndim:int,stacked_axis:int,ragged_axes:list[tuple[int,Array|core.Var]])->int | RaggedAxis
jax._src.interpreters.batching.make_iota(axis_size:AxisSize)->Array
jax._src.interpreters.batching.mask_ragged_axes(operand:Array,ident,axis_spec:RaggedAxis)->Array
jax._src.interpreters.batching.matchaxis(axis_name,sz,src,dst,x,sum_match=False)
jax._src.interpreters.batching.move_stacked_axis(operand,bdim,dst)
jax._src.interpreters.batching.reducer_batcher(prim,ident,batched_args,batch_dims,axes,**params)
jax._src.interpreters.batching.register_vmappable(data_type:type,spec_type:type,axis_size_type:type,to_elt:Callable,from_elt:Callable,make_iota:Callable|None)
jax._src.interpreters.batching.resolve_ragged_axes(vals,dims)
jax._src.interpreters.batching.resolve_ragged_axes_against_inputs_outputs(in_vals,out_vals,dims)
jax._src.interpreters.batching.shape_as_bdim(stacked_axis:int,data_shape:core.Shape)->int | RaggedAxis
jax._src.interpreters.batching.to_elt(trace:Trace,get_idx:GetIdx,x:Vmappable,spec:MapSpec)->Elt
jax._src.interpreters.batching.transpose_ragged_axes(dim:RaggedAxis,perm:tuple[int,...])->RaggedAxis
jax._src.interpreters.batching.unregister_vmappable(data_type:type)->None
jax._src.interpreters.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax._src.interpreters.batching.vtile(f_flat:lu.WrappedFun,in_axes_flat:tuple[int|None,...],out_axes_flat:tuple[int|None,...],tile_size:int|None,axis_name:AxisName,main_type:type[BatchTrace]=BatchTrace)
jax._src.interpreters.batching.zeros_like_batched(batched_args,batch_dims)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/xla.py----------------------------------------
A:jax._src.interpreters.xla._scalar_types->jax._src.dtypes.python_scalar_dtypes.keys()
A:jax._src.interpreters.xla.aval->jax._src.core.physical_aval(aval)
A:jax._src.interpreters.xla.proto->jax._src.lib.xla_client.OpSharding()
A:jax._src.interpreters.xla.proto.tile_assignment_dimensions->list(sharding)
A:jax._src.interpreters.xla.proto.tile_assignment_devices->list(range(np.prod(sharding)))
A:jax._src.interpreters.xla.typ->type(x)
A:jax._src.interpreters.xla.handler->canonicalize_dtype_handlers.get(typ)
A:jax._src.interpreters.xla.aval_fn->pytype_aval_mappings.get(typ)
A:jax._src.interpreters.xla.dtype->numpy.dtype(x)
A:jax._src.interpreters.xla.pytype_aval_mappings[core.DArray]->operator.attrgetter('_aval')
A:jax._src.interpreters.xla.c->jax._src.lib.xla_client.XlaBuilder(f'primitive_computation_{prim.name}')
A:jax._src.interpreters.xla.counts->itertools.count()
A:jax._src.interpreters.xla.ctx->TranslationContext(builder=c, platform=platform, axis_env=axis_env, name_stack=source_info_util.new_name_stack())
A:jax._src.interpreters.xla.ans->f(ctx.builder, *args, **kw)
A:jax._src.interpreters.xla.num_elements->len(c.get_shape(ans).tuple_shapes())
A:jax._src.interpreters.xla._backend_specific_translations->defaultdict(dict)
A:jax._src.interpreters.xla.wrapped->self._wrap_fn(key, value)
A:jax._src.interpreters.xla.translations->_TranslationRuleAdapter([_translations], _wrap_old_translation)
A:jax._src.interpreters.xla.retself[key]->_TranslationRuleAdapter(translation_tables, _wrap_old_translation)
A:jax._src.interpreters.xla.backend_specific_translations->_BackendSpecificTranslationsAdapter()
jax._src.interpreters.xla.TranslationContext
jax._src.interpreters.xla.TranslationContext.replace(self,**kw)
jax._src.interpreters.xla._BackendSpecificTranslationsAdapter(defaultdict)
jax._src.interpreters.xla._BackendSpecificTranslationsAdapter.__missing__(self,key)
jax._src.interpreters.xla._TranslationRuleAdapter(self,translations,wrap_fn:Callable[[core.Primitive,Callable],TranslationRule])
jax._src.interpreters.xla._TranslationRuleAdapter.__init__(self,translations,wrap_fn:Callable[[core.Primitive,Callable],TranslationRule])
jax._src.interpreters.xla._TranslationRuleAdapter.__setitem__(self,key:core.Primitive,value:Callable)
jax._src.interpreters.xla._canonicalize_masked_array_dtype(x)
jax._src.interpreters.xla._canonicalize_ndarray_dtype(x)
jax._src.interpreters.xla._canonicalize_python_scalar_dtype(typ,x)
jax._src.interpreters.xla._make_abstract_python_scalar(typ,val)
jax._src.interpreters.xla._make_array_shape(aval:ShapedArray)->Sequence[xc.Shape]
jax._src.interpreters.xla._make_shaped_array_for_numpy_array(x:np.ndarray)->ShapedArray
jax._src.interpreters.xla._make_shaped_array_for_numpy_scalar(x:np.generic)->ShapedArray
jax._src.interpreters.xla._wrap_old_translation(prim:core.Primitive,f:Callable)->TranslationRule
jax._src.interpreters.xla.abstractify(x)->Any
jax._src.interpreters.xla.aval_to_xla_shapes(aval:core.AbstractValue)->Sequence[xc.Shape]
jax._src.interpreters.xla.canonicalize_dtype(x)
jax._src.interpreters.xla.identity(x)
jax._src.interpreters.xla.parameter(builder,num,shape,name=None,replicated=None)
jax._src.interpreters.xla.primitive_subcomputation(platform:str,axis_env:'AxisEnv',prim:core.Primitive,avals_in:Sequence[core.AbstractValue],avals_out:Sequence[core.AbstractValue],**params)
jax._src.interpreters.xla.register_initial_style_primitive(prim:core.Primitive)
jax._src.interpreters.xla.register_translation(prim:core.Primitive,rule:TranslationRule,*,platform:Optional[str]=None)->None
jax._src.interpreters.xla.sharding_to_proto(sharding:SpatialSharding)
jax._src.interpreters.xla.tuple_sharding_proto(elems)
jax._src.interpreters.xla.xla_destructure(c,ans)
jax._src.xla.TranslationContext
jax._src.xla.TranslationContext.replace(self,**kw)
jax._src.xla._BackendSpecificTranslationsAdapter(defaultdict)
jax._src.xla._BackendSpecificTranslationsAdapter.__missing__(self,key)
jax._src.xla._TranslationRuleAdapter(self,translations,wrap_fn:Callable[[core.Primitive,Callable],TranslationRule])
jax._src.xla._TranslationRuleAdapter.__setitem__(self,key:core.Primitive,value:Callable)
jax._src.xla._canonicalize_masked_array_dtype(x)
jax._src.xla._canonicalize_ndarray_dtype(x)
jax._src.xla._canonicalize_python_scalar_dtype(typ,x)
jax._src.xla._make_abstract_python_scalar(typ,val)
jax._src.xla._make_array_shape(aval:ShapedArray)->Sequence[xc.Shape]
jax._src.xla._wrap_old_translation(prim:core.Primitive,f:Callable)->TranslationRule
jax._src.xla.abstractify(x)->Any
jax._src.xla.aval_to_xla_shapes(aval:core.AbstractValue)->Sequence[xc.Shape]
jax._src.xla.canonicalize_dtype(x)
jax._src.xla.identity(x)
jax._src.xla.parameter(builder,num,shape,name=None,replicated=None)
jax._src.xla.primitive_subcomputation(platform:str,axis_env:'AxisEnv',prim:core.Primitive,avals_in:Sequence[core.AbstractValue],avals_out:Sequence[core.AbstractValue],**params)
jax._src.xla.register_initial_style_primitive(prim:core.Primitive)
jax._src.xla.register_translation(prim:core.Primitive,rule:TranslationRule,*,platform:Optional[str]=None)->None
jax._src.xla.xla_destructure(c,ans)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py----------------------------------------
A:jax._src.interpreters.pxla.logger->logging.getLogger(__name__)
A:jax._src.interpreters.pxla.arg->jax._src.interpreters.xla.canonicalize_dtype(arg)
A:jax._src.interpreters.pxla.zeros->numpy.zeros((), dtype=np.dtype(np.bool_))
A:jax._src.interpreters.pxla.aval->jax._src.core.mapped_aval(aval.shape[axis_idx], axis_idx, aval)
A:jax._src.interpreters.pxla.x->numpy.zeros(x.shape, dtype=np.dtype(bool))
A:jax._src.interpreters.pxla.abstract_args->unsafe_map(xla.abstractify, args)
A:jax._src.interpreters.pxla.(compiled_fun, fingerprint)->parallel_callable(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, is_explicit_global_axis_size, *abstract_args)
A:jax._src.interpreters.pxla.compiled_fun->xla_pmap_impl_lazy(fun, *args, **params)
A:jax._src.interpreters.pxla.emap_info->EmapInfo(backend, devices)
A:jax._src.interpreters.pxla.t->self.main.with_cur_sublevel()
A:jax._src.interpreters.pxla.ans->jax._src.interpreters.batching.vtile(fun, tuple((a.get(name, None) for a in in_axes)), tuple((a.get(name, None) for a in out_axes)), tile_size=size, axis_name=name, main_type=SPMDBatchTrace).call_wrapped(*in_tracers)
A:jax._src.interpreters.pxla.out_tracers->map(t.full_raise, ans)
A:jax._src.interpreters.pxla.(outvals, out_axes_src)->unzip2(((t.val, t.shard_axes) for t in out_tracers))
A:jax._src.interpreters.pxla.out_axes->out_axes_thunk()
A:jax._src.interpreters.pxla.out->jax.pmap(lambda _, x: x, in_axes=(0, out_axis_src.get(axis_name)), out_axes=out_axis, devices=None if devices is None else list(devices), backend=backend, donate_argnums=donate_argnums_)(np.arange(axis_size), outval)
A:jax._src.interpreters.pxla.in_axes->tuple((arg_axis[i] for arg_axis in all_axes))
A:jax._src.interpreters.pxla.f->HashableFunction(lambda *args: primitive.bind(*args, **params), (primitive, tuple(params.items())))
A:jax._src.interpreters.pxla.FakePrimitive->namedtuple('FakePrimitive', ['multiple_results', 'bind'])
A:jax._src.interpreters.pxla.(vals, shard_axes)->unzip2([(t.val, t.shard_axes) for t in tracers])
A:jax._src.interpreters.pxla.names->tuple((f.name for f in core.thread_local_state.trace_state.axis_env if f.main_trace is self.main))
A:jax._src.interpreters.pxla.all_axes->tuple((_map_schedule(map(s.get, names)) for s in shard_axes))
A:jax._src.interpreters.pxla.(f_mapped, out_shard_axes)->_multi_pmap(f, info, names, all_axes)
A:jax._src.interpreters.pxla.outvals->f_mapped(*vals)
A:jax._src.interpreters.pxla.bind->HashableFunction(lambda _: jax.lax.axis_index(frame.name), (jax.lax.axis_index, frame.name))
A:jax._src.interpreters.pxla.fake_primitive->FakePrimitive(multiple_results=False, bind=bind)
A:jax._src.interpreters.pxla.in_tracers->map(partial(MapTracer, t), vals, shard_axes)
A:jax._src.interpreters.pxla.(out, outaxes)->unzip2((_match_annot(axis_name, axis_size, v, s, dst) for (v, s, dst) in zip(out, outaxes, out_axes_thunk())))
A:jax._src.interpreters.pxla.range->jax.lax.iota(np.int32, frame.size)
A:jax._src.interpreters.pxla.dummy_tracer->MapTracer(self, range, {frame.name: 0})
A:jax._src.interpreters.pxla.mapped_axes_->set(mapped_axes)
A:jax._src.interpreters.pxla.shard_axis_out->_moveaxis(np.ndim(val), shard_axis_src, src, dst)
A:jax._src.interpreters.pxla.src->_moveaxis(np.ndim(val), shard_axis_src, src, dst).pop(axis_name, None)
A:jax._src.interpreters.pxla.dst->_annot_to_flat(np.ndim(val) + (src is None), shard_axis_out.values(), dst_annotation)
A:jax._src.interpreters.pxla.outval->jax._src.interpreters.batching.broadcast(val, axis_size, dst)
A:jax._src.interpreters.pxla.name->lst.pop(src)
A:jax._src.interpreters.pxla.shard_axes->dict(self.shard_axes)
A:jax._src.interpreters.pxla.pmap_computation->lower_parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, is_explicit_global_axis_size, avals, lowering_parameters=mlir.LoweringParameters())
A:jax._src.interpreters.pxla.pmap_executable->lower_parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, is_explicit_global_axis_size, avals, lowering_parameters=mlir.LoweringParameters()).compile()
A:jax._src.interpreters.pxla.jaxpr_replicas->jax._src.dispatch.jaxpr_replicas(jaxpr)
A:jax._src.interpreters.pxla.sharded_avals->tuple((shard_aval(pci.axis_size, axis, aval) if axis is not None else aval for (axis, aval) in safe_zip(pci.in_axes, pci.avals)))
A:jax._src.interpreters.pxla.(jaxpr, out_sharded_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_final(fun, sharded_avals, pe.debug_info_final(fun, 'pmap'))
A:jax._src.interpreters.pxla.jaxpr->jax._src.dispatch.apply_outfeed_rewriter(jaxpr)
A:jax._src.interpreters.pxla.replicas->find_replicas(jaxpr, pci.axis_size, pci.global_axis_size)
A:jax._src.interpreters.pxla.shards->ShardInfo(sharded_avals, out_sharded_avals, sharded_avals, num_local_shards, num_global_shards)
A:jax._src.interpreters.pxla.backend->jax._src.xla_bridge.get_device_backend(mesh.devices.flat[0])
A:jax._src.interpreters.pxla.pci->ParallelCallableInfo(name, backend, axis_name, axis_size, global_axis_size, devices, in_axes, out_axes_thunk, avals)
A:jax._src.interpreters.pxla.(jaxpr, consts, replicas, shards)->stage_parallel_callable(pci, fun)
A:jax._src.interpreters.pxla.axis_env->jax._src.sharding_impls.AxisEnv(nreps=mesh.size, names=tuple(global_axis_sizes.keys()), sizes=tuple(global_axis_sizes.values()))
A:jax._src.interpreters.pxla.name_stack->jax._src.source_info_util.new_name_stack(wrap_name(fun_name, api_name))
A:jax._src.interpreters.pxla.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.interpreters.pxla.tuple_args->jax._src.dispatch.should_tuple_args(len(in_jaxpr_avals), backend.platform)
A:jax._src.interpreters.pxla.ordered_effects->list(effects.ordered_effects.filter_in(closed_jaxpr.effects))
A:jax._src.interpreters.pxla.unordered_effects->list(effects.ordered_effects.filter_not_in(closed_jaxpr.effects))
A:jax._src.interpreters.pxla.lowering_result->jax._src.interpreters.mlir.lower_jaxpr_to_module(module_name, closed_jaxpr, ordered_effects=ordered_effects, backend_or_name=backend, platforms=lowering_parameters.platforms or (backend.platform,), axis_context=axis_ctx, name_stack=name_stack, donated_args=donated_invars, replicated_args=replicated_args, arg_shardings=in_partitions, result_shardings=out_partitions, arg_names=jaxpr.debug_info and jaxpr.debug_info.arg_names, result_names=jaxpr.debug_info and jaxpr.debug_info.result_paths, num_replicas=num_replicas, num_partitions=num_partitions, lowering_parameters=lowering_parameters)
A:jax._src.interpreters.pxla.executable->UnloadedMeshExecutable.from_hlo(self._name, self._hlo, **self.compile_args, compiler_options=compiler_options)
A:jax._src.interpreters.pxla.handle_outs->global_avals_to_results_handler(self.output_avals, self.output_shardings, self.committed, self.are_out_shardings_from_xla)
A:jax._src.interpreters.pxla.handle_args->InputsHandler(self.xla_executable.local_devices(), self.input_shardings, input_indices)
A:jax._src.interpreters.pxla.execute_fun->ParallelCallableInfo(name, backend, axis_name, axis_size, global_axis_size, devices, in_axes, out_axes_thunk, avals).backend.compile_replicated(is_trivial=False, name=pci.name, computation=hlo, compile_options=compile_options, host_callbacks=host_callbacks, has_unordered_effects=has_unordered_effects, ordered_effects=ordered_effects, in_avals=pci.avals, in_indices=input_indices, in_shardings=in_shardings, kept_var_idx=set(range(len(pci.avals))), out_handler=handle_outs)
A:jax._src.interpreters.pxla.fingerprint->getattr(self.compiled, 'fingerprint', None)
A:jax._src.interpreters.pxla.local_devices_str->', '.join(map(str, pci.local_devices))
A:jax._src.interpreters.pxla.compile_options->jax._src.compiler.get_compile_options(num_replicas=num_replicas, num_partitions=num_partitions, device_assignment=xla_device_assignment, use_spmd_partitioning=spmd_lowering, use_auto_spmd_partitioning=auto_spmd_lowering, env_options_overrides=compiler_options, fdo_profile=fdo_profile, detailed_logging=compiler.use_detailed_logging(computation))
A:jax._src.interpreters.pxla.process_index->jax._src.xla_bridge.process_index(pci.backend)
A:jax._src.interpreters.pxla.local_device_assignment->numpy.array([d for d in device_assignment.flat if d.process_index == process_index])
A:jax._src.interpreters.pxla.in_shardings->tuple((gs if is_unspecified(i) else i for i in in_shardings))
A:jax._src.interpreters.pxla.out_shardings->_get_pmap_sharding(local_device_assignment, out_specs)
A:jax._src.interpreters.pxla.compiled->jax._src.compiler.compile_or_get_cached(pci.backend, hlo, device_assignment, compile_options, host_callbacks)
A:jax._src.interpreters.pxla.self._unsafe_call->self.build_unsafe_call()
A:jax._src.interpreters.pxla.arg_avals->map(xla.abstractify, kept_args)
A:jax._src.interpreters.pxla.self.handler->partial(shard_args, local_devices, input_indices, in_shardings)
A:jax._src.interpreters.pxla.self.has_unordered_effects->bool(unordered_effects)
A:jax._src.interpreters.pxla.self._local_devices->self.xla_executable.local_devices()
A:jax._src.interpreters.pxla.input_bufs->self._add_tokens_to_inputs(input_bufs)
A:jax._src.interpreters.pxla.results->self.xla_executable.execute_sharded(input_bufs)
A:jax._src.interpreters.pxla.result_token_bufs->self.xla_executable.execute_sharded(input_bufs).disassemble_prefix_into_single_device_arrays(len(self.ordered_effects))
A:jax._src.interpreters.pxla.sharded_runtime_token->self.xla_executable.execute_sharded(input_bufs).consume_token()
A:jax._src.interpreters.pxla.out_arrays->self.xla_executable.execute_sharded(input_bufs).disassemble_into_single_device_arrays()
A:jax._src.interpreters.pxla.xla_pmap_p->jax._src.core.MapPrimitive('xla_pmap')
A:jax._src.interpreters.pxla.(donated_invars_known, _)->partition_list(unks_in, params_known['donated_invars'])
A:jax._src.interpreters.pxla.(in_axes_known, _)->partition_list(unks_in, params_known['in_axes'])
A:jax._src.interpreters.pxla.(_, out_axes_known)->partition_list(kept_outs_known, params_known['out_axes'])
A:jax._src.interpreters.pxla.new_params_known->dict(params_known, in_axes=tuple(in_axes_known), out_axes=tuple(out_axes_known), donated_invars=tuple(donated_invars_known))
A:jax._src.interpreters.pxla.(_, donated_invars_staged)->partition_list(inst_in, params_staged['donated_invars'])
A:jax._src.interpreters.pxla.(_, in_axes_staged)->partition_list(inst_in, params_staged['in_axes'])
A:jax._src.interpreters.pxla.(_, out_axes_staged)->partition_list(kept_outs_staged, params_staged['out_axes'])
A:jax._src.interpreters.pxla.new_params_staged->dict(params_staged, in_axes=tuple(in_axes_staged), out_axes=tuple(out_axes_staged), donated_invars=tuple(donated_invars_staged))
A:jax._src.interpreters.pxla.(new_jaxpr, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(eqn.params['call_jaxpr'], used_outputs)
A:jax._src.interpreters.pxla.(_, donated_invars)->partition_list(used_inputs, eqn.params['donated_invars'])
A:jax._src.interpreters.pxla.(_, in_axes)->partition_list(used_inputs, eqn.params['in_axes'])
A:jax._src.interpreters.pxla.(_, out_axes)->partition_list(used_outputs, eqn.params['out_axes'])
A:jax._src.interpreters.pxla.new_params->dict(eqn.params, call_jaxpr=new_jaxpr, donated_invars=tuple(donated_invars), in_axes=tuple(in_axes), out_axes=tuple(out_axes))
A:jax._src.interpreters.pxla.new_eqn->jax._src.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax._src.interpreters.pxla.pe.partial_eval_jaxpr_custom_rules[xla_pmap_p]->partial(pe.call_partial_eval_custom_rule, 'call_jaxpr', _pmap_partial_eval_custom_params_updater, res_aval=_pmap_partial_eval_custom_res_maker)
A:jax._src.interpreters.pxla.ad.primitive_transposes[xla_pmap_p]->partial(ad.map_transpose, xla_pmap_p)
A:jax._src.interpreters.pxla.new_jaxpr->jax._src.core.subst_axis_names_jaxpr(params['call_jaxpr'], shadowed_subst)
A:jax._src.interpreters.pxla.div->jax._src.interpreters.mlir.ir_constant(np.array(axis_env.nreps // math.prod(axis_env.sizes), np.uint32))
A:jax._src.interpreters.pxla.mod->jax._src.interpreters.mlir.ir_constant(np.array(axis_env.sizes[-1], np.uint32))
A:jax._src.interpreters.pxla.dims->list(aval.shape)
A:jax._src.interpreters.pxla.zero->jax._src.interpreters.mlir.ir_constant(np.zeros((), dtype=np.uint32))
A:jax._src.interpreters.pxla.dims_unsqueezed->list(aval.shape).copy()
A:jax._src.interpreters.pxla.mesh_axes->tuple(unsafe_map(partial(_axis_read, axis_env), name))
A:jax._src.interpreters.pxla.(trailing_size, ragged)->divmod(axis_env.nreps, math.prod(axis_env.sizes))
A:jax._src.interpreters.pxla.iota->numpy.arange(math.prod(mesh_spec)).reshape(mesh_spec)
A:jax._src.interpreters.pxla.groups->numpy.reshape(np.moveaxis(iota, mesh_axes, np.arange(len(mesh_axes))), (math.prod(np.take(mesh_spec, mesh_axes)), -1))
A:jax._src.interpreters.pxla.padded_aval->jax._src.core.mapped_aval(aval.shape[axis_idx], axis_idx, aval).update(shape=[axis_env.sizes[-1]] + dims)
A:jax._src.interpreters.pxla.padded->jax._src.interpreters.mlir.full_like_aval(ctx, 0, padded_aval)
A:jax._src.interpreters.pxla.replica_groups->jax._src.interpreters.mlir.dense_int_elements(axis_groups(axis_env, axis_env.names[-1]))
A:jax._src.interpreters.pxla.perm->list(range(1, len(dims)))
A:jax._src.interpreters.pxla.transposed_dims->list(dims)
A:jax._src.interpreters.pxla.new_env->_extend_axis_env(ctx.module_context.axis_env, axis_name, global_axis_size)
A:jax._src.interpreters.pxla.sub_ctx->ctx.module_context.replace(axis_context=sharding_impls.ReplicaAxisContext(new_env), name_stack=ctx.module_context.name_stack.extend(util.wrap_name(name, 'pmap')))
A:jax._src.interpreters.pxla.(sharded_outs, _)->jax._src.interpreters.mlir.jaxpr_subcomp(sub_ctx, call_jaxpr, mlir.TokenSet(), (), *in_nodes_sharded, dim_var_values=ctx.dim_var_values)
A:jax._src.interpreters.pxla.shape->list(aval.shape)
A:jax._src.interpreters.pxla.named_shape->dict(aval.named_shape)
A:jax._src.interpreters.pxla.fun->jax._src.interpreters.batching.vtile(fun, tuple((a.get(name, None) for a in in_axes)), tuple((a.get(name, None) for a in out_axes)), tile_size=size, axis_name=name, main_type=SPMDBatchTrace)
A:jax._src.interpreters.pxla.full_to_shard_p->jax._src.core.Primitive('full_to_shard')
A:jax._src.interpreters.pxla.mesh_shape->list(named_mesh_shape.values())
A:jax._src.interpreters.pxla.manual_axes->list(sorted(manual_axes_set, key=str))
A:jax._src.interpreters.pxla.replicated_axes->list((axis for axis in mesh.axis_names if axis not in manual_axes_set))
A:jax._src.interpreters.pxla.raw_mesh->numpy.arange(math.prod(mesh_shape)).reshape(mesh_shape)
A:jax._src.interpreters.pxla.proto->manual_proto(aval_in, manual_axes, mesh)
A:jax._src.interpreters.pxla.proto.tile_assignment_devices->list(raw_mesh.transpose(tad_perm).reshape(tad_shape).flat)
A:jax._src.interpreters.pxla.sharding_proto->mesh_sharding_specs(mesh.shape, mesh.axis_names)(aval_out, axes).sharding_proto().to_proto()
A:jax._src.interpreters.pxla.sx->jax._src.interpreters.mlir.wrap_with_sharding_op(ctx, x, aval_in, proto, unspecified_dims=unspecified_dims)
A:jax._src.interpreters.pxla.shard_to_full_p->jax._src.core.Primitive('shard_to_full')
A:jax._src.interpreters.pxla.devices->tuple(devices)
A:jax._src.interpreters.pxla._wrapped_with_lu_cache->jax._src.linear_util.cache(fn)
A:jax._src.interpreters.pxla._wrapped_with_weakref_lru_cache->weakref_lru_cache(fn)
A:jax._src.interpreters.pxla.(new_jaxpr, used_consts, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr_consts(jaxpr, used_outputs)
A:jax._src.interpreters.pxla.(jaxpr, global_out_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_final(fun_or_jaxpr, global_in_avals)
A:jax._src.interpreters.pxla.kept_var_idx->set(kept_var_idx)
A:jax._src.interpreters.pxla.(jaxpr, kept_const_idx, kept_var_idx)->prune_unused_inputs(jaxpr)
A:jax._src.interpreters.pxla.global_in_avals->tuple((a for (i, a) in enumerate(global_in_avals) if i in kept_var_idx))
A:jax._src.interpreters.pxla.donated_invars->tuple((x for (i, x) in enumerate(donated_invars) if i in kept_var_idx))
A:jax._src.interpreters.pxla.device_assignment->tuple(da)
A:jax._src.interpreters.pxla.nreps->jax._src.dispatch.jaxpr_replicas(jaxpr)
A:jax._src.interpreters.pxla.in_mlir_shardings->map(_to_logical_sharding, global_in_avals, in_shardings)
A:jax._src.interpreters.pxla.out_mlir_shardings->map(_to_logical_sharding, global_out_avals, out_shardings)
A:jax._src.interpreters.pxla.axis_ctx->jax._src.sharding_impls.ReplicaAxisContext(axis_env)
A:jax._src.interpreters.pxla.num_partitions->len(device_assignment)
A:jax._src.interpreters.pxla.unsupported_effects->jax._src.effects.shardable_ordered_effects.filter_not_in(unsupported_effects)
A:jax._src.interpreters.pxla.(closed_jaxpr, global_in_avals, global_out_avals, donated_invars, kept_var_idx, name_stack)->_trace_to_jaxpr_and_dce(fun_or_jaxpr, global_in_avals, api_name, fun_name, keep_unused, donated_invars, auto_spmd_lowering)
A:jax._src.interpreters.pxla.jaxpr_sharding->list(dispatch.jaxpr_shardings(jaxpr))
A:jax._src.interpreters.pxla.(backend, device_assignment)->_get_and_check_device_assignment(it.chain([(i, MismatchType.ARG_SHARDING, None) for i in in_shardings], [(o, MismatchType.OUT_SHARDING, None) for o in out_shardings], [(js, MismatchType.SHARDING_INSIDE_COMPUTATION, source_info) for (js, source_info) in jaxpr_sharding]), devices_from_context)
A:jax._src.interpreters.pxla.transfer_mem_kind_in_jaxpr->list(jaxpr_transfer_mem_kinds(jaxpr))
A:jax._src.interpreters.pxla.committed->bool(devices_from_context or len(device_assignment) > 1 or any((not is_unspecified(i) for i in in_shardings)) or any((not is_unspecified(js) for (js, _) in jaxpr_sharding)) or any((not is_unspecified(o) for o in out_shardings)) or transfer_mem_kind_in_jaxpr)
A:jax._src.interpreters.pxla.gs->jax._src.sharding_impls.GSPMDSharding.get_replicated(local_devices)
A:jax._src.interpreters.pxla.da_object->_create_da_object(tuple(da_object))
A:jax._src.interpreters.pxla.all_default_mem_kind->are_all_shardings_default_mem_kind(da_object, it.chain(in_shardings, out_shardings, [js for (js, _) in jaxpr_sharding], transfer_mem_kind_in_jaxpr))
A:jax._src.interpreters.pxla.semantic_in_shardings->SemanticallyEqualShardings(in_shardings)
A:jax._src.interpreters.pxla.semantic_out_shardings->SemanticallyEqualShardings(out_shardings)
A:jax._src.interpreters.pxla.(module, keepalive, host_callbacks, unordered_effects, ordered_effects, nreps, tuple_args, shape_poly_state)->_cached_lowering_to_hlo(closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings, semantic_out_shardings, da_object, donated_invars, name_stack, all_default_mem_kind, lowering_parameters=lowering_parameters)
A:jax._src.interpreters.pxla.fun_or_jaxpr->tiling_transform(fun_or_jaxpr, mesh, [get_array_mapping(i.spec) for i in in_shardings], [get_array_mapping(o.spec) for o in out_shardings])
A:jax._src.interpreters.pxla.(jaxpr, out_jaxpr_avals, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_final(fun_or_jaxpr, in_jaxpr_avals)
A:jax._src.interpreters.pxla.in_partitions->map(_to_logical_sharding, global_in_avals, in_shardings)
A:jax._src.interpreters.pxla.out_partitions->map(_to_logical_sharding, global_out_avals, out_shardings)
A:jax._src.interpreters.pxla.num_addressable_devices->len(da_object.addressable_device_list)
A:jax._src.interpreters.pxla.index->tuple(sharding.addressable_devices_indices_map(aval.shape).values())
A:jax._src.interpreters.pxla.(_, out_op_shardings)->jax._src.pjit.get_op_sharding_from_executable(xla_executable)
A:jax._src.interpreters.pxla.(in_pspec, out_pspec)->jax._src.pjit.get_pspec_from_executable(xla_executable, mesh)
A:jax._src.interpreters.pxla._ShardingT->TypeVar('_ShardingT', bound=sharding_impls.XLACompatibleSharding)
A:jax._src.interpreters.pxla.oi->getattr(i, '_original_sharding', None)
A:jax._src.interpreters.pxla.dev->numpy.vectorize(lambda i: da[i], otypes=[object])(np.arange(len(da)))
A:jax._src.interpreters.pxla.xla_device_assignment->numpy.vectorize(lambda i: da[i], otypes=[object])(np.arange(len(da))).reshape((num_replicas, num_partitions))
A:jax._src.interpreters.pxla.compiler_options->dict(safe_zip(compiler_options_keys, compiler_options_values))
A:jax._src.interpreters.pxla.opts.auto_spmd_partitioning_mesh_shape->list(mesh.shape.values())
A:jax._src.interpreters.pxla.opts.auto_spmd_partitioning_mesh_ids->jax._src.sharding_specs.get_logical_mesh_ids(list(mesh.shape.values())).reshape(-1)
A:jax._src.interpreters.pxla.opts.allow_spmd_sharding_propagation_to_output->list(_allow_propagation_to_outputs)
A:jax._src.interpreters.pxla.xla_executable->jax._src.compiler.compile_or_get_cached(backend, computation, dev, compile_options, host_callbacks)
A:jax._src.interpreters.pxla.input_indices->_get_input_indices(global_in_avals, in_shardings, da)
A:jax._src.interpreters.pxla.unsafe_call->jax._src.xla_bridge.get_device_backend(mesh.devices.flat[0]).compile_replicated(is_trivial=False, name=name, computation=computation, compile_options=compile_options, host_callbacks=host_callbacks, has_unordered_effects=has_unordered_effects, ordered_effects=ordered_effects, in_avals=global_in_avals, in_indices=input_indices, in_shardings=in_shardings, kept_var_idx=kept_var_idx, out_avals=global_out_avals, out_shardings=out_shardings, committed=committed, pmap_nreps=pmap_nreps)
A:jax._src.interpreters.pxla.hlo->jax._src.interpreters.mlir.refine_polymorphic_shapes(hlo)
A:jax._src.interpreters.pxla.da->_create_da_object(tuple(device_assignment))
A:jax._src.interpreters.pxla.allow_prop_to_outputs->tuple((is_unspecified(o) for o in out_shardings))
A:jax._src.interpreters.pxla.(xla_executable, compile_options)->_cached_compilation(hlo, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_outputs, tuple(host_callbacks), backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)
A:jax._src.interpreters.pxla.semantics_in_shardings->SemanticallyEqualShardings(in_shardings)
A:jax._src.interpreters.pxla.semantics_out_shardings->SemanticallyEqualShardings(out_shardings)
A:jax._src.interpreters.pxla.(in_shardings_xla, out_shardings_xla)->_get_mesh_pspec_shardings_from_executable(xla_executable, mesh)
A:jax._src.interpreters.pxla.(out_shardings, are_out_shardings_from_xla)->maybe_get_orig_out_sharding(in_shardings, out_shardings, are_out_shardings_from_xla, global_in_avals, global_out_avals)
A:jax._src.interpreters.pxla.out_shardings_xla->get_gspmd_shardings_from_executable(xla_executable, device_assignment, len(global_out_avals), len(ordered_effects), all_default_mem_kind)
A:jax._src.interpreters.pxla.xla_hlo_s->xla_s._to_xla_hlo_sharding(aval.ndim)
A:jax._src.interpreters.pxla.orig_hlo_s->orig._to_xla_hlo_sharding(aval.ndim)
A:jax._src.interpreters.pxla.(in_shardings, out_shardings, committed, da)->_get_metadata_jit_pmap(xla_executable.local_devices(), len(in_shardings), len(out_shardings))
A:jax._src.interpreters.pxla.out_unflat->jax._src.tree_util.tree_unflatten(out_tree, out_flat)
A:jax._src.interpreters.pxla.params->jax._src.stages.CompiledCallParams(self, no_kwargs, in_tree, out_tree)
A:jax._src.interpreters.pxla.(outs, out_flat, args_flat)->jax._src.stages.Compiled.call(params, *args, **kwargs)
A:jax._src.interpreters.pxla.(out_flat, out_tree_dispatch)->reflatten_outputs_for_dispatch(out_tree, out_flat)
A:jax._src.interpreters.pxla.use_fastpath->all((isinstance(x, xc.ArrayImpl) for x in out_flat))
A:jax._src.interpreters.pxla.fastpath_data->MeshExecutableFastpathData(self.xla_executable, out_tree_dispatch, self._in_shardings, self._out_shardings, out_avals, out_committed, kept_var_bitvec)
A:jax._src.interpreters.pxla.out_flat->self.unsafe_call(*args)
A:jax._src.interpreters.pxla.outs->jax._src.tree_util.tree_unflatten(out_tree, out_flat)
A:jax._src.interpreters.pxla.num_args->len(ref_avals)
A:jax._src.interpreters.pxla.str_errors->'\n'.join(errors[:num_errors])
A:jax._src.interpreters.pxla.db_xs->check_device_backend_on_shardings([xs])
A:jax._src.interpreters.pxla.xs->getattr(xs, '_original_sharding', xs)
A:jax._src.interpreters.pxla.(parsed_pspec, _, _)->jax._src.sharding_impls.prepare_axis_resources(pspec, 'pspec to array_mapping')
A:jax._src.interpreters.pxla.(axis, other_axis)->sorted([str(axis), str(other_axis)])
A:jax._src.interpreters.pxla.typing_rule->custom_resource_typing_rules.get(eqn.primitive, None)
A:jax._src.interpreters.pxla.aval_shape->list(aval.shape)
jax._src.interpreters.pxla.DeviceAssignmentMismatch
jax._src.interpreters.pxla.DeviceAssignmentMismatch._dev_ids_plat_str(self)
jax._src.interpreters.pxla.DeviceAssignmentMismatch._maybe_api_name(self,api_name)->str
jax._src.interpreters.pxla.DeviceAssignmentMismatch._str(self,api_name)
jax._src.interpreters.pxla.DeviceAssignmentMismatch.device_ids(self)->Sequence[int]
jax._src.interpreters.pxla.DeviceAssignmentMismatch.m_type_str(self,api_name)
jax._src.interpreters.pxla.DeviceAssignmentMismatch.platform(self)->str
jax._src.interpreters.pxla.DeviceAssignmentMismatch.source_info_str(self)
jax._src.interpreters.pxla.DeviceAssignmentMismatchError(Exception)
jax._src.interpreters.pxla.EmapInfo(NamedTuple)
jax._src.interpreters.pxla.ExecuteReplicated(self,xla_executable,name,backend,in_handler:InputsHandler,out_handler:ResultsHandler,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],keepalive:Any,has_host_callbacks:bool,kept_var_idx:set[int])
jax._src.interpreters.pxla.ExecuteReplicated.__init__(self,xla_executable,name,backend,in_handler:InputsHandler,out_handler:ResultsHandler,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],keepalive:Any,has_host_callbacks:bool,kept_var_idx:set[int])
jax._src.interpreters.pxla.ExecuteReplicated._add_tokens_to_inputs(self,input_bufs)
jax._src.interpreters.pxla.ExecuteReplicated._handle_token_bufs(self,token_bufs,sharded_token)
jax._src.interpreters.pxla.InputsHandler(self,local_devices,in_shardings,input_indices)
jax._src.interpreters.pxla.InputsHandler.__init__(self,local_devices,in_shardings,input_indices)
jax._src.interpreters.pxla.InputsHandler.__str__(self)
jax._src.interpreters.pxla.MapTrace(self,*args,emap_info)
jax._src.interpreters.pxla.MapTrace.__init__(self,*args,emap_info)
jax._src.interpreters.pxla.MapTrace.process_axis_index(self,frame)
jax._src.interpreters.pxla.MapTrace.process_call(self,call_primitive,fun,tracers,params)
jax._src.interpreters.pxla.MapTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.interpreters.pxla.MapTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.interpreters.pxla.MapTrace.process_map(self,map_primitive,fun,tracers,params)
jax._src.interpreters.pxla.MapTrace.process_primitive(self,primitive,tracers,params)
jax._src.interpreters.pxla.MapTrace.pure(self,val)
jax._src.interpreters.pxla.MapTrace.sublift(self,tracer)
jax._src.interpreters.pxla.MapTracer(self,trace:MapTrace,val,shard_axes:dict[core.AxisName,int])
jax._src.interpreters.pxla.MapTracer.__init__(self,trace:MapTrace,val,shard_axes:dict[core.AxisName,int])
jax._src.interpreters.pxla.MapTracer.__str__(self)
jax._src.interpreters.pxla.MapTracer.aval(self)
jax._src.interpreters.pxla.MapTracer.full_lower(self)
jax._src.interpreters.pxla.MeshComputation(self,name:str,hlo:ir.Module|None,donated_invars:Sequence[bool],**compile_args)
jax._src.interpreters.pxla.MeshComputation.__init__(self,name:str,hlo:ir.Module|None,donated_invars:Sequence[bool],**compile_args)
jax._src.interpreters.pxla.MeshComputation.compile(self,compiler_options=None)->MeshExecutable
jax._src.interpreters.pxla.MeshComputation.cost_analysis(self)->dict[str, float]
jax._src.interpreters.pxla.MeshComputation.stablehlo(self)->ir.Module
jax._src.interpreters.pxla.MeshExecutable(self,xla_executable,build_unsafe_call,in_avals,in_shardings,out_shardings,auto_spmd_lowering,kept_var_idx,jaxpr_debug_info=None,unloaded_executable=None)
jax._src.interpreters.pxla.MeshExecutable.__init__(self,xla_executable,build_unsafe_call,in_avals,in_shardings,out_shardings,auto_spmd_lowering,kept_var_idx,jaxpr_debug_info=None,unloaded_executable=None)
jax._src.interpreters.pxla.MeshExecutable.call(self,*args)
jax._src.interpreters.pxla.MeshExecutable.create_cpp_call(self,no_kwargs,in_tree,out_tree)
jax._src.interpreters.pxla.MeshExecutable.create_cpp_call_for_apply_primitive(self,out_tree)
jax._src.interpreters.pxla.MeshExecutable.input_shardings(self)->Sequence[sharding_impls.XLACompatibleSharding]
jax._src.interpreters.pxla.MeshExecutable.output_shardings(self)->Sequence[sharding_impls.XLACompatibleSharding]
jax._src.interpreters.pxla.MeshExecutable.unsafe_call(self)->Callable[..., Any]
jax._src.interpreters.pxla.MeshExecutable.xla_extension_executable(self)
jax._src.interpreters.pxla.MeshExecutableFastpathData(NamedTuple)
jax._src.interpreters.pxla.MismatchType(enum.Enum)
jax._src.interpreters.pxla.MismatchType.__str__(self)
jax._src.interpreters.pxla.ParallelCallableInfo
jax._src.interpreters.pxla.ParallelCallableInfo.local_devices(self)
jax._src.interpreters.pxla.ParallelCallableInfo.out_axes(self)
jax._src.interpreters.pxla.PmapComputation(self,hlo:ir.Module,**compile_args)
jax._src.interpreters.pxla.PmapComputation.__init__(self,hlo:ir.Module,**compile_args)
jax._src.interpreters.pxla.PmapComputation.compile(self,compiler_options=None)->PmapExecutable
jax._src.interpreters.pxla.PmapComputation.stablehlo(self)->ir.Module
jax._src.interpreters.pxla.PmapExecutable(self,xla_executable,build_unsafe_call,fingerprint,in_avals,jaxpr_debug_info,unloaded_executable)
jax._src.interpreters.pxla.PmapExecutable.__init__(self,xla_executable,build_unsafe_call,fingerprint,in_avals,jaxpr_debug_info,unloaded_executable)
jax._src.interpreters.pxla.PmapExecutable.call(self,*args)
jax._src.interpreters.pxla.PmapExecutable.unsafe_call(self)->Callable[..., Any]
jax._src.interpreters.pxla.PmapExecutable.xla_extension_executable(self)
jax._src.interpreters.pxla.ReplicaInfo(NamedTuple)
jax._src.interpreters.pxla.ResultsHandler(self,handlers,out_shardings,out_avals)
jax._src.interpreters.pxla.ResultsHandler.__init__(self,handlers,out_shardings,out_avals)
jax._src.interpreters.pxla.SPMDBatchTrace(batching.BatchTrace)
jax._src.interpreters.pxla.SPMDBatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax._src.interpreters.pxla.SemanticallyEqualShardings
jax._src.interpreters.pxla.SemanticallyEqualShardings.__eq__(self,other)
jax._src.interpreters.pxla.SemanticallyEqualShardings.__hash__(self)
jax._src.interpreters.pxla.ShardInfo(NamedTuple)
jax._src.interpreters.pxla.TileManual
jax._src.interpreters.pxla.TileVectorize
jax._src.interpreters.pxla.UnloadedMeshExecutable
jax._src.interpreters.pxla.UnloadedMeshExecutable.build_unsafe_call(self)
jax._src.interpreters.pxla.UnloadedMeshExecutable.from_hlo(name:str,hlo:ir.Module,global_in_avals:Sequence[ShapedArray],global_out_avals:Sequence[ShapedArray],in_shardings:Sequence[sharding_impls.XLACompatibleSharding|AUTO],out_shardings:Sequence[sharding_impls.XLACompatibleSharding|AUTO|UnspecifiedValue],spmd_lowering:bool,tuple_args:bool,auto_spmd_lowering:bool,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],host_callbacks:list[Any],keepalive:Any,kept_var_idx:set[int],backend:xb.XlaBackend,device_assignment:_DeviceAssignment|Sequence[xc.Device],committed:bool,pmap_nreps:int=1,jaxpr_debug_info:core.JaxprDebugInfo|None=None,shape_poly_state:mlir.ShapePolyLoweringState|None=None,all_default_mem_kind:bool=True,compiler_options=None)->MeshExecutable
jax._src.interpreters.pxla.UnloadedMeshExecutable.load(self)->MeshExecutable
jax._src.interpreters.pxla.UnloadedPmapExecutable
jax._src.interpreters.pxla.UnloadedPmapExecutable.build_execute_fun(self)
jax._src.interpreters.pxla.UnloadedPmapExecutable.from_hlo(hlo:ir.Module,pci:ParallelCallableInfo,replicas:ReplicaInfo,shards:ShardInfo,tuple_args:bool,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],host_callbacks:list[Any],keepalive:Any,jaxpr_debug_info:core.JaxprDebugInfo,compiler_options=None)
jax._src.interpreters.pxla.UnloadedPmapExecutable.load(self)->PmapExecutable
jax._src.interpreters.pxla.WeakRefList(list)
jax._src.interpreters.pxla._annot_to_flat(ndim:int,mapped_axes:Iterable[int],annotation:int|None)->int | None
jax._src.interpreters.pxla._axis_groups(mesh_spec,mesh_axes)
jax._src.interpreters.pxla._axis_read(axis_env,axis_name)
jax._src.interpreters.pxla._cached_compilation(computation,name,mesh,spmd_lowering,tuple_args,auto_spmd_lowering,_allow_propagation_to_outputs,host_callbacks,backend,da,pmap_nreps,compiler_options_keys,compiler_options_values)
jax._src.interpreters.pxla._cached_lowering_to_hlo(closed_jaxpr,api_name,fun_name,backend,semantic_in_shardings,semantic_out_shardings,da_object,donated_invars,name_stack,all_default_mem_kind,lowering_parameters:mlir.LoweringParameters)
jax._src.interpreters.pxla._cast_to_shaped_array(aval:core.AbstractValue)->ShapedArray
jax._src.interpreters.pxla._compile_replicated_mesh_executable_from_hlo(computation,name,global_in_avals,global_out_avals,semantics_in_shardings,semantics_out_shardings,auto_spmd_lowering,compile_options,host_callbacks,has_unordered_effects,ordered_effects,kept_var_idx,backend,da,committed,pmap_nreps,jaxpr_debug_info)
jax._src.interpreters.pxla._compile_replicated_pmap_executable_from_hlo(hlo:ir.Module,pci,input_indices,in_shardings,handle_outs,compile_options,host_callbacks,has_unordered_effects,ordered_effects,jaxpr_debug_info)
jax._src.interpreters.pxla._create_da_object(device_assignment:tuple[xc.Device,...])->_DeviceAssignment
jax._src.interpreters.pxla._emap_impl(fun:lu.WrappedFun,*args,backend:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool)
jax._src.interpreters.pxla._extend_axis_env(env:sharding_impls.AxisEnv,name,size:int)
jax._src.interpreters.pxla._full_to_shard_abstract_eval(x,axes,mesh,**_)
jax._src.interpreters.pxla._full_to_shard_lowering(ctx,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:frozenset[sharding_impls.MeshAxisName])
jax._src.interpreters.pxla._get_and_check_device_assignment(shardings:Iterable[ShardingInfo],devices:Sequence[xc.Device]|None)->tuple[xc.Client, tuple[xc.Device, ...]]
jax._src.interpreters.pxla._get_default_device()->xc.Device
jax._src.interpreters.pxla._get_input_indices(avals:Sequence[ShapedArray],shardings:Sequence[sharding_impls.XLACompatibleSharding],da_object:_DeviceAssignment|Sequence[xc.Device])->Sequence[tuple[Index | None, ...]]
jax._src.interpreters.pxla._get_mesh_pspec_shardings_from_executable(xla_executable,mesh:Mesh)->tuple[Sequence[sharding_impls.NamedSharding], Sequence[sharding_impls.NamedSharding]]
jax._src.interpreters.pxla._get_metadata_jit_pmap(local_devices,num_in_shardings,num_out_shardings)
jax._src.interpreters.pxla._get_out_sharding_from_orig_sharding(out_shardings,out_avals,orig_in_s,orig_aval,are_out_sharding_from_xla)
jax._src.interpreters.pxla._get_pmap_sharding(devices,specs)
jax._src.interpreters.pxla._get_replicated_slices(num_addressable_devices:int,ndim:int|None)
jax._src.interpreters.pxla._gspmd_to_named_sharding(out_s:sharding_impls.GSPMDSharding,orig_in_s:sharding_impls.NamedSharding)->sharding_impls.NamedSharding
jax._src.interpreters.pxla._gspmd_to_positional_sharding(out_s:sharding_impls.GSPMDSharding,orig_in_s:sharding_impls.PositionalSharding)->sharding_impls.PositionalSharding
jax._src.interpreters.pxla._hlo_shard(aval,axis_env,xs,in_axis)
jax._src.interpreters.pxla._hlo_unshard(ctx:mlir.LoweringRuleContext,aval,axis_env,out_axis,xs)
jax._src.interpreters.pxla._map_schedule(idx:tuple[int|None,...])->tuple[int | None, ...]
jax._src.interpreters.pxla._masked_array_error(x,devices,indices,sharding)
jax._src.interpreters.pxla._match_annot(axis_name:core.AxisName,axis_size:int,val:Any,shard_axis_src:dict[core.AxisName,int],dst_annotation:int|None)->tuple[Any, dict[core.AxisName, int]]
jax._src.interpreters.pxla._moveaxis(ndim:int,shard_axes:dict[core.AxisName,int],src:int,dst:int)->dict[core.AxisName, int]
jax._src.interpreters.pxla._multi_pmap(f:Callable,info:EmapInfo,names:list[core.AxisName],all_axes:list[tuple[int|None,...]])->tuple[Callable, dict[core.AxisName, int]]
jax._src.interpreters.pxla._pmap_axis_subst(params,subst,traverse)
jax._src.interpreters.pxla._pmap_dce_rule(used_outputs,eqn)
jax._src.interpreters.pxla._pmap_lowering(ctx,*in_nodes,axis_name,axis_size,global_axis_size,devices,name,call_jaxpr,backend=None,in_axes,out_axes,donated_invars,is_explicit_global_axis_size)
jax._src.interpreters.pxla._pmap_partial_eval_custom_params_updater(unks_in,inst_in,kept_outs_known,kept_outs_staged,num_res,params_known,params_staged)
jax._src.interpreters.pxla._pmap_partial_eval_custom_res_maker(params_known,aval)
jax._src.interpreters.pxla._raise_warnings_or_errors_for_jit_of_pmap(nreps:int,backend:xc.Client,name:str,jaxpr:core.Jaxpr)->None
jax._src.interpreters.pxla._register_out_sharding_handler(sharding_cls:type[_ShardingT],handler:Callable[[sharding_impls.GSPMDSharding,_ShardingT],_ShardingT])->None
jax._src.interpreters.pxla._sanitize_mesh_jaxpr(jaxpr)
jax._src.interpreters.pxla._shard_abstract_array(size,axis:int,x)
jax._src.interpreters.pxla._shard_array(x,devices,indices,sharding)
jax._src.interpreters.pxla._shard_darray(x,devices,indices,sharding)
jax._src.interpreters.pxla._shard_to_full_abstract_eval(x,axes,mesh,**_)
jax._src.interpreters.pxla._shard_to_full_lowering(ctx:mlir.LoweringRuleContext,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:frozenset[sharding_impls.MeshAxisName])
jax._src.interpreters.pxla._shard_token(x,devices,indices,sharding)
jax._src.interpreters.pxla._thread_local_decorator(self,fn)
jax._src.interpreters.pxla._thread_local_decorator.__init__(self,fn)
jax._src.interpreters.pxla._to_logical_sharding(aval:core.AbstractValue,sharding:MaybeSharding|AUTO)->sharding_impls.XLACompatibleSharding | None
jax._src.interpreters.pxla._trace_to_jaxpr_and_dce(fun_or_jaxpr,global_in_avals,api_name,fun_name,keep_unused,donated_invars,auto_spmd_lowering)
jax._src.interpreters.pxla._unravel_index_hlo(axis_env)
jax._src.interpreters.pxla._xla_call_partial_eval_update_params(params:core.ParamDict,kept_inputs:Sequence[bool],num_new_inputs:int)->core.ParamDict
jax._src.interpreters.pxla._xla_call_transpose_update_params(params,undef_primals,nonzero_cts)
jax._src.interpreters.pxla.are_all_shardings_default_mem_kind(da_object,shardings)
jax._src.interpreters.pxla.axis_groups(axis_env:sharding_impls.AxisEnv,name)->tuple[tuple[int, ...]]
jax._src.interpreters.pxla.batched_device_put(aval:core.ShapedArray,sharding:jax.sharding.Sharding,xs:Sequence[Any],devices:Sequence[jax.Device],committed:bool=True)
jax._src.interpreters.pxla.cache_wrap(fn)
jax._src.interpreters.pxla.check_arg_avals_for_call(ref_avals,arg_avals,jaxpr_debug_info:core.JaxprDebugInfo|None=None)
jax._src.interpreters.pxla.check_device_backend_on_shardings(shardings)->bool
jax._src.interpreters.pxla.check_gda_or_array_xla_sharding_match(args,in_xla_shardings:Sequence[sharding_impls.XLACompatibleSharding],jaxpr_debug_info:core.JaxprDebugInfo|None)->None
jax._src.interpreters.pxla.check_if_any_auto(shardings:Iterable[sharding_impls.XLACompatibleSharding|AUTO|UnspecifiedValue])->bool
jax._src.interpreters.pxla.create_mesh_pspec_sharding(mesh:Mesh,pspec:Optional[PartitionSpec],parsed_pspec=None,memory_kind:Optional[str]=None)->sharding_impls.NamedSharding
jax._src.interpreters.pxla.device_put(x,devices:Sequence[xc.ArrayImpl],replicate:bool=False)->list[xc.ArrayImpl]
jax._src.interpreters.pxla.find_replicas(jaxpr:core.Jaxpr,axis_size:int,global_axis_size:int)->ReplicaInfo
jax._src.interpreters.pxla.get_array_mapping(pspec:PartitionSpec)->ArrayMappingOrAutoOrUnspecified
jax._src.interpreters.pxla.get_gspmd_shardings_from_executable(xla_executable,device_assignment:Sequence[xc.Device],num_out_avals:int,num_ordered_effects:int,all_default_mem_kind:bool)->Sequence[sharding_impls.XLACompatibleSharding]
jax._src.interpreters.pxla.global_aval_to_result_handler(aval:core.AbstractValue,out_sharding,committed:bool,is_out_sharding_from_xla:bool)->Callable[[Sequence[xc.ArrayImpl]], Any]
jax._src.interpreters.pxla.global_avals_to_results_handler(global_out_avals:Sequence[ShapedArray],shardings:Sequence[sharding_impls.XLACompatibleSharding],committed:bool,are_out_shardings_from_xla:Sequence[bool])->ResultsHandler
jax._src.interpreters.pxla.identity(x)
jax._src.interpreters.pxla.jaxpr_transfer_mem_kinds(jaxpr:core.Jaxpr)->Iterator[sharding_impls.TransferToMemoryKind]
jax._src.interpreters.pxla.local_aval_to_result_handler(aval:core.AbstractValue,sharding:sharding_impls.XLACompatibleSharding,indices:tuple[Index,...]|None)->Callable[[list[xc.ArrayImpl]], Any]
jax._src.interpreters.pxla.local_avals_to_results_handler(unmapped_local_out_avals:Sequence[ShapedArray],local_shardings:Sequence[sharding_impls.XLACompatibleSharding])->ResultsHandler
jax._src.interpreters.pxla.lower_mesh_computation(fun_or_jaxpr:lu.WrappedFun|core.ClosedJaxpr,api_name:str,fun_name:str,mesh:Mesh,in_shardings:Sequence[sharding_impls.NamedSharding|AUTO],out_shardings:Sequence[sharding_impls.NamedSharding|AUTO|UnspecifiedValue],donated_invars:Sequence[bool],spmd_lowering:bool,global_in_avals:Sequence[core.ShapedArray],tiling_method:TilingMethod|None,lowering_parameters:mlir.LoweringParameters)->MeshComputation
jax._src.interpreters.pxla.lower_parallel_callable(fun:lu.WrappedFun,backend_name:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[xc.Device]|None,name:str,in_axes:Iterable[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool,avals:Sequence[core.AbstractValue],*,lowering_parameters:mlir.LoweringParameters)
jax._src.interpreters.pxla.lower_sharding_computation(fun_or_jaxpr:lu.WrappedFun|core.ClosedJaxpr,api_name:str,fun_name:str,in_shardings:Sequence[MaybeSharding],out_shardings:Sequence[MaybeSharding]|UnspecifiedValue,donated_invars:Sequence[bool],global_in_avals:Sequence[core.ShapedArray],*,keep_unused:bool,inline:bool,devices_from_context:Sequence[xc.Device]|None=None,lowering_parameters:mlir.LoweringParameters)->MeshComputation
jax._src.interpreters.pxla.manual_proto(aval:core.ShapedArray,manual_axes_set:frozenset[sharding_impls.MeshAxisName],mesh:Mesh)
jax._src.interpreters.pxla.maybe_extend_axis_env(*args,**kwargs)
jax._src.interpreters.pxla.maybe_get_orig_out_sharding(in_shardings,out_shardings,are_out_shardings_from_xla,in_avals,out_avals)
jax._src.interpreters.pxla.mesh_global_to_local(mesh,axes:ArrayMapping,aval)
jax._src.interpreters.pxla.mesh_local_to_global(mesh,axes:ArrayMapping,aval)
jax._src.interpreters.pxla.mesh_sharding_specs(axis_sizes,axis_names,allow_uneven_axes=False)
jax._src.interpreters.pxla.parallel_callable(fun:lu.WrappedFun,backend_name:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool,*avals)
jax._src.interpreters.pxla.prune_unused_inputs(jaxpr:core.Jaxpr)->tuple[core.Jaxpr, set[int], set[int]]
jax._src.interpreters.pxla.reflatten_outputs_for_dispatch(out_tree,out_flat)
jax._src.interpreters.pxla.resource_typecheck(jaxpr,resource_env,axis_resources,what_jaxpr_thunk)
jax._src.interpreters.pxla.shard_arg(arg,devices,arg_indices,sharding,canonicalize=True)
jax._src.interpreters.pxla.shard_args(devices:Sequence[xb.xla_client.Device],indices:Sequence[Sequence[Index]],shardings:Sequence[sharding_impls.XLACompatibleSharding],args)->Sequence[jax.Array]
jax._src.interpreters.pxla.shard_aval(size,axis:int,aval)
jax._src.interpreters.pxla.stage_parallel_callable(pci:ParallelCallableInfo,fun:lu.WrappedFun)->tuple[core.Jaxpr, list[Any], ReplicaInfo, ShardInfo]
jax._src.interpreters.pxla.tile_aval_nd(axis_sizes,in_axes:ArrayMapping,aval)
jax._src.interpreters.pxla.untile_aval_nd(axis_sizes,out_axes:ArrayMapping,aval)
jax._src.interpreters.pxla.vtile_by_mesh(fun:lu.WrappedFun,mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping])
jax._src.interpreters.pxla.vtile_manual(manual_axes:frozenset[sharding_impls.MeshAxisName],mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping],*args)
jax._src.interpreters.pxla.xla_call_jvp_update_params(params,nz_tangents)
jax._src.interpreters.pxla.xla_pmap_impl(fun:lu.WrappedFun,*args,**params)
jax._src.interpreters.pxla.xla_pmap_impl_lazy(fun:lu.WrappedFun,*args,backend:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool)->Callable
jax._src.pxla.DeviceAssignmentMismatch
jax._src.pxla.DeviceAssignmentMismatch._dev_ids_plat_str(self)
jax._src.pxla.DeviceAssignmentMismatch._maybe_api_name(self,api_name)->str
jax._src.pxla.DeviceAssignmentMismatch._str(self,api_name)
jax._src.pxla.DeviceAssignmentMismatch.device_ids(self)->Sequence[int]
jax._src.pxla.DeviceAssignmentMismatch.m_type_str(self,api_name)
jax._src.pxla.DeviceAssignmentMismatch.platform(self)->str
jax._src.pxla.DeviceAssignmentMismatch.source_info_str(self)
jax._src.pxla.DeviceAssignmentMismatchError(Exception)
jax._src.pxla.EmapInfo(NamedTuple)
jax._src.pxla.ExecuteReplicated(self,xla_executable,name,backend,in_handler:InputsHandler,out_handler:ResultsHandler,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],keepalive:Any,has_host_callbacks:bool,kept_var_idx:set[int])
jax._src.pxla.ExecuteReplicated._add_tokens_to_inputs(self,input_bufs)
jax._src.pxla.ExecuteReplicated._handle_token_bufs(self,token_bufs,sharded_token)
jax._src.pxla.InputsHandler(self,local_devices,in_shardings,input_indices)
jax._src.pxla.InputsHandler.__str__(self)
jax._src.pxla.MapTrace(self,*args,emap_info)
jax._src.pxla.MapTrace.process_axis_index(self,frame)
jax._src.pxla.MapTrace.process_call(self,call_primitive,fun,tracers,params)
jax._src.pxla.MapTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.pxla.MapTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.pxla.MapTrace.process_map(self,map_primitive,fun,tracers,params)
jax._src.pxla.MapTrace.process_primitive(self,primitive,tracers,params)
jax._src.pxla.MapTrace.pure(self,val)
jax._src.pxla.MapTrace.sublift(self,tracer)
jax._src.pxla.MapTracer(self,trace:MapTrace,val,shard_axes:dict[core.AxisName,int])
jax._src.pxla.MapTracer.__str__(self)
jax._src.pxla.MapTracer.aval(self)
jax._src.pxla.MapTracer.full_lower(self)
jax._src.pxla.MeshComputation(self,name:str,hlo:ir.Module|None,donated_invars:Sequence[bool],**compile_args)
jax._src.pxla.MeshComputation.compile(self,compiler_options=None)->MeshExecutable
jax._src.pxla.MeshComputation.cost_analysis(self)->dict[str, float]
jax._src.pxla.MeshComputation.stablehlo(self)->ir.Module
jax._src.pxla.MeshExecutable(self,xla_executable,build_unsafe_call,in_avals,in_shardings,out_shardings,auto_spmd_lowering,kept_var_idx,jaxpr_debug_info=None,unloaded_executable=None)
jax._src.pxla.MeshExecutable.call(self,*args)
jax._src.pxla.MeshExecutable.create_cpp_call(self,no_kwargs,in_tree,out_tree)
jax._src.pxla.MeshExecutable.create_cpp_call_for_apply_primitive(self,out_tree)
jax._src.pxla.MeshExecutable.unsafe_call(self)->Callable[..., Any]
jax._src.pxla.MeshExecutable.xla_extension_executable(self)
jax._src.pxla.MeshExecutableFastpathData(NamedTuple)
jax._src.pxla.MismatchType(enum.Enum)
jax._src.pxla.MismatchType.__str__(self)
jax._src.pxla.ParallelCallableInfo
jax._src.pxla.ParallelCallableInfo.local_devices(self)
jax._src.pxla.ParallelCallableInfo.out_axes(self)
jax._src.pxla.PmapComputation(self,hlo:ir.Module,**compile_args)
jax._src.pxla.PmapComputation.compile(self,compiler_options=None)->PmapExecutable
jax._src.pxla.PmapComputation.stablehlo(self)->ir.Module
jax._src.pxla.PmapExecutable(self,xla_executable,build_unsafe_call,fingerprint,in_avals,jaxpr_debug_info,unloaded_executable)
jax._src.pxla.PmapExecutable.call(self,*args)
jax._src.pxla.PmapExecutable.unsafe_call(self)->Callable[..., Any]
jax._src.pxla.PmapExecutable.xla_extension_executable(self)
jax._src.pxla.ReplicaInfo(NamedTuple)
jax._src.pxla.ResultsHandler(self,handlers,out_shardings,out_avals)
jax._src.pxla.SPMDBatchTrace(batching.BatchTrace)
jax._src.pxla.SPMDBatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax._src.pxla.SemanticallyEqualShardings
jax._src.pxla.SemanticallyEqualShardings.__eq__(self,other)
jax._src.pxla.SemanticallyEqualShardings.__hash__(self)
jax._src.pxla.ShardInfo(NamedTuple)
jax._src.pxla.TileManual
jax._src.pxla.TileVectorize
jax._src.pxla.UnloadedMeshExecutable
jax._src.pxla.UnloadedMeshExecutable.build_unsafe_call(self)
jax._src.pxla.UnloadedMeshExecutable.from_hlo(name:str,hlo:ir.Module,global_in_avals:Sequence[ShapedArray],global_out_avals:Sequence[ShapedArray],in_shardings:Sequence[sharding_impls.XLACompatibleSharding|AUTO],out_shardings:Sequence[sharding_impls.XLACompatibleSharding|AUTO|UnspecifiedValue],spmd_lowering:bool,tuple_args:bool,auto_spmd_lowering:bool,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],host_callbacks:list[Any],keepalive:Any,kept_var_idx:set[int],backend:xb.XlaBackend,device_assignment:_DeviceAssignment|Sequence[xc.Device],committed:bool,pmap_nreps:int=1,jaxpr_debug_info:core.JaxprDebugInfo|None=None,shape_poly_state:mlir.ShapePolyLoweringState|None=None,all_default_mem_kind:bool=True,compiler_options=None)->MeshExecutable
jax._src.pxla.UnloadedMeshExecutable.load(self)->MeshExecutable
jax._src.pxla.UnloadedPmapExecutable
jax._src.pxla.UnloadedPmapExecutable.build_execute_fun(self)
jax._src.pxla.UnloadedPmapExecutable.from_hlo(hlo:ir.Module,pci:ParallelCallableInfo,replicas:ReplicaInfo,shards:ShardInfo,tuple_args:bool,unordered_effects:list[core.Effect],ordered_effects:list[core.Effect],host_callbacks:list[Any],keepalive:Any,jaxpr_debug_info:core.JaxprDebugInfo,compiler_options=None)
jax._src.pxla.UnloadedPmapExecutable.load(self)->PmapExecutable
jax._src.pxla.WeakRefList(list)
jax._src.pxla._axis_groups(mesh_spec,mesh_axes)
jax._src.pxla._axis_read(axis_env,axis_name)
jax._src.pxla._cached_compilation(computation,name,mesh,spmd_lowering,tuple_args,auto_spmd_lowering,_allow_propagation_to_outputs,host_callbacks,backend,da,pmap_nreps,compiler_options_keys,compiler_options_values)
jax._src.pxla._cached_lowering_to_hlo(closed_jaxpr,api_name,fun_name,backend,semantic_in_shardings,semantic_out_shardings,da_object,donated_invars,name_stack,all_default_mem_kind,lowering_parameters:mlir.LoweringParameters)
jax._src.pxla._cast_to_shaped_array(aval:core.AbstractValue)->ShapedArray
jax._src.pxla._compile_replicated_mesh_executable_from_hlo(computation,name,global_in_avals,global_out_avals,semantics_in_shardings,semantics_out_shardings,auto_spmd_lowering,compile_options,host_callbacks,has_unordered_effects,ordered_effects,kept_var_idx,backend,da,committed,pmap_nreps,jaxpr_debug_info)
jax._src.pxla._compile_replicated_pmap_executable_from_hlo(hlo:ir.Module,pci,input_indices,in_shardings,handle_outs,compile_options,host_callbacks,has_unordered_effects,ordered_effects,jaxpr_debug_info)
jax._src.pxla._create_da_object(device_assignment:tuple[xc.Device,...])->_DeviceAssignment
jax._src.pxla._emap_impl(fun:lu.WrappedFun,*args,backend:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool)
jax._src.pxla._extend_axis_env(env:sharding_impls.AxisEnv,name,size:int)
jax._src.pxla._full_to_shard_abstract_eval(x,axes,mesh,**_)
jax._src.pxla._full_to_shard_lowering(ctx,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:frozenset[sharding_impls.MeshAxisName])
jax._src.pxla._get_and_check_device_assignment(shardings:Iterable[ShardingInfo],devices:Sequence[xc.Device]|None)->tuple[xc.Client, tuple[xc.Device, ...]]
jax._src.pxla._get_default_device()->xc.Device
jax._src.pxla._get_input_indices(avals:Sequence[ShapedArray],shardings:Sequence[sharding_impls.XLACompatibleSharding],da_object:_DeviceAssignment|Sequence[xc.Device])->Sequence[tuple[Index | None, ...]]
jax._src.pxla._get_metadata_jit_pmap(local_devices,num_in_shardings,num_out_shardings)
jax._src.pxla._get_replicated_slices(num_addressable_devices:int,ndim:int|None)
jax._src.pxla._hlo_shard(aval,axis_env,xs,in_axis)
jax._src.pxla._hlo_unshard(ctx:mlir.LoweringRuleContext,aval,axis_env,out_axis,xs)
jax._src.pxla._map_schedule(idx:tuple[int|None,...])->tuple[int | None, ...]
jax._src.pxla._masked_array_error(x,devices,indices,sharding)
jax._src.pxla._moveaxis(ndim:int,shard_axes:dict[core.AxisName,int],src:int,dst:int)->dict[core.AxisName, int]
jax._src.pxla._multi_pmap(f:Callable,info:EmapInfo,names:list[core.AxisName],all_axes:list[tuple[int|None,...]])->tuple[Callable, dict[core.AxisName, int]]
jax._src.pxla._pmap_axis_subst(params,subst,traverse)
jax._src.pxla._pmap_dce_rule(used_outputs,eqn)
jax._src.pxla._pmap_lowering(ctx,*in_nodes,axis_name,axis_size,global_axis_size,devices,name,call_jaxpr,backend=None,in_axes,out_axes,donated_invars,is_explicit_global_axis_size)
jax._src.pxla._pmap_partial_eval_custom_params_updater(unks_in,inst_in,kept_outs_known,kept_outs_staged,num_res,params_known,params_staged)
jax._src.pxla._pmap_partial_eval_custom_res_maker(params_known,aval)
jax._src.pxla._sanitize_mesh_jaxpr(jaxpr)
jax._src.pxla._shard_abstract_array(size,axis:int,x)
jax._src.pxla._shard_array(x,devices,indices,sharding)
jax._src.pxla._shard_darray(x,devices,indices,sharding)
jax._src.pxla._shard_to_full_abstract_eval(x,axes,mesh,**_)
jax._src.pxla._shard_to_full_lowering(ctx:mlir.LoweringRuleContext,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:frozenset[sharding_impls.MeshAxisName])
jax._src.pxla._shard_token(x,devices,indices,sharding)
jax._src.pxla._thread_local_decorator(self,fn)
jax._src.pxla._trace_to_jaxpr_and_dce(fun_or_jaxpr,global_in_avals,api_name,fun_name,keep_unused,donated_invars,auto_spmd_lowering)
jax._src.pxla._unravel_index_hlo(axis_env)
jax._src.pxla._xla_call_partial_eval_update_params(params:core.ParamDict,kept_inputs:Sequence[bool],num_new_inputs:int)->core.ParamDict
jax._src.pxla._xla_call_transpose_update_params(params,undef_primals,nonzero_cts)
jax._src.pxla.axis_groups(axis_env:sharding_impls.AxisEnv,name)->tuple[tuple[int, ...]]
jax._src.pxla.batched_device_put(aval:core.ShapedArray,sharding:jax.sharding.Sharding,xs:Sequence[Any],devices:Sequence[jax.Device],committed:bool=True)
jax._src.pxla.cache_wrap(fn)
jax._src.pxla.check_arg_avals_for_call(ref_avals,arg_avals,jaxpr_debug_info:core.JaxprDebugInfo|None=None)
jax._src.pxla.check_if_any_auto(shardings:Iterable[sharding_impls.XLACompatibleSharding|AUTO|UnspecifiedValue])->bool
jax._src.pxla.device_put(x,devices:Sequence[xc.ArrayImpl],replicate:bool=False)->list[xc.ArrayImpl]
jax._src.pxla.find_replicas(jaxpr:core.Jaxpr,axis_size:int,global_axis_size:int)->ReplicaInfo
jax._src.pxla.get_array_mapping(pspec:PartitionSpec)->ArrayMappingOrAutoOrUnspecified
jax._src.pxla.global_aval_to_result_handler(aval:core.AbstractValue,out_sharding,committed:bool,is_out_sharding_from_xla:bool)->Callable[[Sequence[xc.ArrayImpl]], Any]
jax._src.pxla.global_avals_to_results_handler(global_out_avals:Sequence[ShapedArray],shardings:Sequence[sharding_impls.XLACompatibleSharding],committed:bool,are_out_shardings_from_xla:Sequence[bool])->ResultsHandler
jax._src.pxla.identity(x)
jax._src.pxla.jaxpr_transfer_mem_kinds(jaxpr:core.Jaxpr)->Iterator[sharding_impls.TransferToMemoryKind]
jax._src.pxla.local_aval_to_result_handler(aval:core.AbstractValue,sharding:sharding_impls.XLACompatibleSharding,indices:tuple[Index,...]|None)->Callable[[list[xc.ArrayImpl]], Any]
jax._src.pxla.local_avals_to_results_handler(unmapped_local_out_avals:Sequence[ShapedArray],local_shardings:Sequence[sharding_impls.XLACompatibleSharding])->ResultsHandler
jax._src.pxla.lower_mesh_computation(fun_or_jaxpr:lu.WrappedFun|core.ClosedJaxpr,api_name:str,fun_name:str,mesh:Mesh,in_shardings:Sequence[sharding_impls.NamedSharding|AUTO],out_shardings:Sequence[sharding_impls.NamedSharding|AUTO|UnspecifiedValue],donated_invars:Sequence[bool],spmd_lowering:bool,global_in_avals:Sequence[core.ShapedArray],tiling_method:TilingMethod|None,lowering_parameters:mlir.LoweringParameters)->MeshComputation
jax._src.pxla.lower_parallel_callable(fun:lu.WrappedFun,backend_name:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[xc.Device]|None,name:str,in_axes:Iterable[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool,avals:Sequence[core.AbstractValue],*,lowering_parameters:mlir.LoweringParameters)
jax._src.pxla.manual_proto(aval:core.ShapedArray,manual_axes_set:frozenset[sharding_impls.MeshAxisName],mesh:Mesh)
jax._src.pxla.maybe_extend_axis_env(*args,**kwargs)
jax._src.pxla.mesh_global_to_local(mesh,axes:ArrayMapping,aval)
jax._src.pxla.mesh_local_to_global(mesh,axes:ArrayMapping,aval)
jax._src.pxla.parallel_callable(fun:lu.WrappedFun,backend_name:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool,*avals)
jax._src.pxla.prune_unused_inputs(jaxpr:core.Jaxpr)->tuple[core.Jaxpr, set[int], set[int]]
jax._src.pxla.reflatten_outputs_for_dispatch(out_tree,out_flat)
jax._src.pxla.resource_typecheck(jaxpr,resource_env,axis_resources,what_jaxpr_thunk)
jax._src.pxla.shard_arg(arg,devices,arg_indices,sharding,canonicalize=True)
jax._src.pxla.shard_args(devices:Sequence[xb.xla_client.Device],indices:Sequence[Sequence[Index]],shardings:Sequence[sharding_impls.XLACompatibleSharding],args)->Sequence[jax.Array]
jax._src.pxla.shard_aval(size,axis:int,aval)
jax._src.pxla.stage_parallel_callable(pci:ParallelCallableInfo,fun:lu.WrappedFun)->tuple[core.Jaxpr, list[Any], ReplicaInfo, ShardInfo]
jax._src.pxla.tile_aval_nd(axis_sizes,in_axes:ArrayMapping,aval)
jax._src.pxla.untile_aval_nd(axis_sizes,out_axes:ArrayMapping,aval)
jax._src.pxla.vtile_by_mesh(fun:lu.WrappedFun,mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping])
jax._src.pxla.vtile_manual(manual_axes:frozenset[sharding_impls.MeshAxisName],mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping],*args)
jax._src.pxla.xla_call_jvp_update_params(params,nz_tangents)
jax._src.pxla.xla_pmap_impl(fun:lu.WrappedFun,*args,**params)
jax._src.pxla.xla_pmap_impl_lazy(fun:lu.WrappedFun,*args,backend:str|None,axis_name:core.AxisName,axis_size:int,global_axis_size:int,devices:Sequence[Any]|None,name:str,in_axes:Sequence[int|None],out_axes_thunk:Callable[[],Sequence[int|None]],donated_invars:Sequence[bool],is_explicit_global_axis_size:bool)->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/partial_eval.py----------------------------------------
A:jax._src.interpreters.partial_eval.expl_names_->set(expl_names)
A:jax._src.interpreters.partial_eval.known->self.pval.get_known()
A:jax._src.interpreters.partial_eval.aval->get_aval(x)
A:jax._src.interpreters.partial_eval.const->self._new_const(aval, t).pval.get_known()
A:jax._src.interpreters.partial_eval.tracers->map(self.instantiate_const_abstracted, tracers)
A:jax._src.interpreters.partial_eval.(out_aval, effects)->primitive.abstract_eval(*avals, **params)
A:jax._src.interpreters.partial_eval.name_stack->self._current_truncated_name_stack()
A:jax._src.interpreters.partial_eval.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.interpreters.partial_eval.eqn->new_jaxpr_eqn([*constvars, *invars], outvars, prim, dict(call_jaxpr=closed_call_jaxpr, transpose_jaxpr_thunk=transpose_jaxpr_thunk, out_types=out_types, res_tree=res_tree, lin_tree=lin_tree, out_tree=out_tree), closed_call_jaxpr.effects, source_info_util.current())
A:jax._src.interpreters.partial_eval.out_tracer->JaxprTracer(self, PartialVal.unknown(out_aval), None)
A:jax._src.interpreters.partial_eval.out_tracer.recipe->new_eqn_recipe(tracers, [out_tracer], primitive, params, effects, source)
A:jax._src.interpreters.partial_eval.rule->dce_rules.get(eqn.primitive, _default_dce_rule)
A:jax._src.interpreters.partial_eval.(in_knowns, in_avals, in_consts)->partition_pvals([t.pval for t in tracers])
A:jax._src.interpreters.partial_eval.f_->trace_to_subjaxpr_nounits_fwd(f, self.main, False)
A:jax._src.interpreters.partial_eval.(f_, aux)->trace_to_subjaxpr_nounits_dyn(f, self.main, tuple(in_knowns), f.in_type, False)
A:jax._src.interpreters.partial_eval.f->jax._src.linear_util.annotate(f, tuple(((raise_to_shaped(t.aval), True) for t in explicit_tracers)))
A:jax._src.interpreters.partial_eval.const_params->dict(const_params, in_axes=tuple(const_in_axes), out_axes_thunk=const_out_axes_thunk)
A:jax._src.interpreters.partial_eval.out->cells.get(args, sentinel)
A:jax._src.interpreters.partial_eval.(fwds, out_knowns, out_type, jaxpr, env)->aux()
A:jax._src.interpreters.partial_eval.(out_consts, non_fwd_res)->split_list(out, [sum(out_knowns)])
A:jax._src.interpreters.partial_eval.cin_consts_full[idx]->next(in_consts_)
A:jax._src.interpreters.partial_eval.res->tuple((c for (c, fwd) in zip(res, fwds) if fwd is None))
A:jax._src.interpreters.partial_eval.res_tracers->map(self.new_instantiated_const, res)
A:jax._src.interpreters.partial_eval.env_tracers->map(self.full_raise, env)
A:jax._src.interpreters.partial_eval.staged_params->dict(staged_params, in_axes=staged_in_axes, out_axes=tuple(staged_out_axes), call_jaxpr=call_jaxpr)
A:jax._src.interpreters.partial_eval.(unk_in_axes, const_in_axes)->partition_list(in_knowns, params['in_axes'])
A:jax._src.interpreters.partial_eval.(f, aux)->partial_eval_wrapper_nounits(f, tuple(in_knowns), tuple(in_avals))
A:jax._src.interpreters.partial_eval.(out_knowns, _, jaxpr, _)->aux()
A:jax._src.interpreters.partial_eval.(_, out_axes)->partition_list(out_knowns, out_axes_thunk())
A:jax._src.interpreters.partial_eval.(out_knowns, out_avals_mapped, jaxpr, env)->aux()
A:jax._src.interpreters.partial_eval.(out_consts, res)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax._src.interpreters.partial_eval.call_jaxpr->convert_constvars_jaxpr(jaxpr)
A:jax._src.interpreters.partial_eval.out_axes->params['out_axes_thunk']()
A:jax._src.interpreters.partial_eval.(staged_out_axes, _)->partition_list(out_knowns, out_axes)
A:jax._src.interpreters.partial_eval.const_tracers->map(trace.new_instantiated_const, res)
A:jax._src.interpreters.partial_eval.(jaxpr, res, env)->tracers_to_jaxpr(in_tracers, out_tracers)
A:jax._src.interpreters.partial_eval.(out_knowns, out_avals, out_consts)->partition_pvals(out_pvals)
A:jax._src.interpreters.partial_eval.trace->main.with_cur_sublevel()
A:jax._src.interpreters.partial_eval.new_params->dict(params, call_jaxpr=padded_jaxpr)
A:jax._src.interpreters.partial_eval.(out_knowns, out_avals_mapped, out_consts)->partition_pvals(out_pvals)
A:jax._src.interpreters.partial_eval.staged_out_axes->tuple(out_axes_unknown)
A:jax._src.interpreters.partial_eval.(out_axes_unknown, out_axes_known)->partition_list(out_knowns, out_axes)
A:jax._src.interpreters.partial_eval.(res_ts, lin_ts)->split_list(tracers, [params['res_tree'].num_leaves])
A:jax._src.interpreters.partial_eval.lin_all_known->all((t.is_known() for t in lin_ts))
A:jax._src.interpreters.partial_eval.in_tracers->map(trace.new_arg, pvals)
A:jax._src.interpreters.partial_eval.(in_knowns, in_avals, ())->partition_pvals([t.pval for t in tracers])
A:jax._src.interpreters.partial_eval.out_flat->_interleave_fun(fwd, zeros).call_wrapped()
A:jax._src.interpreters.partial_eval.(out_knowns, out_avals, jaxpr, env)->aux()
A:jax._src.interpreters.partial_eval.closed_jaxpr->jax._src.core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())
A:jax._src.interpreters.partial_eval.fwd_->_interleave_fun(fwd, zeros)
A:jax._src.interpreters.partial_eval.(fwd_, aux)->partial_eval_wrapper_nounits(fwd_, tuple(in_knowns), tuple(in_avals))
A:jax._src.interpreters.partial_eval.(_, res)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax._src.interpreters.partial_eval.converted_jaxpr->Jaxpr(constvars=jaxpr.constvars + env_vars, invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns, effects=jaxpr.effects)
A:jax._src.interpreters.partial_eval.sentinel->object()
A:jax._src.interpreters.partial_eval.(in_avals, which_explicit)->unzip2(in_type)
A:jax._src.interpreters.partial_eval.constval->next(in_consts_iter)
A:jax._src.interpreters.partial_eval.in_consts_full[d.val]->JaxprTracer(trace, PartialVal.unknown(in_avals[d.val]), ConstVar(constval.shape[i]))
A:jax._src.interpreters.partial_eval.in_consts_full[idx]->JaxprTracer(trace, PartialVal.unknown(aval), ConstVar(constval))
A:jax._src.interpreters.partial_eval.in_knowns_iter->iter(in_knowns)
A:jax._src.interpreters.partial_eval.tracer->self._new_const(aval, t)
A:jax._src.interpreters.partial_eval.in_args->merge_lists(in_knowns, in_tracers, in_consts)
A:jax._src.interpreters.partial_eval.out_tracers->map(partial(instantiate_const_at, trace), instantiate, out_tracers)
A:jax._src.interpreters.partial_eval.jaxpr->Jaxpr(constvars, self.invars, expl_outvars, self.eqns, jaxpr_effects)
A:jax._src.interpreters.partial_eval.(_, avals_out, _)->trace_to_jaxpr_dynamic(lu.wrap_init(fun, params), avals, debug_info)
A:jax._src.interpreters.partial_eval.current_name_stack->jax._src.source_info_util.current_name_stack()
A:jax._src.interpreters.partial_eval.fun->trace_to_subjaxpr_nounits(fun, main, instantiate)
A:jax._src.interpreters.partial_eval.(jaxpr, (out_pvals, consts, env))->trace_to_subjaxpr_nounits(fun, main, instantiate).call_wrapped(pvals)
A:jax._src.interpreters.partial_eval.(jaxpr, out_consts, env)->tracers_to_jaxpr(in_tracers, out_tracers_)
A:jax._src.interpreters.partial_eval.FreeVar->namedtuple('FreeVar', ['val'])
A:jax._src.interpreters.partial_eval.ConstVar->namedtuple('ConstVar', ['val'])
A:jax._src.interpreters.partial_eval.LambdaBinding->namedtuple('LambdaBinding', [])
A:jax._src.interpreters.partial_eval.gensym->jax._src.core.gensym()
A:jax._src.interpreters.partial_eval.var->self.frame.tracer_to_var.get(id(tracer))
A:jax._src.interpreters.partial_eval.var_->t_to_var.setdefault(id(t), var)
A:jax._src.interpreters.partial_eval.processed_eqn_ids->set()
A:jax._src.interpreters.partial_eval.in_atoms->map(get_atom, r.in_tracers)
A:jax._src.interpreters.partial_eval.varconstid_to_var[id(r.val)]->newvar(t)
A:jax._src.interpreters.partial_eval.(env_vars, env_vals)->unzip2(env.items())
A:jax._src.interpreters.partial_eval.(const_vars, const_vals)->unzip2(consts.items())
A:jax._src.interpreters.partial_eval.outvars->map(self.makevar, out_tracers)
A:jax._src.interpreters.partial_eval.jaxpr_effects->make_jaxpr_effects(new_constvars, new_invars, new_outvars, new_eqns)
A:jax._src.interpreters.partial_eval.lifted_jaxpr->Jaxpr(constvars, self.invars, expl_outvars, self.eqns, jaxpr_effects).replace(constvars=tuple(constvars), invars=invars, debug_info=dbg)
A:jax._src.interpreters.partial_eval.(constvars, invars)->split_list(jaxpr.invars, [n])
A:jax._src.interpreters.partial_eval.(env_vars, invars)->split_list(jaxpr.invars, [num_env_vars])
A:jax._src.interpreters.partial_eval.known_vals_in->iter(known_vals_in)
A:jax._src.interpreters.partial_eval.(jaxpr_unknown_, out_pvals, residuals)->trace_to_jaxpr_nounits(f, in_pvals, instantiate=instantiate)
A:jax._src.interpreters.partial_eval.jaxpr_unknown->convert_constvars_jaxpr(jaxpr_unknown_)
A:jax._src.interpreters.partial_eval.(jaxpr_known, _, consts_known)->trace_to_jaxpr_dynamic(lu.wrap_init(fun), known_avals)
A:jax._src.interpreters.partial_eval.closed_jaxpr_known->ClosedJaxpr(jaxpr_known, consts_known)
A:jax._src.interpreters.partial_eval.closed_jaxpr_unknown->ClosedJaxpr(jaxpr_unknown, ())
A:jax._src.interpreters.partial_eval.(jaxpr_known, jaxpr_staged, out_unknowns, out_inst, num_res, num_res_ref)->_partial_eval_jaxpr_custom_cached(jaxpr, tuple(in_unknowns), tuple(in_inst), tuple(ensure_out_unknowns), tuple(ensure_out_inst), saveable)
A:jax._src.interpreters.partial_eval.newvar->jax._src.core.gensym([jaxpr_known.jaxpr, jaxpr_staged.jaxpr])
A:jax._src.interpreters.partial_eval.(unks_in, inst_in)->unzip2(map(read, eqn.invars))
A:jax._src.interpreters.partial_eval.(eqn1, eqn2, unks_out, inst_out, res)->rule(saveable, unks_in, inst_in, eqn)
A:jax._src.interpreters.partial_eval.inputs->map(ensure_instantiated, inst_in, eqn.invars)
A:jax._src.interpreters.partial_eval.policy->ensure_enum(saveable(eqn.primitive, *[x.aval for x in eqn.invars], **eqn.params))
A:jax._src.interpreters.partial_eval.offload_eqn->jax._src.core.JaxprEqn(eqn.outvars, resvars, device_put_p, dict(device=TransferToMemoryKind(policy.dst), src=None), set(), source_info_util.new_source_info())
A:jax._src.interpreters.partial_eval.reload_eqn->jax._src.core.JaxprEqn(resvars, eqn.outvars, device_put_p, dict(device=TransferToMemoryKind(policy.src), src=None), set(), source_info_util.new_source_info())
A:jax._src.interpreters.partial_eval.unzipped->unzip2(map(read, jaxpr.outvars))
A:jax._src.interpreters.partial_eval.out_unknowns->map(op.or_, out_unknowns, ensure_out_unknowns)
A:jax._src.interpreters.partial_eval.out_inst->map(op.or_, out_inst, ensure_out_inst)
A:jax._src.interpreters.partial_eval.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax._src.interpreters.partial_eval.(outs_known, _)->partition_list(out_unknowns, jaxpr.outvars)
A:jax._src.interpreters.partial_eval.(non_input_res_refs, _)->partition_list(ref_res_is_input, list(residual_refs))
A:jax._src.interpreters.partial_eval.known_effects->make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res, known_outvars, known_eqns)
A:jax._src.interpreters.partial_eval.jaxpr_known->jax._src.core.ClosedJaxpr(jaxpr_known_, closed_jaxpr.consts)
A:jax._src.interpreters.partial_eval.(_, ins_staged)->partition_list(inst_in, eqn.invars)
A:jax._src.interpreters.partial_eval.(_, outs_staged)->partition_list(out_inst, jaxpr.outvars)
A:jax._src.interpreters.partial_eval.staged_effects->make_jaxpr_effects(jaxpr.constvars, staged_invars, outs_staged, staged_eqns)
A:jax._src.interpreters.partial_eval.jaxpr_staged->jax._src.core.ClosedJaxpr(jaxpr_staged_, closed_jaxpr.consts)
A:jax._src.interpreters.partial_eval.Recompute->RecomputeType()
A:jax._src.interpreters.partial_eval.Saveable->SaveableType()
A:jax._src.interpreters.partial_eval.(jaxpr_known, jaxpr_staged, unks_out, inst_out, num_res)->partial_eval_jaxpr_custom(jaxpr, unks_in, inst_in, False, False, saveable)
A:jax._src.interpreters.partial_eval.(out_binders_known, _)->partition_list(unks_out, eqn.outvars)
A:jax._src.interpreters.partial_eval.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax._src.interpreters.partial_eval.(params_known, params_staged)->params_updater(unks_in, inst_in, map(op.not_, unks_out), inst_out, sum((f is None for f in out_fwd)), num_res, params_known, params_staged)
A:jax._src.interpreters.partial_eval.eqn_known->new_jaxpr_eqn([*ins_known, *res_ref_binders], [*out_binders_known, *res_val_binders], eqn.primitive, params_known, jaxpr_known.effects, eqn.source_info)
A:jax._src.interpreters.partial_eval.eqn_staged->new_jaxpr_eqn([*res_val_vars, *res_ref_binders, *ins_staged], out_binders_staged, eqn.primitive, params_staged, jaxpr_staged.effects, eqn.source_info)
A:jax._src.interpreters.partial_eval.(jaxpr_known_, jaxpr_staged_, unks_out, inst_out, num_res_val, num_res_ref)->partial_eval_jaxpr_stateful(closed_jaxpr.jaxpr, unks_in, inst_in, False, False, saveable)
A:jax._src.interpreters.partial_eval.(out_vars, res_vars)->split_list(jaxpr_known_.outvars, [num_out_primals])
A:jax._src.interpreters.partial_eval.jaxpr_known_->prune_jaxpr_outputs(jaxpr_known_, [True] * num_out_primals + [f is None for f in out_fwd])
A:jax._src.interpreters.partial_eval.(res_val_binders, res_ref_binders)->split_list([newvar(res_aval(params_known, v)) for v in jaxpr_staged.in_avals[:num_res]], [num_res_val])
A:jax._src.interpreters.partial_eval.res_val_vars->subs_list(out_fwd, out_binders_known, res_val_binders)
A:jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom_rules[core.call_p]->partial(call_partial_eval_custom_rule, 'call_jaxpr', lambda _, __, ___, ____, _____, x, y: (x, y))
A:jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom_rules[core.closed_call_p]->partial(closed_call_partial_eval_custom_rule, 'call_jaxpr', lambda _, __, ___, ____, _____, ______, x, y: (x, y))
A:jax._src.interpreters.partial_eval.(fwd_vars, _)->forwarding_rules[eqn.primitive](eqn)
A:jax._src.interpreters.partial_eval.new_jaxpr->Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns, jaxpr.effects, jaxpr.debug_info)
A:jax._src.interpreters.partial_eval._prune_jaxpr_outputs_cached->weakref_lru_cache(_prune_jaxpr_outputs)
A:jax._src.interpreters.partial_eval.jaxpr_->convert_constvars_jaxpr(jaxpr)
A:jax._src.interpreters.partial_eval.(new_jaxpr_, used_inputs_)->dce_jaxpr(jaxpr_, used_outputs)
A:jax._src.interpreters.partial_eval.(used_consts, used_inputs)->split_list(used_inputs_, [len(jaxpr.constvars)])
A:jax._src.interpreters.partial_eval.used_outs->map(read, eqn.outvars)
A:jax._src.interpreters.partial_eval.(used_ins, new_eqn)->rule(used_outs, eqn)
A:jax._src.interpreters.partial_eval.used_inputs->map(op.or_, instantiate, used_inputs)
A:jax._src.interpreters.partial_eval.(new_jaxpr, used_inputs)->dce_jaxpr(jaxpr, used_outputs)
A:jax._src.interpreters.partial_eval.update_params->call_param_updaters.get(map_primitive)
A:jax._src.interpreters.partial_eval.new_eqn->new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax._src.interpreters.partial_eval.new_invars->_move_to_front(closed_jaxpr.jaxpr.invars, to_move)
A:jax._src.interpreters.partial_eval.new_closed_jaxpr->jax._src.core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)
A:jax._src.interpreters.partial_eval.(invar_pos, progenitor_eqns)->self._trace.frame.find_progenitors(self)
A:jax._src.interpreters.partial_eval.arg_info->arg_info_all(dbg)
A:jax._src.interpreters.partial_eval.val->JaxprStackFrame().constvar_to_val.get(frame.tracer_to_var.get(id(self)))
A:jax._src.interpreters.partial_eval.api_util._shaped_abstractify_handlers[DynamicJaxprTracer]->operator.attrgetter('aval')
A:jax._src.interpreters.partial_eval.eff->eff.replace(input_index=all_vars.index(invar)).replace(input_index=all_vars.index(invar))
A:jax._src.interpreters.partial_eval.self.gensym->jax._src.core.gensym()
A:jax._src.interpreters.partial_eval.self.effects->set()
A:jax._src.interpreters.partial_eval.(constvars, constvals)->unzip2(self.constvar_to_val.items())
A:jax._src.interpreters.partial_eval.(jaxpr, constvals)->_inline_literals(jaxpr, constvals)
A:jax._src.interpreters.partial_eval.(jaxpr, out_type)->_add_implicit_outputs(jaxpr)
A:jax._src.interpreters.partial_eval.has_input_effect->any((isinstance(eff, effects.JaxprInputEffect) for eff in eqn.effects))
A:jax._src.interpreters.partial_eval.(consts_out, new_eqn)->const_fold_rules[eqn.primitive](consts_in, eqn)
A:jax._src.interpreters.partial_eval.(fwd_vars, new_eqn)->forwarding_rules[eqn.primitive](eqn)
A:jax._src.interpreters.partial_eval.(new_constvars, new_constvals)->unzip2(consts.items())
A:jax._src.interpreters.partial_eval.self.frame.tracer_to_var[id(tracer)]var->self.frame.newvar(aval)
A:jax._src.interpreters.partial_eval.varself.frame.tracer_to_var[id(tracer)]->self.frame.newvar(tracer.aval)
A:jax._src.interpreters.partial_eval.(out_avals, effects)->primitive.abstract_eval(*avals, **params)
A:jax._src.interpreters.partial_eval.source_info->jax._src.source_info_util.current()
A:jax._src.interpreters.partial_eval.invars->map(self.getvar, tracers)
A:jax._src.interpreters.partial_eval.implicit_tracers->_extract_implicit_args(self, f.in_type, explicit_tracers)
A:jax._src.interpreters.partial_eval.dbg->debug_info_final(f, call_primitive.name)
A:jax._src.interpreters.partial_eval.(jaxpr, out_type, consts)->trace_to_subjaxpr_dynamic2(fun, main, debug_info)
A:jax._src.interpreters.partial_eval.constvars->map(self.getvar, map(self.instantiate_const, call_consts))
A:jax._src.interpreters.partial_eval.(jaxpr, reduced_out_avals, consts)->trace_to_subjaxpr_dynamic(f, self.main, reduced_in_avals, debug_info=debug_info_final(f, map_primitive.name))
A:jax._src.interpreters.partial_eval.ordered_effects->jax._src.effects.ordered_effects.filter_in(jaxpr.effects)
A:jax._src.interpreters.partial_eval.(fun_jaxpr, out_avals, consts)->trace_to_subjaxpr_dynamic(fun, self.main, in_avals)
A:jax._src.interpreters.partial_eval.closed_fun_jaxpr->jax._src.core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())
A:jax._src.interpreters.partial_eval.main_->ref(self.main)
A:jax._src.interpreters.partial_eval.(nz_tangent_avals, zero_avals)->partition_list(in_zeros, in_avals)
A:jax._src.interpreters.partial_eval.(jvp_, out_zeros)->_jvp_jaxpr_zeros(jvp, in_zeros, tuple(zero_avals))
A:jax._src.interpreters.partial_eval.(jaxpr, _, out_consts)->trace_to_subjaxpr_dynamic(jvp_, main_(), in_avals_)
A:jax._src.interpreters.partial_eval.(tracers_res, tracers_lin)->split_list(tracers, [res_tree.num_leaves])
A:jax._src.interpreters.partial_eval.(call_jaxpr, out_avals, call_consts)->trace_to_subjaxpr_dynamic(call, self.main, in_avals_p)
A:jax._src.interpreters.partial_eval.closed_call_jaxpr->jax._src.core.ClosedJaxpr(convert_constvars_jaxpr(call_jaxpr), ())
A:jax._src.interpreters.partial_eval.(transpose_flat, in_tree2)->flatten_fun_nokwargs(lu.wrap_init(transpose), treedef_tuple((res_tree, out_tree)))
A:jax._src.interpreters.partial_eval.(jaxpr, _, consts)->trace_to_subjaxpr_dynamic(transpose_flat, main_(), in_avals_t)
A:jax._src.interpreters.partial_eval.saved_state->jax._src.core.thread_local_state.trace_state.copy()
A:jax._src.interpreters.partial_eval.outcells[args]->fn(*args)
A:jax._src.interpreters.partial_eval.(in_primals, nz_in_tangents)->split_list(primal_tangent_avals, [len(in_zeros)])
A:jax._src.interpreters.partial_eval.symbolic_zeros->map(ad_util.SymbolicZero, zero_avals)
A:jax._src.interpreters.partial_eval.tangents->merge_lists(in_zeros, nz_in_tangents, symbolic_zeros)
A:jax._src.interpreters.partial_eval.(n, ragged)->divmod(len(out), 2)
A:jax._src.interpreters.partial_eval.(out_nz_tangents, _)->partition_list(out_zeros, out_tangents)
A:jax._src.interpreters.partial_eval.sig->inspect.signature(fn)
A:jax._src.interpreters.partial_eval.src_info->fun_sourceinfo(fn)
A:jax._src.interpreters.partial_eval.dummy_args->tree_unflatten(dbg.in_tree, [False] * dbg.in_tree.num_leaves)
A:jax._src.interpreters.partial_eval.dummy_result->tree_unflatten(dbg.out_tree(), [False] * num_leaves)
A:jax._src.interpreters.partial_eval.(jaxpr, out_avals, consts)->trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)
A:jax._src.interpreters.partial_eval.frame->JaxprStackFrame()
A:jax._src.interpreters.partial_eval.ans->trace_to_subjaxpr_nounits(fun, main, instantiate).call_wrapped(*in_tracers_)
A:jax._src.interpreters.partial_eval.(jaxpr, consts)->JaxprStackFrame().to_jaxpr(out_tracers)
A:jax._src.interpreters.partial_eval.(in_avals, keep_inputs)->unzip2(fun.in_type)
A:jax._src.interpreters.partial_eval.partial_specs->_canonicalize_specs(ndims, axes_specs)
A:jax._src.interpreters.partial_eval.specs->_complete_specs(args, partial_specs)
A:jax._src.interpreters.partial_eval.(idxs, implicit_types)->_collect_implicit(args, specs)
A:jax._src.interpreters.partial_eval.d->sizes.setdefault(name, x.shape[i])
A:jax._src.interpreters.partial_eval.spec->dict(spec)
A:jax._src.interpreters.partial_eval.spec[i]->named_tracers.get(id(d), TracerAsName(d))
A:jax._src.interpreters.partial_eval.counter->itertools.count()
A:jax._src.interpreters.partial_eval.idxs[name]->DBIdx(next(counter))
A:jax._src.interpreters.partial_eval.offset->len(implicit_types)
A:jax._src.interpreters.partial_eval.out_type->tuple(zip(out_avals, kept_outs))
A:jax._src.interpreters.partial_eval.self.ref->jax._src.core.get_referent(tracer)
A:jax._src.interpreters.partial_eval.explicit_tracers_->iter(explicit_tracers)
A:jax._src.interpreters.partial_eval.tracers[d1.val]->main.with_cur_sublevel().instantiate_const(d2)
A:jax._src.interpreters.partial_eval.eval_padded->jax._src.linear_util.wrap_init(partial(_eval_jaxpr_padded, jaxpr, consts))
A:jax._src.interpreters.partial_eval.(padded_jaxpr, _, padded_consts)->trace_to_jaxpr_dynamic(eval_padded, in_avals)
A:jax._src.interpreters.partial_eval.last_used->jax._src.core.last_used(jaxpr)
A:jax._src.interpreters.partial_eval.outs->rule(in_avals, out_avals, *map(read, eqn.invars), **eqn.params)
A:jax._src.interpreters.partial_eval.padding_rules[prim]->partial(_trivial_padding_rule, prim)
A:jax._src.interpreters.partial_eval.(padded_jaxpr, padded_consts)->pad_jaxpr(call_jaxpr, ())
A:jax._src.interpreters.partial_eval.(subfuns, bind_params)->prim.get_bind_params(new_params)
A:jax._src.interpreters.partial_eval.(jaxpr, consts, env)->tracers_to_jaxpr(in_tracers, out_tracers)
jax._src.interpreters.partial_eval.BoundedAxisSize(NamedTuple)
jax._src.interpreters.partial_eval.DebugInfo(NamedTuple)
jax._src.interpreters.partial_eval.DynamicJaxprTrace(core.Trace)
jax._src.interpreters.partial_eval.DynamicJaxprTrace._lift_tracers_in_aval(self,aval)
jax._src.interpreters.partial_eval.DynamicJaxprTrace._new_const(self,aval,c)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.frame(self)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.getvar(self,tracer)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.instantiate_const(self,val)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.makevar(self,tracer)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.new_arg(self,aval)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.new_const(self,c)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.post_process_map(self,map_primitive,out_tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_call(self,call_primitive,f,explicit_tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_custom_transpose(self,prim,call,tracers,*,transpose,out_types,lin_tree,res_tree,out_tree)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_map(self,map_primitive,f,tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.process_primitive(self,primitive,tracers,params)
jax._src.interpreters.partial_eval.DynamicJaxprTrace.sublift(self,t)
jax._src.interpreters.partial_eval.DynamicJaxprTracer(self,trace,aval,line_info=None)
jax._src.interpreters.partial_eval.DynamicJaxprTracer.__init__(self,trace,aval,line_info=None)
jax._src.interpreters.partial_eval.DynamicJaxprTracer._assert_live(self)->None
jax._src.interpreters.partial_eval.DynamicJaxprTracer._contents(self)
jax._src.interpreters.partial_eval.DynamicJaxprTracer._origin_msg(self)
jax._src.interpreters.partial_eval.DynamicJaxprTracer.full_lower(self)
jax._src.interpreters.partial_eval.DynamicJaxprTracer.get_referent(self)
jax._src.interpreters.partial_eval.JaxprEqnRecipe(NamedTuple)
jax._src.interpreters.partial_eval.JaxprStackFrame(self)
jax._src.interpreters.partial_eval.JaxprStackFrame.__init__(self)
jax._src.interpreters.partial_eval.JaxprStackFrame.add_eqn(self,eqn:core.JaxprEqn)
jax._src.interpreters.partial_eval.JaxprStackFrame.find_progenitors(self,tracer)
jax._src.interpreters.partial_eval.JaxprStackFrame.newvar(self,aval)
jax._src.interpreters.partial_eval.JaxprStackFrame.to_jaxpr(self,out_tracers:Sequence[Tracer])->tuple[Jaxpr, list[Any]]
jax._src.interpreters.partial_eval.JaxprStackFrame.to_jaxpr2(self,out_tracers)
jax._src.interpreters.partial_eval.JaxprTrace(self,*args,name_stack:source_info_util.NameStack)
jax._src.interpreters.partial_eval.JaxprTrace.__init__(self,*args,name_stack:source_info_util.NameStack)
jax._src.interpreters.partial_eval.JaxprTrace._current_truncated_name_stack(self)
jax._src.interpreters.partial_eval.JaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.instantiate_const(self,tracer:JaxprTracer)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.instantiate_const_abstracted(self,tracer)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.lift(self,val:Tracer)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.new_arg(self,pval:PartialVal)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.new_const(self,val)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.new_instantiated_const(self,val)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.new_instantiated_literal(self,val)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.post_process_call(self,primitive,out_tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.interpreters.partial_eval.JaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.interpreters.partial_eval.JaxprTrace.post_process_map(self,primitive,out_tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.process_call(self,primitive,f,tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.interpreters.partial_eval.JaxprTrace.process_custom_transpose(self,prim,call,tracers,**params)
jax._src.interpreters.partial_eval.JaxprTrace.process_custom_vjp_call(self,prim,f,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.interpreters.partial_eval.JaxprTrace.process_map(self,primitive,f:lu.WrappedFun,tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax._src.interpreters.partial_eval.JaxprTrace.pure(self,val:Any)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTrace.sublift(self,val:JaxprTracer)->JaxprTracer
jax._src.interpreters.partial_eval.JaxprTracer(self,trace:JaxprTrace,pval:PartialVal,recipe:JaxprTracerRecipe|None)
jax._src.interpreters.partial_eval.JaxprTracer.__init__(self,trace:JaxprTrace,pval:PartialVal,recipe:JaxprTracerRecipe|None)
jax._src.interpreters.partial_eval.JaxprTracer.__repr__(self)
jax._src.interpreters.partial_eval.JaxprTracer.aval(self)->AbstractValue
jax._src.interpreters.partial_eval.JaxprTracer.full_lower(self)
jax._src.interpreters.partial_eval.JaxprTracer.get_referent(self)
jax._src.interpreters.partial_eval.JaxprTracer.is_known(self)
jax._src.interpreters.partial_eval.JaxprTracer.parents(self)->Sequence[JaxprTracer]
jax._src.interpreters.partial_eval.Offloadable(NamedTuple)
jax._src.interpreters.partial_eval.PartialVal(cls,xs:tuple[AbstractValue|None,core.Value])
jax._src.interpreters.partial_eval.PartialVal.__new__(cls,xs:tuple[AbstractValue|None,core.Value])
jax._src.interpreters.partial_eval.PartialVal.get_aval(self)->AbstractValue
jax._src.interpreters.partial_eval.PartialVal.get_known(self)->core.Value | None
jax._src.interpreters.partial_eval.PartialVal.is_known(self)->bool
jax._src.interpreters.partial_eval.PartialVal.known(cls,const:core.Value)->PartialVal
jax._src.interpreters.partial_eval.PartialVal.unknown(cls,aval:AbstractValue)->PartialVal
jax._src.interpreters.partial_eval.RecomputeType
jax._src.interpreters.partial_eval.SaveableType
jax._src.interpreters.partial_eval.TracerAsName(self,tracer)
jax._src.interpreters.partial_eval.TracerAsName.__eq__(self,other)
jax._src.interpreters.partial_eval.TracerAsName.__hash__(self)
jax._src.interpreters.partial_eval.TracerAsName.__init__(self,tracer)
jax._src.interpreters.partial_eval._add_implicit_outputs(jaxpr:Jaxpr)->tuple[Jaxpr, OutputType]
jax._src.interpreters.partial_eval._arg_type(idxs:dict[AbstractedAxisName,DBIdx],x:Any,spec:dict[int,AbstractedAxisName])->AbstractValue
jax._src.interpreters.partial_eval._canonicalize_specs(ndims:Sequence[int],specs:Sequence[AbstractedAxesSpec]|None)->list[dict[int, AbstractedAxisName]]
jax._src.interpreters.partial_eval._closed_call_param_updater(params,_,__)
jax._src.interpreters.partial_eval._collect_implicit(args:Sequence[Any],specs:list[dict[int,AbstractedAxisName]])->tuple[dict[AbstractedAxisName, DBIdx], list[AbstractValue]]
jax._src.interpreters.partial_eval._complete_specs(args:Sequence[Any],partial_specs:list[dict[int,AbstractedAxisName]])->list[dict[int, AbstractedAxisName]]
jax._src.interpreters.partial_eval._const_folding_and_forwarding(jaxpr:Jaxpr,constvals:Sequence[Any])->tuple[Jaxpr, tuple[Any, ...]]
jax._src.interpreters.partial_eval._dce_jaxpr(jaxpr:Jaxpr,used_outputs:tuple[bool,...],instantiate:tuple[bool,...])->tuple[Jaxpr, list[bool]]
jax._src.interpreters.partial_eval._default_dce_rule(used_outs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn]
jax._src.interpreters.partial_eval._default_res_aval_updater(params:dict[str,Any],aval:AbstractValue)->AbstractValue
jax._src.interpreters.partial_eval._eval_jaxpr_padded(jaxpr:Jaxpr,consts:list[Const],*args:DynamicJaxprTracer)->list[Const | DynamicJaxprTracer]
jax._src.interpreters.partial_eval._extract_implicit_args(trace:DynamicJaxprTrace,in_type:Sequence[tuple[AbstractValue,bool]],explicit_tracers:Sequence[DynamicJaxprTracer])->Sequence[DynamicJaxprTracer]
jax._src.interpreters.partial_eval._inline_literals(jaxpr:Jaxpr,constvals:Sequence[Any])->tuple[Jaxpr, list[Any]]
jax._src.interpreters.partial_eval._input_type_to_tracers(new_arg:Callable[[AbstractValue],Tracer],in_avals:Sequence[AbstractValue])->Sequence[Tracer]
jax._src.interpreters.partial_eval._interleave_fun(every_others,*args,**kwargs)
jax._src.interpreters.partial_eval._is_bint_axis_size(d:int|core.DArray|core.Var)->bool
jax._src.interpreters.partial_eval._jaxpr_forwarding(jaxpr:Jaxpr)->list[int | None]
jax._src.interpreters.partial_eval._jvp_jaxpr_zeros(in_zeros,zero_avals,*primal_tangent_avals)
jax._src.interpreters.partial_eval._memoize(fn)
jax._src.interpreters.partial_eval._move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:tuple[bool,...])->ClosedJaxpr
jax._src.interpreters.partial_eval._move_to_front(lst:Sequence,to_move:Sequence[bool])->Sequence
jax._src.interpreters.partial_eval._partial_eval_jaxpr_custom_cached(jaxpr:Jaxpr,in_unknowns:tuple[bool,...],in_inst:tuple[bool,...],ensure_out_unknowns:tuple[bool,...],ensure_out_inst:tuple[bool,...],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int, int]
jax._src.interpreters.partial_eval._partial_eval_jaxpr_nounits(jaxpr,in_unknowns,instantiate)
jax._src.interpreters.partial_eval._prune_closed_jaxpr_outputs(jaxpr:ClosedJaxpr,used_outputs:tuple[bool,...])->ClosedJaxpr
jax._src.interpreters.partial_eval._prune_jaxpr_outputs(jaxpr:Jaxpr,used_outputs:tuple[bool,...])->Jaxpr
jax._src.interpreters.partial_eval._spec_to_dict(spec:AbstractedAxesSpec)->dict[int, AbstractedAxisName]
jax._src.interpreters.partial_eval._substitute_axis_sizes(env:dict,aval:AbstractValue)->AbstractValue
jax._src.interpreters.partial_eval._substitute_vars_in_type(consts:dict[Var,Literal],env:dict[Var,Var],a:AbstractValue)->AbstractValue
jax._src.interpreters.partial_eval._trace_to_subjaxpr_nounits(main,instantiate,in_pvals)
jax._src.interpreters.partial_eval._trivial_padding_rule(prim,_,__,*args,**params)
jax._src.interpreters.partial_eval._trivial_padding_rule_multi(prim,_,__,*args,**params)
jax._src.interpreters.partial_eval._update_annotation_known(f:lu.WrappedFun,orig_type:InputType|None,in_knowns:list[bool])->lu.WrappedFun
jax._src.interpreters.partial_eval.abstract_eval_fun(fun,*avals,debug_info=None,**params)
jax._src.interpreters.partial_eval.arg_info_all(dbg:DebugInfo)->list[tuple[str, KeyPath]] | None
jax._src.interpreters.partial_eval.call_padding_rule(prim,in_avals,out_avals,*args,call_jaxpr,**params)
jax._src.interpreters.partial_eval.call_partial_eval_custom_rule(jaxpr_param_name:str,params_updater:ParamsUpdater,saveable:Callable[...,RematCases_],unks_in:list[bool],inst_in:list[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater,ctx:Callable[[core.ParamDict],AbstractContextManager[None]]=trivial_ctx)->tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], list[Var]]
jax._src.interpreters.partial_eval.close_jaxpr(jaxpr:Jaxpr)->ClosedJaxpr
jax._src.interpreters.partial_eval.closed_call_partial_eval_custom_rule(jaxpr_param_name:str,params_updater:ParamsUpdater2,saveable:Callable[...,RematCases_],unks_in:list[bool],inst_in:list[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater)->tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], list[Var]]
jax._src.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr:Jaxpr)->Jaxpr
jax._src.interpreters.partial_eval.convert_envvars_to_constvars(jaxpr:Jaxpr,num_env_vars:int)->Jaxpr
jax._src.interpreters.partial_eval.convert_invars_to_constvars(jaxpr:Jaxpr,n:int)->Jaxpr
jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[bool]]
jax._src.interpreters.partial_eval.dce_jaxpr_call_rule(used_outputs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn | None]
jax._src.interpreters.partial_eval.dce_jaxpr_closed_call_rule(used_outputs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn]
jax._src.interpreters.partial_eval.dce_jaxpr_consts(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[bool], list[bool]]
jax._src.interpreters.partial_eval.debug_info(fn:Callable,in_tree:PyTreeDef|None,out_tree_thunk:Callable[[],PyTreeDef]|None,has_kwargs:bool,traced_for:str)->DebugInfo
jax._src.interpreters.partial_eval.debug_info_final(fn:lu.WrappedFun,traced_for:str)->DebugInfo
jax._src.interpreters.partial_eval.def_trivial_padding(prim:Primitive)->None
jax._src.interpreters.partial_eval.ensure_enum(case:bool|RematCases)->RematCases
jax._src.interpreters.partial_eval.extend_jaxpr_stack(main,frame)
jax._src.interpreters.partial_eval.identity(x)
jax._src.interpreters.partial_eval.infer_lambda_input_type(axes_specs:Sequence[AbstractedAxesSpec]|None,args:Sequence[Any])->InputType
jax._src.interpreters.partial_eval.instantiate_const_at(trace:JaxprTrace,instantiate:bool,tracer)
jax._src.interpreters.partial_eval.make_jaxpr_effects(constvars,invars,outvars,eqns)->effects.Effects
jax._src.interpreters.partial_eval.move_binders_to_back(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax._src.interpreters.partial_eval.move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax._src.interpreters.partial_eval.new_eqn_recipe(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer],primitive:Primitive,params:dict[str,Any],effects:core.Effects,source_info:source_info_util.SourceInfo)->JaxprEqnRecipe
jax._src.interpreters.partial_eval.pad_jaxpr(jaxpr:Jaxpr,consts:Sequence[Const])->tuple[Jaxpr, list[Const]]
jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr:Jaxpr,in_unknowns:Sequence[bool],in_inst:bool|Sequence[bool],ensure_out_unknowns:bool|Sequence[bool],ensure_out_inst:bool|Sequence[bool],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int]
jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom_rule_not_implemented(name:str,saveable:Callable[...,RematCases_],unks_in:Sequence[bool],inst_in:Sequence[bool],eqn:JaxprEqn)->PartialEvalCustomResult
jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr:ClosedJaxpr,unknowns:Sequence[bool],instantiate:bool|Sequence[bool])->tuple[ClosedJaxpr, ClosedJaxpr, list[bool], list[AbstractValue]]
jax._src.interpreters.partial_eval.partial_eval_jaxpr_stateful(jaxpr:Jaxpr,in_unknowns:Sequence[bool],in_inst:bool|Sequence[bool],ensure_out_unknowns:bool|Sequence[bool],ensure_out_inst:bool|Sequence[bool],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int, int]
jax._src.interpreters.partial_eval.partial_eval_wrapper_nounits(in_knowns:Sequence[bool],in_avals:Sequence[AbstractValue],*in_consts:Any)
jax._src.interpreters.partial_eval.partition_pvals(pvals:list[PartialVal])->tuple[list[bool], list[AbstractValue], list[Any]]
jax._src.interpreters.partial_eval.prune_closed_jaxpr_outputs(jaxpr:ClosedJaxpr,used_outputs:Sequence[bool])->ClosedJaxpr
jax._src.interpreters.partial_eval.prune_jaxpr_outputs(jaxpr:Jaxpr,used_outputs:Sequence[bool])->Jaxpr
jax._src.interpreters.partial_eval.recipe_to_eqn(getvar:Callable[[JaxprTracer],Atom],recipe:JaxprEqnRecipe)->core.JaxprEqn
jax._src.interpreters.partial_eval.result_info(dbg:DebugInfo)->list[KeyPath] | None
jax._src.interpreters.partial_eval.sig_info(dbg:DebugInfo)->inspect.BoundArguments | None
jax._src.interpreters.partial_eval.trace_to_jaxpr(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[PartialVal], list[core.Value]]
jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:DebugInfo|None=None,*,keep_inputs:list[bool]|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic2(fun:lu.WrappedFun,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.interpreters.partial_eval.trace_to_jaxpr_final(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:DebugInfo|None=None,keep_inputs:Sequence[bool]|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.interpreters.partial_eval.trace_to_jaxpr_final2(fun:lu.WrappedFun,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.interpreters.partial_eval.trace_to_jaxpr_nounits(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[PartialVal], list[core.Value]]
jax._src.interpreters.partial_eval.trace_to_subjaxpr(main:core.MainTrace,instantiate:bool|Sequence[bool],pvals:Sequence[PartialVal])
jax._src.interpreters.partial_eval.trace_to_subjaxpr_dynamic(fun:lu.WrappedFun,main:core.MainTrace,in_avals:Sequence[AbstractValue],*,keep_inputs:Sequence[bool]|None=None,debug_info:DebugInfo|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.interpreters.partial_eval.trace_to_subjaxpr_dynamic2(fun:lu.WrappedFun,main:core.MainTrace,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.interpreters.partial_eval.trace_to_subjaxpr_nounits(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.interpreters.partial_eval.trace_to_subjaxpr_nounits_dyn(main:core.MainTrace,in_knowns:Sequence[bool],in_type:InputType,instantiate:bool|Sequence[bool],*in_consts:Any)
jax._src.interpreters.partial_eval.trace_to_subjaxpr_nounits_fwd(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.interpreters.partial_eval.trace_to_subjaxpr_nounits_fwd2(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.interpreters.partial_eval.tracers_to_jaxpr(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer])->tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]
jax._src.interpreters.partial_eval.trivial_ctx(_)
jax._src.partial_eval.BoundedAxisSize(NamedTuple)
jax._src.partial_eval.DebugInfo(NamedTuple)
jax._src.partial_eval.DynamicJaxprTrace(core.Trace)
jax._src.partial_eval.DynamicJaxprTrace._lift_tracers_in_aval(self,aval)
jax._src.partial_eval.DynamicJaxprTrace._new_const(self,aval,c)
jax._src.partial_eval.DynamicJaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.frame(self)
jax._src.partial_eval.DynamicJaxprTrace.getvar(self,tracer)
jax._src.partial_eval.DynamicJaxprTrace.instantiate_const(self,val)
jax._src.partial_eval.DynamicJaxprTrace.makevar(self,tracer)
jax._src.partial_eval.DynamicJaxprTrace.new_arg(self,aval)
jax._src.partial_eval.DynamicJaxprTrace.new_const(self,c)
jax._src.partial_eval.DynamicJaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.partial_eval.DynamicJaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.partial_eval.DynamicJaxprTrace.post_process_map(self,map_primitive,out_tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.process_call(self,call_primitive,f,explicit_tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.partial_eval.DynamicJaxprTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.partial_eval.DynamicJaxprTrace.process_map(self,map_primitive,f,tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.process_primitive(self,primitive,tracers,params)
jax._src.partial_eval.DynamicJaxprTrace.sublift(self,t)
jax._src.partial_eval.DynamicJaxprTracer(self,trace,aval,line_info=None)
jax._src.partial_eval.DynamicJaxprTracer._assert_live(self)->None
jax._src.partial_eval.DynamicJaxprTracer._contents(self)
jax._src.partial_eval.DynamicJaxprTracer._origin_msg(self)
jax._src.partial_eval.DynamicJaxprTracer.full_lower(self)
jax._src.partial_eval.DynamicJaxprTracer.get_referent(self)
jax._src.partial_eval.JaxprEqnRecipe(NamedTuple)
jax._src.partial_eval.JaxprStackFrame(self)
jax._src.partial_eval.JaxprStackFrame.add_eqn(self,eqn:core.JaxprEqn)
jax._src.partial_eval.JaxprStackFrame.find_progenitors(self,tracer)
jax._src.partial_eval.JaxprStackFrame.newvar(self,aval)
jax._src.partial_eval.JaxprStackFrame.to_jaxpr(self,out_tracers:Sequence[Tracer])->tuple[Jaxpr, list[Any]]
jax._src.partial_eval.JaxprStackFrame.to_jaxpr2(self,out_tracers)
jax._src.partial_eval.JaxprTrace(self,*args,name_stack:source_info_util.NameStack)
jax._src.partial_eval.JaxprTrace._current_truncated_name_stack(self)
jax._src.partial_eval.JaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax._src.partial_eval.JaxprTrace.instantiate_const(self,tracer:JaxprTracer)->JaxprTracer
jax._src.partial_eval.JaxprTrace.instantiate_const_abstracted(self,tracer)->JaxprTracer
jax._src.partial_eval.JaxprTrace.lift(self,val:Tracer)->JaxprTracer
jax._src.partial_eval.JaxprTrace.new_arg(self,pval:PartialVal)->JaxprTracer
jax._src.partial_eval.JaxprTrace.new_const(self,val)->JaxprTracer
jax._src.partial_eval.JaxprTrace.new_instantiated_const(self,val)->JaxprTracer
jax._src.partial_eval.JaxprTrace.new_instantiated_literal(self,val)->JaxprTracer
jax._src.partial_eval.JaxprTrace.post_process_call(self,primitive,out_tracers,params)
jax._src.partial_eval.JaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax._src.partial_eval.JaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax._src.partial_eval.JaxprTrace.post_process_map(self,primitive,out_tracers,params)
jax._src.partial_eval.JaxprTrace.process_call(self,primitive,f,tracers,params)
jax._src.partial_eval.JaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers,*,symbolic_zeros)
jax._src.partial_eval.JaxprTrace.process_custom_vjp_call(self,prim,f,fwd,bwd,tracers,out_trees,symbolic_zeros)
jax._src.partial_eval.JaxprTrace.process_map(self,primitive,f:lu.WrappedFun,tracers,params)
jax._src.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax._src.partial_eval.JaxprTrace.pure(self,val:Any)->JaxprTracer
jax._src.partial_eval.JaxprTrace.sublift(self,val:JaxprTracer)->JaxprTracer
jax._src.partial_eval.JaxprTracer(self,trace:JaxprTrace,pval:PartialVal,recipe:JaxprTracerRecipe|None)
jax._src.partial_eval.JaxprTracer.__repr__(self)
jax._src.partial_eval.JaxprTracer.aval(self)->AbstractValue
jax._src.partial_eval.JaxprTracer.full_lower(self)
jax._src.partial_eval.JaxprTracer.get_referent(self)
jax._src.partial_eval.JaxprTracer.is_known(self)
jax._src.partial_eval.JaxprTracer.parents(self)->Sequence[JaxprTracer]
jax._src.partial_eval.Offloadable(NamedTuple)
jax._src.partial_eval.PartialVal(cls,xs:tuple[AbstractValue|None,core.Value])
jax._src.partial_eval.PartialVal.get_aval(self)->AbstractValue
jax._src.partial_eval.PartialVal.get_known(self)->core.Value | None
jax._src.partial_eval.PartialVal.is_known(self)->bool
jax._src.partial_eval.PartialVal.known(cls,const:core.Value)->PartialVal
jax._src.partial_eval.PartialVal.unknown(cls,aval:AbstractValue)->PartialVal
jax._src.partial_eval.RecomputeType
jax._src.partial_eval.SaveableType
jax._src.partial_eval.TracerAsName(self,tracer)
jax._src.partial_eval.TracerAsName.__eq__(self,other)
jax._src.partial_eval.TracerAsName.__hash__(self)
jax._src.partial_eval._add_implicit_outputs(jaxpr:Jaxpr)->tuple[Jaxpr, OutputType]
jax._src.partial_eval._arg_type(idxs:dict[AbstractedAxisName,DBIdx],x:Any,spec:dict[int,AbstractedAxisName])->AbstractValue
jax._src.partial_eval._canonicalize_specs(ndims:Sequence[int],specs:Sequence[AbstractedAxesSpec]|None)->list[dict[int, AbstractedAxisName]]
jax._src.partial_eval._closed_call_param_updater(params,_,__)
jax._src.partial_eval._collect_implicit(args:Sequence[Any],specs:list[dict[int,AbstractedAxisName]])->tuple[dict[AbstractedAxisName, DBIdx], list[AbstractValue]]
jax._src.partial_eval._complete_specs(args:Sequence[Any],partial_specs:list[dict[int,AbstractedAxisName]])->list[dict[int, AbstractedAxisName]]
jax._src.partial_eval._const_folding_and_forwarding(jaxpr:Jaxpr,constvals:Sequence[Any])->tuple[Jaxpr, tuple[Any, ...]]
jax._src.partial_eval._dce_jaxpr(jaxpr:Jaxpr,used_outputs:tuple[bool,...],instantiate:tuple[bool,...])->tuple[Jaxpr, list[bool]]
jax._src.partial_eval._default_dce_rule(used_outs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn]
jax._src.partial_eval._default_res_aval_updater(params:dict[str,Any],aval:AbstractValue)->AbstractValue
jax._src.partial_eval._eval_jaxpr_padded(jaxpr:Jaxpr,consts:list[Const],*args:DynamicJaxprTracer)->list[Const | DynamicJaxprTracer]
jax._src.partial_eval._extract_implicit_args(trace:DynamicJaxprTrace,in_type:Sequence[tuple[AbstractValue,bool]],explicit_tracers:Sequence[DynamicJaxprTracer])->Sequence[DynamicJaxprTracer]
jax._src.partial_eval._inline_literals(jaxpr:Jaxpr,constvals:Sequence[Any])->tuple[Jaxpr, list[Any]]
jax._src.partial_eval._input_type_to_tracers(new_arg:Callable[[AbstractValue],Tracer],in_avals:Sequence[AbstractValue])->Sequence[Tracer]
jax._src.partial_eval._interleave_fun(every_others,*args,**kwargs)
jax._src.partial_eval._is_bint_axis_size(d:int|core.DArray|core.Var)->bool
jax._src.partial_eval._jaxpr_forwarding(jaxpr:Jaxpr)->list[int | None]
jax._src.partial_eval._jvp_jaxpr_zeros(in_zeros,zero_avals,*primal_tangent_avals)
jax._src.partial_eval._memoize(fn)
jax._src.partial_eval._move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:tuple[bool,...])->ClosedJaxpr
jax._src.partial_eval._move_to_front(lst:Sequence,to_move:Sequence[bool])->Sequence
jax._src.partial_eval._partial_eval_jaxpr_custom_cached(jaxpr:Jaxpr,in_unknowns:tuple[bool,...],in_inst:tuple[bool,...],ensure_out_unknowns:tuple[bool,...],ensure_out_inst:tuple[bool,...],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int, int]
jax._src.partial_eval._partial_eval_jaxpr_nounits(jaxpr,in_unknowns,instantiate)
jax._src.partial_eval._prune_closed_jaxpr_outputs(jaxpr:ClosedJaxpr,used_outputs:tuple[bool,...])->ClosedJaxpr
jax._src.partial_eval._prune_jaxpr_outputs(jaxpr:Jaxpr,used_outputs:tuple[bool,...])->Jaxpr
jax._src.partial_eval._spec_to_dict(spec:AbstractedAxesSpec)->dict[int, AbstractedAxisName]
jax._src.partial_eval._substitute_axis_sizes(env:dict,aval:AbstractValue)->AbstractValue
jax._src.partial_eval._substitute_vars_in_type(consts:dict[Var,Literal],env:dict[Var,Var],a:AbstractValue)->AbstractValue
jax._src.partial_eval._trace_to_subjaxpr_nounits(main,instantiate,in_pvals)
jax._src.partial_eval._trivial_padding_rule(prim,_,__,*args,**params)
jax._src.partial_eval._trivial_padding_rule_multi(prim,_,__,*args,**params)
jax._src.partial_eval.abstract_eval_fun(fun,*avals,debug_info=None,**params)
jax._src.partial_eval.arg_info_all(dbg:DebugInfo)->list[tuple[str, KeyPath]] | None
jax._src.partial_eval.call_padding_rule(prim,in_avals,out_avals,*args,call_jaxpr,**params)
jax._src.partial_eval.call_partial_eval_custom_rule(jaxpr_param_name:str,params_updater:ParamsUpdater,saveable:Callable[...,RematCases_],unks_in:list[bool],inst_in:list[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater,ctx:Callable[[core.ParamDict],AbstractContextManager[None]]=trivial_ctx)->tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], list[Var]]
jax._src.partial_eval.close_jaxpr(jaxpr:Jaxpr)->ClosedJaxpr
jax._src.partial_eval.closed_call_partial_eval_custom_rule(jaxpr_param_name:str,params_updater:ParamsUpdater2,saveable:Callable[...,RematCases_],unks_in:list[bool],inst_in:list[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater)->tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], list[Var]]
jax._src.partial_eval.convert_constvars_jaxpr(jaxpr:Jaxpr)->Jaxpr
jax._src.partial_eval.convert_envvars_to_constvars(jaxpr:Jaxpr,num_env_vars:int)->Jaxpr
jax._src.partial_eval.convert_invars_to_constvars(jaxpr:Jaxpr,n:int)->Jaxpr
jax._src.partial_eval.dce_jaxpr(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[bool]]
jax._src.partial_eval.dce_jaxpr_call_rule(used_outputs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn | None]
jax._src.partial_eval.dce_jaxpr_closed_call_rule(used_outputs:list[bool],eqn:JaxprEqn)->tuple[list[bool], JaxprEqn]
jax._src.partial_eval.dce_jaxpr_consts(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[bool], list[bool]]
jax._src.partial_eval.def_trivial_padding(prim:Primitive)->None
jax._src.partial_eval.ensure_enum(case:bool|RematCases)->RematCases
jax._src.partial_eval.extend_jaxpr_stack(main,frame)
jax._src.partial_eval.identity(x)
jax._src.partial_eval.infer_lambda_input_type(axes_specs:Sequence[AbstractedAxesSpec]|None,args:Sequence[Any])->InputType
jax._src.partial_eval.instantiate_const_at(trace:JaxprTrace,instantiate:bool,tracer)
jax._src.partial_eval.make_jaxpr_effects(constvars,invars,outvars,eqns)->effects.Effects
jax._src.partial_eval.move_binders_to_back(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax._src.partial_eval.move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax._src.partial_eval.new_eqn_recipe(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer],primitive:Primitive,params:dict[str,Any],effects:core.Effects,source_info:source_info_util.SourceInfo)->JaxprEqnRecipe
jax._src.partial_eval.pad_jaxpr(jaxpr:Jaxpr,consts:Sequence[Const])->tuple[Jaxpr, list[Const]]
jax._src.partial_eval.partial_eval_jaxpr_custom(jaxpr:Jaxpr,in_unknowns:Sequence[bool],in_inst:bool|Sequence[bool],ensure_out_unknowns:bool|Sequence[bool],ensure_out_inst:bool|Sequence[bool],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int]
jax._src.partial_eval.partial_eval_jaxpr_custom_rule_not_implemented(name:str,saveable:Callable[...,RematCases_],unks_in:Sequence[bool],inst_in:Sequence[bool],eqn:JaxprEqn)->PartialEvalCustomResult
jax._src.partial_eval.partial_eval_jaxpr_nounits(jaxpr:ClosedJaxpr,unknowns:Sequence[bool],instantiate:bool|Sequence[bool])->tuple[ClosedJaxpr, ClosedJaxpr, list[bool], list[AbstractValue]]
jax._src.partial_eval.partial_eval_jaxpr_stateful(jaxpr:Jaxpr,in_unknowns:Sequence[bool],in_inst:bool|Sequence[bool],ensure_out_unknowns:bool|Sequence[bool],ensure_out_inst:bool|Sequence[bool],saveable:Callable[...,RematCases_])->tuple[Jaxpr, Jaxpr, list[bool], list[bool], int, int]
jax._src.partial_eval.partial_eval_wrapper_nounits(in_knowns:Sequence[bool],in_avals:Sequence[AbstractValue],*in_consts:Any)
jax._src.partial_eval.partition_pvals(pvals:list[PartialVal])->tuple[list[bool], list[AbstractValue], list[Any]]
jax._src.partial_eval.prune_closed_jaxpr_outputs(jaxpr:ClosedJaxpr,used_outputs:Sequence[bool])->ClosedJaxpr
jax._src.partial_eval.prune_jaxpr_outputs(jaxpr:Jaxpr,used_outputs:Sequence[bool])->Jaxpr
jax._src.partial_eval.recipe_to_eqn(getvar:Callable[[JaxprTracer],Atom],recipe:JaxprEqnRecipe)->core.JaxprEqn
jax._src.partial_eval.result_info(dbg:DebugInfo)->list[KeyPath] | None
jax._src.partial_eval.sig_info(dbg:DebugInfo)->inspect.BoundArguments | None
jax._src.partial_eval.trace_to_jaxpr(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[PartialVal], list[core.Value]]
jax._src.partial_eval.trace_to_jaxpr_dynamic(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:DebugInfo|None=None,*,keep_inputs:list[bool]|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.partial_eval.trace_to_jaxpr_dynamic2(fun:lu.WrappedFun,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.partial_eval.trace_to_jaxpr_final(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:DebugInfo|None=None,keep_inputs:Sequence[bool]|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.partial_eval.trace_to_jaxpr_final2(fun:lu.WrappedFun,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.partial_eval.trace_to_jaxpr_nounits(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:bool|Sequence[bool]=False)->tuple[Jaxpr, list[PartialVal], list[core.Value]]
jax._src.partial_eval.trace_to_subjaxpr(main:core.MainTrace,instantiate:bool|Sequence[bool],pvals:Sequence[PartialVal])
jax._src.partial_eval.trace_to_subjaxpr_dynamic(fun:lu.WrappedFun,main:core.MainTrace,in_avals:Sequence[AbstractValue],*,keep_inputs:Sequence[bool]|None=None,debug_info:DebugInfo|None=None)->tuple[Jaxpr, list[AbstractValue], list[Any]]
jax._src.partial_eval.trace_to_subjaxpr_dynamic2(fun:lu.WrappedFun,main:core.MainTrace,debug_info:DebugInfo|None=None)->tuple[Jaxpr, OutputType, list[Any]]
jax._src.partial_eval.trace_to_subjaxpr_nounits(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.partial_eval.trace_to_subjaxpr_nounits_dyn(main:core.MainTrace,in_knowns:Sequence[bool],in_type:InputType,instantiate:bool|Sequence[bool],*in_consts:Any)
jax._src.partial_eval.trace_to_subjaxpr_nounits_fwd(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.partial_eval.trace_to_subjaxpr_nounits_fwd2(main:core.MainTrace,instantiate:bool|Sequence[bool],in_pvals:Sequence[PartialVal])
jax._src.partial_eval.tracers_to_jaxpr(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer])->tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]
jax._src.partial_eval.trivial_ctx(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/interpreters/mlir.py----------------------------------------
A:jax._src.interpreters.mlir.T->typing.TypeVar('T')
A:jax._src.interpreters.mlir.a->numpy.array(0 if a.item() == 0 else 255, np.uint8)
A:jax._src.interpreters.mlir.int1d->aval_to_ir_type(core.ShapedArray((1,), np.int32))
A:jax._src.interpreters.mlir.i32_type->aval_to_ir_type(core.ShapedArray((), np.int32))
A:jax._src.interpreters.mlir.d->jax._src.lib.mlir.dialects.hlo.ConvertOp(i32_type, d)
A:jax._src.interpreters.mlir.ds->map(lower_dim, sizes)
A:jax._src.interpreters.mlir.ctx_new->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).replace(**ctx_override_kwargs)
A:jax._src.interpreters.mlir.out->handler(val)
A:jax._src.interpreters.mlir.dtype->jax._src.dtypes.canonicalize_dtype(index_avals[0].dtype if index_avals else 'int64')
A:jax._src.interpreters.mlir.aval->jax._src.core.ShapedArray((), np.dtype(np.bool_))
A:jax._src.interpreters.mlir.dyn_size->jax._src.lib.mlir.ir.ShapedType.get_dynamic_size()
A:jax._src.interpreters.mlir.types->aval_to_ir_types(aval)
A:jax._src.interpreters.mlir.handler->_constant_handlers.get(t)
A:jax._src.interpreters.mlir.values->ir_constants(val)
A:jax._src.interpreters.mlir.element_type->dtype_to_ir_type(x.dtype)
A:jax._src.interpreters.mlir.x->numpy.ascontiguousarray(x)
A:jax._src.interpreters.mlir.attr->jax._src.lib.mlir.ir.DenseElementsAttr.get(x, type=element_type, shape=shape)
A:jax._src.interpreters.mlir.(zero_stride_axes,)->numpy.where(np.equal(0, val.strides))
A:jax._src.interpreters.mlir.(other_axes,)->numpy.where(np.not_equal(0, val.strides))
A:jax._src.interpreters.mlir.source_file->re.sub(pattern, '', source_file)
A:jax._src.interpreters.mlir.frame->jax._src.source_info_util.user_frame(source_info)
A:jax._src.interpreters.mlir.file_loc->jax._src.lib.mlir.ir.Location.file(get_canonical_source_file(frame), frame.start_line, frame.start_column)
A:jax._src.interpreters.mlir.name_loc->jax._src.lib.mlir.ir.Location.name(frame.function_name, childLoc=file_loc)
A:jax._src.interpreters.mlir.loc->_source_info_to_location(eqn.primitive, eqn.params, source_info, ctx.name_stack)
A:jax._src.interpreters.mlir.context->make_ir_context()
A:jax._src.interpreters.mlir._platform_specific_lowerings->collections.defaultdict(dict)
A:jax._src.interpreters.mlir._module_name_regex->re.compile('[^\\w.-]')
A:jax._src.interpreters.mlir.ctx->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms))
A:jax._src.interpreters.mlir.res->reducer_body(reducer)
A:jax._src.interpreters.mlir.platforms->tuple(map(xb.canonicalize_platform, platforms))
A:jax._src.interpreters.mlir.(input_output_aliases, donated_args)->_set_up_aliases(in_avals, out_avals, donated_args, arg_memory_kinds, result_memory_kinds)
A:jax._src.interpreters.mlir.unlowerable_effects->lowerable_effects.filter_not_in(jaxpr.effects)
A:jax._src.interpreters.mlir.channel_iter->itertools.count(1)
A:jax._src.interpreters.mlir.dim_vars->tuple(sorted(functools.reduce(lambda acc, new: acc.union(new.get_vars()), all_dim_poly, set())))
A:jax._src.interpreters.mlir.module_name->re.compile('[^\\w.-]').sub('_', module_name)
A:jax._src.interpreters.mlir.attrs['sym_name']->jax._src.lib.mlir.ir.StringAttr.get(module_name)
A:jax._src.interpreters.mlir.attrs['mhlo.num_replicas']->i32_attr(num_replicas)
A:jax._src.interpreters.mlir.attrs['mhlo.num_partitions']->i32_attr(num_partitions)
A:jax._src.interpreters.mlir.module_string->module_to_string(ctx.module)
A:jax._src.interpreters.mlir.output->rule(inner_ctx, *rule_args, **rule_kwargs)
A:jax._src.interpreters.mlir.avals_in->map(aval, eqn.invars)
A:jax._src.interpreters.mlir.avals_out->map(strip_metadata, avals_out)
A:jax._src.interpreters.mlir.donations->collections.defaultdict(collections.deque)
A:jax._src.interpreters.mlir.out_donated_args->list(donated_args)
A:jax._src.interpreters.mlir.input_id->donations[key].popleft()
A:jax._src.interpreters.mlir.self._tokens->collections.OrderedDict(*args, **kwargs)
A:jax._src.interpreters.mlir.num_dim_vars->len(ctx.module_context.shape_poly_state.dim_vars)
A:jax._src.interpreters.mlir.dim_var_types->map(aval_to_ir_types, [core.ShapedArray((), dtypes.canonicalize_dtype(np.int64))] * num_dim_vars)
A:jax._src.interpreters.mlir.input_types->map(aval_to_ir_types, ctx.avals_in)
A:jax._src.interpreters.mlir.output_types->map(aval_to_ir_types, ctx.avals_out)
A:jax._src.interpreters.mlir.num_tokens->len(effects)
A:jax._src.interpreters.mlir.flat_input_types->jax._src.util.flatten(input_types)
A:jax._src.interpreters.mlir.flat_output_types->jax._src.util.flatten(output_types)
A:jax._src.interpreters.mlir.ftype->jax._src.lib.mlir.ir.FunctionType.get(flat_input_types, flat_output_types)
A:jax._src.interpreters.mlir.func_op->lower_jaxpr_to_fun(ctx, fn_name, call_jaxpr, effects, arg_names=arg_names, result_names=result_names)
A:jax._src.interpreters.mlir.func_op.attributes['sym_visibility']->jax._src.lib.mlir.ir.StringAttr.get('private')
A:jax._src.interpreters.mlir.ir_arg_shardings->jax._src.util.flatten([[_to_physical_op_sharding(a, s)] * len(types) for (a, s, types) in zip(in_avals, arg_shardings, input_types)])
A:jax._src.interpreters.mlir.ir_arg_memory_kinds->jax._src.util.flatten([[mk] * len(types) for (mk, types) in zip(arg_memory_kinds, input_types)])
A:jax._src.interpreters.mlir.ir_result_shardings->jax._src.util.flatten([[_to_physical_op_sharding(a, s)] * len(types) for (a, s, types) in zip(out_avals, result_shardings, output_types)])
A:jax._src.interpreters.mlir.ir_result_memory_kinds->jax._src.util.flatten([[mk] * len(types) for (mk, types) in zip(result_memory_kinds, output_types)])
A:jax._src.interpreters.mlir.attrs['mhlo.is_same_data_across_replicas']->jax._src.lib.mlir.ir.BoolAttr.get(True)
A:jax._src.interpreters.mlir.attrs['mhlo.sharding']->get_sharding_attr(sharding)
A:jax._src.interpreters.mlir.output_ids->jax._src.util.unflatten(list(range(len(flat_output_types))), map(len, output_types))
A:jax._src.interpreters.mlir.attrs['tf.aliasing_output']->i32_attr(alias)
A:jax._src.interpreters.mlir.attrs['jax.global_constant']->jax._src.lib.mlir.ir.StringAttr.get('')
A:jax._src.interpreters.mlir.attrs['jax.token']->jax._src.lib.mlir.ir.BoolAttr.get(True)
A:jax._src.interpreters.mlir.func_op.arg_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DictAttr.get(attrs) for attrs in arg_attrs])
A:jax._src.interpreters.mlir.attrs['jax.result_info']->jax._src.lib.mlir.ir.StringAttr.get(name_)
A:jax._src.interpreters.mlir.func_op.result_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DictAttr.get(attrs) for attrs in result_attrs])
A:jax._src.interpreters.mlir.entry_block->rw.regions[0].blocks.append(*scalar_types + scalar_types).add_entry_block()
A:jax._src.interpreters.mlir.(dim_var_values, _, _)->jax._src.util.split_list(flat_args, [num_dim_vars, num_tokens])
A:jax._src.interpreters.mlir.entry_lowering_ctx->LoweringRuleContext(module_context=ctx, primitive=None, avals_in=[], avals_out=None, tokens_in=TokenSet.create([]), tokens_out=None, axis_size_env=None, dim_var_values=dim_var_values)
A:jax._src.interpreters.mlir.(_, token_args, unflattened_args)->jax._src.util.split_list(util.unflatten(flat_args, map(len, input_types)), [num_dim_vars, num_tokens])
A:jax._src.interpreters.mlir.tokens_in->tokens.update_tokens(tokens_out).subset(effects)
A:jax._src.interpreters.mlir.callee_name_stack->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).name_stack.extend(util.wrap_name(name, api_name))
A:jax._src.interpreters.mlir.(out_vals, tokens_out)->jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack), jaxpr.jaxpr, tokens_in, consts, *args, dim_var_values=dim_var_values)
A:jax._src.interpreters.mlir.flat_outputs->jax._src.util.flatten(outs)
A:jax._src.interpreters.mlir.result_type->jax._src.lib.mlir.ir.TupleType.get_tuple(result_types)
A:jax._src.interpreters.mlir.op->jax._src.lib.mlir.dialects.hlo.CustomCallOp.build_generic(results=result_types, operands=operands, attributes=attributes)
A:jax._src.interpreters.mlir.mka->get_compute_type(memory_kind)
A:jax._src.interpreters.mlir.op.attributes['mhlo.frontend_attributes']->jax._src.lib.mlir.ir.DictAttr.get(dict_attr)
A:jax._src.interpreters.mlir.effs->list(ctx.tokens_in.effects())
A:jax._src.interpreters.mlir.unflattened_args->jax._src.util.unflatten(entry_block.arguments, map(len, input_types))
A:jax._src.interpreters.mlir.(dim_var_values, token_args, unflattened_args)->jax._src.util.split_list(unflattened_args, [num_dim_vars, len(ctx.tokens_in)])
A:jax._src.interpreters.mlir.sub_ctx->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).replace(tokens_in=TokenSet(zip(effs, token_args)), dim_var_values=dim_var_values)
A:jax._src.interpreters.mlir.outs->lowering_rule(sub_ctx, *_unwrap_singleton_ir_values(unflattened_args))
A:jax._src.interpreters.mlir.env[v]->tuple(node)
A:jax._src.interpreters.mlir.last_used->jax._src.core.last_used(jaxpr)
A:jax._src.interpreters.mlir.in_nodes->map(read, eqn.invars)
A:jax._src.interpreters.mlir.source_info->eqn.source_info.replace(name_stack=ctx.name_stack + eqn.source_info.name_stack)
A:jax._src.interpreters.mlir.override_rule->get_override_lowering_rule(eqn.primitive)
A:jax._src.interpreters.mlir.rule->xla_fallback_lowering(eqn.primitive)
A:jax._src.interpreters.mlir.eqn_ctx->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).replace(name_stack=source_info.name_stack)
A:jax._src.interpreters.mlir.effects->list(tokens_in.effects())
A:jax._src.interpreters.mlir.rule_ctx->rule_ctx.replace(axis_size_env=axis_size_env).replace(axis_size_env=axis_size_env)
A:jax._src.interpreters.mlir.rule_inputs->map(_unwrap_singleton_ir_values, in_nodes)
A:jax._src.interpreters.mlir.ans->lower_multi_platform(rule_ctx, str(eqn), rules, eqn.effects, *rule_inputs, **eqn.params)
A:jax._src.interpreters.mlir.tokens->tokens.update_tokens(tokens_out).update_tokens(tokens_out)
A:jax._src.interpreters.mlir.out_nodes->jax._src.util.unflatten(call.results, map(len, output_types))
A:jax._src.interpreters.mlir.rule_index->len(kept_rules)
A:jax._src.interpreters.mlir.current_platform_idx->jax._src.lib.mlir.dialects.hlo.ConvertOp(i32_type, current_platform_idx)
A:jax._src.interpreters.mlir.rule_idx_op->jax._src.lib.mlir.dialects.hlo.CaseOp([i32_type], index=current_platform_idx, num_branches=len(platforms))
A:jax._src.interpreters.mlir.branch->jax._src.lib.mlir.dialects.hlo.CaseOp(util.flatten(output_types), index=rule_idx_op, num_branches=len(kept_rules)).regions[i].blocks.append()
A:jax._src.interpreters.mlir.ordered_effects->jax._src.effects.ordered_effects.filter_in(effects)
A:jax._src.interpreters.mlir.case_op->jax._src.lib.mlir.dialects.hlo.CaseOp(util.flatten(output_types), index=rule_idx_op, num_branches=len(kept_rules))
A:jax._src.interpreters.mlir.inner_ctx->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).replace()
A:jax._src.interpreters.mlir.(tokens, results)->jax._src.util.split_list(util.unflatten(results, map(len, output_types)), [len(ordered_effects)])
A:jax._src.interpreters.mlir.tokens_out->tokens.update_tokens(tokens_out).subset(effects).update_tokens(TokenSet(zip(effects, tokens)))
A:jax._src.interpreters.mlir.wrapped_fun->jax._src.linear_util.annotate(wrapped_fun, (*implicit_args, *explicit_args))
A:jax._src.interpreters.mlir.i32_aval->jax._src.core.ShapedArray((), np.dtype('int32'))
A:jax._src.interpreters.mlir.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)
A:jax._src.interpreters.mlir.(out, tokens)->jaxpr_subcomp(ctx.module_context, jaxpr, ctx.tokens_in, _ir_consts(consts), *map(wrap_singleton_ir_values, args), dim_var_values=ctx.dim_var_values)
A:jax._src.interpreters.mlir.call_jaxpr->jax._src.core.ClosedJaxpr(call_jaxpr, ())
A:jax._src.interpreters.mlir.call->jax._src.lib.mlir.dialects.func.CallOp(flat_output_types, ir.FlatSymbolRefAttr.get(func.name.value), flatten_lowering_ir_args(args))
A:jax._src.interpreters.mlir.(tokens, out_nodes)->jax._src.util.split_list(out_nodes, [len(effects)])
A:jax._src.interpreters.mlir.(out_nodes, tokens)->_call_lowering(name, name, call_jaxpr, backend, ctx.module_context, ctx.avals_in, ctx.avals_out, ctx.tokens_in, *args, dim_var_values=ctx.dim_var_values)
A:jax._src.interpreters.mlir.physical_aval_out->jax._src.core.physical_aval(aval_out)
A:jax._src.interpreters.mlir.shape->eval_dynamic_shape_as_tensor(ctx, aval_out.shape)
A:jax._src.interpreters.mlir.broadcast_dimensions->list(range(len(out_shape) - len(op_aval_shape), len(out_shape)))
A:jax._src.interpreters.mlir.aval_out->jax._src.core.physical_aval(aval_out)
A:jax._src.interpreters.mlir.start_indices->eval_dynamic_shape_as_tensor(ctx, start_indices)
A:jax._src.interpreters.mlir.limit_indices->eval_dynamic_shape_as_tensor(ctx, limit_indices)
A:jax._src.interpreters.mlir.strides->eval_dynamic_shape_as_tensor(ctx, strides)
A:jax._src.interpreters.mlir.x_aval->jax._src.core.physical_aval(x_aval)
A:jax._src.interpreters.mlir.slice_sizes->eval_dynamic_shape_as_tensor(ctx, slice_sizes)
A:jax._src.interpreters.mlir.clamped_start->jax._src.lib.mlir.dialects.hlo.ClampOp(shape_tensor([0] * len(start_indices)), shape_tensor(start_indices), hlo.SubtractOp(eval_dynamic_shape_as_tensor(ctx, x_aval.shape), slice_sizes))
A:jax._src.interpreters.mlir.padding_low->eval_dynamic_shape_as_tensor(ctx, padding_low)
A:jax._src.interpreters.mlir.padding_high->eval_dynamic_shape_as_tensor(ctx, padding_high)
A:jax._src.interpreters.mlir.padding_interior->eval_dynamic_shape_as_tensor(ctx, padding_interior)
A:jax._src.interpreters.mlir.zero->ir_constant(np.array(value, dtypes.canonicalize_dtype(aval.dtype)))
A:jax._src.interpreters.mlir.tensor_type->jax._src.lib.mlir.ir.RankedTensorType(x.type)
A:jax._src.interpreters.mlir.real_eq->compare_hlo(rx, ry, 'EQ', 'FLOAT')
A:jax._src.interpreters.mlir.real_cmp->compare_hlo(rx, ry, cmp, 'FLOAT')
A:jax._src.interpreters.mlir.imag_cmp->compare_hlo(hlo.ImagOp(x).result, hlo.ImagOp(y).result, cmp, 'FLOAT')
A:jax._src.interpreters.mlir.min_hlo->partial(_minmax_hlo, hlo.MinOp, 'LT')
A:jax._src.interpreters.mlir.max_hlo->partial(_minmax_hlo, hlo.MaxOp, 'GT')
A:jax._src.interpreters.mlir.wrap_with_sharding_op->partial(_wrap_with_spmd_op, 'Sharding')
A:jax._src.interpreters.mlir.wrap_with_full_to_shard_op->partial(_wrap_with_spmd_op, 'SPMDFullToShardShape')
A:jax._src.interpreters.mlir.wrap_with_shard_to_full_op->partial(_wrap_with_spmd_op, 'SPMDShardToFullShape')
A:jax._src.interpreters.mlir.op.attributes['mhlo.sharding']->get_sharding_attr(sharding_proto)
A:jax._src.interpreters.mlir.func->_emit_lowering_rule_as_fun(partial(f, **params), ctx)
A:jax._src.interpreters.mlir.module_str->jax._src.lib.xla_client._xla.mlir.xla_computation_to_mlir_module(xla_computation)
A:jax._src.interpreters.mlir.src_symtab->jax._src.lib.mlir.ir.SymbolTable(src_module.operation)
A:jax._src.interpreters.mlir.dst_symtab->jax._src.lib.mlir.ir.SymbolTable(dst_module.operation)
A:jax._src.interpreters.mlir.used_names->set()
A:jax._src.interpreters.mlir.private->jax._src.lib.mlir.ir.StringAttr.get('private')
A:jax._src.interpreters.mlir.xla_computation->jax._src.interpreters.xla.primitive_subcomputation(module_ctx.platforms[0], axis_env, prim, ctx.avals_in, ctx.avals_out, **params)
A:jax._src.interpreters.mlir.xla_module->xla_computation_to_mlir_module(xla_computation)
A:jax._src.interpreters.mlir.callee_name->merge_mlir_modules(module_ctx.module, f'xla_fallback_{prim.name}', xla_module)
A:jax._src.interpreters.mlir.channel_handle->jax._src.lib.mlir.dialects.hlo.ChannelHandle.get(channel, RECV_FROM_HOST_TYPE)
A:jax._src.interpreters.mlir.send_op->jax._src.lib.mlir.dialects.hlo.SendOp([operand], token, channel_handle, is_host_transfer=ir.BoolAttr.get(True))
A:jax._src.interpreters.mlir.send_op.attributes['mhlo.frontend_attributes']->jax._src.lib.mlir.ir.DictAttr.get(dict(_xla_host_transfer_handler_name=ir.StringAttr.get(str(name)), _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))
A:jax._src.interpreters.mlir.recv_op->jax._src.lib.mlir.dialects.hlo.RecvOp([aval_to_ir_type(out_aval), hlo.TokenType.get()], token, channel_handle, is_host_transfer=ir.BoolAttr.get(True))
A:jax._src.interpreters.mlir.recv_op.attributes['mhlo.frontend_attributes']->jax._src.lib.mlir.ir.DictAttr.get(dict(_xla_host_transfer_handler_name=ir.StringAttr.get(str(name)), _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))
A:jax._src.interpreters.mlir.send_channel->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).module_context.new_channel()
A:jax._src.interpreters.mlir.dummy_send_aval->jax._src.core.ShapedArray((1,), np.float32)
A:jax._src.interpreters.mlir.dummy_send_val->ir_constant(np.zeros(1, np.float32))
A:jax._src.interpreters.mlir.token->send_to_host(channel, token, operand, operand_aval, callback.__name__, sharding=sharding)
A:jax._src.interpreters.mlir.channel->ModuleContext(backend_or_name=backend_or_name, platforms=platforms, axis_context=axis_context, name_stack=name_stack, keepalives=keepalives, channel_iterator=channel_iter, host_callbacks=host_callbacks, lowering_parameters=lowering_parameters, shape_poly_state=ShapePolyLoweringState(dim_vars, lowering_parameters.platforms)).module_context.new_channel()
A:jax._src.interpreters.mlir.(token, out)->receive_from_host(channel, token, result_aval, callback.__name__, sharding=sharding)
A:jax._src.interpreters.mlir.ifrt_callback->backend.make_python_callback_from_host_send_and_recv(_wrapped_callback, operand_shapes, result_shapes, send_channels, recv_channels, pickle_util.dumps)
A:jax._src.interpreters.mlir.layout->numpy.array(minor_to_major, dtype='int64')
A:jax._src.interpreters.mlir.result_shapes->jax._src.util.flatten([xla.aval_to_xla_shapes(result_aval) for result_aval in result_avals])
A:jax._src.interpreters.mlir.operand_shapes->jax._src.util.flatten([xla.aval_to_xla_shapes(op_aval) for op_aval in operand_avals])
A:jax._src.interpreters.mlir.operand_layouts->jax._src.util.concatenate(map(_aval_to_default_layouts, operand_avals))
A:jax._src.interpreters.mlir.operand_mlir_layouts->map(_layout_to_mlir_layout, operand_layouts)
A:jax._src.interpreters.mlir.result_layouts->jax._src.util.concatenate(map(_aval_to_default_layouts, result_avals))
A:jax._src.interpreters.mlir.result_mlir_layouts->map(_layout_to_mlir_layout, result_layouts)
A:jax._src.interpreters.mlir.out_vals->callback(*args)
A:jax._src.interpreters.mlir.non_empty_out_vals->tuple((out_val for (out_val, result_aval) in zip(out_vals, result_avals) if not is_empty_shape(result_aval.shape)))
A:jax._src.interpreters.mlir.(non_empty_result_avals, non_empty_result_shapes)->jax._src.util.unzip2([(aval, shape) for (aval, shape) in zip(result_avals, result_shapes) if not is_empty_shape(aval.shape)])
A:jax._src.interpreters.mlir.(non_empty_outputs, token)->_emit_tpu_python_callback(backend, ctx, _wrapped_callback, token, operands, operand_avals, operand_shapes, non_empty_result_avals, non_empty_result_shapes, sharding=sharding)
A:jax._src.interpreters.mlir.non_empty_outputs_iter->iter(non_empty_outputs)
A:jax._src.interpreters.mlir.result_types->jax._src.util.flatten([aval_to_ir_types(aval) for aval in result_avals])
A:jax._src.interpreters.mlir.(callback_descriptor, ifrt_callback)->backend.get_emit_python_callback_descriptor(_wrapped_callback, operand_shapes, result_shapes)
A:jax._src.interpreters.mlir.descriptor_operand->ir_constant(callback_descriptor)
A:jax._src.interpreters.mlir.result->jax._src.lib.mlir.dialects.hlo.CustomCallOp([result_type], callback_operands, call_target_name=ir.StringAttr.get(call_target_name), has_side_effect=ir.BoolAttr.get(has_side_effect), api_version=i32_attr(2), called_computations=ir.ArrayAttr.get([]), backend_config=ir.StringAttr.get(str(callback_descriptor)), operand_layouts=None if operand_mlir_layouts is None else ir.ArrayAttr.get(operand_mlir_layouts), result_layouts=None if result_mlir_layouts is None else ir.ArrayAttr.get(result_mlir_layouts))
A:jax._src.interpreters.mlir.lowering_result->lower_jaxpr_to_module(name, closed_jaxpr, backend_or_name=backend_or_name, ordered_effects=[], name_stack=source_info_util.NameStack(), donated_args=[False] * len(closed_jaxpr.jaxpr.invars), axis_context=axis_context, platforms=platforms, lowering_parameters=LoweringParameters())
A:jax._src.interpreters.mlir.operands->list(operands)
A:jax._src.interpreters.mlir.backend_config_attr->jax._src.lib.mlir.ir.DictAttr.get(backend_config)
A:jax._src.interpreters.mlir.attributes->dict(call_target_name=ir.StringAttr.get(call_target_name), has_side_effect=ir.BoolAttr.get(has_side_effect), backend_config=backend_config_attr, api_version=i32_attr(api_version), called_computations=ir.ArrayAttr.get([ir.FlatSymbolRefAttr.get(name) for name in called_computations]))
A:jax._src.interpreters.mlir.attributes['output_operand_aliases']->jax._src.lib.mlir.ir.ArrayAttr.get([hlo.OutputOperandAlias.get(output_tuple_indices=[output_idx] if len(result_types) > 1 else [], operand_index=input_idx, operand_tuple_indices=[]) for (input_idx, output_idx) in operand_output_aliases.items() or ()])
A:jax._src.interpreters.mlir.attributes['indices_of_shape_operands']->jax._src.lib.mlir.ir.DenseIntElementsAttr.get(np.asarray(list(range(len(operands), len(operands) + len(result_shapes))), dtype=np.int64))
A:jax._src.interpreters.mlir.attributes['operand_layouts']->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DenseIntElementsAttr.get(np.atleast_1d(np.asarray(l, dtype=np.int64)), type=ir.IndexType.get()) for l in operand_layouts])
A:jax._src.interpreters.mlir.attributes['result_layouts']->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DenseIntElementsAttr.get(np.atleast_1d(np.asarray(l, dtype=np.int64)), type=ir.IndexType.get()) for l in result_layouts])
A:jax._src.interpreters.mlir.int2d->aval_to_ir_type(core.ShapedArray((1, 2), np.int32))
A:jax._src.interpreters.mlir.pads->eval_dynamic_shape_as_tensor(ctx, pad_lo_hi)
A:jax._src.interpreters.mlir.reducer_type->jax._src.lib.mlir.ir.FunctionType.get(scalar_types + scalar_types, scalar_types)
A:jax._src.interpreters.mlir.reducer->jax._src.lib.mlir.dialects.hlo.ReduceWindowOp(list(map(aval_to_ir_type, out_avals)), operands, init_values, dense_int_elements(window_dimensions), window_strides=dense_int_elements(window_strides), base_dilations=dense_int_elements(base_dilation), window_dilations=dense_int_elements(window_dilation), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).regions[0].blocks.append(*scalar_types + scalar_types)
A:jax._src.interpreters.mlir.rw->jax._src.lib.mlir.dialects.hlo.ReduceWindowOp(list(map(aval_to_ir_type, out_avals)), operands, init_values, dense_int_elements(window_dimensions), window_strides=dense_int_elements(window_strides), base_dilations=dense_int_elements(base_dilation), window_dilations=dense_int_elements(window_dilation), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2)))
A:jax._src.interpreters.mlir.refined_module_str->jax._src.lib.xla_extension.mlir.refine_polymorphic_shapes(module_to_bytecode(module), enable_shape_assertions=True, validate_static_shapes=True)
jax._src.interpreters.mlir.ConstantHandler(self,val:Any)
jax._src.interpreters.mlir.ConstantHandler.__call__(self,val:Any)
jax._src.interpreters.mlir.LoweringParameters
jax._src.interpreters.mlir.LoweringResult(NamedTuple)
jax._src.interpreters.mlir.LoweringRuleContext
jax._src.interpreters.mlir.LoweringRuleContext.replace(self,**kw)
jax._src.interpreters.mlir.LoweringRuleContext.set_tokens_out(self,tokens_out:TokenSet)
jax._src.interpreters.mlir.ModuleContext(self,*,backend_or_name:str|xb.XlaBackend|None,platforms:Sequence[str],axis_context:AxisContext,name_stack:source_info_util.NameStack,keepalives:list[Any],channel_iterator:Iterator[int],host_callbacks:list[Any],lowering_parameters:LoweringParameters,context:ir.Context|None=None,module:ir.Module|None=None,ip:ir.InsertionPoint|None=None,symbol_table:ir.SymbolTable|None=None,cached_primitive_lowerings:None|dict[Any,func_dialect.FuncOp]=None,shape_poly_state=None)
jax._src.interpreters.mlir.ModuleContext.__init__(self,*,backend_or_name:str|xb.XlaBackend|None,platforms:Sequence[str],axis_context:AxisContext,name_stack:source_info_util.NameStack,keepalives:list[Any],channel_iterator:Iterator[int],host_callbacks:list[Any],lowering_parameters:LoweringParameters,context:ir.Context|None=None,module:ir.Module|None=None,ip:ir.InsertionPoint|None=None,symbol_table:ir.SymbolTable|None=None,cached_primitive_lowerings:None|dict[Any,func_dialect.FuncOp]=None,shape_poly_state=None)
jax._src.interpreters.mlir.ModuleContext.add_host_callback(self,host_callback:Any)->None
jax._src.interpreters.mlir.ModuleContext.add_keepalive(self,keepalive:Any)->None
jax._src.interpreters.mlir.ModuleContext.axis_env(self)->sharding_impls.AxisEnv
jax._src.interpreters.mlir.ModuleContext.backend(self)->xb.XlaBackend
jax._src.interpreters.mlir.ModuleContext.new_channel(self)->int
jax._src.interpreters.mlir.ModuleContext.replace(self,**kw)
jax._src.interpreters.mlir.ShapePolyLoweringState(self,dim_vars:tuple[str,...],lowering_platforms:tuple[str,...]|None)
jax._src.interpreters.mlir.ShapePolyLoweringState.__init__(self,dim_vars:tuple[str,...],lowering_platforms:tuple[str,...]|None)
jax._src.interpreters.mlir.TokenSet(self,*args,**kwargs)
jax._src.interpreters.mlir.TokenSet.__init__(self,*args,**kwargs)
jax._src.interpreters.mlir.TokenSet.__len__(self)
jax._src.interpreters.mlir.TokenSet.create(cls,effects:Sequence[core.Effect])->TokenSet
jax._src.interpreters.mlir.TokenSet.effects(self)->set[core.Effect]
jax._src.interpreters.mlir.TokenSet.get(self,effect:core.Effect)->Token
jax._src.interpreters.mlir.TokenSet.items(self)->Sequence[tuple[core.Effect, Token]]
jax._src.interpreters.mlir.TokenSet.subset(self,effects:Sequence[core.Effect])->TokenSet
jax._src.interpreters.mlir.TokenSet.update_tokens(self,tokens:TokenSet)->TokenSet
jax._src.interpreters.mlir._array_ir_types(aval:core.ShapedArray|core.DShapedArray)->Sequence[ir.Type]
jax._src.interpreters.mlir._aval_to_default_layouts(aval)
jax._src.interpreters.mlir._call_lowering(fn_name,stack_name,call_jaxpr,backend,ctx:ModuleContext,avals_in,avals_out,tokens_in,*args,dim_var_values:Sequence[ir.Value],arg_names=None,result_names=None)
jax._src.interpreters.mlir._dtype_to_xla_type_string(dtype:np.dtype)->str
jax._src.interpreters.mlir._dynamic_array_ir_types(aval:core.ShapedArray)->Sequence[ir.Type]
jax._src.interpreters.mlir._emit_lowering_rule_as_fun(lowering_rule,ctx:LoweringRuleContext)->func_dialect.FuncOp
jax._src.interpreters.mlir._emit_tpu_python_callback(backend:xb.XlaBackend,ctx:LoweringRuleContext,callback,token:Any|None,operands:Sequence[ir.Value],operand_avals:Sequence[core.ShapedArray],operand_shapes:Sequence[xc.Shape],result_avals:Sequence[core.ShapedArray],result_shapes:Sequence[xc.Shape],*,sharding:xc.OpSharding|None=None)->tuple[Sequence[ir.Value], Any]
jax._src.interpreters.mlir._get_mem_kind(s:Optional[XLACompatibleSharding])->Optional[str]
jax._src.interpreters.mlir._ir_consts(consts)
jax._src.interpreters.mlir._layout_to_mlir_layout(minor_to_major:Sequence[int]|None)
jax._src.interpreters.mlir._lower_jaxpr_to_fun_cached(ctx,fn_name,call_jaxpr,effects,arg_names=None,result_names=None)
jax._src.interpreters.mlir._masked_array_constant_handler(*args,**kwargs)
jax._src.interpreters.mlir._minmax_hlo(op,cmp,x,y)
jax._src.interpreters.mlir._ndarray_constant_handler(val:np.ndarray)->Sequence[ir.Value]
jax._src.interpreters.mlir._numpy_array_constant(x:np.ndarray)->Sequence[ir.Value]
jax._src.interpreters.mlir._python_scalar_handler(dtype,val)
jax._src.interpreters.mlir._set_up_aliases(avals_in,avals_out,donated_args,arg_memory_kinds,result_memory_kinds)
jax._src.interpreters.mlir._source_info_to_location(primitive:core.Primitive,params:dict,source_info:source_info_util.SourceInfo,name_stack:source_info_util.NameStack)->ir.Location
jax._src.interpreters.mlir._to_logical_op_sharding(aval:core.AbstractValue,sharding:XLACompatibleSharding|None)->xc.HloSharding | None
jax._src.interpreters.mlir._to_physical_op_sharding(aval:core.AbstractValue|None,sharding:xc.HloSharding|None)->xc.OpSharding | None
jax._src.interpreters.mlir._token_constant_handler(val)
jax._src.interpreters.mlir._traceback_to_location(tb:xc.Traceback)->ir.Location
jax._src.interpreters.mlir._unwrap_singleton_ir_values(x)
jax._src.interpreters.mlir._wrap_with_spmd_op(name:str,ctx:LoweringRuleContext,x:ir.Value,aval_out:core.AbstractValue,sharding_proto:xc.OpSharding,unspecified_dims:set[int]|None=None)
jax._src.interpreters.mlir.add_jaxvals_lowering(ctx,x,y)
jax._src.interpreters.mlir.aval_to_ir_type(aval:core.AbstractValue)->ir.Type
jax._src.interpreters.mlir.aval_to_ir_types(aval:core.AbstractValue)->Sequence[ir.Type]
jax._src.interpreters.mlir.broadcast_in_dim(ctx:LoweringRuleContext,op,aval_out:core.AbstractValue,*,broadcast_dimensions)->ir.Value
jax._src.interpreters.mlir.build_xla_computation_helper(closed_jaxpr:core.ClosedJaxpr,*,name:str,platforms:Sequence[str],backend_or_name:str,axis_context:AxisContext)->xc.XlaComputation
jax._src.interpreters.mlir.cache_lowering(f)
jax._src.interpreters.mlir.check_backend_matches(inner_backend:Optional[str],lowering_platforms:Sequence[str])
jax._src.interpreters.mlir.compare_hlo(x,y,direction:str,comparison_type:str|None=None)
jax._src.interpreters.mlir.convert_hlo(ctx:LoweringRuleContext,x,aval_in,aval_out)
jax._src.interpreters.mlir.core_call_lowering(ctx:LoweringRuleContext,*args,name,backend=None,call_jaxpr)
jax._src.interpreters.mlir.create_token()->Token
jax._src.interpreters.mlir.custom_call(call_target_name:str,*,result_types:Sequence[ir.Type],operands:Sequence[ir.Value],backend_config:str|bytes|dict[str,ir.Attribute]='',has_side_effect:bool=False,result_shapes:Sequence[ir.Value]|None=None,called_computations:Sequence[str]=(),api_version:int=2,operand_output_aliases:dict[int,int]|None=None,operand_layouts:Sequence[Sequence[int]]|None=None,result_layouts:Sequence[Sequence[int]]|None=None,extra_attributes:dict[str,ir.Attribute]|None=None)->ir.Operation
jax._src.interpreters.mlir.delegate_lowering(ctx,lowering_fun,*args,**ctx_override_kwargs)
jax._src.interpreters.mlir.dense_bool_elements(xs:Sequence[bool])->ir.DenseElementsAttr
jax._src.interpreters.mlir.dense_int_elements(xs)->ir.DenseIntElementsAttr
jax._src.interpreters.mlir.dtype_to_ir_type(dtype:core.bint|np.dtype|np.generic)->ir.Type
jax._src.interpreters.mlir.dummy_token()->Sequence[ir.Value]
jax._src.interpreters.mlir.dummy_token_type()->Sequence[ir.Type]
jax._src.interpreters.mlir.dynamic_slice(ctx:LoweringRuleContext,aval_out,x,*,start_indices)->ir.Value
jax._src.interpreters.mlir.dynamic_update_slice(ctx:LoweringRuleContext,aval_out,x,update,*,start_indices)->ir.Value
jax._src.interpreters.mlir.emit_python_callback(ctx:LoweringRuleContext,callback,token:Any|None,operands:Sequence[ir.Value],operand_avals:Sequence[core.ShapedArray],result_avals:Sequence[core.ShapedArray],has_side_effect:bool,*,sharding:xc.OpSharding|None=None,operand_layouts:Sequence[Sequence[int]|None]|None=None,result_layouts:Sequence[Sequence[int]|None]|None=None)->tuple[Sequence[ir.Value], Any, Any]
jax._src.interpreters.mlir.eval_dynamic_shape(ctx:LoweringRuleContext,shape:core.Shape)->tuple[int | Value, ...]
jax._src.interpreters.mlir.eval_dynamic_shape_as_ivals(ctx:LoweringRuleContext,shape:core.Shape)->tuple[int | Value, ...]
jax._src.interpreters.mlir.eval_dynamic_shape_as_tensor(ctx:LoweringRuleContext,shape:core.Shape)->Value
jax._src.interpreters.mlir.eval_dynamic_shape_as_vals(ctx:LoweringRuleContext,shape:core.Shape)->tuple[Value, ...]
jax._src.interpreters.mlir.flatten_lowering_ir_args(xs:Sequence[ir.Value|Sequence[ir.Value]])->Sequence[Sequence[ir.Value]]
jax._src.interpreters.mlir.full_like_aval(ctx:LoweringRuleContext,value,aval:core.ShapedArray)->ir.Value
jax._src.interpreters.mlir.get_canonical_source_file(frame:source_info_util.Frame)->str
jax._src.interpreters.mlir.get_compute_type(memory_kind:str)->str
jax._src.interpreters.mlir.get_constant_handler(type_:type)->ConstantHandler
jax._src.interpreters.mlir.get_sharding_attr(sharding_proto:xc.OpSharding)
jax._src.interpreters.mlir.i32_attr(i)
jax._src.interpreters.mlir.i64_attr(i)
jax._src.interpreters.mlir.iota(ctx:LoweringRuleContext,aval_out,*,dimension:int)
jax._src.interpreters.mlir.ir_constant(val:Any)->ir.Value
jax._src.interpreters.mlir.ir_constants(val:Any)->Sequence[ir.Value]
jax._src.interpreters.mlir.is_empty_shape(s:core.Shape)->bool
jax._src.interpreters.mlir.jaxpr_subcomp(ctx:ModuleContext,jaxpr:core.Jaxpr,tokens:TokenSet,consts:Sequence[Sequence[ir.Value]],*args:Sequence[ir.Value],dim_var_values:Sequence[ir.Value])->tuple[Sequence[Sequence[ir.Value]], TokenSet]
jax._src.interpreters.mlir.lower_fun(fun:Callable,multiple_results:bool=True)->Callable
jax._src.interpreters.mlir.lower_jaxpr_to_fun(ctx:ModuleContext,name:str,jaxpr:core.ClosedJaxpr,effects:Sequence[core.Effect],*,create_tokens:bool=False,public:bool=False,replace_tokens_with_dummy:bool=False,replicated_args:Sequence[bool]|None=None,arg_shardings:Sequence[xc.HloSharding|None]|None=None,result_shardings:Sequence[xc.HloSharding|None]|None=None,use_sharding_annotations:bool=True,input_output_aliases:Sequence[int|None]|None=None,num_output_tokens:int=0,api_name:str='jit',arg_names:Sequence[str|None]|None=None,result_names:Sequence[str|None]|None=None,arg_memory_kinds:Sequence[str|None]|None=None,result_memory_kinds:Sequence[str|None]|None=None)->func_dialect.FuncOp
jax._src.interpreters.mlir.lower_jaxpr_to_module(module_name:str,jaxpr:core.ClosedJaxpr,*,ordered_effects:list[core.Effect],backend_or_name:str|xb.XlaBackend|None,platforms:Sequence[str],axis_context:AxisContext,name_stack:source_info_util.NameStack,donated_args:Sequence[bool],replicated_args:Sequence[bool]|None=None,arg_shardings:Sequence[XLACompatibleSharding|None]|None=None,result_shardings:Sequence[XLACompatibleSharding|None]|None=None,arg_names:Sequence[str|None]|None=None,result_names:Sequence[str|None]|None=None,num_replicas:int=1,num_partitions:int=1,all_default_mem_kind:bool=True,lowering_parameters:LoweringParameters)->LoweringResult
jax._src.interpreters.mlir.lower_multi_platform(ctx:LoweringRuleContext,description:str,rules:Sequence[MultiPlatformLoweringRule],effects:effects_lib.Effects,*rule_args:ir.Value,**rule_kwargs)->ir.Value
jax._src.interpreters.mlir.make_ir_context()->ir.Context
jax._src.interpreters.mlir.merge_mlir_modules(dst_module:ir.Module,sym_name:str,src_module:ir.Module)->str
jax._src.interpreters.mlir.module_to_bytecode(module:ir.Module)->bytes
jax._src.interpreters.mlir.module_to_string(module:ir.Module)->str
jax._src.interpreters.mlir.multi_broadcast_in_dim(ctx:LoweringRuleContext,ops:Sequence[ir.Value],ops_avals:Sequence[core.AbstractValue],out_shape:core.Shape)->Sequence[ir.Value]
jax._src.interpreters.mlir.pad(ctx:LoweringRuleContext,aval_out,x,padding_value,padding_low,padding_high,padding_interior)->ir.Value
jax._src.interpreters.mlir.receive_from_host(channel:int,token:hlo.TokenType,out_aval:core.ShapedArray,name:str,*,sharding:xc.OpSharding|None=None)->ir.Value
jax._src.interpreters.mlir.reduce_window(ctx:LoweringRuleContext,*,reducer_name:str,reducer_body:Callable[[ir.Block],Sequence[ir.Value]],operands:Sequence[ir.Value],init_values:Sequence[ir.Value],init_values_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.interpreters.mlir.refine_polymorphic_shapes(module:ir.Module)->ir.Module
jax._src.interpreters.mlir.register_constant_handler(type_:type,handler_fun:ConstantHandler)
jax._src.interpreters.mlir.register_lowering(prim:core.Primitive,rule:LoweringRule,platform:str|None=None)
jax._src.interpreters.mlir.reshape(ctx:LoweringRuleContext,op,aval_out:core.AbstractValue)->ir.Value
jax._src.interpreters.mlir.send_to_host(channel:int,token:hlo.TokenType,operand:Any,aval:core.ShapedArray,name:str,*,sharding:xc.OpSharding|None=None)->ir.Value
jax._src.interpreters.mlir.set_sharding(op,sharding_proto:xc.OpSharding)
jax._src.interpreters.mlir.shape_tensor(sizes:Sequence[int|ir.RankedTensorType])->ir.RankedTensorType
jax._src.interpreters.mlir.sharded_aval(aval:core.AbstractValue,sharding:XLACompatibleSharding|None)->core.AbstractValue
jax._src.interpreters.mlir.slice_op(ctx:LoweringRuleContext,x,aval_out,*,start_indices,limit_indices,strides)->ir.Value
jax._src.interpreters.mlir.token_type()->Sequence[ir.Type]
jax._src.interpreters.mlir.wrap_singleton_ir_values(x:ir.Value|Sequence[ir.Value])->Sequence[ir.Value]
jax._src.interpreters.mlir.wrap_with_memory_kind(x:ir.Value,memory_kind:str,aval_out:core.AbstractValue,is_input:bool=False)->ir.Value
jax._src.interpreters.mlir.xla_computation_to_mlir_module(xla_computation:xc.XlaComputation)->ir.Module
jax._src.interpreters.mlir.xla_fallback_lowering(prim:core.Primitive)
jax._src.interpreters.mlir.zeros_like_lowering(ctx,x)
jax._src.mlir.ConstantHandler(self,val:Any)
jax._src.mlir.LoweringParameters
jax._src.mlir.LoweringResult(NamedTuple)
jax._src.mlir.LoweringRuleContext
jax._src.mlir.LoweringRuleContext.replace(self,**kw)
jax._src.mlir.LoweringRuleContext.set_tokens_out(self,tokens_out:TokenSet)
jax._src.mlir.ModuleContext(self,*,backend_or_name:str|xb.XlaBackend|None,platforms:Sequence[str],axis_context:AxisContext,name_stack:source_info_util.NameStack,keepalives:list[Any],channel_iterator:Iterator[int],host_callbacks:list[Any],lowering_parameters:LoweringParameters,context:ir.Context|None=None,module:ir.Module|None=None,ip:ir.InsertionPoint|None=None,symbol_table:ir.SymbolTable|None=None,cached_primitive_lowerings:None|dict[Any,func_dialect.FuncOp]=None,shape_poly_state=None)
jax._src.mlir.ModuleContext.add_host_callback(self,host_callback:Any)->None
jax._src.mlir.ModuleContext.add_keepalive(self,keepalive:Any)->None
jax._src.mlir.ModuleContext.axis_env(self)->sharding_impls.AxisEnv
jax._src.mlir.ModuleContext.backend(self)->xb.XlaBackend
jax._src.mlir.ModuleContext.replace(self,**kw)
jax._src.mlir.ShapePolyLoweringState(self,dim_vars:tuple[str,...],lowering_platforms:tuple[str,...]|None)
jax._src.mlir.TokenSet(self,*args,**kwargs)
jax._src.mlir.TokenSet.__len__(self)
jax._src.mlir.TokenSet.create(cls,effects:Sequence[core.Effect])->TokenSet
jax._src.mlir.TokenSet.effects(self)->set[core.Effect]
jax._src.mlir.TokenSet.get(self,effect:core.Effect)->Token
jax._src.mlir.TokenSet.items(self)->Sequence[tuple[core.Effect, Token]]
jax._src.mlir.TokenSet.subset(self,effects:Sequence[core.Effect])->TokenSet
jax._src.mlir.TokenSet.update_tokens(self,tokens:TokenSet)->TokenSet
jax._src.mlir._array_ir_types(aval:core.ShapedArray|core.DShapedArray)->Sequence[ir.Type]
jax._src.mlir._aval_to_default_layouts(aval)
jax._src.mlir._call_lowering(fn_name,stack_name,call_jaxpr,backend,ctx:ModuleContext,avals_in,avals_out,tokens_in,*args,dim_var_values:Sequence[ir.Value],arg_names=None,result_names=None)
jax._src.mlir._dtype_to_xla_type_string(dtype:np.dtype)->str
jax._src.mlir._dynamic_array_ir_types(aval:core.ShapedArray)->Sequence[ir.Type]
jax._src.mlir._emit_lowering_rule_as_fun(lowering_rule,ctx:LoweringRuleContext)->func_dialect.FuncOp
jax._src.mlir._emit_tpu_python_callback(backend:xb.XlaBackend,ctx:LoweringRuleContext,callback,token:Any|None,operands:Sequence[ir.Value],operand_avals:Sequence[core.ShapedArray],operand_shapes:Sequence[xc.Shape],result_avals:Sequence[core.ShapedArray],result_shapes:Sequence[xc.Shape],*,sharding:xc.OpSharding|None=None)->tuple[Sequence[ir.Value], Any]
jax._src.mlir._get_mem_kind(s:Optional[XLACompatibleSharding])->Optional[str]
jax._src.mlir._ir_consts(consts)
jax._src.mlir._layout_to_mlir_layout(minor_to_major:Sequence[int]|None)
jax._src.mlir._lower_jaxpr_to_fun_cached(ctx,fn_name,call_jaxpr,effects,arg_names=None,result_names=None)
jax._src.mlir._masked_array_constant_handler(*args,**kwargs)
jax._src.mlir._minmax_hlo(op,cmp,x,y)
jax._src.mlir._ndarray_constant_handler(val:np.ndarray)->Sequence[ir.Value]
jax._src.mlir._python_scalar_handler(dtype,val)
jax._src.mlir._set_up_aliases(avals_in,avals_out,donated_args,arg_memory_kinds,result_memory_kinds)
jax._src.mlir._source_info_to_location(primitive:core.Primitive,params:dict,source_info:source_info_util.SourceInfo,name_stack:source_info_util.NameStack)->ir.Location
jax._src.mlir._token_constant_handler(val)
jax._src.mlir._traceback_to_location(tb:xc.Traceback)->ir.Location
jax._src.mlir._unwrap_singleton_ir_values(x)
jax._src.mlir._wrap_with_spmd_op(name:str,ctx:LoweringRuleContext,x:ir.Value,aval_out:core.AbstractValue,sharding_proto:xc.OpSharding,unspecified_dims:set[int]|None=None)
jax._src.mlir.add_jaxvals_lowering(ctx,x,y)
jax._src.mlir.aval_to_ir_type(aval:core.AbstractValue)->ir.Type
jax._src.mlir.aval_to_ir_types(aval:core.AbstractValue)->Sequence[ir.Type]
jax._src.mlir.broadcast_in_dim(ctx:LoweringRuleContext,op,aval_out:core.AbstractValue,*,broadcast_dimensions)->ir.Value
jax._src.mlir.build_xla_computation_helper(closed_jaxpr:core.ClosedJaxpr,*,name:str,platforms:Sequence[str],backend_or_name:str,axis_context:AxisContext)->xc.XlaComputation
jax._src.mlir.cache_lowering(f)
jax._src.mlir.check_backend_matches(inner_backend:Optional[str],lowering_platforms:Sequence[str])
jax._src.mlir.compare_hlo(x,y,direction:str,comparison_type:str|None=None)
jax._src.mlir.convert_hlo(ctx:LoweringRuleContext,x,aval_in,aval_out)
jax._src.mlir.core_call_lowering(ctx:LoweringRuleContext,*args,name,backend=None,call_jaxpr)
jax._src.mlir.create_token()->Token
jax._src.mlir.custom_call(call_target_name:str,*,result_types:Sequence[ir.Type],operands:Sequence[ir.Value],backend_config:str|bytes|dict[str,ir.Attribute]='',has_side_effect:bool=False,result_shapes:Sequence[ir.Value]|None=None,called_computations:Sequence[str]=(),api_version:int=2,operand_output_aliases:dict[int,int]|None=None,operand_layouts:Sequence[Sequence[int]]|None=None,result_layouts:Sequence[Sequence[int]]|None=None,extra_attributes:dict[str,ir.Attribute]|None=None)->ir.Operation
jax._src.mlir.delegate_lowering(ctx,lowering_fun,*args,**ctx_override_kwargs)
jax._src.mlir.dense_bool_elements(xs:Sequence[bool])->ir.DenseElementsAttr
jax._src.mlir.dense_int_elements(xs)->ir.DenseIntElementsAttr
jax._src.mlir.dtype_to_ir_type(dtype:core.bint|np.dtype|np.generic)->ir.Type
jax._src.mlir.dummy_token()->Sequence[ir.Value]
jax._src.mlir.dummy_token_type()->Sequence[ir.Type]
jax._src.mlir.dynamic_slice(ctx:LoweringRuleContext,aval_out,x,*,start_indices)->ir.Value
jax._src.mlir.dynamic_update_slice(ctx:LoweringRuleContext,aval_out,x,update,*,start_indices)->ir.Value
jax._src.mlir.emit_python_callback(ctx:LoweringRuleContext,callback,token:Any|None,operands:Sequence[ir.Value],operand_avals:Sequence[core.ShapedArray],result_avals:Sequence[core.ShapedArray],has_side_effect:bool,*,sharding:xc.OpSharding|None=None,operand_layouts:Sequence[Sequence[int]|None]|None=None,result_layouts:Sequence[Sequence[int]|None]|None=None)->tuple[Sequence[ir.Value], Any, Any]
jax._src.mlir.eval_dynamic_shape(ctx:LoweringRuleContext,shape:core.Shape)->tuple[int | Value, ...]
jax._src.mlir.eval_dynamic_shape_as_ivals(ctx:LoweringRuleContext,shape:core.Shape)->tuple[int | Value, ...]
jax._src.mlir.eval_dynamic_shape_as_tensor(ctx:LoweringRuleContext,shape:core.Shape)->Value
jax._src.mlir.eval_dynamic_shape_as_vals(ctx:LoweringRuleContext,shape:core.Shape)->tuple[Value, ...]
jax._src.mlir.flatten_lowering_ir_args(xs:Sequence[ir.Value|Sequence[ir.Value]])->Sequence[Sequence[ir.Value]]
jax._src.mlir.full_like_aval(ctx:LoweringRuleContext,value,aval:core.ShapedArray)->ir.Value
jax._src.mlir.get_canonical_source_file(frame:source_info_util.Frame)->str
jax._src.mlir.get_compute_type(memory_kind:str)->str
jax._src.mlir.get_constant_handler(type_:type)->ConstantHandler
jax._src.mlir.i32_attr(i)
jax._src.mlir.i64_attr(i)
jax._src.mlir.iota(ctx:LoweringRuleContext,aval_out,*,dimension:int)
jax._src.mlir.ir_constant(val:Any)->ir.Value
jax._src.mlir.ir_constants(val:Any)->Sequence[ir.Value]
jax._src.mlir.is_empty_shape(s:core.Shape)->bool
jax._src.mlir.jaxpr_subcomp(ctx:ModuleContext,jaxpr:core.Jaxpr,tokens:TokenSet,consts:Sequence[Sequence[ir.Value]],*args:Sequence[ir.Value],dim_var_values:Sequence[ir.Value])->tuple[Sequence[Sequence[ir.Value]], TokenSet]
jax._src.mlir.lower_fun(fun:Callable,multiple_results:bool=True)->Callable
jax._src.mlir.lower_jaxpr_to_fun(ctx:ModuleContext,name:str,jaxpr:core.ClosedJaxpr,effects:Sequence[core.Effect],*,create_tokens:bool=False,public:bool=False,replace_tokens_with_dummy:bool=False,replicated_args:Sequence[bool]|None=None,arg_shardings:Sequence[xc.HloSharding|None]|None=None,result_shardings:Sequence[xc.HloSharding|None]|None=None,use_sharding_annotations:bool=True,input_output_aliases:Sequence[int|None]|None=None,num_output_tokens:int=0,api_name:str='jit',arg_names:Sequence[str|None]|None=None,result_names:Sequence[str|None]|None=None,arg_memory_kinds:Sequence[str|None]|None=None,result_memory_kinds:Sequence[str|None]|None=None)->func_dialect.FuncOp
jax._src.mlir.lower_jaxpr_to_module(module_name:str,jaxpr:core.ClosedJaxpr,*,ordered_effects:list[core.Effect],backend_or_name:str|xb.XlaBackend|None,platforms:Sequence[str],axis_context:AxisContext,name_stack:source_info_util.NameStack,donated_args:Sequence[bool],replicated_args:Sequence[bool]|None=None,arg_shardings:Sequence[XLACompatibleSharding|None]|None=None,result_shardings:Sequence[XLACompatibleSharding|None]|None=None,arg_names:Sequence[str|None]|None=None,result_names:Sequence[str|None]|None=None,num_replicas:int=1,num_partitions:int=1,all_default_mem_kind:bool=True,lowering_parameters:LoweringParameters)->LoweringResult
jax._src.mlir.lower_multi_platform(ctx:LoweringRuleContext,description:str,rules:Sequence[MultiPlatformLoweringRule],effects:effects_lib.Effects,*rule_args:ir.Value,**rule_kwargs)->ir.Value
jax._src.mlir.make_ir_context()->ir.Context
jax._src.mlir.merge_mlir_modules(dst_module:ir.Module,sym_name:str,src_module:ir.Module)->str
jax._src.mlir.module_to_bytecode(module:ir.Module)->bytes
jax._src.mlir.module_to_string(module:ir.Module)->str
jax._src.mlir.multi_broadcast_in_dim(ctx:LoweringRuleContext,ops:Sequence[ir.Value],ops_avals:Sequence[core.AbstractValue],out_shape:core.Shape)->Sequence[ir.Value]
jax._src.mlir.pad(ctx:LoweringRuleContext,aval_out,x,padding_value,padding_low,padding_high,padding_interior)->ir.Value
jax._src.mlir.receive_from_host(channel:int,token:hlo.TokenType,out_aval:core.ShapedArray,name:str,*,sharding:xc.OpSharding|None=None)->ir.Value
jax._src.mlir.reduce_window(ctx:LoweringRuleContext,*,reducer_name:str,reducer_body:Callable[[ir.Block],Sequence[ir.Value]],operands:Sequence[ir.Value],init_values:Sequence[ir.Value],init_values_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.mlir.refine_polymorphic_shapes(module:ir.Module)->ir.Module
jax._src.mlir.register_constant_handler(type_:type,handler_fun:ConstantHandler)
jax._src.mlir.register_lowering(prim:core.Primitive,rule:LoweringRule,platform:str|None=None)
jax._src.mlir.reshape(ctx:LoweringRuleContext,op,aval_out:core.AbstractValue)->ir.Value
jax._src.mlir.send_to_host(channel:int,token:hlo.TokenType,operand:Any,aval:core.ShapedArray,name:str,*,sharding:xc.OpSharding|None=None)->ir.Value
jax._src.mlir.shape_tensor(sizes:Sequence[int|ir.RankedTensorType])->ir.RankedTensorType
jax._src.mlir.sharded_aval(aval:core.AbstractValue,sharding:XLACompatibleSharding|None)->core.AbstractValue
jax._src.mlir.slice_op(ctx:LoweringRuleContext,x,aval_out,*,start_indices,limit_indices,strides)->ir.Value
jax._src.mlir.token_type()->Sequence[ir.Type]
jax._src.mlir.wrap_singleton_ir_values(x:ir.Value|Sequence[ir.Value])->Sequence[ir.Value]
jax._src.mlir.wrap_with_memory_kind(x:ir.Value,memory_kind:str,aval_out:core.AbstractValue,is_input:bool=False)->ir.Value
jax._src.mlir.xla_computation_to_mlir_module(xla_computation:xc.XlaComputation)->ir.Module
jax._src.mlir.xla_fallback_lowering(prim:core.Primitive)
jax._src.mlir.zeros_like_lowering(ctx,x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/state/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/state/discharge.py----------------------------------------
A:jax._src.state.discharge.eval_jaxpr->jax._src.linear_util.wrap_init(partial(_eval_jaxpr_discharge_state, jaxpr, should_discharge, consts))
A:jax._src.state.discharge.(new_jaxpr, _, new_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, in_avals)
A:jax._src.state.discharge.env->Environment({})
A:jax._src.state.discharge.invals->map(env.read, eqn.invars)
A:jax._src.state.discharge.(new_invals, ans)->_discharge_rules[eqn.primitive](in_avals, out_avals, *invals, **eqn.params)
A:jax._src.state.discharge.(subfuns, bind_params)->jax._src.interpreters.partial_eval.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs, run_state_p, uk_params, eqn_effects, source).primitive.get_bind_params(eqn.params)
A:jax._src.state.discharge.ans->_addupdate_discharge(x, val, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.out_vals->jax._src.core.Primitive('run_state').bind(*args, jaxpr=jaxpr, which_linear=which_linear)
A:jax._src.state.discharge.ref_vals->map(env.read, [v for v in jaxpr.invars if id(v.aval) in refs_to_discharge])
A:jax._src.state.discharge.y->_get_discharge(x, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.indexer->tuple((next(idx_) if b else slice(None) for b in indexed_dims))
A:jax._src.state.discharge.idx_->iter(idx)
A:jax._src.state.discharge.(z, x_new)->_swap_discharge(x, val, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.z->_prepend_gather(x, idx, indexed_dims)
A:jax._src.state.discharge.x_new->_prepend_scatter(x, idx, indexed_dims, val)
A:jax._src.state.discharge.out->jax._src.core.Primitive('run_state').bind(*args, **staged_params)
A:jax._src.state.discharge.num_outs->len(jaxpr.outvars)
A:jax._src.state.discharge.(discharged_jaxpr, discharged_consts)->discharge_state(jaxpr, ())
A:jax._src.state.discharge.discharged_closed_jaxpr->jax._src.core.ClosedJaxpr(discharged_jaxpr, discharged_consts)
A:jax._src.state.discharge.fun->jax._src.linear_util.wrap_init(core.jaxpr_as_fun(discharged_closed_jaxpr))
A:jax._src.state.discharge.out_and_ref_vals->jax._src.core.closed_call_p.bind(fun, *args, call_jaxpr=discharged_closed_jaxpr)
A:jax._src.state.discharge.(out_vals, ref_vals)->split_list(out_and_ref_vals, [num_outs])
A:jax._src.state.discharge.ref_vals_iter->iter(ref_vals)
A:jax._src.state.discharge.new_invals->tuple((next(ref_vals_iter) if isinstance(aval, AbstractRef) else None for aval in in_avals))
A:jax._src.state.discharge.sentinel->object()
A:jax._src.state.discharge.run_state_p->jax._src.core.Primitive('run_state')
A:jax._src.state.discharge.(discharged_jaxpr, consts)->discharge_state(jaxpr, ())
A:jax._src.state.discharge.(discharged_jaxpr, body_consts)->discharge_state(jaxpr, ())
A:jax._src.state.discharge.(_, out_nonzero_tangents)->jax._src.interpreters.ad.jvp_jaxpr(core.ClosedJaxpr(discharged_jaxpr, body_consts), nonzero_tangents, instantiate=nonzero_tangents)
A:jax._src.state.discharge.nonzero_tangents->map(operator.or_, nonzero_tangents, out_nonzero_tangents)
A:jax._src.state.discharge.(closed_jvp_jaxpr, _)->jax._src.interpreters.ad.jvp_jaxpr(core.ClosedJaxpr(jaxpr, ()), nonzero_tangents, [])
A:jax._src.state.discharge.jvp_jaxpr->hoist_consts_to_refs(jvp_jaxpr_)
A:jax._src.state.discharge.(out_consts, out_primals, out_tangents)->split_list(out, [len(jvp_consts), len(primals)])
A:jax._src.state.discharge.out_tangents_iter->iter(out_tangents)
A:jax._src.state.discharge.(orig_refs, residual_refs)->split_list(refs, [len(in_avals)])
A:jax._src.state.discharge.residual_vals->jax._src.core.eval_jaxpr(jaxpr, (), *orig_refs)
A:jax._src.state.discharge.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun_, in_avals, debug)
A:jax._src.state.discharge.(residual_refs, orig_refs)->split_list(refs, [num_res])
A:jax._src.state.discharge.()->jax._src.core.eval_jaxpr(jaxpr, (), *residual_vals, *orig_refs)
A:jax._src.state.discharge.(res_val_avals, orig_ref_avals)->split_list([v.aval for v in jaxpr.invars], [num_res])
A:jax._src.state.discharge.(jaxpr, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, [*res_ref_avals, *orig_ref_avals])
A:jax._src.state.discharge.num_inputs->len(eqn.invars)
A:jax._src.state.discharge.(discharged_jaxpr_, discharged_consts)->discharge_state(jaxpr, ())
A:jax._src.state.discharge.discharged_jaxpr->discharged_jaxpr.replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[]).replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[])
A:jax._src.state.discharge.(_, _, out_unknowns, out_inst, _, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_stateful(discharged_jaxpr, in_unknowns=jaxpr_in_unknowns, in_inst=jaxpr_in_unknowns, ensure_out_unknowns=in_unknowns, ensure_out_inst=in_unknowns, saveable=saveable)
A:jax._src.state.discharge.out_unknowns->list(out_unknowns)
A:jax._src.state.discharge.in_unknowns->map(operator.or_, in_unknowns, out_unknowns)
A:jax._src.state.discharge.tracers->tuple((trace.instantiate_const(t) if uk else t for (t, uk) in zip(tracers, in_unknowns)))
A:jax._src.state.discharge.(jaxpr_known_resout, jaxpr_unknown_resin_, _, _, num_res_out, num_res_ref)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_stateful(jaxpr, in_unknowns, in_inst=in_unknowns, ensure_out_unknowns=[], ensure_out_inst=[], saveable=_save_everything)
A:jax._src.state.discharge.(_, res_ref_avals)->split_list([v.aval for v in jaxpr_known_resout.invars], [len(known_invars)])
A:jax._src.state.discharge.(jaxpr_known, new_res_avals)->_convert_outputs_to_writes(jaxpr_known_resout)
A:jax._src.state.discharge.(known_tracers, _)->partition_list(in_unknowns, tracers)
A:jax._src.state.discharge.(known_which_linear, _)->partition_list(in_unknowns, which_linear)
A:jax._src.state.discharge.empty_res->map(ad.zeros_like_aval, all_avals)
A:jax._src.state.discharge.out_flat->jax._src.core.Primitive('run_state').bind(*jaxpr_known_args, jaxpr=jaxpr_known, which_linear=jaxpr_known_which_linear)
A:jax._src.state.discharge.(known_outputs, residuals)->split_list(out_flat, [len(known_tracers)])
A:jax._src.state.discharge.residuals->map(trace.new_instantiated_const, residuals)
A:jax._src.state.discharge.(ref_res, nonref_res)->split_list(residuals, [num_res_ref])
A:jax._src.state.discharge.jaxpr_unknown->_convert_inputs_to_reads(len(new_res_avals), jaxpr_unknown_resin_)
A:jax._src.state.discharge.(_, unknown_tracers)->partition_list(in_unknowns, tracers)
A:jax._src.state.discharge.(_, uk_which_linear)->partition_list(in_unknowns, which_linear)
A:jax._src.state.discharge.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.state.discharge.uk_params->dict(jaxpr=jaxpr_unknown, which_linear=unknown_which_linear)
A:jax._src.state.discharge.(_, eqn_effects)->jax._src.core.Primitive('run_state').abstract_eval(*[v.aval for v in unknown_inputs], **uk_params)
A:jax._src.state.discharge.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs, run_state_p, uk_params, eqn_effects, source)
A:jax._src.state.discharge.(_, unknown_outputs)->split_list(res_ref_unknown_outputs, [num_res])
A:jax._src.state.discharge.(jaxpr, which_linear)->split_dict(eqn.params, ['jaxpr', 'which_linear'])
A:jax._src.state.discharge.(jaxpr_known_resout, jaxpr_staged_resin_, _, _, num_res_out, num_res_ref)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_stateful(jaxpr, in_unknowns, in_unknowns, [], [], saveable)
A:jax._src.state.discharge.(jaxpr_known, res_avals)->_convert_outputs_to_writes(jaxpr_known_resout)
A:jax._src.state.discharge.res_avals->map(AbstractRef, res_avals)
A:jax._src.state.discharge.(known_invars, staged_invars)->partition_list(in_unknowns, eqn.invars)
A:jax._src.state.discharge.(known_outvars, staged_outvars)->partition_list(in_unknowns, eqn.outvars)
A:jax._src.state.discharge.newvar->jax._src.core.gensym()
A:jax._src.state.discharge.nonref_resvars->map(newvar, res_avals)
A:jax._src.state.discharge.ref_resvars->map(newvar, res_ref_avals)
A:jax._src.state.discharge.known_out_resvars->map(newvar, [*res_ref_avals, *res_avals])
A:jax._src.state.discharge.known_params->dict(jaxpr=jaxpr_known, which_linear=jaxpr_known_which_linear)
A:jax._src.state.discharge.(_, known_effects)->jax._src.core.Primitive('run_state').abstract_eval(*[v.aval for v in known_and_res_invars], **known_params)
A:jax._src.state.discharge.eqn_known->jax._src.interpreters.partial_eval.new_jaxpr_eqn(known_and_res_invars, [*known_outvars, *known_out_resvars], run_state_p, known_params, known_effects, eqn.source_info)
A:jax._src.state.discharge.jaxpr_staged->_convert_inputs_to_reads(len(res_avals), jaxpr_staged_resin_)
A:jax._src.state.discharge.(_, staged_which_linear)->partition_list(in_unknowns, which_linear)
A:jax._src.state.discharge.staged_params->dict(jaxpr=jaxpr_staged, which_linear=which_linear_unknown)
A:jax._src.state.discharge.(_, staged_invars)->partition_list(in_unknowns, eqn.invars)
A:jax._src.state.discharge.(_, staged_effects)->jax._src.core.Primitive('run_state').abstract_eval(*[v.aval for v in res_staged_invars], **staged_params)
A:jax._src.state.discharge.(_, staged_outvars)->partition_list(in_unknowns, eqn.outvars)
A:jax._src.state.discharge.(staged_call_jaxpr, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(staged, [v.aval for v in res_staged_invars])
A:jax._src.state.discharge.eqn_staged->jax._src.interpreters.partial_eval.new_jaxpr_eqn(staged_invars, staged_outvars, run_state_p, staged_params, staged_effects, eqn.source_info)
A:jax._src.state.discharge.(res_jaxpr_, tangent_jaxpr_, *_, num_res_out, num_res_ref)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_stateful(jaxpr, which_linear, in_inst=which_linear, ensure_out_inst=[], ensure_out_unknowns=[], saveable=_save_everything)
A:jax._src.state.discharge.num_unknown->sum(which_linear)
A:jax._src.state.discharge.(res_args, _)->partition_list(which_linear, args)
A:jax._src.state.discharge.(_, res_avals)->split_list(res_jaxpr_avals, [num_known])
A:jax._src.state.discharge.(res_jaxpr, _)->_convert_outputs_to_writes(res_jaxpr_)
A:jax._src.state.discharge.res->jax._src.core.Primitive('run_state').bind(*res_args, *empty_res, jaxpr=res_jaxpr, which_linear=(False,) * (len(res_args) + len(empty_res)))
A:jax._src.state.discharge.(ref_res_, nonref_res_)->split_list(res, [num_res_ref])
A:jax._src.state.discharge.(tangent_jaxpr, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(tangent_jaxpr_, [])
A:jax._src.state.discharge.(used_res, used_cts)->split_list(used_inputs, [len(res)])
A:jax._src.state.discharge.(used_nonref_res, used_ref_res)->split_list(used_res, [num_res_out])
A:jax._src.state.discharge.(_, nonref_res)->partition_list(used_nonref_res, nonref_res_)
A:jax._src.state.discharge.(_, ref_res)->partition_list(used_ref_res, ref_res_)
A:jax._src.state.discharge.(_, tangent_args)->partition_list(which_linear, args)
A:jax._src.state.discharge.(_, ct_args)->partition_list(used_cts, tangent_args)
A:jax._src.state.discharge.(jaxpr_trans, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(trans), [v.aval for v in jaxpr.invars])
A:jax._src.state.discharge.(jaxpr_transpose_, consts)->_transpose_jaxpr(jaxpr, which_linear)
A:jax._src.state.discharge.jaxpr_transpose->hoist_consts_to_refs(jaxpr_transpose_)
A:jax._src.state.discharge.const_all_outs->jax._src.core.Primitive('run_state').bind(*consts, *transpose_args, jaxpr=jaxpr_transpose, which_linear=which_linear)
A:jax._src.state.discharge.(_, all_outs)->split_list(const_all_outs, [len(consts)])
A:jax._src.state.discharge.(fun_, out_tree_thunk)->jax._src.api_util.flatten_fun_nokwargs(lu.wrap_init(fun), tree_util.treedef_tuple((in_tree,)))
A:jax._src.state.discharge.debug->jax._src.interpreters.partial_eval.debug_info(fun_, in_tree, out_tree_thunk, False, 'run_state')
A:jax._src.state.discharge.(flat_args, in_tree)->jax._src.tree_util.tree_flatten(args)
A:jax._src.state.discharge.(jaxpr_, consts, _)->initial_style_jaxpr(f, in_tree, map(AbstractRef, avals))
A:jax._src.state.discharge.jaxpr->hoist_consts_to_refs(jaxpr_)
A:jax._src.state.discharge.out_const_flat->jax._src.core.eval_jaxpr(discharged_jaxpr, discharged_consts, *consts, *args)
A:jax._src.state.discharge.(_, out_flat)->split_list(out_const_flat, [len(consts)])
jax._src.state.discharge.DischargeRule(self,in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],*args:Any,**params:Any)
jax._src.state.discharge.DischargeRule.__call__(self,in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],*args:Any,**params:Any)
jax._src.state.discharge.Environment
jax._src.state.discharge.Environment.read(self,v:core.Atom)->Any
jax._src.state.discharge.Environment.write(self,v:core.Var,val:Any)->None
jax._src.state.discharge._addupdate_discharge(x,val,idx,indexed_dims)
jax._src.state.discharge._addupdate_discharge_rule(in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],x,val,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge._closed_call_discharge_rule(in_avals:Sequence[core.AbstractValue],_,*args,call_jaxpr:core.ClosedJaxpr)
jax._src.state.discharge._convert_inputs_to_reads(num_res:int,jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.state.discharge._convert_outputs_to_writes(jaxpr:core.Jaxpr)->tuple[core.Jaxpr, list[core.ShapedArray]]
jax._src.state.discharge._dynamic_index(x,idx,indexed_dims)
jax._src.state.discharge._dynamic_update_index(x,idx,val,indexed_dims)
jax._src.state.discharge._eval_jaxpr_discharge_state(jaxpr:core.Jaxpr,should_discharge:Sequence[bool],consts:Sequence[Any],*args:Any)
jax._src.state.discharge._get_discharge(x,idx,indexed_dims)
jax._src.state.discharge._get_discharge_rule(in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],x,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge._has_refs(eqn:core.JaxprEqn)
jax._src.state.discharge._indexer(idx,indexed_dims)
jax._src.state.discharge._initial_style_jaxpr(fun,in_tree,in_avals)
jax._src.state.discharge._prepend_gather(x,idx,indexed_dims)
jax._src.state.discharge._prepend_scatter(x,idx,indexed_dims,val,*,add=False)
jax._src.state.discharge._run_state_abstract_eval(*avals:core.AbstractValue,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._run_state_bind(*args:Any,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._run_state_discharge_rule(in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],*args:Any,jaxpr:core.Jaxpr,which_linear:Sequence[bool])
jax._src.state.discharge._run_state_impl(*args:Any,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._run_state_jvp(primals:Sequence[Any],tangents:Sequence[Any],*,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._run_state_partial_eval(trace:pe.JaxprTrace,*tracers:pe.JaxprTracer,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._run_state_partial_eval_custom(saveable:Callable[...,pe.RematCases_],in_unknowns:Sequence[bool],in_inst:Sequence[bool],eqn:core.JaxprEqn)
jax._src.state.discharge._run_state_transpose(in_cts,*args,jaxpr:core.Jaxpr,which_linear:tuple[bool,...])
jax._src.state.discharge._swap_discharge(x,val,idx,indexed_dims)
jax._src.state.discharge._swap_discharge_rule(in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],x,val,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge._transpose_jaxpr(jaxpr:core.Jaxpr,which_linear:Sequence[bool])->tuple[core.Jaxpr, Any]
jax._src.state.discharge.discharge_state(jaxpr:core.Jaxpr,consts:Sequence[Any],*,should_discharge:bool|Sequence[bool]=True)->tuple[core.Jaxpr, list[Any]]
jax._src.state.discharge.initial_style_jaxpr(fun:Callable,in_tree:PyTreeDef,in_avals:Sequence[core.AbstractValue])->tuple[core.Jaxpr, list[Any], PyTreeDef]
jax._src.state.discharge.register_discharge_rule(prim:core.Primitive)
jax._src.state.discharge.run_state(f:Callable[...,None])
jax._src.state.discharge.run_state_reference(f:Callable[...,None])


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/state/utils.py----------------------------------------
A:jax._src.state.utils.(const_avals_, const_ref_avals)->partition_list(is_const_ref, all_const_avals)
A:jax._src.state.utils.const_avals->map(AbstractRef, const_avals_)
A:jax._src.state.utils.merged_const_avals->merge_lists(is_const_ref, const_avals, const_ref_avals)
A:jax._src.state.utils.num_consts->len(merged_const_avals)
A:jax._src.state.utils.(all_consts, args)->split_list(consts_args, [num_consts])
A:jax._src.state.utils.(consts, const_refs)->partition_list(is_const_ref, all_consts)
A:jax._src.state.utils.consts->map(lambda x: ref_get(x, ()), consts)
A:jax._src.state.utils.all_consts->merge_lists(is_const_ref, consts, const_refs)
A:jax._src.state.utils.(hoisted_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(_hoist), in_avals)
A:jax._src.state.utils.aval->jax._src.core.raise_to_shaped(core.get_aval(x))
jax._src.state.utils.hoist_consts_to_refs(jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.state.utils.val_to_ref_aval(x)->AbstractRef


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/state/types.py----------------------------------------
A:jax._src.state.types.index_text->jax._src.pretty_printer.text(self.input_index)
A:jax._src.state.types.Aval->TypeVar('Aval', bound=core.AbstractValue)
A:jax._src.state.types.ndim->property(lambda self: len(self.shape))
A:jax._src.state.types.size->property(lambda self: math.prod(self.shape))
jax._src.state.AbstractRef(self,inner_aval:core.AbstractValue)
jax._src.state.AbstractRef.__eq__(self,other)
jax._src.state.AbstractRef.__hash__(self)
jax._src.state.AbstractRef.__repr__(self)->str
jax._src.state.AbstractRef._getitem(self,tracer,idx)->Array
jax._src.state.AbstractRef._setitem(self,tracer,idx,value)->None
jax._src.state.AbstractRef.at(self)
jax._src.state.AbstractRef.at_least_vspace(self)
jax._src.state.AbstractRef.dtype(self)
jax._src.state.AbstractRef.get(tracer,idx=())
jax._src.state.AbstractRef.join(self,other)
jax._src.state.AbstractRef.set(tracer,value,idx=())
jax._src.state.AbstractRef.shape(self)
jax._src.state.AccumEffect(RefEffect)
jax._src.state.ReadEffect(RefEffect)
jax._src.state.RefEffect(effects.JaxprInputEffect)
jax._src.state.RefEffect.__eq__(self,other)
jax._src.state.RefEffect.__hash__(self)
jax._src.state.RefEffect.__str__(self)
jax._src.state.RefEffect._pretty_print(self,context:core.JaxprPpContext)->pp.Doc
jax._src.state.RefView
jax._src.state.RefView.at(self)
jax._src.state.WriteEffect(RefEffect)
jax._src.state.get_ref_state_effects(avals:Sequence[core.AbstractValue],effects:core.Effects)->list[set[StateEffect]]
jax._src.state.shaped_array_ref(shape:tuple[int,...],dtype,weak_type:bool=False,named_shape=None)->AbstractRef[core.AbstractValue]
jax._src.state.types.AbstractRef(self,inner_aval:core.AbstractValue)
jax._src.state.types.AbstractRef.__eq__(self,other)
jax._src.state.types.AbstractRef.__hash__(self)
jax._src.state.types.AbstractRef.__init__(self,inner_aval:core.AbstractValue)
jax._src.state.types.AbstractRef.__repr__(self)->str
jax._src.state.types.AbstractRef._getitem(self,tracer,idx)->Array
jax._src.state.types.AbstractRef._setitem(self,tracer,idx,value)->None
jax._src.state.types.AbstractRef.at(self)
jax._src.state.types.AbstractRef.at_least_vspace(self)
jax._src.state.types.AbstractRef.dtype(self)
jax._src.state.types.AbstractRef.get(tracer,idx=())
jax._src.state.types.AbstractRef.join(self,other)
jax._src.state.types.AbstractRef.set(tracer,value,idx=())
jax._src.state.types.AbstractRef.shape(self)
jax._src.state.types.AccumEffect(RefEffect)
jax._src.state.types.ReadEffect(RefEffect)
jax._src.state.types.RefEffect(effects.JaxprInputEffect)
jax._src.state.types.RefEffect.__eq__(self,other)
jax._src.state.types.RefEffect.__hash__(self)
jax._src.state.types.RefEffect.__str__(self)
jax._src.state.types.RefEffect._pretty_print(self,context:core.JaxprPpContext)->pp.Doc
jax._src.state.types.RefIndexer
jax._src.state.types.RefIndexer.__getitem__(self,slc)
jax._src.state.types.RefView
jax._src.state.types.RefView.at(self)
jax._src.state.types.WriteEffect(RefEffect)
jax._src.state.types._map_ref(size,axis,ref_aval)
jax._src.state.types._ref_raise_to_shaped(ref_aval:AbstractRef,weak_type)
jax._src.state.types._unmap_ref(size,axis_name,axis,ref_aval)
jax._src.state.types.get_ref_state_effects(avals:Sequence[core.AbstractValue],effects:core.Effects)->list[set[StateEffect]]
jax._src.state.types.shaped_array_ref(shape:tuple[int,...],dtype,weak_type:bool=False,named_shape=None)->AbstractRef[core.AbstractValue]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/state/primitives.py----------------------------------------
A:jax._src.state.primitives.get_p->jax._src.core.Primitive('get')
A:jax._src.state.primitives.idx->_pp_idx(context, idx, eqn.params['indexed_dims'])
A:jax._src.state.primitives.(non_slice_idx, indexed_dims)->_get_indexer(ref, idx)
A:jax._src.state.primitives.ref_aval->jax._src.core.get_aval(ref)
A:jax._src.state.primitives.swap_p->jax._src.core.Primitive('swap')
A:jax._src.state.primitives.addupdate_p->jax._src.core.Primitive('addupdate')
A:jax._src.state.primitives.idx_shapes->tuple((i.shape for i in idx))
A:jax._src.state.primitives.shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.out_aval->jax._src.core.ShapedArray(expected_output_shape, ref_aval.dtype)
A:jax._src.state.primitives.val_aval->jax._src.core.raise_to_shaped(val_aval)
A:jax._src.state.primitives.expected_output_shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.slice_shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.pp_ref->partial(pp.color, intensity=pp.Intensity.NORMAL, foreground=pp.Color.GREEN)
A:jax._src.state.primitives.idx_iter->iter(non_slice_idx)
A:jax._src.state.primitives.lhs->jax._src.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.state.primitives.x_i->jax._src.pretty_printer.concat([pp.text(core.pp_var(x, context)), pp.text('['), idx, pp.text(']')])
A:jax._src.state.primitives.y->jax._src.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.state.primitives.x_tangent->jax._src.ad_util.instantiate(x_tangent)
A:jax._src.state.primitives.x_bar->jax._src.core.Primitive('swap').bind(ref, ad_util.instantiate(g), *idx, **params)
A:jax._src.state.primitives.g->jax._src.core.Primitive('get').bind(ref, *idx, **params)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[get_p]->partial(_state_partial_eval_custom, get_p)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[swap_p]->partial(_state_partial_eval_custom, swap_p)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[addupdate_p]->partial(_state_partial_eval_custom, addupdate_p)
A:jax._src.state.primitives.num_idxs_to_left->sum(indexed_dims[:ref_dim])
A:jax._src.state.primitives.idx_is_batched->any((i_dim is not batching.not_mapped for i_dim in idx_dims))
A:jax._src.state.primitives.idxs->tuple_insert(idxs, idx_place, iota)
A:jax._src.state.primitives.indexed_dims->tuple_insert(indexed_dims, ref_dim, True)
A:jax._src.state.primitives.idx_place->[i for (i, i_dim) in enumerate(indexed_dims) if i_dim].index(ref_dim)
A:jax._src.state.primitives.iota->jax._src.lax.lax.broadcasted_iota(np.dtype('int32'), idxs_shape, 0)
A:jax._src.state.primitives.bdim_out->_output_bdim(indexed_dims, ref_dim, idxs_shape)
A:jax._src.state.primitives.val->jax._src.interpreters.batching.moveaxis(val, val_dim, 0)
jax._src.state.primitives._addupdate_abstract_eval(ref_aval:AbstractRef,val_aval:core.AbstractValue,*idx:core.ShapedArray,indexed_dims:tuple[bool])
jax._src.state.primitives._addupdate_impl(ref:AbstractRef,value:Array,*idx:int)
jax._src.state.primitives._addupdate_pp_rule(eqn,context,settings)->pp.Doc
jax._src.state.primitives._addupdate_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._get_abstract_eval(ref_aval:AbstractRef,*idx,indexed_dims)
jax._src.state.primitives._get_impl(ref:AbstractRef,*idx:int,**_)
jax._src.state.primitives._get_indexer(ref:AbstractRef,idx:Indexer)->tuple[Indexer, tuple[bool, ...]]
jax._src.state.primitives._get_jvp(primals:list[Any],tangents:list[Any],**params:Any)
jax._src.state.primitives._get_pp_rule(eqn,context,settings)->pp.Doc
jax._src.state.primitives._get_slice_output_shape(in_shape:tuple[int,...],idx_shapes:tuple[tuple[int,...],...],indexed_dims:tuple[bool,...])->tuple[int, ...]
jax._src.state.primitives._get_transpose(g,ref,*idx,**params)
jax._src.state.primitives._get_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._is_trivial_indexer(idx:Indexer)->bool
jax._src.state.primitives._output_bdim(indexed_dims:tuple[bool,...],ref_dim:int,idxs_shape:tuple[int,...])
jax._src.state.primitives._pp_idx(context,non_slice_idx,indexed_dims)
jax._src.state.primitives._state_partial_eval_custom(prim,saveable,unks_in,inst_in,eqn)
jax._src.state.primitives._swap_abstract_eval(ref_aval:AbstractRef,val_aval:core.AbstractValue,*idx:core.ShapedArray,indexed_dims:tuple[bool])
jax._src.state.primitives._swap_impl(ref:AbstractRef,value:Array,*idx:int,**_)
jax._src.state.primitives._swap_jvp(primals:list[Any],tangents:list[Any],**params:Any)
jax._src.state.primitives._swap_pp_rule(eqn,context,settings)->pp.Doc
jax._src.state.primitives._swap_transpose(g,ref,x,*idx,**params)
jax._src.state.primitives._swap_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._unpack_idx(idx:Indexer,ndim:int)->tuple[tuple[Array, ...], tuple[bool, ...]]
jax._src.state.primitives.addupdate_jvp_rule(primals:list[Any],tangents:list[Any],**params:Any)
jax._src.state.primitives.addupdate_transpose(cts_in,ref,x,*idx,**params)
jax._src.state.primitives.ref_addupdate(ref:AbstractRef,idx:Indexer,x:Array)->None
jax._src.state.primitives.ref_get(ref:Any,idx:Indexer)->Array
jax._src.state.primitives.ref_set(ref:AbstractRef,idx:Indexer,value:Array)->None
jax._src.state.primitives.ref_swap(ref:AbstractRef,idx:Indexer,value:Array)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/ann.py----------------------------------------
A:jax._src.lax.ann.dims->list(operand.shape)
A:jax._src.lax.ann.c->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt'))
A:jax._src.lax.ann.p0->jax._src.interpreters.xla.parameter(c, 0, xc.Shape.scalar_shape(op_type))
A:jax._src.lax.ann.p1->jax._src.interpreters.xla.parameter(c, 1, xc.Shape.scalar_shape(op_type))
A:jax._src.lax.ann.cmp_result->jax._src.lib.mlir.dialects.hlo.CompareOp(p0, p1, comparison_direction=direction)
A:jax._src.lax.ann.op_shape->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt')).get_shape(operand)
A:jax._src.lax.ann.op_dims->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt')).get_shape(operand).dimensions()
A:jax._src.lax.ann.op_type->jax._src.interpreters.mlir.dtype_to_ir_type(ctx.avals_in[0].dtype)
A:jax._src.lax.ann.comparator->_comparator_builder_mlir(ctx, op_type, is_max_k)
A:jax._src.lax.ann.init_val_literal->_get_init_val_literal(op_type, is_max_k)
A:jax._src.lax.ann.iota->jax._src.interpreters.mlir.iota(ctx, core.ShapedArray(ctx.avals_in[0].shape, np.int32), dimension=reduction_dimension)
A:jax._src.lax.ann.init_val->jax._src.interpreters.mlir.ir_constant(init_val_array.reshape(()))
A:jax._src.lax.ann.init_arg->jax._src.lib.xla_client.ops.Constant(c, np.int32(-1))
A:jax._src.lax.ann.out->jax._src.interpreters.mlir.custom_call('ApproxTopK', result_types=[mlir.aval_to_ir_type(aval) for aval in ctx.avals_out], operands=[operand, iota, init_val, init_arg], called_computations=[comparator.name.value], backend_config=backend_config, result_shapes=result_shapes)
A:jax._src.lax.ann.scalar->jax._src.lib.mlir.ir.RankedTensorType.get([], op_type)
A:jax._src.lax.ann.index->jax._src.lib.mlir.ir.RankedTensorType.get([], ir.IntegerType.get_signless(32))
A:jax._src.lax.ann.comparator_type->jax._src.lib.mlir.ir.FunctionType.get(ir_types, result_types)
A:jax._src.lax.ann.entry_block->_comparator_builder_mlir(ctx, op_type, is_max_k).add_entry_block()
A:jax._src.lax.ann.direction->jax._src.lib.mlir.dialects.hlo.ComparisonDirectionAttr.get('GT' if is_max_k else 'LT')
A:jax._src.lax.ann.recall_type->jax._src.lib.mlir.ir.F32Type.get()
A:jax._src.lax.ann.init_val_array->_get_init_val_literal(ctx.avals_in[0].dtype, is_max_k)
A:jax._src.lax.ann.backend_config['is_fallback']->jax._src.interpreters.mlir.ir.BoolAttr.get(fallback)
A:jax._src.lax.ann.(val_out, arg_out)->approx_min_k(operand, k, reduction_dimension, recall_target, reduction_input_size_override, aggregate_to_topk)
A:jax._src.lax.ann.tangent_out->jax._src.ad_util.Zero.from_value(val_out)
A:jax._src.lax.ann.rank->len(arg_shape)
A:jax._src.lax.ann.idx->tuple((arg_out if i == reduction_dimension else iotas[i] for i in range(rank)))
A:jax._src.lax.ann.approx_top_k_p->jax._src.core.Primitive('approx_top_k')
jax._src.lax.ann._approx_top_k_abstract_eval(operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_batch_rule(batch_operands,batch_axes,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_jvp(primals,tangents,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_lowering(ctx,operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk,fallback=False)
jax._src.lax.ann._approx_top_k_tpu_translation(ctx,avals_in,avals_out,operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._comparator_builder(op_type,is_max_k)
jax._src.lax.ann._comparator_builder_mlir(ctx,op_type,is_max_k)
jax._src.lax.ann._get_init_val_literal(op_type,is_max_k)
jax._src.lax.ann.approx_max_k(operand:Array,k:int,reduction_dimension:int=-1,recall_target:float=0.95,reduction_input_size_override:int=-1,aggregate_to_topk:bool=True)->tuple[Array, Array]
jax._src.lax.ann.approx_min_k(operand:Array,k:int,reduction_dimension:int=-1,recall_target:float=0.95,reduction_input_size_override:int=-1,aggregate_to_topk:bool=True)->tuple[Array, Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/fft.py----------------------------------------
A:jax._src.lax.fft.typ->_str_to_fft_type(fft_type)
A:jax._src.lax.fft.(x,)->promote_dtypes_complex(x)
A:jax._src.lax.fft.fft_lengths->tuple(fft_lengths)
A:jax._src.lax.fft.dtype->_real_dtype(x.dtype)
A:jax._src.lax.fft.ndims->len(in_shape)
A:jax._src.lax.fft.out_dtype->numpy.dtype(np.float32 if dtype == np.complex64 else np.float64)
A:jax._src.lax.fft.zero->jax._src.interpreters.mlir.ir_constant(np.array(0, dtype=out_dtype))
A:jax._src.lax.fft.strides_in->jax._src.interpreters.mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, tuple(reversed(strides_in))))
A:jax._src.lax.fft.strides_out->jax._src.interpreters.mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, tuple(reversed(strides_out))))
A:jax._src.lax.fft.double_type->jax._src.interpreters.mlir.ir.RankedTensorType.get((), mlir.ir.F64Type.get())
A:jax._src.lax.fft.(size_fft_lengths,)->jax._src.interpreters.mlir.eval_dynamic_shape_as_vals(ctx, (size_fft_length_prod,))
A:jax._src.lax.fft.size_fft_lengths->jax._src.lib.mlir.dialects.hlo.ConvertOp(double_type, size_fft_lengths)
A:jax._src.lax.fft.one->jax._src.interpreters.mlir.ir_constant(np.float64(1.0))
A:jax._src.lax.fft.in_shape->jax._src.interpreters.mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, in_shape))
A:jax._src.lax.fft.out_shape->jax._src.interpreters.mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, out_shape))
A:jax._src.lax.fft.result_type->jax._src.interpreters.mlir.aval_to_ir_type(out_aval)
A:jax._src.lax.fft.y->fft(x, xla_client.FftType.FFT, fft_lengths)
A:jax._src.lax.fft.dummy_primal->ShapeDtypeStruct(dummy_shape, _real_dtype(t.dtype))
A:jax._src.lax.fft.transpose->linear_transpose(partial(_naive_rfft, fft_lengths=fft_lengths), dummy_primal)
A:jax._src.lax.fft.(result,)->transpose(t)
A:jax._src.lax.fft.x->jax._src.interpreters.batching.moveaxis(x, bd, 0)
A:jax._src.lax.fft.full->partial(lax.full_like, t, dtype=x.dtype)
A:jax._src.lax.fft.mask->jax.lax.concatenate([full(1.0, shape=(1,)), full(2.0, shape=(n - 2 + is_odd,)), full(1.0, shape=(1 - is_odd,))], dimension=0)
A:jax._src.lax.fft.result->fft(t, fft_type, fft_lengths)
A:jax._src.lax.fft.fft_p->Primitive('fft')
jax._src.lax.fft._fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax._src.lax.fft._fft_impl(x,fft_type,fft_lengths)
jax._src.lax.fft._fft_lowering(ctx,x,*,fft_type,fft_lengths)
jax._src.lax.fft._fft_lowering_cpu(ctx,x,*,fft_type,fft_lengths)
jax._src.lax.fft._fft_transpose_rule(t,operand,fft_type,fft_lengths)
jax._src.lax.fft._irfft_transpose(t,fft_lengths)
jax._src.lax.fft._naive_rfft(x,fft_lengths)
jax._src.lax.fft._rfft_transpose(t,fft_lengths)
jax._src.lax.fft._str_to_fft_type(s:str)->xla_client.FftType
jax._src.lax.fft.fft(x,fft_type:Union[xla_client.FftType,str],fft_lengths:Sequence[int])
jax._src.lax.fft.fft_abstract_eval(x,fft_type,fft_lengths)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/utils.py----------------------------------------
A:jax._src.lax.utils.prim->jax._src.core.Primitive(name)
A:jax._src.lax.utils.weak_type->weak_type_rule(*avals, **kwargs)
A:jax._src.lax.utils.least_specialized->max(map(type, avals), key=operator.attrgetter('array_abstraction_level'))
A:jax._src.lax.utils.out->jax._src.core.Primitive(name).impl(*[x.val for x in avals], **kwargs)
A:jax._src.lax.utils.shape->shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.weak_types->weak_type_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_vals->jax._src.core.Primitive(name).impl(*[x.val for x in avals], **kwargs)
A:jax._src.lax.utils.out_shapes->shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_dtypes->dtype_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_named_shapes->named_shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.xla_opname->''.join((term.capitalize() for term in prim.name.split('_')))
A:jax._src.lax.utils.op->getattr(xops, xla_opname)
jax._src.lax.utils._argnum_weak_type(*argnums)
jax._src.lax.utils._input_dtype(x,*_,**__)
jax._src.lax.utils._standard_weak_type_rule(*avals,**kwargs)
jax._src.lax.utils.dtype_to_string(dtype)
jax._src.lax.utils.standard_abstract_eval(prim,shape_rule,dtype_rule,weak_type_rule,named_shape_rule,*avals,**kwargs)
jax._src.lax.utils.standard_multi_result_abstract_eval(prim,shape_rule,dtype_rule,weak_type_rule,named_shape_rule,*avals,**kwargs)
jax._src.lax.utils.standard_named_shape_rule(*avals,**kwargs)
jax._src.lax.utils.standard_primitive(shape_rule,dtype_rule,name,weak_type_rule=None,named_shape_rule=None)
jax._src.lax.utils.standard_translate(prim)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/convolution.py----------------------------------------
A:jax._src.lax.convolution.dnums->jax._src.lib.mlir.dialects.hlo.ConvDimensionNumbers.get(input_batch_dimension=lhs_spec[0], input_feature_dimension=lhs_spec[1], input_spatial_dimensions=list(lhs_spec[2:]), kernel_output_feature_dimension=rhs_spec[0], kernel_input_feature_dimension=rhs_spec[1], kernel_spatial_dimensions=list(rhs_spec[2:]), output_batch_dimension=out_spec[0], output_feature_dimension=out_spec[1], output_spatial_dimensions=list(out_spec[2:]))
A:jax._src.lax.convolution.padding->list(map(np.sum, padding))
A:jax._src.lax.convolution.pad_a->int(np.ceil(pad_len / 2))
A:jax._src.lax.convolution.x->numpy.flip(x, axis)
A:jax._src.lax.convolution.ndims->len(lhs.shape)
A:jax._src.lax.convolution.dn->conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)
A:jax._src.lax.convolution.k_shape->numpy.take(rhs.shape, dn.rhs_spec)
A:jax._src.lax.convolution.effective_k_size->map(lambda k, r: (k - 1) * r + 1, k_sdims, rhs_dilation)
A:jax._src.lax.convolution.rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
A:jax._src.lax.convolution.(quot, rem)->divmod(lhs_feature_count, feature_group_count)
A:jax._src.lax.convolution.lhs_trans->numpy.take(lhs_shape, lhs_perm)
A:jax._src.lax.convolution.rhs_trans->numpy.take(rhs_shape, rhs_perm)
A:jax._src.lax.convolution.out_trans->tuple((lhs_trans[0], rhs_trans[0]) + tuple(out_space))
A:jax._src.lax.convolution.result_dtype->jax._src.lax.lax.naryop_dtype_rule(lax._input_dtype, [lax._any, lax._any], 'conv_general_dilated', lhs, rhs)
A:jax._src.lax.convolution.(lhs_sdims, rhs_sdims, out_sdims)->map(_conv_sdims, dimension_numbers)
A:jax._src.lax.convolution.t_rhs_spec->_conv_spec_transpose(rhs_spec)
A:jax._src.lax.convolution.trans_dimension_numbers->ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
A:jax._src.lax.convolution.revd_weights->jax._src.lax.lax.rev(rhs, rhs_sdims)
A:jax._src.lax.convolution.out->_reshape_axis_into(out_spec[1], out_spec[1] + 1, out)
A:jax._src.lax.convolution.(lhs_trans, rhs_trans, out_trans)->map(_conv_spec_transpose, dimension_numbers)
A:jax._src.lax.convolution.shape->list(x.shape)
A:jax._src.lax.convolution.new_lhs->_reshape_axis_into(lhs_spec[0], lhs_spec[0], new_lhs)
A:jax._src.lax.convolution.new_rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[0], new_rhs)
A:jax._src.lax.convolution.conv_general_dilated_p->jax._src.lax.lax.standard_primitive(_conv_general_dilated_shape_rule, _conv_general_dilated_dtype_rule, 'conv_general_dilated')
A:jax._src.lax.convolution.k1->mul(lax.add(x_re, x_im), y_re)
A:jax._src.lax.convolution.k2->mul(x_re, lax.sub(y_im, y_re))
A:jax._src.lax.convolution.k3->mul(x_im, lax.add(y_re, y_im))
A:jax._src.lax.convolution.preferred_element_type->_real_dtype(preferred_element_type)
A:jax._src.lax.convolution.complex_conv->jax._src.interpreters.mlir.lower_fun(partial(_complex_mul, partial(conv_general_dilated, window_strides=window_strides, padding=padding, lhs_dilation=lhs_dilation, rhs_dilation=rhs_dilation, dimension_numbers=dimension_numbers, feature_group_count=feature_group_count, batch_group_count=batch_group_count, precision=precision, preferred_element_type=preferred_element_type)), multiple_results=False)
A:jax._src.lax.convolution.window_reversal->jax._src.interpreters.mlir.dense_bool_elements([False] * num_spatial_dims)
A:jax._src.lax.convolution.int2d->jax._src.interpreters.mlir.aval_to_ir_type(core.ShapedArray((1, 2), np.int32))
A:jax._src.lax.convolution.pad1->jax._src.interpreters.mlir.eval_dynamic_shape_as_tensor(ctx, pad_lo_hi)
A:jax._src.lax.convolution.d_padding->jax._src.lib.mlir.dialects.hlo.ConcatenateOp(list(map(prep_one_pad, padding)), mlir.i64_attr(0))
A:jax._src.lax.convolution.new_shape->list(np.delete(x.shape, src))
A:jax._src.lax.convolution.(size2, ragged)->divmod(shape[src], size1)
A:jax._src.lax.convolution.pads->jax._src.lax.lax.padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
A:jax._src.lax.convolution.lhs_padded->numpy.add(lhs_shape[2:], np.sum(np.array(pads).reshape(-1, 2), axis=1))
A:jax._src.lax.convolution.out_space->numpy.sum([unpad_out_space, padding], axis=0).tolist()
A:jax._src.lax.convolution.(lhs_perm, rhs_perm, out_perm)->map(getperm, dimension_numbers, charpairs)
A:jax._src.lax.convolution.iota->tuple(range(len(lhs_shape)))
A:jax._src.lax.convolution.(lhs_spec, rhs_spec, out_spec)->conv_general_permutations(dimension_numbers)
A:jax._src.lax.convolution.spatial->sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
A:jax._src.lax.convolution.lhs_dilated_shape->jax._src.lax.lax._dilate_shape(in_shape, lhs_dilation)
A:jax._src.lax.convolution.rhs_dilated_shape->jax._src.lax.lax._dilate_shape(window_dimensions, rhs_dilation)
A:jax._src.lax.convolution.out_dilated_shape->jax._src.lax.lax._dilate_shape(out_shape, window_strides)
A:jax._src.lax.convolution.(pads_lo, _)->jax._src.util.unzip2(padding)
A:jax._src.lax.convolution.pads_from_lhs->map(operator.sub, out_dilated_shape, lhs_dilated_shape)
A:jax._src.lax.convolution.pads_from_rhs->tuple((rd - pd - 1 for (rd, pd) in zip(rhs_dilated_shape, pads_lo)))
A:jax._src.lax.convolution.pads_hi->tuple(map(operator.add, pads_from_lhs, pads_from_rhs))
jax._src.lax.convolution.ConvDimensionNumbers(NamedTuple)
jax._src.lax.convolution._complex_mul(mul,x,y)
jax._src.lax.convolution._conv_general_dilated_batch_rule(batched_args,batch_dims,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,preferred_element_type,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_dtype_rule(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,preferred_element_type,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_lower(ctx,lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,preferred_element_type,expand_complex_convolutions=False,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_shape_rule(lhs:core.ShapedArray,rhs:core.ShapedArray,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,**unused_kwargs)->tuple[int, ...]
jax._src.lax.convolution._conv_general_dilated_transpose_lhs(g,lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,preferred_element_type)
jax._src.lax.convolution._conv_general_dilated_transpose_rhs(g,lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,precision,preferred_element_type)
jax._src.lax.convolution._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)->list[tuple[int, int]]
jax._src.lax.convolution._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._src.lax.convolution._conv_transpose_padding(k,s,padding)
jax._src.lax.convolution._flip_axes(x,axes)
jax._src.lax.convolution._reshape_axis_into(src,dst,x)
jax._src.lax.convolution._reshape_axis_out_of(src,size1,x)
jax._src.lax.convolution.conv(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:str,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.convolution.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)->ConvDimensionNumbers
jax._src.lax.convolution.conv_general_dilated(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,feature_group_count:int=1,batch_group_count:int=1,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.convolution.conv_general_permutations(dimension_numbers)
jax._src.lax.convolution.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax._src.lax.convolution.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads,batch_group_count=1)
jax._src.lax.convolution.conv_transpose(lhs:Array,rhs:Array,strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,transpose_kernel:bool=False,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.convolution.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax._src.lax.convolution.conv_with_general_padding(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],lhs_dilation:Optional[Sequence[int]],rhs_dilation:Optional[Sequence[int]],precision:lax.PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/special.py----------------------------------------
A:jax._src.lax.special.partial_x->exp((b - 1) * log1p(-x) + (a - 1) * log(x) - lbeta)
A:jax._src.lax.special.iterations_remain_cond->lt(iteration, num_iterations)
A:jax._src.lax.special.partial_numerator->nth_partial_numerator(iteration, *inputs)
A:jax._src.lax.special.partial_denominator->nth_partial_denominator(0, *inputs)
A:jax._src.lax.special.c->_const(x, 0)
A:jax._src.lax.special.small_constant->full_like(c, small)
A:jax._src.lax.special.d->reciprocal(d)
A:jax._src.lax.special.delta->mul(c, d)
A:jax._src.lax.special.h->select(lt(abs(partial_denominator), small), broadcast_in_dim(small, partial_denominator.shape, ()), partial_denominator)
A:jax._src.lax.special.tolerance_comparison->ge(abs(sub(delta, _const(delta, 1.0))), threshold)
A:jax._src.lax.special.values[kValuesUnconvergedIdx]->_any(tolerance_comparison)
A:jax._src.lax.special.values->while_loop(while_cond_fn, while_body_fn, values)
A:jax._src.lax.special.iteration_bcast->broadcast_in_dim(iteration, shape, [])
A:jax._src.lax.special.iteration_is_even->eq(iteration_bcast % full_like(iteration_bcast, 2), full_like(iteration_bcast, 0))
A:jax._src.lax.special.iteration_is_one->eq(iteration_bcast, full_like(iteration_bcast, 1))
A:jax._src.lax.special.m->convert_element_type(m, dtype)
A:jax._src.lax.special.one->full_like(a, 1)
A:jax._src.lax.special.two->full_like(x, 2.0)
A:jax._src.lax.special.one_numerator->full_like(x, 1.0)
A:jax._src.lax.special.numerator->select(iteration_is_even, even_numerator, odd_numerator)
A:jax._src.lax.special.result_is_nan->bitwise_or(bitwise_or(bitwise_or(le(a, full_like(a, 0)), le(b, full_like(b, 0))), lt(x, full_like(x, 0))), gt(x, full_like(x, 1)))
A:jax._src.lax.special.converges_rapidly->lt(x, (a + full_like(a, 1)) / (a + b + full_like(b, 2.0)))
A:jax._src.lax.special.a->select(converges_rapidly, a, b)
A:jax._src.lax.special.b->select(converges_rapidly, b, a_orig)
A:jax._src.lax.special.x->x.astype(np.float32).astype(np.float32)
A:jax._src.lax.special.continued_fraction->lentz_thompson_barnett_algorithm(num_iterations=200 if dtype == np.float32 else 600, small=(dtypes.finfo(dtype).eps / 2).astype(dtype), threshold=(dtypes.finfo(dtype).eps / 2).astype(dtype), nth_partial_numerator=nth_partial_betainc_numerator, nth_partial_denominator=nth_partial_betainc_denominator, inputs=[a, b, x])
A:jax._src.lax.special.result->convert_element_type(result, a_dtype)
A:jax._src.lax.special.f->_const(predicates, False)
A:jax._src.lax.special.all_dimensions->tuple(range(len(predicates_shape)))
A:jax._src.lax.special.conditional->bitwise_and(enabled, grad_conditional > _const(grad_conditional, eps))
A:jax._src.lax.special.vals->while_loop(cond_fn, body_fn, init_vals)
A:jax._src.lax.special.is_nan->bitwise_or(_isnan(a), _isnan(x))
A:jax._src.lax.special.x_is_zero->eq(x, full_like(x, 0))
A:jax._src.lax.special.x_is_infinity->eq(x, _const(x, float('inf')))
A:jax._src.lax.special.domain_error->bitwise_or(lt(x, full_like(x, 0)), le(a, full_like(a, 0)))
A:jax._src.lax.special.use_igammac->bitwise_and(gt(x, full_like(x, 1)), gt(x, a))
A:jax._src.lax.special.underflow->lt(ax, -log(dtypes.finfo(a.dtype).max))
A:jax._src.lax.special.ax->exp(ax)
A:jax._src.lax.special.enabled->bitwise_not(bitwise_or(bitwise_or(bitwise_or(x_is_zero, domain_error), underflow), is_nan))
A:jax._src.lax.special.output->select(bitwise_or(domain_error, is_nan), full_like(a, float('nan')), output)
A:jax._src.lax.special.qk_is_nonzero->ne(qk, _const(qk, 0))
A:jax._src.lax.special.t->full_like(x, 1)
A:jax._src.lax.special.ans->select(qk_is_nonzero, r, ans)
A:jax._src.lax.special.dans_da_new->select(qk_is_nonzero, div(dpk_da - ans * dqk_da, qk), dans_da)
A:jax._src.lax.special.grad_conditional->select(qk_is_nonzero, abs(dans_da_new - dans_da), full_like(dans_da, 1))
A:jax._src.lax.special.rescale->gt(abs(pk), reciprocal(_const(pk, eps)))
A:jax._src.lax.special.pkm2->full_like(x, 1)
A:jax._src.lax.special.pkm1->select(rescale, mul(pkm1, _const(pkm1, eps)), pkm1)
A:jax._src.lax.special.qkm2->select(rescale, mul(qkm2, _const(qkm2, eps)), qkm2)
A:jax._src.lax.special.qkm1->select(rescale, mul(qkm1, _const(qkm1, eps)), qkm1)
A:jax._src.lax.special.dpkm2_da->full_like(x, 0)
A:jax._src.lax.special.dqkm2_da->full_like(x, 0)
A:jax._src.lax.special.dpkm1_da->full_like(x, 0)
A:jax._src.lax.special.dqkm1_da->select(rescale, mul(dqkm1_da, _const(dqkm1_da, eps)), dqkm1_da)
A:jax._src.lax.special.out_of_range->bitwise_or(le(x, _const(x, 0)), le(a, _const(a, 0)))
A:jax._src.lax.special.use_igamma->bitwise_or(lt(x, _const(x, 1)), lt(x, a))
A:jax._src.lax.special.igamma_call->_igamma_series(ax, x, a, bitwise_and(enabled, use_igamma), dtype, IgammaMode.VALUE)
A:jax._src.lax.special.igammac_cf_call->_igammac_continued_fraction(ax, x, a, bitwise_and(enabled, bitwise_not(use_igamma)), dtype, IgammaMode.VALUE)
A:jax._src.lax.special.broadcasted_shape->broadcast_shapes(*(a.shape for a in args))
A:jax._src.lax.special.b0->full_like(x, 0)
A:jax._src.lax.special.b1->full_like(x, 0)
A:jax._src.lax.special.b2->full_like(x, 0)
A:jax._src.lax.special.i0e_coeffs_a->numpy.array([-4.4153416464793395e-18, 3.3307945188222384e-17, -2.431279846547955e-16, 1.715391285555133e-15, -1.1685332877993451e-14, 7.676185498604936e-14, -4.856446783111929e-13, 2.95505266312964e-12, -1.726826291441556e-11, 9.675809035373237e-11, -5.189795601635263e-10, 2.6598237246823866e-09, -1.300025009986248e-08, 6.046995022541919e-08, -2.670793853940612e-07, 1.1173875391201037e-06, -4.4167383584587505e-06, 1.6448448070728896e-05, -5.754195010082104e-05, 0.00018850288509584165, -0.0005763755745385824, 0.0016394756169413357, -0.004324309995050576, 0.010546460394594998, -0.02373741480589947, 0.04930528423967071, -0.09490109704804764, 0.17162090152220877, -0.3046826723431984, 0.6767952744094761])
A:jax._src.lax.special.i0e_coeffs_b->numpy.array([-7.233180487874754e-18, -4.830504485944182e-18, 4.46562142029676e-17, 3.461222867697461e-17, -2.8276239805165836e-16, -3.425485619677219e-16, 1.7725601330565263e-15, 3.8116806693526224e-15, -9.554846698828307e-15, -4.150569347287222e-14, 1.54008621752141e-14, 3.8527783827421426e-13, 7.180124451383666e-13, -1.7941785315068062e-12, -1.3215811840447713e-11, -3.1499165279632416e-11, 1.1889147107846439e-11, 4.94060238822497e-10, 3.3962320257083865e-09, 2.266668990498178e-08, 2.0489185894690638e-07, 2.8913705208347567e-06, 6.889758346916825e-05, 0.0033691164782556943, 0.8044904110141088])
A:jax._src.lax.special.half->full_like(x, 0.5)
A:jax._src.lax.special.thirty_two->full_like(x, 32.0)
A:jax._src.lax.special.result_le_8->evaluate_chebyshev_polynomial(half * x - two, i0e_coeffs_a)
A:jax._src.lax.special.result_gt_8->div(evaluate_chebyshev_polynomial(thirty_two / x - two, i0e_coeffs_b), sqrt(x))
A:jax._src.lax.special.regularized_incomplete_beta_p->standard_naryop([_float, _float, _float], 'regularized_incomplete_beta')
A:jax._src.lax.special.lgamma_p->standard_unop(_float, 'lgamma')
A:jax._src.lax.special.digamma_p->standard_unop(_float, 'digamma')
A:jax._src.lax.special.polygamma_p->standard_naryop([_float, _float], 'polygamma')
A:jax._src.lax.special.igamma_p->standard_naryop([_float, _float], 'igamma')
A:jax._src.lax.special.igamma_grad_a_p->standard_naryop([_float, _float], 'igamma_grad_a')
A:jax._src.lax.special.igammac_p->standard_naryop([_float, _float], 'igammac')
A:jax._src.lax.special.random_gamma_grad_p->standard_naryop([_float, _float], 'random_gamma_grad')
A:jax._src.lax.special.zeta_p->standard_naryop([_float, _float], 'zeta')
A:jax._src.lax.special.bessel_i0e_p->standard_unop(_float, 'bessel_i0e')
A:jax._src.lax.special.bessel_i1e_p->standard_unop(_float, 'bessel_i1e')
A:jax._src.lax.special.safe_x->select(x_is_not_tiny, x, full_like(x, eps))
A:jax._src.lax.special.dy_dx->select(x_is_not_tiny, dy_dx, full_like(x, 0.5))
A:jax._src.lax.special.erf_p->standard_unop(_float, 'erf')
A:jax._src.lax.special.erfc_p->standard_unop(_float, 'erfc')
A:jax._src.lax.special.erf_inv_p->standard_unop(_float, 'erf_inv')
jax._src.lax.special.IgammaMode(Enum)
jax._src.lax.special._any(predicates:Array)->Array
jax._src.lax.special._bessel_i1e_jvp(g,y,x)
jax._src.lax.special._i0e_impl32(x)
jax._src.lax.special._i0e_impl64(x)
jax._src.lax.special._igamma_series(ax,x,a,enabled,dtype,mode)
jax._src.lax.special._igammac_continued_fraction(ax,x,a,enabled,dtype,mode)
jax._src.lax.special._up_and_broadcast(doit)
jax._src.lax.special.bessel_i0e(x:ArrayLike)->Array
jax._src.lax.special.bessel_i0e_impl(x)
jax._src.lax.special.bessel_i1e(x:ArrayLike)->Array
jax._src.lax.special.betainc(a:ArrayLike,b:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.betainc_grad_not_implemented(g,a,b,x)
jax._src.lax.special.betainc_gradx(g,a,b,x)
jax._src.lax.special.digamma(x:ArrayLike)->Array
jax._src.lax.special.erf(x:ArrayLike)->Array
jax._src.lax.special.erf_inv(x:ArrayLike)->Array
jax._src.lax.special.erfc(x:ArrayLike)->Array
jax._src.lax.special.evaluate_chebyshev_polynomial(x,coefficients)
jax._src.lax.special.igamma(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.igamma_grad_a(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.igamma_grad_a_impl(a,x,*,dtype)
jax._src.lax.special.igamma_grada(g,a,x)
jax._src.lax.special.igamma_gradx(g,a,x)
jax._src.lax.special.igamma_impl(a,x,*,dtype)
jax._src.lax.special.igammac(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.igammac_grada(g,a,x)
jax._src.lax.special.igammac_gradx(g,a,x)
jax._src.lax.special.igammac_impl(a,x,*,dtype)
jax._src.lax.special.lentz_thompson_barnett_algorithm(*,num_iterations,small,threshold,nth_partial_numerator,nth_partial_denominator,inputs)
jax._src.lax.special.lgamma(x:ArrayLike)->Array
jax._src.lax.special.polygamma(m:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.polygamma_gradm(g,m,x)
jax._src.lax.special.polygamma_gradx(g,m,x)
jax._src.lax.special.random_gamma_grad(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.special.random_gamma_grad_impl(a,x,*,dtype)
jax._src.lax.special.regularized_incomplete_beta_impl(a,b,x,*,dtype)
jax._src.lax.special.zeta(x:ArrayLike,q:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/linalg.py----------------------------------------
A:jax._src.lax.linalg.TFun->TypeVar('TFun', bound=Callable[..., Any])
A:jax._src.lax.linalg.sig->inspect.signature(f)
A:jax._src.lax.linalg.x->jax._src.interpreters.batching.moveaxis(x, bd, 0)
A:jax._src.lax.linalg.(v, w)->jax._src.dispatch.apply_primitive(eigh_p, operand, lower=lower, sort_eigenvalues=sort_eigenvalues, subset_by_index=subset_by_index)
A:jax._src.lax.linalg.permutation->jax.lax.broadcasted_iota(jnp.int32, batch_dims + (m,), len(batch_dims))
A:jax._src.lax.linalg.(lu, pivots, permutation)->Primitive('lu').bind(a)
A:jax._src.lax.linalg.(q, r)->Primitive('qr').bind(x, full_matrices=False)
A:jax._src.lax.linalg.result->jax._src.numpy.lax_numpy.moveaxis(result, 0, -1)
A:jax._src.lax.linalg.b->jax._src.numpy.lax_numpy.moveaxis(b, -1, 0)
A:jax._src.lax.linalg.out->standard_primitive(_triangular_solve_shape_rule, _triangular_solve_dtype_rule, 'triangular_solve').bind(a, b, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.lax.linalg.out_shape->tuple((d_a if d_b == 1 else d_b for (d_a, d_b) in zip(a.shape[:-1] + (1,), b.shape)))
A:jax._src.lax.linalg.(lu_, _, permutation)->lu(lax.stop_gradient(a))
A:jax._src.lax.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: _matvec_multiply(a, x), solve=lambda _, x: lu_solve(lu_, permutation, x, trans=0), transpose_solve=lambda _, x: lu_solve(lu_, permutation, x, trans=1))
A:jax._src.lax.linalg.L->jax._src.numpy.lax_numpy.tril(cholesky_p.bind(x))
A:jax._src.lax.linalg.l->jax.lax.pad(jnp.tril(lu[..., :, :k], -1), zero, l_padding)
A:jax._src.lax.linalg.tmp->triangular_solve(L, sigma_dot, left_side=False, transpose_a=True, conjugate_a=True, lower=True)
A:jax._src.lax.linalg.L_dot->jax.lax.batch_matmul(L, phi(triangular_solve(L, tmp, left_side=True, transpose_a=False, lower=True)), precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.cholesky_p->standard_unop(_float | _complex, 'cholesky')
A:jax._src.lax.linalg.(result, info)->jax._src.lib.lapack.potrf_hlo(operand_aval.dtype, operand, lower=True, a_shape_vals=op_shape_vals)
A:jax._src.lax.linalg.op_shape_vals->jax._src.interpreters.mlir.eval_dynamic_shape_as_ivals(ctx, operand_aval.shape)
A:jax._src.lax.linalg.ok->jax._src.interpreters.mlir.compare_hlo(info, mlir.full_like_aval(ctx, 0, ShapedArray(batch_dims, np.dtype(np.int32))), 'EQ', 'SIGNED')
A:jax._src.lax.linalg.select_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.lax.linalg.vlvr->raise_to_shaped(operand).update(shape=batch_dims + (n, n), dtype=dtype)
A:jax._src.lax.linalg.w->w_real.astype(a.dtype)
A:jax._src.lax.linalg.(w, vl, vr, info)->jax._src.lib.lapack.geev_hlo(operand_aval.dtype, operand, input_shape_vals=op_shape_vals, jobvl=compute_left_eigenvectors, jobvr=compute_right_eigenvectors)
A:jax._src.lax.linalg.select_w_aval->ShapedArray(batch_dims + (1,), np.dtype(np.bool_))
A:jax._src.lax.linalg.select_vl_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.vl->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_vl_aval, broadcast_dimensions=range(len(batch_dims))), select_vl_aval, vl, aval, _nan_like_hlo(ctx, aval), aval)
A:jax._src.lax.linalg.select_vr_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.vr->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_vr_aval, broadcast_dimensions=range(len(batch_dims))), select_vr_aval, vr, aval, _nan_like_hlo(ctx, aval), aval)
A:jax._src.lax.linalg.(l, v)->eig(a, compute_left_eigenvectors=False)
A:jax._src.lax.linalg.eig_p->Primitive('eig')
A:jax._src.lax.linalg.(w, v)->jax._src.dispatch.apply_primitive(eigh_jacobi_p, operand, lower=lower, sort_eigenvalues=sort_eigenvalues)
A:jax._src.lax.linalg.v->jax._src.numpy.lax_numpy.empty(batch_shape + (0, 0), dtype=a.dtype)
A:jax._src.lax.linalg.reshape_aval->operand_aval.update(shape=operand_aval.shape[:-1])
A:jax._src.lax.linalg.eigvals_type->jax._src.interpreters.mlir.aval_to_ir_type(ctx.avals_out[0])
A:jax._src.lax.linalg.eigvecs_type->jax._src.interpreters.mlir.aval_to_ir_type(ctx.avals_out[1])
A:jax._src.lax.linalg.op->jax._src.interpreters.mlir.custom_call('ProductOfElementaryHouseholderReflectors', result_types=[mlir.aval_to_ir_type(aval_out)], operands=[a, taus], api_version=1, result_shapes=result_shapes)
A:jax._src.lax.linalg.eigh_jacobi_p->Primitive('eigh_jacobi')
A:jax._src.lax.linalg.batch_size->jax._src.interpreters.mlir.ir_constant(np.int32(batch_size))
A:jax._src.lax.linalg.(v, w, info)->syevd_impl(operand_aval.dtype, operand, a_shape_vals=op_shape_vals, lower=lower)
A:jax._src.lax.linalg.zeros->jax._src.interpreters.mlir.full_like_aval(ctx, 0, ShapedArray(batch_dims, np.dtype(np.int32)))
A:jax._src.lax.linalg.select_v_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.(eig_vals, eig_vecs)->eigh_qdwh(x)
A:jax._src.lax.linalg.mask->jax._src.numpy.ufuncs.logical_not(jnp.tri(n, k=-1, dtype=bool))
A:jax._src.lax.linalg.re->jax.lax.select(mask, lax.real(x), _T(lax.real(x)))
A:jax._src.lax.linalg.im_mask->jax._src.numpy.ufuncs.logical_not(jnp.tri(n, k=0, dtype=bool))
A:jax._src.lax.linalg.im->jax.lax.select(mask, im, -_T(im))
A:jax._src.lax.linalg.(v, w_real)->Primitive('eigh').bind(symmetrize(a), lower=lower, sort_eigenvalues=sort_eigenvalues, subset_by_index=subset_by_index)
A:jax._src.lax.linalg.eye_n->jax._src.numpy.lax_numpy.eye(n, dtype=a.dtype)
A:jax._src.lax.linalg.dot->partial(lax.dot if g_a.ndim == 2 else lax.batch_matmul, precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.vdag_adot_v->dot(dot(_H(v), a_dot), v)
A:jax._src.lax.linalg.dv->dot(v, ufuncs.multiply(Fmat, vdag_adot_v))
A:jax._src.lax.linalg.dw->jax._src.numpy.ufuncs.real(jnp.diagonal(vdag_adot_v, axis1=-2, axis2=-1))
A:jax._src.lax.linalg.eigh_p->Primitive('eigh')
A:jax._src.lax.linalg._triangular_solve_dtype_rule->partial(naryop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex), 'triangular_solve')
A:jax._src.lax.linalg.g_a->jax.lax.neg(g_a)
A:jax._src.lax.linalg.cotangent_b->tridiagonal_solve(dl, d, du, cotangent)
A:jax._src.lax.linalg.y->jax._src.interpreters.batching.bdim_at_front(y, by, size)
A:jax._src.lax.linalg.y_flat->jax._src.interpreters.batching.bdim_at_front(y, by, size).reshape(y.shape[:-3] + (y.shape[-3] * y.shape[-2], y.shape[-1]))
A:jax._src.lax.linalg.out_flat->tridiagonal_solve(dl, d, du, b_flat)
A:jax._src.lax.linalg.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax._src.lax.linalg.triangular_solve_p->standard_primitive(_triangular_solve_shape_rule, _triangular_solve_dtype_rule, 'triangular_solve')
A:jax._src.lax.linalg.a->_broadcasting_select_hlo(ctx, ok, select_a_aval, a, a_aval, _nan_like_hlo(ctx, a_aval), a_aval)
A:jax._src.lax.linalg.alpha->jax._src.interpreters.mlir.ir_constant(np.array(1, dtype=a_aval.dtype))
A:jax._src.lax.linalg.b_shape_vals->jax._src.interpreters.mlir.eval_dynamic_shape_as_ivals(ctx, b_aval.shape)
A:jax._src.lax.linalg.iotas->jax._src.numpy.lax_numpy.ix_(*(lax.iota(jnp.int32, b) for b in batch_dims + (1,)))
A:jax._src.lax.linalg.(result, _)->jax.lax.fori_loop(np.array(0, np.int32), np.array(k, np.int32), _lu_pivots_body_fn, (permutation, swaps))
A:jax._src.lax.linalg.pivots->raise_to_shaped(pivots)
A:jax._src.lax.linalg.permutations->raise_to_shaped(pivots).update(shape=batch_dims + (permutation_size,))
A:jax._src.lax.linalg.lu_pivots_to_permutation_p->Primitive('lu_pivots_to_permutation')
A:jax._src.lax.linalg.m_idx->jax._src.numpy.lax_numpy.arange(m)
A:jax._src.lax.linalg.n_idx->jax._src.numpy.lax_numpy.arange(n)
A:jax._src.lax.linalg.magnitude->jax._src.numpy.ufuncs.abs(a[:, k])
A:jax._src.lax.linalg.i->jax._src.numpy.lax_numpy.argmax(jnp.where(m_idx >= k, magnitude, -jnp.inf))
A:jax._src.lax.linalg.pivot->raise_to_shaped(operand).update(shape=batch_dims + (min(m, n),), dtype=jnp.int32)
A:jax._src.lax.linalg.perm->raise_to_shaped(operand).update(shape=batch_dims + (m,), dtype=jnp.int32)
A:jax._src.lax.linalg.r->jax._src.numpy.lax_numpy.triu(r)
A:jax._src.lax.linalg.(block_pivot, block_perm, lu_block)->_lu_unblocked(a[k:, k:k + b])
A:jax._src.lax.linalg.fn->jax._src.api.vmap(fn)
A:jax._src.lax.linalg.(lu, pivot, perm)->jax._src.dispatch.apply_primitive(lu_p, operand)
A:jax._src.lax.linalg.operand->raise_to_shaped(operand)
A:jax._src.lax.linalg.a_shape->jax._src.numpy.lax_numpy.shape(a)
A:jax._src.lax.linalg.k->min(m, n)
A:jax._src.lax.linalg.ndims->len(a_shape)
A:jax._src.lax.linalg.zero->jax._src.lax.lax._const(lu, 0)
A:jax._src.lax.linalg.u_eye->jax.lax.pad(jnp.eye(n - k, n - k, dtype=dtype), zero, ((k, 0, 0), (k, 0, 0)))
A:jax._src.lax.linalg.la->triangular_solve(l, x, left_side=True, transpose_a=False, lower=True, unit_diagonal=True)
A:jax._src.lax.linalg.lau->triangular_solve(u, la, left_side=False, transpose_a=False, lower=False)
A:jax._src.lax.linalg.l_dot->jax._src.numpy.lax_numpy.matmul(l, jnp.tril(lau, -1), precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.u_dot->jax._src.numpy.lax_numpy.matmul(jnp.triu(lau), u, precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.(lu, pivot, info)->getrf_impl(operand_aval.dtype, operand, a_shape_vals=op_shape_vals)
A:jax._src.lax.linalg.select_lu_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.lu->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_lu_aval, broadcast_dimensions=range(len(batch_dims))), select_lu_aval, lu, out_aval, _nan_like_hlo(ctx, out_aval), out_aval)
A:jax._src.lax.linalg.sub_ctx->ctx.replace(primitive=None, avals_in=[pivot_aval], avals_out=[perm_aval])
A:jax._src.lax.linalg.perm_fn->jax._src.interpreters.mlir.lower_fun(lambda x: lu_pivots_to_permutation(x, m), multiple_results=False)
A:jax._src.lax.linalg.(perm,)->perm_fn(sub_ctx, pivot)
A:jax._src.lax.linalg.lu_p->Primitive('lu')
A:jax._src.lax.linalg.(a_out, taus)->batched_geqrf_impl(a_aval.dtype, a)
A:jax._src.lax.linalg.taus->jax._src.numpy.lax_numpy.where((info == 0)[..., None], taus, nan)
A:jax._src.lax.linalg.ts_type->jax._src.interpreters.mlir.aval_to_ir_type(ctx.avals_out[0])
A:jax._src.lax.linalg.r_type->jax._src.interpreters.mlir.aval_to_ir_type(ctx.avals_out[1])
A:jax._src.lax.linalg.batch->math.prod(batch_dims)
A:jax._src.lax.linalg.(a_out, taus, info_geqrf)->geqrf_impl(a_aval.dtype, a, a_shape_vals=a_shape_vals)
A:jax._src.lax.linalg.a_shape_vals->jax._src.interpreters.mlir.eval_dynamic_shape_as_ivals(ctx, operand_aval.shape)
A:jax._src.lax.linalg.select_ok_a_aval->ShapedArray(batch_dims + [1, 1], np.dtype(np.bool_))
A:jax._src.lax.linalg.ok_a->jax._src.interpreters.mlir.broadcast_in_dim(ctx, ok, select_ok_a_aval, broadcast_dimensions=range(len(batch_dims)))
A:jax._src.lax.linalg.a_out->_broadcasting_select_hlo(ctx, ok_a, select_ok_a_aval, a_out, a_aval, _nan_like_hlo(ctx, a_aval), a_aval)
A:jax._src.lax.linalg.select_ok_taus_aval->ShapedArray(batch_dims + [1], np.dtype(np.bool_))
A:jax._src.lax.linalg.ok_taus->jax._src.interpreters.mlir.broadcast_in_dim(ctx, ok, select_ok_taus_aval, broadcast_dimensions=range(len(batch_dims)))
A:jax._src.lax.linalg.geqrf_p->Primitive('geqrf')
A:jax._src.lax.linalg.(a, info_orgqr)->orgqr_impl(a_aval.dtype, a, taus, a_shape_vals=a_shape_vals, tau_shape_vals=tau_shape_vals)
A:jax._src.lax.linalg.tau_shape_vals->jax._src.interpreters.mlir.eval_dynamic_shape_as_ivals(ctx, taus_aval.shape)
A:jax._src.lax.linalg.select_a_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.householder_product_p->Primitive('householder_product')
A:jax._src.lax.linalg.q->householder_product(r, taus)
A:jax._src.lax.linalg.dx_rinv->triangular_solve(r, dx)
A:jax._src.lax.linalg.qt_dx_rinv->jax._src.numpy.lax_numpy.matmul(_H(q), dx_rinv)
A:jax._src.lax.linalg.qt_dx_rinv_lower->jax._src.numpy.lax_numpy.tril(qt_dx_rinv, -1)
A:jax._src.lax.linalg.I->jax.lax.expand_dims(jnp.eye(n, dtype=do.dtype), range(qt_dx_rinv.ndim - 2))
A:jax._src.lax.linalg.dr->jax._src.numpy.lax_numpy.matmul(qt_dx_rinv - do, r)
A:jax._src.lax.linalg.(r, taus)->geqrf(a)
A:jax._src.lax.linalg.qr_p->Primitive('qr')
A:jax._src.lax.linalg.s->fn(a)
A:jax._src.lax.linalg.u->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_u_aval, broadcast_dimensions=range(len(batch_dims))), select_u_aval, u, u_aval, _nan_like_hlo(ctx, u_aval), u_aval)
A:jax._src.lax.linalg.vt->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_v_aval, broadcast_dimensions=range(len(batch_dims))), select_v_aval, vt, vt_aval, _nan_like_hlo(ctx, vt_aval), vt_aval)
A:jax._src.lax.linalg.(s, U, Vt)->Primitive('svd').bind(A, full_matrices=False, compute_uv=True)
A:jax._src.lax.linalg.ds->jax._src.numpy.ufuncs.real(jnp.diagonal(dS, 0, -2, -1))
A:jax._src.lax.linalg.s_diffs_zeros->jax.lax.expand_dims(s_diffs_zeros, range(s_diffs.ndim - 2))
A:jax._src.lax.linalg.s_zeros->(s == 0).astype(s.dtype)
A:jax._src.lax.linalg.s_inv_mat->jax._src.numpy.lax_numpy.vectorize(jnp.diag, signature='(k)->(k,k)')(s_inv)
A:jax._src.lax.linalg.(s, u, vt, info)->gesvd_impl(operand_aval.dtype, operand, full_matrices=full_matrices, compute_uv=compute_uv, a_shape_vals=a_shape_vals)
A:jax._src.lax.linalg.select_s_aval->ShapedArray(batch_dims + (1,), np.dtype(np.bool_))
A:jax._src.lax.linalg.select_u_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.(u, s, vh)->fn(a)
A:jax._src.lax.linalg.outs->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax._src.lax.linalg.svd_p->Primitive('svd')
A:jax._src.lax.linalg.b_flat->jax._src.numpy.lax_numpy.moveaxis(b, -1, 0).reshape(b.shape[:-3] + (b.shape[-3], b.shape[-2] * b.shape[-1]))
A:jax._src.lax.linalg.dl->jax._src.numpy.lax_numpy.moveaxis(dl, -1, 0)
A:jax._src.lax.linalg.d->jax._src.numpy.lax_numpy.where((info == 0)[..., None], d, real_type(jnp.nan))
A:jax._src.lax.linalg.du->jax._src.numpy.lax_numpy.moveaxis(du, -1, 0)
A:jax._src.lax.linalg.tridiagonal_solve_p->Primitive('tridiagonal_solve')
A:jax._src.lax.linalg.(_, tu_)->jax.lax.scan(lambda tu_, x: double(fwd1, (tu_, x)), du[0] / d[0], (d, du, dl), unroll=32)
A:jax._src.lax.linalg.(_, b_)->jax.lax.scan(lambda b_, x: double(fwd2, (b_, x)), b[0] / d[0:1], (b, d, prepend_zero(tu_), dl), unroll=32)
A:jax._src.lax.linalg.(_, x_)->jax.lax.scan(lambda x_, x: double(bwd1, (x_, x)), b_[-1], (b_[::-1], tu_[::-1]), unroll=32)
A:jax._src.lax.linalg.T->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_T_aval, broadcast_dimensions=range(len(batch_dims))), select_T_aval, T, ctx.avals_out[0], _nan_like_hlo(ctx, ctx.avals_out[0]), ctx.avals_out[0])
A:jax._src.lax.linalg.vs->_broadcasting_select_hlo(ctx, mlir.broadcast_in_dim(ctx, ok, select_vs_aval, broadcast_dimensions=range(len(batch_dims))), select_vs_aval, vs, ctx.avals_out[1], _nan_like_hlo(ctx, ctx.avals_out[1]), ctx.avals_out[1])
A:jax._src.lax.linalg.gees_result->jax._src.lib.lapack.gees_hlo(operand_aval.dtype, operand, jobvs=compute_schur_vectors, sort=sort_eig_vals, select=select_callable, a_shape_vals=a_shape_vals)
A:jax._src.lax.linalg.select_T_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.select_vs_aval->ShapedArray(batch_dims + (1, 1), np.dtype(np.bool_))
A:jax._src.lax.linalg.schur_p->Primitive('schur')
A:jax._src.lax.linalg.hessenberg_p->Primitive('hessenberg')
A:jax._src.lax.linalg.(a, taus, info)->jax._src.lib.lapack.gehrd_hlo(a_aval.dtype, a)
A:jax._src.lax.linalg.select_taus_aval->ShapedArray(batch_dims + (1,), np.dtype(np.bool_))
A:jax._src.lax.linalg.(arr, d, e, taus, info)->Primitive('tridiagonal').bind(jnp.asarray(a), lower=lower)
A:jax._src.lax.linalg.nan->jax._src.numpy.lax_numpy.where((info == 0)[..., None, None], arr, nan).dtype.type(jnp.nan)
A:jax._src.lax.linalg.arr->jax._src.numpy.lax_numpy.where((info == 0)[..., None, None], arr, nan)
A:jax._src.lax.linalg.e->jax._src.numpy.lax_numpy.where((info == 0)[..., None], e, real_type(jnp.nan))
A:jax._src.lax.linalg.tridiagonal_p->Primitive('tridiagonal')
A:jax._src.lax.linalg.(a, d, e, taus, info)->sytrd_impl(a_aval.dtype, a, lower=lower)
A:jax._src.lax.linalg.out_shapes->list(lax_internal.broadcast_shapes(tuple(which_aval.shape), tuple(x_aval.shape), tuple(y_aval.shape)))
A:jax._src.lax.linalg.(which, x, y)->jax._src.interpreters.mlir.multi_broadcast_in_dim(ctx, (which, x, y), (which_aval, x_aval, y_aval), out_shapes)
jax._src.lax.linalg._H(x:Array)->Array
jax._src.lax.linalg._T(x:Array)->Array
jax._src.lax.linalg._broadcasting_select_hlo(ctx,which,which_aval,x,x_aval,y,y_aval)->ir.Value
jax._src.lax.linalg._check_solve_shapes(a:Array,b:Array)
jax._src.lax.linalg._cholesky_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._cholesky_cpu_lowering(ctx,operand)
jax._src.lax.linalg._cholesky_jvp_rule(primals,tangents)
jax._src.lax.linalg._cholesky_lowering(ctx,x)
jax._src.lax.linalg._eig_cpu_lowering(ctx,operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg._eigh_abstract_eval(operand,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._eigh_batching_rule(batched_args,batch_dims,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._eigh_cpu_gpu_lowering(syevd_impl,ctx,operand,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._eigh_impl(operand,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._eigh_jacobi_abstract_eval(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jacobi_impl(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jacobi_lowering_rule(ctx,operand,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jvp_rule(primals,tangents,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._eigh_tpu_impl(x,*,lower,sort_eigenvalues,subset_by_index)
jax._src.lax.linalg._empty_svd(a,*,full_matrices,compute_uv)
jax._src.lax.linalg._generic_lu_pivots_to_permutation(swaps,permutation_size)
jax._src.lax.linalg._geqrf_abstract_eval(operand)
jax._src.lax.linalg._geqrf_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._geqrf_cpu_gpu_lowering(geqrf_impl,batched_geqrf_impl,ctx,a,*,platform:str)
jax._src.lax.linalg._geqrf_lowering_rule(ctx,operand)
jax._src.lax.linalg._hessenberg_abstract_eval(a)
jax._src.lax.linalg._hessenberg_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._hessenberg_cpu_hlo(ctx,a)
jax._src.lax.linalg._householder_product_abstract_eval(a,taus)
jax._src.lax.linalg._householder_product_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._householder_product_cpu_gpu_lowering(orgqr_impl,ctx,a,taus,*,platform:str)
jax._src.lax.linalg._householder_product_lowering_rule(ctx,a,taus)
jax._src.lax.linalg._lu_abstract_eval(operand)
jax._src.lax.linalg._lu_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._lu_blocked(a,block_size=128)
jax._src.lax.linalg._lu_cpu_gpu_lowering(getrf_impl,ctx,operand,*,platform:str)
jax._src.lax.linalg._lu_impl(operand)
jax._src.lax.linalg._lu_jvp_rule(primals,tangents)
jax._src.lax.linalg._lu_pivots_body_fn(i,permutation_and_swaps)
jax._src.lax.linalg._lu_pivots_to_permutation_abstract_eval(pivots,*,permutation_size)
jax._src.lax.linalg._lu_pivots_to_permutation_batching_rule(batched_args,batch_dims,*,permutation_size)
jax._src.lax.linalg._lu_pivots_to_permutation_gpu_lowering(lowering,ctx,pivots,*,permutation_size)
jax._src.lax.linalg._lu_python(x)
jax._src.lax.linalg._lu_solve(lu:Array,permutation:Array,b:Array,trans:int)->Array
jax._src.lax.linalg._lu_solve_core(lu:Array,permutation:Array,b:Array,trans:int)->Array
jax._src.lax.linalg._lu_tpu_lowering_rule(ctx,operand)
jax._src.lax.linalg._lu_unblocked(a)
jax._src.lax.linalg._matvec_multiply(a:Array,b:Array)->Array
jax._src.lax.linalg._nan_like_hlo(ctx:mlir.LoweringRuleContext,aval)->ir.Value
jax._src.lax.linalg._qr_abstract_eval(operand,*,full_matrices)
jax._src.lax.linalg._qr_batching_rule(batched_args,batch_dims,*,full_matrices)
jax._src.lax.linalg._qr_impl(operand,*,full_matrices)
jax._src.lax.linalg._qr_lowering(a,*,full_matrices)
jax._src.lax.linalg._schur_abstract_eval(operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_batching_rule(batched_args,batch_dims,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_cpu_lowering(ctx,operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_impl(operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_jvp_rule(primals,tangents,**kwds)
jax._src.lax.linalg._schur_lowering(ctx,*args,**kwargs)
jax._src.lax.linalg._solve(a:Array,b:Array)->Array
jax._src.lax.linalg._svd_abstract_eval(operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_batching_rule(batched_args,batch_dims,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_cpu_gpu_lowering(gesvd_impl,ctx,operand,*,full_matrices,compute_uv,platform:str)
jax._src.lax.linalg._svd_impl(operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_jvp_rule(primals,tangents,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_tpu(a,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_tpu_lowering_rule(ctx,operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._triangular_solve_batching_rule(batched_args,batch_dims,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_cpu_lower(ctx,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_jvp_rule_a(g_a,ans,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_lowering(ctx,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_shape_rule(a,b,*,left_side=False,**unused_kwargs)
jax._src.lax.linalg._triangular_solve_transpose_rule(cotangent,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._tridiagonal_abstract_eval(a,*,lower)
jax._src.lax.linalg._tridiagonal_batching_rule(batched_args,batch_dims,*,lower)
jax._src.lax.linalg._tridiagonal_cpu_gpu_hlo(sytrd_impl,ctx,a,*,lower)
jax._src.lax.linalg._tridiagonal_solve_batching_rule(batched_args,batch_dims,*,m,n,ldb,t)
jax._src.lax.linalg._tridiagonal_solve_gpu_lowering(lowering,ctx,dl,d,du,b,*,m,n,ldb,t)
jax._src.lax.linalg._tridiagonal_solve_jax(dl,d,du,b,**kw)
jax._src.lax.linalg._tridiagonal_solve_transpose_rule(cotangent,dl,d,du,b,*,m,n,ldb,t)
jax._src.lax.linalg._warn_on_positional_kwargs(f:TFun)->TFun
jax._src.lax.linalg.cholesky(x:Array,*,symmetrize_input:bool=True)->Array
jax._src.lax.linalg.eig(x:ArrayLike,*,compute_left_eigenvectors:bool=True,compute_right_eigenvectors:bool=True)->list[Array]
jax._src.lax.linalg.eig_abstract_eval(operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_batching_rule(batched_args,batch_dims,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_impl(operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_jvp_rule(primals,tangents,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_lower(*args,**kw)
jax._src.lax.linalg.eigh(x:Array,*,lower:bool=True,symmetrize_input:bool=True,sort_eigenvalues:bool=True,subset_by_index:Optional[tuple[int,int]]=None)->tuple[Array, Array]
jax._src.lax.linalg.eigh_jacobi(x:ArrayLike,*,lower:bool=True,sort_eigenvalues:bool=True)->tuple[Array, Array]
jax._src.lax.linalg.geqrf(a:ArrayLike)->tuple[Array, Array]
jax._src.lax.linalg.hessenberg(a:ArrayLike)->tuple[Array, Array]
jax._src.lax.linalg.householder_product(a:ArrayLike,taus:ArrayLike)->Array
jax._src.lax.linalg.lu(x:ArrayLike)->tuple[Array, Array, Array]
jax._src.lax.linalg.lu_pivots_to_permutation(pivots:ArrayLike,permutation_size:int)->Array
jax._src.lax.linalg.lu_solve(lu:ArrayLike,permutation:ArrayLike,b:ArrayLike,trans:int=0)->Array
jax._src.lax.linalg.qr(x:ArrayLike,*,full_matrices:bool=True)->tuple[Array, Array]
jax._src.lax.linalg.qr_jvp_rule(primals,tangents,*,full_matrices)
jax._src.lax.linalg.schur(x:ArrayLike,*,compute_schur_vectors:bool=True,sort_eig_vals:bool=False,select_callable:Optional[Callable[...,Any]]=None)->tuple[Array, Array]
jax._src.lax.linalg.svd(x:ArrayLike,*,full_matrices:bool=True,compute_uv:bool=True)->Union[Array, tuple[Array, Array, Array]]
jax._src.lax.linalg.symmetrize(x:Array)->Array
jax._src.lax.linalg.triangular_solve(a:ArrayLike,b:ArrayLike,*,left_side:bool=False,lower:bool=False,transpose_a:bool=False,conjugate_a:bool=False,unit_diagonal:bool=False)->Array
jax._src.lax.linalg.tridiagonal(a:ArrayLike,*,lower=True)->tuple[Array, Array, Array, Array]
jax._src.lax.linalg.tridiagonal_solve(dl:Array,d:Array,du:Array,b:Array)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/parallel.py----------------------------------------
A:jax._src.lax.parallel.(leaves, treedef)->jax.tree_util.tree_flatten(x)
A:jax._src.lax.parallel.axis_index_groups->_canonicalize_axis_index_groups(axis_index_groups)
A:jax._src.lax.parallel.out_flat->jax._src.core.AxisPrimitive('pmin').bind(*leaves, axes=axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.x->jax._src.lib.mlir.dialects.hlo.BroadcastInDimOp(mlir.aval_to_ir_type(x_aval.update(shape=new_shape)), x, mlir.dense_int_elements(broadcast_dimensions))
A:jax._src.lax.parallel.n->psum(1, axis_name=axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.idx->jax._src.lax.lax.expand_dims(idx, (-1,))
A:jax._src.lax.parallel.validx->jax._src.numpy.lax_numpy.where(val == x, idx, dtypes.iinfo(dtypes.dtype(idx)).max)
A:jax._src.lax.parallel.axis_space->range(sum((len(group) for group in axis_index_groups)))
A:jax._src.lax.parallel.group_size->len(replica_groups[0])
A:jax._src.lax.parallel.result->jax._src.core.AxisPrimitive('pgather').bind(src_last_batched, idx, axes=axes)
A:jax._src.lax.parallel.pos_contract->unzip2(((lhs_subs.index(n), rhs_subs.index(n)) for n in subs_contract))
A:jax._src.lax.parallel.pos_batch->unzip2(((lhs_subs.index(n), rhs_subs.index(n)) for n in subs_batch))
A:jax._src.lax.parallel.(in_spec, out_spec)->spec.split('->')
A:jax._src.lax.parallel.(all_in_subs, all_in_named)->unzip2(XeinsumSpecParser(in_spec).parse_args())
A:jax._src.lax.parallel.((out_subs, out_named),)->XeinsumSpecParser(out_spec).parse_args()
A:jax._src.lax.parallel.xs->list(operands)
A:jax._src.lax.parallel.other_named->set().union(*[named for (i, named) in enumerate(all_in_named) if i != idx])
A:jax._src.lax.parallel.other_subs->set().union(*[subs for (i, subs) in enumerate(all_in_subs) if i != idx])
A:jax._src.lax.parallel.subs_reduce->list(set(in_subs) - {*out_subs, *other_subs})
A:jax._src.lax.parallel.named_reduce_axes->list(set(in_named) - {*out_named, *other_named})
A:jax._src.lax.parallel.xs[idx]->psum(xs[idx], axis_name=subs_reduce_axes + named_reduce_axes)
A:jax._src.lax.parallel.named_contract->list((set(lhs_named) & set(rhs_named)) - set(out_named))
A:jax._src.lax.parallel.end->self.spec.index(',', self.pos, end)
A:jax._src.lax.parallel.(subscript, cont)->self.parse_subscript()
A:jax._src.lax.parallel.axis_name->self.parse_axis_name()
A:jax._src.lax.parallel.(cont, result)->self.parse_arg()
A:jax._src.lax.parallel.result[pname]->sum(((name,) if isinstance(name, int) else subst(name) for name in axis_name), ())
A:jax._src.lax.parallel.(unmapped_axes, unmapped_vals_in)->transform_unmapped(0, unmapped_vals_in)
A:jax._src.lax.parallel.unmapped_vals_out->prim.bind(*unmapped_vals_in, axes=unmapped_axes, axis_index_groups=None)
A:jax._src.lax.parallel.(mapped_axes, mapped_vals_in)->transform_mapped(0, mapped_vals_in)
A:jax._src.lax.parallel.mapped_vals_out->prim.bind(*mapped_vals_in, axes=mapped_axes, axis_index_groups=None)
A:jax._src.lax.parallel.vals_out->_reduction_with_positional_batcher(prim, vals_in, dims_in, axis_index_groups, lambda d, d_vals_in: (tuple((axis for axis in axes if axis != frame_name)), [if_unmapped(v, axis_size) for v in d_vals_in]), lambda d, d_vals_in: (tuple((axis + (axis >= d) if isinstance(axis, int) else axis if axis != frame_name else d for axis in axes)), d_vals_in))
A:jax._src.lax.parallel.replica_groups->_replica_groups(ctx.module_context.axis_env, axis_name, axis_index_groups)
A:jax._src.lax.parallel.pos_axes->tuple((axis for axis in axes if isinstance(axis, int)))
A:jax._src.lax.parallel.len_0->len(axis_index_groups[0])
A:jax._src.lax.parallel.reducer->jax._src.interpreters.mlir.lower_fun(pos_fn, multiple_results=False)
A:jax._src.lax.parallel.aval_out->aval.update(shape=np.delete(np.array(aval.shape, dtype=np.int64), positional_axes))
A:jax._src.lax.parallel.reducer_ctx->ctx.replace(primitive=None, avals_in=[scalar_aval] * 2, avals_out=[scalar_aval])
A:jax._src.lax.parallel.args->map(_positional_reduce, ctx.avals_in, args)
A:jax._src.lax.parallel.is_spmd->isinstance(axis_context, (sharding_impls.SPMDAxisContext, sharding_impls.ShardingContext))
A:jax._src.lax.parallel.channel->ctx.module_context.new_channel()
A:jax._src.lax.parallel.other_args->dict(channel_handle=hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE), use_global_device_ids=ir.BoolAttr.get(True))
A:jax._src.lax.parallel.op->jax._src.lib.mlir.dialects.hlo.ReduceScatterOp(mlir.aval_to_ir_type(x_aval.update(shape=scatter_out_shape)), x, scatter_dimension=mlir.i64_attr(scatter_dimension), replica_groups=_replica_groups_hlo(replica_groups), **other_args)
A:jax._src.lax.parallel.scalar_aval->jax._src.core.raise_to_shaped(x).update(shape=())
A:jax._src.lax.parallel.scalar_type->jax._src.interpreters.mlir.aval_to_ir_type(scalar_aval)
A:jax._src.lax.parallel.reducer_block->jax._src.lib.mlir.dialects.hlo.ReduceScatterOp(mlir.aval_to_ir_type(x_aval.update(shape=scatter_out_shape)), x, scatter_dimension=mlir.i64_attr(scatter_dimension), replica_groups=_replica_groups_hlo(replica_groups), **other_args).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.parallel.lower_reducer->jax._src.interpreters.mlir.lower_fun(prim.bind, multiple_results=False)
A:jax._src.lax.parallel.out_nodes->lower_reducer(reducer_ctx, *([a] for a in reducer_block.arguments))
A:jax._src.lax.parallel.cts->map(broadcast_positional, cts, args)
A:jax._src.lax.parallel.(nonzero_out_cts, treedef)->jax.tree_util.tree_flatten(cts)
A:jax._src.lax.parallel.nonzero_in_cts->jax._src.core.AxisPrimitive('psum').bind(*nonzero_out_cts, axes=tuple(named_axes), axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.psum_p->jax._src.core.AxisPrimitive('psum')
A:jax._src.lax.parallel.batching.primitive_batchers[psum_p]->partial(_reduction_batcher, psum_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[psum_p]->partial(_batched_reduction_collective, psum_p, lambda v, axis_size: axis_size * v)
A:jax._src.lax.parallel.core.axis_substitution_rules[psum_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.size->math.prod([core.axis_frame(name).size for name in named_axes])
A:jax._src.lax.parallel.pmax_p->jax._src.core.AxisPrimitive('pmax')
A:jax._src.lax.parallel.batching.primitive_batchers[pmax_p]->partial(_reduction_batcher, pmax_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[pmax_p]->partial(_batched_reduction_collective, pmax_p, lambda v, axis_size: v)
A:jax._src.lax.parallel.core.axis_substitution_rules[pmax_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.pmin_p->jax._src.core.AxisPrimitive('pmin')
A:jax._src.lax.parallel.batching.primitive_batchers[pmin_p]->partial(_reduction_batcher, pmin_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[pmin_p]->partial(_batched_reduction_collective, pmin_p, lambda v, axis_size: v)
A:jax._src.lax.parallel.core.axis_substitution_rules[pmin_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.(srcs, dsts)->unzip2(perm)
A:jax._src.lax.parallel.full_perm->full_perm.reshape((-1, 2)).reshape((-1, 2))
A:jax._src.lax.parallel.grp->list(sorted(grp))
A:jax._src.lax.parallel.inverse_perm->list(zip(dsts, srcs))
A:jax._src.lax.parallel.remaining_axes->tuple((axis for axis in axis_name if axis != frame_name))
A:jax._src.lax.parallel.perm_indices->numpy.zeros(axis_size, dtype=int)
A:jax._src.lax.parallel.ppermute_p->jax._src.core.AxisPrimitive('ppermute')
A:jax._src.lax.parallel.batching.primitive_batchers[ppermute_p]->partial(_collective_batcher, ppermute_p)
A:jax._src.lax.parallel.core.axis_substitution_rules[ppermute_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.new_shape->list(x_aval.shape)
A:jax._src.lax.parallel.cur_device_id->axis_index(axis_name)
A:jax._src.lax.parallel.flat_groups->numpy.array(axis_index_groups).flatten()
A:jax._src.lax.parallel.split_count->len(replica_groups[0])
A:jax._src.lax.parallel.channel_handle->jax._src.lib.mlir.dialects.hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE)
A:jax._src.lax.parallel.pos->self.parse_axis_name().index(frame_name)
A:jax._src.lax.parallel.x_concat->_foldaxis(concat_axis, _moveaxis(d, concat_axis, x))
A:jax._src.lax.parallel.input_aval->raise_to_shaped(x)
A:jax._src.lax.parallel.shape->list(src.shape)
A:jax._src.lax.parallel.all_to_all_p->jax._src.core.AxisPrimitive('all_to_all')
A:jax._src.lax.parallel.core.axis_substitution_rules[all_to_all_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.axis_size->psum(1, axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.out->jax._src.core.AxisPrimitive('pdot').bind(x, y, axis_name=axis_name, pos_contract=pos_contract, pos_batch=pos_batch, precision=precision)
A:jax._src.lax.parallel.index->_index_in_group(axis_name, axis_index_groups)
A:jax._src.lax.parallel.outs->jax._src.lax.lax.squeeze(outs, [scatter_dimension])
A:jax._src.lax.parallel.sums->psum(outs, axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.lowering->jax._src.interpreters.mlir.lower_fun(_all_gather_via_psum, multiple_results=False)
A:jax._src.lax.parallel.x_aval->jax._src.core.raise_to_shaped(x)
A:jax._src.lax.parallel.out_shape->list(np.shape(x))
A:jax._src.lax.parallel.y->_splitaxis(dy, axis_size, y)
A:jax._src.lax.parallel.all_gather_p->jax._src.core.AxisPrimitive('all_gather')
A:jax._src.lax.parallel.core.axis_substitution_rules[all_gather_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.scatter_out_shape->list(x_aval.shape)
A:jax._src.lax.parallel.reduce_scatter_p->jax._src.core.AxisPrimitive('reduce_scatter')
A:jax._src.lax.parallel.reduce_scatter_lowering_for_psum->partial(_reduce_scatter_lowering, lax.add_p, psum)
A:jax._src.lax.parallel.core.axis_substitution_rules[reduce_scatter_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.bind->partial(reduce_scatter_p.bind, axis_name=axis_name, scatter_dimension=scatter_dimension, axis_index_groups=axis_index_groups, axis_size=axis_size, tiled=tiled)
A:jax._src.lax.parallel.axis_pos->list(axis_env.names).index(axis_name)
A:jax._src.lax.parallel.div->jax._src.interpreters.mlir.ir_constant(np.array(nreplicas * math.prod(axis_env.sizes[axis_pos + 1:]), dtype=np.uint32))
A:jax._src.lax.parallel.mod->jax._src.interpreters.mlir.ir_constant(np.array(axis_env.sizes[axis_pos], dtype=np.uint32))
A:jax._src.lax.parallel.device_id->jax._src.lib.mlir.dialects.hlo.ReplicaIdOp()
A:jax._src.lax.parallel.unsigned_index->jax._src.lib.mlir.dialects.hlo.RemOp(hlo.DivOp(device_id, div), mod)
A:jax._src.lax.parallel.frame->jax._src.core.axis_frame(name)
A:jax._src.lax.parallel.axis_index_p->jax._src.core.Primitive('axis_index')
A:jax._src.lax.parallel.core.axis_substitution_rules[axis_index_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.trace->jax._src.core.axis_frame(name).main_trace.with_cur_sublevel()
A:jax._src.lax.parallel.pdot_p->jax._src.core.AxisPrimitive('pdot')
A:jax._src.lax.parallel.core.axis_substitution_rules[pdot_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.common_named_shape->jax._src.core.join_named_shapes(x.named_shape, y.named_shape)
A:jax._src.lax.parallel.remaining_axis_names->tuple((n for n in axis_name if n != frame_name))
A:jax._src.lax.parallel.((pos_contract, pos_batch), result_batch_dim)->jax._src.lax.lax._dot_general_batch_dim_nums((x.ndim, y.ndim), dims_in, [pos_contract, pos_batch])
A:jax._src.lax.parallel.local_out->jax._src.lax.lax.dot_general(x, y, dimension_numbers=(pos_contract, pos_batch), precision=precision, preferred_element_type=None)
A:jax._src.lax.parallel.src_axes_front->moveaxis(src, axes, range(len(axes)))
A:jax._src.lax.parallel.src_one_axis_front->moveaxis(src, axes, range(len(axes))).reshape((-1,) + non_axes_shape)
A:jax._src.lax.parallel.offset_dims->tuple(range(idx.ndim - 1, idx.ndim + src_one_axis_front.ndim - 2))
A:jax._src.lax.parallel.dnums->jax._src.lax.slicing.GatherDimensionNumbers(offset_dims=offset_dims, collapsed_slice_dims=(0,), start_index_map=(0,))
A:jax._src.lax.parallel.src_last_batched->moveaxis(src, dsrc, -1)
A:jax._src.lax.parallel.new_axes->tuple((dsrc if axis == frame_name else axis + (dsrc <= axis) if isinstance(axis, int) else axis for axis in axes))
A:jax._src.lax.parallel.pgather_p->jax._src.core.AxisPrimitive('pgather')
A:jax._src.lax.parallel.core.axis_substitution_rules[pgather_p]->partial(_subst_all_names_in_param, 'axes')
jax._src.lax.parallel.XeinsumSpecParser(self,spec:str)
jax._src.lax.parallel.XeinsumSpecParser.__init__(self,spec:str)
jax._src.lax.parallel.XeinsumSpecParser.cur(self)
jax._src.lax.parallel.XeinsumSpecParser.eof(self)
jax._src.lax.parallel.XeinsumSpecParser.maybe_take(self,char:str,on_eof:bool=False)
jax._src.lax.parallel.XeinsumSpecParser.parse_arg(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_args(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_axis_name(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_subscript(self)
jax._src.lax.parallel._all_gather_abstract_eval(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_batched_collective(frame_size,frame_name,_,vals_in,dims_in,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_batcher(vals_in,dims_in,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_impl(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_lowering(ctx,x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled,platform=None)
jax._src.lax.parallel._all_gather_transpose_rule(cts,x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_via_psum(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_to_all_abstract_eval(x,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_batched_collective(axis_size,frame_name,_,vals_in,dims_in,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_batcher(vals_in,dims_in,*,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_lowering(ctx,x,*,split_axis,concat_axis,axis_name,axis_index_groups)
jax._src.lax.parallel._all_to_all_transpose_rule(cts,x,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._allreduce_abstract_eval(*args,axes,axis_index_groups)
jax._src.lax.parallel._allreduce_impl(pos_reducer,*args,axes,axis_index_groups)
jax._src.lax.parallel._allreduce_lowering(prim,pos_fn,ctx,*args,axes,axis_index_groups)
jax._src.lax.parallel._axis_index_abstract_eval(*,axis_name)
jax._src.lax.parallel._axis_index_bind(*,axis_name)
jax._src.lax.parallel._axis_index_lowering(ctx,*,axis_name)
jax._src.lax.parallel._axis_index_of_val(x,val,axis_name)
jax._src.lax.parallel._batched_reduction_collective(prim,if_unmapped,axis_size,frame_name,_,vals_in,dims_in,axes,axis_index_groups)
jax._src.lax.parallel._build_axis_index_lowering_hlo(ctx,axis_name,axis_env)
jax._src.lax.parallel._canonicalize_axis_index_groups(axis_index_groups)
jax._src.lax.parallel._collective_batcher(prim,args,dims,**params)
jax._src.lax.parallel._expand(dim,size,index,tiled,x)
jax._src.lax.parallel._foldaxis(axis,x)
jax._src.lax.parallel._index_in_group(axis_name,axis_index_groups)
jax._src.lax.parallel._moveaxis(src,dst,x)
jax._src.lax.parallel._pdot_abstract_eval(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_impl(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_lowering(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_transpose_lhs(g,x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_transpose_rhs(g,x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_vmap_batching_rule(vals_in,dims_in,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_vmap_collective_rule(axis_size,frame_name,_,vals_in,dims_in,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pgather_abstract_eval(src,idx,*,axes)
jax._src.lax.parallel._pgather_batcher(vals_in,dims_in,*,axes)
jax._src.lax.parallel._pgather_collective_batcher(axis_size,frame_name,_,vals_in,dims_in,*,axes)
jax._src.lax.parallel._pgather_impl(src,idx,*,axes)
jax._src.lax.parallel._pgather_parallel_lowering(ctx,src,idx,*,axes)
jax._src.lax.parallel._ppermute_batcher(axis_size,frame_name,_,vals_in,dims_in,axis_name,perm)
jax._src.lax.parallel._ppermute_lowering(ctx,x,*,axis_name,perm)
jax._src.lax.parallel._ppermute_transpose_rule(t,x,perm,axis_name)
jax._src.lax.parallel._psum_transpose_rule(cts,*args,axes,axis_index_groups)
jax._src.lax.parallel._reduce_scatter_abstract_eval(x,*,axis_name,scatter_dimension,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_batcher(vals_in,dims_in,*,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_collective(frame_size,frame_name,_,vals_in,dims_in,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_lowering(prim,reducer,ctx,x,*,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_lowering_via_reducer(prim,reducer,ctx,x,*,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_transpose_rule(cts,x,*,axis_name,scatter_dimension,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_via_reducer(x,*,reducer,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduction_batcher(prim,vals_in,dims_in,*,axes,axis_index_groups)
jax._src.lax.parallel._reduction_with_positional_batcher(prim,vals_in,dims_in,axis_index_groups,transform_unmapped,transform_mapped)
jax._src.lax.parallel._replica_groups(axis_env,axis_name,axis_index_groups)
jax._src.lax.parallel._replica_groups_hlo(replica_groups:Sequence[Sequence[int]])->ir.DenseIntElementsAttr
jax._src.lax.parallel._splitaxis(axis,factor,x)
jax._src.lax.parallel._subst_all_names_in_param(pname:str,params:core.ParamDict,subst:core.AxisSubst,traverse:bool)->core.ParamDict
jax._src.lax.parallel._validate_reduce_axis_index_groups(axis_index_groups)
jax._src.lax.parallel._vmap_process_axis_index(self,frame)
jax._src.lax.parallel.all_gather(x,axis_name,*,axis_index_groups=None,axis=0,tiled=False)
jax._src.lax.parallel.all_to_all(x,axis_name,split_axis,concat_axis,*,axis_index_groups=None,tiled=False)
jax._src.lax.parallel.axis_index(axis_name)
jax._src.lax.parallel.pargmax(x,axis_name)
jax._src.lax.parallel.pargmin(x,axis_name)
jax._src.lax.parallel.pdot(x,y,axis_name,pos_contract=((),()),pos_batch=((),()),precision=None)
jax._src.lax.parallel.pgather(src,idx,axes:Union[int,AxisName])
jax._src.lax.parallel.pmax(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.pmean(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.pmin(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.ppermute(x,axis_name,perm)
jax._src.lax.parallel.pshuffle(x,axis_name,perm)
jax._src.lax.parallel.psum(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.psum_bind(*args,axes,axis_index_groups)
jax._src.lax.parallel.psum_scatter(x,axis_name,*,scatter_dimension=0,axis_index_groups=None,tiled=False)
jax._src.lax.parallel.pswapaxes(x,axis_name,axis,*,axis_index_groups=None)
jax._src.lax.parallel.xeinsum(spec:str,*operands)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/windowed_reductions.py----------------------------------------
A:jax._src.lax.windowed_reductions.(flat_operands, operand_tree)->jax.tree_util.tree_flatten(operand)
A:jax._src.lax.windowed_reductions.(flat_init_values, init_value_tree)->jax.tree_util.tree_flatten(init_value)
A:jax._src.lax.windowed_reductions.padding->tuple(padding)
A:jax._src.lax.windowed_reductions.monoid_reducer->_get_monoid_window_reducer(computation, flat_init_values)
A:jax._src.lax.windowed_reductions.flat_init_avals->map(lax._abstractify, flat_init_values)
A:jax._src.lax.windowed_reductions.(jaxpr, consts, out_tree)->jax._src.lax.lax._variadic_reduction_jaxpr(computation, tuple(flat_init_avals), init_value_tree)
A:jax._src.lax.windowed_reductions.out_flat->jax._src.core.Primitive('reduce_window').bind(*flat_operands, *flat_init_values, jaxpr=jaxpr, consts=consts, window_dimensions=tuple(window_dimensions), window_strides=tuple(window_strides), padding=padding, base_dilation=tuple(base_dilation), window_dilation=tuple(window_dilation))
A:jax._src.lax.windowed_reductions.aval->jax._src.core.get_aval(x)
A:jax._src.lax.windowed_reductions.init_value->jax._src.lax.lax._const(operand, -np.inf)
A:jax._src.lax.windowed_reductions.(jaxpr, consts)->jax._src.lax.lax._reduction_jaxpr(logaddexp, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(out,)->jax._src.core.Primitive('reduce_window').bind(operand, init_value, jaxpr=jaxpr, consts=consts, window_dimensions=tuple(window_dimensions), window_strides=tuple(window_strides), padding=tuple(padding), base_dilation=tuple(base_dilation), window_dilation=tuple(window_dilation))
A:jax._src.lax.windowed_reductions.(select_jaxpr, select_consts)->jax._src.lax.lax._reduction_jaxpr(select, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(scatter_jaxpr, scatter_consts)->jax._src.lax.lax._reduction_jaxpr(scatter, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(operand_avals, init_val_avals)->jax._src.util.split_list(avals, [len(avals) // 2])
A:jax._src.lax.windowed_reductions.out_shape->_common_reduce_window_shape_rule(operand_avals[0], window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax._src.lax.windowed_reductions.(operand_bdims, init_value_bdims)->jax._src.util.split_list(batch_dims, [num_operands])
A:jax._src.lax.windowed_reductions.size->next((a.shape[bdim] for (a, bdim) in zip(batched_args, batch_dims) if bdim is not None))
A:jax._src.lax.windowed_reductions.outs->jax._src.core.Primitive('reduce_window').bind(*operands + init_values, jaxpr=jaxpr, consts=consts, window_dimensions=window_dimensions, window_strides=window_strides, padding=padding, base_dilation=base_dilation, window_dilation=window_dilation)
A:jax._src.lax.windowed_reductions.reduce_window_p->jax._src.core.Primitive('reduce_window')
A:jax._src.lax.windowed_reductions.(_, init_value_avals)->jax._src.util.split_list(ctx.avals_in, [len(operands)])
A:jax._src.lax.windowed_reductions.(out_nodes, _)->jax._src.interpreters.mlir.jaxpr_subcomp(ctx.module_context, scatter_jaxpr, mlir.TokenSet(), scatter_consts, *([a] for a in scatter.arguments), dim_var_values=ctx.dim_var_values)
A:jax._src.lax.windowed_reductions.pads->jax._src.lax.convolution._conv_general_vjp_lhs_padding(input_shape, window_dimensions, window_strides, cotangent.shape, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.pad_cotangent->jax._src.lax.lax.pad(cotangent, lax._zero(cotangent), padding_config)
A:jax._src.lax.windowed_reductions.result->jax._src.lax.slicing.slice(result, (0,) * len(result.shape), result.shape, base_dilation)
A:jax._src.lax.windowed_reductions.operand->jax._src.lax.lax.pad(operand, select_identity(operand.dtype), tuple(((0, 0, d - 1) for d in base_dilation)))
A:jax._src.lax.windowed_reductions.reduce_window_sum_p->jax._src.lax.lax.standard_primitive(_reduce_window_sum_shape_rule, lax._input_dtype, 'reduce_window_sum')
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_sum_p]->partial(_reduce_window_batch_rule, _reduce_window_sum)
A:jax._src.lax.windowed_reductions.operand_shape->jax._src.lax.lax._dilate_shape(operand_shape, base_dilation)
A:jax._src.lax.windowed_reductions.window_dimensions->jax._src.lax.lax._dilate_shape(window_dimensions, window_dilation)
A:jax._src.lax.windowed_reductions.operand_padded->tuple((d + pl + ph for (d, (pl, ph)) in zip(operand_shape, padding)))
A:jax._src.lax.windowed_reductions.reduce_window_max_p->jax._src.lax.lax.standard_primitive(_common_reduce_window_shape_rule, lax._input_dtype, 'reduce_window_max')
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_max_p]->partial(_reduce_window_batch_rule, _reduce_window_max)
A:jax._src.lax.windowed_reductions.reduce_window_min_p->jax._src.lax.lax.standard_primitive(_common_reduce_window_shape_rule, lax._input_dtype, 'reduce_window_min')
A:jax._src.lax.windowed_reductions._reduce_window_min_batch_rule->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_min_p]->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax._src.lax.windowed_reductions.scalar_aval->operand_aval.update(shape=())
A:jax._src.lax.windowed_reductions.select_and_scatter_p->jax._src.lax.lax.standard_primitive(_select_and_scatter_shape_rule, lax._input_dtype, 'select_and_scatter')
A:jax._src.lax.windowed_reductions.scalar_type->jax._src.interpreters.mlir.aval_to_ir_type(scalar_aval)
A:jax._src.lax.windowed_reductions.op->jax._src.lib.mlir.dialects.hlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2)))
A:jax._src.lax.windowed_reductions.select->jax._src.lib.mlir.dialects.hlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).select.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.windowed_reductions.scatter->jax._src.lib.mlir.dialects.hlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).scatter.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.windowed_reductions.val_out->_select_and_gather_add(source, operand, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.tangent_out->_select_and_gather_add(g_source, operand, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.source_t->_select_and_gather_add(t, operand, select_prim, window_dimensions, window_strides, padding, ones, ones)
A:jax._src.lax.windowed_reductions.source->jax._src.interpreters.batching.bdim_at_front(source, s_bdim, size)
A:jax._src.lax.windowed_reductions.out->_select_and_gather_add(t, x, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.select_and_scatter_add_p->jax._src.lax.lax.standard_primitive(_select_and_scatter_add_shape_rule, lax._input_dtype, 'select_and_scatter_add')
A:jax._src.lax.windowed_reductions.etype->jax._src.interpreters.mlir.dtype_to_ir_type(dtype)
A:jax._src.lax.windowed_reductions.word_type->jax._src.interpreters.mlir.dtype_to_ir_type(word_dtype)
A:jax._src.lax.windowed_reductions.word_type_ab_aval->ab_aval.update(dtype=word_dtype)
A:jax._src.lax.windowed_reductions.double_word_type_ab_aval->ab_aval.update(dtype=double_word_dtype)
A:jax._src.lax.windowed_reductions.a->jax._src.lib.mlir.dialects.hlo.BitcastConvertOp(mlir.aval_to_ir_type(word_type_ab_aval), a)
A:jax._src.lax.windowed_reductions.b->jax._src.lib.mlir.dialects.hlo.ShiftRightLogicalOp(b, _broadcast_scalar_const(r_nbits, word_type_ab_aval))
A:jax._src.lax.windowed_reductions.st->jax._src.lib.mlir.dialects.hlo.AndOp(t, const(word_dtype, (1 << r_nbits) - 1 << r_nbits))
A:jax._src.lax.windowed_reductions.double_word_out_aval->out_aval.update(dtype=double_word_dtype)
A:jax._src.lax.windowed_reductions.(res,)->jax._src.interpreters.mlir.reduce_window(ctx, reducer_name='reduce_window_select_and_gather_add', reducer_body=reducer_body, operands=[pack(operand, tangents, operand_aval)], init_values=[pack(const(dtype, init), const(dtype, 0), core.ShapedArray((), dtype))], init_values_avals=[core.ShapedArray((), double_word_dtype)], out_avals=[double_word_out_aval], window_dimensions=window_dimensions, window_strides=window_strides, base_dilation=base_dilation, window_dilation=window_dilation, padding=padding)
A:jax._src.lax.windowed_reductions.which->select_prim.bind(kx, ky)
A:jax._src.lax.windowed_reductions.(_, out)->reduce_window((operand, tangents), (np.array(init, dtype=operand.dtype), np.array(0, dtype=operand.dtype)), reducer, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.has_base_dilation->any((d != 1 for d in base_dilation))
A:jax._src.lax.windowed_reductions.t->jax._src.interpreters.batching.bdim_at_front(t, t_bdim, size)
A:jax._src.lax.windowed_reductions.x->jax._src.interpreters.batching.bdim_at_front(x, x_bdim, size)
A:jax._src.lax.windowed_reductions.select_and_gather_add_p->jax._src.lax.lax.standard_primitive(_select_and_gather_add_shape_rule, lax._input_dtype, 'select_and_gather_add')
jax._src.lax.windowed_reductions._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._generic_reduce_window_batch_rule(batched_args,batch_dims,*,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._generic_reduce_window_lower(ctx,*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._get_monoid_window_reducer(monoid_op:Callable,xs:Sequence[Array])->Optional[Callable]
jax._src.lax.windowed_reductions._reduce_window_abstract_eval_rule(*avals,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_batch_rule(reduce_window,batched_args,bdims,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_chooser_jvp_rule(prim,g,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_logaddexp(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_lower(reduce_op,init_value,ctx,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_max(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_min(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_prod(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_sum(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_sum_shape_rule(operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_sum_transpose_rule(cotangent,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add(tangents:Array,operand:Array,select_prim:core.Primitive,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],base_dilation:Sequence[int],window_dilation:Sequence[int])->Array
jax._src.lax.windowed_reductions._select_and_gather_add_batching_rule(batched_args,batch_dims,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_lowering(ctx:mlir.LoweringRuleContext,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation,max_bits=64)
jax._src.lax.windowed_reductions._select_and_gather_add_shape_rule(tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_transpose(t,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_using_variadic_reducewindow(tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_scatter(operand:Array,select:Callable,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]],source:Array,init_value:Array,scatter:Callable)->Array
jax._src.lax.windowed_reductions._select_and_scatter_add(source:Array,operand:Array,select_prim:core.Primitive,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[tuple[int,int]])->Array
jax._src.lax.windowed_reductions._select_and_scatter_add_batch_rule(batched_args,batch_dims,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_impl(source,operand,*,select_prim,window_dimensions,window_strides,padding,expand_padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_shape_rule(source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_transpose(t,source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_lower(ctx,operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_shape_rule(operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions.reduce_window(operand,init_value,computation:Callable,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding,base_dilation=None,window_dilation=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/slicing.py----------------------------------------
A:jax._src.lax.slicing._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.lax.slicing.start_indices->list(start_indices)
A:jax._src.lax.slicing.(dynamic_sizes, static_sizes)->jax._src.lax.lax._extract_tracers_dyn_shape(slice_sizes)
A:jax._src.lax.slicing.static_sizes->jax._src.core.canonicalize_shape(slice_sizes)
A:jax._src.lax.slicing.CLIP->enum.auto()
A:jax._src.lax.slicing.FILL_OR_DROP->enum.auto()
A:jax._src.lax.slicing.PROMISE_IN_BOUNDS->enum.auto()
A:jax._src.lax.slicing.parsed_mode->GatherScatterMode.from_any(mode)
A:jax._src.lax.slicing.dtype->_dtype(operand)
A:jax._src.lax.slicing.(jaxpr, consts)->jax._src.lax.lax._reduction_jaxpr(_apply, lax._abstractify(lax._zero(operand)))
A:jax._src.lax.slicing.unused->jax._src.lax.lax.full(update_shape, 0, operand.dtype)
A:jax._src.lax.slicing._apply->_scatter_apply_cache.setdefault(func, _apply)
A:jax._src.lax.slicing.indices->jax._src.lax.lax.concatenate([counts, indices], len(count_shape) - 1)
A:jax._src.lax.slicing.max_idx->jax._src.lax.lax.expand_dims(np.array([src.shape[ax] for ax in axes]), tuple(range(indices.ndim - 1)))
A:jax._src.lax.slicing.slice_sizes->jax._src.interpreters.mlir.eval_dynamic_shape_as_tensor(ctx, slice_sizes)
A:jax._src.lax.slicing.offset_dims->tuple(np.add(1, dimension_numbers.offset_dims))
A:jax._src.lax.slicing.dnums->ScatterDimensionNumbers(update_window_dims=update_window_dims, inserted_window_dims=inserted_window_dims, scatter_dims_to_operand_dims=scatter_dims_to_operand_dims)
A:jax._src.lax.slicing.limit_indices->list(operand.shape)
A:jax._src.lax.slicing.axis->int(axis)
A:jax._src.lax.slicing.strides[axis]->int(stride)
A:jax._src.lax.slicing.result->jax._src.interpreters.mlir.aval_to_ir_types(aval_out)
A:jax._src.lax.slicing.slice_sizes[axis]->jax._src.core._canonicalize_dimension(slice_size)
A:jax._src.lax.slicing.update->jax._src.lib.mlir.dialects.hlo.ScatterOp(result, operand, indices, updates, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices)).update_computation.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.slicing.diff->tuple(map(operator.sub, limit_indices, start_indices))
A:jax._src.lax.slicing.pads->zip(start_indices, np.subtract(operand_shape, real_limits), np.subtract(strides, 1))
A:jax._src.lax.slicing.real_limits->numpy.add(start_indices, np.where(np.array(t.shape) == 0, 0, np.add(1, np.multiply(np.subtract(t.shape, 1), strides))))
A:jax._src.lax.slicing.new_start_indices->list(start_indices)
A:jax._src.lax.slicing.new_limit_indices->list(limit_indices)
A:jax._src.lax.slicing.new_strides->list(strides)
A:jax._src.lax.slicing.out->scatter_add(zeros, indices, t, scatter_dnums, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode)
A:jax._src.lax.slicing.slice_p->standard_primitive(_slice_shape_rule, _input_dtype, 'slice')
A:jax._src.lax.slicing.(start_indices, dyn)->jax._src.util.split_list(starts_and_dyn_sizes, [x_aval.ndim])
A:jax._src.lax.slicing.tangent_out->scatter_add(masked_g_operand, indices, masked_g_updates, dimension_numbers=dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.zeros->jax._src.lax.lax.full(operand_shape, lax._zero(t))
A:jax._src.lax.slicing.empty_marker->object()
A:jax._src.lax.slicing.size->next((x.shape[ax] for (x, ax) in zip(batched_args, batch_dims) if ax is not None))
A:jax._src.lax.slicing.dims->tuple(range(len(update_shape)))
A:jax._src.lax.slicing.(start_indices, dyn_slice_sizes)->jax._src.util.split_list(start_indices_and_dyn, [ndims])
A:jax._src.lax.slicing.(start_idx_bds, dyn_slice_size_bds)->jax._src.util.split_list(start_idx_and_dyn_bds, [ndims])
A:jax._src.lax.slicing.(index, index_bdim)->_batch_dynamic_slice_indices(start_idx, start_idx_bd)
A:jax._src.lax.slicing.shape->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn)
A:jax._src.lax.slicing.aval->jax._src.core.DShapedArray(shape, x.dtype, False)
A:jax._src.lax.slicing.(out_aval, effects)->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', weak_type_rule=_argnum_weak_type(0)).abstract_eval(x.aval, *(d.aval for d in start_indices), slice_sizes=slice_sizes)
A:jax._src.lax.slicing.out_shape->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn)
A:jax._src.lax.slicing.out_aval->jax._src.core.DShapedArray(tuple(out_shape), x.aval.dtype, x.aval.weak_type)
A:jax._src.lax.slicing.(x_aval, start_indices_avals, dyn_avals)->jax._src.util.split_list(in_avals, [1, x.ndim])
A:jax._src.lax.slicing.slice_sizes_->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn_)
A:jax._src.lax.slicing.dynamic_slice_p->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.aval_out->aval_out.update(shape=lax._merge_dyn_shape(slice_sizes, dyn)).update(shape=lax._merge_dyn_shape(slice_sizes, dyn))
A:jax._src.lax.slicing.val_out->scatter_add(masked_operand, indices, masked_updates, dimension_numbers=dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.g_operand->jax._src.interpreters.ad.instantiate_zeros(g_operand)
A:jax._src.lax.slicing.g_update->jax._src.interpreters.ad.instantiate_zeros(g_update)
A:jax._src.lax.slicing.dynamic_update_slice_p->standard_primitive(_dynamic_update_slice_shape_rule, _dynamic_update_slice_dtype_rule, 'dynamic_update_slice')
A:jax._src.lax.slicing.output_offset_dim_count->len(offset_dims)
A:jax._src.lax.slicing.expanded_indices_shape->list(indices.shape)
A:jax._src.lax.slicing.indices_shape_gen->iter(expanded_indices_shape)
A:jax._src.lax.slicing.ans->tuple((next(slice_sizes_gen) if i in offset_dims else next(indices_shape_gen) for i in range(output_shape_rank)))
A:jax._src.lax.slicing.intarray->partial(np.array, dtype=np.int64)
A:jax._src.lax.slicing.operand_dims->jax._src.lax.lax.shape_as_value(operand.shape)
A:jax._src.lax.slicing.mask->jax._src.lax.lax._reduce_and(mask, [num_batch_dims])
A:jax._src.lax.slicing.batch_dims_in_output->numpy.delete(np.arange(output_ndims), dnums.offset_dims)
A:jax._src.lax.slicing.gather_out->gather(operand, indices, dnums, slice_sizes, indices_are_sorted=indices_are_sorted, mode=GatherScatterMode.PROMISE_IN_BOUNDS)
A:jax._src.lax.slicing.scatter_dnums->jax._src.lib.mlir.dialects.hlo.ScatterDimensionNumbers.get(update_window_dims=list(dnums.update_window_dims), inserted_window_dims=list(dnums.inserted_window_dims), scattered_dims_to_operand_dims=list(dnums.scatter_dims_to_operand_dims), index_vector_dim=len(ctx.avals_in[1].shape) - 1)
A:jax._src.lax.slicing.(operand, operand_bdim)->jax._src.interpreters.batching.move_stacked_axis(operand, operand_bdim, 0)
A:jax._src.lax.slicing.collapsed_slice_dims->tuple(np.add(1, dimension_numbers.collapsed_slice_dims))
A:jax._src.lax.slicing.start_index_map->tuple(np.add(1, dimension_numbers.start_index_map))
A:jax._src.lax.slicing.ragged_slice_sizes->jax._src.interpreters.batching.bdim_as_shape(operand_bdim, slice_sizes)
A:jax._src.lax.slicing.bdim_out->jax._src.interpreters.batching.shape_as_bdim(operand_bdim.stacked_axis, _gather_shape_computation(indices, dnums, ragged_slice_sizes))
A:jax._src.lax.slicing.operand->jax._src.interpreters.batching.bdim_at_front(operand, operand_bdim, size)
A:jax._src.lax.slicing.output_shape->_gather_shape_rule(core.ShapedArray(operand.shape[1:], operand.dtype), core.ShapedArray(indices.shape[1:], dtypes.canonicalize_dtype(indices.dtype)), dimension_numbers=dimension_numbers, slice_sizes=slice_sizes, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)
A:jax._src.lax.slicing.index_dtype->_promote_dtype_for_size(indices.dtype, indices.shape[0])
A:jax._src.lax.slicing.count_shape->list(indices.shape)
A:jax._src.lax.slicing.counts->jax._src.lax.lax.broadcasted_iota(indices.dtype, tuple(count_shape), 0)
A:jax._src.lax.slicing.gather_p->standard_primitive(_gather_shape_rule, _gather_dtype_rule, 'gather', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.dimension_numbers->dimension_numbers._replace(update_window_dims=(*dimension_numbers.update_window_dims, *trailing_window_dims))._replace(update_window_dims=(*dimension_numbers.update_window_dims, *trailing_window_dims))
A:jax._src.lax.slicing.gather_lower->partial(_gather_lower, dimension_numbers=dimension_numbers, slice_sizes=slice_sizes, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)
A:jax._src.lax.slicing.(res,)->jax._src.interpreters.mlir.delegate_lowering(ctx, scatter_lower, operand, indices, updates, avals_in=[core.physical_aval(aval_x), aval_indices, core.physical_aval(aval_updates)], avals_out=[core.physical_aval(aval_y)])
A:jax._src.lax.slicing.gather_fill_fn->jax._src.interpreters.mlir.lower_fun(_gather_fill, multiple_results=False)
A:jax._src.lax.slicing.upper_bound->jax._src.lax.lax.broadcast_in_dim(upper_bound, indices.shape, (len(indices.shape) - 1,))
A:jax._src.lax.slicing.g_updates->jax._src.interpreters.ad.instantiate_zeros(g_updates)
A:jax._src.lax.slicing.gather_dnums->GatherDimensionNumbers(offset_dims=dimension_numbers.update_window_dims, collapsed_slice_dims=dimension_numbers.inserted_window_dims, start_index_map=dimension_numbers.scatter_dims_to_operand_dims)
A:jax._src.lax.slicing.update_t->gather(t, indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes, mode=mode, fill_value=0)
A:jax._src.lax.slicing.operand_t->scatter(t, indices, lax.full(updates_shape, 0, dtype=t.dtype), dimension_numbers=dimension_numbers, indices_are_sorted=indices_are_sorted, unique_indices=True, mode=mode)
A:jax._src.lax.slicing.updates->jax._src.interpreters.batching.bdim_at_front(updates, updates_bdim, size)
A:jax._src.lax.slicing.inserted_window_dims->tuple(np.add(1, dimension_numbers.inserted_window_dims))
A:jax._src.lax.slicing.scatter_dims_to_operand_dims->tuple(np.add(1, dimension_numbers.scatter_dims_to_operand_dims))
A:jax._src.lax.slicing.update_window_dims->tuple(np.add(1, dimension_numbers.update_window_dims))
A:jax._src.lax.slicing.scatter_add_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-add', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_add_p]->partial(_scatter_batching_rule, scatter_add_p)
A:jax._src.lax.slicing.scatter_mul_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-mul', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_mul_p]->partial(_scatter_batching_rule, scatter_mul_p)
A:jax._src.lax.slicing.initial_vals->gather(operand, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.target_vals->gather(val_out, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.num_updates->gather(scatter_add(lax._zeros(operand), indices, lax.select(successful_updates, lax._ones(updates), lax._zeros(updates)), scatter_dnums), indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.num_refs->gather(scatter_add(lax._zeros(operand), indices, lax._ones(updates), scatter_dnums), indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.updates_normalizer->jax._src.lax.lax.select(retained_values, 1.0 / (num_updates + 1), 1.0 / num_updates)
A:jax._src.lax.slicing.updates_coef->jax._src.lax.lax.select(successful_updates, updates_normalizer, lax._zeros(updates))
A:jax._src.lax.slicing.operand_normalizer->jax._src.lax.lax.select(retained_values, 1.0 / (num_updates + 1), lax._zeros(num_updates))
A:jax._src.lax.slicing.target_tangents->gather(g_operand, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.scatter_min_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-min', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_min_p]->partial(_scatter_batching_rule, scatter_min_p)
A:jax._src.lax.slicing.ad.primitive_jvps[scatter_min_p]->partial(_scatter_extremal_jvp, scatter_min_p)
A:jax._src.lax.slicing.scatter_max_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-max', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_max_p]->partial(_scatter_batching_rule, scatter_max_p)
A:jax._src.lax.slicing.ad.primitive_jvps[scatter_max_p]->partial(_scatter_extremal_jvp, scatter_max_p)
A:jax._src.lax.slicing.ids_shape->numpy.array(updates.shape, dtype=np.int64)
A:jax._src.lax.slicing.num_ids->math.prod(ids_shape)
A:jax._src.lax.slicing.update_ids->jax._src.lax.lax.add(lax.reshape(lax.iota(id_dtype, num_ids), ids_shape), lax._ones(updates, dtype=id_dtype))
A:jax._src.lax.slicing.scattered_ids->scatter(lax.full(operand.shape, 0, id_dtype), indices, update_ids, dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.gathered_update_ids->gather(scattered_ids, indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax._src.lax.slicing.masked_operand->jax._src.lax.lax.select(lax.eq(scattered_ids, lax._zeros(scattered_ids)), operand, lax._zeros(operand))
A:jax._src.lax.slicing.masked_updates->jax._src.lax.lax.select(lax.eq(update_ids, gathered_update_ids), updates, lax._zeros(updates))
A:jax._src.lax.slicing.masked_g_operand->jax._src.lax.lax.select(lax.eq(scattered_ids, lax._zeros(scattered_ids)), g_operand, lax._zeros(g_operand))
A:jax._src.lax.slicing.masked_g_updates->jax._src.lax.lax.select(lax.eq(update_ids, gathered_update_ids), g_updates, lax._zeros(g_updates))
A:jax._src.lax.slicing.scatter_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_p]->partial(_scatter_batching_rule, scatter_p)
A:jax._src.lax.slicing.scatter_lower->partial(_scatter_lower, update_jaxpr=update_jaxpr, update_consts=update_consts, dimension_numbers=dimension_numbers, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode)
A:jax._src.lax.slicing.(update_jaxpr, update_consts)->jax._src.lax.lax._reduction_jaxpr(_scatter_reduction_computation, core.ShapedArray((), operand_dtype))
A:jax._src.lax.slicing.clip_fn->jax._src.interpreters.mlir.lower_fun(_clamp_scatter_indices, multiple_results=False)
A:jax._src.lax.slicing.((indices,),)->clip_fn(ctx.replace(avals_out=None), operand, indices, updates, dnums=dimension_numbers)
A:jax._src.lax.slicing.op->jax._src.lib.mlir.dialects.hlo.ScatterOp(result, operand, indices, updates, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices))
A:jax._src.lax.slicing.scalar_type->jax._src.interpreters.mlir.aval_to_ir_type(core.ShapedArray((), real_dtype))
A:jax._src.lax.slicing.update_ctx->ctx.module_context.replace(name_stack=source_info_util.new_name_stack())
A:jax._src.lax.slicing.(out_nodes, _)->jax._src.interpreters.mlir.jaxpr_subcomp(update_ctx, update_jaxpr, mlir.TokenSet(), update_consts, (update.arguments[0],), (update.arguments[1],), dim_var_values=ctx.dim_var_values)
A:jax._src.lax.slicing.real_dtype->_real_dtype(aval_out.dtype)
A:jax._src.lax.slicing.operand_type_part->jax._src.interpreters.mlir.aval_to_ir_types(core.ShapedArray(aval_out.shape, real_dtype))
A:jax._src.lax.slicing.scatter->jax._src.lib.mlir.dialects.hlo.ScatterOp(operand_type_part, operand_part, indices, updates_part, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices))
A:jax._src.lax.slicing.reducer->jax._src.lib.mlir.dialects.hlo.ScatterOp(operand_type_part, operand_part, indices, updates_part, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices)).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.slicing.real->_scatter(hlo.RealOp(operand).result, hlo.RealOp(updates).result)
A:jax._src.lax.slicing.imag->_scatter(hlo.ImagOp(operand).result, hlo.ImagOp(updates).result)
A:jax._src.lax.slicing.d->jax._src.core.dimension_as_value(d)
A:jax._src.lax.slicing.d_arr->jax._src.lax.lax.convert_element_type(d, _dtype(i))
jax._src.lax.slicing.GatherDimensionNumbers(NamedTuple)
jax._src.lax.slicing.GatherScatterMode(enum.Enum)
jax._src.lax.slicing.GatherScatterMode.from_any(s:Optional[Union[str,'GatherScatterMode']])
jax._src.lax.slicing.ScatterDimensionNumbers(NamedTuple)
jax._src.lax.slicing._batch_dynamic_slice_indices(indices,bdims)
jax._src.lax.slicing._clamp_scatter_indices(operand,indices,updates,*,dnums)
jax._src.lax.slicing._dynamic_slice_batching_rule(batched_args,batch_dims,*,slice_sizes)
jax._src.lax.slicing._dynamic_slice_dtype_rule(operand,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_indices(operand:Union[Array,np.ndarray],start_indices:Union[Union[Array,np.ndarray],Sequence[ArrayLike]])->list[ArrayLike]
jax._src.lax.slicing._dynamic_slice_jvp(primals,tangents,*,slice_sizes)
jax._src.lax.slicing._dynamic_slice_lower(ctx,x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_padding_rule(in_avals,out_avals,x,*starts_and_dyn,slice_sizes)
jax._src.lax.slicing._dynamic_slice_shape_rule(operand,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_staging_rule(trace,x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_transpose_rule(t,operand,*start_indices,slice_sizes)
jax._src.lax.slicing._dynamic_slice_typecheck_rule(_,x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_update_slice_batching_rule(batched_args,batch_dims)
jax._src.lax.slicing._dynamic_update_slice_dtype_rule(operand,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_jvp(primals,tangents)
jax._src.lax.slicing._dynamic_update_slice_lower(ctx,x,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_shape_rule(operand,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices)
jax._src.lax.slicing._gather_batching_rule(batched_args,batch_dims,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_dtype_rule(operand,indices,*,fill_value,**kwargs)
jax._src.lax.slicing._gather_fill(operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,fill_value,output_shape)
jax._src.lax.slicing._gather_jvp_rule(g,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_lower(ctx,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_lower_opaque(ctx,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)->ir.Value
jax._src.lax.slicing._gather_pad_rule(in_avals,out_avals,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_shape_computation(indices,dimension_numbers,slice_sizes)
jax._src.lax.slicing._gather_shape_rule(operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_transpose_rule(t,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._is_sorted(dims,op_name,name)
jax._src.lax.slicing._no_duplicate_dims(dims,op_name,name)
jax._src.lax.slicing._promote_dtype_for_size(dtype,size)
jax._src.lax.slicing._real_dtype(dtype)
jax._src.lax.slicing._scatter_add_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_add_lower_gpu(ctx,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_add_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_batching_rule(scatter_op,batched_args,batch_dims,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_dtype_rule(operand,indices,updates,**kwargs)
jax._src.lax.slicing._scatter_extremal_jvp(scatter_op,primals,tangents,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_lower(ctx,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_lower_opaque(ctx,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,unique_indices,indices_are_sorted,mode)
jax._src.lax.slicing._scatter_mul_jvp_rhs(g,x,i,y,*,dimension_numbers,indices_are_sorted,unique_indices,mode,**kw)
jax._src.lax.slicing._scatter_mul_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_shape_rule(operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._slice_batching_rule(batched_args,batch_dims,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_impl(x,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_lower(ctx,x,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_shape_rule(operand,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_transpose_rule(t,operand,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._sorted_dims_in_range(dims,rank,op_name,name)
jax._src.lax.slicing.dynamic_index_in_dim(operand:Union[Array,np.ndarray],index:Union[int,Array],axis:int=0,keepdims:bool=True)->Array
jax._src.lax.slicing.dynamic_slice(operand:Union[Array,np.ndarray],start_indices:Union[Union[Array,np.ndarray],Sequence[ArrayLike]],slice_sizes:Shape)->Array
jax._src.lax.slicing.dynamic_slice_in_dim(operand:Union[Array,np.ndarray],start_index:ArrayLike,slice_size:int,axis:int=0)->Array
jax._src.lax.slicing.dynamic_update_index_in_dim(operand:Union[Array,np.ndarray],update:ArrayLike,index:ArrayLike,axis:int)->Array
jax._src.lax.slicing.dynamic_update_slice(operand:Union[Array,np.ndarray],update:ArrayLike,start_indices:Union[Array,Sequence[ArrayLike]])->Array
jax._src.lax.slicing.dynamic_update_slice_in_dim(operand:Union[Array,np.ndarray],update:ArrayLike,start_index:ArrayLike,axis:int)->Array
jax._src.lax.slicing.gather(operand:ArrayLike,start_indices:ArrayLike,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape,*,unique_indices:bool=False,indices_are_sorted:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None,fill_value=None)->Array
jax._src.lax.slicing.index_in_dim(operand:Union[Array,np.ndarray],index:int,axis:int=0,keepdims:bool=True)->Array
jax._src.lax.slicing.index_take(src:Array,idxs:Array,axes:Sequence[int])->Array
jax._src.lax.slicing.scatter(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_add(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_apply(operand:Array,scatter_indices:Array,func:Callable[[Array],Array],dimension_numbers:ScatterDimensionNumbers,*,update_shape:Shape=(),indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_max(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_min(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_mul(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.slice(operand:ArrayLike,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)->Array
jax._src.lax.slicing.slice_in_dim(operand:Union[Array,np.ndarray],start_index:Optional[int],limit_index:Optional[int],stride:int=1,axis:int=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/qdwh.py----------------------------------------
A:jax._src.lax.qdwh.y->jax._src.lax.linalg.cholesky(x, symmetrize_input=False)
A:jax._src.lax.qdwh.(q, _)->jax._src.lax.linalg.qr(y, full_matrices=False)
A:jax._src.lax.qdwh.q1->_mask(lax.slice(q, (0, 0), (M, N)), (m, n))
A:jax._src.lax.qdwh.q2->_mask(q2, (n, n)).T.conj()
A:jax._src.lax.qdwh.x->_mask(x, (m, n))
A:jax._src.lax.qdwh.z->jax._src.lax.linalg.triangular_solve(y, z, left_side=True, lower=True, transpose_a=True, conjugate_a=True).T.conj()
A:jax._src.lax.qdwh.eps->float(jnp.finfo(x.dtype).eps)
A:jax._src.lax.qdwh.alpha->(jnp.sqrt(jnp.linalg.norm(x, ord=1)) * jnp.sqrt(jnp.linalg.norm(x, ord=jnp.inf))).astype(x.dtype)
A:jax._src.lax.qdwh.tol_norm->jax.numpy.cbrt(tol_l)
A:jax._src.lax.qdwh.dd->jax.numpy.cbrt(4.0 * (1.0 / l2 - 1.0) / l2)
A:jax._src.lax.qdwh.sqd->jax.numpy.sqrt(1.0 + dd)
A:jax._src.lax.qdwh.a->jax.numpy.real(a)
A:jax._src.lax.qdwh.u->jax.lax.cond(c > 100, true_fn, false_fn, operand=u)
A:jax._src.lax.qdwh.is_unconverged->jax.numpy.logical_or(iterating_l, iterating_u)
A:jax._src.lax.qdwh.(u, _, num_iters, is_unconverged, _)->jax.lax.while_loop(cond_fun=cond_fun, body_fun=body_fun, init_val=(u, l, iter_idx, is_unconverged, is_not_max_iteration))
A:jax._src.lax.qdwh.is_converged->jax.numpy.logical_not(is_unconverged)
A:jax._src.lax.qdwh.is_hermitian->jax._src.core.concrete_or_error(bool, is_hermitian, 'The `is_hermitian` argument must be statically specified to use `qdwh` within JAX transformations.')
A:jax._src.lax.qdwh.(u, h, num_iters, is_converged)->_qdwh(x, m, n, is_hermitian, max_iterations, eps)
jax._src.lax.qdwh._dynamic_concat(a,b,m,axis=0)
jax._src.lax.qdwh._mask(x,dims,alternative=0)
jax._src.lax.qdwh._pad_in_dim(x,low=0,high=0,interior=0,fill_value=0,axis=0)
jax._src.lax.qdwh._qdwh(x,m,n,is_hermitian,max_iterations,eps)
jax._src.lax.qdwh._use_cholesky(u,m,n,params)
jax._src.lax.qdwh._use_qr(u,m,n,params)
jax._src.lax.qdwh.qdwh(x,*,is_hermitian=False,max_iterations=None,eps=None,dynamic_shape:Optional[tuple[int,int]]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/svd.py----------------------------------------
A:jax._src.lax.svd.k->min(m, n)
A:jax._src.lax.svd.s->jax.numpy.maximum(s, 0.0)
A:jax._src.lax.svd.u->jax.numpy.where(return_nan, jnp.full((m, k), fill_value, dtype=a.dtype), jnp.eye(m, k, dtype=a.dtype))
A:jax._src.lax.svd.vh->jax.numpy.where(return_nan, jnp.full((k, n), fill_value, dtype=a.dtype), jnp.eye(k, n, dtype=a.dtype))
A:jax._src.lax.svd.(u, h, _, _)->jax.lax.linalg.qdwh(a, is_hermitian=hermitian, max_iterations=max_iterations)
A:jax._src.lax.svd.(v, s)->jax.lax.linalg.eigh(h)
A:jax._src.lax.svd.s_out->jax.numpy.flip(s)
A:jax._src.lax.svd.v_out->jax.numpy.fliplr(v)
A:jax._src.lax.svd.(u_out, r)->jax.lax.linalg.qr(u_out, full_matrices=False)
A:jax._src.lax.svd.eps->float(jnp.finfo(a.dtype).eps)
A:jax._src.lax.svd.u_out->jax.numpy.hstack((u_out, u_out_null))
A:jax._src.lax.svd.a->a.T.conj().T.conj()
A:jax._src.lax.svd.(q_full, a_full)->jax.lax.linalg.qr(a, full_matrices=True)
A:jax._src.lax.svd.(q, a)->jax.lax.linalg.qr(a, full_matrices=False)
A:jax._src.lax.svd.(u_out, s_out, v_out)->_svd_tall_and_square_input(a, hermitian, compute_uv, max_iterations)
A:jax._src.lax.svd.full_matrices->jax._src.core.concrete_or_error(bool, full_matrices, 'The `full_matrices` argument must be statically specified to use `svd` within JAX transformations.')
A:jax._src.lax.svd.compute_uv->jax._src.core.concrete_or_error(bool, compute_uv, 'The `compute_uv` argument must be statically specified to use `svd` within JAX transformations.')
A:jax._src.lax.svd.hermitian->jax._src.core.concrete_or_error(bool, hermitian, 'The `hermitian` argument must be statically specified to use `qdwh` within JAX transformations.')
A:jax._src.lax.svd.max_iterations->jax._src.core.concrete_or_error(int, max_iterations, 'The `max_iterations` argument must be statically specified to use `qdwh` within JAX transformations.')
A:jax._src.lax.svd.all_zero->jax.numpy.all(a == 0)
A:jax._src.lax.svd.non_finite->jax.numpy.logical_not(jnp.all(jnp.isfinite(a)))
jax._src.lax.svd._constant_svd(a:Any,return_nan:bool,full_matrices:bool,compute_uv:bool=True)->Union[Any, Sequence[Any]]
jax._src.lax.svd._qdwh_svd(a:Any,full_matrices:bool,compute_uv:bool=True,hermitian:bool=False,max_iterations:int=10)->Union[Any, Sequence[Any]]
jax._src.lax.svd._svd_tall_and_square_input(a:Any,hermitian:bool,compute_uv:bool,max_iterations:int)->Union[Any, Sequence[Any]]
jax._src.lax.svd.svd(a:Any,full_matrices:bool,compute_uv:bool=True,hermitian:bool=False,max_iterations:int=10)->Union[Any, Sequence[Any]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/other.py----------------------------------------
A:jax._src.lax.other.lhs_array->jax._src.numpy.lax_numpy.asarray(lhs)
A:jax._src.lax.other.filter_shape->tuple(filter_shape)
A:jax._src.lax.other.dimension_numbers->jax._src.lax.convolution.conv_dimension_numbers(lhs_array.shape, (1, 1) + filter_shape, dimension_numbers)
A:jax._src.lax.other.spatial_size->math.prod(filter_shape)
A:jax._src.lax.other.rhs->jax._src.numpy.lax_numpy.moveaxis(rhs, (0, 1), (rhs_spec[0], rhs_spec[1]))
A:jax._src.lax.other.out->jax._src.numpy.lax_numpy.moveaxis(out, (-2, -1), (out_spec[0], out_spec[1]))
A:jax._src.lax.other.c_precision->jax._src.lax.lax.canonicalize_precision(precision)
A:jax._src.lax.other.lhs_precision->type_cast(Optional[lax.PrecisionType], c_precision[0] if isinstance(c_precision, tuple) and len(c_precision) == 2 else c_precision)
A:jax._src.lax.other.patches->conv_general_dilated_patches(lhs=lhs_array, filter_shape=filter_shape, window_strides=window_strides, padding=padding, lhs_dilation=lhs_dilation, rhs_dilation=rhs_dilation, dimension_numbers=dimension_numbers, precision=lhs_precision)
A:jax._src.lax.other.(lhs_spec, rhs_spec, out_spec)->jax._src.lax.convolution.conv_dimension_numbers(lhs_array.shape, (1, 1) + tuple(filter_shape), dimension_numbers)
A:jax._src.lax.other.lhs_b_dims->sorted(lhs_b_dims)
jax._src.lax.other.conv_general_dilated_local(lhs:jax.typing.ArrayLike,rhs:jax.typing.ArrayLike,window_strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],filter_shape:Sequence[int],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:Optional[convolution.ConvGeneralDilatedDimensionNumbers]=None,precision:lax.PrecisionLike=None)->jax.Array
jax._src.lax.other.conv_general_dilated_patches(lhs:jax.typing.ArrayLike,filter_shape:Sequence[int],window_strides:Sequence[int],padding:Union[str,Sequence[tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:Optional[convolution.ConvGeneralDilatedDimensionNumbers]=None,precision:Optional[lax.PrecisionType]=None,preferred_element_type:Optional[DType]=None)->jax.Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/stack.py----------------------------------------
A:jax._src.lax.stack.elem->jax.tree_util.tree_map(lambda x: lax.dynamic_index_in_dim(x, self._size - 1, 0, keepdims=False), self._data)
A:jax._src.lax.stack.(leaves, treedef)->jax.tree_util.tree_flatten(self._data)
jax._src.lax.stack.Stack(self,size,data)
jax._src.lax.stack.Stack.__init__(self,size,data)
jax._src.lax.stack.Stack.__repr__(self)
jax._src.lax.stack.Stack.create(capacity:int,prototype:Any)->Stack
jax._src.lax.stack.Stack.empty(self)->Any
jax._src.lax.stack.Stack.flatten(self)
jax._src.lax.stack.Stack.pop(self)->tuple[Any, Stack]
jax._src.lax.stack.Stack.push(self,elem:Any)->Stack
jax._src.lax.stack.Stack.unflatten(treedef,leaves)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/eigh.py----------------------------------------
A:jax._src.lax.eigh.padded->jax.lax.pad(operand, jnp.array(0, operand.dtype), [(0, d, 0) for d in static_slice_sizes])
A:jax._src.lax.eigh.out->jax.lax.dynamic_slice(padded, tuple((jnp.int32(i) for i in start_indices)), static_slice_sizes)
A:jax._src.lax.eigh.operand->jax.lax.dynamic_update_slice(operand, t, start_indices)
A:jax._src.lax.eigh.start_indices->tuple((jnp.int32(i) for i in start_indices))
A:jax._src.lax.eigh.t->_mask(update, update_dims, t)
A:jax._src.lax.eigh.negative_column_norms->_mask(negative_column_norms, (n,), jnp.nan)
A:jax._src.lax.eigh.sort_idxs->jax._src.numpy.lax_numpy.argsort(eig_vals)
A:jax._src.lax.eigh.X->jax._src.numpy.lax_numpy.dot(P, V1)
A:jax._src.lax.eigh.H_norm->jax._src.numpy.linalg.norm(H)
A:jax._src.lax.eigh.(Q, _)->jax._src.numpy.linalg.qr(X, mode='complete')
A:jax._src.lax.eigh.V1->_mask(Q, (n, rank))
A:jax._src.lax.eigh.V2->_slice(Q, (0, rank), (n, n - rank), (N, N))
A:jax._src.lax.eigh.error_matrix->jax._src.numpy.lax_numpy.dot(error_matrix, V1)
A:jax._src.lax.eigh.error->jax._src.numpy.linalg.norm(error_matrix)
A:jax._src.lax.eigh.(V1, V2, error)->body_f_after_matmul(X)
A:jax._src.lax.eigh.one->jax._src.numpy.lax_numpy.ones(1, dtype=jnp.int32)
A:jax._src.lax.eigh.(V1, V2, _, error)->jax.lax.while_loop(cond_f, body_f, (V1, V2, one, error))
A:jax._src.lax.eigh.(U, _, _, _)->jax._src.lax.qdwh.qdwh(H_shift, is_hermitian=True, dynamic_shape=(n, n))
A:jax._src.lax.eigh.I->_mask(jnp.eye(N, dtype=H.dtype), (n, n))
A:jax._src.lax.eigh.rank_minus->jax._src.numpy.lax_numpy.round(jnp.trace(ufuncs.real(P_minus))).astype(jnp.int32)
A:jax._src.lax.eigh.(V_minus, V_plus)->jax.lax.cond(swap, lambda : _projector_subspace(P_plus, H, n, rank_plus, swap=True), lambda : _projector_subspace(P_minus, H, n, rank_minus, swap=False))
A:jax._src.lax.eigh.V_minus->jax._src.numpy.lax_numpy.dot(V0, V_minus)
A:jax._src.lax.eigh.V_plus->jax._src.numpy.lax_numpy.dot(V0, V_plus)
A:jax._src.lax.eigh.n->jax._src.numpy.lax_numpy.asarray(n, jnp.int32)
A:jax._src.lax.eigh.agenda->agenda.push(_Subproblem(offset=jnp.int32(0), size=n)).push(_Subproblem(offset=jnp.int32(0), size=n))
A:jax._src.lax.eigh.eigenvectors->_update_slice(eigenvectors, eig_vecs, (0, offset), (n, b))
A:jax._src.lax.eigh.H0_norm->jax._src.numpy.linalg.norm(_mask(H, (n, n)))
A:jax._src.lax.eigh.H->_mask(H, (n, n))
A:jax._src.lax.eigh.V->_slice(eigenvectors, (0, offset), (n, b), (N, B))
A:jax._src.lax.eigh.(eig_vecs, eig_vals)->jax.lax.linalg.eigh(H, sort_eigenvalues=False)
A:jax._src.lax.eigh.eig_vecs->jax._src.numpy.lax_numpy.dot(V, eig_vecs)
A:jax._src.lax.eigh.eig_vals->_mask(ufuncs.real(eig_vals), (n,), jnp.nan)
A:jax._src.lax.eigh.blocks->_update_slice(blocks, jnp.diag(H)[:, None], (offset, 0), (b, 1))
A:jax._src.lax.eigh.split_point->jax._src.numpy.reductions.nanmedian(_mask(jnp.diag(ufuncs.real(H)), (b,), jnp.nan))
A:jax._src.lax.eigh.(H_minus, V_minus, H_plus, V_plus, rank)->split_spectrum(H, b, split_point, V0=V)
A:jax._src.lax.eigh.should_update_minus->should_update_range(offset, offset + rank, subset_by_index)
A:jax._src.lax.eigh.(blocks, eigenvectors, agenda)->jax.lax.cond(should_update_plus, lambda : updated_plus_state, lambda : (blocks, eigenvectors, agenda))
A:jax._src.lax.eigh.should_update_plus->should_update_range(offset + rank, offset + b, subset_by_index)
A:jax._src.lax.eigh.norm->jax._src.numpy.linalg.norm(H)
A:jax._src.lax.eigh.eps->jax._src.numpy.lax_numpy.asarray(jnp.finfo(H.dtype).eps, dtype=norm.dtype)
A:jax._src.lax.eigh.off_diag_norm->jax._src.numpy.linalg.norm(H - jnp.diag(jnp.diag(ufuncs.real(H)).astype(H.dtype)))
A:jax._src.lax.eigh.cutoff->min(N, termination_size)
A:jax._src.lax.eigh.i->int(N / multiplier)
A:jax._src.lax.eigh.bucket_size->_round_up(i, granularity)
A:jax._src.lax.eigh.buckets->jax._src.numpy.lax_numpy.array(buckets, dtype='int32')
A:jax._src.lax.eigh.((offset, b), agenda)->agenda.push(_Subproblem(offset=jnp.int32(0), size=n)).push(_Subproblem(offset=jnp.int32(0), size=n)).pop()
A:jax._src.lax.eigh.which->jax._src.numpy.lax_numpy.where(buckets < b, jnp.iinfo(jnp.int32).max, buckets)
A:jax._src.lax.eigh.choice->jax._src.numpy.lax_numpy.argmin(which)
A:jax._src.lax.eigh.(_, blocks, eigenvectors)->jax.lax.while_loop(loop_cond, loop_body, (agenda, blocks, eigenvectors))
A:jax._src.lax.eigh.(eig_vals, eig_vecs)->_eigh_work(H, n, termination_size=termination_size, subset_by_index=subset_by_index)
jax._src.lax.eigh._Subproblem(NamedTuple)
jax._src.lax.eigh._eigh_work(H,n,termination_size,subset_by_index)
jax._src.lax.eigh._mask(x,dims,alternative=0)
jax._src.lax.eigh._projector_subspace(P,H,n,rank,maxiter=2,swap=False)
jax._src.lax.eigh._round_up(i,n)
jax._src.lax.eigh._slice(operand,start_indices,dynamic_slice_sizes,static_slice_sizes,fill_value=0)
jax._src.lax.eigh._update_slice(operand,update,start_indices,update_dims)
jax._src.lax.eigh.eigh(H,*,precision='float32',termination_size=256,n=None,sort_eigenvalues=True,subset_by_index=None)
jax._src.lax.eigh.split_spectrum(H,n,split_point,V0=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/lax.py----------------------------------------
A:jax._src.lax.lax.T->TypeVar('T')
A:jax._src.lax.lax.checked->canonicalize_shape(shape)
A:jax._src.lax.lax.rank->len(idx_shape)
A:jax._src.lax.lax.ndim->numpy.ndim(array)
A:jax._src.lax.lax.result_shape->broadcast_shapes(np.shape(x), np.shape(y))
A:jax._src.lax.lax.dyn_shape_it->iter(dyn_shape)
A:jax._src.lax.lax.shape->jax._src.core.canonicalize_shape(shape)
A:jax._src.lax.lax.source_info->jax._src.source_info_util.current()
A:jax._src.lax.lax.out_tracer->jax._src.interpreters.partial_eval.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)
A:jax._src.lax.lax.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe([operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p, dict(shape=shape, broadcast_dimensions=broadcast_dimensions), core.no_effects, source_info_util.current())
A:jax._src.lax.lax.rounding_method->RoundingMethod(rounding_method)
A:jax._src.lax.lax.operand->jax._src.interpreters.batching.moveaxis(operand, bdim, 0)
A:jax._src.lax.lax.old_dtype->jax._src.dtypes.canonicalize_dtype(operand.dtype)
A:jax._src.lax.lax.old_weak_type->jax._src.dtypes.is_weakly_typed(operand)
A:jax._src.lax.lax.new_dtype->jax._src.dtypes.canonicalize_dtype(new_dtype)
A:jax._src.lax.lax.DEFAULT->_enum_descriptor('default')
A:jax._src.lax.lax.HIGH->_enum_descriptor('high')
A:jax._src.lax.lax.HIGHEST->_enum_descriptor('highest')
A:jax._src.lax.lax.arg0->self._strings.get(arg0, arg0)
A:jax._src.lax.lax.dims->numpy.delete(np.arange(prototype_arg.ndim), new_bdim)
A:jax._src.lax.lax.(dyn_shape, static_shape)->_extract_tracers_dyn_shape(shape)
A:jax._src.lax.lax.new_sizes->tuple(new_sizes)
A:jax._src.lax.lax.same_shape->jax._src.core.definitely_equal_shape(np.shape(operand), new_sizes)
A:jax._src.lax.lax.(dyn_shape, static_new_sizes)->_extract_tracers_dyn_shape(new_sizes)
A:jax._src.lax.lax.permutation->tuple((operator.index(d) for d in permutation))
A:jax._src.lax.lax.(flat_operands, operand_tree)->jax.tree_util.tree_flatten(operands)
A:jax._src.lax.lax.(flat_init_values, init_value_tree)->jax.tree_util.tree_flatten(init_values)
A:jax._src.lax.lax.monoid_reducer->_get_monoid_reducer(computation, flat_init_values)
A:jax._src.lax.lax.flat_init_avals->safe_map(_abstractify, flat_init_values)
A:jax._src.lax.lax.(jaxpr, consts, out_tree)->_variadic_reduction_jaxpr(computation, tuple(flat_init_avals), init_value_tree)
A:jax._src.lax.lax.out->lower_comparator(sub_ctx, *[[a] for a in comparator.arguments], num_keys=num_keys)
A:jax._src.lax.lax.result->select(_isnan(x), full_like(result, np.nan), result)
A:jax._src.lax.lax.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_comp, tuple(flat_in_avals))
A:jax._src.lax.lax.avals->jax.tree_util.tree_unflatten(aval_tree, flat_avals)
A:jax._src.lax.lax.(flat_in_avals, in_tree)->jax.tree_util.tree_flatten((avals, avals))
A:jax._src.lax.lax.comp->jax._src.linear_util.wrap_init(computation)
A:jax._src.lax.lax.(flat_comp, out_tree)->jax._src.api_util.flatten_fun_nokwargs(comp, in_tree)
A:jax._src.lax.lax.aval->jax._src.core.DShapedArray(_merge_dyn_shape(shape, dyn_shape), dtype, False)
A:jax._src.lax.lax.dtype->_dtype(example)
A:jax._src.lax.lax.dimension->kwargs.pop('dimension')
A:jax._src.lax.lax.(k, v)->Primitive('sort').bind(keys, values, dimension=dimension, is_stable=is_stable, num_keys=1)
A:jax._src.lax.lax.k->int(k)
A:jax._src.lax.lax.fill_value->_convert_element_type(fill_value, dtype, weak_type)
A:jax._src.lax.lax.scalar_zero->_convert_element_type(0, aval.dtype, aval.weak_type)
A:jax._src.lax.lax.offset->int(offset)
A:jax._src.lax.lax.bool_eye->eq(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)), broadcasted_iota(np.int32, shape, 1))
A:jax._src.lax.lax.axes->frozenset(axes)
A:jax._src.lax.lax.base_shape->tuple(np.take(shape, axes))
A:jax._src.lax.lax.bool_tri->ge(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)), broadcasted_iota(np.int32, shape, 1))
A:jax._src.lax.lax.exponent_bits->operator.index(exponent_bits)
A:jax._src.lax.lax.mantissa_bits->operator.index(mantissa_bits)
A:jax._src.lax.lax.dimensions->tuple(np.add(1, dimensions))
A:jax._src.lax.lax.dims_set->set(dimensions)
A:jax._src.lax.lax.val->jax._src.dtypes.scalar_type_of(example)(val)
A:jax._src.lax.lax.(lo, hi, _)->slice(start_dimension, stop_dimension).indices(len(operand.shape))
A:jax._src.lax.lax.size->next((x.shape[ax] for (x, ax) in zip(batched_args, batch_dims) if ax is not None))
A:jax._src.lax.lax.batch->tuple(range(lhs.ndim - 2))
A:jax._src.lax.lax.ShapedArray.broadcast->jax._src.core.aval_method(broadcast)
A:jax._src.lax.lax.ShapedArray.transpose->jax._src.core.aval_method(transpose)
A:jax._src.lax.lax.ShapedArray.reshape->jax._src.core.aval_method(reshape)
A:jax._src.lax.lax.n->numpy.prod(input_shape[list(axes)])
A:jax._src.lax.lax.ShapedArray._iter->staticmethod(_iter)
A:jax._src.lax.lax.core.DShapedArray._iter->staticmethod(_iter)
A:jax._src.lax.lax.typename->dtype_to_string(dtype)
A:jax._src.lax.lax.dtype_rule->partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name, allow_extended_dtype=allow_extended_dtype, require_same=require_same_dtypes)
A:jax._src.lax.lax.weak_type_rule->partial(_naryop_weak_type_rule, name)
A:jax._src.lax.lax.prim->standard_primitive(shape_rule, dtype_rule, name, weak_type_rule=weak_type_rule)
A:jax._src.lax.lax.standard_unop->partial(unop, _identity)
A:jax._src.lax.lax.typenames->', '.join((t.__name__ for t in types))
A:jax._src.lax.lax.pos->next((i for (i, aval) in enumerate(avals) if aval.dtype == dtypes.float0))
A:jax._src.lax.lax.shape_rule->partial(broadcasting_shape_rule, name)
A:jax._src.lax.lax.standard_naryop->partial(naryop, _input_dtype)
A:jax._src.lax.lax.bcast_dims->list(range(aval_which.ndim))
A:jax._src.lax.lax.arg->xops.BroadcastInDim(arg, aval_out.shape, bcast_dims)
A:jax._src.lax.lax.x_shape->numpy.shape(x)
A:jax._src.lax.lax.broadcasted_args->jax._src.interpreters.mlir.multi_broadcast_in_dim(ctx, args, avals_in, aval_out.shape)
A:jax._src.lax.lax.neg_p->standard_unop(_num, 'neg')
A:jax._src.lax.lax.sign_p->standard_unop(_num, 'sign')
A:jax._src.lax.lax.nextafter_p->standard_naryop([_float, _float], 'nextafter')
A:jax._src.lax.lax.floor_p->standard_unop(_float, 'floor')
A:jax._src.lax.lax.ceil_p->standard_unop(_float, 'ceil')
A:jax._src.lax.lax.round_p->standard_unop(_float, 'round')
A:jax._src.lax.lax.is_finite_p->unop(_fixed_dtype(np.bool_), _float, 'is_finite')
A:jax._src.lax.lax.exp_p->standard_unop(_float | _complex, 'exp')
A:jax._src.lax.lax.exp2_p->standard_unop(_float | _complex, 'exp2')
A:jax._src.lax.lax.log2->jax._src.interpreters.mlir.broadcast_in_dim(ctx, log2, x_aval, broadcast_dimensions=())
A:jax._src.lax.lax.log_p->standard_unop(_float | _complex, 'log')
A:jax._src.lax.lax.expm1_p->standard_unop(_float | _complex, 'expm1')
A:jax._src.lax.lax.log1p_p->standard_unop(_float | _complex, 'log1p')
A:jax._src.lax.lax.tanh_p->standard_unop(_float | _complex, 'tanh')
A:jax._src.lax.lax.logistic_p->standard_unop(_float | _complex, 'logistic')
A:jax._src.lax.lax.one->_const(x, 1)
A:jax._src.lax.lax.sin_p->standard_unop(_float | _complex, 'sin')
A:jax._src.lax.lax.cos_p->standard_unop(_float | _complex, 'cos')
A:jax._src.lax.lax.tan_p->standard_unop(_float | _complex, 'tan')
A:jax._src.lax.lax.asin_p->standard_unop(_float | _complex, 'asin')
A:jax._src.lax.lax.rpart->real(result)
A:jax._src.lax.lax.acos_p->standard_unop(_float | _complex, 'acos')
A:jax._src.lax.lax.atan_p->standard_unop(_float | _complex, 'atan')
A:jax._src.lax.lax.atan2_p->standard_naryop([_float | _complex, _float | _complex], 'atan2')
A:jax._src.lax.lax.sinh_p->standard_unop(_float | _complex, 'sinh')
A:jax._src.lax.lax.cosh_p->standard_unop(_float | _complex, 'cosh')
A:jax._src.lax.lax.asinh_p->standard_unop(_float | _complex, 'asinh')
A:jax._src.lax.lax.acosh_p->standard_unop(_float | _complex, 'acosh')
A:jax._src.lax.lax.atanh_p->standard_unop(_float | _complex, 'atanh')
A:jax._src.lax.lax.real_p->unop(_complex_basetype, _complex, 'real')
A:jax._src.lax.lax.imag_p->unop(_complex_basetype, _complex, 'imag')
A:jax._src.lax.lax.complex_p->naryop(_complex_dtype, [_complex_elem_types, _complex_elem_types], 'complex')
A:jax._src.lax.lax.conj_p->unop(_complex_dtype, _complex_elem_types | _complex, 'conj')
A:jax._src.lax.lax.ad.primitive_jvps[conj_p]->partial(ad.linear_jvp, conj_p)
A:jax._src.lax.lax.abs_p->unop(_complex_basetype, _signedint | _float | _complex, 'abs')
A:jax._src.lax.lax.sqrt_p->standard_unop(_float | _complex, 'sqrt')
A:jax._src.lax.lax.rsqrt_p->standard_unop(_float | _complex, 'rsqrt')
A:jax._src.lax.lax.cbrt_p->standard_unop(_float, 'cbrt')
A:jax._src.lax.lax.pow_p->naryop(_pow_dtype_rule, [_float | _complex, _int | _float | _complex], 'pow', require_same_dtypes=False)
A:jax._src.lax.lax.y_dtype->jax._src.dtypes.dtype(y)
A:jax._src.lax.lax.(x, y)->jax._src.interpreters.mlir.multi_broadcast_in_dim(ctx, (x, y), avals_in, aval_out.shape)
A:jax._src.lax.lax.x->pad(operand, _zero(operand), padding_config)
A:jax._src.lax.lax.y->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim').bind(operand, *dyn_shape, shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax._src.lax.lax.jac->mul(y, pow(x, sub(y, _ones(y))))
A:jax._src.lax.lax.convert->jax._src.interpreters.mlir.lower_fun(partial(convert_element_type, new_dtype=out_aval.dtype), False)
A:jax._src.lax.lax.x_aval_->x_aval.update(dtype=out_aval.dtype)
A:jax._src.lax.lax.y_aval_->y_aval.update(dtype=out_aval.dtype)
A:jax._src.lax.lax.[(x_,)]->convert(ctx.replace(avals_in=[x_aval], avals_out=[x_aval_]), x)
A:jax._src.lax.lax.[(y_,)]->convert(ctx.replace(avals_in=[y_aval], avals_out=[y_aval_]), y)
A:jax._src.lax.lax.ctx_->ctx.replace(avals_in=[x_aval_, y_aval_])
A:jax._src.lax.lax.integer_pow_p->standard_primitive(_attrgetter('shape'), _integer_pow_dtype_rule, 'integer_pow')
A:jax._src.lax.lax.lowering->jax._src.interpreters.mlir.cache_lowering(lowering)
A:jax._src.lax.lax.not_p->standard_unop(_bool_or_int, 'not')
A:jax._src.lax.lax.and_p->standard_naryop([_bool_or_int, _bool_or_int], 'and')
A:jax._src.lax.lax.or_p->standard_naryop([_bool_or_int, _bool_or_int], 'or')
A:jax._src.lax.lax.xor_p->standard_naryop([_bool_or_int, _bool_or_int], 'xor')
A:jax._src.lax.lax.population_count_p->standard_unop(_int, 'population_count')
A:jax._src.lax.lax.clz_p->standard_unop(_int, 'clz')
A:jax._src.lax.lax.primal_out->sub(x, y)
A:jax._src.lax.lax.sub_p->standard_naryop([_num, _num], 'sub')
A:jax._src.lax.lax.mul_p->standard_naryop([_num, _num], 'mul')
A:jax._src.lax.lax.div_p->standard_naryop([_num, _num], 'div')
A:jax._src.lax.lax.rem_p->standard_naryop([_int | _float, _int | _float], 'rem')
A:jax._src.lax.lax.rx->real(x)
A:jax._src.lax.lax.ry->real(y)
A:jax._src.lax.lax.pick_x->select(eq(rx, ry), lax_cmp_pick_x(imag(x), imag(y)), lax_cmp_pick_x(rx, ry))
A:jax._src.lax.lax.shift_left_p->standard_naryop([_int, _int], 'shift_left')
A:jax._src.lax.lax.shift_right_arithmetic_p->standard_naryop([_int, _int], 'shift_right_arithmetic')
A:jax._src.lax.lax.shift_right_logical_p->standard_naryop([_int, _int], 'shift_right_logical')
A:jax._src.lax.lax.base_aval_x->jax._src.core.physical_aval(aval_x)
A:jax._src.lax.lax.base_aval_y->jax._src.core.physical_aval(aval_y)
A:jax._src.lax.lax.base_aval_out->jax._src.core.ShapedArray(base_aval_x.shape, aval_out.dtype)
A:jax._src.lax.lax.reduce_axes->tuple(range(aval_out.ndim, base_aval_out.ndim))
A:jax._src.lax.lax.(res,)->jax._src.interpreters.mlir.delegate_lowering(ctx, partial(_compare_lower_hlo, direction, False), x, y, avals_in=[base_aval_x, base_aval_y], avals_out=[base_aval_out])
A:jax._src.lax.lax._opaque_eq_hlo->partial(_opaque_comparison_hlo, 'EQ', hlo.AndOp, _get_bitwise_and_identity)
A:jax._src.lax.lax._opaque_ne_hlo->partial(_opaque_comparison_hlo, 'NE', hlo.OrOp, _get_bitwise_or_identity)
A:jax._src.lax.lax.broadcast_avals_in->tuple((core.ShapedArray(aval_out.shape, aval.dtype) for aval in avals_in))
A:jax._src.lax.lax.eq_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'eq', allow_extended_dtype=True)
A:jax._src.lax.lax.ne_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'ne', allow_extended_dtype=True)
A:jax._src.lax.lax.ge_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'ge')
A:jax._src.lax.lax.gt_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'gt')
A:jax._src.lax.lax.le_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'le')
A:jax._src.lax.lax.lt_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'lt')
A:jax._src.lax.lax.eq_to_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'eq_to')
A:jax._src.lax.lax.le_to_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'le_to')
A:jax._src.lax.lax.lt_to_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'lt_to')
A:jax._src.lax.lax.params->dict(dtype=dtype, shape=shape, dimension=dimension)
A:jax._src.lax.lax.convert_element_type_p->Primitive('convert_element_type')
A:jax._src.lax.lax.aval_in->aval_in.update(dtype=_real_dtype(aval_in.dtype)).update(dtype=_real_dtype(aval_in.dtype))
A:jax._src.lax.lax.bitcast_convert_type_p->standard_primitive(_bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule, 'bitcast_convert_type', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.config->jax._src.lib.xla_client.PrecisionConfig()
A:jax._src.lax.lax.lhs_batch_shape->tuple((lhs.shape[i] for i in lhs_batch))
A:jax._src.lax.lax.rhs_batch_shape->tuple((rhs.shape[i] for i in rhs_batch))
A:jax._src.lax.lax.lhs_contracting_shape->tuple((lhs.shape[i] for i in lhs_contracting))
A:jax._src.lax.lax.rhs_contracting_shape->tuple((rhs.shape[i] for i in rhs_contracting))
A:jax._src.lax.lax.batch_shape->tuple((lhs_shape[i] for i in lhs_batch))
A:jax._src.lax.lax.lhs_contract_or_batch->tuple(sorted(tuple(lhs_contracting) + tuple(lhs_batch)))
A:jax._src.lax.lax.lhs_tensored_shape->tuple_delete(lhs_shape, lhs_contract_or_batch)
A:jax._src.lax.lax.rhs_contract_or_batch->tuple(sorted(tuple(rhs_contracting) + tuple(rhs_batch)))
A:jax._src.lax.lax.rhs_tensored_shape->tuple_delete(rhs_shape, rhs_contract_or_batch)
A:jax._src.lax.lax.idx_->set(idx)
A:jax._src.lax.lax.x_kept->remaining(range(x_ndim), x_contract, x_batch)
A:jax._src.lax.lax.y_kept->remaining(range(np.ndim(y)), y_contract, y_batch)
A:jax._src.lax.lax.(ans_batch, ans_y, _)->ranges_like(x_batch, y_kept, x_kept)
A:jax._src.lax.lax.(ans_batch, _, ans_y)->ranges_like(x_batch, x_kept, y_kept)
A:jax._src.lax.lax.x_contract_sorted_by_y->list(np.take(x_contract, np.argsort(y_contract)))
A:jax._src.lax.lax.out_axes->numpy.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
A:jax._src.lax.lax.x_bar->_convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)
A:jax._src.lax.lax.y_bar->_convert_element_type(y_bar, y.aval.dtype, y.aval.weak_type)
A:jax._src.lax.lax.(new_dimension_numbers, result_stack_dim)->_dot_general_batch_dim_nums((np.ndim(lhs), np.ndim(rhs)), (left_stack_dim, right_stack_dim), dimension_numbers)
A:jax._src.lax.lax.lhs->jax._src.interpreters.mlir.convert_hlo(ctx, lhs, lhs_aval, core.ShapedArray(lhs_aval.shape, np.float32))
A:jax._src.lax.lax.lhs_shape->numpy.shape(lhs)
A:jax._src.lax.lax.rhs->jax._src.interpreters.mlir.convert_hlo(ctx, rhs, rhs_aval, core.ShapedArray(rhs_aval.shape, np.float32))
A:jax._src.lax.lax.rhs_shape->numpy.shape(rhs)
A:jax._src.lax.lax.batched_out->dot_general(lhs, rhs, new_dimension_numbers, precision=precision, preferred_element_type=preferred_element_type)
A:jax._src.lax.lax.result_batch_dim->jax._src.interpreters.batching.shape_as_bdim(result_stack_dim, _dot_general_shape_computation(lhs_shape, rhs_shape, new_dimension_numbers))
A:jax._src.lax.lax.lhs_contract->bump_dims(lhs_contract, lbd)
A:jax._src.lax.lax.rhs_contract->bump_dims(rhs_contract, rbd)
A:jax._src.lax.lax.lhs_batch->bump_dims(lhs_batch, lbd)
A:jax._src.lax.lax.rhs_batch->bump_dims(rhs_batch, rbd)
A:jax._src.lax.lax.lhs_->_replace_masked_values(lhs, 0, padded_axes)
A:jax._src.lax.lax.dot_general_p->standard_primitive(_dot_general_shape_rule, _dot_general_dtype_rule, 'dot_general')
A:jax._src.lax.lax.dot_dnums->jax._src.lib.mlir.dialects.hlo.DotDimensionNumbers.get(lhs_batching_dimensions=list(lhs_batch), rhs_batching_dimensions=list(rhs_batch), lhs_contracting_dimensions=list(lhs_contracting), rhs_contracting_dimensions=list(rhs_contracting))
A:jax._src.lax.lax.operand_ndim->numpy.ndim(operand)
A:jax._src.lax.lax.(out_aval, effects)->Primitive('iota').abstract_eval(dtype=dtype, shape=shape, dimension=dimension)
A:jax._src.lax.lax.out_shape->_ceil_divide(in_shape, window_strides)
A:jax._src.lax.lax.out_aval->jax._src.core.DShapedArray(tuple(out_shape), dtype, False)
A:jax._src.lax.lax.bdims->tuple(np.delete(broadcast_dimensions, unit_dims))
A:jax._src.lax.lax.new_operand->jax._src.interpreters.batching.moveaxis(operand, stacked_axis, 0)
A:jax._src.lax.lax.new_broadcast_dimensions->tuple(np.add(1, broadcast_dimensions))
A:jax._src.lax.lax.stacked_size->len(sizes)
A:jax._src.lax.lax.out_bdim->jax._src.interpreters.batching.make_batch_axis(result.ndim, 0, zip(out_ragged_axes, out_ragged_sizes))
A:jax._src.lax.lax.y_dot->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim').bind(operand_dot, *dyn_shape, shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax._src.lax.lax.operand_tracer->trace.instantiate_const(operand)
A:jax._src.lax.lax.dyn_shape_tracers->map(trace.instantiate_const, dyn_shape)
A:jax._src.lax.lax.dyn_shape_tracers_->iter(dyn_shape_tracers)
A:jax._src.lax.lax.aval_out->aval_out.update(shape=_merge_dyn_shape(shape, dyn_shape)).update(shape=_merge_dyn_shape(shape, dyn_shape))
A:jax._src.lax.lax.new_eqn->jax._src.interpreters.partial_eval.new_eqn_recipe([operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p, dict(shape=shape, broadcast_dimensions=broadcast_dimensions), core.no_effects, source_info_util.current()).replpace(params=printed_params, invars=eqn.invars[:1])
A:jax._src.lax.lax.broadcast_in_dim_p->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
A:jax._src.lax.lax._clamp_dtype_rule->partial(naryop_dtype_rule, _input_dtype, [_any, _any, _any], 'clamp')
A:jax._src.lax.lax.min->broadcast_in_dim(min, x.shape, [0])
A:jax._src.lax.lax.max->broadcast_in_dim(max, x.shape, [0])
A:jax._src.lax.lax.clamp_p->standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')
A:jax._src.lax.lax.op->jax._src.lib.mlir.dialects.hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes))
A:jax._src.lax.lax.concat_size->sum((o.shape[dimension] for o in operands))
A:jax._src.lax.lax.limit_points->numpy.cumsum([shape[dimension] for shape in operand_shapes]).tolist()
A:jax._src.lax.lax.starts->numpy.zeros((len(operands), t.ndim), dtype=int).tolist()
A:jax._src.lax.lax.limits->numpy.tile(t.shape, (len(operands), 1)).tolist()
A:jax._src.lax.lax.concatenate_p->standard_primitive(_concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate')
A:jax._src.lax.lax.op_shape->numpy.shape(operand)
A:jax._src.lax.lax.(lo, hi, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax.lax.unpad_config->safe_zip(np.negative(lo), np.negative(hi), np.zeros_like(interior))
A:jax._src.lax.lax.unpadded->pad(t, np.array(0.0, t.dtype), unpad_config)
A:jax._src.lax.lax.padding_config->list(padding_config)
A:jax._src.lax.lax.mask->pad(full_like(operand, True, np.bool_), False, padding_config)
A:jax._src.lax.lax.broadcasted_padding->broadcast_in_dim(padding_value, x.shape, (operand_bdim,))
A:jax._src.lax.lax.pad_p->standard_primitive(_pad_shape_rule, _pad_dtype_rule, 'pad')
A:jax._src.lax.lax.(low, high, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax.lax.(operand, bdim)->jax._src.interpreters.batching.move_stacked_axis(operand, bdim, 0)
A:jax._src.lax.lax.bdim_out->jax._src.interpreters.batching.shape_as_bdim(out_stack_dim, _compute_squeeze_shape(batching.bdim_as_shape(bdim, operand.shape), dimensions))
A:jax._src.lax.lax.squeeze_p->standard_primitive(_squeeze_shape_rule, _squeeze_dtype_rule, 'squeeze')
A:jax._src.lax.lax.av->jax._src.core.DShapedArray(_merge_dyn_shape(new_sizes, dyn), x.dtype, x.weak_type)
A:jax._src.lax.lax.reshape_p->standard_primitive(_reshape_shape_rule, _reshape_dtype_rule, 'reshape')
A:jax._src.lax.lax.rev_p->standard_primitive(_rev_shape_rule, _input_dtype, 'rev')
A:jax._src.lax.lax.res_bdim->jax._src.interpreters.batching.transpose_ragged_axes(bdim.move_stacked_axis(0), perm)
A:jax._src.lax.lax.transpose_p->standard_primitive(_transpose_shape_rule, _input_dtype, 'transpose')
A:jax._src.lax.lax.zeros->full_like(t, 0)
A:jax._src.lax.lax.which->broadcast_in_dim(which, cases[0].shape, [0])
A:jax._src.lax.lax.out_dot->select_n(which, *case_tangents)
A:jax._src.lax.lax.z->_zeros(next((t for t in case_tangents if type(t) is not ad_util.Zero)))
A:jax._src.lax.lax.physical_aval_out->jax._src.core.physical_aval(aval_out)
A:jax._src.lax.lax.aval_which_bcast->jax._src.core.physical_aval(aval_out).update(dtype=aval_which.dtype)
A:jax._src.lax.lax.which_bcast->jax._src.interpreters.mlir.broadcast_in_dim(ctx, which, aval_which_bcast, broadcast_dimensions=bcast_dims)
A:jax._src.lax.lax.pred->jax._src.interpreters.mlir.compare_hlo(which, mlir.full_like_aval(ctx, offset + mid, which_aval), lt, compare_type)
A:jax._src.lax.lax.select_n_p->standard_primitive(_select_shape_rule, _select_dtype_rule, 'select_n', weak_type_rule=_select_weak_type_rule)
A:jax._src.lax.lax.(operand_avals, init_val_avals)->split_list(avals, [len(avals) // 2])
A:jax._src.lax.lax.(operands, init_values)->jax._src.util.split_list(values, [len(values) // 2])
A:jax._src.lax.lax.(operand_bdims, init_value_bdims)->split_list(batch_dims, [num_operands])
A:jax._src.lax.lax.input_shape->numpy.array(primals[0].shape, dtype=int)
A:jax._src.lax.lax.non_axes->numpy.delete(np.arange(len(input_shape)), axes)
A:jax._src.lax.lax.primals->Primitive('sort').bind(*primals + (iotas[dimension],), dimension=dimension, is_stable=is_stable, num_keys=num_keys)
A:jax._src.lax.lax.tangents->tuple((reshape(t, new_shape, permutation) for t in tangents))
A:jax._src.lax.lax.reducer->jax._src.lib.mlir.dialects.hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes)).regions[0].blocks.append(*ir_types + ir_types)
A:jax._src.lax.lax.xs->reducer(*xs1 + xs2)
A:jax._src.lax.lax.(primal_xs, init_values)->split_list(primals, [len(primals) // 2])
A:jax._src.lax.lax.(tangent_xs, tangent_init)->split_list(tangents, [len(tangents) // 2])
A:jax._src.lax.lax.(operand_avals, init_avals)->split_list(avals, [num_operands])
A:jax._src.lax.lax.join->jax._src.core.join_named_shapes(*(a.named_shape for a in operand_avals))
A:jax._src.lax.lax.reduce_p->jax._src.core.Primitive('reduce')
A:jax._src.lax.lax.reducer_ctx->ctx.module_context.replace(name_stack=source_info_util.new_name_stack())
A:jax._src.lax.lax.(out_nodes, _)->jax._src.interpreters.mlir.jaxpr_subcomp(reducer_ctx, jaxpr, mlir.TokenSet(), consts, *([a] for a in reducer.arguments), dim_var_values=ctx.dim_var_values)
A:jax._src.lax.lax.broadcast_dimensions->tuple(np.delete(np.arange(len(input_shape)), axes))
A:jax._src.lax.lax.operand_->_replace_masked_values(operand, ident(aval.dtype), padded_axes)
A:jax._src.lax.lax.reduce_sum_p->standard_primitive(_reduce_sum_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_sum'), 'reduce_sum')
A:jax._src.lax.lax.pe.padding_rules[reduce_sum_p]->partial(_reducer_padding, _reduce_sum, _get_sum_identity)
A:jax._src.lax.lax.(primals_out, tangents_out)->_reduce_jvp(reducer, [_const(primals[0], 1)], primals, tangents, axes)
A:jax._src.lax.lax.reduce_prod_p->standard_primitive(_reduce_op_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_prod'), 'reduce_prod')
A:jax._src.lax.lax.pe.padding_rules[reduce_prod_p]->partial(_reducer_padding, _reduce_prod, _get_prod_identity)
A:jax._src.lax.lax.location_indicators->convert_element_type(_eq_meet(operand, reshape(ans, shape)), g.dtype)
A:jax._src.lax.lax.counts->_reduce_sum(location_indicators, axes)
A:jax._src.lax.lax.reduce_max_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_max')
A:jax._src.lax.lax.pe.padding_rules[reduce_max_p]->partial(_reducer_padding, _reduce_max, _get_max_identity)
A:jax._src.lax.lax.reduce_min_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_min')
A:jax._src.lax.lax.pe.padding_rules[reduce_min_p]->partial(_reducer_padding, _reduce_min, _get_min_identity)
A:jax._src.lax.lax.indices->broadcasted_iota(index_dtype, np.shape(operand), axis)
A:jax._src.lax.lax.pick_op_val->bitwise_or(value_comparator(op_val, acc_val), ne(op_val, op_val))
A:jax._src.lax.lax.pick_op_index->bitwise_or(pick_op_val, bitwise_and(eq(op_val, acc_val), lt(op_index, acc_index)))
A:jax._src.lax.lax.res->reduce([operand, indices], [get_identity(operand.dtype), np.array(0, index_dtype)], reducer_fn, axes)
A:jax._src.lax.lax.argmin_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmin', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.argmax_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmax', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_or_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_or', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_and_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_and', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_xor_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_xor', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.scalar_type->jax._src.interpreters.mlir.aval_to_ir_type(core.ShapedArray((), dtype))
A:jax._src.lax.lax.reducer_region->jax._src.lib.mlir.dialects.hlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes)).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.lax.add->reducer(*reducer_region.arguments)
A:jax._src.lax.lax.reduce_precision_p->standard_primitive(_reduce_precision_shape_rule, partial(unop_dtype_rule, _identity, _float, 'reduce_precision'), name='reduce_precision')
A:jax._src.lax.lax.args->tuple((raise_to_shaped(arg) for arg in args))
A:jax._src.lax.lax.shapes->' '.join((str(a.shape) for a in args))
A:jax._src.lax.lax.(x_keys, y_keys)->_operands_to_keys(*operands, num_keys=num_keys)
A:jax._src.lax.lax.idx->tuple((primals[-1] if i == dimension else iotas[i] for i in range(len(shape))))
A:jax._src.lax.lax.tangents_out->tuple((t if type(t) is ad_util.Zero else t[idx] for t in tangents))
A:jax._src.lax.lax.(prototype_arg, new_bdim)->next(((a, b) for (a, b) in zip(batched_args, batch_dims) if b is not None))
A:jax._src.lax.lax.sort_p->Primitive('sort')
A:jax._src.lax.lax.sort->jax._src.lib.mlir.dialects.hlo.SortOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out], mlir.flatten_lowering_ir_args(operands), dimension=mlir.i64_attr(dimension), is_stable=ir.BoolAttr.get(is_stable))
A:jax._src.lax.lax.scalar_types->safe_map(mlir.aval_to_ir_type, scalar_avals)
A:jax._src.lax.lax.comparator->jax._src.lib.mlir.dialects.hlo.SortOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out], mlir.flatten_lowering_ir_args(operands), dimension=mlir.i64_attr(dimension), is_stable=ir.BoolAttr.get(is_stable)).comparator.blocks.append(*util.flatten(zip(scalar_types, scalar_types)))
A:jax._src.lax.lax.lower_comparator->jax._src.interpreters.mlir.lower_fun(partial(_sort_lt_comparator), multiple_results=False)
A:jax._src.lax.lax.sub_ctx->ctx.replace(primitive=None, avals_in=util.flatten(zip(scalar_avals, scalar_avals)), avals_out=[core.ShapedArray((), np.bool_)])
A:jax._src.lax.lax.primals_out->top_k(operand, k)
A:jax._src.lax.lax.tangent_out->jax._src.lax.slicing.gather(tangent, gather_indices, dnums, slice_sizes)
A:jax._src.lax.lax._iota->broadcast_in_dim(_iota, gather_index_shape, (i,))
A:jax._src.lax.lax.gather_indices->concatenate(gather_indices, dimension=rank)
A:jax._src.lax.lax.dnums->jax._src.lax.slicing.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=tuple(range(rank)), start_index_map=tuple(range(rank)))
A:jax._src.lax.lax.perm->numpy.arange(operand.ndim)
A:jax._src.lax.lax.(top_k_v, top_k_i)->top_k(transpose(operand, perm), k=k)
A:jax._src.lax.lax.top_k_p->Primitive('top_k')
A:jax._src.lax.lax.(k_value,)->jax._src.interpreters.mlir.eval_dynamic_shape_as_vals(ctx, (k,))
A:jax._src.lax.lax.create_token_p->Primitive('create_token')
A:jax._src.lax.lax.after_all_p->Primitive('after_all')
A:jax._src.lax.lax.infeed_effect->InOutFeedEffect()
A:jax._src.lax.lax.outfeed_effect->InOutFeedEffect()
A:jax._src.lax.lax.(flat_shapes, treedef)->jax.tree_util.tree_flatten(shape)
A:jax._src.lax.lax.xs_and_token->Primitive('infeed').bind(token, shapes=tuple(flat_shapes), partitions=partitions)
A:jax._src.lax.lax.infeed_p->Primitive('infeed')
A:jax._src.lax.lax.output_types->safe_map(mlir.aval_to_ir_types, ctx.avals_out[:-1])
A:jax._src.lax.lax.flat_output_types->jax._src.util.flatten(output_types)
A:jax._src.lax.lax.layouts->jax._src.lib.mlir.ir.ArrayAttr.get([ir.ArrayAttr.get([mlir.i64_attr(i) for i in range(len(aval.shape) - 1, -1, -1)]) for aval in shapes])
A:jax._src.lax.lax.infeed->jax._src.lib.mlir.dialects.hlo.InfeedOp(flat_output_types + [hlo.TokenType.get()], token, infeed_config=ir.StringAttr.get(''), layout=layouts)
A:jax._src.lax.lax.(flat_xs, _)->jax.tree_util.tree_flatten(xs)
A:jax._src.lax.lax.outfeed_p->Primitive('outfeed')
A:jax._src.lax.lax.outfeed->jax._src.lib.mlir.dialects.hlo.OutfeedOp(mlir.flatten_lowering_ir_args(xs), token, outfeed_config=ir.StringAttr.get(''))
A:jax._src.lax.lax.rng_uniform_p->Primitive('rng_uniform')
A:jax._src.lax.lax.(shape,)->jax._src.interpreters.mlir.ir_constants(np.array(aval_out.shape, np.int64))
A:jax._src.lax.lax.key_type->jax._src.lib.mlir.ir.RankedTensorType(key.type)
A:jax._src.lax.lax.u32_type->jax._src.lib.mlir.ir.IntegerType.get_unsigned(32)
A:jax._src.lax.lax.u64_type->jax._src.lib.mlir.ir.IntegerType.get_unsigned(64)
A:jax._src.lax.lax.etype->jax._src.interpreters.mlir.dtype_to_ir_type(dtype)
A:jax._src.lax.lax.algorithm_attr->_rng_algorithm(algorithm)
A:jax._src.lax.lax.output_shape->jax._src.interpreters.mlir.shape_tensor(mlir.eval_dynamic_shape(ctx, out_vals_aval.shape))
A:jax._src.lax.lax.rng_bit_generator_p->Primitive('rng_bit_generator')
A:jax._src.lax.lax.(axis_name, static_broadcasted_tuple, donate_tuple)->jax._src.api._shared_code_pmap(_identity_fn, None, (), (), sharded_dim, sharded_dim)
A:jax._src.lax.lax.p->jax._src.api._prepare_pmap(_identity_fn, sharded_dim, sharded_dim, static_broadcasted_tuple, donate_tuple, None, None, None, args, kwargs)
A:jax._src.lax.lax.out_flat->jax._src.interpreters.pxla.xla_pmap_impl(p.flat_fun, *p.flat_args, backend=None, axis_name=axis_name, axis_size=p.local_axis_size, global_axis_size=p.global_axis_size, devices=p.devices, in_axes=p.in_axes_flat, out_axes_thunk=p.out_axes_thunk, name=p.flat_fun.__name__, donated_invars=p.donated_invars, is_explicit_global_axis_size=p.is_explicit_global_axis_size)
A:jax._src.lax.lax.sharded_dim->_which_dim_sharded(a.sharding)
A:jax._src.lax.lax.copy_p->jax._src.core.Primitive('copy')
A:jax._src.lax.lax.iota_p->Primitive('iota')
A:jax._src.lax.lax.iota->broadcasted_iota(dtype, shape, dimension + 1)
A:jax._src.lax.lax.pad_sizes->numpy.maximum(0, (out_shape - 1) * window_strides + window_shape - in_shape)
A:jax._src.lax.lax.obj_arr->numpy.array(obj)
A:jax._src.lax.lax.x_len->len(x)
A:jax._src.lax.lax.removed->set(itertools.chain(*removed_lists))
A:jax._src.lax.lax.higher_dtype->jax._src.dtypes.promote_types(a_dtype, b_dtype)
A:jax._src.lax.lax.a->convert_element_type(a, b_dtype)
A:jax._src.lax.lax.b->convert_element_type(b, a_dtype)
A:jax._src.lax.lax.empty_p->jax._src.core.Primitive('empty')
A:jax._src.lax.lax.phys_aval->jax._src.core.physical_aval(aval)
A:jax._src.lax.lax.tie_p->jax._src.core.Primitive('tie')
A:jax._src.lax.lax.buf.aval->jax._src.core.ShapedArray(buf.shape, buf.dtype)
A:jax._src.lax.lax.phys_handler->phys_handler_maker(phys_aval, phys_sharding, committed, is_out_sharding_from_xla)
jax._src.lax.lax.BIntRules
jax._src.lax.lax.BIntRules.global_sharded_result_handler(aval,out_sharding,committed,is_out_sharding_from_xla)
jax._src.lax.lax.BIntRules.physical_element_aval(dtype)->core.ShapedArray
jax._src.lax.lax.BIntRules.physical_hlo_sharding(aval,hlo_sharding:xc.HloSharding)->xc.HloSharding
jax._src.lax.lax.BIntRules.result_handler(sticky_device,aval)
jax._src.lax.lax.InOutFeedEffect(effects.Effect)
jax._src.lax.lax.PaddingType(enum.Enum)
jax._src.lax.lax.Precision(self,arg0)
jax._src.lax.lax.Precision.__init__(self,arg0)
jax._src.lax.lax.Precision.__str__(self)->str
jax._src.lax.lax.RoundingMethod(enum.IntEnum)
jax._src.lax.lax._abs_jvp_rule(g,ans,x)
jax._src.lax.lax._abstractify(x)
jax._src.lax.lax._add_inverse(r,x,y)
jax._src.lax.lax._add_jvp(primals,tangents)
jax._src.lax.lax._add_transpose(t,x,y)
jax._src.lax.lax._after_all_abstract_eval(*operands)
jax._src.lax.lax._after_all_lowering(ctx,*operands)
jax._src.lax.lax._argminmax_dtype_rule(operand,*,axes,index_dtype)
jax._src.lax.lax._argminmax_shape_rule(operand,*,axes,index_dtype)
jax._src.lax.lax._array_copy(arr:ArrayLike)->Array
jax._src.lax.lax._balanced_eq(x,z,y)
jax._src.lax.lax._bit_width(d)
jax._src.lax.lax._bitcast_convert_type_dtype_rule(operand,*,new_dtype)
jax._src.lax.lax._bitcast_convert_type_lower(ctx,operand,*,new_dtype)
jax._src.lax.lax._bitcast_convert_type_shape_rule(operand,*,new_dtype)
jax._src.lax.lax._broadcast_in_dim_abstract_eval(x,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_batch_rule(batched_args,batch_dims,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_fwd_rule(eqn)
jax._src.lax.lax._broadcast_in_dim_jvp_rule(primals,tangents,*,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_lower(ctx,x,*dyn_shape,shape,broadcast_dimensions)->Sequence[ir.Value]
jax._src.lax.lax._broadcast_in_dim_padding_rule(in_avals,out_avals,x,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_partial_eval(trace,operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_pp_rule(eqn,context,settings)
jax._src.lax.lax._broadcast_in_dim_shape_rule(operand,*,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_staging_rule(trace,x,*dyn,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_transpose_rule(ct,operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_typecheck_rule(_,operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_ranks(s1,s2)
jax._src.lax.lax._broadcast_shapes_cached(*shapes:tuple[int,...])->tuple[int, ...]
jax._src.lax.lax._broadcast_shapes_uncached(*shapes)
jax._src.lax.lax._broadcast_translate(op,ctx,avals_in,avals_out,*args)
jax._src.lax.lax._canonicalize_float_for_sort(x)
jax._src.lax.lax._ceil_divide(x1,x2)
jax._src.lax.lax._check_shapelike(fun_name,arg_name,obj,non_zero_shape=False)
jax._src.lax.lax._clamp_batch_rule(batched_args,batch_dims,**params)
jax._src.lax.lax._clamp_shape_rule(min,operand,max)
jax._src.lax.lax._compare_lower_hlo(direction:str,total_order:bool,ctx,x,y)
jax._src.lax.lax._compare_lower_hlo_opaque(direction:str,ctx,avals_in,aval_out,x,y)
jax._src.lax.lax._complex_transpose_rule(t,x,y)
jax._src.lax.lax._compute_argminmax(value_comparator,get_identity,operand,*,index_dtype,axes)
jax._src.lax.lax._compute_squeeze_shape(shape,dimensions)
jax._src.lax.lax._concatenate_batch_rule(batched_args,batch_dims,*,dimension)
jax._src.lax.lax._concatenate_dtype_rule(*operands,**kwargs)
jax._src.lax.lax._concatenate_lower(ctx,*xs,dimension)
jax._src.lax.lax._concatenate_pad_rule(in_avals,out_avals,*operands,dimension)
jax._src.lax.lax._concatenate_shape_rule(*operands,**kwargs)
jax._src.lax.lax._concatenate_transpose_rule(t,*operands,dimension)
jax._src.lax.lax._conj_impl(x,**kw)
jax._src.lax.lax._conj_transpose_rule(t,x,*,input_dtype)
jax._src.lax.lax._const(example,val)
jax._src.lax.lax._convert_element_type(operand:ArrayLike,new_dtype:Optional[DTypeLike]=None,weak_type:bool=False)
jax._src.lax.lax._convert_element_type_dtype_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_jvp_rule(tangent,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_lower(ctx,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_shape_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_transpose_rule(ct,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_weak_type_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_elt_type_folding_rule(consts,eqn)
jax._src.lax.lax._convert_elt_type_fwd_rule(eqn)
jax._src.lax.lax._convert_elt_type_pp_rule(eqn,context,settings)
jax._src.lax.lax._copy_impl(prim,*args,**kwargs)
jax._src.lax.lax._copy_impl_pmap_sharding(sharded_dim,*args,**kwargs)
jax._src.lax.lax._create_token_lowering(ctx,*operands)
jax._src.lax.lax._delta(dtype:DTypeLike,shape:Shape,axes:Sequence[int])->Array
jax._src.lax.lax._dilate_shape(shape,dilation)
jax._src.lax.lax._div_transpose_rule(cotangent,x,y)
jax._src.lax.lax._dot_general_batch_dim_nums(ndims,batch_dims,dimension_numbers)
jax._src.lax.lax._dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_dtype_rule(lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_lower(ctx,lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[np.dtype],platform:str='default')
jax._src.lax.lax._dot_general_padding_rule(in_avals,out_avals,lhs,rhs,*,dimension_numbers,**params)
jax._src.lax.lax._dot_general_pp_rule(eqn,context,settings)->pp.Doc
jax._src.lax.lax._dot_general_shape_computation(lhs_shape,rhs_shape,dimension_numbers)
jax._src.lax.lax._dot_general_shape_rule(lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_transpose_lhs(g,x,y,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike],swap_ans=False)
jax._src.lax.lax._dot_general_transpose_rhs(g,x,y,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dyn_shape_staging_rule(trace,prim,out_aval,*args,**params)
jax._src.lax.lax._empty_lower(ctx,*,dtype)
jax._src.lax.lax._enum_descriptor(self,val)
jax._src.lax.lax._enum_descriptor.__get__(self,_,owner)
jax._src.lax.lax._enum_descriptor.__init__(self,val)
jax._src.lax.lax._eq_meet(a,b)
jax._src.lax.lax._exp2_lower(ctx,x)
jax._src.lax.lax._extract_tracers_dyn_shape(shape:Sequence[Union[int,core.Tracer]])->tuple[list[core.Tracer], list[Optional[int]]]
jax._src.lax.lax._eye(dtype:DTypeLike,shape:Shape,offset:int)->Array
jax._src.lax.lax._get_bitwise_and_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_bitwise_or_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_max_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_min_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_monoid_reducer(monoid_op:Callable,xs:Sequence[Array])->Optional[Callable]
jax._src.lax.lax._get_prod_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_sum_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._identity(x)
jax._src.lax.lax._identity_fn(x)
jax._src.lax.lax._infeed_abstract_eval(token,*,shapes,partitions)
jax._src.lax.lax._infeed_lowering(ctx,token,*,shapes,partitions)
jax._src.lax.lax._integer_pow(x,*,y)
jax._src.lax.lax._integer_pow_dtype_rule(x,*,y)
jax._src.lax.lax._integer_pow_jvp(g,x,*,y)
jax._src.lax.lax._integer_pow_lowering(ctx,x,*,y)
jax._src.lax.lax._iota_abstract_eval(*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_batching_rule(in_vals,in_dims,*,dtype,shape,dimension)
jax._src.lax.lax._iota_lower(ctx,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_padding_rule(in_avals,out_avals,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_pp_rule(eqn,context,settings)
jax._src.lax.lax._iota_staging_rule(trace,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_typecheck_rule(_,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iscomplex(x)->bool
jax._src.lax.lax._isnan(x:ArrayLike)->Array
jax._src.lax.lax._iter(tracer)
jax._src.lax.lax._maybe_broadcast(target_shape,x)
jax._src.lax.lax._maybe_upcast(result_dtype,preferred_element_type)
jax._src.lax.lax._merge_dyn_shape(static_shape:Sequence[Optional[int]],dyn_shape:Sequence[Any])->tuple[Union[int, mlir.Value, core.Tracer], ...]
jax._src.lax.lax._minmax_complex_lowering(x,y,*,lax_cmp_pick_x)
jax._src.lax.lax._mul_inverse(r,x,y)
jax._src.lax.lax._mul_transpose(ct,x,y)
jax._src.lax.lax._nary_lower_hlo(op:Callable,ctx,*args:Union[ir.Value,Sequence[ir.Value]],explicit_type=False,**params)
jax._src.lax.lax._naryop_weak_type_rule(name,*avals,**kwargs)
jax._src.lax.lax._opaque_comparison_hlo(direction,reduction_op,identity,ctx,avals_in,aval_out,x,y)
jax._src.lax.lax._operands_to_keys(*operands,num_keys=1)
jax._src.lax.lax._outfeed_abstract_eval(token,*xs,partitions)
jax._src.lax.lax._outfeed_lowering(ctx,token,*xs,partitions)
jax._src.lax.lax._pad_batch_rule(batched_args,batch_dims,*,padding_config)
jax._src.lax.lax._pad_dtype_rule(operand,padding_value,*,padding_config)
jax._src.lax.lax._pad_lower(ctx,x,padding_value,*,padding_config)
jax._src.lax.lax._pad_shape_rule(operand,padding_value,*,padding_config)
jax._src.lax.lax._pad_transpose(t,operand,padding_value,*,padding_config)
jax._src.lax.lax._pow_dtype_rule(x,y)
jax._src.lax.lax._pow_jvp_lhs(g,ans,x,y)
jax._src.lax.lax._pow_jvp_rhs(g,ans,x,y)
jax._src.lax.lax._pow_lower(ctx,x,y)
jax._src.lax.lax._precision_config(precision)
jax._src.lax.lax._real_dtype(dtype)
jax._src.lax.lax._reduce_and(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_batch_rule(batched_args,batch_dims,*,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_chooser_jvp_rule(g,ans,operand,*,axes)
jax._src.lax.lax._reduce_chooser_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_dtype_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_jvp(reducer,init_values,primals,tangents,axes)
jax._src.lax.lax._reduce_jvp_rule(primals,tangents,*,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_logical_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_lower(ctx,*values,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_max(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_min(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_named_shape_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_number_dtype_rule(name,operand,*args,**kw)
jax._src.lax.lax._reduce_op_shape_rule(operand,*,axes,input_shape=None)
jax._src.lax.lax._reduce_or(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_precision_lower(ctx,operand,*,exponent_bits,mantissa_bits)
jax._src.lax.lax._reduce_precision_shape_rule(operand,*,exponent_bits,mantissa_bits)
jax._src.lax.lax._reduce_prod(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_prod_jvp_rule(primals,tangents,*,axes)
jax._src.lax.lax._reduce_shape_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_sum(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_sum_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_sum_transpose_rule(cotangent,operand,*,axes)
jax._src.lax.lax._reduce_weak_type_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_xor(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reducer_padding(traceable,ident,in_avals,out_avals,operand,*,axes)
jax._src.lax.lax._reduction_jaxpr(computation,aval)
jax._src.lax.lax._replace_masked_values(x,val,padded_axes)
jax._src.lax.lax._reshape_batch_rule(batched_args,batch_dims,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_dtype_rule(operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_lower(ctx,x,*dyn_shape,new_sizes,dimensions)
jax._src.lax.lax._reshape_shape_rule(operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_staging_rule(trace,x,*dyn,new_sizes,dimensions)
jax._src.lax.lax._reshape_transpose_rule(t,operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_typecheck_rule(_,operand,*dyn_shape,new_sizes,dimensions)
jax._src.lax.lax._rev_batch_rule(batched_args,batch_dims,*,dimensions)
jax._src.lax.lax._rev_lower(ctx,x,*,dimensions)
jax._src.lax.lax._rev_shape_rule(operand,*,dimensions)
jax._src.lax.lax._rng_algorithm(algorithm:RandomAlgorithm)
jax._src.lax.lax._rng_bit_generator_dtype_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_lowering(ctx,key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_named_shape_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_shape_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_weak_type_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_uniform_abstract_eval(a,b,*,shape)
jax._src.lax.lax._rng_uniform_lowering(ctx,a,b,*,shape)
jax._src.lax.lax._round_lower(ctx,x,*,rounding_method)
jax._src.lax.lax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax._src.lax.lax._select_dtype_rule(which,*cases)
jax._src.lax.lax._select_hlo_lowering(ctx,which,*cases)
jax._src.lax.lax._select_hlo_lowering_opaque(ctx,which,*cases)
jax._src.lax.lax._select_jvp(primals,tangents)
jax._src.lax.lax._select_shape_rule(which,*cases)
jax._src.lax.lax._select_transpose_rule(t,which,*cases)
jax._src.lax.lax._select_weak_type_rule(which,*cases)
jax._src.lax.lax._sign_lower_hlo(ctx,x)
jax._src.lax.lax._sort_abstract_eval(*args,**kwargs)
jax._src.lax.lax._sort_batch_rule(batched_args,batch_dims,*,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_jvp(primals,tangents,*,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_le_comparator(*operands,num_keys=1)
jax._src.lax.lax._sort_lower(ctx,*operands,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_lt_comparator(*operands,num_keys=1)
jax._src.lax.lax._squeeze_batch_rule(batched_args,batch_dims,*,dimensions)
jax._src.lax.lax._squeeze_dtype_rule(operand,*,dimensions)
jax._src.lax.lax._squeeze_lower(ctx,operand,*,dimensions)
jax._src.lax.lax._squeeze_shape_rule(operand,*,dimensions)
jax._src.lax.lax._squeeze_transpose_rule(t,operand,*,dimensions)
jax._src.lax.lax._stop_gradient_batch_rule(batched_args,batch_dims)
jax._src.lax.lax._stop_gradient_jvp_rule(primals,tangents)
jax._src.lax.lax._sub_jvp(primals,tangents)
jax._src.lax.lax._sub_transpose(t,x,y)
jax._src.lax.lax._tan_impl(x)
jax._src.lax.lax._top_k_abstract_eval(operand,*,k)
jax._src.lax.lax._top_k_batch_rule(batched_args,batch_dims,*,k)
jax._src.lax.lax._top_k_jvp(primals,tangents,*,k)
jax._src.lax.lax._top_k_lower(ctx,operand,k)
jax._src.lax.lax._transpose_batch_rule(batched_args,batch_dims,*,permutation)
jax._src.lax.lax._transpose_lower(ctx,x,*,permutation)
jax._src.lax.lax._transpose_shape_rule(operand,*,permutation)
jax._src.lax.lax._tri(dtype:DTypeLike,shape:Shape,offset:int)->Array
jax._src.lax.lax._try_broadcast_shapes(shapes:Sequence[tuple[int,...]])->Optional[tuple[int, ...]]
jax._src.lax.lax._unary_reduce_lower(reducer,unit_factory,ctx,x,*,axes)
jax._src.lax.lax._unbroadcast(aval,x)
jax._src.lax.lax._upcast_fp16_for_computation(f)
jax._src.lax.lax._validate_preferred_element_type(input_dtype,preferred_element_type)
jax._src.lax.lax._validate_shapes(shapes:Sequence[Shape])
jax._src.lax.lax._variadic_reduction_jaxpr(computation,flat_avals,aval_tree)
jax._src.lax.lax._which_dim_sharded(s:PmapSharding)->Optional[int]
jax._src.lax.lax.abs(x:ArrayLike)->Array
jax._src.lax.lax.acos(x:ArrayLike)->Array
jax._src.lax.lax.acos_impl(x)
jax._src.lax.lax.acosh(x:ArrayLike)->Array
jax._src.lax.lax.add(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.after_all(*operands)
jax._src.lax.lax.argmax(operand:ArrayLike,axis:int,index_dtype:DTypeLike)->Array
jax._src.lax.lax.argmin(operand:ArrayLike,axis:int,index_dtype:DTypeLike)->Array
jax._src.lax.lax.asarray(x:ArrayLike)->Array
jax._src.lax.lax.asin(x:ArrayLike)->Array
jax._src.lax.lax.asin_impl(x)
jax._src.lax.lax.asinh(x:ArrayLike)->Array
jax._src.lax.lax.atan(x:ArrayLike)->Array
jax._src.lax.lax.atan2(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.atan_impl(x)
jax._src.lax.lax.atanh(x:ArrayLike)->Array
jax._src.lax.lax.batch_matmul(lhs:Array,rhs:Array,precision:PrecisionLike=None)->Array
jax._src.lax.lax.bitcast_convert_type(operand:ArrayLike,new_dtype:DTypeLike)->Array
jax._src.lax.lax.bitwise_and(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.bitwise_not(x:ArrayLike)->Array
jax._src.lax.lax.bitwise_or(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.bitwise_xor(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.broadcast(operand:ArrayLike,sizes:Sequence[int])->Array
jax._src.lax.lax.broadcast_hlo(aval_out:core.ShapedArray,avals:Sequence[core.ShapedArray],args:Sequence[ir.Value])->Sequence[ir.Value]
jax._src.lax.lax.broadcast_in_dim(operand:ArrayLike,shape:Shape,broadcast_dimensions:Sequence[int])->Array
jax._src.lax.lax.broadcast_shapes(*shapes)
jax._src.lax.lax.broadcast_to_rank(x:Array,rank:int)->Array
jax._src.lax.lax.broadcasted_iota(dtype:DTypeLike,shape:Shape,dimension:int)->Array
jax._src.lax.lax.broadcasting_shape_rule(name,*avals)
jax._src.lax.lax.canonicalize_precision(precision:PrecisionLike)->Optional[tuple[PrecisionType, PrecisionType]]
jax._src.lax.lax.cbrt(x:ArrayLike)->Array
jax._src.lax.lax.ceil(x:ArrayLike)->Array
jax._src.lax.lax.check_same_dtypes(name:str,*avals:core.UnshapedArray)->None
jax._src.lax.lax.clamp(min:ArrayLike,x:ArrayLike,max:ArrayLike)->Array
jax._src.lax.lax.clz(x:ArrayLike)->Array
jax._src.lax.lax.collapse(operand:Array,start_dimension:int,stop_dimension:Optional[int]=None)->Array
jax._src.lax.lax.complex(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.concatenate(operands:Union[Array,Sequence[ArrayLike]],dimension:int)->Array
jax._src.lax.lax.conj(x:ArrayLike)->Array
jax._src.lax.lax.convert_element_type(operand:ArrayLike,new_dtype:DTypeLike)->Array
jax._src.lax.lax.cos(x:ArrayLike)->Array
jax._src.lax.lax.cosh(x:ArrayLike)->Array
jax._src.lax.lax.create_token(_=None)
jax._src.lax.lax.div(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.dot(lhs:Array,rhs:Array,precision:PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.dot_general(lhs:ArrayLike,rhs:ArrayLike,dimension_numbers:DotDimensionNumbers,precision:PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.empty(dtype)
jax._src.lax.lax.eq(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.exp(x:ArrayLike)->Array
jax._src.lax.lax.exp2(x:ArrayLike)->Array
jax._src.lax.lax.expand_dims(array:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.expm1(x:ArrayLike)->Array
jax._src.lax.lax.floor(x:ArrayLike)->Array
jax._src.lax.lax.full(shape:Shape,fill_value:ArrayLike,dtype:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.full_like(x:Union[ArrayLike,DuckTypedArray],fill_value:ArrayLike,dtype:Optional[DTypeLike]=None,shape:Optional[Shape]=None)->Array
jax._src.lax.lax.ge(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.gt(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.imag(x:ArrayLike)->Array
jax._src.lax.lax.infeed(token,shape=None,partitions=None)
jax._src.lax.lax.integer_pow(x:ArrayLike,y:int)->Array
jax._src.lax.lax.iota(dtype:DTypeLike,size:int)->Array
jax._src.lax.lax.is_finite(x:ArrayLike)->Array
jax._src.lax.lax.le(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.log(x:ArrayLike)->Array
jax._src.lax.lax.log1p(x:ArrayLike)->Array
jax._src.lax.lax.logistic(x:ArrayLike)->Array
jax._src.lax.lax.logistic_impl(x)
jax._src.lax.lax.lt(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.max(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.min(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.mul(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.naryop(result_dtype,accepted_dtypes,name,allow_extended_dtype=False,require_same_dtypes=False)
jax._src.lax.lax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,require_same=True,allow_extended_dtype=False,**kwargs)
jax._src.lax.lax.ne(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.neg(x:ArrayLike)->Array
jax._src.lax.lax.nextafter(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.lax.lax.outfeed(token,xs,partitions=None)
jax._src.lax.lax.pad(operand:ArrayLike,padding_value:ArrayLike,padding_config:Sequence[tuple[int,int,int]])->Array
jax._src.lax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax._src.lax.lax.population_count(x:ArrayLike)->Array
jax._src.lax.lax.pow(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.precision_attr(precision:PrecisionType)->ir.ArrayAttr
jax._src.lax.lax.ranges_like(*xs)
jax._src.lax.lax.real(x:ArrayLike)->Array
jax._src.lax.lax.reciprocal(x:ArrayLike)->Array
jax._src.lax.lax.reduce(operands:Any,init_values:Any,computation:Callable[[Any,Any],Any],dimensions:Sequence[int])->Any
jax._src.lax.lax.reduce_precision(operand:Union[float,ArrayLike],exponent_bits:int,mantissa_bits:int)->Array
jax._src.lax.lax.rem(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.remaining(original,*removed_lists)
jax._src.lax.lax.reshape(operand:ArrayLike,new_sizes:Shape,dimensions:Optional[Sequence[int]]=None)->Array
jax._src.lax.lax.rev(operand:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.rng_bit_generator(key,shape,dtype=np.uint32,algorithm=RandomAlgorithm.RNG_DEFAULT)
jax._src.lax.lax.rng_uniform(a,b,shape)
jax._src.lax.lax.round(x:ArrayLike,rounding_method:RoundingMethod=RoundingMethod.AWAY_FROM_ZERO)->Array
jax._src.lax.lax.rsqrt(x:ArrayLike)->Array
jax._src.lax.lax.select(pred:ArrayLike,on_true:ArrayLike,on_false:ArrayLike)->Array
jax._src.lax.lax.select_n(which:ArrayLike,*cases:ArrayLike)->Array
jax._src.lax.lax.shape_as_value(shape:core.Shape)
jax._src.lax.lax.shift_left(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.shift_right_arithmetic(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.shift_right_logical(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.sign(x:ArrayLike)->Array
jax._src.lax.lax.sin(x:ArrayLike)->Array
jax._src.lax.lax.sinh(x:ArrayLike)->Array
jax._src.lax.lax.sort(operand:Union[Array,Sequence[Array]],dimension:int=-1,is_stable:bool=True,num_keys:int=1)->Union[Array, tuple[Array, ...]]
jax._src.lax.lax.sort_key_val(keys:Array,values:ArrayLike,dimension:int=-1,is_stable:bool=True)->tuple[Array, Array]
jax._src.lax.lax.sqrt(x:ArrayLike)->Array
jax._src.lax.lax.square(x:ArrayLike)->Array
jax._src.lax.lax.squeeze(array:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.stop_gradient(x:T)->T
jax._src.lax.lax.sub(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.tan(x:ArrayLike)->Array
jax._src.lax.lax.tanh(x:ArrayLike)->Array
jax._src.lax.lax.tie_in(x:Any,y:T)->T
jax._src.lax.lax.top_k(operand:ArrayLike,k:int)->tuple[Array, Array]
jax._src.lax.lax.transpose(operand:ArrayLike,permutation:Sequence[int]|np.ndarray)->Array
jax._src.lax.lax.tuple_delete(tup,idx)
jax._src.lax.lax.unop(result_dtype,accepted_dtypes,name)
jax._src.lax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax._src.lax.lax.zeros_like_array(x:ArrayLike)->Array
jax._src.lax.lax.zeros_like_shaped_array(aval:ShapedArray)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/loops.py----------------------------------------
A:jax._src.lax.control_flow.loops.T->TypeVar('T')
A:jax._src.lax.control_flow.loops.new_dtype->jax._src.dtypes.result_type(in_vals[i], out_avals[i])
A:jax._src.lax.control_flow.loops.in_vals[i]->jax._src.lax.lax.convert_element_type(in_vals[i], new_dtype)
A:jax._src.lax.control_flow.loops.Carry->TypeVar('Carry')
A:jax._src.lax.control_flow.loops.X->TypeVar('X')
A:jax._src.lax.control_flow.loops.Y->TypeVar('Y')
A:jax._src.lax.control_flow.loops.(xs_flat, xs_tree)->tree_flatten(xs)
A:jax._src.lax.control_flow.loops.length->int(length)
A:jax._src.lax.control_flow.loops.unique_lengths->set(lengths)
A:jax._src.lax.control_flow.loops.(carry, y)->split_list(out, [num_carry])
A:jax._src.lax.control_flow.loops.stacked_y->tree_map(stack, *maybe_reversed(ys))
A:jax._src.lax.control_flow.loops.(init_flat, init_tree)->tree_flatten(init)
A:jax._src.lax.control_flow.loops.(in_flat, in_tree)->tree_flatten((init, xs))
A:jax._src.lax.control_flow.loops.carry_avals->tuple(_map(_abstractify, init_flat))
A:jax._src.lax.control_flow.loops.(jaxpr, consts, out_tree)->_initial_style_jaxpr(f, in_tree, (*carry_avals, *x_avals), 'scan')
A:jax._src.lax.control_flow.loops.out_tree_children->out_tree.children()
A:jax._src.lax.control_flow.loops.(init_flat, carry_avals, carry_avals_out, init_tree, *rest)->_create_jaxpr(init)
A:jax._src.lax.control_flow.loops.(new_init_flat, changed)->_promote_weak_typed_inputs(init_flat, carry_avals, carry_avals_out)
A:jax._src.lax.control_flow.loops.init->tree_unflatten(init_tree, new_init_flat)
A:jax._src.lax.control_flow.loops.disallowed_effects->jax._src.effects.control_flow_allowed_effects.filter_not_in(joined_effects)
A:jax._src.lax.control_flow.loops.out->jax._src.core.AxisPrimitive('while').bind(*cond_consts, *remaining_body_consts, *refs, *carry, body_jaxpr=core.ClosedJaxpr(new_body_jaxpr, ()), cond_jaxpr=core.ClosedJaxpr(new_cond_jaxpr, ()), body_nconsts=num_remaining_consts, cond_nconsts=cond_nconsts)
A:jax._src.lax.control_flow.loops.sig->inspect.signature(body_fun)
A:jax._src.lax.control_flow.loops.(leaves_and_paths, in_carry_tree)->tree_flatten_with_path(in_carry)
A:jax._src.lax.control_flow.loops.(paths, in_carry_flat)->unzip2(leaves_and_paths)
A:jax._src.lax.control_flow.loops.in_avals->_map(_abstractify, in_carry_flat)
A:jax._src.lax.control_flow.loops.out_carry->tree_unflatten(out_carry_tree, out_avals)
A:jax._src.lax.control_flow.loops.differences->'\n'.join((f'  * {component(path)} has type {in_aval.str_short()} but the corresponding output carry component has type {out_aval.str_short()}{_aval_mismatch_extra(in_aval, out_aval)}\n' for (path, in_aval, out_aval) in zip(paths, in_avals, out_avals) if not core.typematch(in_aval, out_aval)))
A:jax._src.lax.control_flow.loops.(consts, init, xs)->split_list(args, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.x->_map(partial(_dynamic_index_array, i_), x_avals, xs)
A:jax._src.lax.control_flow.loops.ys->_map(_concatenate, y_avals, ys, ys_rem)
A:jax._src.lax.control_flow.loops.([i], carry, ys)->split_list(vals, [1, num_carry])
A:jax._src.lax.control_flow.loops.out_flat->jax._src.core.AxisPrimitive('scan').bind(*consts + consts_dot + init + init_dot + xs + xs_dot, reverse=reverse, length=length, jaxpr=jaxpr_jvp_rearranged, num_consts=num_consts + len(consts_dot), num_carry=num_carry + len(init_dot), linear=jaxpr_jvp_linear, unroll=unroll)
A:jax._src.lax.control_flow.loops.(carry_out, y_updates)->split_list(out_flat, [num_carry])
A:jax._src.lax.control_flow.loops.ys_out->_map(partial(_update_array, i_), y_avals, ys, y_updates)
A:jax._src.lax.control_flow.loops.ys_init->_map(partial(_empty_array, length), y_avals)
A:jax._src.lax.control_flow.loops.(_, *outs)->while_loop(cond_fun, body_fun, init_val)
A:jax._src.lax.control_flow.loops.(num_blocks, rem)->divmod(length, unroll)
A:jax._src.lax.control_flow.loops.partition->partial(_partition_leading, num_blocks, block_length)
A:jax._src.lax.control_flow.loops.xs_block->_map(partition, x_avals, xs)
A:jax._src.lax.control_flow.loops.prepend_aval->partial(_prepend_dim_to_aval, block_length)
A:jax._src.lax.control_flow.loops.x_block_avals->_map(prepend_aval, x_avals)
A:jax._src.lax.control_flow.loops.y_block_avals->_map(prepend_aval, y_avals)
A:jax._src.lax.control_flow.loops.f_impl_block->partial(_scan_impl_unrolled, reverse=reverse, length=block_length, num_consts=num_consts, num_carry=num_carry, linear=linear, f_impl=f_impl, x_avals=x_avals, y_avals=y_avals)
A:jax._src.lax.control_flow.loops.outs->jax._src.core.AxisPrimitive('while').bind(*cconsts + bconsts + new_init, cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr_batched, body_nconsts=body_nconsts, body_jaxpr=body_jaxpr_batched)
A:jax._src.lax.control_flow.loops.(carry, ys_blocks)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.combine->partial(_combine_leading, num_blocks, block_length)
A:jax._src.lax.control_flow.loops.(_, _, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(_, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.f_impl->jax._src.core.jaxpr_as_fun(jaxpr)
A:jax._src.lax.control_flow.loops.split->partial(_split_leading_dim, length_div)
A:jax._src.lax.control_flow.loops.(xs_rem, xs)->unzip2(_map(split, x_avals, xs))
A:jax._src.lax.control_flow.loops.(xs, xs_rem)->unzip2(_map(split, x_avals, xs))
A:jax._src.lax.control_flow.loops.(carry, ys)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.(carry, ys_rem)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.(carry_avals, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.ys_avals->_map(partial(_prepend_dim_to_aval, length), y_avals)
A:jax._src.lax.control_flow.loops.(const_nz, init_nz, xs_nz)->split_list(nonzeros, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_jvp, nonzeros_out)->jax._src.interpreters.ad.jvp_jaxpr(jaxpr, nonzeros, instantiate=carry_nz + [False] * num_ys)
A:jax._src.lax.control_flow.loops.carry_nz->_map(operator.or_, carry_nz, nonzeros_out)
A:jax._src.lax.control_flow.loops.all_tangents->split_list(tangents, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts_dot, init_dot, xs_dot)->_map(_prune_zeros, all_tangents)
A:jax._src.lax.control_flow.loops.jaxpr_jvp_rearranged->jax._src.interpreters.ad.rearrange_binders(jaxpr_jvp, [num_consts, num_carry, num_xs], [len(consts_dot), len(init_dot), len(xs_dot)], [num_carry, num_ys], [len(init_dot), sum(nonzeros_out) - len(init_dot)])
A:jax._src.lax.control_flow.loops.(consts_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.jaxpr_jvp_linear->tuple(consts_linear + [True] * len(consts_dot) + init_linear + [True] * len(init_dot) + xs_linear + [True] * len(xs_dot))
A:jax._src.lax.control_flow.loops.(carry, carry_dot, ys, ys_dot)->split_list(out_flat, [num_carry, len(init_dot), num_ys])
A:jax._src.lax.control_flow.loops.tangents_out_iter->iter(carry_dot + ys_dot)
A:jax._src.lax.control_flow.loops.(const_uk, init_uk, xs_uk)->split_list(unknowns, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_known, jaxpr_unknown, out_uk, res_avals)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr, unknowns, instantiate=carry_uk + [False] * num_ys)
A:jax._src.lax.control_flow.loops.(carry_uk_out, ys_uk)->split_list(unks_out, [num_carry])
A:jax._src.lax.control_flow.loops.carry_uk->_map(operator.or_, carry_uk, carry_uk_out)
A:jax._src.lax.control_flow.loops.num_res->len(res_avals)
A:jax._src.lax.control_flow.loops.jaxpr_unknown->jax._src.interpreters.partial_eval.move_binders_to_front(jaxpr_unknown, [False] * sum(unknowns) + [pval.is_known() for pval in res_pvals])
A:jax._src.lax.control_flow.loops.(jaxpr_known_, invar_pvals_out, jaxpr_known_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_nounits(lu.wrap_init(core.jaxpr_as_fun(jaxpr_known)), const_pvals + other_pvals, instantiate=[True] * (len(out_uk) - sum(out_uk)) + [False] * num_res)
A:jax._src.lax.control_flow.loops.jaxpr_known->jax._src.core.ClosedJaxpr(jaxpr_known_, jaxpr.consts)
A:jax._src.lax.control_flow.loops.fwds_known->jax._src.interpreters.partial_eval._jaxpr_forwarding(jaxpr_known.jaxpr)
A:jax._src.lax.control_flow.loops.jaxpr_known_->jaxpr_known_.replace(outvars=[x for (x, i) in zip(jaxpr_known_.outvars, fwds_known) if i is None]).replace(outvars=[x for (x, i) in zip(jaxpr_known_.outvars, fwds_known) if i is None])
A:jax._src.lax.control_flow.loops.out_known->jax._src.core.AxisPrimitive('while').bind(*in_consts, cond_nconsts=cond_nconsts_known, cond_jaxpr=cond_jaxpr_known, body_nconsts=body_nconsts_known, body_jaxpr=body_jaxpr_known)
A:jax._src.lax.control_flow.loops.out_known_iter->iter(out_known)
A:jax._src.lax.control_flow.loops.(out_known, extensive_res)->split_list(out_known, [len(out_uk) - sum(out_uk)])
A:jax._src.lax.control_flow.loops.intensive_res->_map(newvar, intensive_avals)
A:jax._src.lax.control_flow.loops.extensive_res->_map(newvar, ext_avals)
A:jax._src.lax.control_flow.loops.linear_unknown->tuple([False] * len(intensive_res) + [l for (l, uk) in zip(linear, unknowns) if uk] + [False] * len(extensive_res))
A:jax._src.lax.control_flow.loops.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.loops.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res], out_tracers, scan_p, dict(reverse=reverse, length=length, unroll=unroll, jaxpr=jaxpr_unknown, linear=linear_unknown, num_consts=len(intensive_res) + sum(const_uk), num_carry=sum(carry_uk)), jaxpr_unknown.effects, source)
A:jax._src.lax.control_flow.loops.(consts_lin, init_lin, xs_lin)->split_list(linear, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts, _, xs)->split_list(args, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(ires, _)->split_list(consts, [num_ires])
A:jax._src.lax.control_flow.loops.(_, eres)->split_list(xs, [sum(xs_lin)])
A:jax._src.lax.control_flow.loops.(ct_carry, ct_ys)->split_list(cts, [num_carry])
A:jax._src.lax.control_flow.loops.ct_carry->_map(ad.instantiate_zeros_aval, carry_avals, ct_carry)
A:jax._src.lax.control_flow.loops.ct_consts->_map(ad_util.zeros_like_aval, jaxpr.in_avals[num_ires:num_consts])
A:jax._src.lax.control_flow.loops.jaxpr_trans->_transpose_scan_jaxpr(num_ires, num_consts - num_ires, num_eres, jaxpr, reduce_axes, ct_ys_is_zeros)
A:jax._src.lax.control_flow.loops.(ct_consts, ct_init, ct_xs)->split_list(outs, [num_consts - num_ires, num_carry])
A:jax._src.lax.control_flow.loops.(res1_avals, c_avals, a_avals, res2_avals)->split_list(jaxpr.in_avals, [num_res1, num_c, num_a])
A:jax._src.lax.control_flow.loops.num_ys->len(ct_ys_is_zeros)
A:jax._src.lax.control_flow.loops.(b_carry_avals, b_ys_avals)->split_list(list(jaxpr.out_avals), [num_b])
A:jax._src.lax.control_flow.loops.(res1, c_bar, b_bar, ys_bar_stripped, res2)->split_list(res1_cbar_bbar_res2, [num_res1, num_c, num_b, len(b_ys_avals_stripped)])
A:jax._src.lax.control_flow.loops.ys_bar_stripped_iter->iter(ys_bar_stripped)
A:jax._src.lax.control_flow.loops.cbar_abar->jax._src.interpreters.ad.backward_pass(jaxpr.jaxpr, reduce_axes, False, jaxpr.consts, primals, b_bar + ys_bar)
A:jax._src.lax.control_flow.loops.(_, new_c_bar, a_bar, _)->split_list(cbar_abar, [num_res1, num_c, num_a])
A:jax._src.lax.control_flow.loops.a_bar->_map(ad.instantiate_zeros_aval, a_avals, a_bar)
A:jax._src.lax.control_flow.loops.c_bar->_map(ad.instantiate_zeros_aval, c_avals, _map(ad.add_tangents, c_bar, new_c_bar))
A:jax._src.lax.control_flow.loops.(const_batched, init_batched, xs_batched)->split_list(orig_batched, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_batched, batched_out)->jax._src.interpreters.batching.batch_jaxpr(jaxpr, axis_size, batched, instantiate=carry_batched + [False] * num_ys, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.carry_batched->_map(operator.or_, carry_batched, carry_batched_out)
A:jax._src.lax.control_flow.loops.(consts_bdims, init_bdims, xs_bdims)->split_list(dims, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.padded_jaxpr->jax._src.core.ClosedJaxpr(*pe.pad_jaxpr(jaxpr.jaxpr, jaxpr.consts))
A:jax._src.lax.control_flow.loops.(used_carry_out, used_extensive_out)->split_list(used_outputs, [num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_dce, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr.jaxpr, used_outputs, instantiate=[False] * num_consts + used_carry_out + [False] * num_xs)
A:jax._src.lax.control_flow.loops.(used_consts, used_carry_in, used_extensive_in)->split_list(used_inputs, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.used_carry_out->_map(operator.or_, used_carry_out, used_carry_in)
A:jax._src.lax.control_flow.loops.new_params->dict(eqn.params, num_consts=sum(used_consts), num_carry=sum(used_carry_in), linear=tuple(new_linear), jaxpr=core.ClosedJaxpr(jaxpr_dce, jaxpr.consts))
A:jax._src.lax.control_flow.loops.(_, new_effects)->jax._src.interpreters.partial_eval.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res], out_tracers, scan_p, dict(reverse=reverse, length=length, unroll=unroll, jaxpr=jaxpr_unknown, linear=linear_unknown, num_consts=len(intensive_res) + sum(const_uk), num_carry=sum(carry_uk)), jaxpr_unknown.effects, source).primitive.abstract_eval(*[v.aval for v in new_invars], **new_params)
A:jax._src.lax.control_flow.loops.new_eqn->jax._src.interpreters.partial_eval.new_jaxpr_eqn(new_invars, new_outvars, eqn.primitive, new_params, new_effects, eqn.source_info)
A:jax._src.lax.control_flow.loops.(const_uk, carry_uk, xs_uk)->split_list(unks_in, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_known_, jaxpr_staged_, unks_out, inst_out, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=unks_in, in_inst=True, ensure_out_unknowns=carry_uk + [False] * num_ys, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.loops.jaxpr_staged->jax._src.interpreters.partial_eval.move_binders_to_front(jaxpr_staged, [False] * sum(inst_in) + _map(operator.not_, loop_dep_res))
A:jax._src.lax.control_flow.loops.(jaxpr_known_hoist, jaxpr_known_loop, loop_dep, consts_known_lp_avals)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr_known, [False] * num_const_known + [True] * (num_carry_known + num_xs_known), [True] * (len(unks_out) - sum(unks_out)) + [False] * num_res)
A:jax._src.lax.control_flow.loops.(_, loop_dep_res)->split_list(loop_dep, [len(loop_dep) - num_res])
A:jax._src.lax.control_flow.loops.(intensive_avals, ext_avals_mapped)->partition_list(loop_dep_res, res_avals)
A:jax._src.lax.control_flow.loops.newvar->jax._src.core.gensym([cond_jaxpr.jaxpr])
A:jax._src.lax.control_flow.loops.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax._src.lax.control_flow.loops.(out_binders_known, _)->partition_list(carry_uk, eqn.outvars)
A:jax._src.lax.control_flow.loops.(_, linear_known_)->split_list(linear_known_, [num_const_known])
A:jax._src.lax.control_flow.loops.params_known->dict(cond_jaxpr=cond_jaxpr_known, body_jaxpr=body_jaxpr_known, cond_nconsts=len(cond_consts_uk) - sum(cond_consts_uk), body_nconsts=len(body_consts_uk) - sum(body_consts_uk))
A:jax._src.lax.control_flow.loops.(consts_known_hoist, ins_known_lp)->split_list(ins_known, [num_const_known])
A:jax._src.lax.control_flow.loops.out_hoist->jax._src.core.jaxpr_as_fun(jaxpr_known_hoist)(*consts_known_hoist)
A:jax._src.lax.control_flow.loops.(intensive_res, consts_known_lp)->split_list(out_hoist, [num_intensive_res])
A:jax._src.lax.control_flow.loops.out_loop->jax._src.core.AxisPrimitive('scan').bind(*consts_known_lp, *ins_known_lp, **params_known)
A:jax._src.lax.control_flow.loops.(call_jaxpr_, _, call_jaxpr_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(known, [v.aval for v in ins_known])
A:jax._src.lax.control_flow.loops.call_jaxpr->jax._src.core.ClosedJaxpr(call_jaxpr_, call_jaxpr_consts)
A:jax._src.lax.control_flow.loops.eqn_known->jax._src.interpreters.partial_eval.new_jaxpr_eqn(ins_known, out_binders_known, while_p, params_known, effects_known, eqn.source_info)
A:jax._src.lax.control_flow.loops.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax._src.lax.control_flow.loops.params_staged->dict(eqn.params, jaxpr=jaxpr_staged, num_consts=len(intensive_res) + eqn.params['num_consts'], linear=tuple(linear_staged))
A:jax._src.lax.control_flow.loops.eqn_staged->jax._src.interpreters.partial_eval.new_jaxpr_eqn([*intensive_res, *eqn.invars, *extensive_res], out_binders_staged, eqn.primitive, params_staged, jaxpr_staged.effects, eqn.source_info)
A:jax._src.lax.control_flow.loops.tc->partial(_typecheck_param, 'scan')
A:jax._src.lax.control_flow.loops.(const_avals, init_avals, x_avals)->split_list(avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(const_avals_jaxpr, init_avals_jaxpr, x_avals_jaxpr)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(carry_avals_jaxpr, y_avals_mapped)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.x_avals_mapped->_map(partial(core.mapped_aval, length, 0), x_avals)
A:jax._src.lax.control_flow.loops.printed_params->dict(eqn.params)
A:jax._src.lax.control_flow.loops.(consts, carry, xs)->split_list(args, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts_linear, carry_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts_avals, carry_avals, xs_avals)->split_list(in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(remaining_const_avals, in_ref_avals)->partition_list(is_ref, consts_avals)
A:jax._src.lax.control_flow.loops.(remaining_consts, in_refs)->partition_list(is_ref, consts)
A:jax._src.lax.control_flow.loops.(remaining_consts_linear, in_refs_linear)->partition_list(is_ref, consts_linear)
A:jax._src.lax.control_flow.loops.num_refs->sum(is_ref)
A:jax._src.lax.control_flow.loops.(discharged_jaxpr, discharged_consts)->jax._src.state.discharge.discharge_state(jaxpr, ())
A:jax._src.lax.control_flow.loops.(consts, refs, carry, xs)->split_list(refs_and_args, [num_remaining_consts, num_refs, num_carry])
A:jax._src.lax.control_flow.loops.consts_with_refs->merge_lists(is_ref, consts, refs)
A:jax._src.lax.control_flow.loops.outs_and_refs->jax._src.core.eval_jaxpr(discharged_jaxpr, (), *consts_with_refs, *carry, *xs)
A:jax._src.lax.control_flow.loops.(carry, ys, out_refs)->split_list(outs_and_refs, [num_carry, num_extensive_out])
A:jax._src.lax.control_flow.loops.(new_jaxpr, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(wrapped), new_in_avals)
A:jax._src.lax.control_flow.loops.all_out->jax._src.core.AxisPrimitive('scan').bind(*remaining_consts, *in_refs, *carry, *xs, jaxpr=core.ClosedJaxpr(new_jaxpr, ()), length=length, num_consts=num_remaining_consts, num_carry=num_refs + num_carry, unroll=unroll, reverse=reverse, linear=new_linear)
A:jax._src.lax.control_flow.loops.(refs_out, carry_out, ys_out)->split_list(all_out, [num_refs, num_carry])
A:jax._src.lax.control_flow.loops.avals->_map(core.get_aval, args)
A:jax._src.lax.control_flow.loops.scan_p->jax._src.core.AxisPrimitive('scan')
A:jax._src.lax.control_flow.loops.batching.axis_primitive_batchers[scan_p]->partial(_scan_batching_rule, None)
A:jax._src.lax.control_flow.loops.core.custom_typechecks[scan_p]->partial(_scan_typecheck, False)
A:jax._src.lax.control_flow.loops.val->body_fun(val)
A:jax._src.lax.control_flow.loops.(init_vals, in_tree)->tree_flatten((init_val,))
A:jax._src.lax.control_flow.loops.init_avals->tuple(_map(_abstractify, init_vals))
A:jax._src.lax.control_flow.loops.(cond_jaxpr, cond_consts, cond_tree)->_initial_style_jaxpr(cond_fun, in_tree, init_avals, 'while_cond')
A:jax._src.lax.control_flow.loops.(body_jaxpr, body_consts, body_tree)->_initial_style_jaxpr(body_fun, in_tree, init_avals, 'while_loop')
A:jax._src.lax.control_flow.loops.(init_vals, init_avals, body_jaxpr, in_tree, *rest)->_create_jaxpr(new_init_val)
A:jax._src.lax.control_flow.loops.(new_init_vals, changed)->_promote_weak_typed_inputs(init_vals, init_avals, body_jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.(new_init_val,)->tree_unflatten(in_tree, new_init_vals)
A:jax._src.lax.control_flow.loops.in_tree_children->in_tree.children()
A:jax._src.lax.control_flow.loops.joined_effects->_join_while_effects(body_jaxpr, cond_jaxpr, body_nconsts, cond_nconsts)
A:jax._src.lax.control_flow.loops.eff->eff.replace(input_index=index).replace(input_index=index)
A:jax._src.lax.control_flow.loops.(cconst_bat, bconst_bat, init_bat)->split_list(orig_batched, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cconsts, bconsts, init)->split_list(args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cconst_dims, bconst_dims, init_dims)->split_list(dims, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(_, carry_bat_out)->jax._src.interpreters.batching.batch_jaxpr(body_jaxpr, axis_size, bconst_bat + carry_bat, instantiate=carry_bat, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.carry_bat->safe_map(operator.or_, carry_bat, carry_bat_out)
A:jax._src.lax.control_flow.loops.(_, (pred_bat,))->jax._src.interpreters.batching.batch_jaxpr(cond_jaxpr, axis_size, cconst_bat + carry_bat, instantiate=False, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.(body_jaxpr_batched, _)->jax._src.interpreters.batching.batch_jaxpr_axes(body_jaxpr, axis_size, bconst_dims + carry_dims, carry_dims, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_batched, _)->jax._src.interpreters.batching.batch_jaxpr_axes(cond_jaxpr, axis_size, cconst_dims + carry_dims, (None,), axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.cond_rank->len(cond_jaxpr.out_avals[0].shape)
A:jax._src.lax.control_flow.loops.(cconst_nz, bconst_nz, init_nz)->split_list(nonzeros, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(body_jvp, nonzeros_out)->jax._src.interpreters.ad.jvp_jaxpr(body_jaxpr, body_nonzeros, instantiate=carry_nz)
A:jax._src.lax.control_flow.loops.(cconst, bconst, init)->split_list(primals, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(_, bconst_dot, init_dot)->split_list(tangents, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.bconst_dot->_prune_zeros(bconst_dot)
A:jax._src.lax.control_flow.loops.init_dot->_prune_zeros(init_dot)
A:jax._src.lax.control_flow.loops.body_jvp_rearranged->jax._src.interpreters.ad.rearrange_binders(body_jvp, [body_nconsts, num_carry], [len(bconst_dot), len(init_dot)], [num_carry], [len(init_dot)])
A:jax._src.lax.control_flow.loops.cond_jaxpr_augmented->jax._src.core.ClosedJaxpr(cond_jaxpr_augmented, cond_jaxpr.consts)
A:jax._src.lax.control_flow.loops.(out_carry, out_carry_dot)->split_list(out, [num_carry])
A:jax._src.lax.control_flow.loops.out_tangents_iter->iter(out_carry_dot)
A:jax._src.lax.control_flow.loops.params->dict(cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr, body_nconsts=body_nconsts, body_jaxpr=body_jaxpr)
A:jax._src.lax.control_flow.loops.(cond_consts_uk, body_consts_uk, carry_init_uk)->split_list(unks_in, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(body_jaxpr_known, _, carry_out_uk, body_res_avals)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(body_jaxpr, body_consts_uk + carry_uk, instantiate=carry_uk)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_known, _, cond_uk, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(cond_jaxpr, cond_consts_uk + carry_uk, instantiate=False)
A:jax._src.lax.control_flow.loops.body_jaxpr_known->jax._src.core.ClosedJaxpr(jaxpr_known_, body_jaxpr.consts)
A:jax._src.lax.control_flow.loops.out_tracers_->trace.default_process_primitive(while_p, tracers, params)
A:jax._src.lax.control_flow.loops.(jaxpr_known_, _, carry_uk_out, _, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(body_jaxpr.jaxpr, in_unknowns=body_unks_in, in_inst=True, ensure_out_unknowns=carry_uk, ensure_out_inst=True, saveable=ad_checkpoint.nothing_saveable)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_known_, _, [cond_uk], _, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(cond_jaxpr.jaxpr, cond_unks_in, in_inst=True, ensure_out_unknowns=False, ensure_out_inst=True, saveable=ad_checkpoint.nothing_saveable)
A:jax._src.lax.control_flow.loops.cond_jaxpr_known->jax._src.core.ClosedJaxpr(cond_jaxpr_known_, cond_jaxpr.consts)
A:jax._src.lax.control_flow.loops.effects_known->jax._src.core.join_effects(cond_jaxpr_known.effects, body_jaxpr_known.effects)
A:jax._src.lax.control_flow.loops.batched->bool(pred_aval.shape)
A:jax._src.lax.control_flow.loops.cond_ordered_effects->jax._src.effects.ordered_effects.filter_in(cond_jaxpr.effects)
A:jax._src.lax.control_flow.loops.pred->cond(args)
A:jax._src.lax.control_flow.loops.args->body(args)
A:jax._src.lax.control_flow.loops.(_, out)->while_loop(new_cond, new_body, (pred, args))
A:jax._src.lax.control_flow.loops.loop_carry_types->_map(mlir.aval_to_ir_types, ctx.avals_in)
A:jax._src.lax.control_flow.loops.body_effects->jax._src.effects.ordered_effects.filter_in(body_jaxpr.effects)
A:jax._src.lax.control_flow.loops.num_tokens->len(body_effects)
A:jax._src.lax.control_flow.loops.flat_loop_carry_types->jax._src.util.flatten(loop_carry_types)
A:jax._src.lax.control_flow.loops.flat_args->jax._src.interpreters.mlir.flatten_lowering_ir_args(args)
A:jax._src.lax.control_flow.loops.while_op->jax._src.lib.mlir.dialects.hlo.WhileOp(flat_loop_carry_types, flat_args)
A:jax._src.lax.control_flow.loops.cond_block->jax._src.lib.mlir.dialects.hlo.WhileOp(flat_loop_carry_types, flat_args).regions[0].blocks.append(*flat_loop_carry_types)
A:jax._src.lax.control_flow.loops.name_stack->ctx.module_context.name_stack.extend('while')
A:jax._src.lax.control_flow.loops.cond_args->jax._src.util.unflatten(flat_cond_args, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(x, _, z)->jax._src.util.split_list(cond_args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.cond_ctx->ctx.module_context.replace(name_stack=name_stack.extend('cond'))
A:jax._src.lax.control_flow.loops.(((pred,),), _)->jax._src.interpreters.mlir.jaxpr_subcomp(cond_ctx, cond_jaxpr.jaxpr, mlir.TokenSet(), cond_consts, *x + z, dim_var_values=ctx.dim_var_values)
A:jax._src.lax.control_flow.loops.pred_ctx->jax._src.interpreters.mlir.LoweringRuleContext(module_context=ctx.module_context, primitive=None, avals_in=[pred_aval], avals_out=[pred_aval.update(shape=())], tokens_in=mlir.TokenSet(), tokens_out=None)
A:jax._src.lax.control_flow.loops.(pred,)->jax._src.lax.lax._unary_reduce_lower(hlo.OrOp, lambda dtype: np.array(False, dtype), pred_ctx, pred, axes=tuple(range(len(pred_aval.shape))))
A:jax._src.lax.control_flow.loops.body_block->jax._src.lib.mlir.dialects.hlo.WhileOp(flat_loop_carry_types, flat_args).regions[1].blocks.append(*flat_loop_carry_types)
A:jax._src.lax.control_flow.loops.body_args->jax._src.util.unflatten(flat_body_args, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(token_args, body_args)->jax._src.util.split_list(body_args, [num_tokens])
A:jax._src.lax.control_flow.loops.tokens_in->jax._src.interpreters.mlir.TokenSet(zip(body_effects, token_args))
A:jax._src.lax.control_flow.loops.(x, y, z)->jax._src.util.split_list(body_args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.body_ctx->ctx.module_context.replace(name_stack=name_stack.extend('body'))
A:jax._src.lax.control_flow.loops.(new_z, tokens_out)->jax._src.interpreters.mlir.jaxpr_subcomp(body_ctx, body_jaxpr.jaxpr, tokens_in, body_consts, *y + z, dim_var_values=ctx.dim_var_values)
A:jax._src.lax.control_flow.loops.body_pred_ctx->ctx.module_context.replace(name_stack=name_stack.extend('body_pred'))
A:jax._src.lax.control_flow.loops.(((body_pred,),), _)->jax._src.interpreters.mlir.jaxpr_subcomp(body_pred_ctx, cond_jaxpr.jaxpr, mlir.TokenSet(), cond_consts, *x + z, dim_var_values=ctx.dim_var_values)
A:jax._src.lax.control_flow.loops.new_z->_map(partial(_pred_bcast_select_hlo, ctx, pred_aval, body_pred), new_z, z, body_jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.outputs->jax._src.util.unflatten(while_op.results, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(tokens, _, _, z)->jax._src.util.split_list(outputs, [num_tokens, cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cond_consts, body_consts, carry)->split_list(args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cond_consts_avals, body_consts_avals, carry_avals)->split_list(in_avals, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(remaining_body_consts, refs)->partition_list(is_ref, body_consts)
A:jax._src.lax.control_flow.loops.(remaining_body_const_avals, ref_avals)->partition_list(is_ref, body_consts_avals)
A:jax._src.lax.control_flow.loops.(discharged_body_jaxpr, discharged_consts)->jax._src.state.discharge.discharge_state(body_jaxpr, ())
A:jax._src.lax.control_flow.loops.(consts, refs, carry)->split_list(consts_refs_carry, [cond_nconsts, num_refs])
A:jax._src.lax.control_flow.loops.consts_and_refs->merge_lists(is_ref, consts, refs)
A:jax._src.lax.control_flow.loops.carry_refs->jax._src.core.eval_jaxpr(discharged_body_jaxpr, (), *consts_and_refs, *carry)
A:jax._src.lax.control_flow.loops.(carry, refs_out)->split_list(carry_refs, [num_carry])
A:jax._src.lax.control_flow.loops.(new_body_jaxpr, _, new_body_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(new_body), [*remaining_body_const_avals, *[a.inner_aval for a in ref_avals], *carry_avals])
A:jax._src.lax.control_flow.loops.(new_cond_jaxpr, _, new_cond_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(new_cond), [*cond_consts_avals, *[a.inner_aval for a in ref_avals], *carry_avals])
A:jax._src.lax.control_flow.loops.(refs_out, carry_out)->split_list(out, [num_refs])
A:jax._src.lax.control_flow.loops.updated_body_consts->merge_lists(is_ref, [None] * num_remaining_consts, refs_out)
A:jax._src.lax.control_flow.loops.while_p->jax._src.core.AxisPrimitive('while')
A:jax._src.lax.control_flow.loops.batching.axis_primitive_batchers[while_p]->partial(_while_loop_batching_rule, None)
A:jax._src.lax.control_flow.loops.x_y_aval->jax._src.core.physical_aval(x_y_aval)
A:jax._src.lax.control_flow.loops.bcast_pred->jax._src.interpreters.mlir.broadcast_in_dim(ctx, pred, core.DShapedArray(x_y_aval.shape, np.dtype(np.bool_)), broadcast_dimensions=list(range(len(pred_aval.shape))))
A:jax._src.lax.control_flow.loops.body_fun->weakref.ref(body_fun)
A:jax._src.lax.control_flow.loops.lower_dtype->jax._src.dtypes.canonicalize_dtype(lax.dtype(lower))
A:jax._src.lax.control_flow.loops.upper_dtype->jax._src.dtypes.canonicalize_dtype(lax.dtype(upper))
A:jax._src.lax.control_flow.loops.lower_weak->jax._src.dtypes.is_weakly_typed(lower)
A:jax._src.lax.control_flow.loops.upper_weak->jax._src.dtypes.is_weakly_typed(upper)
A:jax._src.lax.control_flow.loops.lower_->int(lower)
A:jax._src.lax.control_flow.loops.upper_->int(upper)
A:jax._src.lax.control_flow.loops.((_, result), _)->scan(_fori_scan_body_fun(body_fun), (lower_, init_val), None, length=upper_ - lower_)
A:jax._src.lax.control_flow.loops.lower->jax._src.lax.lax.convert_element_type(lower, dtype)
A:jax._src.lax.control_flow.loops.upper->jax._src.lax.lax.convert_element_type(upper, dtype)
A:jax._src.lax.control_flow.loops.(_, _, result)->while_loop(_fori_cond_fun, _fori_body_fun(body_fun), (lower, upper, init_val))
A:jax._src.lax.control_flow.loops.(_, ys)->scan(g, (), xs)
A:jax._src.lax.control_flow.loops.key->jax._src.interpreters.batching.moveaxis(key, bd, 0)
A:jax._src.lax.control_flow.loops.(stacked_keys, stacked_bits)->map(map_body, key)
A:jax._src.lax.control_flow.loops.(elems_flat, tree)->tree_flatten(elems)
A:jax._src.lax.control_flow.loops.a->tree_unflatten(tree, a_flat)
A:jax._src.lax.control_flow.loops.b->tree_unflatten(tree, b_flat)
A:jax._src.lax.control_flow.loops.c->fn(a, b)
A:jax._src.lax.control_flow.loops.(c_flat, _)->tree_flatten(c)
A:jax._src.lax.control_flow.loops.axis->jax._src.util.canonicalize_axis(axis, elems_flat[0].ndim)
A:jax._src.lax.control_flow.loops.num_elems->int(elems_flat[0].shape[axis])
A:jax._src.lax.control_flow.loops.reduced_elems->combine([slicing.slice_in_dim(elem, 0, -1, stride=2, axis=axis) for elem in elems], [slicing.slice_in_dim(elem, 1, None, stride=2, axis=axis) for elem in elems])
A:jax._src.lax.control_flow.loops.odd_elems->_scan(reduced_elems)
A:jax._src.lax.control_flow.loops.even_elems->combine(odd_elems, [slicing.slice_in_dim(e, 2, None, stride=2, axis=axis) for e in elems])
A:jax._src.lax.control_flow.loops.scans->_scan(elems_flat)
A:jax._src.lax.control_flow.loops.reducer_p->jax._src.lax.lax.standard_primitive(_cumred_shape_rule, partial(_cumred_dtype_rule, name), name)
A:jax._src.lax.control_flow.loops.batching.primitive_batchers[reducer_p]->partial(_cumred_batch_rule, reducer_p)
A:jax._src.lax.control_flow.loops.cumsum_p->_cumulative_reduction_primitive('cumsum', lax.add, windowed_reductions._reduce_window_sum)
A:jax._src.lax.control_flow.loops.cumlogsumexp_p->_cumulative_reduction_primitive('cumlogsumexp', logaddexp, windowed_reductions._reduce_window_logaddexp)
A:jax._src.lax.control_flow.loops.cumprod_p->_cumulative_reduction_primitive('cumprod', lax.mul, windowed_reductions._reduce_window_prod)
A:jax._src.lax.control_flow.loops.cummax_p->_cumulative_reduction_primitive('cummax', lax.max, windowed_reductions._reduce_window_max)
A:jax._src.lax.control_flow.loops.cummin_p->_cumulative_reduction_primitive('cummin', lax.min, windowed_reductions._reduce_window_min)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cumlogsumexp_p]->partial(_cumulative_jvp_rule, combine_fn=logaddexp)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cumprod_p]->partial(_cumulative_jvp_rule, combine_fn=lax.mul)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cummin_p]->partial(_cumulative_jvp_rule, combine_fn=lax.min)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cummax_p]->partial(_cumulative_jvp_rule, combine_fn=lax.max)
jax._src.lax.control_flow._scan_impl(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow._scan_impl_block_unrolled(*args,reverse,length,num_consts,num_carry,linear,block_length,f_impl,x_avals,y_avals)
jax._src.lax.control_flow._scan_impl_loop(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow._scan_impl_unrolled(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.associative_scan(fn:Callable,elems,reverse:bool=False,axis:int=0)
jax._src.lax.control_flow.cumlogsumexp(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cummax(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cummin(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cumprod(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cumred_reduce_window_impl(window_reduce:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.cumsum(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.fori_loop(lower,upper,body_fun,init_val)
jax._src.lax.control_flow.loops._aval_mismatch_extra(a1:core.AbstractValue,a2:core.AbstractValue)->str
jax._src.lax.control_flow.loops._check_scan_carry_type(body_fun,in_carry,out_carry_tree,out_avals)
jax._src.lax.control_flow.loops._combine_leading(sz0,sz1,aval,x)
jax._src.lax.control_flow.loops._concatenate(aval,x1,x2)
jax._src.lax.control_flow.loops._cumred_batch_rule(prim,batched_args,batch_dims,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumred_dtype_rule(name,operand,*args,**kw)
jax._src.lax.control_flow.loops._cumred_shape_rule(x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumsum_transpose_rule(t,operand,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumulative_jvp_rule(primals,tangents,*,axis:int,reverse:bool,combine_fn:Callable)
jax._src.lax.control_flow.loops._cumulative_reduction_primitive(name,reduce_fn,reduce_window_fn)
jax._src.lax.control_flow.loops._dynamic_index_array(i,aval,x)
jax._src.lax.control_flow.loops._empty_array(sz,aval)
jax._src.lax.control_flow.loops._fori_body_fun(body_fun)
jax._src.lax.control_flow.loops._fori_cond_fun(loop_carry)
jax._src.lax.control_flow.loops._fori_scan_body_fun(body_fun)
jax._src.lax.control_flow.loops._index_array(i,aval,x)
jax._src.lax.control_flow.loops._interleave(a,b,axis)
jax._src.lax.control_flow.loops._join_while_effects(body_jaxpr,cond_jaxpr,body_nconsts,cond_nconsts)->effects.Effects
jax._src.lax.control_flow.loops._maybe_put(x)
jax._src.lax.control_flow.loops._partition_leading(sz0,sz1,aval,x)
jax._src.lax.control_flow.loops._pred_bcast_select_hlo(ctx,pred_aval:core.ShapedArray,pred:ir.Value,xs:Sequence[ir.Value],ys:Sequence[ir.Value],x_y_aval:core.AbstractValue)->Sequence[ir.Value]
jax._src.lax.control_flow.loops._prepend_dim_to_aval(sz,aval)
jax._src.lax.control_flow.loops._promote_weak_typed_inputs(in_vals,in_avals,out_avals)
jax._src.lax.control_flow.loops._rng_bit_generator_batching_rule(batched_args,batch_dims,*,shape,dtype,algorithm)
jax._src.lax.control_flow.loops._scan_abstract_eval(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_batching_rule(spmd_axis_name,axis_size,axis_name,main_type,args,dims,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.lax.control_flow.loops._scan_dce_rule(used_outputs:list[bool],eqn:core.JaxprEqn)->tuple[list[bool], core.JaxprEqn]
jax._src.lax.control_flow.loops._scan_impl(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_impl_block_unrolled(*args,reverse,length,num_consts,num_carry,linear,block_length,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_impl_loop(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_impl_unrolled(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_jvp(primals,tangents,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.lax.control_flow.loops._scan_padding_rule(in_avals,out_avals,*args,jaxpr,**params)
jax._src.lax.control_flow.loops._scan_partial_eval(trace,*tracers,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.loops._scan_pp_rule(eqn,context,settings)
jax._src.lax.control_flow.loops._scan_state_discharge_rule(in_avals,out_avals,*args,jaxpr,num_consts,num_carry,linear,unroll,reverse,length)
jax._src.lax.control_flow.loops._scan_transpose(reduce_axes,cts,*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_typecheck(bind_time,*in_atoms,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._split_leading_dim(i,aval,x)
jax._src.lax.control_flow.loops._stack(aval,vals)
jax._src.lax.control_flow.loops._transpose_scan_jaxpr(num_res1,num_c,num_res2,jaxpr,reduce_axes,ct_ys_is_zeros)
jax._src.lax.control_flow.loops._update_array(i,aval,xs,x)
jax._src.lax.control_flow.loops._while_discharge_rule(in_avals,out_avals,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax._src.lax.control_flow.loops._while_loop_abstract_eval(*avals,cond_jaxpr,body_jaxpr,body_nconsts,cond_nconsts)
jax._src.lax.control_flow.loops._while_loop_batching_rule(spmd_axis_name,axis_size,axis_name,main_type,args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.lax.control_flow.loops._while_loop_jvp(primals,tangents,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.lax.control_flow.loops._while_lowering(ctx,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax._src.lax.control_flow.loops._while_partial_eval(trace:pe.JaxprTrace,*tracers:pe.Tracer,cond_nconsts:int,cond_jaxpr:pe.ClosedJaxpr,body_nconsts:int,body_jaxpr:pe.ClosedJaxpr)->Sequence[pe.Tracer]
jax._src.lax.control_flow.loops._while_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.loops._while_transpose_error(*_,**kwargs)
jax._src.lax.control_flow.loops._while_typecheck(_,*in_atoms,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax._src.lax.control_flow.loops.associative_scan(fn:Callable,elems,reverse:bool=False,axis:int=0)
jax._src.lax.control_flow.loops.cumlogsumexp(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cummax(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cummin(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cumprod(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cumred_gpu_impl(window_reduce:Callable,reduce_fn:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops.cumred_reduce_window_impl(window_reduce:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops.cumsum(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.fori_loop(lower,upper,body_fun,init_val)
jax._src.lax.control_flow.loops.map(f,xs)
jax._src.lax.control_flow.loops.scan(f:Callable[[Carry,X],tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->tuple[Carry, Y]
jax._src.lax.control_flow.loops.scan_bind(*args,**params)
jax._src.lax.control_flow.loops.while_loop(cond_fun:Callable[[T],BooleanNumeric],body_fun:Callable[[T],T],init_val:T)->T
jax._src.lax.control_flow.map(f,xs)
jax._src.lax.control_flow.scan(f:Callable[[Carry,X],tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->tuple[Carry, Y]
jax._src.lax.control_flow.scan_bind(*args,**params)
jax._src.lax.control_flow.while_loop(cond_fun:Callable[[T],BooleanNumeric],body_fun:Callable[[T],T],init_val:T)->T


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/for_loop.py----------------------------------------
A:jax._src.lax.control_flow.for_loop.S->TypeVar('S')
A:jax._src.lax.control_flow.for_loop.T->TypeVar('T')
A:jax._src.lax.control_flow.for_loop.for_p->jax._src.core.Primitive('for')
A:jax._src.lax.control_flow.for_loop.(const_avals, const_ref_avals)->partition_list(is_const_ref, all_const_avals)
A:jax._src.lax.control_flow.for_loop.const_avals->map(AbstractRef, const_avals)
A:jax._src.lax.control_flow.for_loop.merged_const_avals->merge_lists(is_const_ref, const_avals, const_ref_avals)
A:jax._src.lax.control_flow.for_loop.num_consts->len(merged_const_avals)
A:jax._src.lax.control_flow.for_loop.(all_consts, args)->split_list(consts_args, [num_consts])
A:jax._src.lax.control_flow.for_loop.(consts, const_refs)->partition_list(is_const_ref, all_consts)
A:jax._src.lax.control_flow.for_loop.consts->map(lambda x: ref_get(x, ()), consts)
A:jax._src.lax.control_flow.for_loop.all_consts->merge_lists(is_const_ref, consts, const_refs)
A:jax._src.lax.control_flow.for_loop.(hoisted_jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(_hoist), in_avals)
A:jax._src.lax.control_flow.for_loop.(f, out_tree_thunk)->flatten_fun_nokwargs(lu.wrap_init(f), treedef_tuple((tree_structure(0), state_tree)))
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, [*in_avals, *res_ref_avals])
A:jax._src.lax.control_flow.for_loop.vals->for_loop(rest_steps, functools.partial(body, i), vals, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(flat_state, state_tree)->tree_flatten(init_state)
A:jax._src.lax.control_flow.for_loop.state_avals->map(state_utils.val_to_ref_aval, flat_state)
A:jax._src.lax.control_flow.for_loop.idx_aval->jax._src.core.ShapedArray((), jnp.dtype('int32'))
A:jax._src.lax.control_flow.for_loop.(jaxpr, consts, out_tree)->_trace_to_jaxpr_with_refs(body, state_tree, [idx_aval, *state_avals])
A:jax._src.lax.control_flow.for_loop.jaxpr->_hoist_consts_to_refs(jaxpr)
A:jax._src.lax.control_flow.for_loop.out_flat->jax._src.lax.control_flow.loops.fori_loop(0, nsteps, fori_body, flat_state)
A:jax._src.lax.control_flow.for_loop.Carry->TypeVar('Carry')
A:jax._src.lax.control_flow.for_loop.X->TypeVar('X')
A:jax._src.lax.control_flow.for_loop.Y->TypeVar('Y')
A:jax._src.lax.control_flow.for_loop.(xs_flat, xs_tree)->tree_flatten(xs)
A:jax._src.lax.control_flow.for_loop.length->int(length)
A:jax._src.lax.control_flow.for_loop.unique_lengths->set(lengths)
A:jax._src.lax.control_flow.for_loop.x_avals->tuple(map(core.ShapedArray, x_shapes, x_dtypes))
A:jax._src.lax.control_flow.for_loop.init_flat->tree_leaves(init)
A:jax._src.lax.control_flow.for_loop.(_, in_tree)->tree_flatten((init, xs))
A:jax._src.lax.control_flow.for_loop.carry_avals->tuple(map(_abstractify, init_flat))
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, out_tree)->_initial_style_jaxpr(f, in_tree, carry_avals + x_avals, 'scan')
A:jax._src.lax.control_flow.for_loop.(jaxpr, out_tree)->_create_jaxpr(init)
A:jax._src.lax.control_flow.for_loop.(_, ys_avals)->tree_unflatten(out_tree, jaxpr.out_avals)
A:jax._src.lax.control_flow.for_loop.ys->tree_map(lambda aval: jnp.zeros([length, *aval.shape], aval.dtype), ys_avals)
A:jax._src.lax.control_flow.for_loop.carry->tree_map(lambda x: x[()], carry_refs)
A:jax._src.lax.control_flow.for_loop.x->tree_map(lambda x: x[i], xs_refs)
A:jax._src.lax.control_flow.for_loop.(carry, y)->f(carry, x)
A:jax._src.lax.control_flow.for_loop.(init, _, ys)->for_loop(length, for_body, (init, xs, ys), reverse=reverse, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.nonlocal_state_effects->jax._src.core.join_effects(*aval_effects)
A:jax._src.lax.control_flow.for_loop.out_vals->jax._src.core.Primitive('for').bind(*args, jaxpr=jaxpr, reverse=reverse, which_linear=which_linear, nsteps=nsteps, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, consts)->discharge_state(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.i->jax.numpy.int32(i)
A:jax._src.lax.control_flow.for_loop.state->body(i, state)
A:jax._src.lax.control_flow.for_loop.(_, state)->jax.lax.while_loop(cond, while_body, (i, state))
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, body_consts)->discharge_state(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.(_, out_batched)->jax._src.interpreters.batching.batch_jaxpr(core.ClosedJaxpr(discharged_jaxpr, body_consts), axis_size, [False] + batched, instantiate=batched, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.for_loop.batched->map(operator.or_, batched, out_batched)
A:jax._src.lax.control_flow.for_loop.(batched_jaxpr_, _)->jax._src.interpreters.batching.batch_jaxpr(core.ClosedJaxpr(jaxpr, []), axis_size, [False] + batched, [], axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.for_loop.batching.axis_primitive_batchers[for_p]->functools.partial(_for_vmap, None)
A:jax._src.lax.control_flow.for_loop.(_, out_nonzero_tangents)->jax._src.interpreters.ad.jvp_jaxpr(core.ClosedJaxpr(discharged_jaxpr, body_consts), [False] + nonzero_tangents, instantiate=nonzero_tangents)
A:jax._src.lax.control_flow.for_loop.nonzero_tangents->map(operator.or_, nonzero_tangents, out_nonzero_tangents)
A:jax._src.lax.control_flow.for_loop.closed_jaxpr->jax._src.core.ClosedJaxpr(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.(jvp_jaxpr_, _)->jax._src.interpreters.ad.jvp_jaxpr(closed_jaxpr, [False] + nonzero_tangents, [])
A:jax._src.lax.control_flow.for_loop.(out_primals, out_tangents)->split_list(out_flat, [len(primals)])
A:jax._src.lax.control_flow.for_loop.out_tangents_iter->iter(out_tangents)
A:jax._src.lax.control_flow.for_loop.loop_var_refs->map(operator.not_, loop_invar_refs)
A:jax._src.lax.control_flow.for_loop.(_, _, loop_var_outputs, _, _)->_partial_eval_jaxpr_custom(jaxpr, loop_var_inputs, _save_everything)
A:jax._src.lax.control_flow.for_loop.num_inputs->len(eqn.invars)
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, discharged_consts)->discharge_state(jaxpr, consts)
A:jax._src.lax.control_flow.for_loop.discharged_jaxpr->discharged_jaxpr.replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[]).replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[])
A:jax._src.lax.control_flow.for_loop.(_, _, out_unknowns, _, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(discharged_jaxpr, jaxpr_in_unknowns, [True] * len(jaxpr_in_unknowns), in_unknowns, False, _save_everything)
A:jax._src.lax.control_flow.for_loop.out_unknowns->list(out_unknowns)
A:jax._src.lax.control_flow.for_loop.in_unknowns->map(operator.or_, in_unknowns, out_unknowns)
A:jax._src.lax.control_flow.for_loop.tracers->tuple((trace.instantiate_const(t) if uk else t for (t, uk) in zip(tracers, in_unknowns)))
A:jax._src.lax.control_flow.for_loop.(jaxpr_known_resout, jaxpr_unknown_resin_, uk_out, inst_out, num_res)->_partial_eval_jaxpr_custom(jaxpr, [False, *in_unknowns], _save_everything)
A:jax._src.lax.control_flow.for_loop.loop_invar_res->_loop_invariant_outputs(jaxpr_known_resout)
A:jax._src.lax.control_flow.for_loop.(jaxpr_known, res_avals)->_convert_outputs_to_writes(nsteps, jaxpr_known_resout, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.(known_tracers, _)->partition_list(in_unknowns, tracers)
A:jax._src.lax.control_flow.for_loop.empty_res->map(ad_util.zeros_like_aval, res_avals)
A:jax._src.lax.control_flow.for_loop.(known_outputs, residuals)->split_list(out_flat, [len(known_tracers)])
A:jax._src.lax.control_flow.for_loop.residuals->map(trace.new_instantiated_const, residuals)
A:jax._src.lax.control_flow.for_loop.(jaxpr_unknown_resin, used_inputs)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr_unknown_resin_, [], [True] * num_res + [True, *in_unknowns])
A:jax._src.lax.control_flow.for_loop.(used_res, (used_i,), used_refs)->split_list(used_inputs, [num_res, 1])
A:jax._src.lax.control_flow.for_loop.jaxpr_unknown->_convert_inputs_to_reads(nsteps, len(res_avals), jaxpr_unknown_resin, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.used_and_known->map(operator.and_, used_refs, map(operator.not_, in_unknowns))
A:jax._src.lax.control_flow.for_loop.(_, known_used)->partition_list(used_refs, used_and_known)
A:jax._src.lax.control_flow.for_loop.(_, used_tracers)->partition_list(used_refs, tracers)
A:jax._src.lax.control_flow.for_loop.(_, used_which_linear)->partition_list(used_refs, which_linear)
A:jax._src.lax.control_flow.for_loop.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.for_loop.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs, for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps, reverse=reverse, which_linear=which_linear_unknown, unroll=unroll), core.no_effects, source)
A:jax._src.lax.control_flow.for_loop.(_, unknown_outputs)->split_list(res_ref_unknown_outputs, [num_res])
A:jax._src.lax.control_flow.for_loop.(unknown_outputs, _)->partition_list(known_used, unknown_outputs)
A:jax._src.lax.control_flow.for_loop.(jaxpr, nsteps, reverse, which_linear, unroll)->split_dict(eqn.params, ['jaxpr', 'nsteps', 'reverse', 'which_linear', 'unroll'])
A:jax._src.lax.control_flow.for_loop.(_, _, out_unknowns, out_inst, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(discharged_jaxpr, jaxpr_in_unknowns, True, ensure_out_unknowns=in_unknowns, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.for_loop.(jaxpr_known_resout, jaxpr_staged_resin_, _, _, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr, [False, *in_unknowns], [True, *in_inst], [], [], saveable)
A:jax._src.lax.control_flow.for_loop.(known_invars, _)->partition_list(in_unknowns, eqn.invars)
A:jax._src.lax.control_flow.for_loop.(known_outvars, _)->partition_list(in_unknowns, eqn.outvars)
A:jax._src.lax.control_flow.for_loop.newvar->jax._src.core.gensym()
A:jax._src.lax.control_flow.for_loop.resvars->map(newvar, res_avals)
A:jax._src.lax.control_flow.for_loop.(call_jaxpr_, _, call_jaxpr_consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(staged, [v.aval for v in [*resvars, *eqn.invars]])
A:jax._src.lax.control_flow.for_loop.call_jaxpr->jax._src.core.ClosedJaxpr(call_jaxpr_, call_jaxpr_consts)
A:jax._src.lax.control_flow.for_loop.eqn_known->jax._src.interpreters.partial_eval.new_jaxpr_eqn(known_invars, [*known_outvars, *resvars], core.closed_call_p, dict(call_jaxpr=call_jaxpr), call_jaxpr.effects, eqn.source_info)
A:jax._src.lax.control_flow.for_loop.jaxpr_staged->_convert_inputs_to_reads(nsteps, len(res_avals), jaxpr_staged_resin_, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.params_staged->dict(eqn.params, jaxpr=jaxpr_staged, reverse=reverse, nsteps=nsteps, which_linear=which_linear_unknown, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(_, ans)->partition_list(out_inst, ans)
A:jax._src.lax.control_flow.for_loop.(_, outvars)->partition_list(out_inst, eqn.outvars)
A:jax._src.lax.control_flow.for_loop.eqn_staged->jax._src.interpreters.partial_eval.new_jaxpr_eqn([*resvars, *eqn.invars], outvars, core.closed_call_p, dict(call_jaxpr=call_jaxpr), call_jaxpr.effects, eqn.source_info)
A:jax._src.lax.control_flow.for_loop.(orig_refs, residual_refs)->split_list(refs, [len(in_avals) - 1])
A:jax._src.lax.control_flow.for_loop.residual_vals->jax._src.core.eval_jaxpr(jaxpr, (), i, *orig_refs)
A:jax._src.lax.control_flow.for_loop.(residual_refs, orig_refs)->split_list(refs, [num_res])
A:jax._src.lax.control_flow.for_loop.()->jax._src.core.eval_jaxpr(jaxpr, (), *residual_vals, i, *orig_refs)
A:jax._src.lax.control_flow.for_loop.(res_val_avals, (i_aval,), orig_ref_avals)->split_list([v.aval for v in jaxpr.invars], [num_res, 1])
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, ())->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, [i_aval, *res_ref_avals, *orig_ref_avals])
A:jax._src.lax.control_flow.for_loop.(res_jaxpr, tangent_jaxpr_, *_)->_partial_eval_jaxpr_custom(jaxpr, [False, *which_linear], _save_everything)
A:jax._src.lax.control_flow.for_loop.res->jax._src.core.eval_jaxpr(res_jaxpr, (), i, *res_args)
A:jax._src.lax.control_flow.for_loop.(tangent_jaxpr, used)->jax._src.interpreters.partial_eval.dce_jaxpr(tangent_jaxpr_, [])
A:jax._src.lax.control_flow.for_loop.(used_res, (used_i,), used_ct)->split_list(used, [len(res), 1])
A:jax._src.lax.control_flow.for_loop.(jaxpr_trans, _, _)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(trans), [v.aval for v in jaxpr.invars])
A:jax._src.lax.control_flow.for_loop.jaxpr_transpose->transpose_jaxpr(jaxpr, which_linear)
A:jax._src.lax.control_flow.for_loop.all_outs->jax._src.core.Primitive('for').bind(*args_, jaxpr=jaxpr_transpose, nsteps=nsteps, reverse=not reverse, which_linear=tuple(which_linear_transpose), unroll=unroll)
jax._src.lax.control_flow.for_loop.Ref(Generic[T])
jax._src.lax.control_flow.for_loop._convert_inputs_to_reads(nsteps:int,num_res:int,jaxpr:core.Jaxpr,loop_invar_res:Sequence[bool])->core.Jaxpr
jax._src.lax.control_flow.for_loop._convert_outputs_to_writes(nsteps:int,jaxpr:core.Jaxpr,loop_invar_res:Sequence[bool])->tuple[core.Jaxpr, list[core.ShapedArray]]
jax._src.lax.control_flow.for_loop._for_abstract_eval(*avals,jaxpr,**__)
jax._src.lax.control_flow.for_loop._for_discharge_rule(in_avals,_,*args:Any,jaxpr:core.Jaxpr,reverse:bool,which_linear:Sequence[bool],nsteps:int,unroll:int)->tuple[Sequence[Optional[Any]], Sequence[Any]]
jax._src.lax.control_flow.for_loop._for_impl(*args,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_impl_unrolled(body,nsteps,unroll,*args)
jax._src.lax.control_flow.for_loop._for_jvp(primals,tangents,*,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_partial_eval(trace:pe.JaxprTrace,*tracers:pe.JaxprTracer,jaxpr:core.Jaxpr,nsteps:int,reverse:bool,which_linear:tuple[bool,...],unroll:int)->list[pe.JaxprTracer]
jax._src.lax.control_flow.for_loop._for_partial_eval_custom(saveable,in_unknowns,in_inst,eqn)
jax._src.lax.control_flow.for_loop._for_transpose(in_cts,*args,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_vmap(spmd_axis_name,axis_size,axis_name,main_type,args,dims,*,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._hoist_consts_to_refs(jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.lax.control_flow.for_loop._is_read_only(ref_effects:set[StateEffect])->bool
jax._src.lax.control_flow.for_loop._loop_invariant_outputs(jaxpr:core.Jaxpr)->list[bool]
jax._src.lax.control_flow.for_loop._partial_eval_jaxpr_custom(jaxpr,in_unknowns,policy)
jax._src.lax.control_flow.for_loop._trace_to_jaxpr_with_refs(f,state_tree:PyTreeDef,state_avals:Sequence[core.AbstractValue])->tuple[core.Jaxpr, list[Any], PyTreeDef]
jax._src.lax.control_flow.for_loop.discharged_for_loop(nsteps,body,init_state,*,reverse:bool=False)
jax._src.lax.control_flow.for_loop.for_loop(nsteps:Union[int,Sequence[int]],body:Callable[[Array,Ref[S]],None],init_state:S,*,reverse:bool=False,unroll:int=1)->S
jax._src.lax.control_flow.for_loop.run_state(f,init_state)
jax._src.lax.control_flow.for_loop.scan(f:Callable[[Carry,X],tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->tuple[Carry, Y]
jax._src.lax.control_flow.for_loop.transpose_jaxpr(jaxpr:core.Jaxpr,which_linear:list[bool])->core.Jaxpr


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/conditionals.py----------------------------------------
A:jax._src.lax.control_flow.conditionals._no_operand_sentinel->object()
A:jax._src.lax.control_flow.conditionals.index_dtype->jax._src.dtypes.result_type(index)
A:jax._src.lax.control_flow.conditionals.branches->tuple((core.subst_axis_names_jaxpr(jaxpr, subst) for jaxpr in params['branches']))
A:jax._src.lax.control_flow.conditionals.index->jax._src.core.raise_as_much_as_possible(index)
A:jax._src.lax.control_flow.conditionals.lo->numpy.array(0, np.int32)
A:jax._src.lax.control_flow.conditionals.hi->numpy.array(len(branches) - 1, np.int32)
A:jax._src.lax.control_flow.conditionals.(ops, ops_tree)->tree_flatten(operands)
A:jax._src.lax.control_flow.conditionals.ops_avals->tuple(map(_abstractify, ops))
A:jax._src.lax.control_flow.conditionals.(jaxprs, consts, out_trees)->_initial_style_jaxprs_with_common_consts((true_fun, false_fun), ops_tree, ops_avals, 'cond')
A:jax._src.lax.control_flow.conditionals.joined_effects->jax._src.core.join_effects(*(branch.effects for branch in branches))
A:jax._src.lax.control_flow.conditionals.disallowed_effects->jax._src.effects.control_flow_allowed_effects.filter_not_in(joined_effects)
A:jax._src.lax.control_flow.conditionals.out->jax._src.core.AxisPrimitive('cond').bind(index, *res, *cts, branches=branches_trans, linear=linear_trans)
A:jax._src.lax.control_flow.conditionals.pred_dtype->jax._src.dtypes.result_type(pred)
A:jax._src.lax.control_flow.conditionals.(linear_ops, ops_tree2)->tree_flatten(linear)
A:jax._src.lax.control_flow.conditionals.false_jaxpr->replace_jaxpr_effects(false_jaxpr, joined_effects)
A:jax._src.lax.control_flow.conditionals.true_jaxpr->replace_jaxpr_effects(true_jaxpr, joined_effects)
A:jax._src.lax.control_flow.conditionals.ba->inspect.signature(_cond_with_per_branch_args).bind(*args, **kwargs)
A:jax._src.lax.control_flow.conditionals.eff->eff.replace(input_index=eff.input_index + 1).replace(input_index=eff.input_index + 1)
A:jax._src.lax.control_flow.conditionals.idx->list(range(np.ndim(pred)))
A:jax._src.lax.control_flow.conditionals.pred->jax._src.lax.lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)
A:jax._src.lax.control_flow.conditionals.predicate->jax._src.lax.lax.eq(index, lax._const(index, i))
A:jax._src.lax.control_flow.conditionals.branches_batched->tuple((batching.batch_jaxpr(jaxpr, axis_size, ops_bat, out_bat, axis_name, spmd_axis_name, main_type)[0] for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.branches_jvp->tuple((ad.jvp_jaxpr(jaxpr, ops_nz, instantiate=out_nz)[0] for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.ops_dot->_prune_zeros(ops_dot)
A:jax._src.lax.control_flow.conditionals.ops_lin->tuple(linear)
A:jax._src.lax.control_flow.conditionals.(out_primals, out_tangents)->split_list(out, [len(out_nz)])
A:jax._src.lax.control_flow.conditionals.out_tangents_iter->iter(out_tangents)
A:jax._src.lax.control_flow.conditionals.params->dict(branches=branches_unknown, linear=tuple(linear_unknown))
A:jax._src.lax.control_flow.conditionals.(_, _, out_uks, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(branch_jaxpr, ops_uk, instantiate=False)
A:jax._src.lax.control_flow.conditionals.(branch_jaxpr_known, branch_jaxpr_unknown, _, res_avals)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_nounits(branch_jaxpr, ops_uk, instantiate=out_uks)
A:jax._src.lax.control_flow.conditionals.(all_res_avals, res_avals_per_branch)->_merge_branch_residuals(branch_res_avals)
A:jax._src.lax.control_flow.conditionals.num_res->len(res_indices)
A:jax._src.lax.control_flow.conditionals.branches_known->_join_cond_outputs(branches_known_, all_res_avals, res_avals_per_branch, num_known_outs)
A:jax._src.lax.control_flow.conditionals.branches_unknown->_join_cond_pe_staged_jaxpr_inputs(branches_unknown, all_res_avals, res_avals_per_branch)
A:jax._src.lax.control_flow.conditionals.out_consts_res->jax._src.core.AxisPrimitive('cond').bind(*in_consts, branches=branches_known, linear=tuple(linear_known))
A:jax._src.lax.control_flow.conditionals.(out_consts, res)->split_list(out_consts_res, [len(out_consts_res) - num_res])
A:jax._src.lax.control_flow.conditionals.index_tracer->trace.instantiate_const(tracers[0])
A:jax._src.lax.control_flow.conditionals.res_tracers->map(trace.new_instantiated_const, res)
A:jax._src.lax.control_flow.conditionals.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.conditionals.eqn->jax._src.interpreters.partial_eval.new_eqn_recipe([index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params, core.join_effects(*(j.effects for j in branches_unknown)), source)
A:jax._src.lax.control_flow.conditionals.(_, _, unks_out_, _, _)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=ops_uk, in_inst=True, ensure_out_unknowns=False, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.conditionals.unks_out->map(operator.or_, unks_out, unks_out_)
A:jax._src.lax.control_flow.conditionals.(jaxpr_known, jaxpr_staged, _, inst_out, num_res)->jax._src.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=ops_uk, in_inst=True, ensure_out_unknowns=unks_out, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.conditionals.branches_staged->_join_cond_pe_staged_jaxpr_inputs(branches_staged_, all_res_avals, res_avals_per_branch)
A:jax._src.lax.control_flow.conditionals.newvar->jax._src.core.gensym([j.jaxpr for j in jaxprs], suffix='_')
A:jax._src.lax.control_flow.conditionals.res_binders->map(newvar, all_res_avals)
A:jax._src.lax.control_flow.conditionals.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax._src.lax.control_flow.conditionals.(out_binders_known, _)->partition_list(unks_out, eqn.outvars)
A:jax._src.lax.control_flow.conditionals.params_known->dict(branches=branches_known, linear=tuple(linear_known))
A:jax._src.lax.control_flow.conditionals.effects_known->_join_cond_effects(branches_known)
A:jax._src.lax.control_flow.conditionals.eqn_known->jax._src.interpreters.partial_eval.new_jaxpr_eqn(ins_known, [*out_binders_known, *res_binders], cond_p, params_known, effects_known, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax._src.lax.control_flow.conditionals.params_staged->dict(branches=branches_staged, linear=tuple(linear_staged))
A:jax._src.lax.control_flow.conditionals.effects_staged->_join_cond_effects(branches_staged)
A:jax._src.lax.control_flow.conditionals.eqn_staged->jax._src.interpreters.partial_eval.new_jaxpr_eqn([eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged, cond_p, params_staged, effects_staged, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.branch_res_tagged_avals->map(enumerate_equal, branch_res_avals)
A:jax._src.lax.control_flow.conditionals.all_tagged_avals->_ordered_unique(util.concatenate(branch_res_tagged_avals))
A:jax._src.lax.control_flow.conditionals.outs_and_residuals->jax._src.core.jaxpr_as_fun(jaxpr)(*args)
A:jax._src.lax.control_flow.conditionals.(outs, residuals)->split_list(outs_and_residuals, [num_non_res_outputs])
A:jax._src.lax.control_flow.conditionals.aug_residuals->jax._src.util.subvals(aug_residuals, zip(res_indices, residuals))
A:jax._src.lax.control_flow.conditionals.all_res_vars->map(newvar, all_res_avals)
A:jax._src.lax.control_flow.conditionals.aug_res_vars->list(util.subvals(all_res_vars, zip(res_indices, res_vars)))
A:jax._src.lax.control_flow.conditionals.jaxpr_aug->jax._src.core.ClosedJaxpr(jaxpr_aug, jaxpr.consts)
A:jax._src.lax.control_flow.conditionals.d->collections.OrderedDict(((x, None) for x in xs))
A:jax._src.lax.control_flow.conditionals.(_, used_inputs_)->jax._src.interpreters.partial_eval.dce_jaxpr(jaxpr, used_outputs, instantiate=False)
A:jax._src.lax.control_flow.conditionals.used_inputs->map(operator.or_, used_inputs, used_inputs_)
A:jax._src.lax.control_flow.conditionals.new_params->dict(eqn.params, branches=tuple(dce_branches), linear=tuple(dce_linear))
A:jax._src.lax.control_flow.conditionals.new_effects->_join_cond_effects(dce_branches_)
A:jax._src.lax.control_flow.conditionals.new_eqn->jax._src.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, [True, *used_inputs]) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_effects, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.(res_avals, primal_avals)->split_list(jaxpr.in_avals, [num_res])
A:jax._src.lax.control_flow.conditionals.primal_avals->map(raise_to_shaped, primal_avals)
A:jax._src.lax.control_flow.conditionals.(res, cts_out)->split_list(args, [num_res])
A:jax._src.lax.control_flow.conditionals.cts_in->jax._src.interpreters.ad.backward_pass(jaxpr.jaxpr, reduce_axes, False, jaxpr.consts, primals, cts_out)
A:jax._src.lax.control_flow.conditionals.(_, cts_in)->split_list(cts_in, [num_res])
A:jax._src.lax.control_flow.conditionals.in_avals->map(raise_to_shaped, branches[0].in_avals)
A:jax._src.lax.control_flow.conditionals.branches_trans->tuple((_transpose_cond_jaxpr(jaxpr, num_res, reduce_axes) for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.cts->map(ad.instantiate_zeros_aval, branches[0].out_avals, cts)
A:jax._src.lax.control_flow.conditionals.out_iter->iter(out)
A:jax._src.lax.control_flow.conditionals.tc->partial(_typecheck_param, 'cond')
A:jax._src.lax.control_flow.conditionals.jaxpr0_in_avals_str->_avals_short(jaxpr0.in_avals)
A:jax._src.lax.control_flow.conditionals.jaxpr0_out_avals_str->_avals_short(jaxpr0.out_avals)
A:jax._src.lax.control_flow.conditionals.avals->map(core.get_aval, args)
A:jax._src.lax.control_flow.conditionals.cond_p->jax._src.core.AxisPrimitive('cond')
A:jax._src.lax.control_flow.conditionals.batching.axis_primitive_batchers[cond_p]->partial(_cond_batching_rule, None)
A:jax._src.lax.control_flow.conditionals.core.custom_typechecks[cond_p]->partial(_cond_typecheck, False)
A:jax._src.lax.control_flow.conditionals.ordered_effects->list(effects.ordered_effects.filter_in(joined_effects))
A:jax._src.lax.control_flow.conditionals.num_tokens->len(ordered_effects)
A:jax._src.lax.control_flow.conditionals.tokens_in->ctx.tokens_in.subset(ordered_effects)
A:jax._src.lax.control_flow.conditionals.flat_output_types->jax._src.util.flatten(output_types)
A:jax._src.lax.control_flow.conditionals.case_op->jax._src.lib.mlir.dialects.hlo.CaseOp(flat_output_types, index=index, num_branches=len(branches))
A:jax._src.lax.control_flow.conditionals.name_stack->ctx.module_context.name_stack.extend('cond')
A:jax._src.lax.control_flow.conditionals.branch->jax._src.lib.mlir.dialects.hlo.CaseOp(flat_output_types, index=index, num_branches=len(branches)).regions[i].blocks.append()
A:jax._src.lax.control_flow.conditionals.sub_ctx->ctx.module_context.replace(name_stack=name_stack.extend(f'branch_{i}_fun'))
A:jax._src.lax.control_flow.conditionals.(out_vals, tokens_out)->jax._src.interpreters.mlir.jaxpr_subcomp(sub_ctx, jaxpr.jaxpr, tokens_in, consts, *map(mlir.wrap_singleton_ir_values, args), dim_var_values=ctx.dim_var_values)
A:jax._src.lax.control_flow.conditionals.tokens_and_outputs->jax._src.util.unflatten(case_op.results, map(len, output_types))
A:jax._src.lax.control_flow.conditionals.(tokens, outputs)->jax._src.util.split_list(tokens_and_outputs, [num_tokens])
A:jax._src.lax.control_flow.conditionals.discharged_branches->tuple((core.ClosedJaxpr(discharge_state(branch.jaxpr, ())[0], ()) for branch in branches))
A:jax._src.lax.control_flow.conditionals.out_vals->jax._src.core.AxisPrimitive('cond').bind(*args, branches=discharged_branches, linear=linear)
A:jax._src.lax.control_flow.conditionals.(out_vals, out_ref_vals)->jax._src.util.split_list(out_vals, [len(out_avals)])
A:jax._src.lax.control_flow.conditionals.ref_val_iter->iter(out_ref_vals)
jax._src.lax.control_flow.cond(*args,**kwargs)
jax._src.lax.control_flow.cond_bind(*args,branches,linear)
jax._src.lax.control_flow.conditionals._bcast_select(pred,on_true,on_false)
jax._src.lax.control_flow.conditionals._bcast_select_n(pred,*cases)
jax._src.lax.control_flow.conditionals._cond(pred,true_fun:Callable,false_fun:Callable,*operands,operand=_no_operand_sentinel,linear=None)
jax._src.lax.control_flow.conditionals._cond_abstract_eval(*avals,branches,**_)
jax._src.lax.control_flow.conditionals._cond_axis_substitution(params,subst,traverse)
jax._src.lax.control_flow.conditionals._cond_batching_rule(spmd_axis_name,axis_size,axis_name,main_type,args,dims,branches,linear)
jax._src.lax.control_flow.conditionals._cond_dce_rule(used_outputs:list[bool],eqn:core.JaxprEqn)->tuple[list[bool], core.JaxprEqn]
jax._src.lax.control_flow.conditionals._cond_jvp(primals,tangents,branches,linear)
jax._src.lax.control_flow.conditionals._cond_lowering(ctx,index,*args,branches,linear)
jax._src.lax.control_flow.conditionals._cond_partial_eval(trace,*tracers,branches,linear)
jax._src.lax.control_flow.conditionals._cond_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.conditionals._cond_state_discharge_rule(in_avals,out_avals,*args,branches,linear)
jax._src.lax.control_flow.conditionals._cond_transpose(reduce_axes,cts,*args,branches,linear)
jax._src.lax.control_flow.conditionals._cond_typecheck(bind_time,*in_atoms,branches,linear)
jax._src.lax.control_flow.conditionals._cond_with_per_branch_args(pred,true_operand,true_fun:Callable,false_operand,false_fun:Callable)
jax._src.lax.control_flow.conditionals._join_cond_effects(branches:Sequence[core.Jaxpr])->effects.Effects
jax._src.lax.control_flow.conditionals._join_cond_outputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr,num_non_res_outputs)
jax._src.lax.control_flow.conditionals._join_cond_pe_staged_jaxpr_inputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr)
jax._src.lax.control_flow.conditionals._merge_branch_residuals(branch_res_avals)
jax._src.lax.control_flow.conditionals._ordered_unique(xs)
jax._src.lax.control_flow.conditionals._transpose_cond_jaxpr(jaxpr,num_res,reduce_axes)
jax._src.lax.control_flow.conditionals.cond(*args,**kwargs)
jax._src.lax.control_flow.conditionals.cond_bind(*args,branches,linear)
jax._src.lax.control_flow.conditionals.switch(index,branches:Sequence[Callable],*operands,operand=_no_operand_sentinel)
jax._src.lax.control_flow.switch(index,branches:Sequence[Callable],*operands,operand=_no_operand_sentinel)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/solves.py----------------------------------------
A:jax._src.lax.control_flow.solves._RootTuple->collections.namedtuple('_RootTuple', 'f, solve, l_and_s')
A:jax._src.lax.control_flow.solves.params_list->split_list(args, list(const_lengths))
A:jax._src.lax.control_flow.solves.(guess_flat, in_args_tree)->tree_flatten((initial_guess,))
A:jax._src.lax.control_flow.solves.guess_avals->tuple(_map(_abstractify, guess_flat))
A:jax._src.lax.control_flow.solves.(f_jaxpr, f_consts, out_tree)->_initial_style_jaxpr(f, in_args_tree, guess_avals)
A:jax._src.lax.control_flow.solves.(in_tree,)->treedef_children(in_args_tree)
A:jax._src.lax.control_flow.solves.(solve_jaxpr, solve_consts, solution_tree)->_initial_style_jaxpr(partial(solve, f), in_args_tree, guess_avals)
A:jax._src.lax.control_flow.solves.(unchecked_zeros, f_jvp)->jax.linearize(f, x)
A:jax._src.lax.control_flow.solves.(l_and_s_jaxpr, l_and_s_consts, out_tree)->_initial_style_jaxpr(linearize_and_solve, treedef_tuple((in_tree,) * 2), guess_avals * 2)
A:jax._src.lax.control_flow.solves.const_lengths->_LinearSolveTuple(*_map(len, all_consts))
A:jax._src.lax.control_flow.solves.jaxprs->_LinearSolveTuple(matvec_jaxpr, vecmat_jaxpr, solve_jaxpr, tr_solve_jaxpr)
A:jax._src.lax.control_flow.solves.solution_flat->_custom_root(const_lengths, jaxprs, *_flatten(all_consts) + guess_flat)
A:jax._src.lax.control_flow.solves.(params, initial_guess)->_split_root_args(args, const_lengths)
A:jax._src.lax.control_flow.solves.solution->jax._src.core.jaxpr_as_fun(jaxprs.solve)(*params.solve + initial_guess)
A:jax._src.lax.control_flow.solves.(params, _)->_split_linear_solve_args(primals, const_lengths)
A:jax._src.lax.control_flow.solves.sol->_custom_root(const_lengths, jaxprs, *primals)
A:jax._src.lax.control_flow.solves.f_out_vals->len(jaxprs.f.out_avals)
A:jax._src.lax.control_flow.solves.(solution, aux)->split_list(sol, [f_out_vals])
A:jax._src.lax.control_flow.solves.(params_dot, _)->_split_root_args(tangents, const_lengths)
A:jax._src.lax.control_flow.solves.f->jax._src.core.jaxpr_as_fun(jaxprs.f)
A:jax._src.lax.control_flow.solves.linearize_and_solve->partial(core.jaxpr_as_fun(jaxprs.l_and_s), *params.l_and_s)
A:jax._src.lax.control_flow.solves.(_, rhs)->jax._src.interpreters.ad.jvp(lu.wrap_init(f_at_solution)).call_wrapped(params.f, params_dot.f)
A:jax._src.lax.control_flow.solves.solution_dot->_map(operator.neg, linearize_and_solve(*solution, *rhs))
A:jax._src.lax.control_flow.solves.transpose_fun->jax.linear_transpose(linear_fun, primals)
A:jax._src.lax.control_flow.solves.(y,)->transpose_fun(x)
A:jax._src.lax.control_flow.solves.actual_shapes->_map(np.shape, tree_leaves(actual))
A:jax._src.lax.control_flow.solves.expected_shapes->_map(np.shape, tree_leaves(expected))
A:jax._src.lax.control_flow.solves.(b_flat, in_args_tree)->tree_flatten((b,))
A:jax._src.lax.control_flow.solves.b_avals->tuple(_map(_abstractify, b_flat))
A:jax._src.lax.control_flow.solves.(tree,)->treedef_children(in_args_tree)
A:jax._src.lax.control_flow.solves.y->fun(x)
A:jax._src.lax.control_flow.solves.(y, aux)->fun(x)
A:jax._src.lax.control_flow.solves.(matvec_jaxpr, matvec_consts, out_tree)->_initial_style_jaxpr(_shape_checked(matvec, 'matvec', False), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.(solve_jaxpr, solve_consts, out_tree)->_initial_style_jaxpr(_shape_checked(partial(solve, matvec), 'solve', has_aux), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.vecmat->_transpose_one_output(matvec, b)
A:jax._src.lax.control_flow.solves.(vecmat_jaxpr, vecmat_consts, out_tree)->_initial_style_jaxpr(vecmat, in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.(tr_solve_jaxpr, tr_solve_consts, out_tree)->_initial_style_jaxpr(_shape_checked(partial(transpose_solve, vecmat), 'transpose_solve', has_aux), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.out_flat->jax._src.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(all_consts) + b_flat, const_lengths=const_lengths, jaxprs=jaxprs)
A:jax._src.lax.control_flow.solves.(params, b)->_split_linear_solve_args(args, const_lengths)
A:jax._src.lax.control_flow.solves.x->jax._src.core.AxisPrimitive('custom_linear_solve').bind(*primals, **kwargs)
A:jax._src.lax.control_flow.solves.zeros->_map(ad_util.Zero.from_value, x)
A:jax._src.lax.control_flow.solves.(_, out_tangent)->jax._src.interpreters.ad.jvp(lu.wrap_init(func)).call_wrapped(params + list(x), params_dot + zeros)
A:jax._src.lax.control_flow.solves.kwargs->dict(const_lengths=const_lengths, jaxprs=jaxprs)
A:jax._src.lax.control_flow.solves.(params_dot, b_dot)->_split_linear_solve_args(tangents, const_lengths)
A:jax._src.lax.control_flow.solves.num_x_leaves->len(b_dot)
A:jax._src.lax.control_flow.solves.(x_leaves, _)->split_list(x, [num_x_leaves])
A:jax._src.lax.control_flow.solves.matvec_tangents->_tangent_linear_map(core.jaxpr_as_fun(jaxprs.matvec), params.matvec, params_dot.matvec, *x_leaves)
A:jax._src.lax.control_flow.solves.rhs->_map(ad.add_tangents, b_dot, _map(operator.neg, matvec_tangents))
A:jax._src.lax.control_flow.solves.x_dot->jax._src.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(params) + rhs, **kwargs)
A:jax._src.lax.control_flow.solves.(dx_leaves, daux_leaves)->split_list(x_dot, [num_x_leaves])
A:jax._src.lax.control_flow.solves.daux_leaves->_map(ad_util.Zero.from_value, daux_leaves)
A:jax._src.lax.control_flow.solves.(x_cotangent, _)->split_list(cotangent, [len(b)])
A:jax._src.lax.control_flow.solves.cotangent_b_full->jax._src.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(params.transpose()) + x_cotangent, const_lengths=const_lengths.transpose(), jaxprs=jaxprs.transpose())
A:jax._src.lax.control_flow.solves.(cotangent_b, _)->split_list(cotangent_b_full, [len(b)])
A:jax._src.lax.control_flow.solves.(params_dims, b_dims)->_split_linear_solve_args(dims, const_lengths)
A:jax._src.lax.control_flow.solves.(params_bat, orig_b_bat)->_split_linear_solve_args(orig_bat, const_lengths)
A:jax._src.lax.control_flow.solves.num_operator_out_avals->len(matvec.out_avals)
A:jax._src.lax.control_flow.solves.(solve_jaxpr_batched, solve_x_bat)->jax._src.interpreters.batching.batch_jaxpr(solve, axis_size, solve_bat + b_bat, instantiate=x_bat, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.(vecmat_jaxpr_batched, vecmat_x_bat)->jax._src.interpreters.batching.batch_jaxpr(vecmat, axis_size, vecmat_bat + b_bat, instantiate=b_bat, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.x_bat_out->_map(operator.or_, vecmat_x_bat + [True] * num_aux, solve_x_bat)
A:jax._src.lax.control_flow.solves.(matvec_jaxpr_batched, matvec_b_bat)->jax._src.interpreters.batching.batch_jaxpr(matvec, axis_size, matvec_bat + x_bat_noaux, instantiate=b_bat, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.b_bat_out->_map(lambda m, s, o: m or s or o, matvec_b_bat, solve_t_b_bat, orig_b_bat)
A:jax._src.lax.control_flow.solves.(solve_t_jaxpr_batched, solve_t_b_aux_bat)->jax._src.interpreters.batching.batch_jaxpr(solve_t, axis_size, solve_t_bat + x_bat_noaux, instantiate=x_bat_out, axis_name=axis_name, spmd_axis_name=spmd_axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.(solve_t_b_bat, _)->split_list(solve_t_b_aux_bat, [len(orig_b_bat)])
A:jax._src.lax.control_flow.solves.batched_jaxprs->_LinearSolveTuple(matvec_jaxpr_batched, vecmat_jaxpr_batched, solve_jaxpr_batched, solve_t_jaxpr_batched)
A:jax._src.lax.control_flow.solves.outs->jax._src.core.AxisPrimitive('custom_linear_solve').bind(*new_params + new_b, const_lengths=const_lengths, jaxprs=batched_jaxprs)
A:jax._src.lax.control_flow.solves.linear_solve_p->jax._src.core.AxisPrimitive('custom_linear_solve')
A:jax._src.lax.control_flow.solves.batching.axis_primitive_batchers[linear_solve_p]->partial(_linear_solve_batching_rule, None)
jax._src.lax.control_flow._custom_linear_solve_impl(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False,has_aux=False)
jax._src.lax.control_flow.custom_root(f,initial_guess,solve,tangent_solve,has_aux=False)
jax._src.lax.control_flow.solves._LinearSolveTuple(collections.namedtuple('_LinearSolveTuple','matvec,vecmat,solve,transpose_solve'))
jax._src.lax.control_flow.solves._LinearSolveTuple.transpose(self)
jax._src.lax.control_flow.solves._check_shapes(func_name,expected_name,actual,expected)
jax._src.lax.control_flow.solves._custom_linear_solve_impl(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._custom_linear_solve_jvp(primals,tangents,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._custom_root(const_lengths,jaxprs,*args)
jax._src.lax.control_flow.solves._flatten(args)
jax._src.lax.control_flow.solves._linear_solve_abstract_eval(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._linear_solve_batching_rule(spmd_axis_name,axis_size,axis_name,main_type,args,dims,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._linear_solve_transpose_rule(cotangent,*primals,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._root_jvp(const_lengths,jaxprs,primals,tangents)
jax._src.lax.control_flow.solves._split_linear_solve_args(args,const_lengths)
jax._src.lax.control_flow.solves._split_root_args(args,const_lengths)
jax._src.lax.control_flow.solves._tangent_linear_map(func,params,params_dot,*x)
jax._src.lax.control_flow.solves._transpose_one_output(linear_fun,primals)
jax._src.lax.control_flow.solves.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False,has_aux=False)
jax._src.lax.control_flow.solves.custom_root(f,initial_guess,solve,tangent_solve,has_aux=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/_src/lax/control_flow/common.py----------------------------------------
A:jax._src.lax.control_flow.common.param_str->str(param)
A:jax._src.lax.control_flow.common.msg->sep.join([msg, param_str])
A:jax._src.lax.control_flow.common.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.lax.control_flow.common.debug->jax._src.interpreters.partial_eval.debug_info(fun, in_tree, out_tree, False, primitive_name or '<unknown>')
A:jax._src.lax.control_flow.common.(jaxpr, _, consts)->jax._src.interpreters.partial_eval.trace_to_jaxpr_dynamic(traceable, in_avals)
A:jax._src.lax.control_flow.common.(jaxpr, consts, out_tree)->_initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name)
A:jax._src.lax.control_flow.common.closed_jaxpr->jax._src.core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr), ())
A:jax._src.lax.control_flow.common.(jaxprs, all_consts, all_out_trees)->unzip3((_initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name) for fun in funs))
A:jax._src.lax.control_flow.common.tracer_id->id(c)
A:jax._src.lax.control_flow.common.canonical_id->len(canonical_refs)
A:jax._src.lax.control_flow.common.newvar->jax._src.core.gensym(jaxprs, suffix='_')
A:jax._src.lax.control_flow.common.unused_ref_const_vars->map(newvar, canonical_ref_avals)
A:jax._src.lax.control_flow.common.(nonref_constvars, ref_constvars)->partition_list(is_ref, jaxpr.constvars)
A:jax._src.lax.control_flow.common.const_prefix->jax._src.util.concatenate(unused_const_vars[:i])
A:jax._src.lax.control_flow.common.const_suffix->jax._src.util.concatenate(unused_const_vars[i + 1:])
A:jax._src.lax.control_flow.common.jaxpr->jaxpr.replace(effects=effects).replace(effects=effects)
A:jax._src.lax.control_flow.common.effects->jax._src.interpreters.partial_eval.make_jaxpr_effects(jaxpr.constvars, jaxpr.invars, jaxpr.outvars, jaxpr.eqns)
A:jax._src.lax.control_flow.common.jaxprs->tuple((pad_jaxpr_constvars(i, jaxpr) for (i, jaxpr) in enumerate(jaxprs)))
A:jax._src.lax.control_flow.common.diff->tree_map(_show_diff, tree_unflatten(tree1, avals1), tree_unflatten(tree2, avals2))
A:jax._src.lax.control_flow.common.actual_tree_children->actual_tree.children()
jax._src.lax.control_flow._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax._src.lax.control_flow._initial_style_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow._initial_style_jaxprs_with_common_consts(funs:Sequence[Callable],in_tree,in_avals,primitive_name:str)
jax._src.lax.control_flow._initial_style_open_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._abstractify(x)
jax._src.lax.control_flow.common._avals_short(avals)
jax._src.lax.control_flow.common._check_tree(func_name,expected_name,actual_tree,expected_tree,has_aux=False)
jax._src.lax.control_flow.common._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax._src.lax.control_flow.common._initial_style_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._initial_style_jaxprs_with_common_consts(funs:Sequence[Callable],in_tree,in_avals,primitive_name:str)
jax._src.lax.control_flow.common._initial_style_open_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._make_closed_jaxpr(traceable:lu.WrappedFun,in_avals:Sequence[core.AbstractValue])
jax._src.lax.control_flow.common._prune_zeros(ts)
jax._src.lax.control_flow.common._show_diff(array1,array2)
jax._src.lax.control_flow.common._typecheck_param(prim,param,name,msg_required,pred)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/ops/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/ad.py----------------------------------------
A:jax.interpreters.ad.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/batching.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/xla.py----------------------------------------
A:jax.interpreters.xla.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/pxla.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/partial_eval.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/interpreters/mlir.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/lax/__init__.py----------------------------------------
A:jax.lax.__init__.__getattr__->_deprecation_getattr(__name__, _deprecations)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.4.20/lib/python3.9/site-packages/jax/lax/linalg.py----------------------------------------

