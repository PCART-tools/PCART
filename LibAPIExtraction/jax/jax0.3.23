
----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/ad_checkpoint.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/stages.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/api_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/custom_transpose.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/flatten_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/core.py----------------------------------------
A:jax.core.self.constvars->list(constvars)
A:jax.core.self.invars->list(invars)
A:jax.core.self.outvars->list(outvars)
A:jax.core.self.eqns->list(eqns)
A:jax.core.doc->pp_jaxpr(self, JaxprPpContext(), JaxprPpSettings(source_info=source_info, print_shapes=print_shapes, custom_pp_eqn_rules=custom_pp_eqn_rules, name_stack=name_stack))
A:jax.core.self.consts->list(consts)
A:jax.core.settings->JaxprPpSettings(source_info=source_info, print_shapes=print_shapes, name_stack=name_stack, custom_pp_eqn_rules=custom_pp_eqn_rules)
A:jax.core.self.aval->raise_to_shaped(aval)
A:jax.core.all_vars->itertools.chain.from_iterable((_jaxpr_vars(j) for j in jaxprs))
A:jax.core.counter->itertools.count(start=start)
A:jax.core.self.hash->hash((val.item(), val.dtype))
A:jax.core.out->ans._trace.main.with_cur_sublevel().process_primitive(self, map(trace.full_raise, args), params)
A:jax.core.self.abstract_eval->_effect_free_abstract_eval(abstract_eval)
A:jax.core.(subfuns, bind_params)->eqn.primitive.get_bind_params(eqn.params)
A:jax.core.ans->max(tracers, key=lambda x: x._trace.level)
A:jax.core.idx->operator.index(idx)
A:jax.core.trace->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel()
A:jax.core.tracer->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel().full_raise(tracer)
A:jax.core.dbg->getattr(tracer, '_debug_info', None)
A:jax.core.line_info->getattr(tracer, '_line_info', None)
A:jax.core.attr->getattr(self.aval, name)
A:jax.core.t->ref(sublevel)
A:jax.core.base->jax._src.pretty_printer.group(pp.nest(2, pp.concat([base, pp.text(' with'), pp.brk(), pp.join(pp.brk(), [pp.text(f'{name} = ') + pp_payload for (name, pp_payload) in contents])])))
A:jax.core.aval_property->namedtuple('aval_property', ['fget'])
A:jax.core.aval_method->namedtuple('aval_method', ['fun'])
A:jax.core.eval_trace->MainTrace(0, EvalTrace)
A:jax.core.stack_str->map('  {}\n'.format, self.stack[::-1])
A:jax.core.new->self.__new__(TraceState)
A:jax.core.AxisEnvFrame->namedtuple('AxisEnvFrame', ['name', 'size', 'main_trace'])
A:jax.core.no_axis_name->object()
A:jax.core.self.trace_stack->TraceStack()
A:jax.core.new.trace_stack->self.trace_stack.copy()
A:jax.core.copy->MainTrace(dynamic.level, dynamic.trace_type, **dynamic.payload)
A:jax.core.self.trace_state->TraceState()
A:jax.core.thread_local_state->ThreadLocalState()
A:jax.core.tls->jax._src.lib.jax_jit.thread_local_state()
A:jax.core.tls.extra_jit_context->jax._src.config._ThreadLocalExtraJitContext(dynamic_trace_state=copy)
A:jax.core.traces->list(filter(lambda x: isinstance(x, Trace), gc.get_referrers(x)))
A:jax.core.tracers->map(top_trace.full_raise, args)
A:jax.core.msgs->'\n\n'.join((f'{tracer}{tracer._origin_msg()}' for tracer in tracers))
A:jax.core.level->stack.next_level()
A:jax.core.main->MainTrace(0, trace_type, **payload)
A:jax.core.leaked_tracers->maybe_find_leaked_tracers(t())
A:jax.core.sublevel->Sublevel(len(thread_local_state.trace_state.substack))
A:jax.core.top_tracer->max((x for x in xs if isinstance(x, Tracer)), default=None, key=attrgetter('_trace.level'))
A:jax.core.bot->Bot()
A:jax.core.handler->_get_special_dim_handler(v)
A:jax.core.fname->getattr(fun, '__name__', fun)
A:jax.core.self.dtype->_dtype_object(dtype)
A:jax.core._bool_nonzero->partialmethod(_forward_to_value, bool)
A:jax.core._float->concretization_function_error(float, True)
A:jax.core._int->partialmethod(_forward_to_value, int)
A:jax.core._complex->concretization_function_error(complex, True)
A:jax.core._hex->partialmethod(_forward_to_value, hex)
A:jax.core._oct->partialmethod(_forward_to_value, oct)
A:jax.core.self.shape->canonicalize_shape(shape)
A:jax.core.ndim->property(lambda self: len(self.shape))
A:jax.core.size->property(lambda self: prod(self.shape))
A:jax.core.named_shape->dict(aval.named_shape)
A:jax.core.shapestr->','.join(map(str, self.shape))
A:jax.core.named_shapestr->','.join((f'{k}:{v}' for (k, v) in self.named_shape.items()))
A:jax.core.dtype->_short_dtype_name(a.dtype)
A:jax.core.pad_shape->tuple((d.dtype.bound if type(d) is DArray and type(d.dtype) is bint else d for d in aval.shape))
A:jax.core.shape->property(lambda self: self._aval.shape)
A:jax.core.dtypestr->_short_dtype_name(self._aval.dtype)
A:jax.core.slices->tuple((slice(int(d._data)) if type(d) is DArray and type(d.dtype) is bint else slice(None) for d in self.shape))
A:jax.core.aval_type->type(aval)
A:jax.core.weak_type->getattr(aval, 'weak_type', False)
A:jax.core.sz1->int(np.prod(s1))
A:jax.core.sz2->int(np.prod(s2))
A:jax.core._dimension_handler_int->DimensionHandler()
A:jax.core.DArrayDimHandler->type('DArrayDimHandler', (DimensionHandler,), {})()
A:jax.core.special_handlers->set()
A:jax.core.(handler, ds)->_dim_handler_and_canonical(d)
A:jax.core.self.__positional->canonicalize_shape(args)
A:jax.core.self.__named->dict(kwargs)
A:jax.core.named->frozenset(self.__named.items())
A:jax.core.new_params->dict(params)
A:jax.core.jaxpr->dict(params).pop('call_jaxpr')
A:jax.core.subfun->jax.linear_util.hashable_partial(lu.wrap_init(eval_jaxpr), jaxpr, ())
A:jax.core.top_trace->find_top_trace(args)
A:jax.core.(fun_, env_trace_todo)->process_env_traces_call(fun, primitive, top_trace and top_trace.level, tuple(params.items()))
A:jax.core.fun_->jax.linear_util.annotate(fun_, fun.in_type)
A:jax.core.outs->map(trace.full_raise, outs)
A:jax.core.params->subst_axis_names(eqn.primitive, eqn.params, subst)
A:jax.core.(outs, cur_todo)->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel().post_process_call(primitive, outs, params)
A:jax.core.todos_list->list(todos)
A:jax.core.axes->dict(params).pop('out_axes')
A:jax.core.new_params['out_axes_thunk']->HashableFunction(lambda : axes, closure=axes)
A:jax.core.out_axes->t(out_axes)
A:jax.core.(_, out_axes_transforms)->todo_and_xforms()
A:jax.core.(fun, todo_and_xforms)->process_env_traces_map(fun, primitive, top_trace and top_trace.level, tuple(params.items()))
A:jax.core.(env_trace_todo, _)->todo_and_xforms()
A:jax.core.(outs, (cur_todo, cur_xform))->primitive.post_process(trace, outs, params)
A:jax.core.(handler, _)->aval_mapping_handlers.get(type(aval), (None, None))
A:jax.core.(_, handler)->aval_mapping_handlers.get(type(aval), (None, None))
A:jax.core.frame->AxisEnvFrame(axis_name, size, tag)
A:jax.core.self.id->id(obj)
A:jax.core.self.axis_names->set()
A:jax.core.subst->NameGatheringSubst()
A:jax.core.new_params[name]->subst_axis_names_jaxpr(jaxpr, shadowed_subst)
A:jax.core.names->tuple(it.chain.from_iterable((subst(name) for name in v.aval.named_shape)))
A:jax.core.new_v->Var(v.count, v.suffix, v.aval.update(named_shape=named_shape))
A:jax.core.new_jaxpr->Jaxpr(constvars, invars, outvars, eqns, jaxpr.effects)
A:jax.core.axis_main->max((axis_frame(a).main_trace for a in used_axis_names(self, params)), default=None, key=lambda t: getattr(t, 'level', -1))
A:jax.core.ctx->JaxprPpContext()
A:jax.core.pp_settings->JaxprPpSettings()
A:jax.core.(ctx, pp_settings)->ctx_factory()
A:jax.core.jaxpr_str->str(pp_jaxpr_eqn_range(jaxpr, 0, 20, ctx, pp_settings))
A:jax.core.msg->'\n\n'.join([msg, 'in equation:', str(pp.nest(2, pp_eqn(eqn, ctx, settings))), f'from source: {src}'])
A:jax.core.(ctx, _)->ctx_factory()
A:jax.core.in_atoms->map(read, eqn.invars)
A:jax.core.(out_type, effects)->check_eqn(prim, in_avals, eqn.params)
A:jax.core.out_type->substitute_vars_in_output_ty(out_type, eqn.invars, eqn.outvars)
A:jax.core.(ctx, settings)->ctx_factory()
A:jax.core.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax.core.aval->aval.update(shape=tuple([env.get(d, d) for d in aval.shape])).update(shape=tuple([env.get(d, d) for d in aval.shape]))
A:jax.core.(out_avals, effects)->prim.abstract_eval(*in_avals, **params)
A:jax.core.self.var_ids->collections.defaultdict(it.count().__next__, {})
A:jax.core.pp_v->jax._src.pretty_printer.text(str(v))
A:jax.core.rule->pp_eqn_rules.get(eqn.primitive)
A:jax.core.lhs->pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)
A:jax.core.kvs->' '.join((f'{k}={v}' for (k, v) in params.items() if _compact_eqn_should_include(k, v)))
A:jax.core.constvars->pp_vars(jaxpr.constvars, context, print_shapes=settings.print_shapes)
A:jax.core.invars->pp_vars(jaxpr.invars, context, print_shapes=settings.print_shapes)
A:jax.core.eqns->eqns_fn()
A:jax.core.outvars->jax._src.pretty_printer.concat([pp.text('('), pp_vars(jaxpr.outvars, context, separator=','), pp.text(')' if len(jaxpr.outvars) != 1 else ',)')])
A:jax.core.lo->max(lo, 0)
A:jax.core.hi->max(lo, min(hi, len(jaxpr.eqns)))
jax.core.AbstractToken(AbstractValue)
jax.core.AbstractToken.at_least_vspace(self)
jax.core.AbstractToken.join(self,other)
jax.core.AbstractToken.str_short(self,short_dtypes=False)
jax.core.AbstractValue
jax.core.AbstractValue.__repr__(self)
jax.core.AbstractValue.at_least_vspace(self)
jax.core.AbstractValue.join(self,other)
jax.core.AbstractValue.str_short(self,short_dtypes=False)
jax.core.AbstractValue.strip_named_shape(self)->AbstractValue
jax.core.AbstractValue.strip_weak_type(self)->AbstractValue
jax.core.AbstractValue.update(self,**kwargs)
jax.core.AxisPrimitive(Primitive)
jax.core.AxisPrimitive.bind(self,*args,**params)
jax.core.Bot(AbstractValue)
jax.core.CallPrimitive(Primitive)
jax.core.CallPrimitive.bind(self,fun,*args,**params)
jax.core.CallPrimitive.get_bind_params(self,params)
jax.core.ClosedCallPrimitive(CallPrimitive)
jax.core.ClosedCallPrimitive.get_bind_params(self,params)
jax.core.ClosedJaxpr(self,jaxpr:Jaxpr,consts:Sequence)
jax.core.ClosedJaxpr.__init__(self,jaxpr:Jaxpr,consts:Sequence)
jax.core.ClosedJaxpr.__repr__(self)
jax.core.ClosedJaxpr.__str__(self)
jax.core.ClosedJaxpr._repr_pretty_(self,p,cycle)
jax.core.ClosedJaxpr.effects(self)->Effects
jax.core.ClosedJaxpr.eqns(self)
jax.core.ClosedJaxpr.in_avals(self)
jax.core.ClosedJaxpr.literals(self)
jax.core.ClosedJaxpr.map_jaxpr(self,f)
jax.core.ClosedJaxpr.out_avals(self)
jax.core.ClosedJaxpr.pretty_print(self,*,source_info=False,print_shapes=True,name_stack=False,custom_pp_eqn_rules=True,**kw)
jax.core.ClosedJaxpr.replace(self,*,jaxpr=None,consts=None)
jax.core.ConcreteArray(self,dtype,val,weak_type=None)
jax.core.ConcreteArray.__eq__(self,other)
jax.core.ConcreteArray.__hash__(self)
jax.core.ConcreteArray.__init__(self,dtype,val,weak_type=None)
jax.core.ConcreteArray.join(self,other)->AbstractValue
jax.core.ConcreteArray.str_short(self,short_dtypes=False)->str
jax.core.ConcreteArray.update(self,dtype=None,val=None,weak_type=None)
jax.core.DArray(self,aval,data)
jax.core.DArray.__eq__(self,other)
jax.core.DArray.__hash__(self)->int
jax.core.DArray.__init__(self,aval,data)
jax.core.DArray.__repr__(self)->str
jax.core.DBIdx(NamedTuple)
jax.core.DConcreteArray(self,shape,dtype,weak_type,val)
jax.core.DConcreteArray.__init__(self,shape,dtype,weak_type,val)
jax.core.DShapedArray(self,shape,dtype,weak_type=False)
jax.core.DShapedArray.__eq__(self,other)
jax.core.DShapedArray.__hash__(self)
jax.core.DShapedArray.__init__(self,shape,dtype,weak_type=False)
jax.core.DShapedArray.at_least_vspace(self)
jax.core.DShapedArray.join(self,other)
jax.core.DShapedArray.str_short(self,short_dtypes=False)->str
jax.core.DShapedArray.update(self,shape=None,dtype=None,weak_type=None)
jax.core.DimensionHandler
jax.core.DimensionHandler.as_value(self,d:DimSize)
jax.core.DimensionHandler.diff(self,d1:DimSize,d2:DimSize)->DimSize
jax.core.DimensionHandler.dilate(self,d:DimSize,dilation:int)->DimSize
jax.core.DimensionHandler.divide_shape_sizes(self,s1:Shape,s2:Shape)->DimSize
jax.core.DimensionHandler.greater_equal(self,d1:DimSize,d2:DimSize)->bool
jax.core.DimensionHandler.is_constant(self,d:DimSize)->bool
jax.core.DimensionHandler.stride(self,d:DimSize,window_size:DimSize,window_stride:DimSize)->DimSize
jax.core.DimensionHandler.sum(self,*ds:DimSize)->DimSize
jax.core.DimensionHandler.symbolic_equal(self,d1:DimSize,d2:DimSize)->bool
jax.core.DropVar(self,aval:AbstractValue)
jax.core.DropVar.__init__(self,aval:AbstractValue)
jax.core.DropVar.__repr__(self)
jax.core.DuplicateAxisNameError(self,var)
jax.core.DuplicateAxisNameError.__init__(self,var)
jax.core.EvalTrace(Trace)
jax.core.EvalTrace.process_call(self,primitive,f,tracers,params)
jax.core.EvalTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.core.EvalTrace.process_custom_transpose(self,primitive,call,tracers,**_)
jax.core.EvalTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,**_)
jax.core.EvalTrace.process_primitive(self,primitive,tracers,params)
jax.core.EvalTrace.pure(self,x)
jax.core.InDBIdx
jax.core.InconclusiveDimensionOperation(Exception)
jax.core.Jaxpr(self,constvars:Sequence[Var],invars:Sequence[Var],outvars:Sequence[Atom],eqns:Sequence[JaxprEqn],effects:Effects=no_effects)
jax.core.Jaxpr.__init__(self,constvars:Sequence[Var],invars:Sequence[Var],outvars:Sequence[Atom],eqns:Sequence[JaxprEqn],effects:Effects=no_effects)
jax.core.Jaxpr.__str__(self)
jax.core.Jaxpr._repr_pretty_(self,p,cycle)
jax.core.Jaxpr.pretty_print(self,*,source_info=False,print_shapes=True,custom_pp_eqn_rules=True,name_stack=False,**kw)
jax.core.Jaxpr.replace(self,*,constvars=None,invars=None,outvars=None,eqns=None,effects=None)
jax.core.JaxprEqn(NamedTuple)
jax.core.JaxprEqn.__repr__(self)
jax.core.JaxprEqn.replace(self,*args,**kwargs)
jax.core.JaxprPpContext(self)
jax.core.JaxprPpContext.__init__(self)
jax.core.JaxprPpSettings(NamedTuple)
jax.core.JaxprTypeError(TypeError)
jax.core.Literal(self,val,aval)
jax.core.Literal.__init__(self,val,aval)
jax.core.Literal.__repr__(self)
jax.core.MainTrace(self,level,trace_type,**payload)
jax.core.MainTrace.__eq__(self,other:object)->bool
jax.core.MainTrace.__hash__(self)->int
jax.core.MainTrace.__init__(self,level,trace_type,**payload)
jax.core.MainTrace.__repr__(self)->str
jax.core.MainTrace.with_cur_sublevel(self)
jax.core.MapPrimitive(Primitive)
jax.core.MapPrimitive.bind(self,fun,*args,**params)
jax.core.MapPrimitive.get_bind_params(self,params)
jax.core.MapPrimitive.post_process(self,trace,out_tracers,params)
jax.core.MapPrimitive.process(self,trace,fun,tracers,params)
jax.core.NameGatheringSubst(self)
jax.core.NameGatheringSubst.__init__(self)
jax.core.NamedShape(self,*args,**kwargs)
jax.core.NamedShape.__eq__(self,other)
jax.core.NamedShape.__getitem__(self,idx)
jax.core.NamedShape.__hash__(self)
jax.core.NamedShape.__init__(self,*args,**kwargs)
jax.core.NamedShape.__str__(self)
jax.core.NamedShape.named_items(self)
jax.core.NamedShape.named_rank(self)
jax.core.NamedShape.named_sizes(self)
jax.core.NamedShape.names(self)
jax.core.NamedShape.positional(self)
jax.core.NamedShape.positional_rank(self)
jax.core.NamedShape.rank(self)
jax.core.NamedShape.total(self)
jax.core.OutDBIdx
jax.core.Primitive(self,name:str)
jax.core.Primitive.__init__(self,name:str)
jax.core.Primitive.__repr__(self)
jax.core.Primitive.abstract_eval(self,*args,**params)
jax.core.Primitive.bind(self,*args,**params)
jax.core.Primitive.bind_with_trace(self,trace,args,params)
jax.core.Primitive.def_abstract_eval(self,abstract_eval)
jax.core.Primitive.def_custom_bind(self,bind)
jax.core.Primitive.def_effectful_abstract_eval(self,effectful_abstract_eval)
jax.core.Primitive.def_impl(self,impl)
jax.core.Primitive.get_bind_params(self,params)
jax.core.Primitive.impl(self,*args,**params)
jax.core.ShapedArray(self,shape,dtype,weak_type=False,named_shape=None)
jax.core.ShapedArray.__eq__(self,other)
jax.core.ShapedArray.__hash__(self)
jax.core.ShapedArray.__init__(self,shape,dtype,weak_type=False,named_shape=None)
jax.core.ShapedArray._len(self,ignored_tracer)
jax.core.ShapedArray.at_least_vspace(self)
jax.core.ShapedArray.join(self,other)
jax.core.ShapedArray.str_short(self,short_dtypes=False)
jax.core.ShapedArray.strip_named_shape(self)
jax.core.ShapedArray.update(self,shape=None,dtype=None,weak_type=None,named_shape=None)
jax.core.Sublevel(self,level:int)
jax.core.Sublevel.__eq__(self,other)
jax.core.Sublevel.__init__(self,level:int)
jax.core.Sublevel.__lt__(self,other)
jax.core.Sublevel.__repr__(self)
jax.core.ThreadLocalState(self)
jax.core.ThreadLocalState.__init__(self)
jax.core.Token
jax.core.Trace(self,main:MainTrace,sublevel:Sublevel)
jax.core.Trace.__init__(self,main:MainTrace,sublevel:Sublevel)
jax.core.Trace.__repr__(self)
jax.core.Trace.full_raise(self,val)->Tracer
jax.core.Trace.lift(self,tracer)
jax.core.Trace.process_call(self,call_primitive,f,tracers,params)
jax.core.Trace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.core.Trace.process_custom_transpose(self,prim,call,tracers,**params)
jax.core.Trace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.core.Trace.process_map(self,map_primitive,f,tracers,params)
jax.core.Trace.process_primitive(self,primitive,tracers,params)
jax.core.Trace.pure(self,val)
jax.core.Trace.sublift(self,tracer)
jax.core.TraceStack(self)
jax.core.TraceStack.__init__(self)
jax.core.TraceStack.__repr__(self)->str
jax.core.TraceStack.copy(self)
jax.core.TraceStack.next_level(self)->int
jax.core.TraceStack.pop(self)->None
jax.core.TraceStack.push(self,main_trace:MainTrace)->None
jax.core.TraceState(self)
jax.core.TraceState.__init__(self)
jax.core.TraceState.copy(self)
jax.core.Tracer(self,trace:Trace)
jax.core.Tracer.__abs__(self)
jax.core.Tracer.__add__(self,other)
jax.core.Tracer.__and__(self,other)
jax.core.Tracer.__array__(self,*args,**kw)
jax.core.Tracer.__array_module__(self,types)
jax.core.Tracer.__bool__(self)
jax.core.Tracer.__complex__(self)
jax.core.Tracer.__copy__(self)
jax.core.Tracer.__deepcopy__(self,memo)
jax.core.Tracer.__div__(self,other)
jax.core.Tracer.__divmod__(self,other)
jax.core.Tracer.__dlpack__(self,*args,**kw)
jax.core.Tracer.__eq__(self,other)
jax.core.Tracer.__float__(self)
jax.core.Tracer.__floordiv__(self,other)
jax.core.Tracer.__ge__(self,other)
jax.core.Tracer.__getattr__(self,name)
jax.core.Tracer.__getitem__(self,idx)
jax.core.Tracer.__gt__(self,other)
jax.core.Tracer.__hex__(self)
jax.core.Tracer.__index__(self)
jax.core.Tracer.__init__(self,trace:Trace)
jax.core.Tracer.__int__(self)
jax.core.Tracer.__invert__(self)
jax.core.Tracer.__iter__(self)
jax.core.Tracer.__le__(self,other)
jax.core.Tracer.__len__(self)
jax.core.Tracer.__long__(self)
jax.core.Tracer.__lshift__(self,other)
jax.core.Tracer.__lt__(self,other)
jax.core.Tracer.__matmul__(self,other)
jax.core.Tracer.__mod__(self,other)
jax.core.Tracer.__mul__(self,other)
jax.core.Tracer.__ne__(self,other)
jax.core.Tracer.__neg__(self)
jax.core.Tracer.__nonzero__(self)
jax.core.Tracer.__oct__(self)
jax.core.Tracer.__or__(self,other)
jax.core.Tracer.__pos__(self)
jax.core.Tracer.__pow__(self,other)
jax.core.Tracer.__radd__(self,other)
jax.core.Tracer.__rand__(self,other)
jax.core.Tracer.__rdiv__(self,other)
jax.core.Tracer.__rdivmod__(self,other)
jax.core.Tracer.__reduce__(self)
jax.core.Tracer.__repr__(self)
jax.core.Tracer.__reversed__(self)
jax.core.Tracer.__rfloordiv__(self,other)
jax.core.Tracer.__rlshift__(self,other)
jax.core.Tracer.__rmatmul__(self,other)
jax.core.Tracer.__rmod__(self,other)
jax.core.Tracer.__rmul__(self,other)
jax.core.Tracer.__ror__(self,other)
jax.core.Tracer.__round__(self,ndigits=None)
jax.core.Tracer.__rpow__(self,other)
jax.core.Tracer.__rrshift__(self,other)
jax.core.Tracer.__rshift__(self,other)
jax.core.Tracer.__rsub__(self,other)
jax.core.Tracer.__rtruediv__(self,other)
jax.core.Tracer.__rxor__(self,other)
jax.core.Tracer.__setitem__(self,idx,val)
jax.core.Tracer.__sub__(self,other)
jax.core.Tracer.__truediv__(self,other)
jax.core.Tracer.__xor__(self,other)
jax.core.Tracer._assert_live(self)->None
jax.core.Tracer._contents(self)
jax.core.Tracer._origin_msg(self)->str
jax.core.Tracer._pretty_print(self)
jax.core.Tracer.at(self)
jax.core.Tracer.aval(self)
jax.core.Tracer.get_referent(self)->Any
jax.core.UnshapedArray(self,dtype,weak_type=False)
jax.core.UnshapedArray.__eq__(self,other)
jax.core.UnshapedArray.__hash__(self)
jax.core.UnshapedArray.__init__(self,dtype,weak_type=False)
jax.core.UnshapedArray.__ne__(self,other)
jax.core.UnshapedArray.__repr__(self)
jax.core.UnshapedArray.at_least_vspace(self)->AbstractValue
jax.core.UnshapedArray.join(self,other)
jax.core.UnshapedArray.shape(self)
jax.core.UnshapedArray.str_short(self,short_dtypes=False)->str
jax.core.UnshapedArray.strip_weak_type(self)
jax.core.UnshapedArray.update(self,dtype=None,weak_type=None)
jax.core.Var(self,count:int,suffix:str,aval:AbstractValue)
jax.core.Var.__init__(self,count:int,suffix:str,aval:AbstractValue)
jax.core.Var.__lt__(self,other)
jax.core.Var.__repr__(self)
jax.core._TempAxisName(self,obj)
jax.core._TempAxisName.__eq__(self,other)
jax.core._TempAxisName.__hash__(self)
jax.core._TempAxisName.__init__(self,obj)
jax.core._TempAxisName.__lt__(self,other)
jax.core._TempAxisName.__repr__(self)
jax.core._canonicalize_dimension(dim:DimSize)->DimSize
jax.core._check_call(ctx_factory,prim,in_atoms,params)
jax.core._check_closed_call(*in_atoms,call_jaxpr)
jax.core._check_jaxpr(ctx_factory:Callable[[],Tuple[JaxprPpContext,JaxprPpSettings]],jaxpr:Jaxpr)->None
jax.core._check_map(ctx_factory,prim,in_avals,params)
jax.core._compact_eqn_should_include(k:str,v:Any)->bool
jax.core._dim_handler_and_canonical(*dlist:DimSize)->Tuple[DimensionHandler, Tuple[DimSize, ...]]
jax.core._dtype_object(dtype)
jax.core._effect_free_abstract_eval(abstract_eval)
jax.core._encode_digits_alphabetic(n)
jax.core._forward_to_value(self,fun,ignored_tracer,*args)
jax.core._get_special_dim_handler(dim:DimSize)->Optional[DimensionHandler]
jax.core._initialize_jax_jit_thread_local_state()
jax.core._invalid_shape_error(shape:Shape,context:str='')
jax.core._jaxpr_type_to_callable_annotation(jaxpr:Jaxpr)->InputType
jax.core._jaxpr_vars(jaxpr)
jax.core._map_dshaped_array(size:AxisSize,axis:Optional[int],aval:DShapedArray)->DShapedArray
jax.core._map_shaped_array(size:int,axis:Optional[int],aval:ShapedArray)->ShapedArray
jax.core._param_uses_outfeed(param)
jax.core._short_dtype_name(dtype)->str
jax.core._unmap_dshaped_array(size:AxisSize,axis_name,axis:Optional[int],aval:DShapedArray)->DShapedArray
jax.core._unmap_shaped_array(size:int,axis_name,axis:Optional[int],aval:ShapedArray)->ShapedArray
jax.core._update_thread_local_jit_state(dynamic)
jax.core.apply_todos(todos,outs)
jax.core.as_named_shape(shape)->NamedShape
jax.core.axis_frame(axis_name)
jax.core.bint
jax.core.bint.__str__(self)->str
jax.core.bint.name(self)->str
jax.core.call_bind(primitive:CallPrimitive,fun,*args,**params)
jax.core.call_impl(f:lu.WrappedFun,*args,**params)
jax.core.canonicalize_dim(d:DimSize,context:str='')->DimSize
jax.core.canonicalize_shape(shape:Shape,context:str='')->Shape
jax.core.check_eqn(prim,in_avals,params)
jax.core.check_jaxpr(jaxpr:Jaxpr)
jax.core.check_type(ctx_factory:Callable[[],Tuple[JaxprPpContext,JaxprPpSettings]],env:Set[Var],ty:AbstractValue)->None
jax.core.check_valid_jaxtype(x)
jax.core.concrete_aval(x)
jax.core.concrete_or_error(force:Any,val:Any,context='')
jax.core.concretization_function_error(fun,suggest_astype=False)
jax.core.cur_sublevel()->Sublevel
jax.core.dedup_referents(itr:Iterable[Any])->List[Any]
jax.core.diff_dim(d1:DimSize,d2:DimSize)->DimSize
jax.core.diff_shape(s1:Shape,s2:Shape)->Shape
jax.core.dilate_dim(d:DimSize,dilation:DimSize)->DimSize
jax.core.dilate_shape(s:Shape,dilations:Sequence[int])->Shape
jax.core.dimension_as_value(d:DimSize)
jax.core.divide_shape_sizes(s1:Shape,s2:Shape)->DimSize
jax.core.do_subst_axis_names_jaxpr(jaxpr:Union[Jaxpr,ClosedJaxpr],subst:AxisSubst)
jax.core.ensure_compile_time_eval()
jax.core.escaped_tracer_error(tracer,detail=None)
jax.core.eval_jaxpr(jaxpr:Jaxpr,consts,*args)
jax.core.extend_axis_env(axis_name:AxisName,size:int,tag:Any)
jax.core.extend_axis_env_nd(axes:Iterable[Tuple[AxisName,int]])
jax.core.find_top_trace(xs)->Trace
jax.core.full_lower(val)
jax.core.gensym(jaxprs:Optional[Sequence[Jaxpr]]=None,suffix:str='')->Callable[[AbstractValue], Var]
jax.core.get_aval(x)
jax.core.get_referent(x:Any)->Any
jax.core.greater_equal_dim(d1:DimSize,d2:DimSize)->bool
jax.core.greater_equal_shape(s1:Shape,s2:Shape)->bool
jax.core.has_opaque_dtype(x:Any)
jax.core.is_constant_dim(d:DimSize)->bool
jax.core.is_empty_shape(s:Shape)->bool
jax.core.is_opaque_dtype(dtype)
jax.core.is_special_dim_size(v:Any)->bool
jax.core.jaxpr_as_fun(closed_jaxpr:ClosedJaxpr,*args)
jax.core.jaxpr_uses_outfeed(jaxpr:Jaxpr)->bool
jax.core.jaxprs_in_params(params)->Iterator[Jaxpr]
jax.core.join_effects(*effects:Effects)->Effects
jax.core.join_named_shapes(*named_shapes)
jax.core.lattice_join(x:Optional[AbstractValue],y:Optional[AbstractValue])->AbstractValue
jax.core.leaked_tracer_error(name:str,t,tracers:List[Tracer])->Exception
jax.core.map_bind(primitive:MapPrimitive,fun,*args,out_axes_thunk,**params)
jax.core.mapped_aval(size:AxisSize,axis:Optional[int],aval:AbstractValue)->AbstractValue
jax.core.maybe_find_leaked_tracers(x:Optional[Union[MainTrace,Sublevel]])->List[Tracer]
jax.core.new_base_main(trace_type:Type[Trace],**payload)->Generator[MainTrace, None, None]
jax.core.new_jaxpr_eqn(invars,outvars,primitive,params,effects,source_info=None)
jax.core.new_main(trace_type:Type[Trace],dynamic:bool=False,**payload)->Generator[MainTrace, None, None]
jax.core.new_sublevel()->Generator[None, None, None]
jax.core.pp_aval(a:AbstractValue,context:JaxprPpContext)->str
jax.core.pp_eqn(eqn,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_eqns(eqns,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_jaxpr(jaxpr,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_jaxpr_eqn_range(jaxpr:Jaxpr,lo:int,hi:int,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_jaxpr_skeleton(jaxpr,eqns_fn,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_jaxprs(jaxprs,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_kv_pair(k:str,v:Any,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_kv_pairs(kv_pairs,context:JaxprPpContext,settings:JaxprPpSettings)->pp.Doc
jax.core.pp_var(v:Var,context:JaxprPpContext)->str
jax.core.pp_vars(vs:Sequence[Any],context:JaxprPpContext,*,separator='',print_shapes:bool=False)->pp.Doc
jax.core.primal_dtype_to_tangent_dtype(primal_dtype)
jax.core.primitive_uses_outfeed(prim:Primitive,params:Dict)->bool
jax.core.process_env_traces_call(primitive:CallPrimitive,level:Optional[int],params_tuple:tuple,*args)
jax.core.process_env_traces_map(primitive:MapPrimitive,level:int,params_tuple:tuple,*args)
jax.core.raise_as_much_as_possible(tracer)->Tracer
jax.core.raise_to_shaped(aval:AbstractValue,weak_type=None)
jax.core.reset_trace_state()->bool
jax.core.same_referent(x:Any,y:Any)->bool
jax.core.same_shape_sizes(s1:Shape,s2:Shape)->bool
jax.core.stash_axis_env()
jax.core.str_eqn_compact(primitive_name:str,params:Dict)->str
jax.core.stride_dim(d:DimSize,window_size:DimSize,window_stride:DimSize)->DimSize
jax.core.stride_shape(s:Shape,window_size:Shape,window_stride:Shape)->Shape
jax.core.subjaxprs(jaxpr:Jaxpr)->Iterator[Jaxpr]
jax.core.subst_axis_names(primitive:Primitive,params:ParamDict,subst:AxisSubst,traverse:bool=True)->ParamDict
jax.core.subst_axis_names_eqn(eqn:JaxprEqn,subst:AxisSubst,var_map:Dict[Var,Var])->JaxprEqn
jax.core.subst_axis_names_jaxpr(jaxpr:Union[Jaxpr,ClosedJaxpr],subst:AxisSubst)
jax.core.subst_axis_names_var(v:Var,subst:AxisSubst,var_map:Dict[Var,Var])->Var
jax.core.substitute_vars_in_output_ty(out_type:Sequence[AbstractValue],in_atoms:Sequence[Atom],out_binders:Sequence[Var])->List[AbstractValue]
jax.core.sum_dim(*ds:DimSize)->DimSize
jax.core.sum_shapes(*ss:Shape)->Shape
jax.core.symbolic_equal_dim(d1:DimSize,d2:DimSize)->bool
jax.core.symbolic_equal_one_of_dim(d1:DimSize,dlist:Sequence[DimSize])->bool
jax.core.symbolic_equal_shape(s1:Shape,s2:Shape)->bool
jax.core.trace_state_clean()->bool
jax.core.traverse_jaxpr_params(f,params)
jax.core.typecheck(aval:AbstractValue,x)->bool
jax.core.typecompat(aval_ref:AbstractValue,aval:AbstractValue)->bool
jax.core.typematch(aval1:AbstractValue,aval2:AbstractValue)->bool
jax.core.unmapped_aval(size:AxisSize,axis_name,axis:Optional[int],aval:AbstractValue)->AbstractValue
jax.core.used_axis_names(primitive:Primitive,params:ParamDict)->Set[AxisName]
jax.core.used_axis_names_jaxpr(jaxpr:Union[Jaxpr,ClosedJaxpr])
jax.core.valid_jaxtype(x)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/dtypes.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/custom_batching.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/version.py----------------------------------------
A:jax.version.__version_info__->_version_as_tuple(__version__)
A:jax.version._minimum_jaxlib_version_info->_version_as_tuple(_minimum_jaxlib_version)
jax.version._version_as_tuple(version_str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/prng.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/config.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/tree_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/debug.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/dlpack.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/profiler.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/cloud_tpu_init.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/sharding.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/distributed.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/linear_util.py----------------------------------------
A:jax.linear_util._EMPTY_STORE_VALUE->EmptyStoreValue()
A:jax.linear_util.gen->gen(*gen_static_args + tuple(args), **kwargs)
A:jax.linear_util.(args, kwargs)->next(gen)
A:jax.linear_util.ans->call(fun, *args)
A:jax.linear_util.(gen, out_store)->stack.pop()
A:jax.linear_util.transformation_stack->map(transform_to_str, enumerate(self.transforms))
A:jax.linear_util.out_store->Store()
A:jax.linear_util.cache->fun_caches.setdefault(fun.f, {})
A:jax.linear_util.result->most_recent_entry()
A:jax.linear_util.thread_local.most_recent_entry->weakref.ref(ans)
A:jax.linear_util.out1->aux1()
A:jax.linear_util.out2->aux2()
jax.linear_util.EmptyStoreValue
jax.linear_util.Store(self)
jax.linear_util.Store.__init__(self)
jax.linear_util.Store.__nonzero__(self)
jax.linear_util.Store.reset(self)
jax.linear_util.Store.store(self,val)
jax.linear_util.Store.val(self)
jax.linear_util.StoreException(Exception)
jax.linear_util.WrappedFun(self,f,transforms,stores,params,in_type)
jax.linear_util.WrappedFun.__eq__(self,other)
jax.linear_util.WrappedFun.__hash__(self)
jax.linear_util.WrappedFun.__init__(self,f,transforms,stores,params,in_type)
jax.linear_util.WrappedFun.__name__(self)
jax.linear_util.WrappedFun.__repr__(self)
jax.linear_util.WrappedFun.call_wrapped(self,*args,**kwargs)
jax.linear_util.WrappedFun.populate_stores(self,stores)
jax.linear_util.WrappedFun.wrap(self,gen,gen_static_args,out_store)->WrappedFun
jax.linear_util._CacheLocalContext(self)
jax.linear_util._CacheLocalContext.__init__(self)
jax.linear_util._check_input_type(in_type:core.InputType)->None
jax.linear_util._copy_main_traces(x)
jax.linear_util.annotate(f:WrappedFun,in_type:core.InputType)->WrappedFun
jax.linear_util.cache(call:Callable)
jax.linear_util.fun_name(f)
jax.linear_util.hashable_partial(x,*args)
jax.linear_util.merge_linear_aux(aux1,aux2)
jax.linear_util.transformation(gen,fun:WrappedFun,*gen_static_args)->WrappedFun
jax.linear_util.transformation_with_aux(gen,fun:WrappedFun,*gen_static_args)->Tuple[WrappedFun, Any]
jax.linear_util.wrap_init(f,params=None)->WrappedFun


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/errors.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/collect_profile.py----------------------------------------
A:jax.collect_profile.parser->argparse.ArgumentParser(description=_DESCRIPTION)
A:jax.collect_profile.options->tensorflow.python.profiler.profiler_v2.ProfilerOptions(host_tracer_level=host_tracer_level, device_tracer_level=device_tracer_level, python_tracer_level=python_tracer_level)
A:jax.collect_profile.log_dir_->pathlib.Path(log_dir if log_dir is not None else tempfile.mkdtemp())
A:jax.collect_profile.curr_path->pathlib.Path(log_dir if log_dir is not None else tempfile.mkdtemp()).resolve()
A:jax.collect_profile.latest_folder->max(trace_folders, key=os.path.getmtime)
A:jax.collect_profile.xplane->next(latest_folder.glob('*.xplane.pb'))
A:jax.collect_profile.(result, _)->tensorboard_plugin_profile.convert.raw_to_tool_data.xspace_to_tool_data([xplane], 'trace_viewer^', {})
A:jax.collect_profile.path->jax._src.profiler._write_perfetto_trace_file(str(log_dir_))
jax.collect_profile.collect_profile(port:int,duration_in_ms:int,host:str,log_dir:Optional[str],host_tracer_level:int,device_tracer_level:int,python_tracer_level:int,no_perfetto_link:bool)
jax.collect_profile.main(args)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/test_util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/random.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/util.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/jaxpr_util.py----------------------------------------
A:jax.jaxpr_util.d->collections.defaultdict(lambda : 0)
A:jax.jaxpr_util.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax.jaxpr_util.subs->map(var_defs_and_refs, core.subjaxprs(jaxpr))
A:jax.jaxpr_util.count_width->max((len(str(v)) for v in histogram.values()))
A:jax.jaxpr_util.s->collections.defaultdict(itertools.count(1).__next__)
A:jax.jaxpr_util.func->collections.defaultdict(itertools.count(1).__next__)
A:jax.jaxpr_util.loc->collections.defaultdict(itertools.count(1).__next__)
A:jax.jaxpr_util.raw_frames->zip(*tb.raw_frames())
A:jax.jaxpr_util.json_profile->json.dumps({'string_table': list(s.keys()), 'location': locations, 'function': functions, 'sample_type': sample_type, 'sample': samples})
jax.jaxpr_util._pprof_profile(profile:Dict[Tuple[Optional[xla_client.Traceback],core.Primitive],int])->bytes
jax.jaxpr_util.all_eqns(jaxpr:core.Jaxpr)
jax.jaxpr_util.collect_eqns(jaxpr:core.Jaxpr,key:Callable)
jax.jaxpr_util.histogram(jaxpr:core.Jaxpr,key:Callable,key_fmt:Callable=lambdax:x)
jax.jaxpr_util.pprof_equation_profile(jaxpr:core.Jaxpr)->bytes
jax.jaxpr_util.primitives(jaxpr:core.Jaxpr)
jax.jaxpr_util.primitives_by_shape(jaxpr:core.Jaxpr)
jax.jaxpr_util.primitives_by_source(jaxpr:core.Jaxpr)
jax.jaxpr_util.print_histogram(histogram:Dict[Any,int])
jax.jaxpr_util.source_locations(jaxpr:core.Jaxpr)
jax.jaxpr_util.var_defs_and_refs(jaxpr:core.Jaxpr)
jax.jaxpr_util.vars_by_fanout(jaxpr:core.Jaxpr)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/custom_derivatives.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/abstract_arrays.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/multihost_utils.py----------------------------------------
A:jax.experimental.multihost_utils.in_tree->jax.device_get(_psum(in_tree))
A:jax.experimental.multihost_utils.h->numpy.int32(zlib.crc32(name.encode()))
A:jax.experimental.multihost_utils.reps->jax._src.sharding.OpShardingSharding(inp.sharding._device_assignment, sharding._get_replicated_op_sharding())
A:jax.experimental.multihost_utils.out->pjit(_identity_fn, in_axis_resources=in_axis_resources, out_axis_resources=None)(inp)
A:jax.experimental.multihost_utils.devices->numpy.array(jax.devices()).reshape(jax.process_count(), jax.local_device_count())
A:jax.experimental.multihost_utils.global_mesh->jax.experimental.maps.Mesh(devices, ('processes', 'local_devices'))
A:jax.experimental.multihost_utils.pspec->P('processes')
A:jax.experimental.multihost_utils.s->jax.sharding.MeshPspecSharding(global_mesh, pspec)
A:jax.experimental.multihost_utils.host_np_arr->numpy.expand_dims(host_np_arr, axis=0)
A:jax.experimental.multihost_utils.aval->jax.ShapedArray(host_np_arr.shape, host_np_arr.dtype)
A:jax.experimental.multihost_utils.global_aval->jax.experimental.maps.Mesh(devices, ('processes', 'local_devices'))._local_to_global(pxla._get_array_mapping(pspec), aval)
A:jax.experimental.multihost_utils.global_arr->jax._src.array.make_array_from_single_device_arrays(global_aval.shape, s, bufs)
A:jax.experimental.multihost_utils.in_axis_resources->P('processes')
A:jax.experimental.multihost_utils.inp->numpy.expand_dims(inp, axis=0)
A:jax.experimental.multihost_utils.expected->broadcast_one_to_all(in_tree)
jax.experimental.multihost_utils._handle_array_process_allgather(inp,tiled)
jax.experimental.multihost_utils._identity_fn(x)
jax.experimental.multihost_utils._psum(x:PyTreeDef)->PyTreeDef
jax.experimental.multihost_utils.assert_equal(in_tree,fail_message:str='')
jax.experimental.multihost_utils.broadcast_one_to_all(in_tree:PyTreeDef,is_source:Optional[bool]=None)->PyTreeDef
jax.experimental.multihost_utils.process_allgather(in_tree:PyTreeDef,tiled:bool=False)->PyTreeDef
jax.experimental.multihost_utils.reached_preemption_sync_point(step_id:int)->bool
jax.experimental.multihost_utils.sync_global_devices(name:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/mesh_utils.py----------------------------------------
A:jax.experimental.mesh_utils.assignable_physical_mesh->list(physical_mesh.shape)
A:jax.experimental.mesh_utils.axes->itertools.combinations(assignable_physical_mesh, num_axes)
A:jax.experimental.mesh_utils.indices->itertools.combinations(range(len(assignable_physical_mesh)), num_axes)
A:jax.experimental.mesh_utils.sorted_devices->sorted(jax_devices, key=sort_key)
A:jax.experimental.mesh_utils.(x, y, *_)->_bounds_from_last_device(sorted_devices[-1])
A:jax.experimental.mesh_utils.mesh_shape->tuple(mesh_shape)
A:jax.experimental.mesh_utils.devices->jax.devices()
A:jax.experimental.mesh_utils.device_mesh->numpy.block(blocks.tolist())
A:jax.experimental.mesh_utils.perm->numpy.array(_TRAY_RING_ORDER)
A:jax.experimental.mesh_utils.physical_mesh->_transpose_trick(physical_mesh, mesh_shape)
A:jax.experimental.mesh_utils.(device_mesh, assignment)->_create_device_mesh_for_nd_torus(physical_mesh, mesh_shape)
A:jax.experimental.mesh_utils.granule_mesh->numpy.arange(len(granules)).reshape(dcn_mesh_shape)
A:jax.experimental.mesh_utils.blocks->numpy.vectorize(lambda i: per_granule_meshes[i], otypes=[object])(granule_mesh)
jax.experimental.mesh_utils._bounds_from_last_device(last_device)->Sequence[int]
jax.experimental.mesh_utils._create_device_mesh_for_nd_torus(physical_mesh:np.ndarray,mesh_shape:Sequence[int])->Tuple[np.ndarray, List[Tuple[int, ...]]]
jax.experimental.mesh_utils._get_physical_tpu_mesh(jax_devices:Sequence[Any])->np.ndarray
jax.experimental.mesh_utils._transpose_trick(physical_mesh:np.ndarray,mesh_shape:Sequence[int])->np.ndarray
jax.experimental.mesh_utils.create_device_mesh(mesh_shape:Sequence[int],devices:Optional[Sequence[Any]]=None,*,contiguous_submeshes:bool=False)->np.ndarray
jax.experimental.mesh_utils.create_hybrid_device_mesh(mesh_shape:Sequence[int],dcn_mesh_shape:Sequence[int],devices:Optional[Sequence[Any]]=None,*,process_is_granule:bool=False)->np.ndarray


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/pjit.py----------------------------------------
A:jax.experimental.pjit.FROM_GDA->_FromGdaSingleton()
A:jax.experimental.pjit.unspecified->_is_unspecified(axis_resources[0])
A:jax.experimental.pjit.current_is_unspecified->_is_unspecified(resource)
A:jax.experimental.pjit.(args_flat, _, params, _, out_tree, _)->infer_params(*args, **kwargs)
A:jax.experimental.pjit.out_flat->jax.core.Primitive('pjit').bind(*args_flat, **params)
A:jax.experimental.pjit.outs->tree_unflatten(out_tree, out_flat)
A:jax.experimental.pjit._most_recent_pjit_call_executable->_MostRecentPjitCallExecutable()
A:jax.experimental.pjit.(outs, out_flat, out_tree)->_python_pjit_helper(infer_params, *args, **kwargs)
A:jax.experimental.pjit.fastpath_data->_PjitFastpathData(executable.xla_executable, out_tree, executable._in_shardings, executable._out_shardings, out_avals, out_committed)
A:jax.experimental.pjit.cpp_pjit_f->jax._src.lib.xla_client._xla.pjit(fun, cache_miss, static_argnums)
A:jax.experimental.pjit.in_axis_resources->tuple(in_axis_resources)
A:jax.experimental.pjit.(in_axis_resources, _, _, in_any_auto)->_prepare_axis_resources(in_axis_resources, 'in_axis_resources')
A:jax.experimental.pjit.(out_axis_resources, _, _, _)->_prepare_axis_resources(out_axis_resources, 'out_axis_resources')
A:jax.experimental.pjit.static_argnums->_ensure_index_tuple(static_argnums)
A:jax.experimental.pjit.donate_argnums->rebase_donate_argnums(donate_argnums, static_argnums)
A:jax.experimental.pjit.f->jax.core.jaxpr_as_fun(jaxpr)
A:jax.experimental.pjit.(f, dyn_args)->argnums_partial_except(f, static_argnums, args, allow_invalid=False)
A:jax.experimental.pjit.(args_flat, in_tree)->tree_flatten(dyn_args)
A:jax.experimental.pjit.(flat_fun, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax.experimental.pjit.donated_invars->donation_vector(donate_argnums, dyn_args, ())
A:jax.experimental.pjit.in_shardings->tuple((_pjit_batcher_for_sharding(i, 0, new_parts, mesh, aval.ndim) if is_mapped else i for (is_mapped, i, aval) in zip(is_mapped_in, in_shardings, new_jaxpr.in_avals)))
A:jax.experimental.pjit.out_shardings->tuple((_pjit_batcher_for_sharding(o, 0, new_parts, mesh, aval.ndim) if is_mapped else o for (is_mapped, o, aval) in zip(is_mapped_out, out_shardings, new_jaxpr.out_avals)))
A:jax.experimental.pjit.local_in_avals->treedef_tuple([in_tree, tree_flatten({})[1]]).unflatten(flat_local_in_avals)
A:jax.experimental.pjit.in_positional_semantics->tuple(tree_map(_get_in_positional_semantics, args_flat))
A:jax.experimental.pjit.(global_in_avals, canonicalized_in_shardings_flat)->_process_in_axis_resources(hashable_pytree(in_shardings), local_in_avals, in_tree, in_positional_semantics, tuple((isinstance(a, GDA) for a in args_flat)), resource_env)
A:jax.experimental.pjit.(jaxpr, canonicalized_out_shardings_flat)->_pjit_jaxpr(flat_fun, hashable_pytree(out_shardings), global_in_avals, HashableFunction(out_tree, closure=()))
A:jax.experimental.pjit.canonicalized_in_shardings_flat->tuple((i if _is_from_gda(i) or _is_auto(i) else to_op_sharding_sharding(i, aval.ndim) for (i, aval) in safe_zip(in_shardings_flat, local_in_avals)))
A:jax.experimental.pjit.params->dict(jaxpr=jaxpr, in_shardings=canonicalized_in_shardings_flat, out_shardings=canonicalized_out_shardings_flat, resource_env=resource_env, donated_invars=donated_invars, name=getattr(flat_fun, '__name__', '<unnamed function>'), in_positional_semantics=in_positional_semantics, out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.wrapped->_python_pjit(fun, infer_params)
A:jax.experimental.pjit.(args_flat, flat_local_in_avals, params, in_tree, out_tree, donate_argnums)->infer_params(*args, _global_avals=_global_avals, **kwargs)
A:jax.experimental.pjit.in_is_global->_calc_is_global_sequence(known_params['in_positional_semantics'], known_params['in_shardings'])
A:jax.experimental.pjit.lowering->_pjit_lower(params['jaxpr'], in_shardings, params['out_shardings'], params['resource_env'], params['donated_invars'], params['name'], in_is_global, always_lower=True)
A:jax.experimental.pjit.args_kwargs_in_tree->treedef_tuple([in_tree, tree_flatten({})[1]])
A:jax.experimental.pjit.(vals, treedef)->tree_flatten(pytree)
A:jax.experimental.pjit.vals->tuple(vals)
A:jax.experimental.pjit.axis_tree->tree_map(lambda parsed: parsed.spec, shardings)
A:jax.experimental.pjit.dummy_tree->tree_unflatten(tree, [PytreeLeaf()] * tree.num_leaves)
A:jax.experimental.pjit.errors->prefix_errors(axis_tree, dummy_tree)
A:jax.experimental.pjit.in_shardings_flat->flatten_axis_resources('pjit in_axis_resources', in_tree, in_shardings_thunk(), tupled_args=True)
A:jax.experimental.pjit.canonicalized_shardings->tuple((i if _is_unspecified_or_from_gda_or_auto(i) else to_op_sharding_sharding(i, aval.ndim) for (i, aval) in safe_zip(in_shardings_flat, global_in_avals)))
A:jax.experimental.pjit.in_axis_resources_flat->tuple((i if _is_from_gda(i) or _is_auto(i) else i._parsed_pspec for i in in_shardings_flat))
A:jax.experimental.pjit.global_in_avals->local_to_global(in_positional_semantics, local_in_avals, canonicalized_in_shardings_flat, resource_env.physical_mesh)
A:jax.experimental.pjit.(jaxpr, global_out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, global_in_avals)
A:jax.experimental.pjit.jaxpr->jax.core.ClosedJaxpr(jaxpr, consts)
A:jax.experimental.pjit.out_shardings_flat->flatten_axis_resources('pjit out_axis_resources', out_tree(), out_shardings_thunk(), tupled_args=False)
A:jax.experimental.pjit.canonicalized_out_shardings_flat->tuple((o if _is_unspecified(o) or _is_auto(o) else to_op_sharding_sharding(o, aval.ndim) for (o, aval) in safe_zip(out_shardings_flat, global_out_avals)))
A:jax.experimental.pjit.op_sharding->s._to_xla_op_sharding(len(shape))
A:jax.experimental.pjit.(num_ways_dim_sharded, _)->jax.interpreters.pxla._get_num_ways_dim_sharded(cast(xc.OpSharding, op_sharding))
A:jax.experimental.pjit.self.partitions->tuple(partitions)
A:jax.experimental.pjit.new_partitions->tuple_insert(parts, dim, val)
A:jax.experimental.pjit.axis_spec->tuple(axis_spec)
A:jax.experimental.pjit.partitions->list(parsed_pspec.partitions)
A:jax.experimental.pjit.REPLICATED->CanonicalizedParsedPartitionSpec(ParsedPartitionSpec(None, ()))
A:jax.experimental.pjit.(entries, treedef)->tree_flatten(axis_resources, is_leaf=lambda x: x is None)
A:jax.experimental.pjit.any_auto->jax.interpreters.pxla._check_if_any_auto(it.chain(in_shardings, out_shardings))
A:jax.experimental.pjit.resource_counts->Counter(it.chain.from_iterable(constrained_dims))
A:jax.experimental.pjit.pjit_p->jax.core.Primitive('pjit')
A:jax.experimental.pjit.op->getattr(pjit_in_s, '_original_sharding', pjit_in_s)
A:jax.experimental.pjit.compiled->_pjit_lower(known_params['jaxpr'], known_params['in_shardings'], known_params['out_shardings'], known_params['resource_env'], known_params['donated_invars'], known_params['name'], in_is_global, always_lower=False).compile(_allow_propagation_to_outputs=True, _allow_compile_replicated=False)
A:jax.experimental.pjit.fingerprint->fingerprint.hex().hex()
A:jax.experimental.pjit.shardings_hash->tuple((s._op_sharding_hash if isinstance(s, OpShardingSharding) else s for s in self.shardings))
A:jax.experimental.pjit.da->_fast_path_get_device_assignment(it.chain(in_shardings, out_shardings))
A:jax.experimental.pjit.fun->jax.linear_util.wrap_init(f)
A:jax.experimental.pjit.output_types->safe_map(mlir.aval_to_ir_types, ctx.avals_out)
A:jax.experimental.pjit.flat_output_types->jax._src.util.flatten(output_types)
A:jax.experimental.pjit.sub_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(ctx.module_context.name_stack, wrap_name(name, 'pjit')))
A:jax.experimental.pjit.func->jax.interpreters.mlir.lower_jaxpr_to_fun(sub_ctx, f'pjit_{name}', jaxpr, (), arg_shardings=arg_shardings, result_shardings=result_shardings, use_sharding_annotations=False)
A:jax.experimental.pjit.call->jax._src.lib.mlir.dialects.func.CallOp(flat_output_types, ir.FlatSymbolRefAttr.get(func.name.value), mlir.flatten_lowering_ir_args(args))
A:jax.experimental.pjit.(new_jaxpr, is_mapped_out)->jax.interpreters.batching.batch_jaxpr(jaxpr, axis_size, is_mapped_in, instantiate=False, axis_name=axis_name, main_type=main_type)
A:jax.experimental.pjit.vals_out->jax.core.Primitive('pjit').bind(*vals_in, jaxpr=new_jaxpr, in_shardings=in_shardings, out_shardings=out_shardings, resource_env=resource_env, donated_invars=donated_invars, name=name, in_positional_semantics=in_positional_semantics, out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.batching.spmd_axis_primitive_batchers[pjit_p]->partial(_pjit_batcher, False)
A:jax.experimental.pjit.batching.axis_primitive_batchers[pjit_p]->partial(_pjit_batcher, False, None)
A:jax.experimental.pjit.pxla.spmd_primitive_batchers[pjit_p]->partial(_pjit_batcher, True, None)
A:jax.experimental.pjit.new_op->s._op_sharding.clone()
A:jax.experimental.pjit.tad->list(new_op.tile_assignment_dimensions)
A:jax.experimental.pjit.parsed_pspec->parsed_pspec.insert_axis_partitions(dim, val).insert_axis_partitions(dim, val)
A:jax.experimental.pjit.mps->jax._src.sharding.MeshPspecSharding._from_parsed_pspec(mesh, parsed_pspec)
A:jax.experimental.pjit.(jaxpr_jvp, is_nz_tangents_out)->jax.interpreters.ad.jvp_jaxpr(jaxpr, is_nz_tangents_in, instantiate=False)
A:jax.experimental.pjit._filter_zeros_in->partial(_filter_zeros, is_nz_tangents_in)
A:jax.experimental.pjit._filter_zeros_out->partial(_filter_zeros, is_nz_tangents_out)
A:jax.experimental.pjit.outputs->jax.core.Primitive('pjit').bind(*primals_in, *_filter_zeros_in(tangents_in), jaxpr=jaxpr_jvp, in_shardings=(*in_shardings, *_filter_zeros_in(in_shardings)), out_shardings=(*out_shardings, *_filter_zeros_out(out_shardings)), resource_env=resource_env, donated_invars=(*donated_invars, *_filter_zeros_in(donated_invars)), name=wrap_name(name, 'jvp'), in_positional_semantics=(*in_positional_semantics, *_filter_zeros_in(in_positional_semantics)), out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.(primals_out, tangents_out)->split_list(outputs, [len(jaxpr.jaxpr.outvars)])
A:jax.experimental.pjit.tangents_out_it->iter(tangents_out)
A:jax.experimental.pjit.known_ins->tuple((pv.is_known() for pv in in_pvals))
A:jax.experimental.pjit.unknown_ins->tuple((not k for k in known_ins))
A:jax.experimental.pjit.(known_jaxpr, unknown_jaxpr, unknown_outs, res_avals)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)
A:jax.experimental.pjit.unknown_outs->tuple(unknown_outs)
A:jax.experimental.pjit.known_outs->tuple((not uk for uk in unknown_outs))
A:jax.experimental.pjit.num_residuals->len(res_avals)
A:jax.experimental.pjit.known_params->dict(jaxpr=known_jaxpr, in_shardings=keep_where(in_shardings, known_ins), out_shardings=keep_where(out_shardings, known_outs) + residual_shardings, resource_env=resource_env, donated_invars=keep_where(donated_invars, known_ins), name=name, in_positional_semantics=keep_where(in_positional_semantics, known_ins), out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.(_, out_op_shardings)->_get_op_sharding_from_executable(compiled.xla_executable)
A:jax.experimental.pjit.residual_op_shardings->tuple(out_op_shardings[-num_residuals:])
A:jax.experimental.pjit.residual_shardings->tuple((OpShardingSharding(da, op) for op in residual_op_shardings))
A:jax.experimental.pjit.all_known_outs->jax.core.Primitive('pjit').bind(*(pv.get_known() for pv in in_pvals if pv.is_known()), **known_params)
A:jax.experimental.pjit.(known_out_vals, residual_vals)->split_list(all_known_outs, [len(all_known_outs) - num_residuals])
A:jax.experimental.pjit.unknown_jaxpr->jax.interpreters.partial_eval.move_binders_to_back(unknown_jaxpr, [True] * num_residuals + [False] * sum(unknown_ins))
A:jax.experimental.pjit.unknown_params->dict(jaxpr=unknown_jaxpr, in_shardings=keep_where(in_shardings, unknown_ins) + residual_shardings, out_shardings=keep_where(out_shardings, unknown_outs), resource_env=resource_env, donated_invars=keep_where(donated_invars, unknown_ins) + (False,) * num_residuals, name=name, in_positional_semantics=keep_where(in_positional_semantics, unknown_ins) + (out_positional_semantics,) * num_residuals, out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.eqn->jax.interpreters.partial_eval.new_eqn_recipe((*unknown_tracers_in, *residual_tracers), unknown_tracers_out, pjit_p, unknown_params, unknown_jaxpr.effects, source_info_util.current())
A:jax.experimental.pjit.body->jax.linear_util.hashable_partial(body, jaxpr, reduce_axes, False)
A:jax.experimental.pjit.(primals_and_nz_cts_in, in_treedef)->tree_flatten((primals_in, cts_in))
A:jax.experimental.pjit.(body, cts_out_treedef_thunk)->flatten_fun_nokwargs(body, in_treedef)
A:jax.experimental.pjit.global_cts_in_avals->local_to_global(transpose_in_positional_semantics, global_cts_in_avals, transpose_in_shardings, resource_env.physical_mesh)
A:jax.experimental.pjit.(transpose_jaxpr, global_cts_out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(body, global_cts_in_avals)
A:jax.experimental.pjit.transpose_jaxpr->jax.core.ClosedJaxpr(transpose_jaxpr, consts)
A:jax.experimental.pjit.cts_out_treedef->cts_out_treedef_thunk()
A:jax.experimental.pjit.transpose_out_shardings->prune_type(ad.Zero, in_shardings, tree_unflatten(cts_out_treedef, [object()] * cts_out_treedef.num_leaves))
A:jax.experimental.pjit.nz_cts_out->jax.core.Primitive('pjit').bind(*primals_and_nz_cts_in, jaxpr=transpose_jaxpr, in_shardings=transpose_in_shardings, out_shardings=transpose_out_shardings, resource_env=resource_env, donated_invars=(False,) * len(primals_and_nz_cts_in), name=name, in_positional_semantics=transpose_in_positional_semantics, out_positional_semantics=out_positional_semantics)
A:jax.experimental.pjit.pjit_resources->set(it.chain.from_iterable([d for d in pos_axis_resources if d is not None]))
A:jax.experimental.pjit.aval_resources->set(it.chain.from_iterable((named_axis_resources[a] for a in aval.named_shape)))
A:jax.experimental.pjit.(x_flat, tree)->tree_flatten(x)
A:jax.experimental.pjit.(axis_resources, _, _, _)->_prepare_axis_resources(axis_resources, 'axis_resources', allow_unconstrained_dims=True)
A:jax.experimental.pjit.axis_resources_flat->tuple(flatten_axes('with_sharding_constraint sharding', tree, axis_resources))
A:jax.experimental.pjit.sharding_constraint_p->jax.core.Primitive('sharding_constraint')
A:jax.experimental.pjit.sharding->OpShardingSharding(mps._device_assignment, mps._to_xla_op_sharding(aval.ndim, axis_ctx=axis_ctx))
A:jax.experimental.pjit.y->jax.core.Primitive('sharding_constraint').bind(x, sharding=_pjit_batcher_for_sharding(sharding, d, new_parts, resource_env.physical_mesh, x.ndim), resource_env=resource_env, unconstrained_dims=unconstrained_dims)
A:jax.experimental.pjit.batching.spmd_axis_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, False)
A:jax.experimental.pjit.batching.axis_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, False, None)
A:jax.experimental.pjit.pxla.spmd_primitive_batchers[sharding_constraint_p]->partial(_sharding_constraint_batcher, True, None)
A:jax.experimental.pjit.op_sharding_sharding->OpShardingSharding(s._device_assignment, s._to_xla_op_sharding(ndim))
A:jax.experimental.pjit.gda_sharding->jax.interpreters.pxla._create_mesh_pspec_sharding(arg.mesh, arg.mesh_axes)
A:jax.experimental.pjit.sizes->numpy.fromiter(named_sizes.values(), dtype=np.int64)
A:jax.experimental.pjit.strides->strides_for_sizes(sizes)
A:jax.experimental.pjit.dims->list(reversed(dims))
A:jax.experimental.pjit.flat_assignment->numpy.asarray(assignment, dtype=np.int64)
A:jax.experimental.pjit.mesh_axis_order->unflatten_array(mesh.shape, op_sharding.tile_assignment_devices)
A:jax.experimental.pjit.mesh_axis->iter(mesh_axis_order)
A:jax.experimental.pjit.axis->next(mesh_axis)
A:jax.experimental.pjit.out_op_shardings->_get_op_sharding(output_shardings_from_xla)
A:jax.experimental.pjit.out_ppspec->parse_flatten_op_sharding(output_op_sharding, mesh)
A:jax.experimental.pjit.(in_ppspec, out_ppspec)->_get_ppspec_from_executable(executable, mesh)
A:jax.experimental.pjit.out_partition_spec->_get_partition_spec(out_ppspec)
A:jax.experimental.pjit.in_partition_spec->_get_partition_spec(in_ppspec)
A:jax.experimental.pjit.arr->numpy.array(arr)
A:jax.experimental.pjit.local_sharding->MeshPspecSharding(global_mesh.local_mesh, pspec)
A:jax.experimental.pjit.global_aval->global_mesh._local_to_global(pxla._get_array_mapping(pspec), core.ShapedArray(arr.shape, arrays[0].dtype))
A:jax.experimental.pjit.(flattened_inps, in_tree)->tree_flatten(local_inputs)
A:jax.experimental.pjit.in_pspecs->flatten_axis_resources('input pspecs', in_tree, pspecs, tupled_args=True)
A:jax.experimental.pjit.out->tree_map(_convert, tuple(flattened_inps), out_pspecs)
A:jax.experimental.pjit.local_aval->global_mesh._global_to_local(pxla._get_array_mapping(pspec), arr.aval)
A:jax.experimental.pjit.(flattened_inps, out_tree)->tree_flatten(global_inputs)
A:jax.experimental.pjit.out_pspecs->flatten_axis_resources('output pspecs', out_tree, pspecs, tupled_args=True)
jax.experimental.pjit.CanonicalizedParsedPartitionSpec(self,parsed_pspec:ParsedPartitionSpec)
jax.experimental.pjit.CanonicalizedParsedPartitionSpec.__init__(self,parsed_pspec:ParsedPartitionSpec)
jax.experimental.pjit.CanonicalizedParsedPartitionSpec.__repr__(self)
jax.experimental.pjit.ParsedPartitionSpec(self,user_spec,partitions,sync=SpecSync.IN_SYNC)
jax.experimental.pjit.ParsedPartitionSpec.__eq__(self,other)
jax.experimental.pjit.ParsedPartitionSpec.__getitem__(self,i)
jax.experimental.pjit.ParsedPartitionSpec.__hash__(self)
jax.experimental.pjit.ParsedPartitionSpec.__init__(self,user_spec,partitions,sync=SpecSync.IN_SYNC)
jax.experimental.pjit.ParsedPartitionSpec.__iter__(self)
jax.experimental.pjit.ParsedPartitionSpec.__len__(self)
jax.experimental.pjit.ParsedPartitionSpec.__repr__(self)
jax.experimental.pjit.ParsedPartitionSpec.from_user_input(cls,entry,arg_name,allow_unconstrained_dims=False)
jax.experimental.pjit.ParsedPartitionSpec.get_partition_spec(self)->PartitionSpec
jax.experimental.pjit.ParsedPartitionSpec.insert_axis_partitions(self,dim,val)
jax.experimental.pjit.ParsedPartitionSpec.unsynced_user_spec(self,min_sync)
jax.experimental.pjit.ParsedPartitionSpec.user_spec(self)
jax.experimental.pjit.PytreeLeaf
jax.experimental.pjit.PytreeLeaf.__repr__(self)
jax.experimental.pjit.SameDeviceAssignmentTuple
jax.experimental.pjit.SameDeviceAssignmentTuple.__eq__(self,other)
jax.experimental.pjit.SameDeviceAssignmentTuple.__hash__(self)
jax.experimental.pjit.SpecSync(IntEnum)
jax.experimental.pjit._FromGdaSingleton
jax.experimental.pjit._ListWithW(list)
jax.experimental.pjit._MostRecentPjitCallExecutable(self)
jax.experimental.pjit._MostRecentPjitCallExecutable.__init__(self)
jax.experimental.pjit._PjitFastpathData(NamedTuple)
jax.experimental.pjit._calc_is_global_sequence(in_positional_semantics,in_shardings)
jax.experimental.pjit._check_all_or_none_unspecified(axis_resources,name)
jax.experimental.pjit._check_resources_against_named_axes(what,aval,pos_axis_resources,named_axis_resources)
jax.experimental.pjit._check_resources_mismatch(in_axis_resources_flat,is_gda)
jax.experimental.pjit._check_unique_resources(axis_resources,arg_name)
jax.experimental.pjit._cpp_pjit(fun:Callable,infer_params,static_argnums)
jax.experimental.pjit._create_mesh_pspec_sharding_from_parsed_pspec(mesh,x)
jax.experimental.pjit._create_sharding_for_array(mesh,x)
jax.experimental.pjit._fast_path_get_device_assignment(shardings:Iterable[PjitSharding])->Optional[XLADeviceAssignment]
jax.experimental.pjit._get_in_positional_semantics(arg)->maps._PositionalSemantics
jax.experimental.pjit._get_op_sharding(op_sharding)->Sequence[xc.OpSharding]
jax.experimental.pjit._get_op_sharding_from_executable(executable)->Tuple[Sequence[xc.OpSharding], Sequence[xc.OpSharding]]
jax.experimental.pjit._get_partition_spec(ppspec:Sequence[ParsedPartitionSpec])->Sequence[PartitionSpec]
jax.experimental.pjit._get_ppspec_from_executable(executable,mesh)->Tuple[Sequence[ParsedPartitionSpec], Sequence[ParsedPartitionSpec]]
jax.experimental.pjit._get_pspec_from_executable(executable,mesh:pxla.Mesh)->Tuple[Tuple[PartitionSpec, ...], Tuple[PartitionSpec, ...]]
jax.experimental.pjit._is_from_gda(x)
jax.experimental.pjit._is_unspecified_or_from_gda_or_auto(x)
jax.experimental.pjit._maybe_check_pjit_gda_mesh(args,mesh)
jax.experimental.pjit._maybe_replace_from_gda_with_pspec(in_shardings_flat,args_flat)->Sequence[XLACompatibleSharding]
jax.experimental.pjit._pjit_abstract_eval(*args,jaxpr,out_shardings,resource_env,out_positional_semantics,**_)
jax.experimental.pjit._pjit_batcher(insert_axis,spmd_axis_name,axis_size,axis_name,main_type,vals_in,dims_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._pjit_batcher_for_sharding(s:OpShardingSharding,dim:int,val:Tuple[str,...],mesh,ndim:int)
jax.experimental.pjit._pjit_call_impl(*args,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._pjit_jaxpr(fun,out_shardings_thunk,global_in_avals,out_tree)
jax.experimental.pjit._pjit_jvp(primals_in,tangents_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._pjit_lower(jaxpr:core.ClosedJaxpr,in_shardings,out_shardings,*args,**kwargs)
jax.experimental.pjit._pjit_lower_cached(jaxpr:core.ClosedJaxpr,sdat_in_shardings:SameDeviceAssignmentTuple,sdat_out_shardings:SameDeviceAssignmentTuple,resource_env,donated_invars,name:str,in_is_global:Sequence[bool],always_lower:bool)
jax.experimental.pjit._pjit_lowering(ctx,*args,name,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._pjit_partial_eval(trace,*in_tracers,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._pjit_transpose(reduce_axes,cts_in,*primals_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax.experimental.pjit._prepare_axis_resources(axis_resources,arg_name,allow_unconstrained_dims=False)
jax.experimental.pjit._process_in_axis_resources(in_shardings_thunk,local_in_avals,in_tree,in_positional_semantics,is_gda,resource_env)
jax.experimental.pjit._python_pjit(fun:Callable,infer_params)
jax.experimental.pjit._python_pjit_helper(infer_params,*args,**kwargs)
jax.experimental.pjit._resolve_in_shardings(args,pjit_in_shardings,out_shardings,pjit_mesh)
jax.experimental.pjit._resource_typing_pjit(avals,params,source_info,resource_env,named_axis_resources)
jax.experimental.pjit._resource_typing_sharding_constraint(avals,params,source_info,resource_env,named_axis_resources)
jax.experimental.pjit._sharding_constraint_batcher(insert_axis,spmd_axis_name,axis_size,axis_name,main_type,vals_in,dims_in,sharding,resource_env,unconstrained_dims)
jax.experimental.pjit._sharding_constraint_impl(x,sharding,resource_env,unconstrained_dims)
jax.experimental.pjit._sharding_constraint_mhlo_lowering(ctx,x_node,*,sharding,resource_env,unconstrained_dims)
jax.experimental.pjit.explode_superdims(sizes,dims)
jax.experimental.pjit.flatten_axis_resources(what,tree,shardings,tupled_args)
jax.experimental.pjit.get_array_mapping(axis_resources:Union[ParsedPartitionSpec,_AUTOAxisResource,_UnspecifiedValue])->pxla.ArrayMappingOrAutoOrUnspecified
jax.experimental.pjit.get_unconstrained_dims(sharding:MeshPspecSharding)
jax.experimental.pjit.global_array_to_host_local_array(global_inputs,global_mesh,pspecs)
jax.experimental.pjit.global_to_local(positional_semantics,avals,shardings,mesh)
jax.experimental.pjit.hashable_pytree(pytree)
jax.experimental.pjit.host_local_array_to_global_array(local_inputs,global_mesh,pspecs)
jax.experimental.pjit.local_to_global(positional_semantics,avals,shardings,mesh)
jax.experimental.pjit.parse_flatten_op_sharding(op_sharding:xc.OpSharding,mesh:pxla.Mesh)->Sequence[ParsedPartitionSpec]
jax.experimental.pjit.pjit(fun:Callable,in_axis_resources=_UNSPECIFIED,out_axis_resources=_UNSPECIFIED,static_argnums:Union[int,Sequence[int]]=(),donate_argnums:Union[int,Sequence[int]]=())->stages.Wrapped
jax.experimental.pjit.pjit_check_aval_sharding(shardings,flat_avals,what_aval:str,allow_uneven_sharding:bool)
jax.experimental.pjit.strides_for_sizes(sizes)
jax.experimental.pjit.to_op_sharding_sharding(s:XLACompatibleSharding,ndim:int)->OpShardingSharding
jax.experimental.pjit.unflatten_array(named_sizes,assignment)
jax.experimental.pjit.unflatten_superdims(assignment)
jax.experimental.pjit.with_sharding_constraint(x,axis_resources)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jet.py----------------------------------------
A:jax.experimental.jet.(order,)->set(map(len, series))
A:jax.experimental.jet.treedef->tree_structure(t)
A:jax.experimental.jet.(f, out_tree)->flatten_fun_output(lu.wrap_init(fun))
A:jax.experimental.jet.(out_primals, out_terms)->unzip2(((t.primal, t.terms) for t in out_tracers))
A:jax.experimental.jet.trace->JetTrace(main, core.cur_sublevel())
A:jax.experimental.jet.in_tracers->map(partial(JetTracer, trace), primals, series)
A:jax.experimental.jet.out_tracers->map(trace.full_raise, ans)
A:jax.experimental.jet.(primals_in, series_in)->unzip2(((t.primal, t.terms) for t in tracers))
A:jax.experimental.jet.(out_flat, out_tree_def)->tree_flatten((primals_out, series_out))
A:jax.experimental.jet.(primal_out, terms_out)->rule(primals_in, series_in, **params)
A:jax.experimental.jet.(primals_and_series, in_tree_def)->tree_flatten((primals_in, series_in))
A:jax.experimental.jet.(f_jet, out_tree_def)->traceable(jet_subtrace(f, self.main), in_tree_def)
A:jax.experimental.jet.update_params->call_param_updaters.get(call_primitive)
A:jax.experimental.jet.result->call_primitive.bind(f_jet, *primals_and_series, **new_params)
A:jax.experimental.jet.(primals_out, series_out)->tree_unflatten(out_tree_def(), result)
A:jax.experimental.jet.(primals, series)->tree_unflatten(treedef, x)
A:jax.experimental.jet.(out, treedef)->tree_flatten((primals, series))
A:jax.experimental.jet.zero_term->ZeroTerm()
A:jax.experimental.jet.zero_series->ZeroSeries()
A:jax.experimental.jet.jet_rules[prim]->partial(jet, comp)
A:jax.experimental.jet.primal_out->bind(operand, scatter_indices, updates)
A:jax.experimental.jet.jet_rules[lax.cumprod_p]->partial(_cumulative_jet_rule, combine_fn=lax.mul)
A:jax.experimental.jet.jet_rules[lax.cummax_p]->partial(_cumulative_jet_rule, combine_fn=lax.max)
A:jax.experimental.jet.jet_rules[lax.cummin_p]->partial(_cumulative_jet_rule, combine_fn=lax.min)
A:jax.experimental.jet.(c0, cs)->jet(lambda x: lax.div(one, 1 + lax.square(x)), (x,), (series,))
A:jax.experimental.jet.k->len(series)
A:jax.experimental.jet.(x, series)->jet(lax.div, primals_in, series_in)
A:jax.experimental.jet.vu->sum((_scale(k, j) * v[k - j] * u[j] for j in range(1, k + 1)))
A:jax.experimental.jet.uv->sum((_scale(k, j) * u[k - j] * v[j] for j in range(1, k)))
A:jax.experimental.jet.v[k]->jax.numpy.where(x == 0, 0, fact(k - 1) * (y * vu - uv) / x)
A:jax.experimental.jet.(primal_out, series_out)->_logistic_taylor((primals_in,), (series_in,))
A:jax.experimental.jet.conv->sum((scale(k, j) * v[j] * w[k - j] for j in range(0, k)))
A:jax.experimental.jet.one->jax._src.lax.lax._const(x, 1)
A:jax.experimental.jet.jet_rules[lax.sin_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 0)
A:jax.experimental.jet.jet_rules[lax.cos_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 1)
A:jax.experimental.jet.jet_rules[lax.sinh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 0)
A:jax.experimental.jet.jet_rules[lax.cosh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 1)
A:jax.experimental.jet.op->partial(prim.bind, **params)
A:jax.experimental.jet.jet_rules[lax.dot_general_p]->partial(_bilinear_taylor_rule, lax.dot_general_p)
A:jax.experimental.jet.jet_rules[lax.mul_p]->partial(_bilinear_taylor_rule, lax.mul_p)
A:jax.experimental.jet.jet_rules[lax.conv_general_dilated_p]->partial(_bilinear_taylor_rule, lax.conv_general_dilated_p)
A:jax.experimental.jet.axes->params.pop('axes', None)
A:jax.experimental.jet.location_indicators->jax.lax.convert_element_type(lax_internal._eq_meet(operand, lax.reshape(primal_out, shape)), primal_dtype)
A:jax.experimental.jet.counts->jax._src.lax.lax._reduce_sum(location_indicators, axes)
A:jax.experimental.jet.jet_rules[lax.reduce_max_p]->_gen_reduce_choose_taylor_rule(lax_internal._reduce_max)
A:jax.experimental.jet.jet_rules[lax.reduce_min_p]->_gen_reduce_choose_taylor_rule(lax_internal._reduce_min)
A:jax.experimental.jet.zero->jax.lax.full_like(x, 0, shape=())
A:jax.experimental.jet.negs->jax.lax.select(lax.lt(x, zero), lax.full_like(x, -1), lax.full_like(x, 1.0))
A:jax.experimental.jet.(x, y)->jax.numpy.broadcast_arrays(*primal_in)
A:jax.experimental.jet.max_i->jax.lax.select(xey, (x_i + y_i) / 2, max_i)
A:jax.experimental.jet.min_i->jax.lax.select(xey, (x_i + y_i) / 2, min_i)
A:jax.experimental.jet.bind->partial(lax.scatter_add_p.bind, update_jaxpr=update_jaxpr, update_consts=update_consts, dimension_numbers=dimension_numbers, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
jax.experimental.jet.JetTrace(core.Trace)
jax.experimental.jet.JetTrace.lift(self,val)
jax.experimental.jet.JetTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.experimental.jet.JetTrace.process_call(self,call_primitive,f,tracers,params)
jax.experimental.jet.JetTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.experimental.jet.JetTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.experimental.jet.JetTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.jet.JetTrace.pure(self,val)
jax.experimental.jet.JetTrace.sublift(self,val)
jax.experimental.jet.JetTracer(self,trace,primal,terms)
jax.experimental.jet.JetTracer.__init__(self,trace,primal,terms)
jax.experimental.jet.JetTracer.aval(self)
jax.experimental.jet.JetTracer.full_lower(self)
jax.experimental.jet.ZeroSeries
jax.experimental.jet.ZeroTerm
jax.experimental.jet._abs_taylor_rule(x,series_in,**params)
jax.experimental.jet._atan2_taylor(primals_in,series_in)
jax.experimental.jet._bilinear_taylor_rule(prim,primals_in,series_in,**params)
jax.experimental.jet._cumulative_jet_rule(primals_in,series_in,*,axis:int,reverse:bool,combine_fn:Callable)
jax.experimental.jet._div_taylor_rule(primals_in,series_in)
jax.experimental.jet._erf_inv_rule(primals_in,series_in)
jax.experimental.jet._exp_taylor(primals_in,series_in)
jax.experimental.jet._gather_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet._gen_reduce_choose_taylor_rule(chooser_fun)
jax.experimental.jet._get_ind(f,ind)
jax.experimental.jet._integer_pow_taylor(primals_in,series_in,*,y)
jax.experimental.jet._lax_max_taylor_rule(primal_in,series_in)
jax.experimental.jet._lax_min_taylor_rule(primal_in,series_in)
jax.experimental.jet._log_taylor(primals_in,series_in)
jax.experimental.jet._logistic_taylor(primals_in,series_in)
jax.experimental.jet._pow_taylor(primals_in,series_in)
jax.experimental.jet._scale(k,j)
jax.experimental.jet._scale2(k,j)
jax.experimental.jet._scatter_add_rule(primals_in,series_in,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax.experimental.jet._select_n_taylor_rule(primal_in,series_in,**params)
jax.experimental.jet._sinusoidal_rule(sign,prims,primals_in,series_in)
jax.experimental.jet._tanh_taylor(primals_in,series_in)
jax.experimental.jet._xla_call_param_updater(params,num_inputs)
jax.experimental.jet.def_comp(prim,comp)
jax.experimental.jet.def_deriv(prim,deriv)
jax.experimental.jet.deflinear(prim)
jax.experimental.jet.defzero(prim)
jax.experimental.jet.deriv_prop(prim,deriv,primals_in,series_in)
jax.experimental.jet.fact(n)
jax.experimental.jet.jet(fun,primals,series)
jax.experimental.jet.jet_fun(order,primals,series)
jax.experimental.jet.jet_subtrace(main,primals,series)
jax.experimental.jet.linear_prop(prim,primals_in,series_in,**params)
jax.experimental.jet.traceable(in_tree_def,*primals_and_series)
jax.experimental.jet.zero_prop(prim,primals_in,series_in,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/global_device_array.py----------------------------------------
A:jax.experimental.global_device_array.array_mapping->jax.interpreters.pxla._get_array_mapping(mesh_axes)
A:jax.experimental.global_device_array.aval->jax.core.ShapedArray(global_shape, np.float32)
A:jax.experimental.global_device_array.sharding_spec->_get_sharding_spec(global_shape, global_mesh, mesh_axes)
A:jax.experimental.global_device_array.indices->_get_indices(global_shape, global_mesh, mesh_axes)
A:jax.experimental.global_device_array.h_index->_hashed_index(index)
A:jax.experimental.global_device_array.shard_shape->get_shard_shape(global_shape, global_mesh, mesh_axes)
A:jax.experimental.global_device_array.expected_unique_shards->prod([g // s for (g, s) in safe_zip(global_shape, shard_shape) if g != 0 or s != 0])
A:jax.experimental.global_device_array.m->prod([global_mesh.shape[ma] for ma in mesh_axis])
A:jax.experimental.global_device_array.self._current_process->jax._src.lib.xla_bridge.process_index()
A:jax.experimental.global_device_array.ss->get_shard_shape(self._global_shape, self._global_mesh, self.mesh_axes)
A:jax.experimental.global_device_array.self._sharded_buffer->jax._src.lib.xla_client.ShardedBuffer.create_sharded_buffer(device_buffers)
A:jax.experimental.global_device_array.self._maybe_device_buffers->self._sharded_buffer.get_device_buffers()
A:jax.experimental.global_device_array.global_indices_rid->get_shard_indices_replica_ids(global_shape, global_mesh, mesh_axes)
A:jax.experimental.global_device_array.db->jax.interpreters.pxla._set_aval(db)
A:jax.experimental.global_device_array.device->jax.interpreters.pxla._set_aval(db).device()
A:jax.experimental.global_device_array.buf.aval->jax.core.ShapedArray(buf.shape, buf.dtype)
A:jax.experimental.global_device_array.sh->Shard(device, index, rid, buf)
A:jax.experimental.global_device_array.npy_value->numpy.empty(self.shape, self.dtype)
A:jax.experimental.global_device_array.npy_value[s.index]->numpy.asarray(s.data)
A:jax.experimental.global_device_array.local_arrays->data_callback(local_indices)
A:jax.experimental.global_device_array.dbs->data_callback(cb_inp)
A:jax.experimental.global_device_array.global_idx_rid->get_shard_indices_replica_ids(global_aval.shape, global_mesh, out_axis_resources)
A:jax.experimental.global_device_array.fast_path_args->_GdaFastPathArgs(global_idx_rid, local_devices)
jax.experimental.global_device_array.GlobalDeviceArray(self,global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes,device_buffers:Union[xb.ShardedBuffer,Sequence[DeviceArray]],_gda_fast_path_args:Optional[_GdaFastPathArgs]=None,_enable_checks:bool=True)
jax.experimental.global_device_array.GlobalDeviceArray.__array__(self,dtype=None,context=None)
jax.experimental.global_device_array.GlobalDeviceArray.__eq__(self,other:object)
jax.experimental.global_device_array.GlobalDeviceArray.__init__(self,global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes,device_buffers:Union[xb.ShardedBuffer,Sequence[DeviceArray]],_gda_fast_path_args:Optional[_GdaFastPathArgs]=None,_enable_checks:bool=True)
jax.experimental.global_device_array.GlobalDeviceArray.__repr__(self)
jax.experimental.global_device_array.GlobalDeviceArray.__str__(self)
jax.experimental.global_device_array.GlobalDeviceArray._create_local_shards(self)->Sequence[Shard]
jax.experimental.global_device_array.GlobalDeviceArray._device_buffers(self)
jax.experimental.global_device_array.GlobalDeviceArray._init_buffers(self,device_buffers)
jax.experimental.global_device_array.GlobalDeviceArray._value(self)
jax.experimental.global_device_array.GlobalDeviceArray.addressable_data(self,index)->DeviceArray
jax.experimental.global_device_array.GlobalDeviceArray.addressable_shards(self)->Sequence[Shard]
jax.experimental.global_device_array.GlobalDeviceArray.block_until_ready(self)
jax.experimental.global_device_array.GlobalDeviceArray.from_batched_callback(cls,global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes,data_callback:Callable[[Sequence[Index]],Sequence[ArrayLike]])
jax.experimental.global_device_array.GlobalDeviceArray.from_batched_callback_with_devices(cls,global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes,data_callback:Callable[[Sequence[Tuple[Index,Tuple[Device,...]]]],Sequence[DeviceArray]])
jax.experimental.global_device_array.GlobalDeviceArray.from_callback(cls,global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes,data_callback:Callable[[Index],ArrayLike])
jax.experimental.global_device_array.GlobalDeviceArray.global_shards(self)->Sequence[Shard]
jax.experimental.global_device_array.GlobalDeviceArray.is_fully_replicated(self)->bool
jax.experimental.global_device_array.GlobalDeviceArray.local_data(self,index)->DeviceArray
jax.experimental.global_device_array.GlobalDeviceArray.local_shards(self)->Sequence[Shard]
jax.experimental.global_device_array.GlobalDeviceArray.mesh(self)
jax.experimental.global_device_array.GlobalDeviceArray.mesh_axes(self)->MeshAxes
jax.experimental.global_device_array.GlobalDeviceArray.ndim(self)
jax.experimental.global_device_array.GlobalDeviceArray.shape(self)->Shape
jax.experimental.global_device_array.GlobalDeviceArray.sharding(self)
jax.experimental.global_device_array.GlobalDeviceArray.size(self)
jax.experimental.global_device_array.Shard
jax.experimental.global_device_array._GdaFastPathArgs(NamedTuple)
jax.experimental.global_device_array._gda_array_result_handler(global_aval,out_sharding,committed,is_out_sharding_from_xla)
jax.experimental.global_device_array._gda_mlir_constant_handler(val,canonicalize_types=True)
jax.experimental.global_device_array._gda_shard_arg(x,devices,indices,mode)
jax.experimental.global_device_array._get_indices(global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes)->Tuple[Index, ...]
jax.experimental.global_device_array._get_shard_indices_replica_ids_uncached(global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes)->Mapping[Device, Tuple[Index, int]]
jax.experimental.global_device_array._get_sharding_spec(global_shape,global_mesh,mesh_axes)
jax.experimental.global_device_array.get_shard_indices(global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes)->Mapping[Device, Index]
jax.experimental.global_device_array.get_shard_indices_replica_ids(global_shape:Shape,global_mesh:pxla.Mesh,mesh_axes:MeshAxes)->Mapping[Device, Tuple[Index, int]]
jax.experimental.global_device_array.get_shard_shape(global_shape,global_mesh,mesh_axes)->Shape


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/host_callback.py----------------------------------------
A:jax.experimental.host_callback.(flat_results, result_treedef)->jax._src.lib.pytree.flatten(result)
A:jax.experimental.host_callback.call_res->_call(tap_func, arg, call_with_device=tap_with_device, result_shape=None, identity=True)
A:jax.experimental.host_callback.(call_flat_results, _)->jax._src.lib.pytree.flatten(call_res)
A:jax.experimental.host_callback.printer->functools.partial(_print_tap_func, output_stream=output_stream, threshold=threshold, **kwargs)
A:jax.experimental.host_callback.(flat_args, arg_treedef)->jax._src.lib.pytree.flatten(arg)
A:jax.experimental.host_callback.params['callback']->_CallbackWrapper(callback_func, identity, call_with_device)
A:jax.experimental.host_callback.(flat_results_shape, result_treedef)->jax._src.lib.pytree.flatten(result_shape)
A:jax.experimental.host_callback.params['flat_results_aval']->tuple(flat_results_aval)
A:jax.experimental.host_callback.flat_results->jax.core.Primitive('outside_call').bind(*flat_args, **params)
A:jax.experimental.host_callback._print_tap_lock->threading.Lock()
A:jax.experimental.host_callback.kv_pairs->' '.join([f'{k}: {v}' for (k, v) in sorted(kwargs.items())])
A:jax.experimental.host_callback.id_tap_dep_p->jax.core.Primitive('id_tap_dep')
A:jax.experimental.host_callback.tangents_instantiated->tuple(map(_instantiate_zeros, tangents, primals))
A:jax.experimental.host_callback.ct_res->_instantiate_zeros(cts, arg_res)
A:jax.experimental.host_callback.ct_tap->jax.interpreters.ad.Zero(arg_tap.aval)
A:jax.experimental.host_callback.outside_call_p->jax.core.Primitive('outside_call')
A:jax.experimental.host_callback.results->list(args_to_outfeed)
A:jax.experimental.host_callback.non_empty_flat_results_aval->list(filter(lambda aval: not _aval_is_empty(aval), flat_results_aval))
A:jax.experimental.host_callback.use_outfeed->_use_outfeed(platform)
A:jax.experimental.host_callback.callback_id->_CallbackHandlerData().callback_registry.get(callback)
A:jax.experimental.host_callback.next_token->_CallbackHandlerData().receiver.add_outfeed(comp, current_token, callback_id, args_to_outfeed)
A:jax.experimental.host_callback.after_outfeed_itoken->xops.AfterAll(comp, [current_itoken, next_token])
A:jax.experimental.host_callback.array_sharding_proto->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.token_sharding_proto->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.infeed_sharding_proto->jax.interpreters.xla.tuple_sharding_proto([array_sharding_proto] * len(non_empty_flat_results_aval) + [token_sharding_proto])
A:jax.experimental.host_callback.build_infeed->functools.partial(xops.InfeedWithToken, after_outfeed_itoken, xla_client.Shape.tuple_shape(shape))
A:jax.experimental.host_callback.outs_and_token->jax.interpreters.xla.with_sharding_proto(comp, infeed_sharding_proto, build_infeed)
A:jax.experimental.host_callback.outs->xops.GetTupleElement(outs_and_token, 0)
A:jax.experimental.host_callback.next_itoken->xops.GetTupleElement(outs_and_token, 1)
A:jax.experimental.host_callback.replica_id->jax._src.lib.mlir.dialects.mhlo.ReplicaIdOp()
A:jax.experimental.host_callback.result_arrays->_outside_call_run_callback(arrays, xb.local_devices()[replica_id], send_infeed=False, identity=identity, flat_results_aval=flat_results_aval, **params)
A:jax.experimental.host_callback.(token_and_results_op, keep_alive)->backend.emit_python_callback(wrapped_callback, comp, callback_operands, result_shapes, operand_layouts=None, has_side_effects=True)
A:jax.experimental.host_callback.sharding->jax._src.lib.xla_client.OpSharding()
A:jax.experimental.host_callback.(results, next_token, keep_alive)->jax.interpreters.mlir.emit_python_callback(ctx, wrapped_callback, current_token, callback_operands, callback_operand_avals, callback_flat_results_aval, has_side_effect=True, sharding=sharding)
A:jax.experimental.host_callback.arg->jax._src.api.tree_unflatten(arg_treedef, arrays)
A:jax.experimental.host_callback.unpacked_transforms->_unpack_transforms(transforms)
A:jax.experimental.host_callback.res->jax.core.Primitive('outside_call').bind(*batched_args, **new_params)
A:jax.experimental.host_callback.(actual_flat_results, actual_result_treedef)->jax._src.lib.pytree.flatten(res)
A:jax.experimental.host_callback.canonical_flat_results->tuple(util.safe_map(xla.canonicalize_dtype, actual_flat_results))
A:jax.experimental.host_callback.actual_flat_results_aval->_values_to_avals(canonical_flat_results)
A:jax.experimental.host_callback.non_empty_canonical_flat_results->tuple(filter(lambda r: not _aval_is_empty(r), canonical_flat_results))
A:jax.experimental.host_callback.aval->jax.core.raise_to_shaped(core.get_aval(arg))
A:jax.experimental.host_callback.(jvp_flat_args, jvp_arg_treedef)->jax._src.api.tree_flatten((arg_treedef.unflatten(primals), arg_treedef.unflatten(tangents_instantiated)))
A:jax.experimental.host_callback.out_all->jax.core.Primitive('outside_call').bind(*jvp_flat_args, **dict(_add_transform(params, 'jvp'), arg_treedef=jvp_arg_treedef))
A:jax.experimental.host_callback.(out_primals_tapped, out_tangents_tapped)->jax._src.util.split_list(out_all, [len(primals)])
A:jax.experimental.host_callback.out_primals_tapped->jax.core.Primitive('outside_call').bind(*primals, **params)
A:jax.experimental.host_callback.transforms->params.get('transforms', ())
A:jax.experimental.host_callback.(primals, tangents)->jax._src.util.split_list(args, [nr_primals])
A:jax.experimental.host_callback.all_primals_known->all((p.is_known() for p in primals))
A:jax.experimental.host_callback.some_tangents_unknown->any((not t.is_known() for t in tangents))
A:jax.experimental.host_callback.(prims, _)->params['arg_treedef'].unflatten(args)
A:jax.experimental.host_callback.(_, primals_treedef)->jax._src.api.tree_flatten(prims)
A:jax.experimental.host_callback.outs_known->trace.default_process_primitive(outside_call_p, primals, dict(params, arg_treedef=primals_treedef, transforms=transforms[:-1]))
A:jax.experimental.host_callback.outs_all_unknown->trace.default_process_primitive(outside_call_p, args, params)
A:jax.experimental.host_callback.(outs_primals_unknown, outs_tangents_unknown)->jax._src.util.split_list(outs_all_unknown, [nr_primals])
A:jax.experimental.host_callback.cts_instantiated->tuple(map(_instantiate_zeros, cts, args))
A:jax.experimental.host_callback.(args_unflat, tan_unflat)->params['arg_treedef'].unflatten(args)
A:jax.experimental.host_callback.(_, vjp_arg_treedef)->jax._src.api.tree_flatten(args_unflat)
A:jax.experimental.host_callback.(cts_primals, cts_tangents)->jax._src.util.split_list(cts_instantiated, [nr_primals])
A:jax.experimental.host_callback.cts_tangents_through_tap->jax.core.Primitive('outside_call').bind(*cts_tangents, **dict(_add_transform(params, 'transpose'), arg_treedef=vjp_arg_treedef))
A:jax.experimental.host_callback.new_params->_add_transform(params, 'batch', batch_dims)
A:jax.experimental.host_callback.new_jaxpr->_rewrite_closed_jaxpr(carry_jaxpr, True, True)
A:jax.experimental.host_callback.mk_new_var->jax.core.gensym([jaxpr])
A:jax.experimental.host_callback.last_token_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.last_itoken_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.output_token_var->mk_new_var(last_token_var.aval)
A:jax.experimental.host_callback.output_itoken_var->mk_new_var(last_itoken_var.aval)
A:jax.experimental.host_callback.(cond_jaxpr, _, body_jaxpr, _)->jax._src.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.(branches, linear)->jax._src.util.split_dict(eqn.params, ['branches', 'linear'])
A:jax.experimental.host_callback.(num_consts, num_carry, carry_jaxpr, linear, _, _, _)->jax._src.util.split_dict(eqn.params, ['num_consts', 'num_carry', 'jaxpr', 'linear', 'reverse', 'length', 'unroll'])
A:jax.experimental.host_callback.call_jaxpr->cast(core.Jaxpr, eqn.params['call_jaxpr'])
A:jax.experimental.host_callback.jaxpr->cast(core.ClosedJaxpr, eqn.params['jaxpr'])
A:jax.experimental.host_callback.jaxpr_->cast(core.Jaxpr, eqn.params['jaxpr'])
A:jax.experimental.host_callback.(cond_jaxpr, cond_nconsts, body_jaxpr, body_nconsts)->jax._src.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.transformed_cond_jaxpr->_rewrite_closed_jaxpr(cond_jaxpr, True, True)
A:jax.experimental.host_callback.new_cond_pred_invar->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_cond_jaxpr->jax.core.ClosedJaxpr(core.Jaxpr([], new_cond_invars, [new_cond_pred_invar], [], set()), [])
A:jax.experimental.host_callback.transformed_body_jaxpr->_rewrite_closed_jaxpr(body_jaxpr, True, True)
A:jax.experimental.host_callback.new_body_invars_pred->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_invars_token->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_invars_itoken->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.new_body_token2->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_itoken2->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.new_body_pred2->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_token3->mk_new_var(input_token_var.aval)
A:jax.experimental.host_callback.new_body_itoken3->mk_new_var(input_itoken_var.aval)
A:jax.experimental.host_callback.effects->jax.core.join_effects(*(eqn.effects for eqn in new_body_eqns))
A:jax.experimental.host_callback.new_body_jaxpr->jax.core.ClosedJaxpr(core.Jaxpr([], new_body_invars_cond_constvars + new_body_invars_body_constvars + [new_body_invars_pred] + new_body_invars_carry + [new_body_invars_token, new_body_invars_itoken], [new_body_pred2] + new_body_carry2 + [new_body_token3, new_body_itoken3], new_body_eqns, effects), [])
A:jax.experimental.host_callback.pred_out->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.id_p->jax.core.Primitive('id')
A:jax.experimental.host_callback.self.lock->threading.Lock()
A:jax.experimental.host_callback.self.callback_registry->dict()
A:jax.experimental.host_callback.self.callback_registry_by_id->dict()
A:jax.experimental.host_callback._callback_handler_data->_CallbackHandlerData()
A:jax.experimental.host_callback.callback->_CallbackHandlerData().callback_registry_by_id.get(consumer_id)
A:jax.experimental.host_callback.formatted_e->traceback.format_exc()
A:jax.experimental.host_callback.devices->list(itertools.chain(*[backend.local_devices() for backend in clients]))
A:jax.experimental.host_callback.devices_with_outfeed->list(itertools.chain(*[backend.local_devices() for backend in clients_with_outfeed]))
A:jax.experimental.host_callback._callback_handler_data.receiver->outfeed_receiver_module.start(_callback_input_received, tuple(clients_with_outfeed), max_callback_queue_size_bytes)
A:jax.experimental.host_callback.lock->threading.Lock()
A:jax.experimental.host_callback.cv->threading.Condition(lock=lock)
A:jax.experimental.host_callback.x_on_dev->jax._src.api.device_put(d_idx, device=d)
jax.experimental.host_callback.CallbackException(Exception)
jax.experimental.host_callback._CallbackHandlerData(self)
jax.experimental.host_callback._CallbackHandlerData.__init__(self)
jax.experimental.host_callback._CallbackHandlerData.stop(self)
jax.experimental.host_callback._CallbackWrapper(self,callback_func,identity,call_with_device)
jax.experimental.host_callback._CallbackWrapper.__eq__(self,other)
jax.experimental.host_callback._CallbackWrapper.__hash__(self)
jax.experimental.host_callback._CallbackWrapper.__init__(self,callback_func,identity,call_with_device)
jax.experimental.host_callback._add_transform(params:Dict,name:str,*transform_params)->Dict
jax.experimental.host_callback._aval_is_empty(aval)->bool
jax.experimental.host_callback._call(callback_func:Callable,arg,*,result_shape=None,call_with_device=False,identity=False)
jax.experimental.host_callback._callback_input_received(device,consumer_id,arrays:Tuple)
jax.experimental.host_callback._id_tap_dep_batching_rule(batched_args,batch_dims)
jax.experimental.host_callback._id_tap_dep_jvp_rule(primals,tangents)
jax.experimental.host_callback._id_tap_dep_transpose_rule(cts,arg_res,arg_tap)
jax.experimental.host_callback._initialize_outfeed_receiver(max_callback_queue_size_bytes:int=int(256*1000000.0))
jax.experimental.host_callback._inline_host_callback()->bool
jax.experimental.host_callback._instantiate_zeros(tan,arg)
jax.experimental.host_callback._outside_call_abstract_eval(*args_a:pe.AbstractValue,identity,**params)->Sequence[pe.AbstractValue]
jax.experimental.host_callback._outside_call_batching_rule(batched_args,batch_dims,**params)
jax.experimental.host_callback._outside_call_impl(*args,**params)
jax.experimental.host_callback._outside_call_jvp_rule(primals,tangents,**params)
jax.experimental.host_callback._outside_call_lowering(ctx:mlir.LoweringRuleContext,*args,has_token:bool,identity:bool,flat_results_aval=(),**params)
jax.experimental.host_callback._outside_call_partial_eval_rule(trace,*args,**params)
jax.experimental.host_callback._outside_call_run_callback(arrays,device,*,send_infeed=True,callback,arg_treedef,identity,result_treedef=None,flat_results_aval=None,transforms=(),has_token=False)
jax.experimental.host_callback._outside_call_translation_rule(ctx,avals_in,avals_out,*args_op:XlaOp,has_token,identity,flat_results_aval=(),**params)
jax.experimental.host_callback._outside_call_transpose_rule(cts,*args,**params)
jax.experimental.host_callback._print_tap_func(arg,transforms,*,device=None,output_stream=None,threshold=1024,**kwargs)
jax.experimental.host_callback._register_callback(callback:Callable)->int
jax.experimental.host_callback._rewrite_closed_jaxpr(cjaxpr:core.ClosedJaxpr,has_input_token:bool,has_output_token:bool)->core.ClosedJaxpr
jax.experimental.host_callback._rewrite_eqn(eqn:core.JaxprEqn,eqns:List[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,input_itoken_var:core.Var,output_itoken_var:core.Var,mk_new_var:Callable[[core.AbstractValue],core.Var])
jax.experimental.host_callback._rewrite_jaxpr(jaxpr:core.Jaxpr,has_input_token:bool,has_output_token:bool)->core.Jaxpr
jax.experimental.host_callback._rewrite_while_outfeed_cond(eqn:core.JaxprEqn,eqns:List[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,input_itoken_var:core.Var,output_itoken_var:core.Var,mk_new_var:Callable)
jax.experimental.host_callback._use_outfeed(platform:str)->bool
jax.experimental.host_callback._values_to_avals(vals)->Sequence[core.ShapedArray]
jax.experimental.host_callback.barrier_wait(logging_name:Optional[str]=None)
jax.experimental.host_callback.call(callback_func:Callable,arg,*,result_shape=None,call_with_device=False)
jax.experimental.host_callback.id_print(arg,*,result=None,tap_with_device=False,output_stream=None,threshold=None,**kwargs)
jax.experimental.host_callback.id_tap(tap_func,arg,*,result=None,tap_with_device=False,**kwargs)
jax.experimental.host_callback.stop_outfeed_receiver()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/checkify.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/x64_context.py----------------------------------------
jax.experimental.disable_x64()
jax.experimental.enable_x64(new_val:bool=True)
jax.experimental.x64_context.disable_x64()
jax.experimental.x64_context.enable_x64(new_val:bool=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/ode.py----------------------------------------
A:jax.experimental.ode.y->unravel(y_flat)
A:jax.experimental.ode.(ans_flat, _)->ravel_pytree(ans)
A:jax.experimental.ode.dps_c_mid->jax.numpy.array([6025192743 / 30085553152 / 2, 0, 51252292925 / 65400821598 / 2, -2691868925 / 45128329728 / 2, 187940372067 / 1594534317056 / 2, -1776094331 / 19743644256 / 2, 11237099 / 235043384 / 2], dtype=y0.dtype)
A:jax.experimental.ode.dt->initial_step_size(func_, ts[0], y0, 4, rtol, atol, f0)
A:jax.experimental.ode.(y0, f0)->_promote_dtypes_inexact(y0, f0)
A:jax.experimental.ode.d0->jax.numpy.linalg.norm(y0 / scale.astype(dtype))
A:jax.experimental.ode.d1->jax.numpy.linalg.norm(f0 / scale.astype(dtype))
A:jax.experimental.ode.h0->jax.numpy.where((d0 < 1e-05) | (d1 < 1e-05), 1e-06, 0.01 * d0 / d1)
A:jax.experimental.ode.f1->fun(y1, t0 + h0)
A:jax.experimental.ode.h1->jax.numpy.where((d1 <= 1e-15) & (d2 <= 1e-15), jnp.maximum(1e-06, h0 * 0.001), (0.01 / jnp.max(d1 + d2)) ** (1.0 / (order + 1.0)))
A:jax.experimental.ode.alpha->jax.numpy.array([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1.0, 1.0, 0], dtype=dt.dtype)
A:jax.experimental.ode.beta->jax.numpy.array([[1 / 5, 0, 0, 0, 0, 0, 0], [3 / 40, 9 / 40, 0, 0, 0, 0, 0], [44 / 45, -56 / 15, 32 / 9, 0, 0, 0, 0], [19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729, 0, 0, 0], [9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656, 0, 0], [35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0]], dtype=f0.dtype)
A:jax.experimental.ode.c_sol->jax.numpy.array([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0], dtype=f0.dtype)
A:jax.experimental.ode.c_error->jax.numpy.array([35 / 384 - 1951 / 21600, 0, 500 / 1113 - 22642 / 50085, 125 / 192 - 451 / 720, -2187 / 6784 - -12231 / 42400, 11 / 84 - 649 / 6300, -1.0 / 60.0], dtype=f0.dtype)
A:jax.experimental.ode.ft->func(yi, ti)
A:jax.experimental.ode.k->jax.lax.fori_loop(1, 7, body_fun, k)
A:jax.experimental.ode.dfactor->jax.numpy.where(mean_error_ratio < 1, 1.0, dfactor)
A:jax.experimental.ode.factor->jax.numpy.minimum(ifactor, jnp.maximum(mean_error_ratio ** (-1.0 / order) * safety, dfactor))
A:jax.experimental.ode.(converted, consts)->jax.custom_derivatives.closure_convert(func, y0, t[0], *args)
A:jax.experimental.ode.(y0, unravel)->ravel_pytree(y0)
A:jax.experimental.ode.func->ravel_first_arg(func, unravel)
A:jax.experimental.ode.out->_odeint(func, rtol, atol, mxstep, y0, ts, *args)
A:jax.experimental.ode.(next_y, next_f, next_y_error, k)->runge_kutta_step(func_, y, f, t, dt)
A:jax.experimental.ode.error_ratio->mean_error_ratio(next_y_error, rtol, atol, y, next_y)
A:jax.experimental.ode.new_interp_coeff->interp_fit_dopri(y, next_y, k, dt)
A:jax.experimental.ode.(_, *carry)->jax.lax.while_loop(cond_fun, body_fun, [0] + carry)
A:jax.experimental.ode.y_target->jax.numpy.polyval(interp_coeff, relative_output_time.astype(interp_coeff.dtype))
A:jax.experimental.ode.f0->func_(y0, ts[0])
A:jax.experimental.ode.interp_coeff->jax.numpy.array([y0] * 5)
A:jax.experimental.ode.(_, ys)->jax.lax.scan(scan_fun, init_carry, ts[1:])
A:jax.experimental.ode.ys->_odeint(func, rtol, atol, mxstep, y0, ts, *args)
A:jax.experimental.ode.(y_dot, vjpfun)->jax.vjp(func, y, -t, *args)
A:jax.experimental.ode.(_, y_bar, t0_bar, args_bar)->odeint(aug_dynamics, (ys[i], y_bar, t0_bar, args_bar), jnp.array([-ts[i], -ts[i - 1]]), *args, rtol=rtol, atol=atol, mxstep=mxstep)
A:jax.experimental.ode.(y_bar, t0_bar, args_bar)->tree_map(op.itemgetter(1), (y_bar, t0_bar, args_bar))
A:jax.experimental.ode.((y_bar, t0_bar, args_bar), rev_ts_bar)->jax.lax.scan(scan_fun, init_carry, jnp.arange(len(ts) - 1, 0, -1))
A:jax.experimental.ode.ts_bar->jax.numpy.concatenate([jnp.array([t0_bar]), rev_ts_bar[::-1]])
jax.experimental.ode._odeint(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode._odeint_fwd(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode._odeint_rev(func,rtol,atol,mxstep,res,g)
jax.experimental.ode._odeint_wrapper(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode.abs2(x)
jax.experimental.ode.fit_4th_order_polynomial(y0,y1,y_mid,dy0,dy1,dt)
jax.experimental.ode.initial_step_size(fun,t0,y0,order,rtol,atol,f0)
jax.experimental.ode.interp_fit_dopri(y0,y1,k,dt)
jax.experimental.ode.mean_error_ratio(error_estimate,rtol,atol,y0,y1)
jax.experimental.ode.odeint(func,y0,t,*args,rtol=1.4e-08,atol=1.4e-08,mxstep=jnp.inf)
jax.experimental.ode.optimal_step_size(last_step,mean_error_ratio,safety=0.9,ifactor=10.0,dfactor=0.2,order=5.0)
jax.experimental.ode.ravel_first_arg(f,unravel)
jax.experimental.ode.ravel_first_arg_(unravel,y_flat,*args)
jax.experimental.ode.runge_kutta_step(func,y0,f0,t0,dt)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/callback.py----------------------------------------
A:jax.experimental.callback.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.callback.f->callback_subtrace(f, self.main)
A:jax.experimental.callback.(flat_fun, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax.experimental.callback.out_flat->callback_fun(flat_fun, args_flat, callback, strip_calls)
A:jax.experimental.callback.vals->tuple(it.chain(new_consts, carry_vals, xs_vals))
A:jax.experimental.callback.fun->callback_subtrace(fun, self.main)
A:jax.experimental.callback.trace->main.with_cur_sublevel()
A:jax.experimental.callback.out_tracers->map(trace.full_raise, outs)
A:jax.experimental.callback.(jaxpr_out, consts)->jax.custom_derivatives._initial_style_jaxpr(fun, avals_in)
A:jax.experimental.callback.vals_out->call_primitive.bind(f, *vals_in, **params)
A:jax.experimental.callback.jvp->callback_subtrace(jvp, self.main)
A:jax.experimental.callback.out->primitive.bind(*it.chain(new_consts, vals), fun_jaxpr=closed_fun_jaxpr, num_consts=new_num_consts, **params)
A:jax.experimental.callback.fwd->callback_subtrace(fwd, self.main)
A:jax.experimental.callback.bwd->callback_subtrace(bwd, self.main)
A:jax.experimental.callback.(const_tracers, carry_tracers, xs_tracers)->split_list(tracers, [num_consts, num_carry])
A:jax.experimental.callback.(carry_avals, xs_avals)->tree_map(lambda x: x.aval, (carry_tracers, xs_tracers))
A:jax.experimental.callback.(const_vals, carry_vals, xs_vals)->tree_map(lambda x: x.val, (const_tracers, carry_tracers, xs_tracers))
A:jax.experimental.callback.body_fun->jaxpr_as_fun(body_jaxpr)
A:jax.experimental.callback.(out_carry, y)->split_list(out, [num_carry])
A:jax.experimental.callback.new_body->callback_transform(body, trace.callback, strip_calls=trace.strip_calls)
A:jax.experimental.callback.in_tree->tree_structure(init_avals)
A:jax.experimental.callback.(new_jaxpr, new_consts, _)->jax._src.lax.control_flow._initial_style_jaxpr(new_body, in_tree, tuple(carry_avals + x_avals))
A:jax.experimental.callback.out_vals->jax.lax.scan_p.bind(*vals, reverse=reverse, length=length, num_consts=len(new_consts), num_carry=num_carry, jaxpr=new_jaxpr, linear=linear, unroll=unroll)
A:jax.experimental.callback.(cond_const_tracers, body_const_tracers, init_tracers)->split_list(tracers, [cond_nconsts, body_nconsts])
A:jax.experimental.callback.init_avals->safe_map(lambda x: x.aval, init_tracers)
A:jax.experimental.callback.(cond_const_vals, body_const_vals, init_vals)->tree_map(lambda x: x.val, (cond_const_tracers, body_const_tracers, init_tracers))
A:jax.experimental.callback.cond_fun->jaxpr_as_fun(cond_jaxpr)
A:jax.experimental.callback.new_cond->callback_transform(cond, trace.callback, strip_calls=trace.strip_calls)
A:jax.experimental.callback.(new_cond_jaxpr, new_cond_consts, _)->jax._src.lax.control_flow._initial_style_jaxpr(new_cond, in_tree, tuple(init_avals))
A:jax.experimental.callback.(new_body_jaxpr, new_body_consts, _)->jax._src.lax.control_flow._initial_style_jaxpr(new_body, in_tree, tuple(init_avals))
A:jax.experimental.callback.new_closed_jaxpr->callback_jaxpr(fun_jaxpr, trace.callback, strip_calls=trace.strip_calls)
A:jax.experimental.callback.params['bwd']->callback_subtrace(params['bwd'], main)
A:jax.experimental.callback.thunk->params.pop(thunk_name)
A:jax.experimental.callback.thunk_jaxpr->jax.core.ClosedJaxpr(*thunk())
A:jax.experimental.callback.closed_jaxpr->callback_jaxpr(thunk_jaxpr, trace.callback, trace.strip_calls)
A:jax.experimental.callback.closed_fun_jaxpr->jax.core.ClosedJaxpr(pe.convert_constvars_jaxpr(new_fun_jaxpr), ())
A:jax.experimental.callback.custom_callback_rules[cd.custom_vjp_call_jaxpr_p]->partial(_custom_derivative_call_jaxpr_callback_rule, cd.custom_vjp_call_jaxpr_p)
jax.experimental.callback.CallbackTrace(self,*args,callback,strip_calls)
jax.experimental.callback.CallbackTrace.__init__(self,*args,callback,strip_calls)
jax.experimental.callback.CallbackTrace.lift(self,val)
jax.experimental.callback.CallbackTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.callback.CallbackTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.experimental.callback.CallbackTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.experimental.callback.CallbackTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.callback.CallbackTrace.pure(self,val)
jax.experimental.callback.CallbackTrace.sublift(self,val)
jax.experimental.callback.CallbackTracer(self,trace,val)
jax.experimental.callback.CallbackTracer.__init__(self,trace,val)
jax.experimental.callback.CallbackTracer.aval(self)
jax.experimental.callback.CallbackTracer.full_lower(self)
jax.experimental.callback.FoundValue(Exception)
jax.experimental.callback._callback_fun(callback,strip_calls,*in_vals,**params)
jax.experimental.callback._check_callable(fun)
jax.experimental.callback._contains_query(vals,query)
jax.experimental.callback._custom_derivative_call_jaxpr_callback_rule(primitive,trace,*tracers,fun_jaxpr,num_consts,**params)
jax.experimental.callback._scan_callback_rule(trace,*tracers,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.experimental.callback._while_callback_rule(trace,*tracers,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax.experimental.callback.callback_fun(fun:lu.WrappedFun,in_vals,callback,strip_calls)
jax.experimental.callback.callback_jaxpr(closed_jaxpr,callback,strip_calls)
jax.experimental.callback.callback_subtrace(main,*in_vals,**params)
jax.experimental.callback.callback_transform(fun:Callable,callback:Callable,strip_calls:bool=False)->Callable
jax.experimental.callback.find_by_value(fun:Callable,queries)->Callable
jax.experimental.callback.rewrite(fun:Callable,rules)->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/maps.py----------------------------------------
A:jax.experimental.maps._positional_semantics->_PSThreadLocalState()
A:jax.experimental.maps.self.contents->dict(*args, **kwargs)
A:jax.experimental.maps.thread_resources.env->old_env.with_extra_loop(pxla._Loop(name, length))
A:jax.experimental.maps.result->jax.interpreters.pxla.vtile_manual(f, tuple(manual_mesh_axes), mesh, mesh_in_axes, mesh_out_axes).call_wrapped(*(_slice_tile(arg, spec.get(loop_name, None), i, loop_length) for (arg, spec) in zip(args, loop_in_axes)))
A:jax.experimental.maps.num_mapped_dims->sum((name is not None for name in entry))
A:jax.experimental.maps.user_repr->str(entry)
A:jax.experimental.maps.constr->partial(AxisNamePosWithRank, expected_rank=len(entry))
A:jax.experimental.maps.(entries, treedef)->tree_flatten(axes, is_leaf=_is_axes_leaf)
A:jax.experimental.maps.entries->map(partial(_parse_entry, arg_name), entries)
A:jax.experimental.maps.in_axes->tuple(in_axes)
A:jax.experimental.maps.(in_axes, in_axes_entries, _)->_prepare_axes(in_axes, 'in_axes')
A:jax.experimental.maps.(out_axes, out_axes_entries, out_axes_treedef)->_prepare_axes(out_axes, 'out_axes')
A:jax.experimental.maps.out_axes_entries->tuple(out_axes_entries)
A:jax.experimental.maps.axis_sizes_names->set(axis_sizes.keys())
A:jax.experimental.maps.in_axes_names->set(it.chain(*(spec.keys() for spec in in_axes_entries)))
A:jax.experimental.maps.out_axes_names->set(it.chain(*(spec.keys() for spec in out_axes_entries)))
A:jax.experimental.maps.name->fresh_resource_name()
A:jax.experimental.maps.resources->axis_resources.get(axis, ())
A:jax.experimental.maps.normalized_axis_resources[axis]->tuple(unsafe_map(normalize_resource, resources))
A:jax.experimental.maps.frozen_axis_resources->FrozenDict(normalized_axis_resources)
A:jax.experimental.maps.necessary_resources->set(it.chain(*frozen_axis_resources.values()))
A:jax.experimental.maps.axes_with_resources->set(frozen_axis_resources.keys())
A:jax.experimental.maps.donate_argnums->_ensure_index_tuple(donate_argnums)
A:jax.experimental.maps.has_input_rank_assertions->any((spec.expected_rank is not None for spec in in_axes_entries))
A:jax.experimental.maps.has_output_rank_assertions->any((spec.expected_rank is not None for spec in out_axes_entries))
A:jax.experimental.maps.available_resources->set(resource_env.shape.keys())
A:jax.experimental.maps.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.maps.(fun_flat, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.experimental.maps.donated_invars->donation_vector(donate_argnums, args, ())
A:jax.experimental.maps.in_axes_flat->_flatten_axes('xmap in_axes', in_tree, in_axes, tupled_args=True)
A:jax.experimental.maps.out_axes_thunk->HashableFunction(lambda : tuple(_flatten_axes('xmap out_axes', out_tree(), out_axes, tupled_args=False)), closure=(out_axes_entries, out_axes_treedef))
A:jax.experimental.maps.in_positional_semantics->tuple((_PositionalSemantics.GLOBAL if isinstance(a, GlobalDeviceArray) else _positional_semantics.val for a in args_flat))
A:jax.experimental.maps.axis_resource_count->_get_axis_resource_count(axis_resources, resource_env, in_positional_semantics)
A:jax.experimental.maps.frozen_global_axis_sizes->_get_axis_sizes(args_flat, in_axes_flat, axis_sizes, axis_resource_count, in_positional_semantics)
A:jax.experimental.maps.params->dict(name=getattr(fun, '__name__', '<unnamed function>'), in_axes=tuple(in_axes_flat), out_axes_thunk=out_axes_thunk, donated_invars=donated_invars, global_axis_sizes=frozen_global_axis_sizes, axis_resources=frozen_axis_resources, resource_env=resource_env, backend=backend, spmd_in_axes=None, spmd_out_axes_thunk=None, in_positional_semantics=in_positional_semantics, out_positional_semantics=out_positional_semantics)
A:jax.experimental.maps.f->jax.interpreters.pxla.vtile_manual(f, tuple(manual_mesh_axes), mesh, mesh_in_axes, mesh_out_axes)
A:jax.experimental.maps.(fun_flat, args_flat, params, _, out_tree)->infer_params(*args)
A:jax.experimental.maps.out_flat->XMapPrimitive().bind(fun, *all_args, **new_params)
A:jax.experimental.maps.(fun_flat, args_flat, params, in_tree, out_tree)->infer_params(*args)
A:jax.experimental.maps.computation->make_xmap_callable(fun_flat, params['name'], params['in_axes'], params['out_axes_thunk'], params['donated_invars'], params['global_axis_sizes'], params['axis_resources'], params['resource_env'], params['backend'], params['spmd_in_axes'], params['spmd_out_axes_thunk'], params['in_positional_semantics'], params['out_positional_semantics'], *avals_flat)
A:jax.experimental.maps.in_tree->treedef_tuple([in_tree, tree_flatten({})[1]])
A:jax.experimental.maps.in_avals->treedef_tuple([in_tree, tree_flatten({})[1]]).unflatten(avals_flat)
A:jax.experimental.maps.plan->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes, in_positional_semantics)
A:jax.experimental.maps.(jaxpr, out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_final(fun, mapped_in_avals)
A:jax.experimental.maps.out_axes->out_axes_thunk()
A:jax.experimental.maps.jaxpr->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('call_jaxpr')
A:jax.experimental.maps.(mesh_in_axes, mesh_out_axes)->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes, in_positional_semantics).to_mesh_axes(in_axes, out_axes)
A:jax.experimental.maps.manual_mesh_axes->frozenset(it.chain.from_iterable(plan.physical_axis_resources.values()))
A:jax.experimental.maps.tiling_method->jax.interpreters.pxla.TileVectorize()
A:jax.experimental.maps.env->dict(self.resource_env.shape)
A:jax.experimental.maps.(physical_axis_resources, loop_axis_resources)->_unzip_axis_resources(axis_resources, resource_env)
A:jax.experimental.maps.axis_subst_dict->dict(axis_resources)
A:jax.experimental.maps.map_in_axes->tuple(unsafe_map(lambda spec: spec.get(naxis, None), in_axes))
A:jax.experimental.maps.map_out_axes->tuple(unsafe_map(lambda spec: spec.get(naxis, None), out_axes))
A:jax.experimental.maps.used_loops->set(it.chain.from_iterable(self.loop_axis_resources.values()))
A:jax.experimental.maps.loop_in_axes->_to_resource_axes(in_axes, self.loop_axis_resources)
A:jax.experimental.maps.loop_out_axes->_to_resource_axes(out_axes, self.loop_axis_resources)
A:jax.experimental.maps.(_, stacked_results)->jax.lax.scan(body, 0, (), length=loop_length)
A:jax.experimental.maps.post_process->getattr(trace, 'post_process_xmap', None)
A:jax.experimental.maps.new_params->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk)
A:jax.experimental.maps.subfun->jax.linear_util.hashable_partial(lu.wrap_init(core.eval_jaxpr), jaxpr, ())
A:jax.experimental.maps.axes->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('out_axes')
A:jax.experimental.maps.new_params['out_axes_thunk']->HashableFunction(lambda : axes, closure=axes)
A:jax.experimental.maps.spmd_axes->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk, spmd_in_axes=new_spmd_in_axes, spmd_out_axes_thunk=new_spmd_out_axes_thunk).pop('spmd_out_axes')
A:jax.experimental.maps.new_params['spmd_out_axes_thunk']->HashableFunction(lambda : spmd_axes, closure=spmd_axes)
A:jax.experimental.maps.xmap_p->XMapPrimitive()
A:jax.experimental.maps.new_jaxpr->jax.core.subst_axis_names_jaxpr(params['call_jaxpr'], shadowed_subst)
A:jax.experimental.maps.(all_args, in_tree_def)->tree_flatten(((), args, cts_in))
A:jax.experimental.maps.fun->jax.linear_util.hashable_partial(lu.wrap_init(ad.backward_pass), call_jaxpr, reduce_axes + tuple(params['global_axis_sizes'].keys()), False)
A:jax.experimental.maps.(fun, nz_arg_cts)->jax.interpreters.ad.nonzero_outputs(fun)
A:jax.experimental.maps.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax.experimental.maps.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax.experimental.maps.inner_axis_resources->dict(outer_axis_resources)
A:jax.experimental.maps.used_resources->set()
A:jax.experimental.maps.baxis_resources->set(inner_axis_resources[baxis])
A:jax.experimental.maps.(jaxpr, mapped_out_avals, consts)->trace_to_subjaxpr_dynamic(f, self.main, mapped_in_avals)
A:jax.experimental.maps.spmd_out_axes->spmd_out_axes_thunk()
A:jax.experimental.maps.source_info->jax._src.source_info_util.current()
A:jax.experimental.maps.invars->map(self.getvar, tracers)
A:jax.experimental.maps.constvars->map(self.getvar, map(self.instantiate_const, consts))
A:jax.experimental.maps.outvars->map(self.makevar, out_tracers)
A:jax.experimental.maps.call_jaxpr->convert_constvars_jaxpr(jaxpr)
A:jax.experimental.maps.eqn->jax.interpreters.partial_eval.new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers), unknown_tracers_out, primitive, unknown_params, jaxpr_unknown.effects, source_info_util.current())
A:jax.experimental.maps.(donated_invars_known, _)->jax.interpreters.partial_eval.partition_list(unks_in, params_known['donated_invars'])
A:jax.experimental.maps.(in_axes_known, _)->jax.interpreters.partial_eval.partition_list(unks_in, params_known['in_axes'])
A:jax.experimental.maps.(_, out_axes_known)->partition_list(out_knowns, out_axes)
A:jax.experimental.maps.new_params_known->dict(params_known, in_axes=tuple(in_axes_known), out_axes=(*out_axes_known, *residual_axes), donated_invars=tuple(donated_invars_known))
A:jax.experimental.maps.(_, donated_invars_staged)->jax.interpreters.partial_eval.partition_list(inst_in, params_staged['donated_invars'])
A:jax.experimental.maps.(_, in_axes_staged)->jax.interpreters.partial_eval.partition_list(inst_in, params_staged['in_axes'])
A:jax.experimental.maps.(_, out_axes_staged)->jax.interpreters.partial_eval.partition_list(kept_outs_staged, params_staged['out_axes'])
A:jax.experimental.maps.new_params_staged->dict(params_staged, in_axes=tuple(in_axes_staged), out_axes=tuple(out_axes_staged), donated_invars=tuple(donated_invars_staged))
A:jax.experimental.maps.pe.partial_eval_jaxpr_custom_rules[xmap_p]->partial(pe.call_partial_eval_custom_rule, 'call_jaxpr', _xmap_partial_eval_custom_params_updater)
A:jax.experimental.maps.(in_knowns, in_avals, in_consts)->jax.interpreters.partial_eval.partition_pvals(in_pvals)
A:jax.experimental.maps.(f, aux)->jax.interpreters.partial_eval.partial_eval_wrapper_nounits(f, tuple(in_knowns), tuple(in_avals))
A:jax.experimental.maps.(f, out_named_shapes)->out_local_named_shapes(f, frozenset(global_axis_sizes))
A:jax.experimental.maps.(out_knowns, _, _, _)->aux()
A:jax.experimental.maps.(_, _, jaxpr_unknown, _)->aux()
A:jax.experimental.maps.num_res->len(jaxpr_unknown.constvars)
A:jax.experimental.maps.known_params->dict(params, in_axes=tuple((a for (a, k) in zip(in_axes, in_knowns) if k)), donated_invars=tuple((d for (d, k) in zip(donated_invars, in_knowns) if k)), out_axes_thunk=new_out_axes_thunk)
A:jax.experimental.maps.out->jax.lax.convert_element_type(nonzero, np.dtype(np.bool_))
A:jax.experimental.maps.(out_knowns, out_avals, jaxpr_unknown, env)->aux()
A:jax.experimental.maps.(known_outvals, res)->split_list(out, [len(out) - len(jaxpr_unknown.constvars)])
A:jax.experimental.maps.jaxpr_unknown->jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_unknown)
A:jax.experimental.maps.unknown_params->dict(params, call_jaxpr=jaxpr_unknown, out_axes=tuple(out_axes_unknown), spmd_out_axes=None, donated_invars=(*(False for _ in res), *(d for (d, k) in zip(donated_invars, in_knowns) if not k)), in_axes=(*res_axes(), *(None for _ in env), *(a for (a, k) in zip(in_axes, in_knowns) if not k)))
A:jax.experimental.maps.res_tracers->map(self.new_instantiated_const, res)
A:jax.experimental.maps.env_tracers->map(self.full_raise, env)
A:jax.experimental.maps.new_spmd_in_axes->tuple((spmd_axes if d is not_mapped else insert_spmd_axis(spmd_axes, d) for (spmd_axes, d) in zip(spmd_in_axes, dims)))
A:jax.experimental.maps.dims_out->dims_out_thunk()
A:jax.experimental.maps.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.experimental.maps.new_in_axes->tuple((_fmap_dims(in_axes, lambda a: a + (d is not not_mapped and d <= a)) for (d, in_axes) in zip(dims, params['in_axes'])))
A:jax.experimental.maps.mapped_dims_in->tuple((d if d is not_mapped else d - sum((a < d for a in in_axis.values())) for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax.experimental.maps.(f, mapped_dims_out)->jax.interpreters.batching.batch_subtrace(f, self.main, mapped_dims_in)
A:jax.experimental.maps.(new_spmd_in_axes, new_spmd_out_axes_thunk)->_batch_trace_update_spmd_axes(params['spmd_in_axes'], params['spmd_out_axes_thunk'], self.axis_name, dims, dims_out_thunk)
A:jax.experimental.maps.vals_out->primitive.bind(f, *vals, **new_params)
A:jax.experimental.maps.batching.BatchTrace.process_xmap->partialmethod(_batch_trace_process_xmap, False)
A:jax.experimental.maps.pxla.SPMDBatchTrace.process_xmap->partialmethod(_batch_trace_process_xmap, True)
A:jax.experimental.maps.(vals, dims, srcs)->unzip3(((t.val, t.batch_dim, t.source_info) for t in out_tracers))
A:jax.experimental.maps.trace->main.with_cur_sublevel()
A:jax.experimental.maps.resource_call_jaxpr->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes, in_positional_semantics).subst_axes_with_resources(call_jaxpr)
A:jax.experimental.maps.(vectorized_jaxpr, out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, local_avals)
A:jax.experimental.maps.sub_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(ctx.module_context.name_stack, wrap_name(name, 'xmap')), axis_context=ctx.module_context.axis_context.extend_manual(manual_mesh_axes))
A:jax.experimental.maps.(tiled_outs, _)->jax.interpreters.mlir.jaxpr_subcomp(sub_ctx, vectorized_jaxpr, mlir.TokenSet(), (), *tiled_ins)
A:jax.experimental.maps.(vectorized_jaxpr, global_out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, global_in_avals)
A:jax.experimental.maps.global_sharding_spec->jax.interpreters.pxla.mesh_sharding_specs(mesh.shape, mesh.axis_names)
A:jax.experimental.maps.(global_out_nodes, _)->jax.interpreters.mlir.jaxpr_subcomp(sub_ctx, vectorized_jaxpr, mlir.TokenSet(), (), *([n] for n in global_in_nodes))
A:jax.experimental.maps.zero->numpy.zeros((), dtype=np.int32)
A:jax.experimental.maps.axis_index->jax.lax.axis_index(name)
A:jax.experimental.maps.stride_c->numpy.array(strides[axis], np.int32)
A:jax.experimental.maps.linear_idxs[axis]->jax.lax.add(linear_idxs[axis], lax.mul(axis_index, stride_c))
A:jax.experimental.maps.tile_shape->list(x.shape)
A:jax.experimental.maps.base_idxs->_tile_base_indices(tile_shape, out_axes, axis_sizes)
A:jax.experimental.maps.x->jax.lax.convert_element_type(x, np.dtype(np.float32))
A:jax.experimental.maps.shape->list(x_moved.shape)
A:jax.experimental.maps.padded->jax.lax.dynamic_update_slice(padded, x, base_idxs)
A:jax.experimental.maps.nonzero->jax.lax.ne(out, np.array(0, dtype=np.float32))
A:jax.experimental.maps.named_shape->dict(aval.named_shape)
A:jax.experimental.maps.nlocal->int(np.prod(map(local_res_shape.get, resources), dtype=np.int64))
A:jax.experimental.maps.resource_count_map[axis]->ResourceCount(int(np.prod(map(global_res_shape.get, resources), dtype=np.int64)), nlocal, distributed)
A:jax.experimental.maps.global_axis_sizes->dict(global_axis_sizes)
A:jax.experimental.maps.global_dim_size->axis_resources.get(axis, ()).to_global(ips, local_dim_size)
A:jax.experimental.maps.expected_local_dim_size->axis_resources.get(axis, ()).to_local(ips, global_axis_sizes[name])
A:jax.experimental.maps.arg->arg.squeeze(dim).squeeze(dim)
A:jax.experimental.maps.squeezed_args->map(_squeeze_mapped_axes, flat_args, flat_in_axes)
A:jax.experimental.maps.updates->jax.core.traverse_jaxpr_params(partial(_jaxpr_resources, resource_env=resource_env), eqn.params).values()
A:jax.experimental.maps.x_moved->moveaxis(x, 0, axis)
A:jax.experimental.maps.(tile_size, rem)->divmod(x.shape[dim], n)
A:jax.experimental.maps.defined_axes->set(global_axis_sizes)
A:jax.experimental.maps.undeclared_axes_str->sorted((str(axis) for axis in undeclared_axes))
A:jax.experimental.maps.mesh_in_axes->EvaluationPlan.from_axis_resources(axis_resources, resource_env, global_axis_sizes, in_positional_semantics).to_mesh_axes(in_axes_flat)
A:jax.experimental.maps.s->jax.interpreters.pxla._create_mesh_pspec_sharding(arg.mesh, arg.mesh_axes)
A:jax.experimental.maps.xmap_sharding->jax.interpreters.pxla._create_mesh_pspec_sharding(mesh, pxla.array_mapping_to_axis_resources(xmap_array_mapping))
A:jax.experimental.maps.rec->partial(_check_no_loop_collectives, loop_axis_resources=loop_axis_resources)
A:jax.experimental.maps.gen_fresh_name->jax.core.gensym([jaxpr])
A:jax.experimental.maps.new_jaxpr_params->jax.core.traverse_jaxpr_params(rec, eqn.params)
A:jax.experimental.maps.mps->jax._src.sharding.MeshPspecSharding._from_parsed_pspec(resource_env.physical_mesh, ParsedPartitionSpec((), ()))
A:jax.experimental.maps.unconstrained_dims->get_unconstrained_dims(mps)
A:jax.experimental.maps.op_sharding_sharding->jax.experimental.pjit.OpShardingSharding.get_replicated(mps._device_assignment)
jax.experimental.maps.AxisNamePos(self,*args,user_repr,**kwargs)
jax.experimental.maps.AxisNamePos.__init__(self,*args,user_repr,**kwargs)
jax.experimental.maps.AxisNamePosWithRank(self,*args,expected_rank,**kwargs)
jax.experimental.maps.AxisNamePosWithRank.__init__(self,*args,expected_rank,**kwargs)
jax.experimental.maps.DotDotDotRepr
jax.experimental.maps.DotDotDotRepr.__repr__(self)
jax.experimental.maps.EvaluationPlan(NamedTuple)
jax.experimental.maps.EvaluationPlan.axis_subst(self)->core.AxisSubst
jax.experimental.maps.EvaluationPlan.from_axis_resources(cls,axis_resources:Dict[AxisName,Tuple[ResourceAxisName,...]],resource_env:ResourceEnv,global_axis_sizes:Dict[AxisName,int],in_positional_semantics:Sequence[bool])
jax.experimental.maps.EvaluationPlan.resource_axis_env(self)
jax.experimental.maps.EvaluationPlan.subst_axes_with_resources(self,jaxpr)
jax.experimental.maps.EvaluationPlan.to_mesh_axes(self,in_axes,out_axes=None)
jax.experimental.maps.EvaluationPlan.vectorize_and_loop(self,f:lu.WrappedFun,in_axes,out_axes)->lu.WrappedFun
jax.experimental.maps.FrozenDict(self,*args,**kwargs)
jax.experimental.maps.FrozenDict.__eq__(self,other)
jax.experimental.maps.FrozenDict.__getitem__(self,name)
jax.experimental.maps.FrozenDict.__hash__(self)
jax.experimental.maps.FrozenDict.__init__(self,*args,**kwargs)
jax.experimental.maps.FrozenDict.__iter__(self)
jax.experimental.maps.FrozenDict.__len__(self)
jax.experimental.maps.FrozenDict.__repr__(self)
jax.experimental.maps.NoQuotesStr(str)
jax.experimental.maps.ResourceCount(NamedTuple)
jax.experimental.maps.ResourceCount.to_global(self,semantics,local_size)
jax.experimental.maps.ResourceCount.to_local(self,semantics,global_size)
jax.experimental.maps.SerialLoop(self,length)
jax.experimental.maps.SerialLoop.__eq__(self,other)
jax.experimental.maps.SerialLoop.__hash__(self)
jax.experimental.maps.SerialLoop.__init__(self,length)
jax.experimental.maps.XMapPrimitive(self)
jax.experimental.maps.XMapPrimitive.__init__(self)
jax.experimental.maps.XMapPrimitive.bind(self,fun,*args,in_axes,**params)
jax.experimental.maps.XMapPrimitive.get_bind_params(self,params)
jax.experimental.maps.XMapPrimitive.post_process(self,trace,out_tracers,params)
jax.experimental.maps.XMapPrimitive.process(self,trace,fun,tracers,params)
jax.experimental.maps._PSThreadLocalState(self)
jax.experimental.maps._PSThreadLocalState.__init__(self)
jax.experimental.maps._PositionalSemantics(Enum)
jax.experimental.maps._UniqueResourceName(self,uid,tag=None)
jax.experimental.maps._UniqueResourceName.__eq__(self,other)
jax.experimental.maps._UniqueResourceName.__hash__(self)
jax.experimental.maps._UniqueResourceName.__init__(self,uid,tag=None)
jax.experimental.maps._UniqueResourceName.__repr__(self)
jax.experimental.maps._axis_after_insertion(axis,inserted_named_axes)
jax.experimental.maps._batch_trace_post_process_xmap(self,primitive,out_tracers,params)
jax.experimental.maps._batch_trace_process_xmap(self,is_spmd,primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.maps._batch_trace_update_spmd_axes(spmd_in_axes,spmd_out_axes_thunk,axis_name,dims,dims_out_thunk)
jax.experimental.maps._check_gda_or_array_xmap_partitioning(axis_resources,resource_env,global_axis_sizes,in_axes_flat,in_positional_semantics,args_flat)
jax.experimental.maps._check_no_loop_collectives(jaxpr,loop_axis_resources)
jax.experimental.maps._check_out_avals_vs_out_axes(out_avals:Sequence[core.AbstractValue],out_axes:Sequence[AxisNamePos],global_axis_sizes:Dict[AxisName,int])
jax.experimental.maps._clear_compilation_cache(_)
jax.experimental.maps._delete_aval_axes(aval,axes:AxisNamePos,global_axis_sizes)
jax.experimental.maps._dynamic_jaxpr_process_xmap(self,primitive,f,tracers,params)
jax.experimental.maps._ensure_spmd_and(f)
jax.experimental.maps._fix_inferred_spmd_sharding(jaxpr,resource_env,gen_fresh_name=None)
jax.experimental.maps._flatten_axes(what,tree,axes,tupled_args)
jax.experimental.maps._fmap_dims(axes,f)
jax.experimental.maps._get_axis_resource_count(axis_resources,resource_env,in_positional_semantics)->Dict[ResourceAxisName, ResourceCount]
jax.experimental.maps._get_axis_sizes(args_flat:Iterable[Any],in_axes_flat:Iterable[AxisNamePos],global_axis_sizes:Dict[AxisName,int],axis_resource_count:Dict[AxisName,ResourceCount],in_positional_semantics:Sequence[bool])
jax.experimental.maps._insert_aval_axes(aval,axes:AxisNamePos,local_axis_sizes)
jax.experimental.maps._is_axes_leaf(entry)
jax.experimental.maps._jaxpr_resources(jaxpr,resource_env)->Set[ResourceAxisName]
jax.experimental.maps._jaxpr_trace_process_xmap(self,primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.maps._merge_leading_axis(x,axis:Optional[int])
jax.experimental.maps._parse_entry(arg_name,entry)
jax.experimental.maps._prepare_axes(axes,arg_name)
jax.experimental.maps._process_xmap_default(self,call_primitive,f,tracers,params)
jax.experimental.maps._resource_typing_xmap(avals,params,source_info:source_info_util.SourceInfo,resource_env,outer_axis_resources)
jax.experimental.maps._slice_tile(x,dim:Optional[int],i,n:int)
jax.experimental.maps._thread_local_flag_unsupported(_)
jax.experimental.maps._tile(x,in_axes,axis_sizes)
jax.experimental.maps._tile_base_indices(tile_shape,axes,axis_sizes)
jax.experimental.maps._to_resource_axes(axes_specs:Sequence[AxisNamePos],axis_resources:Dict[AxisName,Tuple[ResourceAxisName,...]])
jax.experimental.maps._typecheck_xmap(*in_atoms,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes,in_positional_semantics,out_positional_semantics)
jax.experimental.maps._untile(x,out_axes,axis_sizes,platform)
jax.experimental.maps._unzip_axis_resources(axis_resources:Dict[AxisName,Tuple[ResourceAxisName,...]],resource_env:ResourceEnv)
jax.experimental.maps._xmap_axis_subst(params,subst,traverse)
jax.experimental.maps._xmap_lowering_rule(ctx,*args,**kwargs)
jax.experimental.maps._xmap_lowering_rule_replica(ctx,*in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,in_positional_semantics,out_positional_semantics,axis_resources,resource_env,backend)
jax.experimental.maps._xmap_lowering_rule_spmd(ctx,*global_in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,in_positional_semantics,out_positional_semantics,axis_resources,resource_env,backend)
jax.experimental.maps._xmap_lowering_rule_spmd_manual(ctx,*global_in_nodes,call_jaxpr,name,in_axes,out_axes,donated_invars,global_axis_sizes,spmd_in_axes,spmd_out_axes,in_positional_semantics,out_positional_semantics,axis_resources,resource_env,backend)
jax.experimental.maps._xmap_partial_eval_custom_params_updater(unks_in:Sequence[bool],inst_in:Sequence[bool],kept_outs_known:Sequence[bool],kept_outs_staged:Sequence[bool],num_res:int,params_known:dict,params_staged:dict)->Tuple[dict, dict]
jax.experimental.maps._xmap_transpose(params,call_jaxpr,args,cts_in,cts_in_avals,reduce_axes)
jax.experimental.maps.fresh_resource_name(tag=None)
jax.experimental.maps.hide_mapped_axes(flat_in_axes,flat_out_axes,*flat_args)
jax.experimental.maps.lookup_exactly_one_of(d:AxisNamePos,names:Set[AxisName])->Optional[int]
jax.experimental.maps.make_xmap_callable(fun:lu.WrappedFun,name,in_axes,out_axes_thunk,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes_thunk,in_positional_semantics,out_positional_semantics,*in_avals)
jax.experimental.maps.out_local_named_shapes(local_axes,*args,**kwargs)
jax.experimental.maps.serial_loop(name:ResourceAxisName,length:int)
jax.experimental.maps.xmap(fun:Callable,in_axes,out_axes,*,axis_sizes:Dict[AxisName,int]={},axis_resources:Dict[AxisName,ResourceSet]={},donate_argnums:Union[int,Sequence[int]]=(),backend:Optional[str]=None)->stages.Wrapped
jax.experimental.maps.xmap_impl(fun:lu.WrappedFun,*args,name,in_axes,out_axes_thunk,donated_invars,global_axis_sizes,axis_resources,resource_env,backend,spmd_in_axes,spmd_out_axes_thunk,in_positional_semantics,out_positional_semantics)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/gda_serialization/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/gda_serialization/serialization.py----------------------------------------
A:jax.experimental.gda_serialization.serialization.TS_CONTEXT->tensorstore.Context({'file_io_concurrency': {'limit': 128}})
A:jax.experimental.gda_serialization.serialization._module_unique_count->itertools.count()
A:jax.experimental.gda_serialization.serialization.device_to_index_map->inp_sharding.devices_indices_map(global_shape)
A:jax.experimental.gda_serialization.serialization.global_idx_rid->jax.experimental.global_device_array.get_shard_indices_replica_ids(global_shape, global_mesh, mesh_axes)
A:jax.experimental.gda_serialization.serialization.m->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL)
A:jax.experimental.gda_serialization.serialization.gcs_bucket->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL).group(1)
A:jax.experimental.gda_serialization.serialization.path_without_bucket->re.fullmatch('^gs://([^/]*)/(.*)$', ckpt_path, re.DOTALL).group(2)
A:jax.experimental.gda_serialization.serialization.self._cv->asyncio.Condition(lock=asyncio.Lock())
A:jax.experimental.gda_serialization.serialization.tensorstore_spec['metadata']->_get_metadata(arr_inp)
A:jax.experimental.gda_serialization.serialization.open_future->tensorstore.open(ts.Spec(tensorstore_spec), create=True, open=True, context=TS_CONTEXT)
A:jax.experimental.gda_serialization.serialization.write_future->t[shard.index].write(shard.data)
A:jax.experimental.gda_serialization.serialization.future_write_state->jax.tree_util.tree_map(_write_array, local_shards)
A:jax.experimental.gda_serialization.serialization.future_writer->jax.tree_util.tree_map(async_serialize, arrays, tensorstore_specs, commit_futures)
A:jax.experimental.gda_serialization.serialization.new_shard_shape->in_sharding.shard_shape(tuple(shape))
A:jax.experimental.gda_serialization.serialization.out->out.astype(dtype).astype(dtype)
A:jax.experimental.gda_serialization.serialization.restricted_domain->t.domain.intersect(requested_domain)
A:jax.experimental.gda_serialization.serialization.requested_bytes->estimate_read_memory_footprint(t[restricted_domain])
A:jax.experimental.gda_serialization.serialization.byte_limiter->_LimitInFlightBytes(concurrent_bytes)
A:jax.experimental.gda_serialization.serialization.future_arrays->jax.tree_util.tree_map(partial(async_deserialize, byte_limiter=byte_limiter), shardings, tensorstore_specs, [None] * len(tensorstore_specs) if global_shapes is None else global_shapes, [None] * len(tensorstore_specs) if dtypes is None else dtypes)
A:jax.experimental.gda_serialization.serialization.current_process->jax.process_index()
A:jax.experimental.gda_serialization.serialization.key_for_barrier->_get_key(self._count)
A:jax.experimental.gda_serialization.serialization.self._count->next(_module_unique_count)
A:jax.experimental.gda_serialization.serialization.self._thread->threading.Thread(target=self._thread_func)
jax.experimental.gda_serialization.serialization.AsyncManager(self,timeout_secs=300)
jax.experimental.gda_serialization.serialization.AsyncManager.__del__(self)
jax.experimental.gda_serialization.serialization.AsyncManager.__init__(self,timeout_secs=300)
jax.experimental.gda_serialization.serialization.AsyncManager._add_futures(self,futures:Sequence[asyncio.Future])
jax.experimental.gda_serialization.serialization.AsyncManager._start_async_commit(self,on_commit_callback)
jax.experimental.gda_serialization.serialization.AsyncManager._thread_func(self)
jax.experimental.gda_serialization.serialization.AsyncManager.check_for_errors(self)
jax.experimental.gda_serialization.serialization.AsyncManager.wait_until_finished(self)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManager(AsyncManager,GlobalAsyncCheckpointManagerBase)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManager.deserialize(self,shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[Dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManager.serialize(self,arrays,tensorstore_specs,*,on_commit_callback)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManagerBase(metaclass=abc.ABCMeta)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManagerBase.check_for_errors(self)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManagerBase.deserialize(self,shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[Dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None)
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManagerBase.serialize(self,arrays,tensorstore_specs,*,on_commit_callback:Callable[[],None])
jax.experimental.gda_serialization.serialization.GlobalAsyncCheckpointManagerBase.wait_until_finished(self)
jax.experimental.gda_serialization.serialization._LimitInFlightBytes(self,num_bytes)
jax.experimental.gda_serialization.serialization._LimitInFlightBytes.__init__(self,num_bytes)
jax.experimental.gda_serialization.serialization._LimitInFlightBytes.release_bytes(self,requested_bytes)
jax.experimental.gda_serialization.serialization._LimitInFlightBytes.wait_for_bytes(self,requested_bytes)
jax.experimental.gda_serialization.serialization._get_key(key:str)
jax.experimental.gda_serialization.serialization._get_metadata(arr)
jax.experimental.gda_serialization.serialization._spec_has_metadata(tree)
jax.experimental.gda_serialization.serialization.async_deserialize(in_sharding,tensorstore_spec,global_shape=None,dtype=None,byte_limiter:Optional[_LimitInFlightBytes]=None)
jax.experimental.gda_serialization.serialization.async_serialize(arr_inp,tensorstore_spec,commit_future=None)
jax.experimental.gda_serialization.serialization.create_async_array_from_callback(global_shape:array.Shape,inp_sharding:sharding.XLACompatibleSharding,data_callback:Callable[[array.Index],asyncio.Future])
jax.experimental.gda_serialization.serialization.create_async_gda_from_callback(global_shape:gda.Shape,global_mesh:Mesh,mesh_axes:gda.MeshAxes,data_callback:Callable[[gda.Index],asyncio.Future])
jax.experimental.gda_serialization.serialization.estimate_read_memory_footprint(t:ts.TensorStore)->int
jax.experimental.gda_serialization.serialization.get_tensorstore_spec(ckpt_path:str)
jax.experimental.gda_serialization.serialization.run_deserialization(shardings:Sequence[sharding.Sharding],tensorstore_specs:Sequence[Dict[str,Any]],global_shapes:Optional[Sequence[array.Shape]]=None,dtypes:Optional[Sequence[typing.DTypeLike]]=None,concurrent_gb:int=32)
jax.experimental.gda_serialization.serialization.run_serialization(arrays,tensorstore_specs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/gda_serialization/serialization_test.py----------------------------------------
A:jax.experimental.gda_serialization.serialization_test.global_mesh->jax._src.test_util.create_global_mesh((2,), 'x')
A:jax.experimental.gda_serialization.serialization_test.mesh_axes->P('x', 'y')
A:jax.experimental.gda_serialization.serialization_test.num->jax._src.util.prod(global_input_shape)
A:jax.experimental.gda_serialization.serialization_test.global_input_data1->numpy.arange(num, dtype=np.int32).reshape(global_input_shape)
A:jax.experimental.gda_serialization.serialization_test.gda1->jax._src.array.make_array_from_callback(global_input_shape, s, lambda idx: data[idx])
A:jax.experimental.gda_serialization.serialization_test.ckpt_dir1->pathlib.Path(self.create_tempdir('first').full_path)
A:jax.experimental.gda_serialization.serialization_test.global_input_data2->numpy.arange(num, num + num).reshape(inp_shape)
A:jax.experimental.gda_serialization.serialization_test.gda2->jax.experimental.global_device_array.GlobalDeviceArray.from_callback(global_input_shape, global_mesh, mesh_axes, cb2)
A:jax.experimental.gda_serialization.serialization_test.ckpt_dir2->pathlib.Path(self.create_tempdir('second').full_path)
A:jax.experimental.gda_serialization.serialization_test.global_mesh1d->jax._src.test_util.create_global_mesh((8,), ('x',))
A:jax.experimental.gda_serialization.serialization_test.gda3->jax.experimental.global_device_array.GlobalDeviceArray.from_callback((0,), global_mesh1d, P(None), cb3)
A:jax.experimental.gda_serialization.serialization_test.ckpt_dir3->pathlib.Path(self.create_tempdir('third').full_path)
A:jax.experimental.gda_serialization.serialization_test.tspecs->jax.tree_util.tree_map(serialization.get_tensorstore_spec, ckpt_paths)
A:jax.experimental.gda_serialization.serialization_test.(m1, m2, m3)->jax.experimental.gda_serialization.serialization.run_deserialization([MeshPspecSharding(global_mesh, pspec), MeshPspecSharding(global_mesh, P('x')), MeshPspecSharding(global_mesh1d, P(None))], tspecs)
A:jax.experimental.gda_serialization.serialization_test.pspec->P('x', 'y')
A:jax.experimental.gda_serialization.serialization_test.a1->jax._src.array.make_array_from_callback(inp_shape, MeshPspecSharding(global_mesh, pspec), lambda idx: global_input_data1[idx])
A:jax.experimental.gda_serialization.serialization_test.a2->jax._src.array.make_array_from_callback(inp_shape, MeshPspecSharding(global_mesh, pspec), lambda idx: global_input_data2[idx])
A:jax.experimental.gda_serialization.serialization_test.a3->jax._src.array.make_array_from_callback((0,), MeshPspecSharding(global_mesh1d, P(None)), cb3)
A:jax.experimental.gda_serialization.serialization_test.ds->MeshPspecSharding(jtu.create_global_mesh((2,), 'x'), P(None))
A:jax.experimental.gda_serialization.serialization_test.(m1,)->jax.experimental.gda_serialization.serialization.run_deserialization([MeshPspecSharding(global_mesh, P(None))], [tspec])
A:jax.experimental.gda_serialization.serialization_test.new_ds->jax._src.sharding.OpShardingSharding.get_replicated(list(global_mesh.devices.flat))
A:jax.experimental.gda_serialization.serialization_test.arr->jax._src.array.make_array_from_callback(global_input_shape, MeshPspecSharding(global_mesh, P('x', 'y')), cb1)
A:jax.experimental.gda_serialization.serialization_test.(m2,)->jax.experimental.gda_serialization.serialization.run_deserialization([new_ds], tspecs, [(8, 2)], [np.float32])
A:jax.experimental.gda_serialization.serialization_test.data->numpy.arange(1024)
A:jax.experimental.gda_serialization.serialization_test.s->MeshPspecSharding(global_mesh, P(None))
A:jax.experimental.gda_serialization.serialization_test.tspec->tensorstore.array(data).spec()
jax.experimental.gda_serialization.serialization_test.CheckpointTest(jtu.JaxTestCase)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_gda(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_jax_array(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_scalar_gda(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_scalar_jax_array(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_with_bigger_shape_gda(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_checkpointing_with_bigger_shape_jax_array(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_deserialize_tensorstore_array_gda(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_deserialize_tensorstore_array_jax_array(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_empty_spec_has_no_metadata(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_spec_has_metadata(self)
jax.experimental.gda_serialization.serialization_test.CheckpointTest.test_spec_has_no_metadata(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/impl_no_xla.py----------------------------------------
A:jax.experimental.jax2tf.impl_no_xla.(lhs, lhs_shape)->_pad_spatial_dims(lhs, lhs_shape, padding)
A:jax.experimental.jax2tf.impl_no_xla.(rhs, rhs_shape)->_transpose_with_shape(rhs, rhs_shape, (2, 3, 1, 0))
A:jax.experimental.jax2tf.impl_no_xla.pads->jax.lax.padtype_to_pads(in_shape, window_shape, window_strides, pad_str)
A:jax.experimental.jax2tf.impl_no_xla.padding->tuple(padding)
A:jax.experimental.jax2tf.impl_no_xla.x->tensorflow.pad(x, padding)
A:jax.experimental.jax2tf.impl_no_xla.x_shape->tuple((p0 + xs + p1 for (xs, (p0, p1)) in zip(x_shape, padding)))
A:jax.experimental.jax2tf.impl_no_xla.pad_a->int(np.ceil(pad_len_same / 2))
A:jax.experimental.jax2tf.impl_no_xla.tf_window_strides->_normalize_window_strides(window_strides)
A:jax.experimental.jax2tf.impl_no_xla.(padding, lhs_dilation, rhs_dilation)->_normalize_padding_and_dilations(padding, lhs_dilation, rhs_dilation, is_conv1d)
A:jax.experimental.jax2tf.impl_no_xla.(lhs, lhs_shape, rhs, rhs_shape)->_transpose_for_tf_conv(lhs, lhs_shape, rhs, rhs_shape, dimension_numbers)
A:jax.experimental.jax2tf.impl_no_xla.is_transpose->any([d != 1 for d in lhs_dilation])
A:jax.experimental.jax2tf.impl_no_xla.is_atrous->any([d != 1 for d in rhs_dilation])
A:jax.experimental.jax2tf.impl_no_xla.padding_type->pads_to_padtype(operand_shape, window_dimensions, window_strides, padding)
A:jax.experimental.jax2tf.impl_no_xla.output->tensorflow.transpose(output, inverse_perm)
A:jax.experimental.jax2tf.impl_no_xla.rhs_t->tensorflow.transpose(rhs_t, (0, 1, 3, 2))
A:jax.experimental.jax2tf.impl_no_xla.tf_out_shape->tuple((tf_out_shape[i] for i in (0, 2, 3, 1)))
A:jax.experimental.jax2tf.impl_no_xla.inverse_perm->_invert_permutation(output_perm)
A:jax.experimental.jax2tf.impl_no_xla.lhs->tensorflow.expand_dims(lhs, lhs_ndim - 1)
A:jax.experimental.jax2tf.impl_no_xla.rhs->tensorflow.expand_dims(rhs, rhs_ndim)
A:jax.experimental.jax2tf.impl_no_xla.result->tensorflow.reshape(result, jax2tf._eval_shape(args.out_aval.shape))
A:jax.experimental.jax2tf.impl_no_xla.new_id->iter(string.ascii_letters)
A:jax.experimental.jax2tf.impl_no_xla.shared_id->next(new_id)
A:jax.experimental.jax2tf.impl_no_xla.out_axis_ids->list(filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids))
A:jax.experimental.jax2tf.impl_no_xla.spec->'{},{}->{}'.format(''.join(lhs_axis_ids), ''.join(rhs_axis_ids), ''.join(out_axis_ids))
A:jax.experimental.jax2tf.impl_no_xla.expansion[d]->slice(None, None, None)
A:jax.experimental.jax2tf.impl_no_xla.indices_cartesian->tensorflow.concat(indices_by_dim, axis=len(operand_shape))
A:jax.experimental.jax2tf.impl_no_xla.scattered->tensorflow.scatter_nd(indices_cartesian, operand, output_shape)
A:jax.experimental.jax2tf.impl_no_xla.mask->tensorflow.scatter_nd(indices_cartesian, tf.ones_like(operand, dtype=np.bool_), output_shape)
A:jax.experimental.jax2tf.impl_no_xla.operand->tensorflow.reshape(operand, (1,) + operand_shape + (1,))
A:jax.experimental.jax2tf.impl_no_xla.output_shape->jax.experimental.jax2tf.jax2tf._eval_shape(_out_aval.shape)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.argmin_p]->partial(_argminmax, True)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.argmax_p]->partial(_argminmax, False)
A:jax.experimental.jax2tf.impl_no_xla.(operand, operand_shape)->_pad_spatial_dims(operand, operand_shape, padding)
A:jax.experimental.jax2tf.impl_no_xla.operand_shape->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[0].shape)
A:jax.experimental.jax2tf.impl_no_xla.has_only_spatial_dims->_validate_reduce_window_inputs(operand_shape, computation_name, dtype, window_dimensions, window_strides, base_dilation, window_dilation)
A:jax.experimental.jax2tf.impl_no_xla.(operand, operand_shape, padding_type)->_padding_reduce_window(operand, operand_shape, computation_name, window_dimensions, window_strides, padding)
A:jax.experimental.jax2tf.impl_no_xla.(operand, window_dimensions, window_strides, dilations)->_reshape_reduce_window(operand, operand_shape, window_dimensions, window_strides, window_dilation, has_only_spatial_dims=has_only_spatial_dims)
A:jax.experimental.jax2tf.impl_no_xla.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_min_p]->partial(_reduce_monoid, computation_name='min')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_max_p]->partial(_reduce_monoid, computation_name='max')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_window_sum_p]->partial(_reduce_monoid, computation_name='add')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.reduce_p]->_unimplemented('reduce')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.select_and_scatter_add_p]->_unimplemented('select_and_scatter_add')
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.rng_bit_generator_p]->_unimplemented('rng_bit_generator')
A:jax.experimental.jax2tf.impl_no_xla.max_start->tensorflow.cast(tf.subtract(max_indices, slice_sizes), dtype=tf.int32)
A:jax.experimental.jax2tf.impl_no_xla.indices->tensorflow.expand_dims(args.dnums.start_index_map, 1)
A:jax.experimental.jax2tf.impl_no_xla.op_shape->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[0].shape)
A:jax.experimental.jax2tf.impl_no_xla.slice_sizes_tf->jax.experimental.jax2tf.jax2tf._eval_shape(slice_sizes)
A:jax.experimental.jax2tf.impl_no_xla.begin->_clip(op_shape, begin, slice_sizes_tf)
A:jax.experimental.jax2tf.impl_no_xla.shrink_mask->sum((2 ** x for x in args.dnums.collapsed_slice_dims))
A:jax.experimental.jax2tf.impl_no_xla.res->tensorflow.strided_slice(args.operand, begin, end, shrink_axis_mask=shrink_mask)
A:jax.experimental.jax2tf.impl_no_xla.expected_offset_dims->tuple(list(range(axis)) + list(range(axis + index_dims, len(op_shape) + index_dims - 1)))
A:jax.experimental.jax2tf.impl_no_xla.squeezed_indices->tensorflow.squeeze(args.start_indices, -1)
A:jax.experimental.jax2tf.impl_no_xla.start_indices->_clip(op_shape, start_indices, update_shape_tf)
A:jax.experimental.jax2tf.impl_no_xla.batch_dims->tuple((x for x in range(len(args.out_aval.shape)) if x not in args.dnums.offset_dims))
A:jax.experimental.jax2tf.impl_no_xla.gather_fill_fn->jax.experimental.jax2tf.jax2tf._convert_jax_impl(lax_slicing._gather_fill, multiple_results=False)
A:jax.experimental.jax2tf.impl_no_xla.gather_args->GatherArgs(operand=operand, start_indices=start_indices, dnums=dimension_numbers, slice_sizes=slice_sizes, op_shape=_in_avals[0].shape, start_indices_shape=_in_avals[1].shape, out_aval=_out_aval)
A:jax.experimental.jax2tf.impl_no_xla.op_size->tensorflow.size(operand)
A:jax.experimental.jax2tf.impl_no_xla.update_shape_tf->jax.experimental.jax2tf.jax2tf._eval_shape(_in_avals[1].shape)
A:jax.experimental.jax2tf.impl_no_xla.end_indices->tensorflow.add(start_indices, update_shape_tf)
A:jax.experimental.jax2tf.impl_no_xla.id_tensor->tensorflow.reshape(tf.range(op_size), op_shape)
A:jax.experimental.jax2tf.impl_no_xla.scattered_indices->tensorflow.strided_slice(id_tensor, start_indices, end_indices)
A:jax.experimental.jax2tf.impl_no_xla.flat_indices->tensorflow.expand_dims(tf.nest.flatten(scattered_indices), -1)
A:jax.experimental.jax2tf.impl_no_xla.flat_update->tensorflow.nest.flatten(update)
A:jax.experimental.jax2tf.impl_no_xla.update->tensorflow.reshape(update, op_shape)
A:jax.experimental.jax2tf.impl_no_xla.update_mask->tensorflow.reshape(update_mask, op_shape)
A:jax.experimental.jax2tf.impl_no_xla.other_axes->tuple([i for i in range(len(operand.shape)) if i not in axes])
A:jax.experimental.jax2tf.impl_no_xla.suboperand->tensorflow.gather_nd(operand, scatter_indices)
A:jax.experimental.jax2tf.impl_no_xla.updated_suboperand->update_op(suboperand, updates)
A:jax.experimental.jax2tf.impl_no_xla.y->update_op(operand, operand_update)
A:jax.experimental.jax2tf.impl_no_xla.operand_update->unsorted_segment_op(updates, tf.squeeze(scatter_indices, -1), operand.shape[0])
A:jax.experimental.jax2tf.impl_no_xla.fwd->partial(shift_axes_forward, axes=scatter_to_operand_dims)
A:jax.experimental.jax2tf.impl_no_xla.inv->partial(fwd, inverse=True)
A:jax.experimental.jax2tf.impl_no_xla.updates_shifted->shift_axes_forward(updates, axes=update_window_dims, forward=False)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_p]->convert_scatter_jax_to_tf(lambda x, y: y)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_add_p]->convert_scatter_jax_to_tf(tf.add, tf.math.unsorted_segment_sum)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_mul_p]->convert_scatter_jax_to_tf(tf.multiply, tf.math.unsorted_segment_prod)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_min_p]->convert_scatter_jax_to_tf(tf.minimum, tf.math.unsorted_segment_min)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.scatter_max_p]->convert_scatter_jax_to_tf(tf.maximum, tf.math.unsorted_segment_max)
A:jax.experimental.jax2tf.impl_no_xla.tf_impl_no_xla[lax.sort_p]->_unimplemented('sort')
jax.experimental.jax2tf.impl_no_xla.GatherArgs
jax.experimental.jax2tf.impl_no_xla.GatherArgs.__post_init__(self)
jax.experimental.jax2tf.impl_no_xla.GatherArgs.__repr__(self)
jax.experimental.jax2tf.impl_no_xla._argminmax(is_min:bool,operand:TfVal,axes:Sequence[int],index_dtype:DType,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._clip(max_indices:Sequence[TfVal],start_indices:Sequence[TfVal],slice_sizes:Sequence[TfVal])
jax.experimental.jax2tf.impl_no_xla._conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:lax.ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,lhs_shape:Sequence[int],rhs_shape:Sequence[int],precision:Optional[Tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._conv_transpose_pads_to_padtype(kernel_sdims,lhs_dilation,padding)
jax.experimental.jax2tf.impl_no_xla._dot_general(lhs,rhs,*,dimension_numbers,precision:Optional[Tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._dynamic_slice(operand,*start_indices,slice_sizes:core.Shape,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._dynamic_update_slice(operand,update,*start_indices,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._error(primitive_name:str,suffix_msg:str='')->Exception
jax.experimental.jax2tf.impl_no_xla._gather(operand,start_indices,*,dimension_numbers,slice_sizes:core.Shape,indices_are_sorted,unique_indices,mode,fill_value,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._gather_for_multidim_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._gather_for_scalar_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._gather_with_batch_dims(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._interior_padding(operand,padding_value,padding_config,operand_shape)
jax.experimental.jax2tf.impl_no_xla._invert_permutation(perm)
jax.experimental.jax2tf.impl_no_xla._normalize_output_perm(output_perm,is_conv1d)
jax.experimental.jax2tf.impl_no_xla._normalize_padding_and_dilations(padding,lhs_dilation,rhs_dilation,is_conv1d)
jax.experimental.jax2tf.impl_no_xla._normalize_window_strides(window_strides)
jax.experimental.jax2tf.impl_no_xla._pad(operand,padding_value,*,padding_config,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._pad_spatial_dims(x,x_shape,padding)
jax.experimental.jax2tf.impl_no_xla._padding_reduce_window(operand,operand_shape,computation_name,window_dimensions,window_strides,padding)
jax.experimental.jax2tf.impl_no_xla._pre_gather_for_multidim_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._pre_gather_for_scalar_indexing(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._pre_gather_with_batch_dims(args:GatherArgs)
jax.experimental.jax2tf.impl_no_xla._reduce_monoid(operand,window_dimensions,window_strides,padding,base_dilation,window_dilation,computation_name,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.impl_no_xla._reduce_window(*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals:Sequence[core.ShapedArray],_out_aval:Tuple[core.ShapedArray,...])->Tuple[TfVal, ...]
jax.experimental.jax2tf.impl_no_xla._reshape_reduce_window(operand,operand_shape,window_dimensions,window_strides,window_dilation,*,has_only_spatial_dims)
jax.experimental.jax2tf.impl_no_xla._transpose_for_tf_conv(lhs,lhs_shape:core.Shape,rhs,rhs_shape:core.Shape,dimension_numbers)
jax.experimental.jax2tf.impl_no_xla._transpose_with_shape(x:TfVal,x_shape:core.Shape,permutation)->Tuple[TfVal, core.Shape]
jax.experimental.jax2tf.impl_no_xla._unimplemented(name)
jax.experimental.jax2tf.impl_no_xla._validate_conv_features(is_transpose,is_atrous,is_depthwise,feature_group_count,batch_group_count,preferred_element_type,lhs_dtype)
jax.experimental.jax2tf.impl_no_xla._validate_reduce_window_inputs(operand_shape,computation_name,dtype,window_dimensions,window_strides,base_dilation,window_dilation)
jax.experimental.jax2tf.impl_no_xla._validate_spatial_dimensions(lhs:TfVal,lhs_shape:core.Shape,rhs:TfVal,rhs_shape:core.Shape)
jax.experimental.jax2tf.impl_no_xla.convert_scatter_jax_to_tf(update_op,unsorted_segment_op=None)
jax.experimental.jax2tf.impl_no_xla.gather_precondition(precondition_fn:Callable[[GatherArgs],None])
jax.experimental.jax2tf.impl_no_xla.pads_to_padtype(in_shape,window_shape,window_strides,padding)->str
jax.experimental.jax2tf.impl_no_xla.shift_axes_forward(operand,axes:Tuple[int,...],inverse:bool=False,forward:bool=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/shape_poly.py----------------------------------------
A:jax.experimental.jax2tf.shape_poly.items->self.monomials()
A:jax.experimental.jax2tf.shape_poly.d->collections.Counter(self)
A:jax.experimental.jax2tf.shape_poly.acc->set()
A:jax.experimental.jax2tf.shape_poly.coeffs->self._coeffs.copy()
A:jax.experimental.jax2tf.shape_poly.other->_ensure_poly(other)
A:jax.experimental.jax2tf.shape_poly.mon->mon1.mul(mon2)
A:jax.experimental.jax2tf.shape_poly.power->int(power)
A:jax.experimental.jax2tf.shape_poly.(lb, ub)->_ensure_poly(self - other).bounds()
A:jax.experimental.jax2tf.shape_poly.divisor->_ensure_poly(divisor)
A:jax.experimental.jax2tf.shape_poly.qmon->mon1.mul(mon2).divide(dmon)
A:jax.experimental.jax2tf.shape_poly.(qcount, rcount)->divmod(count, dcount)
A:jax.experimental.jax2tf.shape_poly.q->_DimPolynomial.from_coeffs({qmon: qcount})
A:jax.experimental.jax2tf.shape_poly.dividend->int(dividend)
A:jax.experimental.jax2tf.shape_poly.(q, r)->_ensure_poly(d - window_size).divmod(window_stride)
A:jax.experimental.jax2tf.shape_poly.lbub->self._coeffs.get(_DimMon(), 0)
A:jax.experimental.jax2tf.shape_poly.sz1->numpy.prod(s1)
A:jax.experimental.jax2tf.shape_poly.sz2->numpy.prod(s2)
A:jax.experimental.jax2tf.shape_poly.core._SPECIAL_DIMENSION_HANDLERS[_DimPolynomial]->DimensionHandlerPoly()
A:jax.experimental.jax2tf.shape_poly.shape->numpy.shape(operand)
A:jax.experimental.jax2tf.shape_poly.(contract_fake_ops, contractions)->opt_einsum.contract_path(*fake_ops, **kwargs)
A:jax.experimental.jax2tf.shape_poly.idx->tuple((i for (i, fake_op) in enumerate(fake_ops) if operand is fake_op))
A:jax.experimental.jax2tf.shape_poly.dim_as_value_p->jax.core.Primitive('dim_as_value')
A:jax.experimental.jax2tf.shape_poly.spec_->spec_.rstrip(',').rstrip(',')
A:jax.experimental.jax2tf.shape_poly.spec_tuple->tuple(map(lambda s: ... if isinstance(s, str) and s.strip() == '...' else s, spec_tuple))
A:jax.experimental.jax2tf.shape_poly.ds_ellipses->tuple((ds for ds in spec_tuple if ds == ...))
A:jax.experimental.jax2tf.shape_poly.dim_spec->dim_spec.strip().strip()
A:jax.experimental.jax2tf.shape_poly.terms->dim_spec.strip().strip().split('+')
A:jax.experimental.jax2tf.shape_poly.term_spec->term_spec.strip().strip()
A:jax.experimental.jax2tf.shape_poly.factors->term_spec.strip().strip().split('*')
A:jax.experimental.jax2tf.shape_poly.factor_spec->factor_spec.strip().strip()
A:jax.experimental.jax2tf.shape_poly.m->re.match('^([a-zA-Z]\\w*)(\\^(\\d+))?$', factor_spec)
A:jax.experimental.jax2tf.shape_poly.var->_DimPolynomial.from_var(m.group(1))
A:jax.experimental.jax2tf.shape_poly.dim_poly->_parse_dim(dim_spec)
A:jax.experimental.jax2tf.shape_poly.dim_size->int(dim_size)
A:jax.experimental.jax2tf.shape_poly.dims->tuple((_process_dim(i, ds) for (i, ds) in enumerate(spec_tuple)))
A:jax.experimental.jax2tf.shape_poly.vint->int(v.val)
A:jax.experimental.jax2tf.shape_poly.dimension_size_p->jax.core.Primitive('dimension_size')
A:jax.experimental.jax2tf.shape_poly.shape_env_jax->dict(zip(dim_vars, dim_values))
A:jax.experimental.jax2tf.shape_poly.aval_shape->_parse_spec(polymorphic_shape, arg_shape)
A:jax.experimental.jax2tf.shape_poly.dim_vars->dim_vars.union(d.get_vars()).union(d.get_vars())
A:jax.experimental.jax2tf.shape_poly.dim_env->_solve_dim_equations(dim_equations)
A:jax.experimental.jax2tf.shape_poly.mon_value->mon1.mul(mon2).evaluate(shapeenv)
A:jax.experimental.jax2tf.shape_poly.v->mon1.mul(mon2).to_var()
A:jax.experimental.jax2tf.shape_poly.var_value->jax._src.lax.lax.div(dim_expr, np.int32(factor_var))
A:jax.experimental.jax2tf.shape_poly.var_remainder->jax._src.lax.lax.rem(dim_expr, np.int32(factor_var))
A:jax.experimental.jax2tf.shape_poly.var_remainder_int->_is_known_constant(var_remainder)
A:jax.experimental.jax2tf.shape_poly.var_value_int->_is_known_constant(var_value)
A:jax.experimental.jax2tf.shape_poly.dim_expr_int->_is_known_constant(dim_expr)
A:jax.experimental.jax2tf.shape_poly.nr_eqns->len(eqns)
A:jax.experimental.jax2tf.shape_poly.unsolved_vars->unsolved_vars.difference(shapeenv.keys()).difference(shapeenv.keys())
A:jax.experimental.jax2tf.shape_poly.eqns_str->'\n  '.join([str(eqn.poly) for eqn in eqns])
jax.experimental.jax2tf.shape_poly.DimEquation
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly(core.DimensionHandler)
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.as_value(self,d:DimSize)
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.divide_shape_sizes(self,s1:Shape,s2:Shape)->DimSize
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.greater_equal(self,d1:DimSize,d2:DimSize)
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.is_constant(self,d:DimSize)->bool
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.stride(self,d:DimSize,window_size:DimSize,window_stride:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly.DimensionHandlerPoly.symbolic_equal(self,d1:core.DimSize,d2:core.DimSize)->bool
jax.experimental.jax2tf.shape_poly.InconclusiveDimensionOperation(self,message:str)
jax.experimental.jax2tf.shape_poly.InconclusiveDimensionOperation.__init__(self,message:str)
jax.experimental.jax2tf.shape_poly.PolyShape(self,*dim_specs)
jax.experimental.jax2tf.shape_poly.PolyShape.__init__(self,*dim_specs)
jax.experimental.jax2tf.shape_poly._DimMon(dict)
jax.experimental.jax2tf.shape_poly._DimMon.__hash__(self)
jax.experimental.jax2tf.shape_poly._DimMon.__lt__(self,other:'_DimMon')
jax.experimental.jax2tf.shape_poly._DimMon.__str__(self)
jax.experimental.jax2tf.shape_poly._DimMon.degree(self)
jax.experimental.jax2tf.shape_poly._DimMon.divide(self,divisor:'_DimMon')->'_DimMon'
jax.experimental.jax2tf.shape_poly._DimMon.evaluate(self,env:ShapeEnv)
jax.experimental.jax2tf.shape_poly._DimMon.from_var(cls,v:str)->'_DimMon'
jax.experimental.jax2tf.shape_poly._DimMon.get_vars(self)->Set[str]
jax.experimental.jax2tf.shape_poly._DimMon.mul(self,other:'_DimMon')->'_DimMon'
jax.experimental.jax2tf.shape_poly._DimMon.to_var(self)->Optional[str]
jax.experimental.jax2tf.shape_poly._DimPolynomial(self,coeffs:Dict[_DimMon,int])
jax.experimental.jax2tf.shape_poly._DimPolynomial.__add__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__floordiv__(self,divisor:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__gt__(self,other:DimSize)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__hash__(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__init__(self,coeffs:Dict[_DimMon,int])
jax.experimental.jax2tf.shape_poly._DimPolynomial.__int__(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__jax_array__(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__le__(self,other:DimSize)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__lt__(self,other:DimSize)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__mod__(self,divisor:DimSize)->int
jax.experimental.jax2tf.shape_poly._DimPolynomial.__mul__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__ne__(self,other:DimSize)->bool
jax.experimental.jax2tf.shape_poly._DimPolynomial.__neg__(self)->'_DimPolynomial'
jax.experimental.jax2tf.shape_poly._DimPolynomial.__pow__(self,power,modulo=None)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__radd__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__rdivmod__(self,other:DimSize)->Tuple[DimSize, int]
jax.experimental.jax2tf.shape_poly._DimPolynomial.__repr__(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__rfloordiv__(self,other)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__rmul__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__rsub__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__rtruediv__(self,dividend:DimSize)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__str__(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.__sub__(self,other:DimSize)->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.__truediv__(self,divisor:DimSize)
jax.experimental.jax2tf.shape_poly._DimPolynomial._division_error_msg(self,dividend,divisor,details:str='')->str
jax.experimental.jax2tf.shape_poly._DimPolynomial.bounds(self)->Tuple[Optional[int], Optional[int]]
jax.experimental.jax2tf.shape_poly._DimPolynomial.divmod(self,divisor:DimSize)->Tuple[DimSize, int]
jax.experimental.jax2tf.shape_poly._DimPolynomial.eq(self,other:DimSize)->bool
jax.experimental.jax2tf.shape_poly._DimPolynomial.evaluate(self,env:ShapeEnv)
jax.experimental.jax2tf.shape_poly._DimPolynomial.from_coeffs(cls,coeffs:Dict[_DimMon,int])->DimSize
jax.experimental.jax2tf.shape_poly._DimPolynomial.from_var(cls,v:str)->'_DimPolynomial'
jax.experimental.jax2tf.shape_poly._DimPolynomial.ge(self,other:DimSize)->bool
jax.experimental.jax2tf.shape_poly._DimPolynomial.get_aval(_:'_DimPolynomial')
jax.experimental.jax2tf.shape_poly._DimPolynomial.get_vars(self)->Set[str]
jax.experimental.jax2tf.shape_poly._DimPolynomial.is_constant(self)
jax.experimental.jax2tf.shape_poly._DimPolynomial.leading_term(self)->Tuple[_DimMon, int]
jax.experimental.jax2tf.shape_poly._DimPolynomial.monomials(self)->Iterable[Tuple[_DimMon, int]]
jax.experimental.jax2tf.shape_poly._DimPolynomial.to_var(self)->Optional[str]
jax.experimental.jax2tf.shape_poly._add(v1,v2)
jax.experimental.jax2tf.shape_poly._dim_as_value(dim:DimSize)
jax.experimental.jax2tf.shape_poly._dim_as_value_abstract(dim:DimSize)->core.AbstractValue
jax.experimental.jax2tf.shape_poly._dimension_size_abstract(aval:core.AbstractValue,**_)->core.AbstractValue
jax.experimental.jax2tf.shape_poly._dimension_size_impl(arg,*,dimension)
jax.experimental.jax2tf.shape_poly._einsum_contract_path(*operands,**kwargs)
jax.experimental.jax2tf.shape_poly._ensure_poly(p:DimSize)->_DimPolynomial
jax.experimental.jax2tf.shape_poly._is_known_constant(v)->Optional[int]
jax.experimental.jax2tf.shape_poly._multiply(v1,v2)
jax.experimental.jax2tf.shape_poly._parse_spec(spec:Optional[Union[str,PolyShape]],arg_shape:Sequence[Optional[int]])->Tuple[DimSize, ...]
jax.experimental.jax2tf.shape_poly._solve_dim_equations(eqns:List[DimEquation])->ShapeEnv
jax.experimental.jax2tf.shape_poly.arg_aval(arg_shape:Sequence[Optional[int]],arg_jax_dtype:DType,polymorphic_shape:Optional[Union[str,PolyShape]])->core.ShapedArray
jax.experimental.jax2tf.shape_poly.get_shape_evaluator(dim_vars:Sequence[str],shape:Sequence[DimSize])->Tuple[Callable, Sequence[core.AbstractValue]]
jax.experimental.jax2tf.shape_poly.is_poly_dim(p:DimSize)->bool
jax.experimental.jax2tf.shape_poly.prepare_dim_var_env(args_avals:Sequence[core.AbstractValue])->Tuple[Sequence[str], Callable]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/call_tf.py----------------------------------------
A:jax.experimental.jax2tf.call_tf.(args_flat_jax, args_treedef)->jax.tree_util.tree_flatten(args_jax)
A:jax.experimental.jax2tf.call_tf.dtype->jax.dtypes.canonicalize_dtype(v.dtype)
A:jax.experimental.jax2tf.call_tf.v->v.astype(dtype).astype(dtype)
A:jax.experimental.jax2tf.call_tf.args_flat_jax->tuple(map(canonical_arg, args_flat_jax))
A:jax.experimental.jax2tf.call_tf.a_tf_dtype->jax.experimental.jax2tf.jax2tf._to_tf_dtype(a_jax.dtype)
A:jax.experimental.jax2tf.call_tf.args_flat_sig_tf->tuple(map(make_tensorspec, args_flat_jax))
A:jax.experimental.jax2tf.call_tf.args_tf->args_treedef.unflatten(args_tf_flat)
A:jax.experimental.jax2tf.call_tf.res_tf->callable_tf(*args_tf)
A:jax.experimental.jax2tf.call_tf.(res_tf_flat, res_treedef_now)->jax.tree_util.tree_flatten(res_tf)
A:jax.experimental.jax2tf.call_tf.function_flat_tf->tensorflow.function(callable_flat_tf, autograph=False, jit_compile=True)
A:jax.experimental.jax2tf.call_tf.res_jax_flat->jax.core.Primitive('call_tf').bind(*args_flat_jax, callable_flat_tf=callable_flat_tf, function_flat_tf=function_flat_tf, args_flat_sig_tf=args_flat_sig_tf)
A:jax.experimental.jax2tf.call_tf.watched_args_tf->tensorflow.nest.map_structure(replace_non_float, args_tf)
A:jax.experimental.jax2tf.call_tf.res->callable_tf(*args_tf)
A:jax.experimental.jax2tf.call_tf.dres_darg->tape.gradient(tf.nest.map_structure(replace_non_float, res), sources=watched_args_tf, output_gradients=ct_res_tf, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.call_tf.ct_args_jax->call_tf(tf_vjp_fun)(args_jax, ct_res_jax)
A:jax.experimental.jax2tf.call_tf.arg_dtype->jax.dtypes.result_type(arg_jax)
A:jax.experimental.jax2tf.call_tf.ct_arg_dtype->jax.core.primal_dtype_to_tangent_dtype(arg_dtype)
A:jax.experimental.jax2tf.call_tf.ct_args_jax_fixed->jax.tree_util.tree_map(fix_float0, args_jax, ct_args_jax)
A:jax.experimental.jax2tf.call_tf.call_tf_p->jax.core.Primitive('call_tf')
A:jax.experimental.jax2tf.call_tf.arg_dlpack->jax.dlpack.to_dlpack(arg_jax, take_ownership=False)
A:jax.experimental.jax2tf.call_tf.args_tf_flat->tuple(map(_arg_jax_to_tf, args_jax_flat))
A:jax.experimental.jax2tf.call_tf.res_tf_flat->callable_flat_tf(*args)
A:jax.experimental.jax2tf.call_tf.(res_tf, _)->jax.experimental.jax2tf.jax2tf._tfval_to_tensor_jax_dtype(res_tf)
A:jax.experimental.jax2tf.call_tf.res_jax_platform->res_tf_platform.lower()
A:jax.experimental.jax2tf.call_tf.res_dlpack->tensorflow.experimental.dlpack.to_dlpack(res_tf)
A:jax.experimental.jax2tf.call_tf.(_, result_avals)->_code_generator_and_avals(function_flat_tf, args_flat_sig_tf, code_gen_optional=True)
A:jax.experimental.jax2tf.call_tf.(code_gen, _)->_code_generator_and_avals(function_flat_tf, args_flat_sig_tf, code_gen_optional=False)
A:jax.experimental.jax2tf.call_tf.concrete_function_flat_tf->tensorflow.function(callable_flat_tf, autograph=False, jit_compile=True).get_concrete_function(*args_flat_sig_tf)
A:jax.experimental.jax2tf.call_tf.func_tf_hlo->tensorflow.function(callable_flat_tf, autograph=False, jit_compile=True).experimental_get_compiler_ir(*args_tf_flat)(stage='hlo_serialized', device_name=tf_device_name)
A:jax.experimental.jax2tf.call_tf.xla_comp->jax._src.lib.xla_client.XlaComputation(func_tf_hlo)
A:jax.experimental.jax2tf.call_tf.xla_comp_parameter_shapes->jax._src.lib.xla_client.XlaComputation(func_tf_hlo).program_shape().parameter_shapes()
A:jax.experimental.jax2tf.call_tf.res_dtype->res_shape.numpy_dtype()
A:jax.experimental.jax2tf.call_tf.jax_res_dtype->jax.dtypes.canonicalize_dtype(res_dtype)
A:jax.experimental.jax2tf.call_tf.result_shape->jax._src.lib.xla_client.XlaComputation(func_tf_hlo).program_shape().result_shape()
A:jax.experimental.jax2tf.call_tf.result_shapes->jax._src.lib.xla_client.XlaComputation(func_tf_hlo).program_shape().result_shape().tuple_shapes()
A:jax.experimental.jax2tf.call_tf.result_avals->tuple(map(canonical_res_aval, result_shapes))
A:jax.experimental.jax2tf.call_tf.captured_ops->tuple((mlir.ir_constant(np.asarray(inp), canonicalize_types=False) for inp in captured_inputs))
A:jax.experimental.jax2tf.call_tf.submodule->jax.interpreters.mlir.xla_computation_to_mhlo_module(xla_comp)
A:jax.experimental.jax2tf.call_tf.symtab->jax._src.lib.mlir.ir.SymbolTable(submodule.operation)
A:jax.experimental.jax2tf.call_tf.fn->jax.interpreters.mlir.merge_mhlo_modules(ctx.module, f'call_tf_{function_flat_tf.name}', submodule)
A:jax.experimental.jax2tf.call_tf.call->jax._src.lib.mlir.dialects.func.CallOp(callee_result_types, ir.FlatSymbolRefAttr.get(fn), tuple(args_op) + captured_ops)
jax.experimental.jax2tf.call_tf(callable_tf:Callable)->Callable
jax.experimental.jax2tf.call_tf._call_tf_abstract_eval(*_,function_flat_tf,args_flat_sig_tf,**__)
jax.experimental.jax2tf.call_tf._call_tf_impl(*args_jax_flat,callable_flat_tf,**_)
jax.experimental.jax2tf.call_tf._call_tf_lowering(ctx,*args_op,function_flat_tf,args_flat_sig_tf,**_)
jax.experimental.jax2tf.call_tf._code_generator_and_avals(function_flat_tf,args_flat_sig_tf,code_gen_optional=False)->Tuple[Optional[Callable[[mlir.ModuleContext, Sequence[ir.Value]], Sequence[ir.Value]]], Sequence[core.ShapedArray]]
jax.experimental.jax2tf.call_tf._jax2tf_call_tf(*args:TfVal,callable_flat_tf:Callable,**_)->TfVal
jax.experimental.jax2tf.call_tf.call_tf(callable_tf:Callable)->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/jax2tf.py----------------------------------------
A:jax.experimental.jax2tf.jax2tf._VALID_SCOPE_REGEX->re.compile('^[A-Za-z0-9.][A-Za-z0-9_.\\/>-]*$')
A:jax.experimental.jax2tf.jax2tf._INVALID_SCOPE_CHAR->re.compile('[^A-Za-z0-9_.\\/-]')
A:jax.experimental.jax2tf.jax2tf.scope_name->re.compile('[^A-Za-z0-9_.\\/-]').sub('_', name)
A:jax.experimental.jax2tf.jax2tf._thread_local_state->_ThreadLocalState()
A:jax.experimental.jax2tf.jax2tf.fun_name->getattr(fun_jax, '__name__', 'unknown')
A:jax.experimental.jax2tf.jax2tf.name_stack->jax._src.util.wrap_name(fun_name, 'jax2tf')
A:jax.experimental.jax2tf.jax2tf.(fun_flat_jax, args_flat_tf, in_tree, out_tree_thunk)->flatten_fun_jax(fun_jax, args_tf, kwargs_tf)
A:jax.experimental.jax2tf.jax2tf.polymorphic_shapes_->tuple(polymorphic_shapes)
A:jax.experimental.jax2tf.jax2tf.polymorphic_shapes_flat->tuple(api_util.flatten_axes('jax2tf.convert polymorphic_shapes', in_tree, (polymorphic_shapes_, {k: None for k in kwargs_tf.keys()})))
A:jax.experimental.jax2tf.jax2tf.args_and_avals->tuple(map(preprocess_arg_tf, range(len(args_flat_tf)), args_flat_tf, polymorphic_shapes_flat))
A:jax.experimental.jax2tf.jax2tf.(args_flat_tf, args_avals_flat)->jax._src.util.unzip2(args_and_avals)
A:jax.experimental.jax2tf.jax2tf.(dim_vars, get_dim_values_jax)->jax.experimental.jax2tf.shape_poly.prepare_dim_var_env(args_avals_flat)
A:jax.experimental.jax2tf.jax2tf.(dim_values, _)->_interpret_fun_jax(get_dim_values_jax, args_flat_tf, args_avals_flat, '')
A:jax.experimental.jax2tf.jax2tf.shape_env->zip(dim_vars, dim_values)
A:jax.experimental.jax2tf.jax2tf.(outs_tf, out_avals)->_interpret_fun_jax(fun_flat_jax, args_flat_tf, args_avals_flat, name_stack, fresh_constant_cache=True, experimental_native_lowering=experimental_native_lowering)
A:jax.experimental.jax2tf.jax2tf.out_flat_tf->converted_fun_flat_with_custom_gradient_tf(*args_flat_tf)
A:jax.experimental.jax2tf.jax2tf.out_tf->jax.tree_util.tree_unflatten(out_tree_thunk(), out_flat_tf)
A:jax.experimental.jax2tf.jax2tf.(tval, _)->_tfval_to_tensor_jax_dtype(val)
A:jax.experimental.jax2tf.jax2tf.(args_flat_tf, in_tree)->jax.tree_util.tree_flatten((args_tf, kwargs_tf))
A:jax.experimental.jax2tf.jax2tf.(tree_args, tree_kwargs)->jax.tree_util.tree_unflatten(in_tree, args_flat_jax)
A:jax.experimental.jax2tf.jax2tf.tree_res->fun_jax(*tree_args, **tree_kwargs)
A:jax.experimental.jax2tf.jax2tf.(res_flat_jax, out_tree)->jax.tree_util.tree_flatten(tree_res)
A:jax.experimental.jax2tf.jax2tf.lowered->fun_jax.lower(*tree_args, **tree_kwargs)
A:jax.experimental.jax2tf.jax2tf.(arg_tf, arg_jax_dtype)->_tfval_to_tensor_jax_dtype(arg_tf)
A:jax.experimental.jax2tf.jax2tf.arg_tf->tensorflow.identity(arg_tf, f'jax2tf_arg_{arg_idx}')
A:jax.experimental.jax2tf.jax2tf.tf_arg_shape->numpy.shape(arg_tf)
A:jax.experimental.jax2tf.jax2tf.arg_shape->tuple((d.value if isinstance(d, tf.compat.v1.Dimension) else d for d in tf_arg_shape))
A:jax.experimental.jax2tf.jax2tf.arg_aval->jax.experimental.jax2tf.shape_poly.arg_aval(arg_shape, arg_jax_dtype, polymorphic_shape)
A:jax.experimental.jax2tf.jax2tf.out_cts_flat_polymorphic_shapes->tuple((str(out_aval.shape) for out_aval in out_avals))
A:jax.experimental.jax2tf.jax2tf.(_, pullback_jax)->jax.vjp(fun_flat_jax, *args_flat_jax)
A:jax.experimental.jax2tf.jax2tf.out_cts_fixed_flat->list(map(fix_out_ct, out_cts_flat_jax, out_avals))
A:jax.experimental.jax2tf.jax2tf.in_cts_flat_jax->pullback_jax(out_cts_fixed_flat)
A:jax.experimental.jax2tf.jax2tf.in_cts_fixed_flat_jax->tuple(map(fix_in_ct, in_cts_flat_jax, args_avals_flat))
A:jax.experimental.jax2tf.jax2tf.in_cts_flat->convert(fun_vjp_jax, with_gradient=False, polymorphic_shapes=vjp_polymorphic_shapes)(args_flat_tf, out_cts_flat_tf)
A:jax.experimental.jax2tf.jax2tf._thread_local_state.name_stack->jax._src.util.extend_name_stack(_thread_local_state.name_stack, extra_name_stack)
A:jax.experimental.jax2tf.jax2tf.subtrace_fun->_interpret_subtrace(lu.wrap_init(fun_jax), main, args_avals)
A:jax.experimental.jax2tf.jax2tf.d_var->d.to_var()
A:jax.experimental.jax2tf.jax2tf.abstracted_axes->tuple(abstracted_axes)
A:jax.experimental.jax2tf.jax2tf.backend->jax.default_backend()
A:jax.experimental.jax2tf.jax2tf.mhlo_module->fun_jax.lower(*tree_args, **tree_kwargs).mhlo()
A:jax.experimental.jax2tf.jax2tf.mhlo_module_text->jax.interpreters.mlir.module_to_string(mhlo_module)
A:jax.experimental.jax2tf.jax2tf.custom_calls->re.findall('mhlo.custom_call.*call_target_name\\s+=\\s+"([^"]+)".*loc\\(([^\\)]+)\\)', mhlo_module_text)
A:jax.experimental.jax2tf.jax2tf.bad_custom_calls->tuple(filter(lambda cc: cc[0] != 'Sharding', custom_calls))
A:jax.experimental.jax2tf.jax2tf.m->tensorflow.Module()
A:jax.experimental.jax2tf.jax2tf.out_shapes->tuple((tuple((d if type(d) is int else None for d in out_aval.shape)) for out_aval in out_avals))
A:jax.experimental.jax2tf.jax2tf.out_types->tuple((_out_type(out_aval.dtype) for out_aval in out_avals))
A:jax.experimental.jax2tf.jax2tf.args_tf->tuple(map(_shard_value, args_tf, args_avals, lowered.compile_args['in_shardings']))
A:jax.experimental.jax2tf.jax2tf.res->tensorflow.stop_gradient(res)
A:jax.experimental.jax2tf.jax2tf.conversion_dtype->promote_tf_dtype(operand.dtype)
A:jax.experimental.jax2tf.jax2tf._in_avals->map(_jax_physical_aval, _in_avals)
A:jax.experimental.jax2tf.jax2tf._out_aval->_jax_physical_aval(_out_aval)
A:jax.experimental.jax2tf.jax2tf.results_jax->impl_jax(*args_jax, **kwargs)
A:jax.experimental.jax2tf.jax2tf.(results_tf, _)->_interpret_fun_jax(impl_multiple_results_jax, args_tf, _in_avals, extra_name_stack)
A:jax.experimental.jax2tf.jax2tf.trace->TensorFlowTrace(main, core.cur_sublevel())
A:jax.experimental.jax2tf.jax2tf.in_tracers->tuple((TensorFlowTracer(trace, val, aval) for (val, aval) in zip(in_vals, in_avals)))
A:jax.experimental.jax2tf.jax2tf.(outs_tf, _)->_interpret_fun_jax(core.jaxpr_as_fun(jaxpr), args_tf, jaxpr.in_avals, extra_name_stack)
A:jax.experimental.jax2tf.jax2tf.(aval,)->jax.core.ShapedArray((), _to_jax_dtype(x.dtype)).dtype._rules.physical_avals(aval)
A:jax.experimental.jax2tf.jax2tf.aval->jax.core.ShapedArray((), _to_jax_dtype(x.dtype))
A:jax.experimental.jax2tf.jax2tf.jax_dtype->_jax_physical_dtype(jax_dtype)
A:jax.experimental.jax2tf.jax2tf.(_, tf_val)->_ThreadLocalState().constant_cache.get(const_key, (None, None))
A:jax.experimental.jax2tf.jax2tf.val->val.__jax_array__().__jax_array__()
A:jax.experimental.jax2tf.jax2tf.tf_val->tensorflow.convert_to_tensor(_maybe_decode_gda(val), dtype=conversion_dtype)
A:jax.experimental.jax2tf.jax2tf.(dim_vars, dim_values)->jax._src.util.unzip2(_thread_local_state.shape_env)
A:jax.experimental.jax2tf.jax2tf.(eval_shape_jax, dim_avals)->jax.experimental.jax2tf.shape_poly.get_shape_evaluator(dim_vars, shape)
A:jax.experimental.jax2tf.jax2tf.(shape_values_tf, _)->_interpret_fun_jax(eval_shape_jax, dim_values, dim_avals, '')
A:jax.experimental.jax2tf.jax2tf.phys_aval->_jax_physical_aval(self._aval)
A:jax.experimental.jax2tf.jax2tf.aval_int->int(_eval_shape([aval_dim]))
A:jax.experimental.jax2tf.jax2tf.(tf_val, jax_dtype)->_tfval_to_tensor_jax_dtype(val, memoize_constants=True)
A:jax.experimental.jax2tf.jax2tf.(impl, impl_needs_avals)->self.get_primitive_impl(primitive)
A:jax.experimental.jax2tf.jax2tf.(out_aval, _)->primitive.abstract_eval(*args_avals, **params)
A:jax.experimental.jax2tf.jax2tf.current_name_stack->_get_current_name_stack()
A:jax.experimental.jax2tf.jax2tf.scope->str(current_name_stack)
A:jax.experimental.jax2tf.jax2tf.op_metadata->jax.interpreters.xla.make_op_metadata(primitive, params, name_stack=current_name_stack, source_info=source_info_util.current())
A:jax.experimental.jax2tf.jax2tf.op_metadata_proto->tensorflow.compiler.xla.xla_data_pb2.OpMetadata(op_type=op_metadata.op_type, op_name=op_metadata.op_name, source_file=op_metadata.source_file, source_line=op_metadata.source_line)
A:jax.experimental.jax2tf.jax2tf.val_out->invoke_impl()
A:jax.experimental.jax2tf.jax2tf.out->tensorflow.stop_gradient(out)
A:jax.experimental.jax2tf.jax2tf.interpreted_fun->_interpret_subtrace(fun, self.main, avals)
A:jax.experimental.jax2tf.jax2tf.extra_name_stack->jax._src.util.wrap_name(params['name'], 'jit')
A:jax.experimental.jax2tf.jax2tf.(tf_res_vals, tf_res_avals)->jax._src.util.unzip2(tf_res_out)
A:jax.experimental.jax2tf.jax2tf.tf_vals_out->tensorflow.function(f_tf, autograph=False, jit_compile=True)(*vals)
A:jax.experimental.jax2tf.jax2tf.vals_out->_interpret_subtrace(fun, self.main, avals).call_wrapped(*vals)
A:jax.experimental.jax2tf.jax2tf.vals->tuple((t.val for t in out_tracers))
A:jax.experimental.jax2tf.jax2tf.tf_impl[unexpected]->partial(_unexpected_primitive, unexpected)
A:jax.experimental.jax2tf.jax2tf.x_signed->tensorflow.cast(x, signed_dtype)
A:jax.experimental.jax2tf.jax2tf.res_signed->tensorflow.math.negative(x_signed)
A:jax.experimental.jax2tf.jax2tf.sign->_sign(operand)
A:jax.experimental.jax2tf.jax2tf.floor->tensorflow.math.floor(operand)
A:jax.experimental.jax2tf.jax2tf.cond->tensorflow.math.equal(operand, tf.constant(np.array(0.5), operand.dtype))
A:jax.experimental.jax2tf.jax2tf.x->tensorflow.cast(x, unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.acos_p]->_convert_jax_impl(lax_internal.acos_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.asin_p]->_convert_jax_impl(lax_internal.asin_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.atan_p]->_convert_jax_impl(lax_internal.atan_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.logistic_p]->_convert_jax_impl(lax_internal.logistic_impl, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.complex_component_dtype->{tf.complex64: tf.float32, tf.complex128: tf.float64}.get(y.dtype)
A:jax.experimental.jax2tf.jax2tf.zero->tensorflow.constant(0, complex_component_dtype)
A:jax.experimental.jax2tf.jax2tf.one->tensorflow.constant(1, complex_component_dtype)
A:jax.experimental.jax2tf.jax2tf.i->tensorflow.complex(zero, one)
A:jax.experimental.jax2tf.jax2tf.dtype->_to_tf_dtype(dtype)
A:jax.experimental.jax2tf.jax2tf.shape_tf->_eval_shape(shape)
A:jax.experimental.jax2tf.jax2tf.vec->tensorflow.range(tf.cast(shape_tf[dimension], tf.int32), dtype=tf.int32)
A:jax.experimental.jax2tf.jax2tf.quotient->tensorflow.math.floordiv(lhs, rhs)
A:jax.experimental.jax2tf.jax2tf.select->tensorflow.math.logical_and(tf.not_equal(_sign(lhs), _sign(rhs)), tf.not_equal(tf.math.floormod(lhs, rhs), 0))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.max_p]->partial(_minmax, is_min=False)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.min_p]->partial(_minmax, is_min=True)
A:jax.experimental.jax2tf.jax2tf.y->tensorflow.cast(y, unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.clamp_y->tensorflow.where(_shift_in_bounds(x, y), y, x_bits - 1)
A:jax.experimental.jax2tf.jax2tf.y_lt_x_bits->tensorflow.math.less(y_comp, x_bits)
A:jax.experimental.jax2tf.jax2tf.y_ge_0->tensorflow.math.greater_equal(y_comp, 0)
A:jax.experimental.jax2tf.jax2tf.argnums->tensorflow.nest.flatten(argnums)
A:jax.experimental.jax2tf.jax2tf._out_aval_cast->tensorflow.nest.map_structure(cast_aval, kwargs['_out_aval'])
A:jax.experimental.jax2tf.jax2tf.kwargs->dict(kwargs, _in_avals=_in_avals_cast, _out_aval=_out_aval_cast)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.or_p]->handle_boolean_args(tf.bitwise.bitwise_or, argnums=(0, 1), boolean_f=tf.logical_or)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.and_p]->handle_boolean_args(tf.bitwise.bitwise_and, argnums=(0, 1), boolean_f=tf.logical_and)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.xor_p]->handle_boolean_args(tf.bitwise.bitwise_xor, argnums=(0, 1), boolean_f=tf.math.logical_xor)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.gt_p]->handle_boolean_args(tf.math.greater, argnums=(0, 1), boolean_f=boolean_greater)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.lt_p]->handle_boolean_args(tf.math.less, argnums=(0, 1), boolean_f=boolean_less)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.ge_p]->handle_boolean_args(tf.math.greater_equal, argnums=(0, 1), boolean_f=boolean_greater_or_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.le_p]->handle_boolean_args(tf.math.less_equal, argnums=(0, 1), boolean_f=boolean_less_or_equal)
A:jax.experimental.jax2tf.jax2tf.operand->tensorflow.linalg.adjoint(operand)
A:jax.experimental.jax2tf.jax2tf.op_shape_tf_val->_eval_shape(_in_avals[1].shape, _in_avals[1].dtype)
A:jax.experimental.jax2tf.jax2tf.maxval->tensorflow.broadcast_to(maxval, op_shape_tf_val)
A:jax.experimental.jax2tf.jax2tf.minval->tensorflow.math.minimum(tf.broadcast_to(minval, op_shape_tf_val), maxval)
A:jax.experimental.jax2tf.jax2tf.proto->_scatter_dimensions_proto(scatter_indices.shape, dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.out_tf_shape->_aval_to_tf_shape(_out_aval)
A:jax.experimental.jax2tf.jax2tf.dnums_proto->tensorflow.compiler.xla.xla_data_pb2.DotDimensionNumbers()
A:jax.experimental.jax2tf.jax2tf.precision_config_proto->_precision_config_proto(precision)
A:jax.experimental.jax2tf.jax2tf.tf_version->tuple((int(v) for v in tf.__version__.split('.')[:2]))
A:jax.experimental.jax2tf.jax2tf.k1->gen_conv(_add(lhs_real, lhs_imag), rhs_real, preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.k2->gen_conv(lhs_real, tf.math.subtract(rhs_imag, rhs_real), preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.k3->gen_conv(lhs_imag, _add(rhs_real, rhs_imag), preferred_float_et)
A:jax.experimental.jax2tf.jax2tf.with_1s->tensorflow.reshape(operand, _eval_shape(add_1s_shape, dtype=dtype))
A:jax.experimental.jax2tf.jax2tf.dimensions->tensorflow.range(tf.rank(operand))
A:jax.experimental.jax2tf.jax2tf.new_sizes_tf->_eval_shape(new_sizes, _in_avals[0].dtype)
A:jax.experimental.jax2tf.jax2tf.op_aval->_jax_physical_aval(_in_avals[0])
A:jax.experimental.jax2tf.jax2tf.new_shape->tuple((d for (i, d) in enumerate(op_shape) if i not in dimensions))
A:jax.experimental.jax2tf.jax2tf.new_shape_tf->_eval_shape(new_shape, op_aval.dtype)
A:jax.experimental.jax2tf.jax2tf.(low, high, interior)->jax._src.util.unzip3(padding_config)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_sum_p]->axes_to_axis(tf.reduce_sum)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_prod_p]->axes_to_axis(tf.reduce_prod)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_max_p]->handle_boolean_args(axes_to_axis(tf.reduce_max), argnums=[0], boolean_f=axes_to_axis(tf.reduce_any))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_min_p]->handle_boolean_args(axes_to_axis(tf.reduce_min), argnums=[0], boolean_f=axes_to_axis(tf.reduce_all))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_or_p]->axes_to_axis(tf.reduce_any)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_and_p]->axes_to_axis(tf.reduce_all)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.argmin_p]->partial(_argminmax, True)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.argmax_p]->partial(_argminmax, False)
A:jax.experimental.jax2tf.jax2tf._add_fn->tensorflow.function(_add, autograph=False)
A:jax.experimental.jax2tf.jax2tf._ge_fn->tensorflow.function(tf.math.greater_equal, autograph=False)
A:jax.experimental.jax2tf.jax2tf.a->tensorflow.math.conj(a)
A:jax.experimental.jax2tf.jax2tf.b->tensorflow.transpose(b, transpose_dimensions)
A:jax.experimental.jax2tf.jax2tf.st->_shift_right_logical(t, const(double_word_dtype, nbits))
A:jax.experimental.jax2tf.jax2tf.o_spec->tensorflow.TensorSpec((), dtype=op.dtype)
A:jax.experimental.jax2tf.jax2tf.reducer_fn->tensorflow.function(reducer, autograph=False).get_concrete_function(o_spec, o_spec)
A:jax.experimental.jax2tf.jax2tf.init_val->tensorflow.constant(init_val, operand.dtype)
A:jax.experimental.jax2tf.jax2tf.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax.experimental.jax2tf.jax2tf.closed_jaxpr->jax.core.ClosedJaxpr(update_jaxpr, update_consts)
A:jax.experimental.jax2tf.jax2tf.(res,)->_interpret_jaxpr(closed_jaxpr, arg1, arg2, extra_name_stack=None)
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_sum_p]->partial(_specialized_reduce_window, _add, lambda x: 0, name='reduce_window_sum')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_min_p]->partial(_specialized_reduce_window, partial(_minmax_scalar, is_min=True), _get_min_identity, name='reduce_window_min')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.reduce_window_max_p]->partial(_specialized_reduce_window, partial(_minmax_scalar, is_min=False), _get_max_identity, name='reduce_window_max')
A:jax.experimental.jax2tf.jax2tf.reducer_arg_spec->tuple([tf.TensorSpec((), op.dtype) for op in init_vals] * 2)
A:jax.experimental.jax2tf.jax2tf.xla_reducer_computation->tensorflow.function(reducer_computation, autograph=False).get_concrete_function(*reducer_arg_spec)
A:jax.experimental.jax2tf.jax2tf.outs->tuple((tf.stop_gradient(out) for out in outs))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cummax_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_max, lax_reduce_fn=lax.max, extra_name_stack='cummax')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cummin_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_min, lax_reduce_fn=lax.min, extra_name_stack='cummin')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cumsum_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_sum, lax_reduce_fn=lax.add, extra_name_stack='cumsum')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.cumprod_p]->_cumred(lax_reduce_window_fn=lax_windowed_reductions._reduce_window_prod, lax_reduce_fn=lax.mul, extra_name_stack='cumprod')
A:jax.experimental.jax2tf.jax2tf.init_value->tensorflow.zeros((), operand.dtype)
A:jax.experimental.jax2tf.jax2tf.select_fn->tensorflow.function(tf_impl[select_prim], autograph=False).get_concrete_function(init_value, init_value)
A:jax.experimental.jax2tf.jax2tf.scatter_fn->tensorflow.function(_add, autograph=False).get_concrete_function(init_value, init_value)
A:jax.experimental.jax2tf.jax2tf.converted_impl->_convert_jax_impl(impl_wrapper, multiple_results=False, with_physical_avals=True, extra_name_stack='random_bits')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[random.random_gamma_p]->_convert_jax_impl(partial(random_internal._gamma_impl, use_vmap=True), multiple_results=False, extra_name_stack='random_gamma')
A:jax.experimental.jax2tf.jax2tf.key->tensorflow.compiler.tf2xla.python.xla.bitcast_convert_type(key, _to_tf_dtype(jnp.uint64))
A:jax.experimental.jax2tf.jax2tf.(new_key, res)->tensorflow.compiler.tf2xla.python.xla.rng_bit_generator(algorithm_tf.value, key, shape_tf, dtype=_to_tf_dtype(dtype))
A:jax.experimental.jax2tf.jax2tf.new_key->tensorflow.stop_gradient(new_key)
A:jax.experimental.jax2tf.jax2tf.gather_fill_fn->_convert_jax_impl(lax_slicing._gather_fill, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.start_indices->_maybe_cast_to_int64(tf.stack(start_indices))
A:jax.experimental.jax2tf.jax2tf.slice_sizes_tf->_eval_shape(slice_sizes, dtype=_in_avals[0].dtype)
A:jax.experimental.jax2tf.jax2tf.slices->tuple(map(slice, _eval_shape(start_indices), _eval_shape(limit_indices), _eval_shape(strides)))
A:jax.experimental.jax2tf.jax2tf.clip_fn->_convert_jax_impl(lax_slicing._clamp_scatter_indices, multiple_results=False)
A:jax.experimental.jax2tf.jax2tf.scatter_indices->clip_fn(operand, scatter_indices, updates, dnums=dimension_numbers, _in_avals=_in_avals, _out_aval=_in_avals[1])
A:jax.experimental.jax2tf.jax2tf.xla_update_computation->tensorflow.function(update_computation, autograph=False).get_concrete_function(o_spec, o_spec)
A:jax.experimental.jax2tf.jax2tf.branches_tf->list(map(source_info_util.extend_name_stack('cond'), branches_tf))
A:jax.experimental.jax2tf.jax2tf.(cond_consts, body_consts, init_carry)->jax._src.util.split_list(args, [cond_nconsts, body_nconsts])
A:jax.experimental.jax2tf.jax2tf.(pred,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *args, extra_name_stack='while/cond')
A:jax.experimental.jax2tf.jax2tf.body_tf_func->partial(_interpret_jaxpr, body_jaxpr, *body_consts, extra_name_stack='while/body')
A:jax.experimental.jax2tf.jax2tf.(init_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *init_carry, extra_name_stack='while/body_pred')
A:jax.experimental.jax2tf.jax2tf.pred->tensorflow.reduce_any(pred_b, axis=list(range(len(pred_b.shape))))
A:jax.experimental.jax2tf.jax2tf.pred_b_bcast->_broadcast_in_dim(pred_b, shape=_jax_physical_aval(c_aval).shape, broadcast_dimensions=list(range(len(pred_b.shape))), _in_avals=cond_jaxpr.out_avals, _out_aval=core.ShapedArray(c_aval.shape, np.bool_))
A:jax.experimental.jax2tf.jax2tf.(next_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *selected_carry, extra_name_stack='body_pred')
A:jax.experimental.jax2tf.jax2tf.(_, *res_carry)->tensorflow.while_loop(new_cond_tf_func, new_body_tf_func, (init_pred_b, *init_carry))
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[lax.scan_p]->_convert_jax_impl(lax_control_flow._scan_impl, extra_name_stack='scan')
A:jax.experimental.jax2tf.jax2tf.tf_impl_with_avals[ad_checkpoint.remat_p]->_convert_jax_impl(partial(ad_checkpoint.remat_lowering, is_gpu_platform=False), multiple_results=True, extra_name_stack='checkpoint')
A:jax.experimental.jax2tf.jax2tf.(values, indices)->tensorflow.math.top_k(tf.dtypes.cast(operand, conversion_dtype), k=k, sorted=True)
A:jax.experimental.jax2tf.jax2tf.o_aval->jax.core.ShapedArray((), _to_jax_dtype(op.dtype))
A:jax.experimental.jax2tf.jax2tf.xla_comparator_computation->tensorflow.function(lexicographic_comparator, autograph=False).get_concrete_function(*comparator_spec)
A:jax.experimental.jax2tf.jax2tf.results->_interpret_jaxpr(jaxpr, *sharded_args, extra_name_stack=util.wrap_name(name, 'pjit'))
A:jax.experimental.jax2tf.jax2tf.(FFT, IFFT, RFFT, IRFFT)->list(map(xla_client.FftType, [0, 1, 2, 3]))
A:jax.experimental.jax2tf.jax2tf.result->tensorflow.transpose(result, transpose_dimensions)
A:jax.experimental.jax2tf.jax2tf.(wH, vl)->tensorflow.linalg.eig(tf.linalg.adjoint(operand))
A:jax.experimental.jax2tf.jax2tf.wHH->tensorflow.math.conj(wH)
A:jax.experimental.jax2tf.jax2tf.(w, v)->tensorflow.linalg.eigh(operand)
A:jax.experimental.jax2tf.jax2tf.cast_type->{tf.complex64: tf.float32, tf.complex128: tf.float64}.get(operand.dtype)
A:jax.experimental.jax2tf.jax2tf.w->tensorflow.cast(w, cast_type)
A:jax.experimental.jax2tf.jax2tf.a_shape->_eval_shape(a_aval.shape)
A:jax.experimental.jax2tf.jax2tf.rank->len(a.shape)
A:jax.experimental.jax2tf.jax2tf.num_partition_splits->numpy.prod(partition_dimensions)
A:jax.experimental.jax2tf.jax2tf.tile_assignment->numpy.arange(num_partition_splits).reshape(partition_dimensions)
A:jax.experimental.jax2tf.jax2tf.(dim_tf,)->_eval_shape((dim,))
A:jax.experimental.jax2tf.jax2tf.tuple_wrapper->type(m.a)
A:jax.experimental.jax2tf.jax2tf.list_wrapper->type(m.b)
A:jax.experimental.jax2tf.jax2tf.dict_wrapper->type(m.c)
jax.experimental.jax2tf.convert(fun_jax:Callable,*,polymorphic_shapes=None,with_gradient=True,enable_xla=True,experimental_native_lowering='default')->Callable
jax.experimental.jax2tf.dtype_of_val(val:TfVal)->DType
jax.experimental.jax2tf.jax2tf.TensorFlowTrace(core.Trace)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.get_primitive_impl(self,p:core.Primitive)->Tuple[Callable, bool]
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.lift(self,val:core.Tracer)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_call(self,call_primitive:core.Primitive,out_tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_custom_vjp_call_fwd(self,*_,**__)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_call(self,call_primitive:core.Primitive,fun:lu.WrappedFun,tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_map(self,map_primitive,f,tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_primitive(self,primitive:core.Primitive,tracers:Sequence[TensorFlowTracer],params)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.pure(self,val:TfVal)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.sublift(self,val:TensorFlowTracer)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTracer(self,trace:'TensorFlowTrace',val:TfVal,aval:core.AbstractValue)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.__init__(self,trace:'TensorFlowTrace',val:TfVal,aval:core.AbstractValue)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.aval(self)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.full_lower(self)
jax.experimental.jax2tf.jax2tf._ThreadLocalState(self)
jax.experimental.jax2tf.jax2tf._ThreadLocalState.__init__(self)
jax.experimental.jax2tf.jax2tf._abs(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._add(x:TfVal,y:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._approx_top_k(operand:TfVal,k:int,reduction_dimension:int,recall_target:float,is_max_k:bool,reduction_input_size_override:int,aggregate_to_topk:bool)->Tuple[TfVal, TfVal]
jax.experimental.jax2tf.jax2tf._argminmax(is_min:bool,operand:TfVal,axes:Sequence[int],index_dtype:DType,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._assert_matching_abstract_shape(x:TfVal,shape:Sequence[shape_poly.DimSize])
jax.experimental.jax2tf.jax2tf._atan2(y,x,**kwargs)
jax.experimental.jax2tf.jax2tf._aval_to_tf_shape(aval:core.ShapedArray)->Tuple[Optional[int], ...]
jax.experimental.jax2tf.jax2tf._batched_cond_while(*args:TfVal,cond_nconsts:int,cond_jaxpr:core.ClosedJaxpr,body_nconsts:int,body_jaxpr:core.ClosedJaxpr)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._bitcast_convert_type(operand,new_dtype)
jax.experimental.jax2tf.jax2tf._broadcast_in_dim(operand,*,shape,broadcast_dimensions,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._call_wrapped_with_new_constant_cache(fun:lu.WrappedFun,in_vals:Sequence[TfVal],fresh_constant_cache:bool=False)->Sequence[Tuple[TfVal, core.ShapedArray]]
jax.experimental.jax2tf.jax2tf._cbrt(x)
jax.experimental.jax2tf.jax2tf._clamp(minval,operand,maxval,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._common_reduce_window(operand,init_val,reducer,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._concatenate(*operands,dimension)
jax.experimental.jax2tf.jax2tf._cond(index:TfVal,*operands:TfVal,branches:Sequence[core.ClosedJaxpr],linear:Sequence[bool])->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._conj(x,**kwargs)
jax.experimental.jax2tf.jax2tf._conv_general_dilated(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:lax.ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,lhs_shape:Sequence[int],rhs_shape:Sequence[int],precision:Optional[Tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._conv_general_dimension_numbers_proto(dimension_numbers)
jax.experimental.jax2tf.jax2tf._convert_element_type(operand,*,new_dtype,weak_type=False)
jax.experimental.jax2tf.jax2tf._convert_jax_impl(impl_jax:Callable,*,multiple_results=True,with_physical_avals=False,extra_name_stack:Optional[str]=None)->Callable
jax.experimental.jax2tf.jax2tf._cumred(lax_reduce_fn:Callable,lax_reduce_window_fn:Callable,extra_name_stack:str)
jax.experimental.jax2tf.jax2tf._custom_jvp_call(*args:TfVal,call_jaxpr:core.ClosedJaxpr,jvp_jaxpr_thunk:Callable,num_consts:int)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._custom_lin(*args:TfVal,**_)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._custom_vjp_call_jaxpr(*args:TfVal,fun_jaxpr:core.ClosedJaxpr,**_)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._dim_as_value_jax2tf(dim:shape_poly.DimSize)
jax.experimental.jax2tf.jax2tf._dimension_size_jax2tf(op:TfVal,*,dimension)
jax.experimental.jax2tf.jax2tf._div(lhs,rhs)
jax.experimental.jax2tf.jax2tf._dot_general(lhs,rhs,*,dimension_numbers,precision:Optional[Tuple[PrecisionType,PrecisionType]],preferred_element_type:Optional[DType],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._dynamic_slice(operand,*start_indices,slice_sizes:core.Shape,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._dynamic_update_slice(operand,update,*start_indices,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._eig(operand:TfVal,compute_left_eigenvectors:bool,compute_right_eigenvectors:bool)
jax.experimental.jax2tf.jax2tf._eigh(operand:TfVal,lower:bool,sort_eigenvalues:bool,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._empty(*,dtype)
jax.experimental.jax2tf.jax2tf._eval_shape(shape:Sequence[shape_poly.DimSize],dtype=None)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._extended_name_stack(extra_name_stack:Optional[str])
jax.experimental.jax2tf.jax2tf._fft(x,fft_type,fft_lengths)
jax.experimental.jax2tf.jax2tf._gather(operand,start_indices,*,dimension_numbers,slice_sizes:core.Shape,indices_are_sorted,unique_indices,mode,fill_value,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._get_current_name_stack()->Union[NameStack, str]
jax.experimental.jax2tf.jax2tf._get_max_identity(tf_dtype)
jax.experimental.jax2tf.jax2tf._get_min_identity(tf_dtype)
jax.experimental.jax2tf.jax2tf._get_shape_from_tensor_or_array(x)
jax.experimental.jax2tf.jax2tf._integer_pow(x,*,y:int,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._interpret_fun_jax(fun_jax:Callable,args_tf:Sequence[TfVal],args_avals:Sequence[core.ShapedArray],extra_name_stack:Optional[str],fresh_constant_cache:bool=False,experimental_native_lowering:bool=False)->Tuple[Sequence[TfVal], Tuple[core.ShapedArray]]
jax.experimental.jax2tf.jax2tf._interpret_jaxpr(jaxpr:core.ClosedJaxpr,*args_tf:TfVal,extra_name_stack:Optional[str])->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._interpret_subtrace(main:core.MainTrace,in_avals:Sequence[core.ShapedArray],*in_vals:TfVal)
jax.experimental.jax2tf.jax2tf._iota(*,dtype,shape,dimension)
jax.experimental.jax2tf.jax2tf._is_tfval(v:TfVal)->bool
jax.experimental.jax2tf.jax2tf._jax_physical_aval(aval:core.ShapedArray)->core.ShapedArray
jax.experimental.jax2tf.jax2tf._jax_physical_dtype(dtype)
jax.experimental.jax2tf.jax2tf._linear_solve(*args:TfVal,const_lengths,jaxprs,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._lower_native_and_run(fun_jax:Callable,args_avals:Sequence[core.ShapedArray],args_tf:Sequence[TfVal])->Tuple[Sequence[TfVal], Tuple[core.ShapedArray]]
jax.experimental.jax2tf.jax2tf._lu(operand:TfVal,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._maybe_cast_to_int64(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._maybe_decode_gda(gda_or_py_object:Any)
jax.experimental.jax2tf.jax2tf._minmax(x:TfVal,y:TfVal,*,is_min:bool,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)->TfVal
jax.experimental.jax2tf.jax2tf._minmax_scalar(x:TfVal,y:TfVal,*,is_min:bool)->TfVal
jax.experimental.jax2tf.jax2tf._neg(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._not(x)
jax.experimental.jax2tf.jax2tf._pad(operand,padding_value,*,padding_config,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._pjit(*args:TfVal,jaxpr:core.ClosedJaxpr,in_shardings:Sequence[sharding.XLACompatibleSharding],out_shardings:Sequence[sharding.XLACompatibleSharding],resource_env:maps.ResourceEnv,donated_invars,name:str,in_positional_semantics,out_positional_semantics,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)->TfVal
jax.experimental.jax2tf.jax2tf._pjit_sharding_constraint(arg:TfVal,*,sharding:sharding.MeshPspecSharding,resource_env:maps.ResourceEnv,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray,**kwargs)->TfVal
jax.experimental.jax2tf.jax2tf._population_count(x)
jax.experimental.jax2tf.jax2tf._precision__config_module_proto(precision:Optional[Tuple[PrecisionType,PrecisionType]])
jax.experimental.jax2tf.jax2tf._precision_config_proto(precision:Optional[Tuple[PrecisionType,PrecisionType]])
jax.experimental.jax2tf.jax2tf._qr(operand,full_matrices)
jax.experimental.jax2tf.jax2tf._random_bits_impl(keys:TfVal,*,bit_width,shape,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_fold_in_impl(keys:TfVal,msgs:TfVal,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_seed_impl(seeds:TfVal,*,impl,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_split_impl(keys:TfVal,*,count,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_unwrap_impl(keys:TfVal,*,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._random_wrap_impl(base_arr:TfVal,*,impl,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._reduce(*operands:TfVal,computation:Callable,jaxpr:core.Jaxpr,consts:Sequence[Any],dimensions:Sequence[int],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._reduce_window(*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._register_checkpoint_pytrees()
jax.experimental.jax2tf.jax2tf._rem(lhs,rhs)
jax.experimental.jax2tf.jax2tf._reshape(operand,*,new_sizes,dimensions,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._rev(operand,*,dimensions)
jax.experimental.jax2tf.jax2tf._rng_bit_generator(key:TfVal,*,shape,dtype,algorithm)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._rng_uniform(minval:TfVal,maxval:TfVal,*,shape)->TfVal
jax.experimental.jax2tf.jax2tf._round(operand,*,rounding_method,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._sanitize_scope_name(name)
jax.experimental.jax2tf.jax2tf._scatter(operand,scatter_indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._select_and_gather_add(tangents:TfVal,operand:TfVal,select_prim:core.Primitive,window_dimensions:Sequence[int],window_strides:Sequence[int],base_dilation:Sequence[int],window_dilation:Sequence[int],padding:Sequence[Tuple[int,int]],_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._select_and_scatter(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.experimental.jax2tf.jax2tf._select_and_scatter_add(source,operand,*,select_prim,window_dimensions,window_strides,padding,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._shard_value(val:TfVal,aval:core.ShapedArray,sd:sharding.XLACompatibleSharding)->TfVal
jax.experimental.jax2tf.jax2tf._shift_in_bounds(x:TfVal,y:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._shift_left(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_arithmetic(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_arithmetic_raw(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_logical(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_logical_raw(x,y)
jax.experimental.jax2tf.jax2tf._sign(x:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf._slice(operand,start_indices,limit_indices,strides,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._sort(*operands:TfVal,dimension:int,is_stable:bool,num_keys:int)->Tuple[TfVal, ...]
jax.experimental.jax2tf.jax2tf._specialized_reduce_window(reducer,identity,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation,_in_avals,_out_aval,name=None)
jax.experimental.jax2tf.jax2tf._squeeze(operand,*,dimensions,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._svd(operand,full_matrices,compute_uv)
jax.experimental.jax2tf.jax2tf._tfval_to_tensor_jax_dtype(val:TfVal,jax_dtype:Optional[DType]=None,memoize_constants=False)->Tuple[TfVal, DType]
jax.experimental.jax2tf.jax2tf._threefry2x32_jax_impl(*args:TfVal,_in_avals,_out_aval)
jax.experimental.jax2tf.jax2tf._to_jax_dtype(tf_dtype)
jax.experimental.jax2tf.jax2tf._to_tf_dtype(jax_dtype)
jax.experimental.jax2tf.jax2tf._top_k(operand:TfVal,k:int)->Tuple[TfVal, TfVal]
jax.experimental.jax2tf.jax2tf._transpose(operand,*,permutation)
jax.experimental.jax2tf.jax2tf._triangular_solve(a:TfVal,b:TfVal,*,left_side:bool,lower:bool,transpose_a:bool,conjugate_a:bool,unit_diagonal:bool,_in_avals:Sequence[core.ShapedArray],_out_aval:core.ShapedArray)
jax.experimental.jax2tf.jax2tf._tridiagonal_solve(*args:TfVal,_in_avals,_out_aval,**params)
jax.experimental.jax2tf.jax2tf._unexpected_primitive(p:core.Primitive,*args,**kwargs)
jax.experimental.jax2tf.jax2tf._where(which,*cases)
jax.experimental.jax2tf.jax2tf._while(*args:TfVal,cond_nconsts:int,cond_jaxpr:core.ClosedJaxpr,body_nconsts:int,body_jaxpr:core.ClosedJaxpr)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf.convert(fun_jax:Callable,*,polymorphic_shapes=None,with_gradient=True,enable_xla=True,experimental_native_lowering='default')->Callable
jax.experimental.jax2tf.jax2tf.dtype_of_val(val:TfVal)->DType
jax.experimental.jax2tf.jax2tf.flatten_fun_jax(fun_jax:Callable,args_tf:Sequence[TfVal],kwargs_tf:Dict[str,TfVal])->Tuple[Callable, Sequence[TfVal], Any, Callable]
jax.experimental.jax2tf.jax2tf.handle_boolean_args(f,argnums:Sequence[int],boolean_f=None)
jax.experimental.jax2tf.jax2tf.inside_call_tf()
jax.experimental.jax2tf.jax2tf.make_custom_gradient_fn_tf(fun_flat_jax:Callable,args_flat_tf:Sequence[TfVal],polymorphic_shapes_flat:Sequence[str],args_avals_flat:Sequence[core.ShapedArray],out_avals:Sequence[core.ShapedArray])
jax.experimental.jax2tf.jax2tf.preprocess_arg_tf(arg_idx:int,arg_tf:TfVal,polymorphic_shape:Optional[str])->Tuple[TfVal, core.ShapedArray]
jax.experimental.jax2tf.jax2tf.split_to_logical_devices(tensor:TfVal,partition_dimensions:pxla.PartitionsOrReplicated)
jax.experimental.jax2tf.split_to_logical_devices(tensor:TfVal,partition_dimensions:pxla.PartitionsOrReplicated)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax2tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax2tf_test.xs->numpy.ones((8, 256), dtype=np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.y->jax.numpy.tanh(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.cf->f_tf(inputs).get_concrete_function([1.0, 2.0, 3.0], 4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf->tensorflow.function(f_tf)
A:jax.experimental.jax2tf.tests.jax2tf_test.v->tensorflow.Variable(0.7, dtype=jax2tf.dtype_of_val(0.7))
A:jax.experimental.jax2tf.tests.jax2tf_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.jax2tf_test.f_conc->tensorflow.function(f_tf, autograph=True).get_concrete_function(tf.convert_to_tensor(x))
A:jax.experimental.jax2tf.tests.jax2tf_test.x->numpy.arange(np.prod(shape), dtype=np.float32).reshape(shape)
A:jax.experimental.jax2tf.tests.jax2tf_test.n->jax.local_device_count()
A:jax.experimental.jax2tf.tests.jax2tf_test.result->jax.experimental.jax2tf.convert(f_jax)(a, b)
A:jax.experimental.jax2tf.tests.jax2tf_test.big_const->numpy.full((5,), 2 ** 33, dtype=dtype)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_conv->tensorflow.function(f_conv)
A:jax.experimental.jax2tf.tests.jax2tf_test._->tape.gradient(y, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.default_float_type->jax.experimental.jax2tf.dtype_of_val(4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.(u, v)->f_tf(x, y)
A:jax.experimental.jax2tf.tests.jax2tf_test.default_float_dtype->jax.experimental.jax2tf.dtype_of_val(4.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.uv->f_tf((x, y))
A:jax.experimental.jax2tf.tests.jax2tf_test.res_jax->pjit(func_jax, in_axis_resources=in_axis_resources, out_axis_resources=out_axis_resources)(x, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf->f_tf(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf_2->tensorflow.function(f_tf, autograph=False, jit_compile=True)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.my_model->tensorflow.Module()
A:jax.experimental.jax2tf.tests.jax2tf_test.my_model.f->tensorflow.function(f_tf_wrapped, autograph=False, input_signature=[tf.TensorSpec([], tf.float32), tf.TensorSpec([], tf.float32)])
A:jax.experimental.jax2tf.tests.jax2tf_test.model_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(my_model)))
A:jax.experimental.jax2tf.tests.jax2tf_test.restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.tests.jax2tf_test.res_tf_3->restored_f(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_jax->jax.grad(f_jax)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.res->tensorflow.function(f_tf, jit_compile=True, autograph=False)(x1)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_tf->tape.gradient(res, xv)
A:jax.experimental.jax2tf.tests.jax2tf_test.inputs->collections.OrderedDict()
A:jax.experimental.jax2tf.tests.jax2tf_test.u->f_tf(inputs)
A:jax.experimental.jax2tf.tests.jax2tf_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.grad_g->jax.grad(g, allow_int=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.d_dx_jax->grad_g(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.d_dx_tf->jax.experimental.jax2tf.convert(grad_g)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_tf_native->tape.gradient(res, xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_tf_native_0->tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.conv_fn->tensorflow.function(conv_fn, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.g_jax2tf->tape.gradient(res, xs, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.state->dict(float_used=np.array([0.7, 0.9], dtype=np.float32), float_passthrough=np.float16(1.0), float_unused=np.array([1.1, 2.2, 3.3], dtype=np.float32), int_used=np.int16(5), int_passthrough=np.int8(7), int_unused=np.array([1, 2, 3], dtype=np.uint32), bool_used=np.array([True, False, False, True], dtype=np.bool_), bool_passthrough=np.array([True, False, False, True, False], dtype=np.bool_), bool_unused=np.array([[True, False], [False, True]], dtype=np.bool_))
A:jax.experimental.jax2tf.tests.jax2tf_test.(vjp_jax_fun, args_vjp)->jax.experimental.jax2tf.tests.tf_test_util.TransformJaxVJP(jax_f, args, res_jax)
A:jax.experimental.jax2tf.tests.jax2tf_test.(grad_jax,)->vjp_jax_fun(*args_vjp)
A:jax.experimental.jax2tf.tests.jax2tf_test.what_keys->set(what.keys())
A:jax.experimental.jax2tf.tests.jax2tf_test.expected_keys->set(expected.keys())
A:jax.experimental.jax2tf.tests.jax2tf_test.e->numpy.ones_like(w)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_0,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.ZERO)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_None,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(jax_f, args, unconnected_gradients=tf.UnconnectedGradients.NONE)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_jax->tensorflow.function(f_tf_jax, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_jax_0,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, (grad_tf_jax_None,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_jax, args, unconnected_gradients=tf.UnconnectedGradients.NONE)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_vjp_jax_fun->jax.experimental.jax2tf.convert(vjp_jax_fun)
A:jax.experimental.jax2tf.tests.jax2tf_test.(grad_tf_vjp_jax,)->tf_vjp_jax_fun(*args_vjp)
A:jax.experimental.jax2tf.tests.jax2tf_test.xv->tensorflow.nest.map_structure(tf.Variable, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.m->tensorflow.Module()
A:jax.experimental.jax2tf.tests.jax2tf_test.net->JaxModule()
A:jax.experimental.jax2tf.tests.jax2tf_test.images->tensorflow.ones([1, 784])
A:jax.experimental.jax2tf.tests.jax2tf_test.loss->tensorflow.reduce_sum(net(images))
A:jax.experimental.jax2tf.tests.jax2tf_test.params->tape.watched_variables()
A:jax.experimental.jax2tf.tests.jax2tf_test.grads->tape.gradient(loss, params)
A:jax.experimental.jax2tf.tests.jax2tf_test.x2->numpy.ones((3, 4), dtype=np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.x3->jax.numpy.sin(x2)
A:jax.experimental.jax2tf.tests.jax2tf_test.x4->jax.numpy.sin(x3)
A:jax.experimental.jax2tf.tests.jax2tf_test.remat_f->jax.ad_checkpoint.checkpoint(f)
A:jax.experimental.jax2tf.tests.jax2tf_test.arg->numpy.array(3.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_graph->tensorflow.function(f_tf).get_concrete_function().graph.as_graph_def()
A:jax.experimental.jax2tf.tests.jax2tf_test.sin_1->jax.experimental.jax2tf.convert(jnp.sin)(1.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.out->jax.experimental.jax2tf.convert(caller, with_gradient=False)(2.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.run_graph->run.get_concrete_function().graph.as_graph_def()
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn_scalar->jax.experimental.jax2tf.convert(jax_fn_scalar)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn_array->jax.experimental.jax2tf.convert(jax_fn_array)
A:jax.experimental.jax2tf.tests.jax2tf_test.const->numpy.random.uniform(size=(16, 16)).astype(np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_nr_consts->self.CountLargeTfConstants(jax2tf.convert(f), const)
A:jax.experimental.jax2tf.tests.jax2tf_test.f1_nr_consts->self.CountLargeTfConstants(jax2tf.convert(f1), xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.f2_nr_consts->self.CountLargeTfConstants(jax2tf.convert(f2), xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.(res, _)->jax.lax.scan(lambda carry, x: (carry + x + const, None), np.zeros((256,), dtype=np.float32), xs)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf_graph_nr_consts->self.CountLargeTfConstants(jax2tf.convert(f), const)
A:jax.experimental.jax2tf.tests.jax2tf_test.mul->jax.jit(jnp.multiply)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fn->jax.experimental.jax2tf.convert(lambda x: mul(x, 2.0))
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun_with_xla->jax.experimental.jax2tf.convert(fun, enable_xla=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun_without_xla->jax.experimental.jax2tf.convert(fun, enable_xla=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun2_without_xla->jax.experimental.jax2tf.convert(lambda x: fun(x), enable_xla=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_fun2_with_xla->jax.experimental.jax2tf.convert(lambda x: fun(x), enable_xla=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.user_frame->jax._src.source_info_util.user_frame(source_info_util.current())
A:jax.experimental.jax2tf.tests.jax2tf_test.z->jax.named_call(f_callee, name='callee')(y)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, acc)->jax.lax.while_loop(lambda i_acc: i_acc[0] <= 5, body_fun, (0, x))
A:jax.experimental.jax2tf.tests.jax2tf_test.new_carry->jax.numpy.sin(carry)
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, carry)->jax.lax.while_loop(lambda carry: jnp.all(carry <= x), body_fun, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_comp->jax.xla_computation(f_while)(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.backend->jax._src.lib.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.jax2tf_test.modules->jax._src.lib.xla_bridge.get_backend().compile(jax_comp).hlo_modules()
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_opt_hlo->modules[0].to_string()
A:jax.experimental.jax2tf.tests.jax2tf_test.lowered->jax.jit(f_jax, abstracted_axes=abstracted_axes).lower(*args)
A:jax.experimental.jax2tf.tests.jax2tf_test.mhlo_module->jax.jit(f_jax, abstracted_axes=abstracted_axes).lower(*args).compiler_ir(dialect='mhlo')
A:jax.experimental.jax2tf.tests.jax2tf_test.mhlo_module_text->jax.experimental.jax2tf.jax2tf._fixup_mhlo_module_text(mhlo_module_text)
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_res->f_jax(x1)
A:jax.experimental.jax2tf.tests.jax2tf_test.count->numpy.int32(5)
A:jax.experimental.jax2tf.tests.jax2tf_test.x1->numpy.ones((5,), dtype=np.float32)
A:jax.experimental.jax2tf.tests.jax2tf_test.global_data->numpy.arange(np.prod(global_shape)).reshape(global_shape)
A:jax.experimental.jax2tf.tests.jax2tf_test.global_mesh->jax._src.test_util.create_global_mesh((4, 2), ('x', 'y'))
A:jax.experimental.jax2tf.tests.jax2tf_test.mesh_axes->P(('x', 'y'))
A:jax.experimental.jax2tf.tests.jax2tf_test.(params, _)->create_gda((8, 2), global_mesh, mesh_axes)
A:jax.experimental.jax2tf.tests.jax2tf_test.input_data->numpy.arange(16).reshape(2, 8)
A:jax.experimental.jax2tf.tests.jax2tf_test.handle->pjit(jnp.matmul, in_axis_resources=(P('y', 'x'), FROM_GDA), out_axis_resources=None)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_func->tensorflow.function(jax2tf.convert(jax_func, enable_xla=True), jit_compile=True)
A:jax.experimental.jax2tf.tests.jax2tf_test.jax_out->jax_func(input_data=input_data)
A:jax.experimental.jax2tf.tests.jax2tf_test.tf_out->tf_func(input_data=input_data)
A:jax.experimental.jax2tf.tests.jax2tf_test.module->get_serialized_computation(func_jax, x, x, use_pjit=True, in_axis_resources=in_axis_resources, out_axis_resources=out_axis_resources)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.setUp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_64bit_behavior_enable_x64(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_64bit_behavior_not_enable_x64(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_argument_eager_tensor(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_basics(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_constant(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_passed_by_tf(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_returned_by_jax(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_tf_grad(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_checkpoint_name(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_checkpoint_wrapper_types(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_callable_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_tensor_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_nullary_func(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_of_nested_dependent_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_of_nested_independent_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_under_transform_error(self,transform='vmap')
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_under_transform_error_non_tracer(self,transform='vmap')
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_64bit(self,dtype=np.int64,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_jax_arrays(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_jvp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_pytree_readme(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_vjp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_device_array_arg(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_empty(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_enable_xla(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_function(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_grad_kwargs(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradient_with_float0_intermediate(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradient_with_float0_result(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_disabled(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_int_argument(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_pytree(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_unused_argument_readme(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_jvp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_vjp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_ordered_dict_input(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_input_output_naming(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_issue_10586(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_kwargs(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_name_scope(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_convert_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_convert_error_non_tracer(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit_is_compiled(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit_pytree(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_batched_while(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_disabled(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_named(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_simple(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_sub_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_op_metadata_while_and_cond(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_pytrees(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_randint(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_readme_gradient_int(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat_free_var(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_cond(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_shared_constants_under_scan(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_variable_input(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_weak_types(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_global_device_array(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_multiple_args_results(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_pjit_basic1D(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_shape_poly_arange(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_simple(self)
jax.experimental.jax2tf.tests.jax2tf_test.XlaCallModuleTest.test_while(self)
jax.experimental.jax2tf.tests.jax2tf_test.get_serialized_computation(f_jax:Callable,*args,abstracted_axes:Optional[Tuple[Dict[int,str]]]=None,use_pjit:bool=False,in_axis_resources=None,out_axis_resources=None)->str


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/savedmodel_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.savedmodel_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.savedmodel_test.model->tensorflow.Module()
A:jax.experimental.jax2tf.tests.savedmodel_test.model.f->tensorflow.function(prediction_tf, jit_compile=True)
A:jax.experimental.jax2tf.tests.savedmodel_test.x->numpy.ones((2, 3), dtype=np.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.restored_model->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model, save_gradients=False)
A:jax.experimental.jax2tf.tests.savedmodel_test.primal_out->f_jax(x)
A:jax.experimental.jax2tf.tests.savedmodel_test.xv->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.savedmodel_test.y->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model, save_gradients=False).f(xv)
A:jax.experimental.jax2tf.tests.savedmodel_test._->tensorflow.saved_model.save(model, save_dir, options=options)
A:jax.experimental.jax2tf.tests.savedmodel_test.params_vars->tensorflow.nest.map_structure(tf.Variable, params)
A:jax.experimental.jax2tf.tests.savedmodel_test.model._variables->tensorflow.nest.flatten(params_vars)
A:jax.experimental.jax2tf.tests.savedmodel_test.state->numpy.array([1], dtype=np.int32)
A:jax.experimental.jax2tf.tests.savedmodel_test.params->dict(w=np.ones((3, 4), dtype=np.float32), b=np.ones((2, 4), dtype=np.float32))
A:jax.experimental.jax2tf.tests.savedmodel_test.res_out->tensorflow.zeros((batch_size, 2), dtype=tf.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.(res, state_out)->converted_fun_with_custom_gradient(params, state)
A:jax.experimental.jax2tf.tests.savedmodel_test.params_v->tensorflow.nest.map_structure(tf.Variable, params)
A:jax.experimental.jax2tf.tests.savedmodel_test.loss->tensorflow.reduce_sum(res)
A:jax.experimental.jax2tf.tests.savedmodel_test.g->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.model.fn->tensorflow.function(tf_predict, autograph=False)
A:jax.experimental.jax2tf.tests.savedmodel_test.save_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))
A:jax.experimental.jax2tf.tests.savedmodel_test.options->tensorflow.saved_model.SaveOptions(experimental_custom_gradients=True)
A:jax.experimental.jax2tf.tests.savedmodel_test.restored_module->tensorflow.saved_model.load(save_dir)
A:jax.experimental.jax2tf.tests.savedmodel_test.f_tf->jax.experimental.jax2tf.convert(f_jax)
A:jax.experimental.jax2tf.tests.savedmodel_test.res->restored_f(params_v, x)
A:jax.experimental.jax2tf.tests.savedmodel_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(composed_fn, input_args=[x_str])
A:jax.experimental.jax2tf.tests.savedmodel_test.res_restored->restored_f(*args)
A:jax.experimental.jax2tf.tests.savedmodel_test.res_jax->f_jax(params, x)
A:jax.experimental.jax2tf.tests.savedmodel_test.res_tf->composed_fn(x_str)
A:jax.experimental.jax2tf.tests.savedmodel_test.(restored_f, restored_model)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=(params, x), save_gradients=True)
A:jax.experimental.jax2tf.tests.savedmodel_test.g_tf->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.g_restored_f->tape.gradient(loss, params_v)
A:jax.experimental.jax2tf.tests.savedmodel_test.arr->numpy.arange(10, dtype=np.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.numbers_f32->tensorflow.strings.to_number(x_str, out_type=tf.float32)
A:jax.experimental.jax2tf.tests.savedmodel_test.numbers_f16->tensorflow.cast(numbers_f32, tf.float16)
A:jax.experimental.jax2tf.tests.savedmodel_test.x_str->numpy.array(['3.14', '2.78'])
A:jax.experimental.jax2tf.tests.savedmodel_test.res_tf_restored->restored_f(x_str)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest._compare_with_saved_model(self,f_jax,*args)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_eval(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient_disabled(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient_nested(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_pytree(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_grad_integers(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_without_embedding_params(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_save_without_gradients(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_tf_mix_jax_with_uncompileable(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_xla_context_preserved_gather(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_xla_context_preserved_slice(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/sharding_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.sharding_test.prev_xla_flags->os.getenv('XLA_FLAGS')
A:jax.experimental.jax2tf.tests.sharding_test.jax_comp->f_jax.lower(*args).compiler_ir(dialect='hlo')
A:jax.experimental.jax2tf.tests.sharding_test.jax_hlo->f_jax.lower(*args).compiler_ir(dialect='hlo').as_hlo_text()
A:jax.experimental.jax2tf.tests.sharding_test.backend->jax._src.lib.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.sharding_test.device_assignment->numpy.reshape(device_assignment, (-1, num_partitions))
A:jax.experimental.jax2tf.tests.sharding_test.compile_options->jax._src.lib.xla_bridge.get_compile_options(num_replicas=num_replicas, num_partitions=num_partitions, device_assignment=device_assignment, use_spmd_partitioning=use_spmd_partitioning)
A:jax.experimental.jax2tf.tests.sharding_test.jax_optimized_hlo->jax._src.lib.xla_bridge.get_backend().compile(jax_comp, compile_options).hlo_modules()[0].to_string()
A:jax.experimental.jax2tf.tests.sharding_test.f_tf_base->jax.experimental.jax2tf.convert(f_jax, with_gradient=False)
A:jax.experimental.jax2tf.tests.sharding_test.f_tf_fun->tensorflow.function(f_tf, jit_compile=True, autograph=False)
A:jax.experimental.jax2tf.tests.sharding_test.tf_hlo->tensorflow.function(f_tf, jit_compile=True, autograph=False).experimental_get_compiler_ir(*args)(stage='hlo', device_name=device_name)
A:jax.experimental.jax2tf.tests.sharding_test.tf_optimized_hlo->tensorflow.function(f_tf, jit_compile=True).experimental_get_compiler_ir(*args)(stage='optimized_hlo', device_name=device_name)
A:jax.experimental.jax2tf.tests.sharding_test.x->numpy.arange(np.prod(shape), dtype=np.float32).reshape(shape)
A:jax.experimental.jax2tf.tests.sharding_test.hlo->jax_func.lower(x, x).compiler_ir(dialect='hlo').as_hlo_text()
A:jax.experimental.jax2tf.tests.sharding_test.y->jax.experimental.pjit.with_sharding_constraint(y, P('x', 'y'))
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest._assert_sharding_annotations(self,what:str,hlo:str,expected:Sequence[str])
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest._check_sharding_annotations(self,f_jax,args:Sequence[Any],*,expected:Sequence[str],expected_opt:Sequence[str],num_partitions=2,num_variables=0)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest.test_pjit_ShardingConstraint(self)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest.test_pjit_TwoMeshAxisSharding(self)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest.test_pjit_basic1D(self)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest.test_pjit_basic1D_variable(self)
jax.experimental.jax2tf.tests.sharding_test.ShardedJitHloTest.test_pjit_basic2D(self)
jax.experimental.jax2tf.tests.sharding_test.setUpModule()
jax.experimental.jax2tf.tests.sharding_test.tearDownModule()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax_primitives_coverage_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.all_dtypes->set(jtu.dtypes.all)
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.dtypes_tested->dtypes_tested.union({h.dtype}).union({h.dtype})
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.devices->', '.join(l.devices)
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.template->f.read()
A:jax.experimental.jax2tf.tests.jax_primitives_coverage_test.output_file->os.path.join(os.path.dirname(__file__), '../g3doc/jax_primitives_coverage.md')
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest.test_generate_primitives_coverage_doc(self)
jax.experimental.jax2tf.tests.jax_primitives_coverage_test.JaxPrimitiveTest.test_jax_implemented(self,harness:primitive_harness.Harness)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/jax2tf_limitations.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax2tf_limitations.group_method->getattr(cls, harness.group_name, None)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.limitations->group_method(harness)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.supported_dtypes->jax._src.test_util.supported_dtypes()
A:jax.experimental.jax2tf.tests.jax2tf_limitations.nr_special_cases->numpy.count_nonzero(special_cases)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.norm->numpy.linalg.norm(x, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.rank->len(a.shape)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.aH->jax.numpy.conj(a.transpose(list(range(rank - 2)) + [rank - 1, rank - 2]))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.wC->jax.numpy.conj(w)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.closest_diff->min(abs(eigenvalues_array - eigenvalue))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.result->numpy.reshape(np.array(result, dtype=dtype), [*batch_dims, m, m])
A:jax.experimental.jax2tf.tests.jax2tf_limitations.k->min(m, n)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.p_mat->_make_permutation_matrix(perm)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.mask->numpy.isnan(result_jax)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.operand_jax->reconstruct_operand(r_jax)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.operand_tf->reconstruct_operand(r_tf)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.error_norm->jax.numpy.linalg.norm(operand_jax - operand_tf, axis=(-2, -1))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.max_backward_error->jax.numpy.amax(backward_error)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.forward_diff->jax.numpy.diff(s, axis=-1, append=forward_appendant)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.absolute_gap->compute_absolute_gap(r_jax[0], m, n)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.sum_of_ratios->jax.numpy.sum(jnp.divide(y, x), -2, keepdims=True)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.phases->jax.numpy.divide(sum_of_ratios, jnp.abs(sum_of_ratios))
A:jax.experimental.jax2tf.tests.jax2tf_limitations.output->jax.numpy.sum(jnp.einsum('...ij,...ij->...ij', a.conj(), b, precision=lax.Precision.HIGHEST), axis=-2)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.cos_angular_diff->jax.numpy.clip(cos_angular_diff, a_min=0.0, a_max=1.0)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.angular_diff->jax.numpy.arccos(cos_angular_diff)
A:jax.experimental.jax2tf.tests.jax2tf_limitations.v_jax->jax.numpy.swapaxes(r_jax[2][..., :rank, :], -2, -1).conj()
A:jax.experimental.jax2tf.tests.jax2tf_limitations.v_tf->jax.numpy.swapaxes(r_tf[2][..., :rank, :], -2, -1).conj()
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation(self,description:str,*,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Union[DType,Sequence[DType]]=(),enabled:bool=True,modes=('eager','graph','compiled'),skip_tf_run=False,expect_tf_error:bool=True,skip_comparison=False,custom_assert:Optional[Callable]=None,tol=None)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.__init__(self,description:str,*,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Union[DType,Sequence[DType]]=(),enabled:bool=True,modes=('eager','graph','compiled'),skip_tf_run=False,expect_tf_error:bool=True,skip_comparison=False,custom_assert:Optional[Callable]=None,tol=None)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation._pow_test_util(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.acos(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.acosh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.approx_max_k(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.argmax(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.argmin(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.asin(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.asinh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.atan(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.atanh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.bessel_i0e(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.bessel_i1e(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cholesky(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.conv_general_dilated(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cumprod(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.cumsum(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.custom_linear_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.digamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.div(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.dot_general(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.eig(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.eigh(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erf(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erf_inv(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.erfc(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.expm1(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.fft(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.filter(self,dtype:Optional[DType]=None,device:Optional[str]=None,mode:Optional[str]=None)->bool
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.get_max_tolerance_limitation(self,limitations:Sequence['Jax2TfLimitation'])->Optional['Jax2TfLimitation']
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.helper_get_trig_custom_limitation(cls,np_inverse)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.igamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.igammac(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.integer_pow(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.lgamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.limitations_for_harness(cls,harness:primitive_harness.Harness)->Sequence['Jax2TfLimitation']
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.log1p(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.lu(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.max(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.min(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.nextafter(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.pow(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.qr(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_fold_in(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_gamma(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_seed(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.random_split(cls,handess:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_max(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_min(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.reduce_window_add(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.regularized_incomplete_beta(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.rem(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.rng_bit_generator(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.round(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_add(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_max(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_min(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.scatter_mul(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.select_and_gather_add(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.sort(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.svd(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tan(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tanh(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.top_k(cls,harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.triangular_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.tridiagonal_solve(cls,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.jax2tf_limitations.custom_numeric(*,description='customnumericcomparison',dtypes=(),modes=('eager','graph'),devices=('cpu','gpu','tpu'),custom_assert=None,enabled=True,tol=None)->Jax2TfLimitation
jax.experimental.jax2tf.tests.jax2tf_limitations.custom_random_keys_output()
jax.experimental.jax2tf.tests.jax2tf_limitations.missing_tf_kernel(*,description='opnotdefinedfordtype',dtypes,modes=('eager','graph','compiled'),devices=('cpu','gpu','tpu'),enabled=True)->Jax2TfLimitation


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/control_flow_ops_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.control_flow_ops_test.res->jax.lax.cond(True, lambda op: op * x, lambda op: op + x, x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.cond_const->numpy.ones(3, dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const1->numpy.full_like(cond_const, 1.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const2->numpy.full_like(cond_const, 2.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.xs->numpy.arange(4, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.ys->numpy.arange(5, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const->numpy.ones((2,), dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.arg->numpy.full((5,), 0.7)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(c_out, _)->jax.lax.scan(body, 0.0, (xs, ys))
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(res1, res2)->jax.lax.scan(body_fun, 0.0, xs + 1.0)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.setUp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_multiple_results(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_units(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_remat(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_batched_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_single_carry(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/models_test_main.py----------------------------------------
A:jax.experimental.jax2tf.tests.models_test_main.header->header.lower().replace(' ', '-').lower().replace(' ', '-')
A:jax.experimental.jax2tf.tests.models_test_main.output_path->os.path.join(os.path.dirname(__file__), '../g3doc/convert_models_results.md')
A:jax.experimental.jax2tf.tests.models_test_main.template->template.replace('{{errors}}', '\n'.join(error_lines)).replace('{{errors}}', '\n'.join(error_lines))
A:jax.experimental.jax2tf.tests.models_test_main.msg->msg.replace('\\n', '\n').replace('\\t', '\t').replace('\\n', '\n').replace('\\t', '\t')
A:jax.experimental.jax2tf.tests.models_test_main.dtype->jax._src.dtypes.canonicalize_dtype(x.dtype)
A:jax.experimental.jax2tf.tests.models_test_main.converters->list(filter(lambda x: x.name in FLAGS.converters, ALL_CONVERTERS))
A:jax.experimental.jax2tf.tests.models_test_main.harness->harness_fn()
A:jax.experimental.jax2tf.tests.models_test_main.np_assert_allclose->functools.partial(np.testing.assert_allclose, rtol=harness.rtol)
A:jax.experimental.jax2tf.tests.models_test_main.apply_tf->converter.convert_fn(harness)
A:jax.experimental.jax2tf.tests.models_test_main.jax_result->harness_fn().apply_with_vars(*xs)
A:jax.experimental.jax2tf.tests.models_test_main.tf_result->apply_tf(*xs)
A:jax.experimental.jax2tf.tests.models_test_main.error_msg->repr(e).replace('\n\n', '\n')
jax.experimental.jax2tf.tests.models_test_main._crop_convert_error(msg:str)->str
jax.experimental.jax2tf.tests.models_test_main._get_random_data(x:jnp.ndarray)->np.ndarray
jax.experimental.jax2tf.tests.models_test_main._write_markdown(results:Dict[str,List[Tuple[str,str]]])->None
jax.experimental.jax2tf.tests.models_test_main.main(argv:Sequence[str])->None
jax.experimental.jax2tf.tests.models_test_main.test_converters()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/primitive_harness.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitive_harness.all_args->self._args_from_dynargs(dyn_args)
A:jax.experimental.jax2tf.tests.primitive_harness.group_name->str(group_name)
A:jax.experimental.jax2tf.tests.primitive_harness.h->Harness(group_name, name, fun, arg_descriptors, rng_factory=rng_factory, jax_unimplemented=jax_unimplemented, dtype=dtype, **params)
A:jax.experimental.jax2tf.tests.primitive_harness.devices->tuple(devices)
A:jax.experimental.jax2tf.tests.primitive_harness.dtypes->tuple(dtypes)
A:jax.experimental.jax2tf.tests.primitive_harness.cases->tuple((dict(testcase_name=harness.fullname if one_containing is None else '', harness=harness) for harness in harnesses if harness.filter(jtu.device_under_test(), one_containing=one_containing, include_jax_unimpl=include_jax_unimpl)))
A:jax.experimental.jax2tf.tests.primitive_harness.operand->jax._src.test_util.rand_default(rng)(shape, dtype)
A:jax.experimental.jax2tf.tests.primitive_harness._LAX_COMPARATORS->dict(eq=jnp.equal, ne=jnp.not_equal, ge=jnp.greater_equal, gt=jnp.greater, le=jnp.less_equal, lt=jnp.less)
A:jax.experimental.jax2tf.tests.primitive_harness.arg->numpy.array([-1, -2, 0, 1], dtype=dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.index_dtype->tuple(dtypes).canonicalize_dtype(index_dtype)
A:jax.experimental.jax2tf.tests.primitive_harness._min_max_special_cases->tuple(((lhs, rhs) for dtype in jtu.dtypes.all_floating + jtu.dtypes.complex for (lhs, rhs) in [(np.array([np.inf, np.inf], dtype=dtype), np.array([np.nan, np.nan], dtype=dtype)), (np.array([-np.inf, -np.inf], dtype=dtype), np.array([np.nan, np.nan], dtype=dtype))]))
A:jax.experimental.jax2tf.tests.primitive_harness.indices->numpy.array(2, dtype=np.int32)
A:jax.experimental.jax2tf.tests.primitive_harness._gather_input->numpy.arange(1000, dtype=np.float32).reshape((10, 10, 10))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_2d->jax.lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_2d_2->jax.lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0, 1), start_index_map=(0, 1))
A:jax.experimental.jax2tf.tests.primitive_harness.dnums_3d->jax.lax.GatherDimensionNumbers(offset_dims=(1, 2), collapsed_slice_dims=(0,), start_index_map=(0, 1, 2))
A:jax.experimental.jax2tf.tests.primitive_harness.start_indices->numpy.array(start_indices)
A:jax.experimental.jax2tf.tests.primitive_harness.dimension_numbers->jax.lax.ScatterDimensionNumbers(*dimension_numbers)
A:jax.experimental.jax2tf.tests.primitive_harness._lax_sort_multiple_array_first_arg->numpy.random.uniform(0, 2, _lax_sort_multiple_array_shape).astype(np.int32)
A:jax.experimental.jax2tf.tests.primitive_harness.a->jax._src.test_util.rand_default(rng)(shape, dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.matvec->partial(lax.dot, a, precision=lax.Precision.HIGHEST)
A:jax.experimental.jax2tf.tests.primitive_harness.padding->tuple(lax.padtype_to_pads(shape, window_dimensions, window_strides, padding))
A:jax.experimental.jax2tf.tests.primitive_harness.init_val->numpy.array(init_value, dtype=dtype)
A:jax.experimental.jax2tf.tests.primitive_harness.maxval->{np.uint8: 256}.get(dtype, 5)
A:jax.experimental.jax2tf.tests.primitive_harness.shapes_str->'_'.join((jtu.format_shape_dtype_string(s, dtype) for s in shapes))
jax.experimental.jax2tf.tests.primitive_harness.CustomArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.Harness(self,group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.Harness.__init__(self,group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.Harness.__str__(self)
jax.experimental.jax2tf.tests.primitive_harness.Harness._arg_maker(self,arg_descriptor,rng:Rng)
jax.experimental.jax2tf.tests.primitive_harness.Harness._args_from_dynargs(self,dyn_args:Sequence)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_fun(self,*dyn_args)
jax.experimental.jax2tf.tests.primitive_harness.Harness.filter(self,device_under_test:str,*,include_jax_unimpl:bool=False,one_containing:Optional[str]=None)->bool
jax.experimental.jax2tf.tests.primitive_harness.Harness.fullname(self)
jax.experimental.jax2tf.tests.primitive_harness.Limitation(self,description:str,*,enabled:bool=True,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Union[DType,Sequence[DType]]=(),skip_run:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.__init__(self,description:str,*,enabled:bool=True,devices:Union[str,Sequence[str]]=('cpu','gpu','tpu'),dtypes:Union[DType,Sequence[DType]]=(),skip_run:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.__str__(self)
jax.experimental.jax2tf.tests.primitive_harness.Limitation.filter(self,device:Optional[str]=None,dtype:Optional[DType]=None)->bool
jax.experimental.jax2tf.tests.primitive_harness.RandArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.StaticArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness._can_bitcast(dtype,target_dtype)
jax.experimental.jax2tf.tests.primitive_harness._get_max_identity(dtype)
jax.experimental.jax2tf.tests.primitive_harness._get_min_identity(dtype)
jax.experimental.jax2tf.tests.primitive_harness._make_add_any_harness(name,*,shapes=((2,),(2,)),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_argminmax_harness(prim,name,*,shape=(15,),dtype=jnp.float32,axes=(0,),index_dtype=np.int32,arr=None,works_without_xla=True)
jax.experimental.jax2tf.tests.primitive_harness._make_binary_elementwise_harnesses(prim,dtypes,default_dtype=np.float32,broadcasting_dtypes=None,jax_unimplemented=lambda**kwargs:[])
jax.experimental.jax2tf.tests.primitive_harness._make_bitcast_convert_type_harness(name,*,shape=(2,3),dtype=np.float32,new_dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_broadcast_in_dim_harness(name,*,dtype=np.float32,shape=(2,),outshape=(2,),broadcast_dimensions=(0,))
jax.experimental.jax2tf.tests.primitive_harness._make_cholesky_arg(shape,dtype,rng)
jax.experimental.jax2tf.tests.primitive_harness._make_clamp_harness(name,*,min_shape=(),operand_shape=(2,3),max_shape=(),dtype=np.float32,min_max=None)
jax.experimental.jax2tf.tests.primitive_harness._make_comparator_harness(name,*,dtype=np.float32,op=lax.eq_p,op_name='eq',lhs_shape=(),rhs_shape=())
jax.experimental.jax2tf.tests.primitive_harness._make_complex_harness(name,*,shapes=((3,4),(3,4)),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_concatenate_harness(name,*,shapes=[(2,3),(2,3)],dimension=0,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_conj_harness(name,*,shape=(3,4),dtype=np.float32,**kwargs)
jax.experimental.jax2tf.tests.primitive_harness._make_conv_harness(name,*,lhs_shape=(2,3,9,10),rhs_shape=(3,3,4,5),dtype=np.float32,window_strides=(1,1),precision=None,padding=((0,0),(0,0)),lhs_dilation=(1,1),rhs_dilation=(1,1),feature_group_count=1,dimension_numbers=('NCHW','OIHW','NCHW'),batch_group_count=1,preferred_element_type=None,works_without_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_convert_element_type_harness(name,*,shape=(100,100),dtype=np.float32,new_dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_cumreduce_harness(name,*,f_jax=lax_control_flow.cummin,shape=(8,9),dtype=np.float32,axis=0,reverse=False)
jax.experimental.jax2tf.tests.primitive_harness._make_device_put_harness(name,*,shape=(3,4),dtype=np.float32,device=None)
jax.experimental.jax2tf.tests.primitive_harness._make_div_rem_harness(prim,name,*,shapes=((2,),(2,)),dtype=np.float32,arrs=(None,None))
jax.experimental.jax2tf.tests.primitive_harness._make_dot_general_harness(name,*,lhs_shape=(3,4),rhs_shape=(4,2),dtype=np.float32,precision=None,dimension_numbers=(((1,),(0,)),((),())),preferred_element_type=None)
jax.experimental.jax2tf.tests.primitive_harness._make_dynamic_slice_harness(name,shape=(3,),start_indices=(1,),limit_indices=(2,),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_dynamic_update_slice_harness(name,shape=(3,),start_indices=(1,),dtype=np.float32,update_shape=(1,))
jax.experimental.jax2tf.tests.primitive_harness._make_fft_harness(name,*,shape=(14,15,16,17),dtype=np.float32,fft_type=xla_client.FftType.FFT,fft_lengths=(17,))
jax.experimental.jax2tf.tests.primitive_harness._make_integer_pow_harness(name,*,shape=(20,30),dtype=np.int32,y=3)
jax.experimental.jax2tf.tests.primitive_harness._make_iota_harness(name,*,shape=(2,3),dtype=np.float32,dimension=0)
jax.experimental.jax2tf.tests.primitive_harness._make_linear_solve_harnesses()
jax.experimental.jax2tf.tests.primitive_harness._make_pow_harness(name,*,shapes=((20,30),(20,30)),dtype=np.float32,lhs=None,rhs=None)
jax.experimental.jax2tf.tests.primitive_harness._make_real_imag_harness(prim,name,*,shape=(2,3),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_reduce_harness(name,*,shape=(4,6),nr_operands=1,computation=lax.add,dimensions:Sequence[int]=(0,),init_value=0,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_reduce_window_harness(name,*,shape=(4,6),base_dilation=(1,1),computation=lax.add,window_dimensions=(2,2),window_dilation=(1,1),init_value=0,window_strides=(1,1),dtype=np.float32,padding=((0,0),(0,0)),requires_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_reducer_harness(prim,name,*,shape=(2,3),axes=(0,),dtype=np.int32)
jax.experimental.jax2tf.tests.primitive_harness._make_reshape_harness(name,*,shape=(2,3),new_sizes=(3,2),dimensions=(0,1),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_rev_harness(name,*,shape=(4,5),dtype=np.float32,dimensions=(0,))
jax.experimental.jax2tf.tests.primitive_harness._make_round_harness(name,*,shape=(100,100),dtype=np.float32,rounding_method=lax.RoundingMethod.AWAY_FROM_ZERO,operand=None)
jax.experimental.jax2tf.tests.primitive_harness._make_scatter_harness(name,*,shape=(5,),f_lax=lax.scatter_min,indices_are_sorted=False,unique_indices=False,scatter_indices=np.array([[0],[2]]),update_shape=(2,),mode=lax.GatherScatterMode.FILL_OR_DROP,dtype=np.float32,dimension_numbers=((),(0,),(0,)),enable_and_disable_xla=False)
jax.experimental.jax2tf.tests.primitive_harness._make_select_and_gather_add_harness(name,*,shape=(4,6),dtype=np.float32,select_prim=lax.le_p,padding='VALID',window_dimensions=(2,2),window_strides=(1,1),base_dilation=(1,1),window_dilation=(1,1))
jax.experimental.jax2tf.tests.primitive_harness._make_select_and_scatter_add_harness(name,*,shape=(2,4,6),dtype=np.float32,select_prim=lax.ge_p,window_dimensions=(2,2,2),window_strides=(1,1,1),padding=((0,0),(0,0),(0,0)),nb_inactive_dims=0)
jax.experimental.jax2tf.tests.primitive_harness._make_select_n_harness(name,*,shape_pred=(2,3),shape_args=(2,3),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_slice_harness(name,shape=(3,),start_indices=(1,),limit_indices=(2,),strides=None,dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_sort_harness(name,*,operands=None,shape=(5,7),dtype=np.float32,dimension=0,is_stable=False,num_keys=1)
jax.experimental.jax2tf.tests.primitive_harness._make_squeeze_harness(name,shape=(1,2),dimensions=(0,),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_top_k_harness(name,*,operand=None,shape=(5,3),dtype=np.float32,k=2)
jax.experimental.jax2tf.tests.primitive_harness._make_transpose_harness(name,*,shape=(2,3),permutation=(1,0),dtype=np.float32)
jax.experimental.jax2tf.tests.primitive_harness._make_triangular_eigh_operand(shape,dtype,lower:bool,rng:Rng)
jax.experimental.jax2tf.tests.primitive_harness._make_triangular_solve_harness(name,*,left_side=True,lower=False,ab_shapes=((4,4),(4,1)),dtype=np.float32,transpose_a=False,conjugate_a=False,unit_diagonal=False)
jax.experimental.jax2tf.tests.primitive_harness._make_unary_elementwise_harness(*,prim,shape=(20,20),dtype)
jax.experimental.jax2tf.tests.primitive_harness.define(group_name,name,fun,arg_descriptors,*,dtype,rng_factory=jtu.rand_default,jax_unimplemented:Sequence['Limitation']=(),**params)
jax.experimental.jax2tf.tests.primitive_harness.dtypes_to_str(dtype_list:Sequence[DType],empty_means_all=False)->str
jax.experimental.jax2tf.tests.primitive_harness.parameterized(harnesses:Iterable[Harness],*,one_containing:Optional[str]=None,include_jax_unimpl:bool=False)
jax.experimental.jax2tf.tests.primitive_harness.requires_xla_for_reduce(name,dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/call_tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.call_tf_test._parameterized_jit->absl.testing.parameterized.named_parameters((_named_test(with_jit=with_jit) for with_jit in [True, False]))
A:jax.experimental.jax2tf.tests.call_tf_test._->tensorflow.add(1, 1)
A:jax.experimental.jax2tf.tests.call_tf_test.res->reloaded_f(x)
A:jax.experimental.jax2tf.tests.call_tf_test.x->numpy.array(0.7, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.fun_jax->jax.experimental.jax2tf.call_tf(fun_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax->jax.experimental.jax2tf.call_tf(f_tf_non_compileable)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, acc)->tensorflow.while_loop(c, b, [tf.constant(0), tf.constant(0.0)])
A:jax.experimental.jax2tf.tests.call_tf_test.y->numpy.concatenate([x, x])
A:jax.experimental.jax2tf.tests.call_tf_test.res_call_tf->_maybe_jit(with_jit, jax2tf.call_tf(f_tf))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.res_jax->f_jax(x)
A:jax.experimental.jax2tf.tests.call_tf_test.res_call_tf_jit->jax.jit(jax2tf.call_tf(f_tf))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_var_array->numpy.array([3.0, 4.0], dtype=np.float64)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_var->tensorflow.Variable(np.array([3.0], dtype=np.float32))
A:jax.experimental.jax2tf.tests.call_tf_test.v->tensorflow.Variable((4.0, 2.0), dtype=tf.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_out->tf_func(x)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_func->jax.jit(jax2tf.call_tf(tf_func))
A:jax.experimental.jax2tf.tests.call_tf_test.jax_out->jax_func(x)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_tensor->tensorflow.constant(3.0, dtype=np.float64)
A:jax.experimental.jax2tf.tests.call_tf_test.outer_val->numpy.array(3.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.v2->tensorflow.Variable(2.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.v3->tensorflow.Variable(3.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.t4->tensorflow.constant(4.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.t5->tensorflow.constant(5.0, dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_x->_maybe_jit(with_jit, jax.grad(jax2tf.call_tf(func_square_tf)))(x)
A:jax.experimental.jax2tf.tests.call_tf_test.b->numpy.array([[11.0, 12.0, 13.0], [21.0, 22.0, 23.0]], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.c->numpy.array([[31.0, 32.0], [41.0, 42.0], [51.0, 52.0], [61.0, 62.0]], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x_dict->dict(b=b, c=c)
A:jax.experimental.jax2tf.tests.call_tf_test.prediction->inference_fn(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.weights->numpy.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.weighted_pred->jax.numpy.matmul(weights, prediction['r'])
A:jax.experimental.jax2tf.tests.call_tf_test.g_fun_with_tf->jax.grad(partial(loss, jax2tf.call_tf(f_tf)))
A:jax.experimental.jax2tf.tests.call_tf_test.g_fun_with_jax->jax.grad(partial(loss, f_jax))
A:jax.experimental.jax2tf.tests.call_tf_test.g_tf->tape.gradient(res, xv)
A:jax.experimental.jax2tf.tests.call_tf_test.g_jax->grad_g(x, y)
A:jax.experimental.jax2tf.tests.call_tf_test.param->numpy.array([1.0, 2.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.state->dict(array=np.float32(1.0), counter=7, truth=True)
A:jax.experimental.jax2tf.tests.call_tf_test.f_call_tf->jax.experimental.jax2tf.call_tf(f)
A:jax.experimental.jax2tf.tests.call_tf_test.g_call_tf->grad_g_call_tf(x, y)
A:jax.experimental.jax2tf.tests.call_tf_test.g->jax.grad(lambda *args: jnp.sum(f(*args)[0]))(param, state, x)
A:jax.experimental.jax2tf.tests.call_tf_test.inputs->numpy.ones((batch_size, 3), dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.rng->numpy.array([1, 2], dtype=np.uint32)
A:jax.experimental.jax2tf.tests.call_tf_test.params->numpy.float32(0.5)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_model->jax.experimental.jax2tf.convert(jax_model, with_gradient=True)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_loss_fn->partial(_loss_fn, jax_model)
A:jax.experimental.jax2tf.tests.call_tf_test.jax_grad->jax.grad(jax_loss_fn)(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.paramsv->tensorflow.Variable(params)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_prediction->tf_model(paramsv, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_loss->tensorflow.reduce_mean(tf_prediction)
A:jax.experimental.jax2tf.tests.call_tf_test.tf_grad->tape.gradient(tf_loss, paramsv)
A:jax.experimental.jax2tf.tests.call_tf_test.call_tf_loss_fn->partial(_loss_fn, jax2tf.call_tf(tf_model))
A:jax.experimental.jax2tf.tests.call_tf_test.call_tf_grad->jax.grad(call_tf_loss_fn)(params, rng, inputs)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_g->jax.grad(partial(wrapper, f_jax), allow_int=True, argnums=(0, 1))
A:jax.experimental.jax2tf.tests.call_tf_test.grad_g_call_tf->jax.grad(partial(wrapper, jax2tf.call_tf(f_tf)), allow_int=True, argnums=(0, 1))
A:jax.experimental.jax2tf.tests.call_tf_test.grad_jax->jax.grad(grad_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.grad_jax_pure->jax.grad(grad_jax_pure)
A:jax.experimental.jax2tf.tests.call_tf_test.res1->jax.experimental.jax2tf.call_tf(fun_tf)(x)
A:jax.experimental.jax2tf.tests.call_tf_test.hlo->tensorflow.function(fun_tf, jit_compile=True).experimental_get_compiler_ir(x)()
A:jax.experimental.jax2tf.tests.call_tf_test.outer_ct->numpy.array([3.0], dtype=np.float32)
A:jax.experimental.jax2tf.tests.call_tf_test.x_const->tensorflow.constant(0, shape=x.shape, dtype=x.dtype)
A:jax.experimental.jax2tf.tests.call_tf_test.f_tf->jax.experimental.jax2tf.convert(f_jax, polymorphic_shapes=['(b, ...)'])
A:jax.experimental.jax2tf.tests.call_tf_test.(f_tf_rt, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax2->jax.experimental.jax2tf.call_tf(f_tf_rt)
A:jax.experimental.jax2tf.tests.call_tf_test.f_tf2->jax.experimental.jax2tf.convert(f_jax2)
A:jax.experimental.jax2tf.tests.call_tf_test.f_jax_rt->jax.experimental.jax2tf.call_tf(restored_f)
A:jax.experimental.jax2tf.tests.call_tf_test.f_rt->jax.experimental.jax2tf.call_tf(f_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.(restored_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.restored_jax->jax.experimental.jax2tf.call_tf(restored_model.f)
A:jax.experimental.jax2tf.tests.call_tf_test.param_v->tensorflow.Variable(param)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, restored_model)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(lambda x: f_tf(param_v, x), input_args=[x], variables=[param_v])
A:jax.experimental.jax2tf.tests.call_tf_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])
A:jax.experimental.jax2tf.tests.call_tf_test.res_jax_y->f_jax(y)
A:jax.experimental.jax2tf.tests.call_tf_test.(g_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(jax2tf.convert(g, with_gradient=True, polymorphic_shapes=['b, ...']), input_signature=[tf.TensorSpec([None], dtype=tf.float32)])
A:jax.experimental.jax2tf.tests.call_tf_test.g_rt->jax.experimental.jax2tf.call_tf(g_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.(f_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(jax2tf.convert(f_jax, with_gradient=True), input_args=[x], save_gradients=False)
A:jax.experimental.jax2tf.tests.call_tf_test.y_jax->jax.numpy.cos(x_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.z_jax->jax.experimental.jax2tf.call_tf(f_tf_inner)(y_jax)
A:jax.experimental.jax2tf.tests.call_tf_test.y_tf->tensorflow.math.sin(x_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.z_tf->jax.experimental.jax2tf.convert(f_jax)(y_tf)
A:jax.experimental.jax2tf.tests.call_tf_test.xv->tensorflow.Variable(x)
A:jax.experimental.jax2tf.tests.call_tf_test.(_, gf)->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf_outer, (x,))
A:jax.experimental.jax2tf.tests.call_tf_test.expected_res->numpy.sin(np.cos(np.sin(np.cos(np.sin(x)))))
A:jax.experimental.jax2tf.tests.call_tf_test.fun_tf_rt->jax.experimental.jax2tf.convert(fun_jax, polymorphic_shapes=['b, ...'])
A:jax.experimental.jax2tf.tests.call_tf_test.(reloaded_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(fun_tf_rt, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.acc->numpy.array(2.0, dtype=x.dtype)
A:jax.experimental.jax2tf.tests.call_tf_test.f2_tf->tensorflow.function(f2_tf, autograph=False)
A:jax.experimental.jax2tf.tests.call_tf_test.(f2_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f2_tf, input_args=[x])
A:jax.experimental.jax2tf.tests.call_tf_test.(_, (g_f2_ft,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f2_tf, [x])
A:jax.experimental.jax2tf.tests.call_tf_test.(_, (g_f4_ft,))->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f4_tf, [x])
A:jax.experimental.jax2tf.tests.call_tf_test.f4_tf->tensorflow.function(f4_tf, autograph=False)
A:jax.experimental.jax2tf.tests.call_tf_test.(f4_tf, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f4_tf, input_args=[x])
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_bool(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_control_flow(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_dtypes(self,dtype=np.int32,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_devicearray_arg(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_devicearray_no_copy(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_non_compileable_dynamic_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_non_compileable_strings(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_numpy_arg(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_numpy_no_copy(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_numpy_res(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_pytree(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_scalar_arg(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_eval_scalar_res(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_experimental_get_compiler_ir_design_doc(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_function_compile_time_constant_inputs(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_custom(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_int_argument(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_int_argument_unused(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_nested(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_pytree(self,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_grad_with_float0_result(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_higher_order_grad(self,degree=2,with_jit=False)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_module_documentation(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_pmap(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_repro_193754660(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_multiple_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_tensor_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_tensor_capture_x64(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_value_capture(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_different_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_read(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_read_x64(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_with_var_write_error(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_x64_input(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.CallTfTest.test_x64_output(self,with_jit=True)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_custom_grad(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_custom_grad_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_pytree(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_no_gradients(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_shape_poly(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_simple(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_saved_model_variables(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_shape_poly(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_simple(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToJaxTest.test_without_gradient_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.setUp(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_alternate(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_function_dynamic_shape(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_saved_model(self)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_several_round_trips(self,f2_function=False,f2_saved_model=False,f4_function=False,f4_saved_model=False)
jax.experimental.jax2tf.tests.call_tf_test.RoundTripToTfTest.test_shape_polymorphism_error(self)
jax.experimental.jax2tf.tests.call_tf_test._maybe_jit(with_jit:bool,func:Callable)->Callable
jax.experimental.jax2tf.tests.call_tf_test._named_test(**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/primitives_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitives_test.limitations->tuple(filter(lambda l: l.filter(device=device, dtype=harness.dtype), limitations))
A:jax.experimental.jax2tf.tests.primitives_test.device->jax._src.test_util.device_under_test()
A:jax.experimental.jax2tf.tests.primitives_test.args->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.primitives_test.enable_xla->harness.params.get('enable_xla', True)
A:jax.experimental.jax2tf.tests.primitives_test.associative_scan_reductions->harness.params.get('associative_scan_reductions', False)
A:jax.experimental.jax2tf.tests.primitives_test.tf_not_yet_impl->set(jax.experimental.jax2tf.jax2tf.tf_not_yet_impl)
A:jax.experimental.jax2tf.tests.primitives_test.all_primitives->tuple(sorted(all_primitives, key=str))
A:jax.experimental.jax2tf.tests.primitives_test.tfl->Jax2TfLimitation(description='Not implemented in JAX: ' + l.description, devices=l.devices, dtypes=l.dtypes, expect_tf_error=False, skip_tf_run=True)
A:jax.experimental.jax2tf.tests.primitives_test.tf_numerical_discrepancies_table->list(tf_error_table)
A:jax.experimental.jax2tf.tests.primitives_test.devices->', '.join(sorted(l.devices))
A:jax.experimental.jax2tf.tests.primitives_test.modes->', '.join(sorted(l.modes))
A:jax.experimental.jax2tf.tests.primitives_test.template->f.read()
A:jax.experimental.jax2tf.tests.primitives_test.output_file->os.path.join(os.path.dirname(__file__), '../g3doc/primitives_with_limited_support.md')
A:jax.experimental.jax2tf.tests.primitives_test.x->jax.numpy.array([-4, -3, -1, 0, 1, 3, 6])
A:jax.experimental.jax2tf.tests.primitives_test.y->numpy.int32(3)
A:jax.experimental.jax2tf.tests.primitives_test.expected->jax.numpy.floor_divide(x, y)
A:jax.experimental.jax2tf.tests.primitives_test.tf1_res->sess.run(jax2tf.convert(jnp.floor_divide)(x, y))
A:jax.experimental.jax2tf.tests.primitives_test.values->numpy.array([True, False, True], dtype=np.bool_)
A:jax.experimental.jax2tf.tests.primitives_test.indices->jax.numpy.array([[1, 1, 2], [0, 1, 0]])
A:jax.experimental.jax2tf.tests.primitives_test.f_jax->jax.jit(lambda v, u: getattr(v.at[::2, 3:], op)(u))
A:jax.experimental.jax2tf.tests.primitives_test.params->jax.numpy.array([[1.0, 1.5, 2.0], [2.0, 2.5, 3.0], [3.0, 3.5, 4.0]])
A:jax.experimental.jax2tf.tests.primitives_test.update->numpy.float32(6.0)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_boolean_gather(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_gather_rank_change(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_generate_limitations_doc(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_integer_div(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_prim(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_primitive_coverage(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_boolean_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_numerical_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_scatter_static(self,op)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_type_promotion(self,f_jax=jnp.add)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/converters.py----------------------------------------
A:jax.experimental.jax2tf.tests.converters.tf_fn->tensorflow.function(jax2tf.convert(harness.apply_with_vars, enable_xla=False), input_signature=harness.tf_input_signature, autograph=False)
A:jax.experimental.jax2tf.tests.converters.apply_tf->tensorflow.function(jax2tf.convert(harness.apply_with_vars, enable_xla=False), input_signature=harness.tf_input_signature, autograph=False).get_concrete_function()
A:jax.experimental.jax2tf.tests.converters.converter->tensorflow.lite.TFLiteConverter.from_concrete_functions([apply_tf], tf_fn)
A:jax.experimental.jax2tf.tests.converters.tflite_model->tensorflow.lite.TFLiteConverter.from_concrete_functions([apply_tf], tf_fn).convert()
A:jax.experimental.jax2tf.tests.converters.interpreter->tensorflow.lite.Interpreter(model_content=tflite_model)
A:jax.experimental.jax2tf.tests.converters.inputs->tensorflow.lite.Interpreter(model_content=tflite_model).get_input_details()
A:jax.experimental.jax2tf.tests.converters.output_details->tensorflow.lite.Interpreter(model_content=tflite_model).get_output_details()
A:jax.experimental.jax2tf.tests.converters.outputs->tuple((interpreter.tensor(out['index']) for out in output_details))
jax.experimental.jax2tf.tests.converters.Converter
jax.experimental.jax2tf.tests.converters.jax2tfjs(harness:ModelHarness)
jax.experimental.jax2tf.tests.converters.jax2tflite(harness:ModelHarness,use_flex_ops:bool=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/shape_poly_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, b)->jax.experimental.jax2tf.shape_poly._parse_spec('a, b', (2, 3))
A:jax.experimental.jax2tf.tests.shape_poly_test.tshape->tensorflow.TensorShape([None, 3])
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, b, a1)->jax.experimental.jax2tf.shape_poly._parse_spec('a, b, a', (2, 3, 2))
A:jax.experimental.jax2tf.tests.shape_poly_test.(a,)->jax.experimental.jax2tf.shape_poly._parse_spec('a,', (2,))
A:jax.experimental.jax2tf.tests.shape_poly_test.(a, stride)->jax.experimental.jax2tf.shape_poly._parse_spec('a, s', (2, 3))
A:jax.experimental.jax2tf.tests.shape_poly_test.x->self.rng().rand(6, 2, 3)
A:jax.experimental.jax2tf.tests.shape_poly_test.y->jax.numpy.sin(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.f_tf->tensorflow.function(jax2tf.convert(f_jax, polymorphic_shapes=[PS('b', ...)]), autograph=False, jit_compile=True).get_concrete_function(tf.TensorSpec([None, 2, 3], x.dtype))
A:jax.experimental.jax2tf.tests.shape_poly_test.avals->tuple(map(shape_poly.arg_aval, arg_shapes, arg_dtypes, polymorphic_shapes))
A:jax.experimental.jax2tf.tests.shape_poly_test.(dim_vars, get_dim_values_jax)->jax.experimental.jax2tf.shape_poly.prepare_dim_var_env(avals)
A:jax.experimental.jax2tf.tests.shape_poly_test.(dim_values, _)->jax.experimental.jax2tf.jax2tf._interpret_fun_jax(get_dim_values_jax, args_tf, avals, '')
A:jax.experimental.jax2tf.tests.shape_poly_test.shape_env->f_tf(*[tf.ones(a_s, dtype=_f32) for a_s in arg_shapes])
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax->f_jax(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax_grad->jax.grad(lambda x: jnp.sum(f(x)))(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.xv->numpy.arange(24.0).reshape((2, 3, 4))
A:jax.experimental.jax2tf.tests.shape_poly_test.res_tf->f_tf(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_tf_grad->tape.gradient(res_tf, xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_tf, res_tf_grad)->tf_value_and_grad(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.tf_grad->tensorflow.function(tf_value_and_grad, autograph=False).get_concrete_function(tf.TensorSpec([None, 3, 4]))
A:jax.experimental.jax2tf.tests.shape_poly_test.grad_tf->tape.gradient(res_tf, xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.xi->numpy.arange(np.prod(x_shape), dtype=np.int16).reshape(x_shape)
A:jax.experimental.jax2tf.tests.shape_poly_test.yf->numpy.arange(np.prod(x_shape), dtype=np.int16).reshape(x_shape).astype(np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.zb->numpy.array([True, False], dtype=np.bool_)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_tf, g_tf)->jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(f_tf, args)
A:jax.experimental.jax2tf.tests.shape_poly_test.(restored_f, _)->jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf, input_signature=[tf.TensorSpec([None], x.dtype)])
A:jax.experimental.jax2tf.tests.shape_poly_test.f_jax_rt->jax.experimental.jax2tf.call_tf(restored_f)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_jax_rt->f_jax_rt(x)
A:jax.experimental.jax2tf.tests.shape_poly_test.four_ones->numpy.ones((4,))
A:jax.experimental.jax2tf.tests.shape_poly_test.x0->numpy.array([], np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.x45->numpy.ones((4, 5), dtype=np.float32)
A:jax.experimental.jax2tf.tests.shape_poly_test.(res_primal, res_tangent)->jax.experimental.jax2tf.convert(lambda x, xt: jax.jvp(f, (x,), (xt,)), polymorphic_shapes=['b', 'b'])(x, np.array([0.1, 0.2, 0.3]))
A:jax.experimental.jax2tf.tests.shape_poly_test.res_vmap->jax.vmap(f, in_axes=1)(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.res_iter->jax.numpy.stack([f(xv[:, i, :]) for i in range(xv.shape[1])])
A:jax.experimental.jax2tf.tests.shape_poly_test.res_vmap_tf->jax.experimental.jax2tf.convert(jax.vmap(f, in_axes=1), polymorphic_shapes=['b1, b2, ...'])(xv)
A:jax.experimental.jax2tf.tests.shape_poly_test.x1->jax.numpy.array(poly)
A:jax.experimental.jax2tf.tests.shape_poly_test.poly_axes->tuple(map(lambda pa: pa if isinstance(pa, Sequence) or pa is None else (pa,), poly_axes))
A:jax.experimental.jax2tf.tests.shape_poly_test.args->harness.dyn_args_maker(tst.rng())
A:jax.experimental.jax2tf.tests.shape_poly_test.(arg_polymorphic_shapes, arg_tensorspec)->make_arg_polymorphic_shapes(poly_axis)
A:jax.experimental.jax2tf.tests.shape_poly_test.enable_xla->harness.params.get('enable_xla', True)
A:jax.experimental.jax2tf.tests.shape_poly_test.limitations->jax.experimental.jax2tf.tests.jax2tf_limitations.Jax2TfLimitation.limitations_for_harness(h)
A:jax.experimental.jax2tf.tests.shape_poly_test.device->jax._src.test_util.device_under_test()
A:jax.experimental.jax2tf.tests.shape_poly_test.c->collections.Counter([h.dtype for h in hlist])
A:jax.experimental.jax2tf.tests.shape_poly_test.((dtype, _),)->collections.Counter([h.dtype for h in hlist]).most_common(1)
A:jax.experimental.jax2tf.tests.shape_poly_test._NOT_SUPPORTED_YET->frozenset(['lu', 'custom_linear_solve', 'conv_general_dilated', 'tridiagonal_solve', 'iota', 'rng_bit_generator'])
A:jax.experimental.jax2tf.tests.shape_poly_test.arg->ad.make(rng)
A:jax.experimental.jax2tf.tests.shape_poly_test.check_result->all((not l.custom_assert and (not l.skip_comparison) and (l.tol is None) for l in _get_jax2tf_limitations(device, h)))
A:jax.experimental.jax2tf.tests.shape_poly_test.vmap_harness->_make_harness(h.group_name, h.name, jax.vmap(h.dyn_fun, in_axes=0, out_axes=0), new_args, poly_axes=[0] * len(new_args), check_result=check_result, **h.params)
A:jax.experimental.jax2tf.tests.shape_poly_test._POLY_SHAPE_VMAP_TEST_HARNESSES->_make_vmap_primitive_harnesses()
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest.test_dimension_used_as_result(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest.test_dimension_used_as_value(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest.test_dynamic_shapes(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest.test_mean0(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimAsValueTest.test_shape_as_array(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_core_greater_equal(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_dilate_shape(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_dim_vars(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_dim_vars_symbolic_equal(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_evaluate(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_get_vars(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_parse_poly_spec(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_parse_poly_spec_poly(self,dim_spec='3*a*b*a+-2',dim_poly=3*a*b*a-2)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_parse_poly_spec_shapeenv(self,dim_spec='3*a*b*a+-2',dim_poly=3*a*b*a-2)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_bounds(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_compare(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_compare_overload(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_divmod(self,*,dividend,quotient,divisor,remainder)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_equal(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_int_results(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_truediv(self,*,dividend,divisor,quotient)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_poly_truediv_error(self)
jax.experimental.jax2tf.tests.shape_poly_test.DimPolynomialTest.test_stride_shape(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest.test_prim(self,harness:Harness)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest.test_reshape_compiled(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyPrimitivesTest.test_vmap_while(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_arg_avals(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_cond(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_forgot_polymorphic_shapes_error(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_grad_int(self,with_function=True)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_grad_not_var_output(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_gradients_pytree(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_kwargs(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_pytree(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_readme_examples(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model_constant_gradient(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_saved_model_int_function(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_simple_binary(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_simple_unary(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_static_shape_result(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_with_custom_vjp(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyTest.test_with_nested_jit(self)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyVmapPrimitivesTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.shape_poly_test.ShapePolyVmapPrimitivesTest.test_vmap_prim(self,harness:Harness)
jax.experimental.jax2tf.tests.shape_poly_test._flatten_harnesses(harnesses)
jax.experimental.jax2tf.tests.shape_poly_test._get_jax2tf_limitations(device,h:primitive_harness.Harness)->Sequence[Jax2TfLimitation]
jax.experimental.jax2tf.tests.shape_poly_test._make_harness(group_name:str,name:str,func:Callable,args:primitive_harness.ArgDescriptor,*,poly_axes:Sequence[Optional[Union[int,Sequence[int]]]],check_result=True,skip_jax_run=True,tol=None,enable_and_disable_xla=False,expect_error=(None,None),**params)->Union[Harness, Sequence[Harness]]
jax.experimental.jax2tf.tests.shape_poly_test._make_vmap_primitive_harnesses()
jax.experimental.jax2tf.tests.shape_poly_test._test_one_harness(tst:tf_test_util.JaxToTfTestCase,harness:Harness)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/model_harness.py----------------------------------------
A:jax.experimental.jax2tf.tests.model_harness.model->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3)
A:jax.experimental.jax2tf.tests.model_harness.x->numpy.zeros((1, 8, 8, 3), np.float32)
A:jax.experimental.jax2tf.tests.model_harness.variables->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3).init(rng1, x, rng2)
A:jax.experimental.jax2tf.tests.model_harness.lengths->numpy.array([2, 3], np.int32)
A:jax.experimental.jax2tf.tests.model_harness.apply->functools.partial(model.apply, train=False)
A:jax.experimental.jax2tf.tests.model_harness.n_node->numpy.arange(3, 11)
A:jax.experimental.jax2tf.tests.model_harness.n_edge->numpy.arange(4, 12)
A:jax.experimental.jax2tf.tests.model_harness.total_n_node->numpy.sum(n_node)
A:jax.experimental.jax2tf.tests.model_harness.total_n_edge->numpy.sum(n_edge)
A:jax.experimental.jax2tf.tests.model_harness.graphs->_get_gnn_graphs()
A:jax.experimental.jax2tf.tests.model_harness.kwargs->dict(decode=True, deterministic=True, logits_via_embedding=False, share_embeddings=False)
A:jax.experimental.jax2tf.tests.model_harness.config->jax.experimental.jax2tf.tests.flax_models.transformer_wmt.TransformerConfig(**_full_transformer_kwargs())
A:jax.experimental.jax2tf.tests.model_harness.(rng1, rng2)->jax.random.split(random.PRNGKey(0))
A:jax.experimental.jax2tf.tests.model_harness.(output, _)->jax.experimental.jax2tf.tests.flax_models.vae.VAE(latents=3).apply(*args, mutable=['cache'])
jax.experimental.jax2tf.tests.model_harness.ModelHarness
jax.experimental.jax2tf.tests.model_harness.ModelHarness.apply_with_vars(self,*args,**kwargs)
jax.experimental.jax2tf.tests.model_harness.ModelHarness.tf_input_signature(self)
jax.experimental.jax2tf.tests.model_harness._actor_critic_harness()
jax.experimental.jax2tf.tests.model_harness._bilstm_harness()
jax.experimental.jax2tf.tests.model_harness._cnn_harness()
jax.experimental.jax2tf.tests.model_harness._full_transformer_kwargs()
jax.experimental.jax2tf.tests.model_harness._gcn_harness()
jax.experimental.jax2tf.tests.model_harness._get_gnn_graphs()
jax.experimental.jax2tf.tests.model_harness._gnn_harness()
jax.experimental.jax2tf.tests.model_harness._min_transformer_kwargs()
jax.experimental.jax2tf.tests.model_harness._resnet50_harness()
jax.experimental.jax2tf.tests.model_harness._seq2seq_lstm_harness()
jax.experimental.jax2tf.tests.model_harness._transformer_lm1b_harness()
jax.experimental.jax2tf.tests.model_harness._transformer_nlp_seq_harness()
jax.experimental.jax2tf.tests.model_harness._transformer_wmt_harness()
jax.experimental.jax2tf.tests.model_harness._vae_harness()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/tests/tf_test_util.py----------------------------------------
A:jax.experimental.jax2tf.tests.tf_test_util.model_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))
A:jax.experimental.jax2tf.tests.tf_test_util.restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.tests.tf_test_util.model->tensorflow.train.Checkpoint()
A:jax.experimental.jax2tf.tests.tf_test_util.input_signature->tensorflow.nest.map_structure(lambda a: tf.TensorSpec(a.shape, a.dtype), input_args)
A:jax.experimental.jax2tf.tests.tf_test_util.model.f->tensorflow.function(f_tf, autograph=False, input_signature=input_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.restored->SaveAndLoadModel(model, save_gradients=save_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.res_dtype->numpy.result_type(res)
A:jax.experimental.jax2tf.tests.tf_test_util.cts->jax.tree_util.tree_map(make_ct, res_f_of_args)
A:jax.experimental.jax2tf.tests.tf_test_util.(res, pullback)->jax.vjp(f, *args)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_vars->tensorflow.nest.map_structure(tf.Variable, tf_args)
A:jax.experimental.jax2tf.tests.tf_test_util.res_tf->tf_f(*tf_vars)
A:jax.experimental.jax2tf.tests.tf_test_util.grad->tape.gradient(res_tf, tf_vars, unconnected_gradients=unconnected_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.(f1, args1)->TransformTfValueAndGrad(tf_f, tf_args, unconnected_gradients=unconnected_gradients)
A:jax.experimental.jax2tf.tests.tf_test_util.result_jax->func_jax(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.func_tf->jax.experimental.jax2tf.convert(func_jax, enable_xla=enable_xla)
A:jax.experimental.jax2tf.tests.tf_test_util.jax2tf_limits->tuple(filter(lambda l: l.filter(mode=mode), limitations))
A:jax.experimental.jax2tf.tests.tf_test_util.result_tf->tensorflow.nest.map_structure(lambda t: t.numpy(), result_tf)
A:jax.experimental.jax2tf.tests.tf_test_util.jax_comp->jax.xla_computation(func_jax)(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.jax_hlo->jax.xla_computation(func_jax)(*args).as_hlo_text()
A:jax.experimental.jax2tf.tests.tf_test_util.tf_args_signature->_make_tf_input_signature(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_args_no_scalars->tuple(map(lambda a, sig: tf.convert_to_tensor(a, dtype=sig.dtype), args, tf_args_signature))
A:jax.experimental.jax2tf.tests.tf_test_util.tf_func_compiled->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.tf_hlo->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature).experimental_get_compiler_ir(*tf_args_no_scalars)(stage='hlo')
A:jax.experimental.jax2tf.tests.tf_test_util.backend->jax._src.lib.xla_bridge.get_backend()
A:jax.experimental.jax2tf.tests.tf_test_util.modules->jax._src.lib.xla_bridge.get_backend().compile(jax_comp).hlo_modules()
A:jax.experimental.jax2tf.tests.tf_test_util.jax_opt_hlo->modules[0].to_string()
A:jax.experimental.jax2tf.tests.tf_test_util.tf_opt_hlo->tensorflow.function(func_tf, autograph=False, jit_compile=True, input_signature=tf_args_signature).experimental_get_compiler_ir(*tf_args_no_scalars)(stage='optimized_hlo')
A:jax.experimental.jax2tf.tests.tf_test_util.t_arg->numpy.stack([arg] * 4)
A:jax.experimental.jax2tf.tests.tf_test_util.grad_func->jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf->tensorflow.function(jax2tf.convert(jax_fun, include_xla_op_metadata=include_xla_op_metadata), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)])
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf_func->tensorflow.function(f_tf, autograph=False, input_signature=input_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.concrete_f_tf->tensorflow.function(f_tf, autograph=False, input_signature=input_signature).get_concrete_function(*input_signature)
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf_graph->tensorflow.function(tf_fun, autograph=False).get_concrete_function(*args).graph.as_graph_def()
A:jax.experimental.jax2tf.tests.tf_test_util.f_tf_concrete->tensorflow.function(jax2tf.convert(jax_fun, include_xla_op_metadata=include_xla_op_metadata), autograph=False, input_signature=[tf.TensorSpec(x.shape, x.dtype)]).get_concrete_function(tf.convert_to_tensor(x))
A:jax.experimental.jax2tf.tests.tf_test_util.op_metadata->n.get_attr('_XlaOpMetadata')
A:jax.experimental.jax2tf.tests.tf_test_util.op_metadata_proto->tensorflow.compiler.xla.xla_data_pb2.OpMetadata()
A:jax.experimental.jax2tf.tests.tf_test_util.branch->getattr(n, f'_branch_graph_{idx}', None)
jax.experimental.jax2tf.tests.tf_test_util.ComputeTfValueAndGrad(tf_f:Callable,tf_args:Sequence,unconnected_gradients=tf.UnconnectedGradients.ZERO)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.CheckOpMetadata(self,jax_fun,x,expected:Sequence[OpMetadataGraph],include_xla_op_metadata=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.CheckShapePolymorphism(self,f_jax:Callable,*,input_signature:Sequence[tf.TensorSpec],polymorphic_shapes:Optional[Sequence[Any]],expected_output_signature:Optional[tf.TensorSpec]=None,enable_xla:bool=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.ConvertAndCompare(self,func_jax:Callable,*args,enable_xla:bool=True,limitations:Sequence=())
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.CountLargeTfConstants(self,tf_fun:Callable,*args,at_least=256)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.TransformConvertAndCompare(self,func:Callable,arg,transform:Optional[str])
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.setUp(self)
jax.experimental.jax2tf.tests.tf_test_util.OpMetadataGraph
jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadFunction(f_tf:Callable,*,input_signature:Optional[Sequence[tf.TensorSpec]]=None,input_args:Optional[Sequence[Any]]=None,variables:Sequence[tf.Variable]=(),save_gradients=True)->Tuple[Callable, tf.train.Checkpoint]
jax.experimental.jax2tf.tests.tf_test_util.SaveAndLoadModel(model:tf.Module,save_gradients=True)->tf.Module
jax.experimental.jax2tf.tests.tf_test_util.TransformJaxVJP(f:Callable,args,res_f_of_args)
jax.experimental.jax2tf.tests.tf_test_util.TransformTfValueAndGrad(tf_f:Callable,tf_args,unconnected_gradients=tf.UnconnectedGradients.ZERO)
jax.experimental.jax2tf.tests.tf_test_util._make_tf_input_signature(*tf_args)->List[tf.TensorSpec]
jax.experimental.jax2tf.tests.tf_test_util._run_tf_function(func_tf:Callable,*tf_args,mode:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_main_test.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_main_test.FLAGS.model_path->os.path.join(absltest.get_default_test_tmpdir(), 'saved_models')
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.setUp(self)
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.test_train_and_save_features(self,model='mnist_flax')
jax.experimental.jax2tf.examples.saved_model_main_test.SavedModelMainTest.test_train_and_save_full(self,model='mnist_flax',serving_batch_size=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/keras_reuse_main.py----------------------------------------
A:jax.experimental.jax2tf.examples.keras_reuse_main.(tf_accelerator, _)->jax.experimental.jax2tf.examples.saved_model_main.tf_accelerator_and_tolerances()
A:jax.experimental.jax2tf.examples.keras_reuse_main.feature_model_dir->jax.experimental.jax2tf.examples.saved_model_main.savedmodel_dir()
A:jax.experimental.jax2tf.examples.keras_reuse_main.strategy->tensorflow.distribute.OneDeviceStrategy(tf_accelerator)
A:jax.experimental.jax2tf.examples.keras_reuse_main.images->tensorflow.keras.layers.Input(mnist_lib.input_shape, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.keras_reuse_main.keras_feature_extractor->tensorflow_hub.KerasLayer(feature_model_dir, trainable=False)
A:jax.experimental.jax2tf.examples.keras_reuse_main.features->keras_feature_extractor(images)
A:jax.experimental.jax2tf.examples.keras_reuse_main.predictor->tensorflow.keras.layers.Dense(10, activation='softmax')
A:jax.experimental.jax2tf.examples.keras_reuse_main.predictions->predictor(features)
A:jax.experimental.jax2tf.examples.keras_reuse_main.keras_model->tensorflow.keras.Model(images, predictions)
A:jax.experimental.jax2tf.examples.keras_reuse_main.train_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TRAIN, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.keras_reuse_main.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=mnist_lib.test_batch_size)
jax.experimental.jax2tf.examples.keras_reuse_main.main(_)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/keras_reuse_main_test.py----------------------------------------
A:jax.experimental.jax2tf.examples.keras_reuse_main_test.FLAGS.model_path->os.path.join(absltest.get_default_test_tmpdir(), 'saved_models')
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest.setUp(self)
jax.experimental.jax2tf.examples.keras_reuse_main_test.KerasReuseMainTest.test_keras_reuse(self,model='mnist_pure_jax')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_lib.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_lib.tf_fn->jax.experimental.jax2tf.convert(jax_fn, with_gradient=with_gradient, polymorphic_shapes=[None, polymorphic_shapes], enable_xla=enable_xla)
A:jax.experimental.jax2tf.examples.saved_model_lib.param_vars->tensorflow.nest.map_structure(lambda param: tf.Variable(param, trainable=with_gradient), params)
A:jax.experimental.jax2tf.examples.saved_model_lib.tf_graph->tensorflow.function(lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=compile_model)
A:jax.experimental.jax2tf.examples.saved_model_lib.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]->tensorflow.function(lambda inputs: tf_fn(param_vars, inputs), autograph=False, jit_compile=compile_model).get_concrete_function(input_signatures[0])
A:jax.experimental.jax2tf.examples.saved_model_lib.wrapper->_ReusableSavedModelWrapper(tf_graph, param_vars)
A:jax.experimental.jax2tf.examples.saved_model_lib.saved_model_options->tensorflow.saved_model.SaveOptions(experimental_custom_gradients=True)
A:jax.experimental.jax2tf.examples.saved_model_lib.self.variables->tensorflow.nest.flatten(param_vars)
jax.experimental.jax2tf.examples.saved_model_lib._ReusableSavedModelWrapper(self,tf_graph,param_vars)
jax.experimental.jax2tf.examples.saved_model_lib._ReusableSavedModelWrapper.__init__(self,tf_graph,param_vars)
jax.experimental.jax2tf.examples.saved_model_lib.convert_and_save_model(jax_fn:Callable[[Any,Any],Any],params,model_dir:str,*,input_signatures:Sequence[tf.TensorSpec],polymorphic_shapes:Optional[Union[str,jax2tf.PolyShape]]=None,with_gradient:bool=False,enable_xla:bool=True,compile_model:bool=True,saved_model_options:Optional[tf.saved_model.SaveOptions]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/saved_model_main.py----------------------------------------
A:jax.experimental.jax2tf.examples.saved_model_main.train_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TRAIN, batch_size=mnist_lib.train_batch_size)
A:jax.experimental.jax2tf.examples.saved_model_main.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=mnist_lib.test_batch_size)
A:jax.experimental.jax2tf.examples.saved_model_main.the_model_class->pick_model_class()
A:jax.experimental.jax2tf.examples.saved_model_main.model_dir->os.path.join(model_dir, str(FLAGS.model_version))
A:jax.experimental.jax2tf.examples.saved_model_main.model_descr->model_description()
A:jax.experimental.jax2tf.examples.saved_model_main.(predict_fn, predict_params)->pick_model_class().train(train_ds, test_ds, FLAGS.num_epochs, with_classifier=FLAGS.model_classifier_layer)
A:jax.experimental.jax2tf.examples.saved_model_main.(tf_accelerator, tolerances)->tf_accelerator_and_tolerances()
A:jax.experimental.jax2tf.examples.saved_model_main.pure_restored_model->tensorflow.saved_model.load(model_dir)
A:jax.experimental.jax2tf.examples.saved_model_main.test_input->numpy.ones((mnist_lib.test_batch_size,) + mnist_lib.input_shape, dtype=np.float32)
A:jax.experimental.jax2tf.examples.saved_model_main.tolerances->dict(atol=1e-05, rtol=1e-05)
jax.experimental.jax2tf.examples.saved_model_main.model_description()->str
jax.experimental.jax2tf.examples.saved_model_main.pick_model_class()
jax.experimental.jax2tf.examples.saved_model_main.savedmodel_dir(with_version:bool=True)->str
jax.experimental.jax2tf.examples.saved_model_main.tf_accelerator_and_tolerances()
jax.experimental.jax2tf.examples.saved_model_main.train_and_save()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/mnist_lib.py----------------------------------------
A:jax.experimental.jax2tf.examples.mnist_lib.ds->ds.cache().shuffle(1000).batch(batch_size, drop_remainder=True).cache().shuffle(1000).batch(batch_size, drop_remainder=True)
A:jax.experimental.jax2tf.examples.mnist_lib.m->re.search('metadata files were not found in (.+/)mnist/', str(e))
A:jax.experimental.jax2tf.examples.mnist_lib.label->tensorflow.one_hot(x['label'], 10)
A:jax.experimental.jax2tf.examples.mnist_lib.x->flax.linen.log_softmax(x)
A:jax.experimental.jax2tf.examples.mnist_lib.predictions->FlaxMNIST.predict(params, inputs, with_classifier=True)
A:jax.experimental.jax2tf.examples.mnist_lib.target_class->jax.numpy.argmax(labels, axis=1)
A:jax.experimental.jax2tf.examples.mnist_lib.predicted_class->jax.numpy.argmax(predict(params, inputs), axis=1)
A:jax.experimental.jax2tf.examples.mnist_lib.grads->jax.grad(PureJaxMNIST.loss)(params, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.rng->jax.random.PRNGKey(0)
A:jax.experimental.jax2tf.examples.mnist_lib.start_time->time.time()
A:jax.experimental.jax2tf.examples.mnist_lib.params->jax.jit(PureJaxMNIST.update)(params, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.train_acc->PureJaxMNIST.accuracy(FlaxMNIST.predict, optimizer.target, train_ds)
A:jax.experimental.jax2tf.examples.mnist_lib.test_acc->PureJaxMNIST.accuracy(FlaxMNIST.predict, optimizer.target, test_ds)
A:jax.experimental.jax2tf.examples.mnist_lib.model->Module()
A:jax.experimental.jax2tf.examples.mnist_lib.grad->jax.grad(FlaxMNIST.loss)(optimizer.target, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.optimizer->jax.jit(FlaxMNIST.update)(optimizer, inputs, labels)
A:jax.experimental.jax2tf.examples.mnist_lib.init_shape->jax.numpy.ones((1,) + input_shape, jnp.float32)
A:jax.experimental.jax2tf.examples.mnist_lib.optimizer_def->flax.optim.Momentum(learning_rate=step_size, beta=momentum_mass)
A:jax.experimental.jax2tf.examples.mnist_lib.predict_fn->functools.partial(FlaxMNIST.predict, with_classifier=with_classifier)
A:jax.experimental.jax2tf.examples.mnist_lib.fig->matplotlib.pyplot.figure(figsize=(8.0, 4.0), num=title)
A:jax.experimental.jax2tf.examples.mnist_lib.((images, labels),)->list(tfds.as_numpy(ds.take(1)))
A:jax.experimental.jax2tf.examples.mnist_lib.inferred_labels->inference_fn(images)
A:jax.experimental.jax2tf.examples.mnist_lib.digit->matplotlib.pyplot.figure(figsize=(8.0, 4.0), num=title).add_subplot(nr_rows, nr_cols, i + 1)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.Module(self,x,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.Module.__call__(self,x,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.loss(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.predict(params,inputs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.train(train_ds,test_ds,num_epochs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.FlaxMNIST.update(optimizer,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.accuracy(predict:Callable,params,dataset)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.loss(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.predict(params:Sequence[Tuple[Any,Any]],inputs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.train(train_ds,test_ds,num_epochs,with_classifier=True)
jax.experimental.jax2tf.examples.mnist_lib.PureJaxMNIST.update(params,inputs,labels)
jax.experimental.jax2tf.examples.mnist_lib.load_mnist(split:tfds.Split,batch_size:int)
jax.experimental.jax2tf.examples.mnist_lib.plot_images(ds,nr_rows:int,nr_cols:int,title:str,inference_fn:Optional[Callable]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/serving/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/jax2tf/examples/serving/model_server_request.py----------------------------------------
A:jax.experimental.jax2tf.examples.serving.model_server_request.channel->grpc.insecure_channel(FLAGS.prediction_service_addr)
A:jax.experimental.jax2tf.examples.serving.model_server_request.stub->tensorflow_serving.apis.prediction_service_pb2_grpc.PredictionServiceStub(channel)
A:jax.experimental.jax2tf.examples.serving.model_server_request.request->tensorflow_serving.apis.predict_pb2.PredictRequest()
A:jax.experimental.jax2tf.examples.serving.model_server_request.response->requests.post(predict_url, data=data)
A:jax.experimental.jax2tf.examples.serving.model_server_request.(outputs,)->requests.post(predict_url, data=data).outputs.values()
A:jax.experimental.jax2tf.examples.serving.model_server_request.images_json->json.dumps(images.tolist())
A:jax.experimental.jax2tf.examples.serving.model_server_request.test_ds->jax.experimental.jax2tf.examples.mnist_lib.load_mnist(tfds.Split.TEST, batch_size=FLAGS.serving_batch_size)
A:jax.experimental.jax2tf.examples.serving.model_server_request.images_and_labels->tensorflow_datasets.as_numpy(test_ds.take(FLAGS.count_images // FLAGS.serving_batch_size))
A:jax.experimental.jax2tf.examples.serving.model_server_request.predictions_one_hot->serving_call_mnist(images)
A:jax.experimental.jax2tf.examples.serving.model_server_request.predictions_digit->numpy.argmax(predictions_one_hot, axis=1)
A:jax.experimental.jax2tf.examples.serving.model_server_request.labels_digit->numpy.argmax(labels, axis=1)
jax.experimental.jax2tf.examples.serving.model_server_request.main(_)
jax.experimental.jax2tf.examples.serving.model_server_request.serving_call_mnist(images)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/ad.py----------------------------------------
A:jax.experimental.sparse.ad.allow_int->kwargs.pop('allow_int', False)
A:jax.experimental.sparse.ad.raw_value_and_grad_fun->jax.value_and_grad(fun, argnums=argnums, **kwargs)
A:jax.experimental.sparse.ad.argnums->jax.core.concrete_or_error(_ensure_index, argnums)
A:jax.experimental.sparse.ad.(dyn_args_flat, _)->jax.tree_util.tree_flatten(dyn_args, is_leaf=lambda arg: isinstance(arg, BCOO))
A:jax.experimental.sparse.ad.dtype->numpy.dtype(arg)
A:jax.experimental.sparse.ad.(value, grad)->raw_value_and_grad_fun(*args, **kwargs)
A:jax.experimental.sparse.ad.grad->tuple((maybe_copy_index(args[argnum], g) for (argnum, g) in safe_zip(argnums, grad)))
A:jax.experimental.sparse.ad.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, **kwargs)
A:jax.experimental.sparse.ad.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax.experimental.sparse.ad.((_, aux), g)->value_and_grad_f(*args, **kwargs)
jax.experimental.sparse.ad.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable
jax.experimental.sparse.ad.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,**kwargs)->Callable[..., Tuple[Any, Any]]
jax.experimental.sparse.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux=False,**kwargs)->Callable
jax.experimental.sparse.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,**kwargs)->Callable[..., Tuple[Any, Any]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/bcsr.py----------------------------------------
A:jax.experimental.sparse.bcsr.shape->tuple(shape)
A:jax.experimental.sparse.bcsr.props->_validate_bcsr_indices(indices, indptr, shape)
A:jax.experimental.sparse.bcsr.(n_batch, _, _)->_validate_bcsr_indices(indices, indptr, shape)
A:jax.experimental.sparse.bcsr.csr_to_coo->_broadcasting_vmap(csr_to_coo)
A:jax.experimental.sparse.bcsr.bcsr_fromdense_p->jax.core.Primitive('bcsr_fromdense')
A:jax.experimental.sparse.bcsr.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.bcsr.nse->property(lambda self: self.indices.shape[-1])
A:jax.experimental.sparse.bcsr.bcoo_mat->jax.experimental.sparse.bcoo.bcoo_fromdense(mat, nse=nse, index_dtype=index_dtype, n_dense=n_dense, n_batch=n_batch)
A:jax.experimental.sparse.bcsr.(indices, indptr)->jax.experimental.sparse.bcoo._bcoo_to_bcsr(bcoo_mat.indices, shape=mat.shape)
A:jax.experimental.sparse.bcsr.bcsr_todense_p->jax.core.Primitive('bcsr_todense')
A:jax.experimental.sparse.bcsr.bcoo_indices->_bcsr_to_bcoo(indices, indptr, shape=shape)
A:jax.experimental.sparse.bcsr.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.bcsr.n_batch->property(lambda self: self.indices.ndim - 1)
A:jax.experimental.sparse.bcsr.n_sparse->property(lambda _: 2)
A:jax.experimental.sparse.bcsr.n_dense->property(lambda self: self.data.ndim - self.indices.ndim)
A:jax.experimental.sparse.bcsr.(self.data, self.indices, self.indptr)->_safe_asarray(args)
A:jax.experimental.sparse.bcsr.(batch_shape, sparse_shape, dense_shape)->split_list(shape, [n_batch, n_sparse])
A:jax.experimental.sparse.bcsr.data->jax.numpy.zeros((*batch_shape, nse, *dense_shape), dtype)
A:jax.experimental.sparse.bcsr.indices->jax.numpy.full((*batch_shape, nse), jnp.array(sparse_shape[1]), index_dtype)
A:jax.experimental.sparse.bcsr.indptr->jax.numpy.zeros((*batch_shape, sparse_shape[0] + 1), index_dtype)
jax.experimental.sparse.BCSR(self,args,*,shape)
jax.experimental.sparse.BCSR.__repr__(self)
jax.experimental.sparse.BCSR._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.BCSR._sparse_shape(self)
jax.experimental.sparse.BCSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.BCSR.todense(self)
jax.experimental.sparse.BCSR.transpose(self,*args,**kwargs)
jax.experimental.sparse.BCSR.tree_flatten(self)
jax.experimental.sparse.BCSRProperties(NamedTuple)
jax.experimental.sparse.bcsr.BCSR(self,args,*,shape)
jax.experimental.sparse.bcsr.BCSR.__init__(self,args,*,shape)
jax.experimental.sparse.bcsr.BCSR.__repr__(self)
jax.experimental.sparse.bcsr.BCSR._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.bcsr.BCSR._sparse_shape(self)
jax.experimental.sparse.bcsr.BCSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.bcsr.BCSR.todense(self)
jax.experimental.sparse.bcsr.BCSR.transpose(self,*args,**kwargs)
jax.experimental.sparse.bcsr.BCSR.tree_flatten(self)
jax.experimental.sparse.bcsr.BCSRProperties(NamedTuple)
jax.experimental.sparse.bcsr._bcoo_fromdense_abstract_eval(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_fromdense(mat,*,nse,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcsr._bcsr_fromdense_impl(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcsr._bcsr_to_bcoo(indices:jnp.ndarray,indptr:jnp.ndarray,*,shape:Sequence[int])->jnp.ndarray
jax.experimental.sparse.bcsr._bcsr_todense(data,indices,indptr,*,shape)
jax.experimental.sparse.bcsr._bcsr_todense_abstract_eval(data,indices,indptr,*,shape)
jax.experimental.sparse.bcsr._bcsr_todense_impl(data,indices,indptr,*,shape)
jax.experimental.sparse.bcsr._compatible(shape1,shape2)
jax.experimental.sparse.bcsr._validate_bcsr(data:jnp.ndarray,indices:jnp.ndarray,indptr:jnp.ndarray,shape:Sequence[int])->BCSRProperties
jax.experimental.sparse.bcsr._validate_bcsr_indices(indices:jnp.ndarray,indptr:jnp.ndarray,shape:Sequence[int])->BCSRProperties
jax.experimental.sparse.bcsr.bcsr_fromdense(mat,*,nse=None,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcsr.bcsr_todense(mat)
jax.experimental.sparse.bcsr_fromdense(mat,*,nse=None,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcsr_todense(mat)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/csr.py----------------------------------------
A:jax.experimental.sparse.csr.nse->jax.core.concrete_or_error(operator.index, nse, 'nse argument of csr_fromdense()')
A:jax.experimental.sparse.csr.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.csr.(self.data, self.indices, self.indptr)->_safe_asarray(args)
A:jax.experimental.sparse.csr.shape->tuple(shape)
A:jax.experimental.sparse.csr.data->jax.core.ShapedArray((nse,), mat.dtype)
A:jax.experimental.sparse.csr.indices->jax.core.ShapedArray((nse,), index_dtype)
A:jax.experimental.sparse.csr.indptr->jax.core.ShapedArray((mat.shape[0] + 1,), index_dtype)
A:jax.experimental.sparse.csr.diag_size->min(N + k, M)
A:jax.experimental.sparse.csr.k->_const(idx, k)
A:jax.experimental.sparse.csr.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.csr.zero->_const(idx, 0)
A:jax.experimental.sparse.csr.col->jax.lax.add(idx, lax.cond(k <= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.csr.row->jax.numpy.where(true_nonzeros, row, m)
A:jax.experimental.sparse.csr.other->jax.numpy.asarray(other)
A:jax.experimental.sparse.csr.(data, other)->_promote_dtypes(self.data, other)
A:jax.experimental.sparse.csr.csr_todense_p->jax.core.Primitive('csr_todense')
A:jax.experimental.sparse.csr._csr_todense_lowering->jax.interpreters.mlir.lower_fun(_csr_todense_impl, multiple_results=False)
A:jax.experimental.sparse.csr.csr_fromdense_p->jax.core.Primitive('csr_fromdense')
A:jax.experimental.sparse.csr.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.csr.(row, col)->_csr_to_coo(indices, indptr)
A:jax.experimental.sparse.csr._csr_fromdense_lowering->jax.interpreters.mlir.lower_fun(_csr_fromdense_impl, multiple_results=True)
A:jax.experimental.sparse.csr.(data, indices, indptr)->csr_fromdense_mhlo(mat, nnz=nse, index_dtype=np.dtype(index_dtype), data_dtype=dtype, index_type=mlir.dtype_to_ir_type(np.dtype(index_dtype)))
A:jax.experimental.sparse.csr.primals_out->csr_fromdense(M, nse=nse, index_dtype=index_dtype)
A:jax.experimental.sparse.csr.data_dot->_csr_extract(indices, indptr, Mdot)
A:jax.experimental.sparse.csr.csr_matvec_p->jax.core.Primitive('csr_matvec')
A:jax.experimental.sparse.csr._csr_matvec_lowering->jax.interpreters.mlir.lower_fun(_csr_matvec_impl, multiple_results=False)
A:jax.experimental.sparse.csr.v->jax.numpy.asarray(v)
A:jax.experimental.sparse.csr.csr_matmat_p->jax.core.Primitive('csr_matmat')
A:jax.experimental.sparse.csr._csr_matmat_lowering->jax.interpreters.mlir.lower_fun(_csr_matmat_impl, multiple_results=False)
A:jax.experimental.sparse.csr.B->jax.numpy.asarray(B)
jax.experimental.sparse.CSC(self,args,*,shape)
jax.experimental.sparse.CSC.__matmul__(self,other)
jax.experimental.sparse.CSC._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSC._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSC.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.CSC.todense(self)
jax.experimental.sparse.CSC.transpose(self,axes=None)
jax.experimental.sparse.CSC.tree_flatten(self)
jax.experimental.sparse.CSR(self,args,*,shape)
jax.experimental.sparse.CSR.__matmul__(self,other)
jax.experimental.sparse.CSR._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSR._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.CSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.CSR.todense(self)
jax.experimental.sparse.CSR.transpose(self,axes=None)
jax.experimental.sparse.CSR.tree_flatten(self)
jax.experimental.sparse.csr.CSC(self,args,*,shape)
jax.experimental.sparse.csr.CSC.__init__(self,args,*,shape)
jax.experimental.sparse.csr.CSC.__matmul__(self,other)
jax.experimental.sparse.csr.CSC._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSC._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSC.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.csr.CSC.todense(self)
jax.experimental.sparse.csr.CSC.transpose(self,axes=None)
jax.experimental.sparse.csr.CSC.tree_flatten(self)
jax.experimental.sparse.csr.CSR(self,args,*,shape)
jax.experimental.sparse.csr.CSR.__init__(self,args,*,shape)
jax.experimental.sparse.csr.CSR.__matmul__(self,other)
jax.experimental.sparse.csr.CSR._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSR._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.csr.CSR.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.csr.CSR.todense(self)
jax.experimental.sparse.csr.CSR.transpose(self,axes=None)
jax.experimental.sparse.csr.CSR.tree_flatten(self)
jax.experimental.sparse.csr._csr_fromdense_abstract_eval(mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_gpu_lowering(csr_fromdense_mhlo,ctx,mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_impl(mat,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_jvp(primals,tangents,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_fromdense_transpose(ct,M,*,nse,index_dtype)
jax.experimental.sparse.csr._csr_matmat_abstract_eval(data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_gpu_lowering(csr_matmat_mhlo,ctx,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_impl(data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_jvp_left(data_dot,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_jvp_right(B_dot,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matmat_transpose(ct,data,indices,indptr,B,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_abstract_eval(data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_gpu_lowering(csr_matvec_mhlo,ctx,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_impl(data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_jvp_mat(data_dot,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_jvp_vec(v_dot,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_matvec_transpose(ct,data,indices,indptr,v,*,shape,transpose)
jax.experimental.sparse.csr._csr_todense_abstract_eval(data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_gpu_lowering(csr_todense_mhlo,ctx,data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_impl(data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_jvp(data_dot,data,indices,indptr,*,shape)
jax.experimental.sparse.csr._csr_todense_transpose(ct,data,indices,indptr,*,shape)
jax.experimental.sparse.csr.csr_fromdense(mat,*,nse,index_dtype=np.int32)
jax.experimental.sparse.csr.csr_matmat(data,indices,indptr,B,*,shape,transpose=False)
jax.experimental.sparse.csr.csr_matvec(data,indices,indptr,v,*,shape,transpose=False)
jax.experimental.sparse.csr.csr_todense(data,indices,indptr,*,shape)
jax.experimental.sparse.csr_fromdense(mat,*,nse,index_dtype=np.int32)
jax.experimental.sparse.csr_matmat(data,indices,indptr,B,*,shape,transpose=False)
jax.experimental.sparse.csr_matvec(data,indices,indptr,v,*,shape,transpose=False)
jax.experimental.sparse.csr_todense(data,indices,indptr,*,shape)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/coo.py----------------------------------------
A:jax.experimental.sparse.coo.nse->jax.core.concrete_or_error(operator.index, nse, 'nse argument of coo_fromdense()')
A:jax.experimental.sparse.coo.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.coo._info->property(lambda self: COOInfo(shape=self.shape, rows_sorted=self._rows_sorted, cols_sorted=self._cols_sorted))
A:jax.experimental.sparse.coo._bufs->property(lambda self: (self.data, self.row, self.col))
A:jax.experimental.sparse.coo.(self.data, self.row, self.col)->_safe_asarray(args)
A:jax.experimental.sparse.coo.(row, col, data)->jax.lax.sort((self.row, self.col, self.data), num_keys=2)
A:jax.experimental.sparse.coo.shape->tuple(shape)
A:jax.experimental.sparse.coo.data->jax.core.ShapedArray((nse,), mat.dtype)
A:jax.experimental.sparse.coo.rowcol->jax.core.ShapedArray((nse,), index_dtype)
A:jax.experimental.sparse.coo.diag_size->min(N + k, M)
A:jax.experimental.sparse.coo.k->_const(idx, k)
A:jax.experimental.sparse.coo.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.coo.zero->_const(idx, 0)
A:jax.experimental.sparse.coo.row->jax.lax.sub(idx, lax.cond(k >= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.coo.col->jax.lax.add(idx, lax.cond(k <= 0, lambda : zero, lambda : k))
A:jax.experimental.sparse.coo.other->jax.numpy.asarray(other)
A:jax.experimental.sparse.coo.(data, other)->_promote_dtypes(self.data, other)
A:jax.experimental.sparse.coo.self_promoted->COO((data, self.row, self.col), **self._info._asdict())
A:jax.experimental.sparse.coo.coo_todense_p->jax.core.Primitive('coo_todense')
A:jax.experimental.sparse.coo._coo_todense_lowering->jax.interpreters.mlir.lower_fun(_coo_todense_impl, multiple_results=False)
A:jax.experimental.sparse.coo.result->coo_todense_mhlo(data, row, col, shape=shape, data_dtype=dtype, index_dtype=row_aval.dtype)
A:jax.experimental.sparse.coo.coo_fromdense_p->jax.core.Primitive('coo_fromdense')
A:jax.experimental.sparse.coo.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.coo.(row, col)->jax.numpy.nonzero(mat, size=nse)
A:jax.experimental.sparse.coo._coo_fromdense_lowering->jax.interpreters.mlir.lower_fun(_coo_fromdense_impl, multiple_results=True)
A:jax.experimental.sparse.coo.(data, row, col)->coo_fromdense_mhlo(mat, nnz=nse, data_dtype=dtype, index_dtype=np.dtype(index_dtype), index_type=mlir.dtype_to_ir_type(np.dtype(index_dtype)))
A:jax.experimental.sparse.coo.primals_out->_coo_fromdense(M, nse=nse, index_dtype=index_dtype)
A:jax.experimental.sparse.coo.data_dot->_coo_extract(row, col, Mdot)
A:jax.experimental.sparse.coo.coo_matvec_p->jax.core.Primitive('coo_matvec')
A:jax.experimental.sparse.coo.v->jax.numpy.asarray(v)
A:jax.experimental.sparse.coo._coo_matvec_lowering->jax.interpreters.mlir.lower_fun(_coo_matvec_impl, multiple_results=False)
A:jax.experimental.sparse.coo.coo_matmat_p->jax.core.Primitive('coo_matmat')
A:jax.experimental.sparse.coo.B->jax.numpy.asarray(B)
A:jax.experimental.sparse.coo._coo_matmat_lowering->jax.interpreters.mlir.lower_fun(_coo_matmat_impl, multiple_results=False)
jax.experimental.sparse.COO(self,args,*,shape,rows_sorted=False,cols_sorted=False)
jax.experimental.sparse.COO.__matmul__(self,other)
jax.experimental.sparse.COO._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.COO._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.COO._sort_indices(self)
jax.experimental.sparse.COO.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.COO.todense(self)
jax.experimental.sparse.COO.transpose(self,axes=None)
jax.experimental.sparse.COO.tree_flatten(self)
jax.experimental.sparse.COOInfo(NamedTuple)
jax.experimental.sparse.coo.COO(self,args,*,shape,rows_sorted=False,cols_sorted=False)
jax.experimental.sparse.coo.COO.__init__(self,args,*,shape,rows_sorted=False,cols_sorted=False)
jax.experimental.sparse.coo.COO.__matmul__(self,other)
jax.experimental.sparse.coo.COO._empty(cls,shape,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.coo.COO._eye(cls,N,M,k,*,dtype=None,index_dtype='int32')
jax.experimental.sparse.coo.COO._sort_indices(self)
jax.experimental.sparse.coo.COO.fromdense(cls,mat,*,nse=None,index_dtype=np.int32)
jax.experimental.sparse.coo.COO.todense(self)
jax.experimental.sparse.coo.COO.transpose(self,axes=None)
jax.experimental.sparse.coo.COO.tree_flatten(self)
jax.experimental.sparse.coo.COOInfo(NamedTuple)
jax.experimental.sparse.coo._coo_fromdense(mat,*,nse,index_dtype=jnp.int32)
jax.experimental.sparse.coo._coo_fromdense_abstract_eval(mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_gpu_lowering(coo_fromdense_mhlo,ctx,mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_impl(mat,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_jvp(primals,tangents,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_fromdense_transpose(ct,M,*,nse,index_dtype)
jax.experimental.sparse.coo._coo_matmat(data,row,col,B,*,spinfo,transpose=False)
jax.experimental.sparse.coo._coo_matmat_abstract_eval(data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_gpu_lowering(coo_matmat_mhlo,ctx,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_impl(data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_jvp_left(data_dot,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_jvp_right(B_dot,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matmat_transpose(ct,data,row,col,B,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec(data,row,col,v,*,spinfo,transpose=False)
jax.experimental.sparse.coo._coo_matvec_abstract_eval(data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_gpu_lowering(coo_matvec_mhlo,ctx,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_impl(data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_jvp_mat(data_dot,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_jvp_vec(v_dot,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_matvec_transpose(ct,data,row,col,v,*,spinfo,transpose)
jax.experimental.sparse.coo._coo_todense(data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_abstract_eval(data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_gpu_lowering(coo_todense_mhlo,ctx,data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_impl(data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_jvp(data_dot,data,row,col,*,spinfo)
jax.experimental.sparse.coo._coo_todense_transpose(ct,data,row,col,*,spinfo)
jax.experimental.sparse.coo.coo_fromdense(mat,*,nse=None,index_dtype=jnp.int32)
jax.experimental.sparse.coo.coo_matmat(mat,B,*,transpose=False)
jax.experimental.sparse.coo.coo_matvec(mat,v,transpose=False)
jax.experimental.sparse.coo.coo_todense(mat)
jax.experimental.sparse.coo_fromdense(mat,*,nse=None,index_dtype=jnp.int32)
jax.experimental.sparse.coo_matmat(mat,B,*,transpose=False)
jax.experimental.sparse.coo_matvec(mat,v,transpose=False)
jax.experimental.sparse.coo_todense(mat)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/api.py----------------------------------------
A:jax.experimental.sparse.api.todense_p->jax.core.Primitive('todense')
A:jax.experimental.sparse.api.(bufs, tree)->jax.tree_util.tree_flatten(arr)
A:jax.experimental.sparse.api.arr->jax.tree_util.tree_unflatten(tree, bufs)
A:jax.experimental.sparse.api.primals_out->jax.core.Primitive('todense').bind(*primals, tree=tree)
A:jax.experimental.sparse.api.tangents_out->jax.core.Primitive('todense').bind(tangents[0], *primals[1:], tree=tree)
A:jax.experimental.sparse.api.standin->object()
A:jax.experimental.sparse.api.obj->jax.tree_util.tree_unflatten(tree, [standin] * len(bufs))
A:jax.experimental.sparse.api.N->jax.core.concrete_or_error(operator.index, N)
A:jax.experimental.sparse.api.M->jax.core.concrete_or_error(operator.index, M)
A:jax.experimental.sparse.api.k->jax.core.concrete_or_error(operator.index, k)
jax.experimental.sparse.api._todense_abstract_eval(*bufs,tree)
jax.experimental.sparse.api._todense_batching_rule(batched_args,batch_dims,*,tree)
jax.experimental.sparse.api._todense_impl(*bufs,tree)
jax.experimental.sparse.api._todense_jvp(primals,tangents,*,tree)
jax.experimental.sparse.api._todense_transpose(ct,*bufs,tree)
jax.experimental.sparse.api.empty(shape,dtype=None,index_dtype='int32',sparse_format='bcoo',**kwds)
jax.experimental.sparse.api.eye(N,M=None,k=0,dtype=None,index_dtype='int32',sparse_format='bcoo',**kwds)
jax.experimental.sparse.api.todense(arr)
jax.experimental.sparse.empty(shape,dtype=None,index_dtype='int32',sparse_format='bcoo',**kwds)
jax.experimental.sparse.eye(N,M=None,k=0,dtype=None,index_dtype='int32',sparse_format='bcoo',**kwds)
jax.experimental.sparse.todense(arr)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/bcoo.py----------------------------------------
A:jax.experimental.sparse.bcoo.nse->property(lambda self: self.indices.shape[-2])
A:jax.experimental.sparse.bcoo.data->data.at[M - abs(k):].set(0).at[M - abs(k):].set(0)
A:jax.experimental.sparse.bcoo.indices->indices.at[M - abs(k)].set(M).at[M - abs(k)].set(M)
A:jax.experimental.sparse.bcoo.props->_validate_bcoo(data, indices, shape)
A:jax.experimental.sparse.bcoo.mask->jax.numpy.all(lhs_indices[:, None, dims] == rhs_indices[None, :, dims], -1)
A:jax.experimental.sparse.bcoo.dims_to_contract->tuple((i for (i, s) in enumerate(indices.shape[:props.n_batch]) if s == 1))
A:jax.experimental.sparse.bcoo.fill_value->jax.numpy.expand_dims(jnp.array(shape[n_batch:n_batch + n_sparse], dtype=indices.dtype), range(indices.ndim - 1))
A:jax.experimental.sparse.bcoo.f->_broadcasting_vmap(f)
A:jax.experimental.sparse.bcoo.mat->mat.tocoo().tocoo()
A:jax.experimental.sparse.bcoo.shape->tuple(shape)
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, _, _)->_validate_bcoo(lhs_data, lhs_indices, lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.indptr->jax.numpy.zeros(n_rows + 1, index_dtype)
A:jax.experimental.sparse.bcoo.get_ptr->vmap(get_ptr)
A:jax.experimental.sparse.bcoo.bcoo_todense_p->jax.core.Primitive('bcoo_todense')
A:jax.experimental.sparse.bcoo.ind_slices->tuple((np.zeros(s, int) if i_s == 1 else np.arange(s) for (s, i_s) in zip(mat.shape[:n_batch], indices.shape[:n_batch])))
A:jax.experimental.sparse.bcoo.grid->numpy.meshgrid(*batch_slices, np.arange(1), indexing='ij', sparse=True)
A:jax.experimental.sparse.bcoo.sparse_ind->tuple((indices[grid + (slice(None), i)] for i in range(n_sparse)))
A:jax.experimental.sparse.bcoo.batch_slices->tuple((np.arange(s) for s in mat.shape[:n_batch]))
A:jax.experimental.sparse.bcoo.new_spinfo->BCOOInfo(shape=(max(data.shape[0], indices.shape[0]), *spinfo.shape))
A:jax.experimental.sparse.bcoo.bcoo_fromdense_p->jax.core.Primitive('bcoo_fromdense')
A:jax.experimental.sparse.bcoo._nonzero->vmap(_nonzero, 0)
A:jax.experimental.sparse.bcoo.primals_out->_bcoo_spdot_general(*primals, **kwds)
A:jax.experimental.sparse.bcoo.data_dot->data_dot.sum(props.n_batch, keepdims=True).sum(props.n_batch, keepdims=True)
A:jax.experimental.sparse.bcoo.bcoo_extract_p->jax.core.Primitive('bcoo_extract')
A:jax.experimental.sparse.bcoo.(n_batch, _, n_dense, nse)->_validate_bcoo_indices(indices, mat.shape)
A:jax.experimental.sparse.bcoo.result_shape->list(mat.shape)
A:jax.experimental.sparse.bcoo.bcoo_transpose_p->jax.core.Primitive('bcoo_transpose')
A:jax.experimental.sparse.bcoo.permutation->tuple([*range(n_batch), *range(n_swap, result.ndim), *range(n_batch, n_swap)])
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, n_dense, _)->_validate_bcoo(lhs_data_aval, lhs_indices_aval, lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.(batch_perm, sparse_perm, dense_perm)->_validate_permutation(data, indices, permutation, spinfo.shape)
A:jax.experimental.sparse.bcoo.n_batch->property(lambda self: self.indices.ndim - 2)
A:jax.experimental.sparse.bcoo.(batch_perm, _, dense_perm)->_validate_permutation(data, indices, permutation, spinfo.shape)
A:jax.experimental.sparse.bcoo.(data_dot_out, _)->_bcoo_transpose(data_dot, indices, permutation=permutation, spinfo=spinfo)
A:jax.experimental.sparse.bcoo.ct_spinfo->BCOOInfo(tuple((spinfo.shape[p] for p in permutation)))
A:jax.experimental.sparse.bcoo.rev_permutation->list(np.argsort(permutation))
A:jax.experimental.sparse.bcoo.dummy_indices->jax.numpy.zeros([1 for i in range(indices.ndim - 2)] + list(indices.shape[-2:]), dtype=int)
A:jax.experimental.sparse.bcoo.(data_trans, _)->_bcoo_transpose(data_ct, dummy_indices, permutation=rev_permutation, spinfo=ct_spinfo)
A:jax.experimental.sparse.bcoo.batch_dims->list(batch_dims)
A:jax.experimental.sparse.bcoo.batch_size->numpy.prod(mat.shape[:mat.n_batch])
A:jax.experimental.sparse.bcoo.batched_spinfo->BCOOInfo((batch_size, *spinfo.shape))
A:jax.experimental.sparse.bcoo.(data, indices)->_mul(lhs_data, lhs_indices, rhs_data, rhs_indices)
A:jax.experimental.sparse.bcoo.bcoo_dot_general_p->jax.core.Primitive('bcoo_dot_general')
A:jax.experimental.sparse.bcoo.lhs->_validate_bcoo(lhs_data, lhs_indices, lhs_shape)
A:jax.experimental.sparse.bcoo.rhs->_validate_bcoo(rhs_data, rhs_indices, rhs_shape)
A:jax.experimental.sparse.bcoo.bufs->_bcoo_spdot_general(lhs.data, lhs.indices, rhs.data, rhs.indices, lhs_spinfo=lhs._info, rhs_spinfo=rhs._info, dimension_numbers=dimension_numbers)
A:jax.experimental.sparse.bcoo.result->_bcoo_dot_general(lhs_data, lhs_indices, ct, lhs_spinfo=lhs_spinfo, dimension_numbers=dims)
A:jax.experimental.sparse.bcoo.lhs_data->lhs_data.transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_data.ndim)]).transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_data.ndim)])
A:jax.experimental.sparse.bcoo.lhs_indices->lhs_indices.transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_indices.ndim)]).transpose([*lhs_batch_perm, *range(lhs.n_batch, lhs_indices.ndim)])
A:jax.experimental.sparse.bcoo.out_aval->jax.eval_shape(functools.partial(lax.concatenate, dimension=dimension), [core.ShapedArray(op.shape, op.dtype) for op in operands])
A:jax.experimental.sparse.bcoo.(lhs_contracting_b, rhs_contracting_b)->unzip2([(l, r) for (l, r) in safe_zip(lhs_contracting, rhs_contracting) if l < n_batch])
A:jax.experimental.sparse.bcoo.(lhs_contracting_s, rhs_contracting_s)->unzip2([(l, r) for (l, r) in safe_zip(lhs_contracting, rhs_contracting) if l >= n_batch])
A:jax.experimental.sparse.bcoo.sparse_perm->jax.numpy.array([*lhs_contracting_s, *remaining(range(n_sparse), lhs_contracting_s)])
A:jax.experimental.sparse.bcoo.idx->jax.numpy.arange(diag_size, dtype=index_dtype)
A:jax.experimental.sparse.bcoo.prod->jax.lax.dot_general(lhs_data, rhs.at[idx_right].get(mode='fill', fill_value=0), (([], []), (batch_dims, batch_dims)))
A:jax.experimental.sparse.bcoo.out_array->jax.numpy.zeros(out_aval.shape, out_aval.dtype)
A:jax.experimental.sparse.bcoo.out_shape->tuple((shape[i] for i in range(len(shape)) if i not in axes))
A:jax.experimental.sparse.bcoo._bcoo_dot_general_default_lowering->jax.interpreters.mlir.lower_fun(_bcoo_dot_general_impl, multiple_results=False)
A:jax.experimental.sparse.bcoo.x_type->jax._src.lib.mlir.ir.RankedTensorType(x.type)
A:jax.experimental.sparse.bcoo.rhs_ndim->len(ir.RankedTensorType(rhs.type).shape)
A:jax.experimental.sparse.bcoo.col->_collapse_mhlo(mhlo.SliceOp(lhs_indices_2d, start_indices=mlir.dense_int_elements([0, 1]), limit_indices=mlir.dense_int_elements([lhs_indices_2d_shape[0], 2]), strides=mlir.dense_int_elements([1, 1])).result, start=0, end=1)
A:jax.experimental.sparse.bcoo.row->_collapse_mhlo(mhlo.SliceOp(lhs_indices_2d, start_indices=mlir.dense_int_elements([0, 0]), limit_indices=mlir.dense_int_elements([lhs_indices_2d_shape[0], 1]), strides=mlir.dense_int_elements([1, 1])).result, start=0, end=1)
A:jax.experimental.sparse.bcoo.dot_product->bcoo_dot_general_fn(lhs_data, row, col, rhs, shape=lhs_shape, transpose=lhs_transpose, data_dtype=lhs_data_aval.dtype, index_dtype=lhs_indices_aval.dtype, x_dtype=rhs_aval.dtype)
A:jax.experimental.sparse.bcoo.cuda_version->int(xla_bridge.get_backend().platform_version.split()[-1])
A:jax.experimental.sparse.bcoo.lhs_ndim->len(lhs_spinfo.shape)
A:jax.experimental.sparse.bcoo.lhs_kept->remaining(range(lhs_ndim), lhs_contract, lhs_batch)
A:jax.experimental.sparse.bcoo.rhs_kept->remaining(range(rhs_ndim), rhs_contract, rhs_batch)
A:jax.experimental.sparse.bcoo.(ans_batch, ans_lhs, ans_rhs)->map(list, ranges_like(lhs_batch, lhs_kept, rhs_kept))
A:jax.experimental.sparse.bcoo.lhs_contract_sorted_by_rhs->list(np.take(lhs_contract, np.argsort(rhs_contract)))
A:jax.experimental.sparse.bcoo.out_axes->list(np.argsort(list(rhs_batch) + rhs_contract_sorted_by_lhs + rhs_kept))
A:jax.experimental.sparse.bcoo.dummy_data->jax.numpy.ones([1 for i in range(lhs_indices.ndim - 2)] + [lhs_indices.shape[-2]])
A:jax.experimental.sparse.bcoo.dummy_spinfo->BCOOInfo(tuple(lhs_indices.shape[:-2]) + tuple((1 for i in range(lhs_indices.shape[-1]))))
A:jax.experimental.sparse.bcoo.(_, lhs_indices_T)->_bcoo_transpose(dummy_data, lhs_indices, permutation=permutation, spinfo=dummy_spinfo)
A:jax.experimental.sparse.bcoo.result_T->bcoo_dot_general_sampled(ct, rhs, lhs_indices_T, dimension_numbers=dims)
A:jax.experimental.sparse.bcoo.(result, _)->_bcoo_transpose(result_T, lhs_indices_T, permutation=out_axes, spinfo=dummy_spinfo)
A:jax.experimental.sparse.bcoo.rhs_contract_sorted_by_lhs->list(np.take(rhs_contract, np.argsort(lhs_contract)))
A:jax.experimental.sparse.bcoo.(new_dimension_numbers, result_batch_dim)->_dot_general_batch_dim_nums((len(lhs_shape), len(rhs_shape)), (batch_dims[0], batch_dims[2]), dimension_numbers)
A:jax.experimental.sparse.bcoo.batched_out->_bcoo_spdot_general(lhs_data, lhs_indices, rhs_data, rhs_indices, dimension_numbers=new_dimension_numbers, lhs_spinfo=BCOOInfo(new_lhs_shape), rhs_spinfo=BCOOInfo(new_rhs_shape))
A:jax.experimental.sparse.bcoo.bcoo_dot_general_sampled_p->jax.core.Primitive('bcoo_dot_general_sampled')
A:jax.experimental.sparse.bcoo.dense_result->jax.lax.dot_general(A, B, dimension_numbers=dimension_numbers)
A:jax.experimental.sparse.bcoo.(dense_result,)->jax.interpreters.partial_eval.abstract_eval_fun(lambda *args: [lax.dot_general(*args, dimension_numbers=dimension_numbers)], A, B)
A:jax.experimental.sparse.bcoo.(sparse_result,)->jax.interpreters.partial_eval.abstract_eval_fun(lambda *args: [bcoo_extract(*args)], indices, dense_result)
A:jax.experimental.sparse.bcoo.mat_shape->_dot_general_validated_shape(A_shape, B_shape, dimension_numbers)
A:jax.experimental.sparse.bcoo.(indices, ct)->_bcoo_extract_transpose(ct, indices, mat)
A:jax.experimental.sparse.bcoo.(A, B)->jax.interpreters.ad.get_primitive_transpose(lax.dot_general_p)(ct, A, B, **kwds)
A:jax.experimental.sparse.bcoo.bcoo_spdot_general_p->jax.core.Primitive('bcoo_spdot_general')
A:jax.experimental.sparse.bcoo.overlap->(lhs_i[:, None] == rhs_i[None, :]).all(-1)
A:jax.experimental.sparse.bcoo.lhs_fill_value->jax.numpy.expand_dims(jnp.array([lhs_shape[d] for d in lhs_contracting], dtype=lhs_i.dtype), range(lhs_i.ndim - 1))
A:jax.experimental.sparse.bcoo.rhs_fill_value->jax.numpy.expand_dims(jnp.array([rhs_shape[d] for d in rhs_contracting], dtype=rhs_i.dtype), range(rhs_i.ndim - 1))
A:jax.experimental.sparse.bcoo.lhs_valid->(lhs_i < lhs_fill_value).all(-1)
A:jax.experimental.sparse.bcoo.rhs_valid->(rhs_i < rhs_fill_value).all(-1)
A:jax.experimental.sparse.bcoo.out_data->jax.numpy.where(overlap & lhs_valid[:, None] & rhs_valid[None, :], lhs_data[:, None] * rhs_data[None, :], 0).ravel()
A:jax.experimental.sparse.bcoo.out_indices->out_indices.reshape(len(out_data), out_indices.shape[-1]).reshape(len(out_data), out_indices.shape[-1])
A:jax.experimental.sparse.bcoo.(data_aval, indices_aval)->_bcoo_spdot_general_abstract_eval(lhs_data.aval, lhs_indices.aval, rhs_data.aval, rhs_indices.aval, lhs_spinfo=lhs_spinfo, rhs_spinfo=rhs_spinfo, dimension_numbers=dimension_numbers)
A:jax.experimental.sparse.bcoo.rhs_data->rhs_data.transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_data.ndim)]).transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_data.ndim)])
A:jax.experimental.sparse.bcoo.rhs_indices->rhs_indices.transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_indices.ndim)]).transpose([*rhs_batch_perm, *range(rhs.n_batch, rhs_indices.ndim)])
A:jax.experimental.sparse.bcoo.func->_broadcasting_vmap(func, in_axes=0)
A:jax.experimental.sparse.bcoo._->_dot_general_validated_shape(lhs_shape, rhs_shape, dimension_numbers)
A:jax.experimental.sparse.bcoo.bcoo_sort_indices_p->jax.core.Primitive('bcoo_sort_indices')
A:jax.experimental.sparse.bcoo.(indices, perm)->f(indices)
A:jax.experimental.sparse.bcoo.permute->_broadcasting_vmap(permute)
A:jax.experimental.sparse.bcoo.(*indices, perm)->jax.lax.sort((*idx_cols, lax.iota(indices.dtype, nse)), num_keys=N)
A:jax.experimental.sparse.bcoo.data_out->permute(data_out, mapping, data)
A:jax.experimental.sparse.bcoo.(data_out, indices_out)->jax.core.Primitive('bcoo_sum_duplicates').bind(data, indices, spinfo=new_spinfo, nse=nse)
A:jax.experimental.sparse.bcoo.(indices_out, perm)->f(indices)
A:jax.experimental.sparse.bcoo.indices_dot_out->jax.interpreters.ad.Zero.from_value(indices_out)
A:jax.experimental.sparse.bcoo._bcoo_sort_indices_mhlo->jax.interpreters.mlir.lower_fun(_bcoo_sort_indices_impl, multiple_results=True)
A:jax.experimental.sparse.bcoo.bcoo_sum_duplicates_p->jax.core.Primitive('bcoo_sum_duplicates')
A:jax.experimental.sparse.bcoo.(indices_out, mapping, nse_batched)->f(indices)
A:jax.experimental.sparse.bcoo.indices_out->_adjust_indices_nse(indices_out, nse=nse, shape=spinfo.shape)
A:jax.experimental.sparse.bcoo.fill->jax.lax.broadcast_in_dim(operand=jnp.array(shape[props.n_batch:props.n_batch + props.n_sparse], dtype=indices.dtype), shape=(*indices.shape[:-2], nse - props.nse, indices.shape[-1]), broadcast_dimensions=(indices.ndim - 1,))
A:jax.experimental.sparse.bcoo.mapping->jax.numpy.zeros(nse, dtype='int32')
A:jax.experimental.sparse.bcoo.out_of_bounds->(indices >= fill_value).any(-1, keepdims=True)
A:jax.experimental.sparse.bcoo.(indices_unique, inv_idx, nse)->_unique(indices, axis=0, return_inverse=True, return_true_size=True, size=props.nse, fill_value=fill_value)
A:jax.experimental.sparse.bcoo._bcoo_sum_duplicates_mhlo->jax.interpreters.mlir.lower_fun(_bcoo_sum_duplicates_impl, multiple_results=True)
A:jax.experimental.sparse.bcoo.new_d->jax.numpy.zeros_like(d, shape=new_d_shape).at[idx].set(d)
A:jax.experimental.sparse.bcoo.meshes->jax.numpy.meshgrid(*(jnp.arange(d, dtype=i.dtype) for d in (*i.shape[:n], nse)), indexing='ij')
A:jax.experimental.sparse.bcoo.new_i->jax.numpy.broadcast_to(i[:, n:], new_i_shape)
A:jax.experimental.sparse.bcoo._update->_broadcasting_vmap(_update)
A:jax.experimental.sparse.bcoo.(new_data, new_indices)->_update(new_data, new_indices)
A:jax.experimental.sparse.bcoo.new_data->jax.numpy.where(keep_data, new_data, 0)
A:jax.experimental.sparse.bcoo.new_indices->jax.numpy.where(keep, new_indices - starts, sparse_shape)
A:jax.experimental.sparse.bcoo.(batch_dims, sparse_dims, dense_dims)->split_list(broadcast_dimensions, [props.n_batch, props.n_sparse])
A:jax.experimental.sparse.bcoo.new_n_batch->min(broadcast_dimensions[props.n_batch:], default=len(shape))
A:jax.experimental.sparse.bcoo.dimension->operator.index(dimension)
A:jax.experimental.sparse.bcoo.offsets->numpy.cumsum([0] + [op.shape[dimension] for op in operands[:-1]], dtype=operands[0].indices.dtype)
A:jax.experimental.sparse.bcoo.cuml_shape->numpy.cumprod(new_sizes)
A:jax.experimental.sparse.bcoo.ind->tuple((i if s != 1 else 0 for (i, s) in zip(ind, v.shape)))
A:jax.experimental.sparse.bcoo.(batch_perm, sparse_perm, _)->_validate_permutation(mat.data, mat.indices, dimensions or tuple(range(mat.ndim)), mat.shape)
A:jax.experimental.sparse.bcoo.index_cols->tuple((indices[..., i] for i in sparse_perm))
A:jax.experimental.sparse.bcoo.sparse_shape->jax.numpy.expand_dims(sparse_shape, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.flat_indices->jax.numpy.ravel_multi_index(index_cols, dims=sparse_shape, mode='clip')
A:jax.experimental.sparse.bcoo.new_index_cols->jax.numpy.unravel_index(flat_indices, sparse_sizes)
A:jax.experimental.sparse.bcoo.oob_indices->(indices >= jnp.array(mat.shape[mat.n_batch:], dtype=indices.dtype)).any(-1)
A:jax.experimental.sparse.bcoo.(start_batch, start_sparse, start_dense)->split_list(start_indices, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.(end_batch, end_sparse, end_dense)->split_list(limit_indices, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.starts->jax.numpy.expand_dims(starts, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.ends->jax.numpy.expand_dims(jnp.array(end_sparse, dtype=new_indices.dtype), range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.keep->jax.numpy.all((new_indices >= starts) & (new_indices < starts + sizes), -1, keepdims=True)
A:jax.experimental.sparse.bcoo.keep_data->jax.lax.expand_dims(keep[..., 0], range(mat.n_batch + 1, mat.n_batch + 1 + mat.n_dense))
A:jax.experimental.sparse.bcoo.start_indices->tuple((jnp.asarray(i) for i in start_indices))
A:jax.experimental.sparse.bcoo.slice_sizes->tuple((operator.index(i) for i in slice_sizes))
A:jax.experimental.sparse.bcoo.(size_batch, size_sparse, size_dense)->split_list(slice_sizes, [mat.n_batch, mat.n_sparse])
A:jax.experimental.sparse.bcoo.sizes->jax.numpy.expand_dims(sizes, range(mat.n_batch + 1))
A:jax.experimental.sparse.bcoo.(out_data, out_indices, out_shape)->_bcoo_multiply_sparse(lhs.data, lhs.indices, rhs.data, rhs.indices, lhs_spinfo=lhs._info, rhs_spinfo=rhs._info)
A:jax.experimental.sparse.bcoo.(n_batch, n_sparse, _, nse)->_validate_bcoo(data, indices, shape)
A:jax.experimental.sparse.bcoo.axes->sorted(set(axes))
A:jax.experimental.sparse.bcoo.dense_axes->tuple((ax - n_sparse + 1 for ax in axes if ax >= n_batch + n_sparse))
A:jax.experimental.sparse.bcoo.new_batch_dims->tuple(sorted(set(range(n_batch)) - batch_axes))
A:jax.experimental.sparse.bcoo.new_batch_shape->tuple((data.shape[i] for i in new_batch_dims))
A:jax.experimental.sparse.bcoo.new_nse->int(nse * np.prod([data.shape[i] for i in batch_axes]))
A:jax.experimental.sparse.bcoo._mul->_broadcasting_vmap(_mul)
A:jax.experimental.sparse.bcoo.(lhs_data, lhs_indices)->_unbatch_bcoo(lhs_data, lhs_indices, lhs_shape)
A:jax.experimental.sparse.bcoo.(rhs_data, rhs_indices)->_unbatch_bcoo(rhs_data, rhs_indices, rhs_shape)
A:jax.experimental.sparse.bcoo.dims->jax.numpy.array([i for (i, (s1, s2)) in enumerate(safe_zip(lhs_shape[:lhs.n_sparse], rhs_shape[:rhs.n_sparse])) if s1 != 1 and s2 != 1], dtype=int)
A:jax.experimental.sparse.bcoo.(i_lhs, i_rhs)->jax.numpy.nonzero(mask, size=nse, fill_value=(lhs.nse, rhs.nse))
A:jax.experimental.sparse.bcoo.v->jax.lax.expand_dims(v, range(len(shape) - v.ndim))
A:jax.experimental.sparse.bcoo.dtype->property(lambda self: self.data.dtype)
A:jax.experimental.sparse.bcoo.n_sparse->property(lambda self: self.indices.shape[-1])
A:jax.experimental.sparse.bcoo.n_dense->property(lambda self: self.data.ndim - 1 - self.n_batch)
A:jax.experimental.sparse.bcoo._info->property(lambda self: BCOOInfo(self.shape, self.indices_sorted, self.unique_indices))
A:jax.experimental.sparse.bcoo._bufs->property(lambda self: (self.data, self.indices))
A:jax.experimental.sparse.bcoo.(self.data, self.indices)->_safe_asarray(args)
A:jax.experimental.sparse.bcoo.(batch_shape, sparse_shape, dense_shape)->split_list(shape, [n_batch, n_sparse])
A:jax.experimental.sparse.bcoo.diag_size->min(N + k, M)
A:jax.experimental.sparse.bcoo.k->_const(idx, k)
A:jax.experimental.sparse.bcoo.zero->_const(idx, 0)
A:jax.experimental.sparse.bcoo.mat_T->bcoo_transpose(self, permutation=axes)
A:jax.experimental.sparse.bcoo.shape_T->tuple((self.shape[i] for i in axes))
jax.experimental.sparse.BCOO(self,args,*,shape,indices_sorted=False,unique_indices=False)
jax.experimental.sparse.BCOO.__repr__(self)
jax.experimental.sparse.BCOO._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.BCOO._eye(cls,N,M,k,*,dtype=None,index_dtype='int32',n_batch=0,n_dense=0)
jax.experimental.sparse.BCOO.from_scipy_sparse(cls,mat,*,index_dtype=None,n_dense=0,n_batch=0)
jax.experimental.sparse.BCOO.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.BCOO.sort_indices(self)
jax.experimental.sparse.BCOO.sum_duplicates(self,nse=None,remove_zeros=True)
jax.experimental.sparse.BCOO.todense(self)
jax.experimental.sparse.BCOO.transpose(self,axes=None)
jax.experimental.sparse.BCOO.tree_flatten(self)
jax.experimental.sparse.BCOO.update_layout(self,*,n_batch=None,n_dense=None,on_inefficient='error')
jax.experimental.sparse.BCOOInfo(NamedTuple)
jax.experimental.sparse.BCOOProperties(NamedTuple)
jax.experimental.sparse.bcoo.BCOO(self,args,*,shape,indices_sorted=False,unique_indices=False)
jax.experimental.sparse.bcoo.BCOO.__init__(self,args,*,shape,indices_sorted=False,unique_indices=False)
jax.experimental.sparse.bcoo.BCOO.__repr__(self)
jax.experimental.sparse.bcoo.BCOO._empty(cls,shape,*,dtype=None,index_dtype='int32',n_dense=0,n_batch=0,nse=0)
jax.experimental.sparse.bcoo.BCOO._eye(cls,N,M,k,*,dtype=None,index_dtype='int32',n_batch=0,n_dense=0)
jax.experimental.sparse.bcoo.BCOO.from_scipy_sparse(cls,mat,*,index_dtype=None,n_dense=0,n_batch=0)
jax.experimental.sparse.bcoo.BCOO.fromdense(cls,mat,*,nse=None,index_dtype=np.int32,n_dense=0,n_batch=0)
jax.experimental.sparse.bcoo.BCOO.sort_indices(self)
jax.experimental.sparse.bcoo.BCOO.sum_duplicates(self,nse=None,remove_zeros=True)
jax.experimental.sparse.bcoo.BCOO.todense(self)
jax.experimental.sparse.bcoo.BCOO.transpose(self,axes=None)
jax.experimental.sparse.bcoo.BCOO.tree_flatten(self)
jax.experimental.sparse.bcoo.BCOO.update_layout(self,*,n_batch=None,n_dense=None,on_inefficient='error')
jax.experimental.sparse.bcoo.BCOOInfo(NamedTuple)
jax.experimental.sparse.bcoo.BCOOProperties(NamedTuple)
jax.experimental.sparse.bcoo._adjust_indices_nse(indices,*,nse,shape)
jax.experimental.sparse.bcoo._bcoo_broadcast_in_dim(data,indices,*,spinfo,shape,broadcast_dimensions)
jax.experimental.sparse.bcoo._bcoo_dot_general(lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_abstract_eval(lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_cuda_lowering(coo_matvec_lowering,coo_matmat_lowering,ctx,lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_gpu_lowering(coo_matvec_lowering,coo_matmat_lowering,ctx,lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_impl(lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_jvp_lhs(lhs_data_dot,lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_jvp_rhs(rhs_dot,lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_abstract_eval(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_batch_rule(batched_args,batch_dims,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_impl(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_jvp_A(A_dot,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_jvp_B(B_dot,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_sampled_transpose(ct,A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_dot_general_transpose(ct,lhs_data,lhs_indices,rhs,*,dimension_numbers,lhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_extract_abstract_eval(indices,mat)
jax.experimental.sparse.bcoo._bcoo_extract_batching_rule(batched_args,batch_dims)
jax.experimental.sparse.bcoo._bcoo_extract_impl(indices,mat)
jax.experimental.sparse.bcoo._bcoo_extract_jvp(mat_dot,indices,mat)
jax.experimental.sparse.bcoo._bcoo_extract_transpose(ct,indices,mat)
jax.experimental.sparse.bcoo._bcoo_from_elt(cont,axis_size,elt,axis)
jax.experimental.sparse.bcoo._bcoo_fromdense(mat,*,nse,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcoo._bcoo_fromdense_abstract_eval(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_batching_rule(batched_args,batch_dims,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_impl(mat,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_jvp(primals,tangents,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_fromdense_transpose(ct,M,*,nse,n_batch,n_dense,index_dtype)
jax.experimental.sparse.bcoo._bcoo_multiply_dense(data,indices,v,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_multiply_sparse(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo,rhs_spinfo)
jax.experimental.sparse.bcoo._bcoo_multiply_sparse_unbatched(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_shape,rhs_shape)
jax.experimental.sparse.bcoo._bcoo_rdot_general(lhs,rhs_data,rhs_indices,*,dimension_numbers:DotDimensionNumbers,rhs_spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_reduce_sum(data,indices,*,spinfo,axes)
jax.experimental.sparse.bcoo._bcoo_set_nse(mat,nse)
jax.experimental.sparse.bcoo._bcoo_sort_indices_abstract_eval(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_batching_rule(batched_args,batch_dims,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_impl(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_jvp(primals,tangents,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_sort_indices_unbatched(indices)
jax.experimental.sparse.bcoo._bcoo_spdot_general(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo:BCOOInfo,rhs_spinfo:BCOOInfo,dimension_numbers:DotDimensionNumbers)
jax.experimental.sparse.bcoo._bcoo_spdot_general_abstract_eval(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo:BCOOInfo,rhs_spinfo:BCOOInfo,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_spdot_general_batch_rule(batched_args,batch_dims,*,lhs_spinfo:BCOOInfo,rhs_spinfo:BCOOInfo,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_spdot_general_impl(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo:BCOOInfo,rhs_spinfo:BCOOInfo,dimension_numbers)
jax.experimental.sparse.bcoo._bcoo_spdot_general_jvp(primals,tangents,**kwds)
jax.experimental.sparse.bcoo._bcoo_spdot_general_unbatched(lhs_data,lhs_indices,rhs_data,rhs_indices,*,lhs_spinfo,rhs_spinfo,lhs_contracting,rhs_contracting)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates(data,indices,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_abstract_eval(data,indices,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_batching_rule(batched_args,batch_dims,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_impl(data,indices,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_jvp(primals,tangents,*,spinfo,nse)
jax.experimental.sparse.bcoo._bcoo_sum_duplicates_unbatched(indices,*,shape)
jax.experimental.sparse.bcoo._bcoo_to_bcsr(indices:jnp.ndarray,*,shape:Sequence[int],index_dtype=jnp.int32)
jax.experimental.sparse.bcoo._bcoo_to_elt(cont,_,val,axis)
jax.experimental.sparse.bcoo._bcoo_todense(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_abstract_eval(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_batching_rule(batched_args,batch_dims,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_impl(data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_jvp(data_dot,data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_todense_transpose(ct,data,indices,*,spinfo)
jax.experimental.sparse.bcoo._bcoo_transpose(data,indices,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_abstract_eval(data,indices,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_batch_rule(batched_args,batch_dims,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_impl(data,indices,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_jvp(primals,tangents,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._bcoo_transpose_transpose(ct,data,indices,*,permutation:Sequence[int],spinfo:BCOOInfo)
jax.experimental.sparse.bcoo._collapse_mhlo(x,start,end)
jax.experimental.sparse.bcoo._dot_general_validated_shape(lhs_shape:Shape,rhs_shape:Shape,dimension_numbers:DotDimensionNumbers)->Shape
jax.experimental.sparse.bcoo._tuple_replace(tup,ind,val)
jax.experimental.sparse.bcoo._unbatch_bcoo(data,indices,shape)
jax.experimental.sparse.bcoo._validate_bcoo(data:jnp.ndarray,indices:jnp.ndarray,shape:Sequence[int])->BCOOProperties
jax.experimental.sparse.bcoo._validate_bcoo_indices(indices:jnp.ndarray,shape:Sequence[int])->BCOOProperties
jax.experimental.sparse.bcoo._validate_permutation(data,indices,permutation,shape)
jax.experimental.sparse.bcoo.bcoo_broadcast_in_dim(mat,*,shape,broadcast_dimensions)
jax.experimental.sparse.bcoo.bcoo_concatenate(operands,*,dimension)
jax.experimental.sparse.bcoo.bcoo_dot_general(lhs,rhs,*,dimension_numbers)
jax.experimental.sparse.bcoo.bcoo_dot_general_sampled(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo.bcoo_dynamic_slice(mat,start_indices:Sequence[Any],slice_sizes:Sequence[int])
jax.experimental.sparse.bcoo.bcoo_eliminate_zeros(mat,nse=None)
jax.experimental.sparse.bcoo.bcoo_extract(indices,mat)
jax.experimental.sparse.bcoo.bcoo_fromdense(mat,*,nse=None,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcoo.bcoo_multiply_dense(sp_mat,v)
jax.experimental.sparse.bcoo.bcoo_multiply_sparse(lhs,rhs)
jax.experimental.sparse.bcoo.bcoo_reduce_sum(mat,*,axes)
jax.experimental.sparse.bcoo.bcoo_reshape(mat,*,new_sizes,dimensions)
jax.experimental.sparse.bcoo.bcoo_slice(mat,*,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)
jax.experimental.sparse.bcoo.bcoo_sort_indices(mat)
jax.experimental.sparse.bcoo.bcoo_sum_duplicates(mat,nse=None)
jax.experimental.sparse.bcoo.bcoo_todense(mat)
jax.experimental.sparse.bcoo.bcoo_transpose(mat,*,permutation:Sequence[int])
jax.experimental.sparse.bcoo.bcoo_update_layout(mat,*,n_batch=None,n_dense=None,on_inefficient='error')
jax.experimental.sparse.bcoo_broadcast_in_dim(mat,*,shape,broadcast_dimensions)
jax.experimental.sparse.bcoo_concatenate(operands,*,dimension)
jax.experimental.sparse.bcoo_dot_general(lhs,rhs,*,dimension_numbers)
jax.experimental.sparse.bcoo_dot_general_sampled(A,B,indices,*,dimension_numbers)
jax.experimental.sparse.bcoo_dynamic_slice(mat,start_indices:Sequence[Any],slice_sizes:Sequence[int])
jax.experimental.sparse.bcoo_extract(indices,mat)
jax.experimental.sparse.bcoo_fromdense(mat,*,nse=None,n_batch=0,n_dense=0,index_dtype=jnp.int32)
jax.experimental.sparse.bcoo_multiply_dense(sp_mat,v)
jax.experimental.sparse.bcoo_multiply_sparse(lhs,rhs)
jax.experimental.sparse.bcoo_reduce_sum(mat,*,axes)
jax.experimental.sparse.bcoo_reshape(mat,*,new_sizes,dimensions)
jax.experimental.sparse.bcoo_slice(mat,*,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)
jax.experimental.sparse.bcoo_sort_indices(mat)
jax.experimental.sparse.bcoo_sum_duplicates(mat,nse=None)
jax.experimental.sparse.bcoo_todense(mat)
jax.experimental.sparse.bcoo_transpose(mat,*,permutation:Sequence[int])
jax.experimental.sparse.bcoo_update_layout(mat,*,n_batch=None,n_dense=None,on_inefficient='error')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/linalg.py----------------------------------------
A:jax.experimental.sparse.linalg.X->_mm(XPR, B)
A:jax.experimental.sparse.linalg.P->_mm(XPR, diff_rayleigh_ortho)
A:jax.experimental.sparse.linalg.AX->A(X)
A:jax.experimental.sparse.linalg.theta->jax.numpy.sum(X * AX, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.R->_project_out(jnp.concatenate((X, P), axis=1), R)
A:jax.experimental.sparse.linalg.XPR->jax.numpy.concatenate((X, P, R), axis=1)
A:jax.experimental.sparse.linalg.(theta, Q)->_rayleigh_ritz_orth(A, XPR)
A:jax.experimental.sparse.linalg.normB->jax.numpy.linalg.norm(B, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.normX->jax.numpy.linalg.norm(X, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.(q, _)->jax.numpy.linalg.qr(Q[:k, k:].T)
A:jax.experimental.sparse.linalg.diff_rayleigh_ortho->_mm(Q[:, k:], q)
A:jax.experimental.sparse.linalg.normP->jax.numpy.linalg.norm(P, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.resid_norms->jax.numpy.linalg.norm(R, ord=2, axis=0)
A:jax.experimental.sparse.linalg.converged->jax.numpy.sum(res_converged)
A:jax.experimental.sparse.linalg.diagnostics->_generate_diagnostics(XPR, X, P, R, theta, converged, resid_norms / reltol)
A:jax.experimental.sparse.linalg.(state, diagnostics)->jax.lax.scan(lambda state, _: body(state), state, xs=None, length=m)
A:jax.experimental.sparse.linalg.state->jax.lax.while_loop(cond, body, state)
A:jax.experimental.sparse.linalg.test_output->A(jnp.zeros((n, 1), dtype=X.dtype))
A:jax.experimental.sparse.linalg.XTX->_mm(X.T, X)
A:jax.experimental.sparse.linalg.DX->diagdiag(XTX)
A:jax.experimental.sparse.linalg.orthX->abserr(XTX - DX)
A:jax.experimental.sparse.linalg.PTP->_mm(P.T, P)
A:jax.experimental.sparse.linalg.DP->diagdiag(PTP)
A:jax.experimental.sparse.linalg.orthP->abserr(PTP - DP)
A:jax.experimental.sparse.linalg.PX->abserr(X.T @ P)
A:jax.experimental.sparse.linalg.(w, V)->_eigh_ascending(inner)
A:jax.experimental.sparse.linalg.norms->jax.numpy.linalg.norm(orthoX, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.inner->_mm(X.T, X)
A:jax.experimental.sparse.linalg.padded->jax.numpy.maximum(w, tau)
A:jax.experimental.sparse.linalg.orthoX->_mm(X, scaledV)
A:jax.experimental.sparse.linalg.U->_orthonormalize(U)
A:jax.experimental.sparse.linalg.normU->jax.numpy.linalg.norm(U, ord=2, axis=0, keepdims=True)
A:jax.experimental.sparse.linalg.basis->_svqb(basis)
A:jax.experimental.sparse.linalg.SAS->_mm(S.T, A(S))
A:jax.experimental.sparse.linalg.(Xupper, Xlower)->jax.numpy.split(X, [k], axis=0)
A:jax.experimental.sparse.linalg.(u, s, vt)->jax.numpy.linalg.svd(Xupper)
A:jax.experimental.sparse.linalg.y->jax.numpy.concatenate([Xupper + _mm(u, vt), Xlower], axis=0)
A:jax.experimental.sparse.linalg.other->jax.numpy.concatenate([jnp.eye(m, dtype=X.dtype), jnp.zeros((n - k - m, m), dtype=X.dtype)], axis=0)
A:jax.experimental.sparse.linalg.w->_mm(y, vt.T * ((2 * (1 + s)) ** (-1 / 2))[jnp.newaxis, :])
A:jax.experimental.sparse.linalg.spsolve_p->jax.core.Primitive('spsolve')
jax.experimental.sparse.linalg._check_inputs(A,X)
jax.experimental.sparse.linalg._eigh_ascending(A)
jax.experimental.sparse.linalg._extend_basis(X,m)
jax.experimental.sparse.linalg._generate_diagnostics(prev_XPR,X,P,R,theta,converged,adj_resid)
jax.experimental.sparse.linalg._lobpcg_standard_callable(A:Callable[[jnp.ndarray],jnp.ndarray],X:jnp.ndarray,m:int,tol:Union[jnp.ndarray,float,None],debug:bool=False)
jax.experimental.sparse.linalg._lobpcg_standard_matrix(A:jnp.ndarray,X:jnp.ndarray,m:int,tol:Union[jnp.ndarray,float,None],debug:bool=False)
jax.experimental.sparse.linalg._mm(a,b,precision=jax.lax.Precision.HIGHEST)
jax.experimental.sparse.linalg._orthonormalize(basis)
jax.experimental.sparse.linalg._project_out(basis,U)
jax.experimental.sparse.linalg._rayleigh_ritz_orth(A,S)
jax.experimental.sparse.linalg._spsolve_abstract_eval(data,indices,indptr,b,tol,reorder)
jax.experimental.sparse.linalg._spsolve_gpu_lowering(ctx,data,indices,indptr,b,tol,reorder)
jax.experimental.sparse.linalg._svqb(X)
jax.experimental.sparse.linalg.lobpcg_standard(A:Union[jnp.ndarray,Callable[[jnp.ndarray],jnp.ndarray]],X:jnp.ndarray,m:int=100,tol:Union[jnp.ndarray,float,None]=None)
jax.experimental.sparse.linalg.spsolve(data,indices,indptr,b,tol=1e-06,reorder=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/_base.py----------------------------------------
A:jax.experimental.sparse._base.self.shape->tuple(shape)
A:jax.experimental.sparse._base.shape->list(self.shape)
jax.experimental.sparse._base.JAXSparse(self,args,*,shape)
jax.experimental.sparse._base.JAXSparse.T(self)
jax.experimental.sparse._base.JAXSparse.__add__(self,other)
jax.experimental.sparse._base.JAXSparse.__getitem__(self,item)
jax.experimental.sparse._base.JAXSparse.__init__(self,args,*,shape)
jax.experimental.sparse._base.JAXSparse.__matmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__mul__(self,other)
jax.experimental.sparse._base.JAXSparse.__neg__(self)
jax.experimental.sparse._base.JAXSparse.__pos__(self)
jax.experimental.sparse._base.JAXSparse.__radd__(self,other)
jax.experimental.sparse._base.JAXSparse.__repr__(self)
jax.experimental.sparse._base.JAXSparse.__rmatmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__rmul__(self,other)
jax.experimental.sparse._base.JAXSparse.__rsub__(self,other)
jax.experimental.sparse._base.JAXSparse.__sub__(self,other)
jax.experimental.sparse._base.JAXSparse.block_until_ready(self)
jax.experimental.sparse._base.JAXSparse.ndim(self)
jax.experimental.sparse._base.JAXSparse.size(self)
jax.experimental.sparse._base.JAXSparse.sum(self,*args,**kwargs)
jax.experimental.sparse._base.JAXSparse.transpose(self,axes=None)
jax.experimental.sparse._base.JAXSparse.tree_flatten(self)
jax.experimental.sparse._base.JAXSparse.tree_unflatten(cls,aux_data,children)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/random.py----------------------------------------
A:jax.experimental.sparse.random.shape->tuple(map(operator.index, shape))
A:jax.experimental.sparse.random.n_batch->operator.index(n_batch)
A:jax.experimental.sparse.random.n_dense->operator.index(n_dense)
A:jax.experimental.sparse.random.(batch_shape, sparse_shape, dense_shape)->map(tuple, split_list(shape, [n_batch, n_sparse]))
A:jax.experimental.sparse.random.batch_size->numpy.prod(batch_shape)
A:jax.experimental.sparse.random.sparse_size->numpy.prod(sparse_shape)
A:jax.experimental.sparse.random.nse->operator.index(nse)
A:jax.experimental.sparse.random.flat_ind->jax.random.choice(key, sparse_size, shape=(nse,), replace=not unique_indices)
A:jax.experimental.sparse.random.keys->jax.random.split(key, batch_size + 1)
A:jax.experimental.sparse.random.data->generator(data_key, shape=data_shape, dtype=dtype, **kwds)
A:jax.experimental.sparse.random.indices->_indices(index_keys).reshape(indices_shape).astype(indices_dtype)
A:jax.experimental.sparse.random.mat->jax.experimental.sparse.BCOO((data, indices), shape=shape)
jax.experimental.sparse.random.random_bcoo(key,shape,*,dtype=jnp.float_,indices_dtype=jnp.int_,nse=0.2,n_batch=0,n_dense=0,unique_indices=True,sorted_indices=False,generator=random.uniform,**kwds)
jax.experimental.sparse.random_bcoo(key,shape,*,dtype=jnp.float_,indices_dtype=jnp.int_,nse=0.2,n_batch=0,n_dense=0,unique_indices=True,sorted_indices=False,generator=random.uniform,**kwds)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/util.py----------------------------------------
A:jax.experimental.sparse.util.(args_flat, in_tree)->jax.tree_util.tree_flatten(args)
A:jax.experimental.sparse.util.in_axes_flat->flatten_axes('vmap in_axes', in_tree, in_axes, kws=False)
A:jax.experimental.sparse.util.size->max((arg.shape[i] for (arg, i) in safe_zip(args_flat, in_axes_flat) if i is not None))
A:jax.experimental.sparse.util.(args_flat, in_axes_flat)->zip(*((arg, None) if i is None else (lax.squeeze(arg, (i,)), None) if arg.shape[i] == 1 else (arg, i) for (arg, i) in zip(args_flat, in_axes_flat)))
A:jax.experimental.sparse.util.new_args->jax.tree_util.tree_unflatten(in_tree, args_flat)
A:jax.experimental.sparse.util.new_in_axes->jax.tree_util.tree_unflatten(in_tree, in_axes_flat)
A:jax.experimental.sparse.util.mat->jax.numpy.asarray(mat)
A:jax.experimental.sparse.util.mask->mask.sum(list(range(n_batch, mask.ndim))).sum(list(range(n_batch, mask.ndim)))
jax.experimental.sparse.CuSparseEfficiencyWarning(SparseEfficiencyWarning)
jax.experimental.sparse.SparseEfficiencyError(ValueError)
jax.experimental.sparse.SparseEfficiencyWarning(UserWarning)
jax.experimental.sparse.util.CuSparseEfficiencyWarning(SparseEfficiencyWarning)
jax.experimental.sparse.util.SparseEfficiencyError(ValueError)
jax.experimental.sparse.util.SparseEfficiencyWarning(UserWarning)
jax.experimental.sparse.util._asarray_or_float0(arg)
jax.experimental.sparse.util._broadcasting_vmap(fun,in_axes=0,out_axes=0)
jax.experimental.sparse.util._coo_extract(row,col,mat)
jax.experimental.sparse.util._count_stored_elements(mat,n_batch=0,n_dense=0)
jax.experimental.sparse.util._count_stored_elements_per_batch(mat,n_batch=0,n_dense=0)
jax.experimental.sparse.util._csr_extract(indices,indptr,mat)
jax.experimental.sparse.util._csr_to_coo(indices,indptr)
jax.experimental.sparse.util._is_arginfo(*args)
jax.experimental.sparse.util._is_aval(*args)
jax.experimental.sparse.util._is_pytree_placeholder(*args)
jax.experimental.sparse.util._safe_asarray(args)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/sparse/transform.py----------------------------------------
A:jax.experimental.sparse.transform.self._buffers->list(bufs)
A:jax.experimental.sparse.transform.data_ref->self._push(data)
A:jax.experimental.sparse.transform.indices_ref->self._push(indices)
A:jax.experimental.sparse.transform.data->SparsifyEnv().data(arr)
A:jax.experimental.sparse.transform.val->getattr(obj, name)
A:jax.experimental.sparse.transform.(spvalue,)->arrays_to_spvalues(self.main.spenv, [val])
A:jax.experimental.sparse.transform.spenv->SparsifyEnv()
A:jax.experimental.sparse.transform.out_spvalues->arrays_to_spvalues(spenv, out_bufs if primitive.multiple_results else [out_bufs])
A:jax.experimental.sparse.transform.out_bufs->prim.bind(*(spenv.data(val) for val in invals), **eqn.params)
A:jax.experimental.sparse.transform.out_tracers->tuple((SparseTracer(self, spvalue=spvalue) for spvalue in out_spvalues))
A:jax.experimental.sparse.transform.spvalues->arrays_to_spvalues(spenv, args)
A:jax.experimental.sparse.transform.(fun, out_spvalues)->sparsify_subtrace(wrapped_fun, main, spvalues)
A:jax.experimental.sparse.transform.params->eqn.params.copy()
A:jax.experimental.sparse.transform.bufs_out->call_primitive.bind(fun, *in_bufs, **params)
A:jax.experimental.sparse.transform.trace->main.with_cur_sublevel()
A:jax.experimental.sparse.transform.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.sparse.transform.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(f, params), in_tree)
A:jax.experimental.sparse.transform.out->jax.experimental.sparse.bcoo_dynamic_slice(args[0], args[1:], **params)
A:jax.experimental.sparse.transform.env[var]->SparsifyEnv().dense(a)
A:jax.experimental.sparse.transform.invals->safe_map(read, eqn.invars)
A:jax.experimental.sparse.transform.fun->jax.linear_util.wrap_init(core.jaxpr_as_fun(sp_call_jaxpr))
A:jax.experimental.sparse.transform.(spvalues_flat, in_tree)->tree_flatten(spvalues, is_leaf=_is_spvalue)
A:jax.experimental.sparse.transform.in_avals_flat->spvalues_to_avals(spenv, spvalues_flat)
A:jax.experimental.sparse.transform.(jaxpr, out_avals_flat, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, in_avals_flat)
A:jax.experimental.sparse.transform.result->jax.experimental.sparse.BCOO.fromdense(result)
A:jax.experimental.sparse.transform.f_raw->sparsify_raw(f)
A:jax.experimental.sparse.transform.(spvalues_out, out_tree)->f_raw(spenv, *spvalues, **params)
A:jax.experimental.sparse.transform.arr->arr.sum_duplicates(nse=arr.nse).sum_duplicates(nse=arr.nse)
A:jax.experimental.sparse.transform.spvalue->SparsifyEnv().sparse(out_shape, **kwds)
A:jax.experimental.sparse.transform.buf->SparsifyEnv().data(spvalue)
A:jax.experimental.sparse.transform.buf_out->prim.bind(buf, **kwargs)
A:jax.experimental.sparse.transform.out_spvalue->SparsifyEnv().sparse(out_shape, mat.data, mat.indices)
A:jax.experimental.sparse.transform.sparse_rules[_prim]->_zero_preserving_unary_op(_prim)
A:jax.experimental.sparse.transform.permutation->tuple(permutation)
A:jax.experimental.sparse.transform.args->spvalues_to_arrays(spenv, operands)
A:jax.experimental.sparse.transform.mat_transposed->jax.experimental.sparse.bcoo_transpose(args[0], permutation=permutation)
A:jax.experimental.sparse.transform.out_shape->tuple((s for (i, s) in enumerate(arr.shape) if i not in dimensions))
A:jax.experimental.sparse.transform.out_data->bcoo_multiply_dense(X_promoted, spenv.data(Y))
A:jax.experimental.sparse.transform.out_indices->jax.lax.concatenate([spenv.indices(X), spenv.indices(Y)], dimension=spenv.indices(X).ndim - 2)
A:jax.experimental.sparse.transform.(X_promoted, Y_promoted)->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.mat->jax.experimental.sparse.bcoo_broadcast_in_dim(operand_promoted, shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax.experimental.sparse.transform.X_promoted->spvalues_to_arrays(spenv, X)
A:jax.experimental.sparse.transform.operand_promoted->spvalues_to_arrays(spenv, operand)
A:jax.experimental.sparse.transform.operands->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.dimensions->tuple((canonicalize_axis(dim, arr.ndim) for dim in dimensions))
A:jax.experimental.sparse.transform.indices->SparsifyEnv().indices(arr)
A:jax.experimental.sparse.transform.batch_dims->tuple((d for d in dimensions if d < n_batch))
A:jax.experimental.sparse.transform.sparse_dims->numpy.array([i for i in range(n_sparse) if i + n_batch not in dimensions], dtype=int)
A:jax.experimental.sparse.transform.dense_dims->tuple((d - n_sparse + 1 for d in dimensions if d >= n_batch + n_sparse))
A:jax.experimental.sparse.transform.data_out->jax.lax.squeeze(data, batch_dims + dense_dims)
A:jax.experimental.sparse.transform.indices_out->jax.lax.squeeze(indices[..., sparse_dims], batch_dims)
A:jax.experimental.sparse.transform.(operand,)->spvalues_to_arrays(spenv, spvalues)
A:jax.experimental.sparse.transform.(out_flat, out_tree)->tree_flatten(out)
A:jax.experimental.sparse.transform.(sp_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped, avals_flat)
A:jax.experimental.sparse.transform.sp_jaxpr->jax.interpreters.partial_eval.ClosedJaxpr(sp_jaxpr, consts)
A:jax.experimental.sparse.transform.(cond_const_spvalues, body_const_spvalues, init_val_spvalues)->split_list(spvalues, [cond_nconsts, body_nconsts])
A:jax.experimental.sparse.transform.(cond_sp_jaxpr, _)->_sparsify_jaxpr(spenv, cond_jaxpr, *cond_const_spvalues, *init_val_spvalues)
A:jax.experimental.sparse.transform.(body_sp_jaxpr, out_tree)->_sparsify_jaxpr(spenv, body_jaxpr, *body_const_spvalues, *init_val_spvalues)
A:jax.experimental.sparse.transform.(cond_consts, _)->tree_flatten(spvalues_to_arrays(spenv, cond_const_spvalues))
A:jax.experimental.sparse.transform.(body_consts, _)->tree_flatten(spvalues_to_arrays(spenv, body_const_spvalues))
A:jax.experimental.sparse.transform.(init_vals, _)->tree_flatten(spvalues_to_arrays(spenv, init_val_spvalues))
A:jax.experimental.sparse.transform.out_flat->jax.lax.cond_p.bind(*args, branches=sp_branches, linear=sp_linear, **params)
A:jax.experimental.sparse.transform.(sp_call_jaxpr, out_tree)->_sparsify_jaxpr(spenv, pe.ClosedJaxpr(call_jaxpr, ()), *spvalues)
A:jax.experimental.sparse.transform.(args_flat, _)->tree_flatten(spvalues_to_arrays(spenv, spvalues))
A:jax.experimental.sparse.transform.donated_invars->tuple((False for arg in args_flat))
A:jax.experimental.sparse.transform.(const_spvalues, carry_spvalues, xs_spvalues)->split_list(spvalues, [num_consts, num_carry])
A:jax.experimental.sparse.transform.(sp_jaxpr, _)->_sparsify_jaxpr(spenv, jaxpr, *const_spvalues, *carry_spvalues, *xs_spvalues)
A:jax.experimental.sparse.transform.(consts, _)->tree_flatten(spvalues_to_arrays(spenv, const_spvalues))
A:jax.experimental.sparse.transform.(carry, carry_tree)->tree_flatten(spvalues_to_arrays(spenv, carry_spvalues))
A:jax.experimental.sparse.transform.(xs, xs_tree)->tree_flatten(spvalues_to_arrays(spenv, xs_spvalues))
A:jax.experimental.sparse.transform.(const_linear, carry_linear, xs_linear)->split_list(params.pop('linear'), [num_consts, num_carry])
A:jax.experimental.sparse.transform.sp_linear->tuple(_duplicate_for_sparse_spvalues(operands, linear))
A:jax.experimental.sparse.transform.carry_out->tree_unflatten(carry_tree, out[:len(carry)])
A:jax.experimental.sparse.transform.xs_out->tree_unflatten(xs_tree, out[len(carry):])
A:jax.experimental.sparse.transform.(sp_branches, treedefs)->zip(*(_sparsify_jaxpr(spenv, jaxpr, *operands) for jaxpr in branches))
A:jax.experimental.sparse.transform.(args, _)->tree_flatten(spvalues_to_arrays(spenv, (pred, *operands)))
A:jax.experimental.sparse.transform.(treedef, static_idx, dynamic_idx)->jax._src.numpy.lax_numpy._split_index_for_jit(idx, arr.shape)
jax.experimental.sparse.SparseTracer(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.SparseTracer.aval(self)
jax.experimental.sparse.SparseTracer.full_lower(self)
jax.experimental.sparse.SparseTracer.spenv(self)
jax.experimental.sparse.sparsify(f,use_tracer=False)
jax.experimental.sparse.sparsify_fun(wrapped_fun,args:List[ArrayOrSparse])
jax.experimental.sparse.sparsify_raw(f)
jax.experimental.sparse.sparsify_subtrace(main,spvalues,*bufs)
jax.experimental.sparse.transform.SparseTrace(core.Trace)
jax.experimental.sparse.transform.SparseTrace.lift(self,val:core.Tracer)
jax.experimental.sparse.transform.SparseTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.sparse.transform.SparseTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.sparse.transform.SparseTrace.pure(self,val:Any)
jax.experimental.sparse.transform.SparseTrace.sublift(self,val:SparseTracer)
jax.experimental.sparse.transform.SparseTracer(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.transform.SparseTracer.__init__(self,trace:core.Trace,*,spvalue)
jax.experimental.sparse.transform.SparseTracer.aval(self)
jax.experimental.sparse.transform.SparseTracer.full_lower(self)
jax.experimental.sparse.transform.SparseTracer.spenv(self)
jax.experimental.sparse.transform.SparsifyEnv(self,bufs=())
jax.experimental.sparse.transform.SparsifyEnv.__init__(self,bufs=())
jax.experimental.sparse.transform.SparsifyEnv._push(self,arr:Array)->int
jax.experimental.sparse.transform.SparsifyEnv.data(self,spvalue:'SparsifyValue')->Array
jax.experimental.sparse.transform.SparsifyEnv.dense(self,data)
jax.experimental.sparse.transform.SparsifyEnv.indices(self,spvalue:'SparsifyValue')->Array
jax.experimental.sparse.transform.SparsifyEnv.sparse(self,shape,data=None,indices=None,*,data_ref=None,indices_ref=None,indices_sorted=False,unique_indices=False)
jax.experimental.sparse.transform.SparsifyValue(NamedTuple)
jax.experimental.sparse.transform.SparsifyValue.is_dense(self)
jax.experimental.sparse.transform.SparsifyValue.is_sparse(self)
jax.experimental.sparse.transform.SparsifyValue.ndim(self)
jax.experimental.sparse.transform._add_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._broadcast_in_dim_sparse(spenv,*spvalues,shape,broadcast_dimensions)
jax.experimental.sparse.transform._concatenate_sparse(spenv,*spvalues,dimension)
jax.experimental.sparse.transform._cond_sparse(spenv,pred,*operands,branches,linear,**params)
jax.experimental.sparse.transform._dot_general_sparse(spenv,*spvalues,dimension_numbers,precision,preferred_element_type)
jax.experimental.sparse.transform._duplicate_for_sparse_spvalues(spvalues,params)
jax.experimental.sparse.transform._dynamic_slice_sparse_rule(spenv,*operands,**params)
jax.experimental.sparse.transform._ensure_unique_indices(spenv,spvalue)
jax.experimental.sparse.transform._mul_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._raise_unimplemented_primitive(primitive)
jax.experimental.sparse.transform._reduce_sum_sparse(spenv,*spvalues,axes)
jax.experimental.sparse.transform._reshape(self,*args,**kwargs)
jax.experimental.sparse.transform._reshape_sparse(spenv,*spvalues,new_sizes,dimensions)
jax.experimental.sparse.transform._scan_sparse(spenv,*spvalues,jaxpr,num_consts,num_carry,**params)
jax.experimental.sparse.transform._slice_sparse_rule(spenv,*operands,**params)
jax.experimental.sparse.transform._sparse_rewriting_take(arr,idx,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax.experimental.sparse.transform._sparsify_jaxpr(spenv,jaxpr,*spvalues)
jax.experimental.sparse.transform._sparsify_with_interpreter(f)
jax.experimental.sparse.transform._sparsify_with_tracer(fun)
jax.experimental.sparse.transform._squeeze_sparse(spenv,*spvalues,dimensions)
jax.experimental.sparse.transform._sub_sparse(spenv,*spvalues)
jax.experimental.sparse.transform._sum(self,*args,**kwargs)
jax.experimental.sparse.transform._todense_sparse_rule(spenv,spvalue,*,tree)
jax.experimental.sparse.transform._transpose_sparse(spenv,*spvalues,permutation)
jax.experimental.sparse.transform._while_sparse(spenv,*spvalues,cond_jaxpr,cond_nconsts,body_jaxpr,body_nconsts)
jax.experimental.sparse.transform._xla_call_sparse(spenv,*spvalues,call_jaxpr,donated_invars,**params)
jax.experimental.sparse.transform._zero_preserving_unary_op(prim)
jax.experimental.sparse.transform.arrays_to_spvalues(spenv:SparsifyEnv,args:Any)->Any
jax.experimental.sparse.transform.eval_sparse(jaxpr:core.Jaxpr,consts:Sequence[Array],spvalues:Sequence[SparsifyValue],spenv:SparsifyEnv)->Sequence[SparsifyValue]
jax.experimental.sparse.transform.popattr(obj,name)
jax.experimental.sparse.transform.setnewattr(obj,name,val)
jax.experimental.sparse.transform.sparsify(f,use_tracer=False)
jax.experimental.sparse.transform.sparsify_fun(wrapped_fun,args:List[ArrayOrSparse])
jax.experimental.sparse.transform.sparsify_raw(f)
jax.experimental.sparse.transform.sparsify_subtrace(main,spvalues,*bufs)
jax.experimental.sparse.transform.spvalues_to_arrays(spenv:SparsifyEnv,spvalues:Any)->Any
jax.experimental.sparse.transform.spvalues_to_avals(spenv:SparsifyEnv,spvalues:Any)->Any


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/compilation_cache/compilation_cache.py----------------------------------------
A:jax.experimental.compilation_cache.compilation_cache._cache->GFileCache(path)
A:jax.experimental.compilation_cache.compilation_cache.cache_key->get_cache_key(xla_computation, compile_options, backend)
A:jax.experimental.compilation_cache.compilation_cache.xla_executable_serialized->GFileCache(path).get(cache_key)
A:jax.experimental.compilation_cache.compilation_cache.xla_executable_deserialized->backend.deserialize_executable(xla_executable_serialized, compile_options)
A:jax.experimental.compilation_cache.compilation_cache.serialized_executable->backend.serialize_executable(executable)
A:jax.experimental.compilation_cache.compilation_cache.fresh_hash_obj->hashlib.sha256()
A:jax.experimental.compilation_cache.compilation_cache.hash_obj->hashlib.sha256()
A:jax.experimental.compilation_cache.compilation_cache.serialized_hlo->xla_computation.as_serialized_hlo_module_proto()
A:jax.experimental.compilation_cache.compilation_cache.scrubbed_hlo->re.sub(b' at 0x[a-f0-9]+>', b' at 0x...>', serialized_hlo)
A:jax.experimental.compilation_cache.compilation_cache.xla_flags_env_var->os.getenv('XLA_FLAGS')
jax.experimental.compilation_cache.compilation_cache._hash_bool(hash_obj,bool_var)
jax.experimental.compilation_cache.compilation_cache._hash_compile_options(hash_obj,compile_options_obj)
jax.experimental.compilation_cache.compilation_cache._hash_computation(hash_obj,xla_computation)
jax.experimental.compilation_cache.compilation_cache._hash_debug_options(hash_obj,debug_obj)
jax.experimental.compilation_cache.compilation_cache._hash_executable_build_options(hash_obj,executable_obj)
jax.experimental.compilation_cache.compilation_cache._hash_int(hash_obj,int_var)
jax.experimental.compilation_cache.compilation_cache._hash_platform(hash_obj,backend)
jax.experimental.compilation_cache.compilation_cache._hash_string(hash_obj,str_var)
jax.experimental.compilation_cache.compilation_cache._hash_xla_flags(hash_obj)
jax.experimental.compilation_cache.compilation_cache._log_cache_key_hash(hash_obj,last_serialized:str,hashfn)
jax.experimental.compilation_cache.compilation_cache.get_cache_key(xla_computation,compile_options,backend)->str
jax.experimental.compilation_cache.compilation_cache.get_executable(xla_computation,compile_options,backend)->Optional[xla.XlaLoadedExecutable]
jax.experimental.compilation_cache.compilation_cache.initialize_cache(path)
jax.experimental.compilation_cache.compilation_cache.is_initialized()
jax.experimental.compilation_cache.compilation_cache.put_executable(module_name,xla_computation,compile_options,executable:xla.XlaLoadedExecutable,backend)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/compilation_cache/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/compilation_cache/cache_interface.py----------------------------------------
jax.experimental.compilation_cache.cache_interface.CacheInterface(ABC)
jax.experimental.compilation_cache.cache_interface.CacheInterface.get(self,key:str)
jax.experimental.compilation_cache.cache_interface.CacheInterface.put(self,key:str,value:bytes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/experimental/compilation_cache/gfile_cache.py----------------------------------------
A:jax.experimental.compilation_cache.gfile_cache.self._path->etils.epath.Path(path)
jax.experimental.compilation_cache.gfile_cache.GFileCache(self,path:str)
jax.experimental.compilation_cache.gfile_cache.GFileCache.__init__(self,path:str)
jax.experimental.compilation_cache.gfile_cache.GFileCache.get(self,key:str)
jax.experimental.compilation_cache.gfile_cache.GFileCache.put(self,key:str,value:bytes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/example_libraries/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/example_libraries/optimizers.py----------------------------------------
A:jax.example_libraries.optimizers.OptimizerState->namedtuple('OptimizerState', ['packed_state', 'tree_def', 'subtree_defs'])
A:jax.example_libraries.optimizers.(init, update, get_params)->opt_maker(*args, **kwargs)
A:jax.example_libraries.optimizers.(x0_flat, tree)->tree_flatten(x0_tree)
A:jax.example_libraries.optimizers.(states_flat, subtrees)->unzip2(map(tree_flatten, initial_states))
A:jax.example_libraries.optimizers.(grad_flat, tree2)->tree_flatten(grad_tree)
A:jax.example_libraries.optimizers.states->map(tree_unflatten, subtrees, states_flat)
A:jax.example_libraries.optimizers.new_states->map(partial(update, i), grad_flat, states)
A:jax.example_libraries.optimizers.(new_states_flat, subtrees2)->unzip2(map(tree_flatten, new_states))
A:jax.example_libraries.optimizers.params->map(get_params, states)
A:jax.example_libraries.optimizers.step_size->make_schedule(step_size)
A:jax.example_libraries.optimizers.v0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.g_sq->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.m->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.g_sq_inv_sqrt->jax.numpy.where(g_sq > 0, 1.0 / jnp.sqrt(g_sq), 0.0)
A:jax.example_libraries.optimizers.avg_sq_grad->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.mom->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.m0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.u0->jax.numpy.zeros_like(x0)
A:jax.example_libraries.optimizers.u->jax.numpy.maximum(b2 * u, jnp.abs(g))
A:jax.example_libraries.optimizers.lst->list(seq)
A:jax.example_libraries.optimizers.idx->splice([None] * ndim, axis, [slice(None)])
A:jax.example_libraries.optimizers.x0->jax.numpy.atleast_1d(x0)
A:jax.example_libraries.optimizers.accum_inv_sqrt->jax.numpy.where(accum > 0, 1.0 / jnp.sqrt(accum), 0)
A:jax.example_libraries.optimizers.step_num->jax.numpy.minimum(step_num, decay_steps)
A:jax.example_libraries.optimizers.boundaries->jax.numpy.array(boundaries)
A:jax.example_libraries.optimizers.values->jax.numpy.array(values)
A:jax.example_libraries.optimizers.(leaves, _)->tree_flatten(tree)
A:jax.example_libraries.optimizers.norm->l2_norm(grad_tree)
A:jax.example_libraries.optimizers.subtrees->map(tree_unflatten, subtree_defs, states_flat)
A:jax.example_libraries.optimizers.(sentinels, tree_def)->tree_flatten(marked_pytree)
A:jax.example_libraries.optimizers.(states_flat, subtree_defs)->unzip2(map(tree_flatten, subtrees))
jax.example_libraries.optimizers.JoinPoint(self,subtree)
jax.example_libraries.optimizers.JoinPoint.__init__(self,subtree)
jax.example_libraries.optimizers.JoinPoint.__iter__(self)
jax.example_libraries.optimizers.Optimizer(NamedTuple)
jax.example_libraries.optimizers.adagrad(step_size,momentum=0.9)
jax.example_libraries.optimizers.adam(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.example_libraries.optimizers.adamax(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.example_libraries.optimizers.clip_grads(grad_tree,max_norm)
jax.example_libraries.optimizers.constant(step_size)->Schedule
jax.example_libraries.optimizers.exponential_decay(step_size,decay_steps,decay_rate)
jax.example_libraries.optimizers.inverse_time_decay(step_size,decay_steps,decay_rate,staircase=False)
jax.example_libraries.optimizers.l2_norm(tree)
jax.example_libraries.optimizers.make_schedule(scalar_or_schedule:Union[float,Schedule])->Schedule
jax.example_libraries.optimizers.momentum(step_size:Schedule,mass:float)
jax.example_libraries.optimizers.nesterov(step_size:Schedule,mass:float)
jax.example_libraries.optimizers.optimizer(opt_maker:Callable[...,Tuple[Callable[[Params],State],Callable[[Step,Updates,Params],Params],Callable[[State],Params]]])->Callable[..., Optimizer]
jax.example_libraries.optimizers.pack_optimizer_state(marked_pytree)
jax.example_libraries.optimizers.piecewise_constant(boundaries:Any,values:Any)
jax.example_libraries.optimizers.polynomial_decay(step_size,decay_steps,final_step_size,power=1.0)
jax.example_libraries.optimizers.rmsprop(step_size,gamma=0.9,eps=1e-08)
jax.example_libraries.optimizers.rmsprop_momentum(step_size,gamma=0.9,eps=1e-08,momentum=0.9)
jax.example_libraries.optimizers.sgd(step_size)
jax.example_libraries.optimizers.sm3(step_size,momentum=0.9)
jax.example_libraries.optimizers.unpack_optimizer_state(opt_state)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/example_libraries/stax.py----------------------------------------
A:jax.example_libraries.stax.(k1, k2)->jax.random.split(rng)
A:jax.example_libraries.stax.filter_shape_iter->iter(filter_shape)
A:jax.example_libraries.stax.output_shape->jax.lax.conv_transpose_shape_tuple(input_shape, kernel_shape, strides, padding, dimension_numbers)
A:jax.example_libraries.stax.Conv->functools.partial(GeneralConv, ('NHWC', 'HWIO', 'NHWC'))
A:jax.example_libraries.stax.Conv1DTranspose->functools.partial(GeneralConvTranspose, ('NHC', 'HIO', 'NHC'))
A:jax.example_libraries.stax.ConvTranspose->functools.partial(GeneralConvTranspose, ('NHWC', 'HWIO', 'NHWC'))
A:jax.example_libraries.stax.shape->tuple((d for (i, d) in enumerate(input_shape) if i not in axis))
A:jax.example_libraries.stax.ed->tuple((None if i in axis else slice(None) for i in range(jnp.ndim(x))))
A:jax.example_libraries.stax.z->standardize(x, axis, epsilon=epsilon)
A:jax.example_libraries.stax.Tanh->elementwise(jnp.tanh)
A:jax.example_libraries.stax.Relu->elementwise(relu)
A:jax.example_libraries.stax.Exp->elementwise(jnp.exp)
A:jax.example_libraries.stax.LogSoftmax->elementwise(log_softmax, axis=-1)
A:jax.example_libraries.stax.Softmax->elementwise(softmax, axis=-1)
A:jax.example_libraries.stax.Softplus->elementwise(softplus)
A:jax.example_libraries.stax.Sigmoid->elementwise(sigmoid)
A:jax.example_libraries.stax.Elu->elementwise(elu)
A:jax.example_libraries.stax.LeakyRelu->elementwise(leaky_relu)
A:jax.example_libraries.stax.Selu->elementwise(selu)
A:jax.example_libraries.stax.Gelu->elementwise(gelu)
A:jax.example_libraries.stax.padding_vals->jax.lax.padtype_to_pads(input_shape, window_shape, strides, padding)
A:jax.example_libraries.stax.out_shape->jax.lax.reduce_window_shape_tuple(input_shape, window_shape, strides, padding_vals, ones, ones)
A:jax.example_libraries.stax.out->jax.lax.reduce_window(inputs, init_val, reducer, window_shape, strides, padding)
A:jax.example_libraries.stax.MaxPool->_pooling_layer(lax.max, -jnp.inf)
A:jax.example_libraries.stax.SumPool->_pooling_layer(lax.add, 0.0)
A:jax.example_libraries.stax.spatial_shape->tuple((inputs.shape[i] for i in range(inputs.ndim) if i not in non_spatial_axes))
A:jax.example_libraries.stax.one->jax.numpy.ones(spatial_shape, dtype=inputs.dtype)
A:jax.example_libraries.stax.window_sizes->jax.numpy.expand_dims(window_sizes, i)
A:jax.example_libraries.stax.AvgPool->_pooling_layer(lax.add, 0.0, _normalize_by_window_size)
A:jax.example_libraries.stax.Flatten->Flatten()
A:jax.example_libraries.stax.Identity->Identity()
A:jax.example_libraries.stax.FanInSum->FanInSum()
A:jax.example_libraries.stax.concat_size->sum((shape[ax] for shape in input_shape))
A:jax.example_libraries.stax.rng->kwargs.pop('rng', None)
A:jax.example_libraries.stax.keep->jax.random.bernoulli(rng, rate, inputs.shape)
A:jax.example_libraries.stax.nlayers->len(layers)
A:jax.example_libraries.stax.(init_funs, apply_funs)->zip(*layers)
A:jax.example_libraries.stax.(rng, layer_rng)->jax.random.split(rng)
A:jax.example_libraries.stax.(input_shape, param)->init_fun(layer_rng, input_shape)
A:jax.example_libraries.stax.inputs->fun(param, inputs, rng=rng, **kwargs)
A:jax.example_libraries.stax.rngs->jax.random.split(rng, nlayers)
jax.example_libraries.stax.BatchNorm(axis=(0,1,2),epsilon=1e-05,center=True,scale=True,beta_init=zeros,gamma_init=ones)
jax.example_libraries.stax.Dense(out_dim,W_init=glorot_normal(),b_init=normal())
jax.example_libraries.stax.Dropout(rate,mode='train')
jax.example_libraries.stax.FanInConcat(axis=-1)
jax.example_libraries.stax.FanInSum()
jax.example_libraries.stax.FanOut(num)
jax.example_libraries.stax.Flatten()
jax.example_libraries.stax.GeneralConv(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.example_libraries.stax.GeneralConvTranspose(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.example_libraries.stax.Identity()
jax.example_libraries.stax._normalize_by_window_size(dims,strides,padding)
jax.example_libraries.stax._pooling_layer(reducer,init_val,rescaler=None)
jax.example_libraries.stax.elementwise(fun,**fun_kwargs)
jax.example_libraries.stax.parallel(*layers)
jax.example_libraries.stax.serial(*layers)
jax.example_libraries.stax.shape_dependent(make_layer)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/image/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/fft.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/ndimage.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/special.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/linalg.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/signal.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/cluster/vq.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/sparse/linalg.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/beta.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/multivariate_normal.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/chi2.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/cauchy.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/bernoulli.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/logistic.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/expon.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/nbinom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/poisson.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/multinomial.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/betabinom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/t.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/laplace.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/geom.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/gennorm.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/pareto.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/norm.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/dirichlet.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/gamma.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/stats/uniform.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/optimize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/scipy/interpolate/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/lib/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/lib/xla_bridge.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/numpy/__init__.py----------------------------------------
A:jax.numpy.__init__.globals()[name]->jax._src.numpy.lax_numpy._not_implemented(func, module='numpy')
jax.numpy.__init__._init()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/numpy/fft.py----------------------------------------
A:jax.numpy.fft.globals()[name]->jax._src.numpy.lax_numpy._not_implemented(func)
jax.numpy.fft._init()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/numpy/linalg.py----------------------------------------
A:jax.numpy.linalg.globals()[name]->jax._src.numpy.lax_numpy._not_implemented(func)
jax.numpy.linalg._init()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/tools/colab_tpu.py----------------------------------------
jax.tools.colab_tpu.setup_tpu(tpu_driver_version='tpu_driver_20221011')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/tools/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/tools/jax_to_ir.py----------------------------------------
A:jax.tools.jax_to_ir.fn_curried->functools.partial(fn, **constants)
A:jax.tools.jax_to_ir.comp->jax.xla_computation(ordered_wrapper)(*args)
A:jax.tools.jax_to_ir.serialized_proto->f.get_concrete_function(*args).graph.as_graph_def().SerializeToString()
A:jax.tools.jax_to_ir.debug_txt->str(g)
A:jax.tools.jax_to_ir.f->tensorflow.function(f, autograph=False)
A:jax.tools.jax_to_ir.g->tensorflow.function(f, autograph=False).get_concrete_function(*args).graph.as_graph_def()
A:jax.tools.jax_to_ir.args->tuple((tf.identity(a, name=name) for (a, (name, _)) in zip(args, input_shapes)))
A:jax.tools.jax_to_ir.jax_to_hlo->functools.partial(jax_to_ir, format='HLO')
A:jax.tools.jax_to_ir.jax_to_tf->functools.partial(jax_to_ir, format='TF')
A:jax.tools.jax_to_ir.(module_name, fn_name)->FLAGS.fn.rsplit('.', 1)
A:jax.tools.jax_to_ir.module->importlib.import_module(module_name)
A:jax.tools.jax_to_ir.fn->getattr(module, fn_name)
A:jax.tools.jax_to_ir.v->jax.numpy.asarray(v)
A:jax.tools.jax_to_ir.(ir, debug_ir)->jax_to_ir(fn, input_shapes, constants=constants, format=FLAGS.ir_format)
A:jax.tools.jax_to_ir.match->re.compile(f"^({'|'.join(_DT)})\\[\\s*(\\d*[\\s*,\\d+]*)\\s*\\]$").match(s)
A:jax.tools.jax_to_ir.shape->tuple((int(d.strip()) for d in match.group(2).split(',')))
A:jax.tools.jax_to_ir._SHAPE_RE->re.compile(f"^({'|'.join(_DT)})\\[\\s*(\\d*[\\s*,\\d+]*)\\s*\\]$")
jax.tools.jax_to_ir.jax_to_ir(fn,input_shapes,*,constants=None,format)
jax.tools.jax_to_ir.main(argv)
jax.tools.jax_to_ir.parse_shape_str(s)
jax.tools.jax_to_ir.set_up_flags()
jax.tools.jax_to_ir.tf_wrap_with_input_names(f,input_shapes)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/nn/initializers.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/iree.py----------------------------------------
A:jax._src.iree.outputs->self.module_object[self.function_name](*inputs)
A:jax._src.iree.self.iree_config->iree.runtime.system_api.Config(self.runtime_driver)
A:jax._src.iree.iree_binary->iree.compiler.compile_str(computation, target_backends=[self.compiler_driver], input_type='mhlo', extra_args=extra_args)
A:jax._src.iree.vm_module->iree.runtime.VmModule.from_flatbuffer(self.iree_config.vm_instance, iree_binary)
A:jax._src.iree.module_object->iree.runtime.load_vm_module(vm_module, self.iree_config)
jax._src.iree.IreeBuffer(self,client,device,buffer)
jax._src.iree.IreeBuffer.__array__(self,dtype=None,context=None)
jax._src.iree.IreeBuffer.__init__(self,client,device,buffer)
jax._src.iree.IreeBuffer.__repr__(self)
jax._src.iree.IreeBuffer._value(self)
jax._src.iree.IreeBuffer.block_until_ready(self)->IreeBuffer
jax._src.iree.IreeBuffer.copy_to_device(self,device)
jax._src.iree.IreeBuffer.copy_to_host_async(self)
jax._src.iree.IreeBuffer.device(self)
jax._src.iree.IreeBuffer.platform(self)
jax._src.iree.IreeBuffer.to_iree(self)
jax._src.iree.IreeClient(self,*,iree_backend:Optional[str]=None)
jax._src.iree.IreeClient.__init__(self,*,iree_backend:Optional[str]=None)
jax._src.iree.IreeClient.buffer_from_pyval(self,argument:Any,device:Optional[IreeDevice],force_copy:bool=True,host_buffer_semantics:xla_client.HostBufferSemantics=xla_client.HostBufferSemantics.ZERO_COPY)->IreeBuffer
jax._src.iree.IreeClient.compile(self,computation:str,compile_options:xla_client.CompileOptions)->IreeExecutable
jax._src.iree.IreeClient.device_count(self)->int
jax._src.iree.IreeClient.devices(self)->List[IreeDevice]
jax._src.iree.IreeClient.get_default_device_assignment(self,num_replicas:int)->List[IreeDevice]
jax._src.iree.IreeClient.local_device_count(self)->int
jax._src.iree.IreeClient.local_devices(self)->List[IreeDevice]
jax._src.iree.IreeClient.process_index(self)->int
jax._src.iree.IreeDevice(self,client)
jax._src.iree.IreeDevice.__init__(self,client)
jax._src.iree.IreeDevice.__str__(self)->str
jax._src.iree.IreeDevice.live_buffers(self)->List[IreeBuffer]
jax._src.iree.IreeDevice.transfer_from_outfeed(self,shape:xla_client.Shape)
jax._src.iree.IreeDevice.transfer_to_infeed(self,literal:Any)
jax._src.iree.IreeExecutable(self,client,devices,module_object,function_name)
jax._src.iree.IreeExecutable.__init__(self,client,devices,module_object,function_name)
jax._src.iree.IreeExecutable.execute(self,arguments:Sequence[IreeBuffer])->List[IreeBuffer]
jax._src.iree.IreeExecutable.local_devices(self)->List[IreeDevice]
jax._src.iree.iree_client_factory()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/ad_checkpoint.py----------------------------------------
A:jax._src.ad_checkpoint.name_p->jax.core.Primitive('name')
A:jax._src.ad_checkpoint.names_not_to_save->frozenset(names_not_to_save)
A:jax._src.ad_checkpoint.names_which_can_be_saved->set(names_which_can_be_saved)
A:jax._src.ad_checkpoint.checkpoint_policies->types.SimpleNamespace(everything_saveable=everything_saveable, nothing_saveable=nothing_saveable, checkpoint_dots=checkpoint_dots, checkpoint_dots_with_no_batch_dims=dot_with_no_batch_dims, save_any_names_but_these=save_any_names_but_these, save_only_these_names=save_only_these_names, save_from_both_policies=save_from_both_policies)
A:jax._src.ad_checkpoint.(fun_, args)->_remat_static_argnums(fun, static_argnums, args)
A:jax._src.ad_checkpoint.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax._src.ad_checkpoint.(jaxpr, consts, out_tree)->_trace_to_jaxpr(fun_, in_tree, tuple(in_avals))
A:jax._src.ad_checkpoint.out_flat->jax.core.Primitive('remat2').bind(*consts, *args_flat, jaxpr=jaxpr, prevent_cse=prevent_cse, differentiated=False, policy=policy)
A:jax._src.ad_checkpoint.nargs->len(args)
A:jax._src.ad_checkpoint.static_argnums_->frozenset((d % len(args) for d in static_argnums))
A:jax._src.ad_checkpoint.new_fun->_dyn_args_fun(fun, static_argnums_, tuple(static_args), nargs)
A:jax._src.ad_checkpoint.self.hash->id(val)
A:jax._src.ad_checkpoint.debug->jax.interpreters.partial_eval.debug_info(fun, in_tree, True, 'checkpoint')
A:jax._src.ad_checkpoint.(flat_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax._src.ad_checkpoint.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals, debug)
A:jax._src.ad_checkpoint.new_e->jax.core.ConcretizationTypeError.__new__(core.ConcretizationTypeError)
A:jax._src.ad_checkpoint.(args, in_tree)->tree_flatten((args, kwargs))
A:jax._src.ad_checkpoint.(args, kwargs)->tree_unflatten(in_tree, args)
A:jax._src.ad_checkpoint.src->jax._src.source_info_util.summarize(eqn.source_info)
A:jax._src.ad_checkpoint.remat_p->jax.core.Primitive('remat2')
A:jax._src.ad_checkpoint.(jaxpr_jvp_, out_nz)->jax.interpreters.ad.jvp_jaxpr(pe.close_jaxpr(jaxpr), in_nonzeros, False)
A:jax._src.ad_checkpoint.jaxpr_jvp->jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_jvp_.jaxpr)
A:jax._src.ad_checkpoint.outs->jax.core.Primitive('remat2').bind(*jaxpr_jvp_.consts, *primals, *nonzero_tangents, jaxpr=jaxpr_jvp, prevent_cse=prevent_cse, differentiated=differentiated, policy=policy)
A:jax._src.ad_checkpoint.(out_primals, out_tangents_)->split_list(outs, [len(jaxpr.outvars)])
A:jax._src.ad_checkpoint.out_tangents_->iter(out_tangents_)
A:jax._src.ad_checkpoint.(jaxpr_known, jaxpr_staged, out_unknowns, out_inst, num_res)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr, in_unknowns, [True] * len(in_unknowns), False, False, policy)
A:jax._src.ad_checkpoint.(_, out_inst_unknown)->partition_list(out_inst, out_unknowns)
A:jax._src.ad_checkpoint.(jaxpr_unknown, in_used_staged)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr_staged, out_inst_unknown)
A:jax._src.ad_checkpoint.(used_res, in_used_staged)->split_list(in_used_staged, [num_res])
A:jax._src.ad_checkpoint.(jaxpr_known, in_used_known)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr_known, out_used_known)
A:jax._src.ad_checkpoint.num_res->sum(used_res)
A:jax._src.ad_checkpoint.(_, in_consts_)->unzip2((t.pval for t in tracers if t.pval.is_known()))
A:jax._src.ad_checkpoint.(_, in_consts)->partition_list(in_used_known, in_consts_)
A:jax._src.ad_checkpoint.out_consts->jax.core.eval_jaxpr(jaxpr_known, (), *in_consts)
A:jax._src.ad_checkpoint.(out_knowns, residuals)->split_list(out_consts, [len(out_consts) - num_res])
A:jax._src.ad_checkpoint.res_tracers->map(trace.new_instantiated_const, residuals)
A:jax._src.ad_checkpoint.(_, tracers_staged)->partition_list(in_used_staged, tracers)
A:jax._src.ad_checkpoint.new_params->dict(eqn.params, jaxpr=new_jaxpr)
A:jax._src.ad_checkpoint.recipe->jax.interpreters.partial_eval.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p, new_params, jaxpr_unknown.effects, source_info_util.current())
A:jax._src.ad_checkpoint.pe.partial_eval_jaxpr_custom_rules[remat_p]->partial(pe.call_partial_eval_custom_rule, 'jaxpr', remat_partial_eval_custom_params_updater)
A:jax._src.ad_checkpoint.(transposed_jaxpr_, in_zeros)->transpose_jaxpr(pe.close_jaxpr(jaxpr), in_linear, out_zeros, reduce_axes)
A:jax._src.ad_checkpoint.transposed_jaxpr->jax.core.ClosedJaxpr(transposed_jaxpr_, consts)
A:jax._src.ad_checkpoint.(args, _)->tree_flatten((in_primals, out_cts))
A:jax._src.ad_checkpoint.in_cts_nz->jax.core.Primitive('remat2').bind(*consts, *args, jaxpr=transposed_jaxpr, **params)
A:jax._src.ad_checkpoint.(ins_flat, out_cts_flat)->split_list(args_flat, [len(in_lin) - sum(in_lin)])
A:jax._src.ad_checkpoint.ins_iter->iter(ins_flat)
A:jax._src.ad_checkpoint.(lin_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_nounits(lu.wrap_init(core.jaxpr_as_fun(jaxpr)), in_pvals, False)
A:jax._src.ad_checkpoint.out_cts_iter->iter(out_cts_flat)
A:jax._src.ad_checkpoint.in_cts->jax.interpreters.ad.backward_pass(lin_jaxpr, reduce_axes, False, consts, dummy_args, out_cts)
A:jax._src.ad_checkpoint.(in_cts_nz, _)->partition_list(in_zeros, in_cts)
A:jax._src.ad_checkpoint.(transposed_jaxpr_, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(transposed, in_avals)
A:jax._src.ad_checkpoint.(jaxpr_batched_, out_batched)->jax.interpreters.batching.batch_jaxpr_axes(pe.close_jaxpr(jaxpr), axis_size, dims, [batching.zero_if_mapped] * len(jaxpr.outvars), axis_name=axis_name, main_type=main_type)
A:jax._src.ad_checkpoint.(new_jaxpr, used_inputs)->jax.interpreters.partial_eval.dce_jaxpr(eqn.params['jaxpr'], used_outputs)
A:jax._src.ad_checkpoint.new_eqn->jax.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax._src.ad_checkpoint.args->_optimization_barrier(args)
A:jax._src.ad_checkpoint.avals_out->tuple((v.aval for v in jaxpr.outvars))
A:jax._src.ad_checkpoint.unif->jax.lax.rng_uniform(np.float32(0), np.float32(1), shape=())
A:jax._src.ad_checkpoint.results->jax.core.eval_jaxpr(jaxpr, (), *args)
A:jax._src.ad_checkpoint.carry_res->jax.lax.while_loop(cond, body, carry_init)
A:jax._src.ad_checkpoint.barrier_types->map(mlir.aval_to_ir_types, ctx.avals_in)
A:jax._src.ad_checkpoint.flat_barrier_types->jax._src.util.flatten(barrier_types)
A:jax._src.ad_checkpoint.flat_args->jax.interpreters.mlir.flatten_lowering_ir_args(args)
A:jax._src.ad_checkpoint.barrier_op->jax._src.lib.mlir.dialects.mhlo.OptimizationBarrierOp(flat_barrier_types, flat_args)
A:jax._src.ad_checkpoint.(flat_args, treedef)->tree_flatten(arg)
A:jax._src.ad_checkpoint.optimization_barrier_p->jax.core.Primitive('optimization_barrier')
jax._src.ad_checkpoint.WrapHashably(self,val)
jax._src.ad_checkpoint.WrapHashably.__eq__(self,other)
jax._src.ad_checkpoint.WrapHashably.__hash__(self)
jax._src.ad_checkpoint.WrapHashably.__init__(self,val)
jax._src.ad_checkpoint._dummy_like(aval:core.AbstractValue)->Any
jax._src.ad_checkpoint._dyn_args_fun(fun:Callable,static_argnums:FrozenSet[int],static_args:Tuple[WrapHashably,...],nargs:int)
jax._src.ad_checkpoint._optimization_barrier(arg)
jax._src.ad_checkpoint._optimization_barrier_abstract_eval(*args)
jax._src.ad_checkpoint._optimization_barrier_lowering_rule(ctx,*args)
jax._src.ad_checkpoint._remat_static_argnums(fun,static_argnums,args)
jax._src.ad_checkpoint._remat_translation_using_cond(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._remat_translation_using_opt_barrier(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._remat_translation_using_while(*args,jaxpr:core.Jaxpr)
jax._src.ad_checkpoint._trace_to_jaxpr(fun,in_tree,in_avals)
jax._src.ad_checkpoint._transpose_jaxpr(jaxpr,in_lin,out_zeros,reduce_axes)
jax._src.ad_checkpoint.checkpoint(fun:Callable,*,prevent_cse:bool=True,policy:Optional[Callable[...,bool]]=None,static_argnums:Union[int,Tuple[int,...]]=())->Callable
jax._src.ad_checkpoint.checkpoint_dots(prim,*_,**__)->bool
jax._src.ad_checkpoint.checkpoint_name(x,name)
jax._src.ad_checkpoint.dot_with_no_batch_dims(prim,*_,**params)->bool
jax._src.ad_checkpoint.everything_saveable(*_,**__)->bool
jax._src.ad_checkpoint.name_batcher(args,dims,*,name)
jax._src.ad_checkpoint.name_jvp(primals,tangents,*,name)
jax._src.ad_checkpoint.nothing_saveable(*_,**__)->bool
jax._src.ad_checkpoint.print_saved_residuals(f,*args,**kwargs)
jax._src.ad_checkpoint.remat_abstract_eval(*args,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_dce(used_outputs:List[bool],eqn:core.JaxprEqn)->Tuple[List[bool], Optional[core.JaxprEqn]]
jax._src.ad_checkpoint.remat_impl(*args,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_jvp(primals,tangents,jaxpr,prevent_cse,differentiated,policy)
jax._src.ad_checkpoint.remat_lowering(*args,jaxpr:core.Jaxpr,prevent_cse:bool,differentiated:bool,is_gpu_platform:bool=False,**_)
jax._src.ad_checkpoint.remat_partial_eval(trace,*tracers,jaxpr,**params)
jax._src.ad_checkpoint.remat_partial_eval_custom_params_updater(*args)
jax._src.ad_checkpoint.remat_transpose(reduce_axes,out_cts,*in_primals,jaxpr,**params)
jax._src.ad_checkpoint.remat_vmap(axis_size,axis_name,main_type,args,dims,*,jaxpr,**params)
jax._src.ad_checkpoint.save_any_names_but_these(*names_not_to_save)
jax._src.ad_checkpoint.save_from_both_policies(policy_1,policy_2)
jax._src.ad_checkpoint.save_only_these_names(*names_which_can_be_saved)
jax._src.ad_checkpoint.saved_residuals(f,*args,**kwargs)->List[Tuple[core.AbstractValue, str]]
jax._src.ad_checkpoint.transpose_jaxpr(jaxpr:core.ClosedJaxpr,in_linear:Union[bool,Sequence[bool]],out_zeros:Union[bool,Sequence[bool]],reduce_axes:Sequence[core.AxisName])->Tuple[core.ClosedJaxpr, List[bool]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/stages.py----------------------------------------
A:jax._src.stages.xla_ext_exe->self.xla_extension_executable()
A:jax._src.stages.donate_argnums->frozenset(donate_argnums)
A:jax._src.stages.(flat_avals, _)->jax.tree_util.tree_flatten(in_avals)
A:jax._src.stages.exe->self.runtime_executable()
A:jax._src.stages.shardings_flat->self._executable.output_shardings()
A:jax._src.stages.kws->', '.join(kwargs.keys())
A:jax._src.stages.(args_flat, in_tree)->jax.tree_util.tree_flatten((args, kwargs))
A:jax._src.stages.out_flat->self._executable.call(*args_flat)
A:jax._src.stages.kw->dict(_allow_propagation_to_outputs=True)
jax._src.stages.ArgInfo
jax._src.stages.Compiled(self,executable,args_info,out_tree,no_kwargs=False)
jax._src.stages.Compiled.__init__(self,executable,args_info,out_tree,no_kwargs=False)
jax._src.stages.Compiled.as_text(self)->Optional[str]
jax._src.stages.Compiled.compiler_ir(self)
jax._src.stages.Compiled.cost_analysis(self)->Optional[Any]
jax._src.stages.Compiled.input_shardings(self)
jax._src.stages.Compiled.memory_analysis(self)->Optional[Any]
jax._src.stages.Compiled.output_shardings(self)
jax._src.stages.Compiled.runtime_executable(self)->Optional[Any]
jax._src.stages.Executable(Protocol)
jax._src.stages.Executable.as_text(self)->str
jax._src.stages.Executable.call(self,*args_flat)->Sequence[Any]
jax._src.stages.Executable.cost_analysis(self)->Any
jax._src.stages.Executable.input_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.Executable.memory_analysis(self)->Any
jax._src.stages.Executable.output_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.Executable.runtime_executable(self)->Any
jax._src.stages.Lowered(self,lowering:XlaLowering,args_info,out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowered.__init__(self,lowering:XlaLowering,args_info,out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowered.as_text(self,dialect:Optional[str]=None)->str
jax._src.stages.Lowered.compile(self)->Compiled
jax._src.stages.Lowered.compiler_ir(self,dialect:Optional[str]=None)->Optional[Any]
jax._src.stages.Lowered.from_flat_info(cls,lowering:XlaLowering,in_tree:tree_util.PyTreeDef,in_avals,donate_argnums:Tuple[int,...],out_tree:tree_util.PyTreeDef,no_kwargs:bool=False)
jax._src.stages.Lowering(Protocol)
jax._src.stages.Lowering.as_text(self,dialect:Optional[str]=None)->str
jax._src.stages.Lowering.compile(self)->Executable
jax._src.stages.Lowering.compiler_ir(self,dialect:Optional[str]=None)->Any
jax._src.stages.Stage
jax._src.stages.Stage.donate_argnums(self)
jax._src.stages.Stage.in_avals(self)
jax._src.stages.Stage.in_tree(self)->tree_util.PyTreeDef
jax._src.stages.Wrapped(self,*args,**kwargs)
jax._src.stages.Wrapped.__call__(self,*args,**kwargs)
jax._src.stages.Wrapped.lower(self,*args,**kwargs)->Lowered
jax._src.stages.XlaExecutable(Executable)
jax._src.stages.XlaExecutable.as_text(self)->str
jax._src.stages.XlaExecutable.call(self,*args_flat)->Sequence[Any]
jax._src.stages.XlaExecutable.cost_analysis(self)->List[Dict[str, float]]
jax._src.stages.XlaExecutable.input_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.XlaExecutable.memory_analysis(self)->Any
jax._src.stages.XlaExecutable.output_shardings(self)->Sequence[jax.sharding.XLACompatibleSharding]
jax._src.stages.XlaExecutable.runtime_executable(self)->Any
jax._src.stages.XlaExecutable.xla_extension_executable(self)->xla.XlaLoadedExecutable
jax._src.stages.XlaLowering(Lowering)
jax._src.stages.XlaLowering.as_text(self,dialect:Optional[str]=None)->str
jax._src.stages.XlaLowering.compile(self)->Executable
jax._src.stages.XlaLowering.compiler_ir(self,dialect:Optional[str]=None)->Any
jax._src.stages.XlaLowering.hlo(self)->xc.XlaComputation
jax._src.stages.XlaLowering.mhlo(self)->mlir.ir.Module
jax._src.stages.make_args_info(in_tree,in_avals,donate_argnums)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/basearray.py----------------------------------------
jax.Array(abc.ABC)
jax.Array.at(self)
jax._src.basearray.Array(abc.ABC)
jax._src.basearray.Array.at(self)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/basearray.pyi----------------------------------------
jax.Array.T(self)->Array
jax.Array.__abs__(self)->Array
jax.Array.__add__(self,other)->Array
jax.Array.__and__(self,other)->Array
jax.Array.__array__(self)->np.ndarray
jax.Array.__bool__(self)->bool
jax.Array.__complex__(self)->complex
jax.Array.__divmod__(self,other)->Array
jax.Array.__dlpack__(self)->Any
jax.Array.__eq__(self,other)->Array
jax.Array.__float__(self)->float
jax.Array.__floordiv__(self,other)->Array
jax.Array.__ge__(self,other)->Array
jax.Array.__getitem__(self,key,indices_are_sorted=False,unique_indices=False)->Array
jax.Array.__gt__(self,other)->Array
jax.Array.__index__(self)->int
jax.Array.__int__(self)->int
jax.Array.__invert__(self)->Array
jax.Array.__iter__(self)->Any
jax.Array.__le__(self,other)->Array
jax.Array.__len__(self)->int
jax.Array.__lshift__(self,other)->Array
jax.Array.__lt__(self,other)->Array
jax.Array.__matmul__(self,other)->Array
jax.Array.__mod__(self,other)->Array
jax.Array.__mul__(self,other)->Array
jax.Array.__ne__(self,other)->Array
jax.Array.__neg__(self)->Array
jax.Array.__or__(self,other)->Array
jax.Array.__pos__(self)->Array
jax.Array.__pow__(self,other)->Array
jax.Array.__radd__(self,other)->Array
jax.Array.__rand__(self,other)->Array
jax.Array.__rdivmod__(self,other)->Array
jax.Array.__reversed__(self)->Any
jax.Array.__rfloordiv__(self,other)->Array
jax.Array.__rlshift__(self,other)->Array
jax.Array.__rmatmul__(self,other)->Array
jax.Array.__rmod__(self,other)->Array
jax.Array.__rmul__(self,other)->Array
jax.Array.__ror__(self,other)->Array
jax.Array.__round__(self,ndigits=None)->Array
jax.Array.__rpow__(self,other)->Array
jax.Array.__rrshift__(self,other)->Array
jax.Array.__rshift__(self,other)->Array
jax.Array.__rsub__(self,other)->Array
jax.Array.__rtruediv__(self,other)->Array
jax.Array.__rxor__(self,other)->Array
jax.Array.__setitem__(self,key,value)->None
jax.Array.__sub__(self,other)->Array
jax.Array.__truediv__(self,other)->Array
jax.Array.__xor__(self,other)->Array
jax.Array.all(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None)->Array
jax.Array.any(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None)->Array
jax.Array.argmax(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax.Array.argmin(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax.Array.argpartition(self,kth,axis=-1,kind='introselect',order=None)->Array
jax.Array.argsort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax.Array.astype(self,dtype)->Array
jax.Array.choose(self,choices,out=None,mode='raise')->Array
jax.Array.clip(self,min=None,max=None,out=None)->Array
jax.Array.compress(self,condition,axis:Optional[int]=None,out=None)->Array
jax.Array.conj(self)->Array
jax.Array.conjugate(self)->Array
jax.Array.copy(self)->Array
jax.Array.cumprod(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None)->Array
jax.Array.cumsum(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None)->Array
jax.Array.diagonal(self,offset=0,axis1:int=0,axis2:int=1)->Array
jax.Array.dot(self,b,*,precision=None)->Array
jax.Array.flatten(self)->Array
jax.Array.imag(self)->Array
jax.Array.item(self,*args)->Any
jax.Array.max(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.mean(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=False,*,where=None)->Array
jax.Array.min(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.nbytes(self)->int
jax.Array.nonzero(self,*,size=None,fill_value=None)->Array
jax.Array.prod(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.ptp(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=False)->Array
jax.Array.ravel(self,order='C')->Array
jax.Array.real(self)->Array
jax.Array.repeat(self,repeats,axis:Optional[int]=None,*,total_repeat_length=None)->Array
jax.Array.reshape(self,*args,order='C')->Array
jax.Array.round(self,decimals=0,out=None)->Array
jax.Array.searchsorted(self,v,side='left',sorter=None)->Array
jax.Array.shape(self)->Tuple[int, ...]
jax.Array.sort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax.Array.squeeze(self,axis:Optional[Union[int,Tuple[int,...]]]=None)->Array
jax.Array.std(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax.Array.sum(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax.Array.swapaxes(self,axis1:int,axis2:int)->Array
jax.Array.take(self,indices,axis:Optional[int]=None,out=None,mode=None)->Array
jax.Array.tobytes(self,order='C')->bytes
jax.Array.tolist(self)->List[Any]
jax.Array.trace(self,offset=0,axis1:int=0,axis2:int=1,dtype=None,out=None)->Array
jax.Array.transpose(self,*args)->Array
jax.Array.var(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax.Array.view(self,dtype=None,type=None)->Array
jax.Array.weak_type(self)->bool
jax._src.basearray.Array.T(self)->Array
jax._src.basearray.Array.__abs__(self)->Array
jax._src.basearray.Array.__add__(self,other)->Array
jax._src.basearray.Array.__and__(self,other)->Array
jax._src.basearray.Array.__array__(self)->np.ndarray
jax._src.basearray.Array.__bool__(self)->bool
jax._src.basearray.Array.__complex__(self)->complex
jax._src.basearray.Array.__divmod__(self,other)->Array
jax._src.basearray.Array.__dlpack__(self)->Any
jax._src.basearray.Array.__eq__(self,other)->Array
jax._src.basearray.Array.__float__(self)->float
jax._src.basearray.Array.__floordiv__(self,other)->Array
jax._src.basearray.Array.__ge__(self,other)->Array
jax._src.basearray.Array.__getitem__(self,key,indices_are_sorted=False,unique_indices=False)->Array
jax._src.basearray.Array.__gt__(self,other)->Array
jax._src.basearray.Array.__index__(self)->int
jax._src.basearray.Array.__init__(self,shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax._src.basearray.Array.__int__(self)->int
jax._src.basearray.Array.__invert__(self)->Array
jax._src.basearray.Array.__iter__(self)->Any
jax._src.basearray.Array.__le__(self,other)->Array
jax._src.basearray.Array.__len__(self)->int
jax._src.basearray.Array.__lshift__(self,other)->Array
jax._src.basearray.Array.__lt__(self,other)->Array
jax._src.basearray.Array.__matmul__(self,other)->Array
jax._src.basearray.Array.__mod__(self,other)->Array
jax._src.basearray.Array.__mul__(self,other)->Array
jax._src.basearray.Array.__ne__(self,other)->Array
jax._src.basearray.Array.__neg__(self)->Array
jax._src.basearray.Array.__or__(self,other)->Array
jax._src.basearray.Array.__pos__(self)->Array
jax._src.basearray.Array.__pow__(self,other)->Array
jax._src.basearray.Array.__radd__(self,other)->Array
jax._src.basearray.Array.__rand__(self,other)->Array
jax._src.basearray.Array.__rdivmod__(self,other)->Array
jax._src.basearray.Array.__reversed__(self)->Any
jax._src.basearray.Array.__rfloordiv__(self,other)->Array
jax._src.basearray.Array.__rlshift__(self,other)->Array
jax._src.basearray.Array.__rmatmul__(self,other)->Array
jax._src.basearray.Array.__rmod__(self,other)->Array
jax._src.basearray.Array.__rmul__(self,other)->Array
jax._src.basearray.Array.__ror__(self,other)->Array
jax._src.basearray.Array.__round__(self,ndigits=None)->Array
jax._src.basearray.Array.__rpow__(self,other)->Array
jax._src.basearray.Array.__rrshift__(self,other)->Array
jax._src.basearray.Array.__rshift__(self,other)->Array
jax._src.basearray.Array.__rsub__(self,other)->Array
jax._src.basearray.Array.__rtruediv__(self,other)->Array
jax._src.basearray.Array.__rxor__(self,other)->Array
jax._src.basearray.Array.__setitem__(self,key,value)->None
jax._src.basearray.Array.__sub__(self,other)->Array
jax._src.basearray.Array.__truediv__(self,other)->Array
jax._src.basearray.Array.__xor__(self,other)->Array
jax._src.basearray.Array.all(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.any(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argmax(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argmin(self,axis:Optional[int]=None,out=None,keepdims=None)->Array
jax._src.basearray.Array.argpartition(self,kth,axis=-1,kind='introselect',order=None)->Array
jax._src.basearray.Array.argsort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax._src.basearray.Array.astype(self,dtype)->Array
jax._src.basearray.Array.choose(self,choices,out=None,mode='raise')->Array
jax._src.basearray.Array.clip(self,min=None,max=None,out=None)->Array
jax._src.basearray.Array.compress(self,condition,axis:Optional[int]=None,out=None)->Array
jax._src.basearray.Array.conj(self)->Array
jax._src.basearray.Array.conjugate(self)->Array
jax._src.basearray.Array.copy(self)->Array
jax._src.basearray.Array.cumprod(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None)->Array
jax._src.basearray.Array.cumsum(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None)->Array
jax._src.basearray.Array.diagonal(self,offset=0,axis1:int=0,axis2:int=1)->Array
jax._src.basearray.Array.dot(self,b,*,precision=None)->Array
jax._src.basearray.Array.flatten(self)->Array
jax._src.basearray.Array.imag(self)->Array
jax._src.basearray.Array.item(self,*args)->Any
jax._src.basearray.Array.max(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.mean(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.min(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.nbytes(self)->int
jax._src.basearray.Array.nonzero(self,*,size=None,fill_value=None)->Array
jax._src.basearray.Array.prod(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.ptp(self,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=False)->Array
jax._src.basearray.Array.ravel(self,order='C')->Array
jax._src.basearray.Array.real(self)->Array
jax._src.basearray.Array.repeat(self,repeats,axis:Optional[int]=None,*,total_repeat_length=None)->Array
jax._src.basearray.Array.reshape(self,*args,order='C')->Array
jax._src.basearray.Array.round(self,decimals=0,out=None)->Array
jax._src.basearray.Array.searchsorted(self,v,side='left',sorter=None)->Array
jax._src.basearray.Array.shape(self)->Tuple[int, ...]
jax._src.basearray.Array.sort(self,axis:Optional[int]=-1,kind='quicksort',order=None)->Array
jax._src.basearray.Array.squeeze(self,axis:Optional[Union[int,Tuple[int,...]]]=None)->Array
jax._src.basearray.Array.std(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.sum(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)->Array
jax._src.basearray.Array.swapaxes(self,axis1:int,axis2:int)->Array
jax._src.basearray.Array.take(self,indices,axis:Optional[int]=None,out=None,mode=None)->Array
jax._src.basearray.Array.tobytes(self,order='C')->bytes
jax._src.basearray.Array.tolist(self)->List[Any]
jax._src.basearray.Array.trace(self,offset=0,axis1:int=0,axis2:int=1,dtype=None,out=None)->Array
jax._src.basearray.Array.transpose(self,*args)->Array
jax._src.basearray.Array.var(self,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)->Array
jax._src.basearray.Array.view(self,dtype=None,type=None)->Array
jax._src.basearray.Array.weak_type(self)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/api_util.py----------------------------------------
A:jax._src.api_util.x->jax.core.concrete_or_error(None, x, 'expected a static index or sequence of indices.')
A:jax._src.api_util.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax._src.api_util.(args, in_tree)->tree_flatten(py_args)
A:jax._src.api_util.ans->fun(*args)
A:jax._src.api_util.py_args->tree_unflatten(in_tree, args_flat)
A:jax._src.api_util.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax._src.api_util.(aux_flat, aux_tree)->tree_flatten(aux)
A:jax._src.api_util.dyn_argnums->tuple((i for i in range(len(args)) if i not in static_argnums))
A:jax._src.api_util.dyn_args->tuple((args[i] for i in dyn_argnums))
A:jax._src.api_util.static_argnums->sorted(set(static_argnums))
A:jax._src.api_util.sentinel->object()
A:jax._src.api_util.fixed_args_->iter(fixed_args)
A:jax._src.api_util.fixed_kwargs[k]->Hashable(arg)
A:jax._src.api_util.kwargs->dict({k: v.val for (k, v) in fixed_kwargs.val.items()}, **dyn_kwargs)
A:jax._src.api_util.donate->bool(i in donate_argnums)
A:jax._src.api_util.donate_argnums->sorted(set(donate_argnums))
A:jax._src.api_util.proxy->object()
A:jax._src.api_util.dummy->tree_unflatten(treedef, [object()] * treedef.num_leaves)
A:jax._src.api_util.(treedef, leaf)->treedef_children(treedef)
A:jax._src.api_util.weak_type->getattr(x, 'weak_type', False)
A:jax._src.api_util.named_shape->getattr(x, 'named_shape', {})
A:jax._src.api_util.dtype->jax._src.dtypes.result_type(x)
jax._src.api_util._HashableWithStrictTypeEquality(self,val)
jax._src.api_util._HashableWithStrictTypeEquality.__eq__(self,other)
jax._src.api_util._HashableWithStrictTypeEquality.__hash__(self)
jax._src.api_util._HashableWithStrictTypeEquality.__init__(self,val)
jax._src.api_util._argnames_partial(fixed_kwargs:WrapKwArgs,*args,**dyn_kwargs)
jax._src.api_util._argnums_partial(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax._src.api_util._dtype(x)
jax._src.api_util._ensure_inbounds(allow_invalid:bool,num_args:int,argnums:Sequence[int])->Tuple[int, ...]
jax._src.api_util._ensure_index(x:Any)->Union[int, Tuple[int, ...]]
jax._src.api_util._ensure_index_tuple(x:Any)->Tuple[int, ...]
jax._src.api_util._ensure_str(x:str)->str
jax._src.api_util._ensure_str_tuple(x:Union[str,Iterable[str]])->Tuple[str, ...]
jax._src.api_util._shaped_abstractify_slow(x)
jax._src.api_util.api_hook(fun,tag:str)
jax._src.api_util.apply_flat_fun(fun,io_tree,*py_args)
jax._src.api_util.apply_flat_fun_nokwargs(fun,io_tree,py_args)
jax._src.api_util.argnames_partial_except(f:lu.WrappedFun,static_argnames:Tuple[str,...],kwargs:Dict[str,Any])
jax._src.api_util.argnums_partial(f,dyn_argnums,args,require_static_args_hashable=True)
jax._src.api_util.argnums_partial_except(f:lu.WrappedFun,static_argnums:Tuple[int,...],args:Tuple[Any,...],*,allow_invalid:bool)
jax._src.api_util.donation_vector(donate_argnums,args,kwargs)->Tuple[bool, ...]
jax._src.api_util.flatten_axes(name,treedef,axis_tree,*,kws=False,tupled_args=False)
jax._src.api_util.flatten_fun(in_tree,*args_flat)
jax._src.api_util.flatten_fun_nokwargs(in_tree,*args_flat)
jax._src.api_util.flatten_fun_nokwargs2(in_tree,*args_flat)
jax._src.api_util.flattened_fun_in_tree(fn:lu.WrappedFun)->Optional[Tuple[PyTreeDef, bool]]
jax._src.api_util.is_hashable(arg)
jax._src.api_util.rebase_donate_argnums(donate_argnums,static_argnums)->Tuple[int, ...]
jax._src.api_util.shaped_abstractify(x)
jax._src.api_util.validate_argnames(sig:inspect.Signature,argnames:Tuple[str,...],argnames_name:str)->None
jax._src.api_util.validate_argnums(sig:inspect.Signature,argnums:Tuple[int,...],argnums_name:str)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/environment_info.py----------------------------------------
A:jax._src.environment_info.python_version->sys.version.replace('\n', ' ')
A:jax._src.environment_info.devices_short->str(np.array(jax.devices())).replace('\n', '')
A:jax._src.environment_info.info->textwrap.dedent(f'  jax:    {jax.__version__}\n  jaxlib: {lib.version_str}\n  numpy:  {np.__version__}\n  python: {python_version}\n  jax.devices ({jax.device_count()} total, {jax.local_device_count()} local): {devices_short}\n  process_count: {jax.process_count()}')
A:jax._src.environment_info.nvidia_smi->try_nvidia_smi()
jax._src.environment_info.print_environment_info(return_string:bool=False)->Union[None, str]
jax._src.environment_info.try_nvidia_smi()->Optional[str]
jax.print_environment_info(return_string:bool=False)->Union[None, str]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/device_array.py----------------------------------------
A:jax._src.device_array._forward_to_value->partial(_forward_method, '_value')
A:jax._src.device_array.device_buffer->device_buffer.clone().clone()
A:jax._src.device_array.type_x->type(x)
A:jax._src.device_array.self._npy_value->numpy.asarray(self.device_buffer)
A:jax._src.device_array.prefix->'{}('.format(self.__class__.__name__.lstrip('_'))
A:jax._src.device_array.s->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width)
A:jax._src.device_array.(fun, args, arr_state)->self._value.__reduce__()
A:jax._src.device_array.np_value->fun(*args)
A:jax._src.device_array.jnp_value->jax.device_put(np_value)
A:jax._src.device_array.jnp_value.aval->jax.device_put(np_value).aval.update(**aval_state)
A:jax._src.device_array.deleted_buffer->DeletedBuffer()
jax._src.device_array.DeletedBuffer(object)
jax._src.device_array._DeviceArray(self,aval:core.ShapedArray,device:Optional[Device],device_buffer:Buffer)
jax._src.device_array._DeviceArray.__cuda_array_interface__(self)
jax._src.device_array._DeviceArray.__init__(self,aval:core.ShapedArray,device:Optional[Device],device_buffer:Buffer)
jax._src.device_array._DeviceArray._check_if_deleted(self)
jax._src.device_array._DeviceArray._value(self)
jax._src.device_array._DeviceArray.block_until_ready(self)
jax._src.device_array._DeviceArray.copy_to_host_async(self)
jax._src.device_array._DeviceArray.delete(self)
jax._src.device_array._DeviceArray.device(self)
jax._src.device_array._DeviceArray.dtype(self)
jax._src.device_array._DeviceArray.ndim(self)
jax._src.device_array._DeviceArray.shape(self)
jax._src.device_array._DeviceArray.size(self)
jax._src.device_array._DeviceArray.unsafe_buffer_pointer(self)
jax._src.device_array._forward_method(attrname,self,fun,*args)
jax._src.device_array.device_array_supports_weakrefs()
jax._src.device_array.make_device_array(aval:core.ShapedArray,device:Optional[Device],device_buffer:Buffer)->Union[Buffer, '_DeviceArray']
jax._src.device_array.reconstruct_device_array(fun,args,arr_state,aval_state)
jax._src.device_array.type_is_device_array(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugging.py----------------------------------------
A:jax._src.debugging.DebugEffect->enum.Enum('DebugEffect', ['PRINT', 'ORDERED_PRINT'])
A:jax._src.debugging.debug_callback_p->jax.core.Primitive('debug_callback')
A:jax._src.debugging.axis_size->next((x.shape[i] for (x, i) in zip(args, dims) if i is not None))
A:jax._src.debugging.args_idx->map(functools.partial(get_arg_at_dim, i), dims, args)
A:jax._src.debugging.sharding->jax._src.lib.xla_client.OpSharding()
A:jax._src.debugging.(result, token, keepalive)->jax.interpreters.mlir.emit_python_callback(ctx, _callback, None, list(args), ctx.avals_in, ctx.avals_out, True, sharding=sharding)
A:jax._src.debugging.(flat_args, in_tree)->jax.tree_util.tree_flatten((args, kwargs))
A:jax._src.debugging.(args, kwargs)->jax.tree_util.tree_unflatten(in_tree, flat_args)
A:jax._src.debugging.formatter->_DebugPrintFormatChecker()
A:jax._src.debugging.inspect_sharding_p->jax.core.Primitive('inspect_sharding')
A:jax._src.debugging.sharding_callbacks->weakref.WeakValueDictionary()
A:jax._src.debugging.devices->list(axis_context.mesh.devices.flat)
A:jax._src.debugging.pspec->jax.experimental.pjit.parse_flatten_op_sharding(op_sharding, mesh)[0].get_partition_spec()
A:jax._src.debugging.trivial_sharding->jax._src.lib.xla_client.OpSharding()
A:jax._src.debugging.op_sharding->hlo_sharding.to_proto()
A:jax._src.debugging.sharding_callback_info->ShardingCallbackInfo(_hlo_sharding_callback, ctx.module_context)
A:jax._src.debugging.key->str(id(sharding_callback_info))
A:jax._src.debugging.fun->jax.linear_util.wrap_init(lambda *args: [])
A:jax._src.debugging.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_avals)
A:jax._src.debugging.closed_jaxpr->jax.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.debugging.trivial_comp->jax.interpreters.mlir.build_xla_computation_helper(closed_jaxpr, name='tmp_xla_computation', platform=module_context.platform, backend_or_name=module_context.backend_or_name, axis_context=module_context.axis_context)
A:jax._src.debugging.base_height->int(10 * scale)
A:jax._src.debugging.base_width->int(base_height * aspect_ratio)
A:jax._src.debugging.device_kind->next(iter(sharding.device_set)).platform.upper()
A:jax._src.debugging.device_indices_map->jax._src.lib.xla_client.OpSharding().devices_indices_map(tuple(shape))
A:jax._src.debugging.slcs->tuple(map(_raise_to_slice, slcs))
A:jax._src.debugging.chunk_idxs->tuple(map(_slice_to_chunk_idx, shape, slcs))
A:jax._src.debugging.heights[chunk_idxs]->int(chunk_height)
A:jax._src.debugging.widths[chunk_idxs]->int(chunk_width)
A:jax._src.debugging.vert->slice(0, 1, None)
A:jax._src.debugging.widths[(0, *chunk_idxs)]->int(horiz_size / shape[0] * base_width)
A:jax._src.debugging.table->rich.table.Table(show_header=False, show_lines=True, padding=0, highlight=True, pad_edge=False, box=rich.box.SQUARE)
A:jax._src.debugging.console->rich.console.Console(width=max_width)
A:jax._src.debugging.width->min(max(width, min_width), max_width)
A:jax._src.debugging.(left_padding, remainder)->divmod(width - len(entry) - 2, 2)
A:jax._src.debugging.(top_padding, remainder)->divmod(height - 2, 2)
A:jax._src.debugging.padding->tuple((max(x, 0) for x in padding))
jax._src.debugging.ShardingCallbackInfo(self,callback,module_context)
jax._src.debugging.ShardingCallbackInfo.__init__(self,callback,module_context)
jax._src.debugging._DebugPrintFormatChecker(string.Formatter)
jax._src.debugging._DebugPrintFormatChecker.check_unused_args(self,used_args,args,kwargs)
jax._src.debugging._debug_callback_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.debugging._format_print_callback(fmt:str,*args,**kwargs)
jax._src.debugging._inspect_sharding_abstract_eval(aval,**_)
jax._src.debugging._inspect_sharding_batching_rule(args,_,*,callback)
jax._src.debugging._inspect_sharding_impl(value,*,callback)
jax._src.debugging._inspect_sharding_jvp_rule(primals,_,**params)
jax._src.debugging._inspect_sharding_lowering_rule(ctx:mlir.LoweringRuleContext,value,*,callback)
jax._src.debugging._raise_to_slice(slc:Union[slice,int])
jax._src.debugging._slice_to_chunk_idx(size:int,slc:slice)->int
jax._src.debugging.debug_callback(callback:Callable[...,Any],*args:Any,ordered:bool=False,**kwargs:Any)
jax._src.debugging.debug_callback_abstract_eval(*flat_avals,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_callback_batching_rule(args,dims,**params)
jax._src.debugging.debug_callback_impl(*args,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_callback_jvp_rule(primals,tangents,**params)
jax._src.debugging.debug_callback_lowering(ctx,*args,effect,callback,**params)
jax._src.debugging.debug_callback_transpose_rule(*flat_args,callback:Callable[...,Any],effect:DebugEffect)
jax._src.debugging.debug_print(fmt:str,*args,ordered:bool=False,**kwargs)->None
jax._src.debugging.inspect_array_sharding(value,*,callback:Callable[[Sharding],None])
jax._src.debugging.inspect_sharding_infer_sharding_from_operands(arg_shapes,arg_shardings,shape,backend_string)
jax._src.debugging.inspect_sharding_partition(shapes,arg_shardings,result_shape,result_sharding,backend_string)
jax._src.debugging.inspect_sharding_prop_user_sharding(sharding,backend_string)
jax._src.debugging.visualize_array_sharding(arr,**kwargs)
jax._src.debugging.visualize_sharding(shape:Sequence[int],sharding:Sharding,*,use_color:bool=False,scale:float=1.0,min_width:int=9,max_width:int=80)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/dispatch.py----------------------------------------
A:jax._src.dispatch.aval->jax.core.ShapedArray(tuple(shape), buf.dtype)
A:jax._src.dispatch.compiled_fun->xla_callable(fun, device, backend, name, donated_invars, keep_unused, *arg_specs)
A:jax._src.dispatch.old_token.aval->jax.core.ShapedArray((0,), np.bool_)
A:jax._src.dispatch.(_, arg_devices)->jax._src.util.unzip2(arg_specs)
A:jax._src.dispatch.device->_xla_callable_device(nreps, backend, device, arg_devices)
A:jax._src.dispatch.out->prim.bind(*args, **params)
A:jax._src.dispatch.compiled->compile_or_get_cached(backend, xla_computation, options, host_callbacks)
A:jax._src.dispatch.arg_specs->unsafe_map(arg_spec, args)
A:jax._src.dispatch.clone->jax.linear_util.WrappedFun(fun.f, fun.transforms, stores, fun.params, fun.in_type)
A:jax._src.dispatch._->jax.linear_util.WrappedFun(fun.f, fun.transforms, stores, fun.params, fun.in_type).call_wrapped(*args)
A:jax._src.dispatch.fun_info->jax.interpreters.partial_eval.fun_sourceinfo(fun.f)
A:jax._src.dispatch.(in_avals, in_shardings)->jax._src.util.unzip2(arg_specs)
A:jax._src.dispatch.(da, in_shardings)->not_none_device_or_backend_on_jit(backend, device, len(in_shardings))
A:jax._src.dispatch.computation->sharded_lowering(fun, device, backend, name, donated_invars, False, keep_unused, *arg_specs)
A:jax._src.dispatch.xla_callable->jax.linear_util.cache(_xla_callable_uncached)
A:jax._src.dispatch.start_time->time.time()
A:jax._src.dispatch.(abstract_args, arg_devices)->jax._src.util.unzip2([a for (i, a) in enumerate(arg_specs) if i in kept_var_idx])
A:jax._src.dispatch.in_type->tuple(unsafe_zip(abstract_args, itertools.repeat(True)))
A:jax._src.dispatch.fun->jax.linear_util.annotate(fun, in_type)
A:jax._src.dispatch.(jaxpr, out_type, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_final2(fun, pe.debug_info_final(fun, 'jit'))
A:jax._src.dispatch.(out_avals, kept_outputs)->jax._src.util.unzip2(out_type)
A:jax._src.dispatch.has_outfeed->jax.core.jaxpr_uses_outfeed(jaxpr)
A:jax._src.dispatch.jaxpr->apply_outfeed_rewriter(jaxpr)
A:jax._src.dispatch.(jaxpr, kept_const_idx, kept_var_idx)->_prune_unused_inputs(jaxpr)
A:jax._src.dispatch.kept_var_idx->set(range(len(fun.in_type)))
A:jax._src.dispatch.nreps->jaxpr_replicas(jaxpr)
A:jax._src.dispatch.(jaxpr, consts)->jax.interpreters.partial_eval.pad_jaxpr(jaxpr, consts)
A:jax._src.dispatch.tuple_args->should_tuple_args(len(abstract_args), backend.platform)
A:jax._src.dispatch.axis_env->jax.interpreters.xla.AxisEnv(nreps, (), ())
A:jax._src.dispatch.name_stack->jax._src.util.new_name_stack(util.wrap_name(name, 'jit'))
A:jax._src.dispatch.closed_jaxpr->jax.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.dispatch.lowering_result->jax.interpreters.mlir.lower_jaxpr_to_module(module_name, closed_jaxpr, unordered_effects, ordered_effects, backend, backend.platform, mlir.ReplicaAxisContext(axis_env), name_stack, donated_invars)
A:jax._src.dispatch.(new_jaxpr, used_consts, used_inputs)->jax.interpreters.partial_eval.dce_jaxpr_consts(jaxpr, used_outputs)
A:jax._src.dispatch.call_jaxpr->eqn.params.get('call_jaxpr')
A:jax._src.dispatch.arg_device->_device_from_arg_devices(arg_devices)
A:jax._src.dispatch.(in_avals, which_explicit)->jax._src.util.unzip2(in_type)
A:jax._src.dispatch.needs_out_handling->any((type(d) is core.InDBIdx for (a, _) in out_type or [] if type(a) is core.DShapedArray for d in a.shape))
A:jax._src.dispatch.explicit_args_->iter(explicit_args)
A:jax._src.dispatch.args->list(explicit_args)
A:jax._src.dispatch.handlers->map(partial(aval_to_result_handler, sticky_device), out_avals)
A:jax._src.dispatch.dyn_outs->any((type(aval) is core.DShapedArray and any((type(d) in (core.InDBIdx, core.OutDBIdx) for d in aval.shape)) for aval in out_avals))
A:jax._src.dispatch.buf_aval->jax.core.ShapedArray(tuple(pad_shape), buf.dtype, aval.weak_type)
A:jax._src.dispatch.data->maybe_create_array_from_da(buf, buf_aval, sticky_device)
A:jax._src.dispatch.tokens_flat->flatten(tokens)
A:jax._src.dispatch.(token_bufs, output_bufs)->jax._src.util.split_list(output_bufs, [num_output_tokens])
A:jax._src.dispatch.(device,)->compile_or_get_cached(backend, xla_computation, options, host_callbacks).local_devices()
A:jax._src.dispatch.in_flat->flatten((device_put(x, device) for (i, x) in enumerate(args) if i in kept_var_idx))
A:jax._src.dispatch.(in_flat, token_handler)->_add_tokens(has_unordered_effects, ordered_effects, has_host_callbacks, device, in_flat)
A:jax._src.dispatch.(out_flat, runtime_token)->compile_or_get_cached(backend, xla_computation, options, host_callbacks).execute_with_token(in_flat)
A:jax._src.dispatch.out_flat->compile_or_get_cached(backend, xla_computation, options, host_callbacks).execute(in_flat)
A:jax._src.dispatch.out_bufs->unflatten(out_flat, output_buffer_counts)
A:jax._src.dispatch.input_bufs_flip->list(unsafe_zip(*input_bufs))
A:jax._src.dispatch.out_bufs_flat_rep->compile_or_get_cached(backend, xla_computation, options, host_callbacks).execute_sharded_on_local_devices(input_bufs_flip)
A:jax._src.dispatch.module_str->xe.mlir.xla_computation_to_mlir_module(self._hlo)
A:jax._src.dispatch.self._executable->XlaCompiledComputation.from_xla_computation(self.name, self._hlo, self._in_type, self._out_type, **self.compile_args)
A:jax._src.dispatch._ir_dump_counter->itertools.count()
A:jax._src.dispatch.id->next(_ir_dump_counter)
A:jax._src.dispatch.serialized_computation->jax.interpreters.mlir.module_to_string(computation)
A:jax._src.dispatch.cached_executable->_cache_read(serialized_computation, module_name, compile_options, backend)
A:jax._src.dispatch.num_output_tokens->len(ordered_effects)
A:jax._src.dispatch.input_handler->_input_handler(backend, in_type, out_type)
A:jax._src.dispatch.result_handler->jax.interpreters.pxla.global_aval_to_result_handler(a, s, True, False)
A:jax._src.dispatch.options->jax._src.lib.xla_bridge.get_compile_options(num_replicas=nreps, num_partitions=1, device_assignment=(sticky_device,) if sticky_device else None)
A:jax._src.dispatch.buffer_counts->get_buffer_counts(out_avals, ordered_effects, has_unordered_effects)
A:jax._src.dispatch.unsafe_call->partial(_execute_trivial, jaxpr, device, consts, out_avals, result_handlers, has_unordered_effects, ordered_effects, kept_var_idx, bool(host_callbacks))
A:jax._src.dispatch.result_handlers->map(partial(aval_to_result_handler, device), out_avals)
A:jax._src.dispatch.ref_avals_fmt->', '.join((str(a) for a in ref_avals))
A:jax._src.dispatch.arg_avals_fmt->', '.join((str(a) for a in arg_avals))
A:jax._src.dispatch.x->_copy_device_array_to_device(x, device)
A:jax._src.dispatch.backend->jax._src.lib.xla_bridge.get_device_backend(device)
A:jax._src.dispatch._scalar_types->jax._src.dtypes.python_scalar_dtypes.keys()
A:jax._src.dispatch.moved_buf->jax._src.lib.xla_bridge.get_device_backend(device).buffer_from_pyval(np.asarray(buf), device)
A:jax._src.dispatch.a->jax.interpreters.xla.abstractify(x)
A:jax._src.dispatch.map_->s.devices_indices_map(x.shape)
A:jax._src.dispatch.device_put_p->jax.core.Primitive('device_put')
jax._src.dispatch.RuntimeTokenSet(self)
jax._src.dispatch.RuntimeTokenSet.__init__(self)
jax._src.dispatch.RuntimeTokenSet.block_until_ready(self)
jax._src.dispatch.RuntimeTokenSet.clear(self)
jax._src.dispatch.RuntimeTokenSet.get_token(self,eff:core.Effect,device:Device)->RuntimeToken
jax._src.dispatch.RuntimeTokenSet.set_output_runtime_token(self,device:Device,token:RuntimeToken)
jax._src.dispatch.RuntimeTokenSet.set_output_token(self,device:Device,token:RuntimeToken)
jax._src.dispatch.RuntimeTokenSet.update_token(self,eff:core.Effect,token:RuntimeToken)
jax._src.dispatch.SimpleResultHandler(self,handlers)
jax._src.dispatch.SimpleResultHandler.__init__(self,handlers)
jax._src.dispatch.SimpleResultHandler.__iter__(self)
jax._src.dispatch.SimpleResultHandler.__len__(self)
jax._src.dispatch.XlaCompiledComputation(self,xla_executable,in_avals,kept_var_idx,unsafe_call,keepalive:Any)
jax._src.dispatch.XlaCompiledComputation.__init__(self,xla_executable,in_avals,kept_var_idx,unsafe_call,keepalive:Any)
jax._src.dispatch.XlaCompiledComputation.call(self,*args)
jax._src.dispatch.XlaCompiledComputation.from_trivial_jaxpr(jaxpr,consts,device,in_avals,out_avals,has_unordered_effects,ordered_effects,kept_var_idx,keepalive:Optional[Any],host_callbacks:List[Any])->XlaCompiledComputation
jax._src.dispatch.XlaCompiledComputation.from_xla_computation(name:str,xla_computation:Optional[ir.Module],in_type:Optional[pe.InputType],out_type:Optional[pe.OutputType],nreps:int,device:Optional[Device],backend:Backend,tuple_args:bool,in_avals:Sequence[core.AbstractValue],out_avals:Sequence[core.AbstractValue],has_unordered_effects:bool,ordered_effects:List[core.Effect],kept_var_idx:Set[int],keepalive:Optional[Any],host_callbacks:List[Any])->XlaCompiledComputation
jax._src.dispatch.XlaCompiledComputation.is_trivial(self)
jax._src.dispatch.XlaCompiledComputation.xla_executable(self)
jax._src.dispatch.XlaCompiledComputation.xla_extension_executable(self)
jax._src.dispatch.XlaComputation(self,name:str,hlo,is_trivial:bool,donated_invars:Optional[Sequence[bool]],in_type:Optional[pe.InputType],out_type:Optional[pe.OutputType],**compile_args)
jax._src.dispatch.XlaComputation.__init__(self,name:str,hlo,is_trivial:bool,donated_invars:Optional[Sequence[bool]],in_type:Optional[pe.InputType],out_type:Optional[pe.OutputType],**compile_args)
jax._src.dispatch.XlaComputation.compile(self)->XlaCompiledComputation
jax._src.dispatch.XlaComputation.hlo(self)->xc.XlaComputation
jax._src.dispatch.XlaComputation.is_trivial(self)
jax._src.dispatch.XlaComputation.mhlo(self)->ir.Module
jax._src.dispatch._add_tokens(has_unordered_effects:bool,ordered_effects:List[core.Effect],has_host_callbacks:bool,device:Device,input_bufs)
jax._src.dispatch._backend_supports_unbounded_dynamic_shapes(backend:Backend)->bool
jax._src.dispatch._cache_read(computation:Union[str,bytes,ir.Module],module_name:str,compile_options:CompileOptions,backend:Backend)->Optional[XlaLoadedExecutable]
jax._src.dispatch._cache_write(computation:Union[str,bytes,ir.Module],module_name:str,compile_options:CompileOptions,backend:Backend,compiled:XlaLoadedExecutable)
jax._src.dispatch._check_special(name,dtype,buf)
jax._src.dispatch._copy_array_to_device(x:jax.Array,device:Optional[xc.Device])->jax.Array
jax._src.dispatch._copy_device_array_to_device(x:Union[device_array.DeviceArrayProtocol,device_array._DeviceArray],device:Optional[xc.Device])->Union[device_array.DeviceArrayProtocol, device_array._DeviceArray]
jax._src.dispatch._device_from_arg_devices(devices:Sequence[Optional[Device]])->Optional[Device]
jax._src.dispatch._device_put_array(x,device:Optional[Device])
jax._src.dispatch._device_put_device_array(x:Union[device_array.DeviceArrayProtocol,device_array._DeviceArray],device:Optional[Device])
jax._src.dispatch._device_put_impl(x,device:Optional[Union[Device,jax.sharding.Sharding]]=None)
jax._src.dispatch._device_put_lowering(ctx,x,*,device)
jax._src.dispatch._device_put_scalar(x,device)
jax._src.dispatch._device_put_token(_,device)
jax._src.dispatch._dump_ir_to_file(name:str,ir:str)
jax._src.dispatch._dynamic_array_result_handler(sticky_device,aval,env,buf)
jax._src.dispatch._execute_compiled(name:str,compiled:XlaLoadedExecutable,input_handler:Optional[Callable],output_buffer_counts:Sequence[int],result_handler:Callable,has_unordered_effects:bool,ordered_effects:List[core.Effect],kept_var_idx,has_host_callbacks:bool,*args)
jax._src.dispatch._execute_replicated(name:str,compiled:XlaLoadedExecutable,input_handler:Optional[Callable],output_buffer_counts:Sequence[int],result_handler:Callable,has_unordered_effects:bool,ordered_effects:List[core.Effect],kept_var_idx,has_host_callbacks:bool,*args,from_lower_sharding_computation:bool=False)
jax._src.dispatch._execute_trivial(jaxpr,device:Optional[Device],consts,avals,handlers,has_unordered_effects:bool,ordered_effects:List[core.Effect],kept_var_idx,host_callbacks,*args)
jax._src.dispatch._input_handler(backend:Backend,in_type:Optional[pe.InputType],out_type:Optional[pe.OutputType])->Optional[Callable]
jax._src.dispatch._is_bint_axis_size(d:core.AxisSize)->bool
jax._src.dispatch._make_string_safe_for_filename(s:str)->str
jax._src.dispatch._prune_unused_inputs(jaxpr:core.Jaxpr)->Tuple[core.Jaxpr, Set[int], Set[int]]
jax._src.dispatch._result_handler(backend:Backend,sticky_device:Optional[Device],out_type:Optional[pe.OutputType])->Callable
jax._src.dispatch._xla_call_impl(fun:lu.WrappedFun,*args,device,backend,name,donated_invars,inline,keep_unused:bool)
jax._src.dispatch._xla_callable_device(nreps,backend,device,arg_devices)->Optional[Device]
jax._src.dispatch._xla_callable_uncached(fun:lu.WrappedFun,device,backend,name,donated_invars,keep_unused,*arg_specs)
jax._src.dispatch.apply_outfeed_rewriter(jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.dispatch.apply_primitive(prim,*args,**params)
jax._src.dispatch.arg_spec(x:Any)->ArgSpec
jax._src.dispatch.array_result_handler(sticky_device:Optional[Device],aval:core.ShapedArray)
jax._src.dispatch.aval_to_num_buffers(aval:core.AbstractValue)->int
jax._src.dispatch.aval_to_result_handler(sticky_device:Optional[Device],aval:core.AbstractValue)->ResultHandler
jax._src.dispatch.backend_compile(backend,built_c,options,host_callbacks)
jax._src.dispatch.check_arg_avals_for_call(ref_avals,arg_avals)
jax._src.dispatch.check_special(name,bufs)
jax._src.dispatch.compile_or_get_cached(backend,computation:ir.Module,compile_options,host_callbacks)
jax._src.dispatch.device_put(x,device:Optional[Device]=None)->Tuple[Any, ...]
jax._src.dispatch.dynamic_array_result_handler(sticky_device:Optional[Device],aval:core.DShapedArray)
jax._src.dispatch.eqn_replicas(eqn)
jax._src.dispatch.get_buffer_counts(out_avals,ordered_effects,has_unordered_effects)
jax._src.dispatch.initial_style_primitive_replicas(params)
jax._src.dispatch.is_single_device_sharding(sharding)->bool
jax._src.dispatch.jaxpr_has_bints(jaxpr:core.Jaxpr)->bool
jax._src.dispatch.jaxpr_has_primitive(jaxpr,prim_name:str)
jax._src.dispatch.jaxpr_literals(jaxpr)
jax._src.dispatch.jaxpr_replicas(jaxpr)->int
jax._src.dispatch.jaxpr_shardings(jaxpr)->Iterator[jax.sharding.XLACompatibleSharding]
jax._src.dispatch.log_elapsed_time(fmt:str)
jax._src.dispatch.lower_xla_callable(fun:lu.WrappedFun,device,backend,name,donated_invars,always_lower:bool,keep_unused:bool,*arg_specs)
jax._src.dispatch.maybe_create_array_from_da(buf,aval,device)
jax._src.dispatch.needs_check_special()
jax._src.dispatch.not_none_device_or_backend_on_jit(backend,device,num_ins)
jax._src.dispatch.prefetch(x)
jax._src.dispatch.raise_warnings_or_errors_for_jit_of_pmap(nreps,backend,name,jaxpr)
jax._src.dispatch.sharded_lowering(fun,device,backend,name,donated_invars,always_lower,keep_unused,*arg_specs)
jax._src.dispatch.should_tuple_args(num_args:int,platform:str)
jax._src.dispatch.simple_impl(prim)
jax._src.dispatch.wait_for_tokens()
jax._src.dispatch.xla_primitive_callable(prim,*arg_specs:ArgSpec,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/traceback_util.py----------------------------------------
A:jax._src.traceback_util.C->TypeVar('C', bound=Callable[..., Any])
A:jax._src.traceback_util.path->os.path.abspath(path)
A:jax._src.traceback_util.path_prefix->os.path.abspath(path_prefix)
A:jax._src.traceback_util.common->os.path.commonpath([path, path_prefix])
A:jax._src.traceback_util.frames->list(traceback.walk_tb(tb))
A:jax._src.traceback_util.out->types.TracebackType(out, f, f.f_lasti, lineno)
A:jax._src.traceback_util.tb->traceback.extract_stack(e.__traceback__.tb_frame)
A:jax._src.traceback_util.mode->filtering_mode()
A:jax._src.traceback_util.filtered_tb->filter_traceback(e.__traceback__)
A:jax._src.traceback_util.msg->format_exception_only(e)
A:jax._src.traceback_util.unfiltered->UnfilteredStackTrace(msg)
jax._src.traceback_util.UnfilteredStackTrace(Exception)
jax._src.traceback_util.add_call_stack_frames(tb)
jax._src.traceback_util.add_tracebackhide_to_hidden_frames(tb)
jax._src.traceback_util.api_boundary(fun:C)->C
jax._src.traceback_util.filter_traceback(tb)
jax._src.traceback_util.filtering_mode()
jax._src.traceback_util.format_exception_only(e)
jax._src.traceback_util.ignore_known_hidden_frame(f)
jax._src.traceback_util.include_frame(f)
jax._src.traceback_util.ipython_supports_tracebackhide()
jax._src.traceback_util.is_reraiser_frame(f)
jax._src.traceback_util.is_under_reraiser(e)
jax._src.traceback_util.path_starts_with(path,path_prefix)
jax._src.traceback_util.register_exclusion(path)
jax._src.traceback_util.running_under_ipython()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/custom_transpose.py----------------------------------------
A:jax._src.custom_transpose.out_store->StoreEqual()
A:jax._src.custom_transpose.flatten_fun_nokwargs->transformation_with_aux(api_util.flatten_fun_nokwargs.args[0])
A:jax._src.custom_transpose.(_, res_tree)->tree_flatten(res_arg)
A:jax._src.custom_transpose.(_, lin_tree)->tree_flatten(lin_arg)
A:jax._src.custom_transpose.(args_flat, in_tree)->tree_flatten((res_arg, lin_arg))
A:jax._src.custom_transpose.(flat_fun, out_tree2)->flatten_fun_nokwargs(lu.wrap_init(self.fun), in_tree)
A:jax._src.custom_transpose.(out_types_flat, out_tree)->tree_flatten(out_types)
A:jax._src.custom_transpose.out_flat->CustomTransposePrimitive('custom_transpose_call').bind(flat_fun, *args_flat, transpose=self.transpose, out_types=out_types_flat, lin_tree=lin_tree, res_tree=res_tree, out_tree=out_tree)
A:jax._src.custom_transpose.full_tree->tree_fill(0, full_treedef)
A:jax._src.custom_transpose.entire->tree_fill(0, entire)
A:jax._src.custom_transpose.prefix->tree_fill(0, prefix)
A:jax._src.custom_transpose.(transpose_jaxpr, transpose_consts)->thunk()
A:jax._src.custom_transpose.transpose_jaxpr->jax.core.ClosedJaxpr(pe.convert_constvars_jaxpr(transpose_jaxpr), ())
A:jax._src.custom_transpose.args_flat->tree_leaves((res_arg, ct_out))
A:jax._src.custom_transpose.ct_ins->jax.core.jaxpr_as_fun(transpose_jaxpr)(*transpose_consts, *args_flat)
A:jax._src.custom_transpose.top_trace->jax.core.find_top_trace(args)
A:jax._src.custom_transpose.tracers->map(top_trace.full_raise, args)
A:jax._src.custom_transpose.outs->jax.core.find_top_trace(args).process_custom_transpose(self, call, tracers, **params)
A:jax._src.custom_transpose.new_params->dict(params)
A:jax._src.custom_transpose.new_params['transpose']->make_transpose_from_thunk(new_params.pop('transpose_jaxpr_thunk'), new_params['lin_tree'])
A:jax._src.custom_transpose.call->jax.linear_util.wrap_init(core.jaxpr_as_fun(new_params.pop('call_jaxpr')))
A:jax._src.custom_transpose.transpose->make_transpose_from_thunk(params['transpose_jaxpr_thunk'], lin_tree)
A:jax._src.custom_transpose.call_in_tree->treedef_tuple((res_tree, lin_tree))
A:jax._src.custom_transpose.(res_arg, lin_arg)->tree_unflatten(call_in_tree, args)
A:jax._src.custom_transpose.ct_out->tree_unflatten(out_tree, cts)
A:jax._src.custom_transpose.ct_lin->transpose(res_arg, ct_out)
A:jax._src.custom_transpose.(ct_lin_flat, _)->tree_flatten(tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None), is_leaf=lambda x: x is None)
A:jax._src.custom_transpose.custom_transpose_p->CustomTransposePrimitive('custom_transpose_call')
jax._src.custom_transpose.CustomTransposePrimitive(core.Primitive)
jax._src.custom_transpose.CustomTransposePrimitive.bind(self,call,*args,**params)
jax._src.custom_transpose.CustomTransposePrimitive.get_bind_params(self,params)
jax._src.custom_transpose.StoreEqual(lu.Store)
jax._src.custom_transpose.StoreEqual.store(self,val)
jax._src.custom_transpose.check_transpose_rule_trees(rule,lin_tree,rule_out_tree)
jax._src.custom_transpose.custom_transpose(self,fun:Callable)
jax._src.custom_transpose.custom_transpose.__init__(self,fun:Callable)
jax._src.custom_transpose.custom_transpose.def_transpose(self,transpose:Callable)
jax._src.custom_transpose.custom_transpose_lowering(*args,call_jaxpr,**params)
jax._src.custom_transpose.custom_transpose_transpose_rule(cts,*args,out_types,res_tree,lin_tree,out_tree,**params)
jax._src.custom_transpose.custom_transpose_typecheck(*in_atoms,out_types,**params)
jax._src.custom_transpose.is_treedef_prefix(entire,prefix)
jax._src.custom_transpose.make_transpose_from_thunk(thunk,lin_tree)
jax._src.custom_transpose.rule_name(rule)
jax._src.custom_transpose.transformation_with_aux(gen,fun:lu.WrappedFun,*gen_static_args)->Tuple[lu.WrappedFun, Any]
jax._src.custom_transpose.tree_broadcast(full_treedef,tree,is_leaf=None)
jax._src.custom_transpose.tree_fill(x,treedef)
jax._src.custom_transpose.tree_fill_like(x,tree)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/custom_api_util.py----------------------------------------
A:jax._src.custom_api_util._custom_wrapper_types->set()
jax._src.custom_api_util.forward_attr(self_,name)
jax._src.custom_api_util.register_custom_decorator_type(cls)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/flatten_util.py----------------------------------------
A:jax._src.flatten_util.(leaves, treedef)->tree_flatten(pytree)
A:jax._src.flatten_util.(flat, unravel_list)->_ravel_list(leaves)
A:jax._src.flatten_util.to_dtype->jax._src.dtypes.result_type(*from_dtypes)
A:jax._src.flatten_util.(sizes, shapes)->unzip2(((jnp.size(x), jnp.shape(x)) for x in lst))
A:jax._src.flatten_util.indices->numpy.cumsum(sizes)
A:jax._src.flatten_util.chunks->jax.numpy.split(arr, indices[:-1])
A:jax._src.flatten_util.raveled->jax.numpy.concatenate([ravel(e) for e in lst])
A:jax._src.flatten_util.arr_dtype->jax._src.dtypes.dtype(arr)
jax._src.flatten_util._ravel_list(lst)
jax._src.flatten_util.ravel_pytree(pytree)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/source_info_util.py----------------------------------------
A:jax._src.source_info_util.scopes->tuple(map(Scope, name))
A:jax._src.source_info_util.scope->elem.wrap(scope)
A:jax._src.source_info_util.frames->itertools.islice(user_frames(source_info), num_frames)
A:jax._src.source_info_util.self.context->new_source_info()
A:jax._src.source_info_util._source_info_context->_SourceInfoContext()
A:jax._src.source_info_util.source_info->source_info.replace(traceback=xla_client.Traceback.get_traceback()).replace(traceback=xla_client.Traceback.get_traceback())
A:jax._src.source_info_util._source_info_context.context->_SourceInfoContext().context.replace(traceback=c, name_stack=name_stack)
A:jax._src.source_info_util.filtered_tb->jax._src.traceback_util.filter_traceback(c.as_python_traceback())
A:jax._src.source_info_util.msg->jax._src.traceback_util.format_exception_only(e)
A:jax._src.source_info_util.exp->JaxStackTraceBeforeTransformation(msg).with_traceback(filtered_tb)
A:jax._src.source_info_util.new_context->prev_context.replace(name_stack=curr_name_stack.transform(name))
jax._src.source_info_util.Frame(NamedTuple)
jax._src.source_info_util.JaxStackTraceBeforeTransformation(Exception)
jax._src.source_info_util.NameStack
jax._src.source_info_util.NameStack.__add__(self,other:'NameStack')->'NameStack'
jax._src.source_info_util.NameStack.__getitem__(self,idx)->'NameStack'
jax._src.source_info_util.NameStack.__len__(self)
jax._src.source_info_util.NameStack.__radd__(self,other:'NameStack')->'NameStack'
jax._src.source_info_util.NameStack.__str__(self)->str
jax._src.source_info_util.NameStack.extend(self,name:Union[Tuple[str,...],str])->'NameStack'
jax._src.source_info_util.NameStack.transform(self,transform_name:str)->'NameStack'
jax._src.source_info_util.NameStack.wrap_name(self,name:str)->str
jax._src.source_info_util.Scope(NamedTuple)
jax._src.source_info_util.Scope.wrap(self,stack:Tuple[str,...])->Tuple[str, ...]
jax._src.source_info_util.SourceInfo(NamedTuple)
jax._src.source_info_util.SourceInfo.replace(self,*,traceback:Optional[Traceback]=None,name_stack:Optional[NameStack]=None)->'SourceInfo'
jax._src.source_info_util.Transform(NamedTuple)
jax._src.source_info_util.Transform.wrap(self,stack:Tuple[str,...])->Tuple[str, ...]
jax._src.source_info_util._SourceInfoContext(self)
jax._src.source_info_util._SourceInfoContext.__init__(self)
jax._src.source_info_util._raw_frame_to_frame(code:types.CodeType,lasti:int)->Frame
jax._src.source_info_util.current()->SourceInfo
jax._src.source_info_util.current_name_stack()->NameStack
jax._src.source_info_util.extend_name_stack(name:str)->Iterator[NameStack]
jax._src.source_info_util.has_user_context(e)
jax._src.source_info_util.is_user_filename(filename:str)->bool
jax._src.source_info_util.new_source_info()->SourceInfo
jax._src.source_info_util.register_exclusion(path)
jax._src.source_info_util.reset_name_stack()->Iterator[None]
jax._src.source_info_util.set_name_stack(name_stack:NameStack)->Iterator[None]
jax._src.source_info_util.summarize(source_info:SourceInfo,num_frames=1)->str
jax._src.source_info_util.transform_name_stack(name:str)->Iterator[NameStack]
jax._src.source_info_util.user_context(c:Optional[Traceback],*,name_stack:Optional[NameStack]=None)
jax._src.source_info_util.user_frame(source_info:SourceInfo)->Optional[Frame]
jax._src.source_info_util.user_frames(source_info:SourceInfo)->Iterator[Frame]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/dtypes.py----------------------------------------
A:jax._src.dtypes.dtype->canonicalize_dtype(dtype)
A:jax._src.dtypes.ftype->to_inexact_dtype(dtype)
A:jax._src.dtypes.typ->dtype(x)
A:jax._src.dtypes.smallest_normal->float.fromhex('0x1p-126')
A:jax._src.dtypes.self.smallest_normal->bfloat16(smallest_normal)
A:jax._src.dtypes.tiny->float.fromhex('0x1p-126')
A:jax._src.dtypes.eps->float.fromhex('0x1p-7')
A:jax._src.dtypes.epsneg->float.fromhex('0x1p-8')
A:jax._src.dtypes.max->float.fromhex('0x1.FEp127')
A:jax._src.dtypes.obj->object.__new__(np.finfo)
A:jax._src.dtypes.obj.eps->bfloat16(eps)
A:jax._src.dtypes.obj.epsneg->bfloat16(epsneg)
A:jax._src.dtypes.obj.max->bfloat16(max)
A:jax._src.dtypes.obj.min->bfloat16(-max)
A:jax._src.dtypes.obj.resolution->bfloat16(resolution)
A:jax._src.dtypes.obj._machar->_Bfloat16MachArLike()
A:jax._src.dtypes.obj.tiny->bfloat16(tiny)
A:jax._src.dtypes.obj._str_tiny->float_to_str(tiny)
A:jax._src.dtypes.obj._str_smallest_normal->float_to_str(tiny)
A:jax._src.dtypes.obj._str_max->float_to_str(max)
A:jax._src.dtypes.obj._str_epsneg->float_to_str(epsneg)
A:jax._src.dtypes.obj._str_eps->float_to_str(eps)
A:jax._src.dtypes.obj._str_resolution->float_to_str(resolution)
A:jax._src.dtypes.cls._finfo_cache[_bfloat16_dtype]->cls._bfloat16_finfo()
A:jax._src.dtypes.lattice->_type_promotion_lattice(jax_numpy_dtype_promotion)
A:jax._src.dtypes.new_upper_bounds->set().union(*(lattice[b] for b in upper_bounds[n]))
A:jax._src.dtypes.N->set(nodes)
A:jax._src.dtypes.CUB->set.intersection(*(UB[n] for n in N))
A:jax._src.dtypes.dt->numpy.result_type(x)
A:jax._src.dtypes.(dtypes, weak_types)->zip(*(_dtype_and_weaktype(arg) for arg in args))
A:jax._src.dtypes.result_type->_least_upper_bound(config.jax_numpy_dtype_promotion, *{_jax_type(d, w) for (d, w) in zip(dtypes, weak_types)})
A:jax._src.dtypes.out_dtype->dtype(result_type)
A:jax._src.dtypes.out_weak_type->any((result_type is t for t in _weak_types))
A:jax._src.dtypes.(dtype, weak_type)->_lattice_result_type(*(float_ if arg is None else arg for arg in args))
jax._src.dtypes.TypePromotionError(ValueError)
jax._src.dtypes._Bfloat16MachArLike(self)
jax._src.dtypes._Bfloat16MachArLike.__init__(self)
jax._src.dtypes._canonicalize_dtype(x64_enabled,dtype)
jax._src.dtypes._dtype_and_weaktype(value)
jax._src.dtypes._issubclass(a,b)
jax._src.dtypes._jax_type(dtype,weak_type)
jax._src.dtypes._lattice_result_type(*args)
jax._src.dtypes._least_upper_bound(jax_numpy_dtype_promotion,*nodes)
jax._src.dtypes._make_lattice_upper_bounds(jax_numpy_dtype_promotion)
jax._src.dtypes._scalar_type_to_dtype(typ:type,value:Any=None)
jax._src.dtypes._type_promotion_lattice(jax_numpy_dtype_promotion)
jax._src.dtypes.canonicalize_dtype(dtype)
jax._src.dtypes.coerce_to_array(x,dtype=None)
jax._src.dtypes.dtype(x,*,canonicalize=False)
jax._src.dtypes.finfo(cls,dtype)
jax._src.dtypes.finfo.__new__(cls,dtype)
jax._src.dtypes.finfo._bfloat16_finfo()
jax._src.dtypes.is_python_scalar(x)
jax._src.dtypes.is_weakly_typed(x)
jax._src.dtypes.issubdtype(a,b)->bool
jax._src.dtypes.promote_types(a,b)
jax._src.dtypes.result_type(*args,return_weak_type_flag=False)
jax._src.dtypes.scalar_type_of(x)
jax._src.dtypes.to_complex_dtype(dtype)
jax._src.dtypes.to_inexact_dtype(dtype)
jax._src.dtypes.to_numeric_dtype(dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/api.py----------------------------------------
A:jax._src.api._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.api.F->TypeVar('F', bound=Callable)
A:jax._src.api.T->TypeVar('T')
A:jax._src.api.U->TypeVar('U')
A:jax._src.api.leaves->tree_leaves(output)
A:jax._src.api.argnums->_ensure_index(argnums)
A:jax._src.api.argnames->tuple((k for (i, (k, param)) in enumerate(parameters.items()) if param.kind == _POSITIONAL_OR_KEYWORD and i in argnums))
A:jax._src.api.donate_argnums->rebase_donate_argnums(donate_argnums, static_argnums)
A:jax._src.api.sig->inspect.signature(fun)
A:jax._src.api.(static_argnums, static_argnames)->_infer_argnums_and_argnames(sig, static_argnums, static_argnames)
A:jax._src.api.f->jax.linear_util.annotate(f, in_type)
A:jax._src.api.(f, args)->argnums_partial(f, dyn_argnums, args)
A:jax._src.api.(f, kwargs)->argnames_partial_except(f, static_argnames, kwargs)
A:jax._src.api.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax._src.api.donated_invars->donation_vector(donate_tuple, dyn_args, kwargs)
A:jax._src.api.(closed_fun, in_tree, args_flat, donated_invars)->_prepare_jit(fun, static_argnums, static_argnames, donate_argnums, args, kwargs)
A:jax._src.api.(flat_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.api.in_type->tuple(zip(in_avals, keep_inputs))
A:jax._src.api.flat_fun->jax.linear_util.annotate(flat_fun, in_type)
A:jax._src.api.out_flat->jax.core.named_call_p.bind(flat_f, name=name)
A:jax._src.api.f_jitted.lower->_jit_lower(fun, static_argnums, static_argnames, device, backend, donate_argnums, inline, keep_unused, None)
A:jax._src.api._cpp_jit_cache->jax._src.lib.jax_jit.CompiledFunctionCache()
A:jax._src.api.out_pytree_def->out_tree()
A:jax._src.api.out->jax.interpreters.partial_eval.abstract_eval_fun(wrapped_fun.call_wrapped, *map(shaped_abstractify, args_flat), debug_info=debug_info)
A:jax._src.api.execute->jax.interpreters.pxla.parallel_callable.most_recent_entry()
A:jax._src.api.fastpath_data->_PmapFastpathData(version=1, xla_executable=execute_replicated.xla_executable, in_handler=in_handler, out_handler=out_handler, out_pytree_def=out_pytree_def, input_sharding_specs=[i.sharding_spec for i in in_handler.in_shardings], input_devices=in_handler.local_devices, input_indices=in_handler.input_indices, input_array_shardings=in_handler.in_shardings, out_sharding_specs=[s.sharding_spec for s in out_handler.out_shardings], out_indices=out_indices, out_avals=out_handler.out_avals, out_array_shardings=out_array_shardings, out_committed=out_committed)
A:jax._src.api.backend_->jax._src.lib.xla_bridge.get_backend(backend)
A:jax._src.api.cpp_jitted_f->jax._src.lib.jax_jit.jit(fun, cache_miss, get_device_info, static_argnums=static_argnums, static_argnames=static_argnames, donate_argnums=donate_argnums, cache=_cpp_jit_cache, **jitted_f_kwargs)
A:jax._src.api.f_jitted->wraps(fun)(cpp_jitted_f)
A:jax._src.api.device->getattr(x, '_device', None)
A:jax._src.api.arg_specs_and_devices->map(arg_spec, args_flat)
A:jax._src.api.(in_avals, _)->unzip2(arg_specs_and_devices)
A:jax._src.api.computation->jax.interpreters.pxla.lower_parallel_callable(p.flat_fun, backend, axis_name, axis_size=p.local_axis_size, global_axis_size=axis_size, devices=p.devices, name=p.flat_fun.__name__, in_axes=p.in_axes_flat, out_axes_thunk=p.out_axes_thunk, donated_invars=p.donated_invars, global_arg_shapes=p.global_arg_shapes_flat, avals=abstract_args)
A:jax._src.api.static_argnums->_ensure_index_tuple(static_argnums)
A:jax._src.api.fun_name->getattr(fun, '__name__', 'unknown')
A:jax._src.api.(names, sizes)->unzip2(axis_env)
A:jax._src.api.(f, dyn_args)->argnums_partial(f, dyn_argnums, args)
A:jax._src.api.in_parts_flat->tuple(flatten_axes('xla_computation in_parts', in_tree.children()[0], in_parts))
A:jax._src.api.(jaxtree_fun, out_tree)->flatten_fun(f, in_tree)
A:jax._src.api.avals->map(shaped_abstractify, args_flat)
A:jax._src.api.(jaxpr, out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(jaxtree_fun, avals)
A:jax._src.api.jaxpr->jax._src.dispatch.apply_outfeed_rewriter(jaxpr)
A:jax._src.api.axis_env_->make_axis_env(dispatch.jaxpr_replicas(jaxpr))
A:jax._src.api.out_parts_flat->tuple(flatten_axes('xla_computation out_parts', out_tree(), out_parts))
A:jax._src.api.lowering_result->jax.interpreters.mlir.lower_jaxpr_to_module(f'xla_computation_{fun_name}', core.ClosedJaxpr(jaxpr, consts), unordered_effects=unordered_effects, ordered_effects=ordered_effects, backend_or_name=backend, platform=platform, axis_context=mlir.ReplicaAxisContext(axis_env_), name_stack=new_name_stack(wrap_name(fun_name, 'xla_computation')), donated_args=donated_invars, arg_shardings=None if in_parts_flat is None else map(xla.sharding_to_proto, in_parts_flat), result_shardings=None if out_parts_flat is None else map(xla.sharding_to_proto, out_parts_flat))
A:jax._src.api.built->jax._src.lib.xla_client._xla.mlir.mlir_module_to_xla_computation(mlir.module_to_string(lowering_result.module), use_tuple_args=should_tuple, return_tuple=True)
A:jax._src.api.out_shape->tree_unflatten(out_tree(), out_shapes_flat)
A:jax._src.api.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, holomorphic=holomorphic, allow_int=allow_int, reduce_axes=reduce_axes)
A:jax._src.api.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax._src.api.((_, aux), g)->value_and_grad_f(*args, **kwargs)
A:jax._src.api.reduce_axes->_ensure_str_tuple(reduce_axes)
A:jax._src.api.(f_partial, dyn_args)->argnums_partial(f, argnums, args, require_static_args_hashable=False)
A:jax._src.api.(ans, vjp_py)->_vjp(f_partial, *dyn_args, reduce_axes=reduce_axes)
A:jax._src.api.(ans, vjp_py, aux)->_vjp(f_partial, *dyn_args, has_aux=True, reduce_axes=reduce_axes)
A:jax._src.api.g->vjp_py(lax_internal._one(ans))
A:jax._src.api.aval->jax.core.unmapped_aval(len(devices), core.no_axis_name, 0, core.raise_to_shaped(core.get_aval(x)))
A:jax._src.api._check_input_dtype_grad->partial(_check_input_dtype_revderiv, 'grad')
A:jax._src.api._check_output_dtype_grad->partial(_check_output_dtype_revderiv, 'grad')
A:jax._src.api.pushfwd->partial(_jvp, f_partial, dyn_args, has_aux=True)
A:jax._src.api.(y, jac)->vmap(pushfwd, out_axes=(None, -1))(_std_basis(dyn_args))
A:jax._src.api.(y, jac, aux)->vmap(pushfwd, out_axes=(None, -1, None))(_std_basis(dyn_args))
A:jax._src.api.jac_tree->tree_transpose(tree_structure(example_args), tree_structure(y), jac_tree)
A:jax._src.api.(y, pullback)->_vjp(f_partial, *dyn_args)
A:jax._src.api.(y, pullback, aux)->_vjp(f_partial, *dyn_args, has_aux=True)
A:jax._src.api.jac->vmap(pullback)(_std_basis(y))
A:jax._src.api._check_input_dtype_jacrev->partial(_check_input_dtype_revderiv, 'jacrev')
A:jax._src.api._check_output_dtype_jacrev->partial(_check_output_dtype_revderiv, 'jacrev')
A:jax._src.api.(leaves, _)->tree_flatten(pytree)
A:jax._src.api.ndim->property(lambda self: len(self.shape))
A:jax._src.api.dtype->jax._src.dtypes.result_type(*leaves)
A:jax._src.api.flat_basis->jax.numpy.eye(ndim, dtype=dtype)
A:jax._src.api.(leaves, treedef)->tree_flatten(pytree)
A:jax._src.api.parts->_split(arr, np.cumsum(map(np.size, leaves[:-1])), axis)
A:jax._src.api.in_axes->tuple(in_axes)
A:jax._src.api.in_axes_flat->tuple(flatten_axes('pmap in_axes', in_tree, (dyn_in_axes, 0)))
A:jax._src.api.(args, kwargs)->tree_unflatten(tree, vals)
A:jax._src.api.axis_sizes->jax.core.dedup_referents((_get_axis_size(name, np.shape(x), d) for (x, d) in zip(vals, dims) if d is not None))
A:jax._src.api.(position_only_tree, leaf)->treedef_children(tree)
A:jax._src.api.sizes->tree_unflatten(tree, sizes)
A:jax._src.api.dyn_in_axes->tuple((in_axes[i] for i in dyn_argnums))
A:jax._src.api.dyn_global_arg_shapes->tuple((global_arg_shapes[i] for i in dyn_argnums))
A:jax._src.api.(args, in_tree)->tree_flatten(py_args)
A:jax._src.api.global_arg_shapes_flat->tuple(flatten_axes('pmap global_arg_shapes', in_tree, (dyn_global_arg_shapes, None), kws=True))
A:jax._src.api.local_axis_size->_mapped_axis_size(in_tree, args, in_axes_flat, 'pmap', kws=True)
A:jax._src.api.out_axes_thunk->HashableFunction(lambda : tuple(flatten_axes('pmap out_axes', out_tree(), tree_unflatten(out_axes_treedef, list(out_axes_leaves)))), closure=(tuple(out_axes_leaves), out_axes_treedef))
A:jax._src.api.(out_axes_leaves, out_axes_treedef)->tree_flatten(out_axes)
A:jax._src.api.p->_prepare_pmap(fun, in_axes, out_axes, static_broadcasted_tuple, donate_tuple, global_arg_shapes, devices, args, kwargs)
A:jax._src.api.static_broadcasted_tuple->_ensure_index_tuple(static_broadcasted_argnums)
A:jax._src.api.donate_tuple->rebase_donate_argnums(_ensure_index_tuple(donate_argnums), static_broadcasted_tuple)
A:jax._src.api.(axis_name, static_broadcasted_tuple, donate_tuple)->_shared_code_pmap(fun, axis_name, static_broadcasted_argnums, donate_argnums, in_axes, out_axes)
A:jax._src.api.f_pmapped_->_get_f_mapped(fun=fun, axis_name=axis_name, in_axes=in_axes, out_axes=out_axes, static_broadcasted_tuple=static_broadcasted_tuple, devices=devices, backend=backend, axis_size=axis_size, global_arg_shapes=global_arg_shapes, donate_tuple=donate_tuple)
A:jax._src.api.(out_tree, out_flat)->f_pmapped_(*args, **kwargs)
A:jax._src.api.pmap_f.lower->_pmap_lower(fun, axis_name, in_axes, out_axes, static_broadcasted_tuple, devices, backend, axis_size, global_arg_shapes, donate_tuple)
A:jax._src.api.cpp_mapped_f->jax._src.lib.pmap_lib.pmap(fun, cache_miss, static_broadcasted_tuple, partial(pxla._shard_arg, mode=pxla.InputsHandlerMode.pmap))
A:jax._src.api.pmap_f->wraps(fun)(cpp_mapped_f)
A:jax._src.api.abstract_args->list(map(shaped_abstractify, p.flat_args))
A:jax._src.api.(ps_flat, tree_def)->tree_flatten(primals)
A:jax._src.api.(ts_flat, tree_def_2)->tree_flatten(tangents)
A:jax._src.api.(out_primals, out_tangents)->jvp_fun.call_wrapped(ps_flat, ts_flat)
A:jax._src.api.out_tree->out_tree()
A:jax._src.api.(flat_fun, out_aux_trees)->flatten_fun_nokwargs2(fun, in_tree)
A:jax._src.api.(jvp_fun, aux)->jax.interpreters.ad.jvp(flat_fun, has_aux=True)
A:jax._src.api.(out_tree, aux_tree)->out_aux_trees()
A:jax._src.api.(primals_flat, in_tree)->tree_flatten(primals)
A:jax._src.api.(out_primals, out_pvals, jaxpr, consts)->jax.interpreters.ad.linearize(jaxtree_fun, *primals_flat)
A:jax._src.api.out_primal_py->tree_unflatten(out_tree, out_primal)
A:jax._src.api.primal_avals->list(map(core.get_aval, primals_flat))
A:jax._src.api.lifted_jvp->Partial(partial(_lift_linearized, jaxpr, primal_avals, (in_tree, out_tree), out_pvals), consts)
A:jax._src.api.tangent_avals->list(map(core.get_aval, tangents))
A:jax._src.api.tangents_out->eval_jaxpr(jaxpr, consts, *tangents)
A:jax._src.api.tangents_out_->iter(tangents_out)
A:jax._src.api.expected_tangent_dtype->jax.core.primal_dtype_to_tangent_dtype(_dtype(arg))
A:jax._src.api.ans->fun(*args)
A:jax._src.api.(out_primal, out_vjp)->jax.interpreters.ad.vjp(flat_fun, primals_flat, reduce_axes=reduce_axes)
A:jax._src.api.(out_primal, out_vjp, aux)->jax.interpreters.ad.vjp(flat_fun, primals_flat, has_aux=True, reduce_axes=reduce_axes)
A:jax._src.api.vjp_py->Partial(partial(_vjp_pullback_wrapper, ct_dtypes, ct_shapes, (out_tree, in_tree)), out_vjp)
A:jax._src.api.in_avals->map(shaped_abstractify, primals_flat)
A:jax._src.api.in_dtypes->map(dtypes.dtype, in_avals)
A:jax._src.api.in_pvals->map(pe.PartialVal.unknown, in_avals)
A:jax._src.api.(jaxpr, out_pvals, const)->jax.interpreters.partial_eval.trace_to_jaxpr_nounits(flat_fun, in_pvals, instantiate=True)
A:jax._src.api.(out_avals, _)->unzip2(out_type)
A:jax._src.api.out_dtypes->map(dtypes.dtype, out_avals)
A:jax._src.api.(out_cts, out_tree2)->tree_flatten(out_cotangent)
A:jax._src.api.in_cts->map(ad.instantiate_zeros, in_cts)
A:jax._src.api.(flat_args, in_tree)->tree_flatten((args, kwargs))
A:jax._src.api.axes_specs->_flat_axes_specs(abstracted_axes, *args, **kwargs)
A:jax._src.api.(in_avals, keep_inputs)->unzip2(in_type)
A:jax._src.api.(in_avals, in_tree, keep_inputs)->abstractify(args, kwargs)
A:jax._src.api.(f, out_tree)->flatten_fun(f, in_tree)
A:jax._src.api.(jaxpr, out_type, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic2(f)
A:jax._src.api.closed_jaxpr->jax.core.ClosedJaxpr(jaxpr, consts)
A:jax._src.api.(a1, a2)->next(((a1, a2) for (a1, a2) in zip(avals[:-1], avals[1:]) if a1 != a2))
A:jax._src.api.stacked_aval->avals[0].update(shape=(len(devices),) + avals[0].shape)
A:jax._src.api.sharding_spec->jax.interpreters.pxla._create_pmap_sharding_spec(aval)
A:jax._src.api.(buf,)->jax._src.dispatch.device_put(x, devices[0])
A:jax._src.api.size->property(lambda self: prod(self.shape))
A:jax._src.api.named->frozenset(self.named_shape.items())
A:jax._src.api.(wrapped_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax._src.api.debug_info->jax.interpreters.partial_eval.debug_info(fun, in_tree, True, 'eval_shape')
A:jax._src.api.(_, in_tree)->tree_flatten(())
A:jax._src.api.lu_f->jax.linear_util.wrap_init(lambda : fun(*args, **kwargs))
A:jax._src.api.(flat_f, out_tree)->flatten_fun_nokwargs(lu_f, in_tree)
jax.ShapeDtypeStruct(self,shape,dtype,named_shape=None,sharding=None)
jax.ShapeDtypeStruct.__eq__(self,other)
jax.ShapeDtypeStruct.__hash__(self)
jax.ShapeDtypeStruct.__len__(self)
jax.ShapeDtypeStruct.__repr__(self)
jax._src.api.PmapCallInfo(NamedTuple)
jax._src.api.ShapeDtypeStruct(self,shape,dtype,named_shape=None,sharding=None)
jax._src.api.ShapeDtypeStruct.__eq__(self,other)
jax._src.api.ShapeDtypeStruct.__hash__(self)
jax._src.api.ShapeDtypeStruct.__init__(self,shape,dtype,named_shape=None,sharding=None)
jax._src.api.ShapeDtypeStruct.__len__(self)
jax._src.api.ShapeDtypeStruct.__repr__(self)
jax._src.api._BackendAndDeviceInfo(NamedTuple)
jax._src.api._FastpathData(NamedTuple)
jax._src.api._PmapFastpathData(NamedTuple)
jax._src.api._check_arg(arg)
jax._src.api._check_callable(fun)
jax._src.api._check_input_dtype_jacfwd(holomorphic:bool,x:Any)->None
jax._src.api._check_input_dtype_revderiv(name,holomorphic,allow_int,x)
jax._src.api._check_output_dtype_jacfwd(holomorphic,x)
jax._src.api._check_output_dtype_revderiv(name,holomorphic,x)
jax._src.api._check_scalar(x)
jax._src.api._cpp_jit(fun:Callable,*,static_argnums:Tuple[int,...],static_argnames:Tuple[str,...],device:Optional[xc.Device],backend:Optional[str],donate_argnums:Tuple[int,...],inline:bool,keep_unused:bool)->stages.Wrapped
jax._src.api._cpp_jit_clear_cache(self)
jax._src.api._cpp_pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices:Optional[Sequence[xc.Device]]=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=(),global_arg_shapes:Optional[Tuple[Tuple[int,...],...]]=None)->Any
jax._src.api._device_array_use_fast_path(execute,out_pytree_def,args_flat,out_flat)
jax._src.api._device_get(x)
jax._src.api._flat_axes_specs(abstracted_axes,*args,**kwargs)->List[pe.AbstractedAxesSpec]
jax._src.api._get_f_mapped(*,fun:Callable,axis_name:Optional[AxisName],in_axes=0,out_axes=0,static_broadcasted_tuple:Tuple[int,...],devices:Optional[Sequence[xc.Device]],backend:Optional[str],axis_size:Optional[int],donate_tuple:Tuple[int,...],global_arg_shapes:Optional[Tuple[Tuple[int,...],...]])
jax._src.api._infer_argnums_and_argnames(sig:inspect.Signature,argnums:Union[int,Iterable[int],None],argnames:Union[str,Iterable[str],None])->Tuple[Tuple[int, ...], Tuple[str, ...]]
jax._src.api._isgeneratorfunction(fun)
jax._src.api._jacfwd_unravel(input_pytree,output_pytree_leaf,arr)
jax._src.api._jacrev_unravel(output_pytree,input_pytree_leaf,arr)
jax._src.api._jax_array_use_fast_path(execute,out_pytree_def,args_flat,out_flat)
jax._src.api._jit(use_cpp_jit:bool,fun:Callable,static_argnums:Union[int,Iterable[int],None]=None,static_argnames:Union[str,Iterable[str],None]=None,device:Optional[xc.Device]=None,backend:Optional[str]=None,donate_argnums:Union[int,Iterable[int]]=(),inline:bool=False,keep_unused:bool=False,abstracted_axes:Optional[Any]=None)->stages.Wrapped
jax._src.api._jit_lower(fun,static_argnums,static_argnames,device,backend,donate_argnums,inline,keep_unused:bool,abstracted_axes:Optional[PytreeOfAbstractedAxesSpec])
jax._src.api._jvp(fun:lu.WrappedFun,primals,tangents,has_aux=False)
jax._src.api._lift_linearized(jaxpr,primal_avals,io_tree,out_pvals,consts,*py_args)
jax._src.api._mapped_axis_size(tree,vals,dims,name,*,kws=False)
jax._src.api._nan_check_posthook(fun,args,kwargs,output)
jax._src.api._pmap_lower(fun,axis_name,in_axes,out_axes,static_broadcasted_tuple,devices,backend,axis_size,global_arg_shapes,donate_tuple)
jax._src.api._possible_downcast(x,example)
jax._src.api._prepare_jit(fun,static_argnums,static_argnames,donate_argnums,args,kwargs)
jax._src.api._prepare_pmap(fun,in_axes,out_axes,static_broadcasted_tuple,donate_tuple,global_arg_shapes,in_devices,args,kwargs)
jax._src.api._python_jit(fun:Callable,*,static_argnums:Tuple[int,...],static_argnames:Tuple[str,...],device:Optional[xc.Device],backend:Optional[str],donate_argnums:Tuple[int,...],inline:bool,keep_unused:bool,abstracted_axes:Optional[PytreeOfAbstractedAxesSpec])->stages.Wrapped
jax._src.api._python_pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices:Optional[Sequence[xc.Device]]=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=(),global_arg_shapes:Optional[Tuple[Tuple[int,...],...]]=None)->stages.Wrapped
jax._src.api._shared_code_pmap(fun,axis_name,static_broadcasted_argnums,donate_argnums,in_axes,out_axes)
jax._src.api._split(x,indices,axis)
jax._src.api._std_basis(pytree)
jax._src.api._unravel_array_into_pytree(pytree,axis,example,arr)
jax._src.api._update_debug_special_global(_)
jax._src.api._update_debug_special_thread_local(_)
jax._src.api._valid_jaxtype(arg)
jax._src.api._vjp(fun:lu.WrappedFun,*primals,has_aux=False,reduce_axes=())
jax._src.api._vjp_pullback_wrapper(cotangent_dtypes,cotangent_shapes,io_tree,fun,py_args)
jax._src.api.block_until_ready(x)
jax._src.api.checkpoint(fun:Callable,*,concrete:bool=False,prevent_cse:bool=True,static_argnums:Union[int,Tuple[int,...]]=(),policy:Optional[Callable[...,bool]]=None)->Callable
jax._src.api.clear_backends()
jax._src.api.device_get(x:Any)
jax._src.api.device_put(x,device:Optional[Union[xc.Device,jax.sharding.Sharding]]=None)
jax._src.api.device_put_replicated(x:Any,devices:Sequence[xc.Device])
jax._src.api.device_put_sharded(shards:Sequence[Any],devices:Sequence[xc.Device])
jax._src.api.disable_jit(disable:bool=True)
jax._src.api.effects_barrier()
jax._src.api.eval_shape(fun:Callable,*args,**kwargs)
jax._src.api.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable
jax._src.api.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax._src.api.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax._src.api.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False)->Callable
jax._src.api.jit(fun:Callable,*,static_argnums:Union[int,Iterable[int],None]=None,static_argnames:Union[str,Iterable[str],None]=None,device:Optional[xc.Device]=None,backend:Optional[str]=None,donate_argnums:Union[int,Iterable[int]]=(),inline:bool=False,keep_unused:bool=False,abstracted_axes:Optional[Any]=None)->stages.Wrapped
jax._src.api.jvp(fun:Callable,primals,tangents,has_aux:bool=False)->Tuple[Any, ...]
jax._src.api.linear_transpose(fun:Callable,*primals,reduce_axes=())->Callable
jax._src.api.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax._src.api.make_jaxpr(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,return_shape:bool=False,abstracted_axes:Optional[Any]=None)->Callable[..., core.ClosedJaxpr]
jax._src.api.named_call(fun:Callable[...,Any],*,name:Optional[str]=None)->Callable[..., Any]
jax._src.api.named_scope(name:str)->Generator[None, None, None]
jax._src.api.pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices:Optional[Sequence[xc.Device]]=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=(),global_arg_shapes:Optional[Tuple[Tuple[int,...],...]]=None)->Any
jax._src.api.pure_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,**kwargs:Any)
jax._src.api.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable[..., Tuple[Any, Any]]
jax._src.api.vjp(fun:Callable,*primals,has_aux:bool=False,reduce_axes=())->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax._src.api.vmap(fun:F,in_axes:Union[int,Sequence[Any]]=0,out_axes:Any=0,axis_name:Optional[Hashable]=None,axis_size:Optional[int]=None,spmd_axis_name:Optional[Hashable]=None)->F
jax._src.api.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,in_parts=None,out_parts=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:Optional[bool]=None,return_shape:bool=False,donate_argnums:Union[int,Iterable[int]]=())->Callable
jax.block_until_ready(x)
jax.checkpoint(fun:Callable,*,concrete:bool=False,prevent_cse:bool=True,static_argnums:Union[int,Tuple[int,...]]=(),policy:Optional[Callable[...,bool]]=None)->Callable
jax.clear_backends()
jax.device_get(x:Any)
jax.device_put(x,device:Optional[Union[xc.Device,jax.sharding.Sharding]]=None)
jax.device_put_replicated(x:Any,devices:Sequence[xc.Device])
jax.device_put_sharded(shards:Sequence[Any],devices:Sequence[xc.Device])
jax.disable_jit(disable:bool=True)
jax.effects_barrier()
jax.eval_shape(fun:Callable,*args,**kwargs)
jax.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable
jax.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False)->Callable
jax.jit(fun:Callable,*,static_argnums:Union[int,Iterable[int],None]=None,static_argnames:Union[str,Iterable[str],None]=None,device:Optional[xc.Device]=None,backend:Optional[str]=None,donate_argnums:Union[int,Iterable[int]]=(),inline:bool=False,keep_unused:bool=False,abstracted_axes:Optional[Any]=None)->stages.Wrapped
jax.jvp(fun:Callable,primals,tangents,has_aux:bool=False)->Tuple[Any, ...]
jax.linear_transpose(fun:Callable,*primals,reduce_axes=())->Callable
jax.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax.make_jaxpr(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,return_shape:bool=False,abstracted_axes:Optional[Any]=None)->Callable[..., core.ClosedJaxpr]
jax.named_call(fun:Callable[...,Any],*,name:Optional[str]=None)->Callable[..., Any]
jax.named_scope(name:str)->Generator[None, None, None]
jax.pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,out_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices:Optional[Sequence[xc.Device]]=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=(),global_arg_shapes:Optional[Tuple[Tuple[int,...],...]]=None)->Any
jax.pure_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,**kwargs:Any)
jax.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False,allow_int:bool=False,reduce_axes:Sequence[AxisName]=())->Callable[..., Tuple[Any, Any]]
jax.vjp(fun:Callable,*primals,has_aux:bool=False,reduce_axes=())->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax.vmap(fun:F,in_axes:Union[int,Sequence[Any]]=0,out_axes:Any=0,axis_name:Optional[Hashable]=None,axis_size:Optional[int]=None,spmd_axis_name:Optional[Hashable]=None)->F
jax.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,in_parts=None,out_parts=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:Optional[bool]=None,return_shape:bool=False,donate_argnums:Union[int,Iterable[int]]=())->Callable


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/custom_batching.py----------------------------------------
A:jax._src.custom_batching.(args_flat, in_tree)->tree_flatten(args)
A:jax._src.custom_batching.(flat_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(self.fun), in_tree)
A:jax._src.custom_batching.debug->jax.interpreters.partial_eval.debug_info(self.fun, in_tree, False, 'custom_vmap')
A:jax._src.custom_batching.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_fun, in_avals, debug)
A:jax._src.custom_batching.closed_call->jax.core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr), ())
A:jax._src.custom_batching.out_flat->jax.core.Primitive('custom_vmap_call').bind(*consts, *args_flat, call=closed_call, rule=self.vmap_rule, in_tree=in_tree, out_tree=out_tree())
A:jax._src.custom_batching.(f, out_axes)->jax.interpreters.batching.batch_subtrace(f)
A:jax._src.custom_batching.f->custom_vmap(f)
A:jax._src.custom_batching.outs->jax.core.Primitive('custom_vmap_call').bind(*primals, *tangents, call=jvp_call, rule=jvp_of_rule_rule, in_tree=jvp_in_tree, out_tree=jvp_out_tree)
A:jax._src.custom_batching.args_flat->map(maybe_bdim_at_front, args_flat, dims)
A:jax._src.custom_batching.args->tree_merge(in_batched, mapped_args, bcast_args)
A:jax._src.custom_batching.in_batched->tree_unflatten(in_tree, flat_in_batched)
A:jax._src.custom_batching.(out, out_batched)->call_rule(rule, axis_size, mutually_batched, primals)
A:jax._src.custom_batching.(flat_outs, tree1)->tree_flatten(out)
A:jax._src.custom_batching.(flat_out_batched, tree2)->tree_flatten(out_batched)
A:jax._src.custom_batching.mutually_batched->tree_map(operator.and_, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.extra_batched_ps->tree_map(lambda pb, tb: 0 if pb and (not tb) else None, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.extra_batched_ts->tree_map(lambda pb, tb: 0 if tb and (not pb) else None, in_batched_ps, in_batched_ts)
A:jax._src.custom_batching.out_mutually_batched->jax.linear_util.Store()
A:jax._src.custom_batching.(flat_ps_ts, tree_ps_ts)->tree_flatten((primals, tangents))
A:jax._src.custom_batching.(flat_extra_batched_ps_ts, tree_ps_ts2)->tree_flatten((extra_batched_ps, extra_batched_ts), is_leaf=lambda x: x is None)
A:jax._src.custom_batching.(to_vmap_over_extra_batched_dims_flat, out_tree2)->flatten_fun_nokwargs(lu.wrap_init(to_vmap_over_extra_batched_dims), tree_ps_ts)
A:jax._src.custom_batching.(flat_out_ps_ts, flat_out_axes)->vmap_unrestricted(to_vmap_over_extra_batched_dims_flat, *flat_ps_ts, in_axes=flat_extra_batched_ps_ts, axis_name=core.no_axis_name, axis_size=axis_size)
A:jax._src.custom_batching.(n, ragged)->divmod(len(flat_out_ps_ts), 2)
A:jax._src.custom_batching.flat_out_ps->map(maybe_bdim_at_front, flat_out_ps, flat_out_axes_p)
A:jax._src.custom_batching.flat_out_ts->map(maybe_bdim_at_front, flat_out_ts, flat_out_axes_t)
A:jax._src.custom_batching.(out_ps, out_ts)->tree_unflatten(out_tree2(), [*flat_out_ps, *flat_out_ts])
A:jax._src.custom_batching.(out_extra_batched_ps, out_extra_batched_ts)->tree_unflatten(out_tree2(), [*flat_out_extra_batched_ps, *flat_out_extra_batched_ts])
A:jax._src.custom_batching.out_batched_ps->tree_map(operator.or_, out_mutually_batched.val, out_extra_batched_ps)
A:jax._src.custom_batching.out_batched_ts->tree_map(operator.or_, out_mutually_batched.val, out_extra_batched_ts)
A:jax._src.custom_batching.tangents->map(ad.instantiate_zeros, tangents)
A:jax._src.custom_batching.(jvp_call, _)->jax.interpreters.ad.jvp_jaxpr(call, [True] * len(primals), True)
A:jax._src.custom_batching.jvp_in_tree->treedef_tuple((in_tree, in_tree))
A:jax._src.custom_batching.jvp_out_tree->treedef_tuple((out_tree, out_tree))
A:jax._src.custom_batching.(out_primals, out_tangents)->jax._src.util.split_list(outs, [len(outs) // 2])
A:jax._src.custom_batching.custom_vmap_p->jax.core.Primitive('custom_vmap_call')
A:jax._src.custom_batching.lhs->tree_map(lambda l, x: x if l else None, mask, tree)
A:jax._src.custom_batching.rhs->tree_map(lambda l, x: None if l else x, mask, tree)
A:jax._src.custom_batching.(mapped_args, bcast_args)->tree_split(in_batched, list(args))
A:jax._src.custom_batching.out->jax.lax.map(to_map, mapped_args)
A:jax._src.custom_batching.out_batched->tree_map(lambda _: True, out)
jax._src.custom_batching.call_rule(rule,axis_size,in_batched,args)
jax._src.custom_batching.check_vmap_rule_trees(rule,original_out_tree,out_tree,out_batched_tree)
jax._src.custom_batching.custom_vmap(self,fun:Callable)
jax._src.custom_batching.custom_vmap.__init__(self,fun:Callable)
jax._src.custom_batching.custom_vmap.def_vmap(self,vmap_rule:Callable)->Callable
jax._src.custom_batching.custom_vmap_abstract_eval(*in_avals,call,**_)
jax._src.custom_batching.custom_vmap_batching(args_flat,dims,*,call,rule,in_tree,out_tree)
jax._src.custom_batching.custom_vmap_impl(*args,call,rule,in_tree,out_tree)
jax._src.custom_batching.custom_vmap_jvp(primals,tangents,*,call,rule,in_tree,out_tree)
jax._src.custom_batching.ensure_list(xs)
jax._src.custom_batching.maybe_bdim_at_front(x,bdim)
jax._src.custom_batching.rule_name(rule)
jax._src.custom_batching.sequential_vmap(f)
jax._src.custom_batching.tree_merge(mask,lhs_tree,rhs_tree)
jax._src.custom_batching.tree_split(mask,tree)
jax._src.custom_batching.vmap_unrestricted(f:lu.WrappedFun,*args,in_axes,axis_name,axis_size)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax_reference.py----------------------------------------
A:jax._src.lax_reference.quotient->numpy.floor_divide(lhs, rhs)
A:jax._src.lax_reference.select->numpy.logical_and(np.sign(lhs) != np.sign(rhs), np.remainder(lhs, rhs) != 0)
A:jax._src.lax_reference.iinfo->numpy.iinfo(x.dtype)
A:jax._src.lax_reference.x->x.view(f'uint{np.iinfo(x.dtype).bits}').view(f'uint{np.iinfo(x.dtype).bits}')
A:jax._src.lax_reference.m->list(map(np.uint32, m[:-1]))
A:jax._src.lax_reference.bits->(x[..., None] & mask).astype(np.bool_)
A:jax._src.lax_reference.out->numpy.full(operand.shape[:2] + tuple(outspace), fill_value, operand.dtype)
A:jax._src.lax_reference.pads->padtype_to_pads(op.shape, dims, strides, padding)
A:jax._src.lax_reference.(lhs_perm, rhs_perm, out_perm)->_conv_general_permutations(dimension_numbers)
A:jax._src.lax_reference.padding->padtype_to_pads(np.take(lhs.shape, lhs_perm)[2:], np.take(rhs.shape, rhs_perm)[2:], window_strides, padding)
A:jax._src.lax_reference.trans_lhs->transpose(lhs, lhs_perm)
A:jax._src.lax_reference.trans_rhs->transpose(rhs, rhs_perm)
A:jax._src.lax_reference.new_id->itertools.count()
A:jax._src.lax_reference.shared_id->next(new_id)
A:jax._src.lax_reference.out_axis_ids->filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)
A:jax._src.lax_reference.in_reshape->numpy.ones(len(shape), dtype=np.int32)
A:jax._src.lax_reference.dimensions->frozenset(dimensions)
A:jax._src.lax_reference.(lo, hi, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax_reference.outshape->numpy.add(np.add(np.add(lo_pos, hi_pos), operand.shape), np.multiply(interior, np.subtract(operand.shape, 1)))
A:jax._src.lax_reference.lhs_slices->tuple((_slice(None, None, step) for step in factors))
A:jax._src.lax_reference.trim_slices->tuple((_slice(-l if l < 0 else 0, h if h < 0 else None) for (l, h) in zip(lo, hi)))
A:jax._src.lax_reference.strides->numpy.ones(len(start_indices)).astype(int)
A:jax._src.lax_reference.slices->tuple((_slice(abs(lo) if lo < 0 else 0, hi % dim if hi < 0 else None) for ((lo, hi), dim) in zip(pads, np.shape(arr))))
A:jax._src.lax_reference.idx->tuple((_slice(start, start + size) for (start, size) in zip(start_indices, slice_sizes)))
A:jax._src.lax_reference.updated_operand->numpy.copy(operand)
A:jax._src.lax_reference.reducer->_make_reducer(computation, init_value)
A:jax._src.lax_reference.op->_dilate(op, base_dilation, init_value)
A:jax._src.lax_reference.view->numpy.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)
A:jax._src.lax_reference.idxs->list(np.ix_(*[np.arange(d) for d in keys.shape]))
A:jax._src.lax_reference.idxs[dimension]->numpy.argsort(keys, axis=dimension)
A:jax._src.lax_reference.(view, view_axes, rhs_axes, out_axes)->_conv_view(lhs, rhs.shape, window_strides, pads, 0.0)
A:jax._src.lax_reference.out_shape->numpy.ceil(np.true_divide(in_shape, window_strides)).astype(int)
A:jax._src.lax_reference.lhs->_pad(lhs, [(0, 0)] * 2 + list(pads), pad_value)
A:jax._src.lax_reference.dim->len(filter_shape)
A:jax._src.lax_reference.out_strides->numpy.multiply(window_strides, lhs.strides[2:])
A:jax._src.lax_reference.view_axes->list(range(view.ndim))
A:jax._src.lax_reference.outspace->numpy.add(operand.shape[2:], np.multiply(np.subtract(factors, 1), np.subtract(operand.shape[2:], 1)))
A:jax._src.lax_reference.monoid_record->_monoids.get(getattr(py_binop, '__name__'))
A:jax._src.lax_reference.MonoidRecord->collections.namedtuple('MonoidRecord', ['reducer', 'identity'])
A:jax._src.lax_reference.result->numpy.full(np.delete(np.shape(operand), axis), init_val, dtype=np.asarray(operand).dtype)
A:jax._src.lax_reference.out_idx->tuple(np.delete(idx, axis))
A:jax._src.lax_reference.result[out_idx]->py_binop(result[out_idx], operand[idx])
jax._src.lax_reference._conv(lhs,rhs,window_strides,pads)
jax._src.lax_reference._conv_general_permutations(dimension_numbers)
jax._src.lax_reference._conv_view(lhs,rhs_shape,window_strides,pads,pad_value)
jax._src.lax_reference._dilate(operand,factors,fill_value=0)
jax._src.lax_reference._get_max_identity(dt)
jax._src.lax_reference._get_min_identity(dt)
jax._src.lax_reference._identity_getter(op)
jax._src.lax_reference._make_reducer(py_binop,init_val)
jax._src.lax_reference._pad(arr,pads,pad_value)
jax._src.lax_reference._reducer_from_pyfunc(py_binop,init_val)
jax._src.lax_reference.bessel_i0e(x)
jax._src.lax_reference.bessel_i1e(x)
jax._src.lax_reference.betainc(a,b,x)
jax._src.lax_reference.bitcast_convert_type(operand,dtype)
jax._src.lax_reference.broadcast(operand,sizes)
jax._src.lax_reference.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax._src.lax_reference.clamp(min,operand,max)
jax._src.lax_reference.clz(x)
jax._src.lax_reference.complex(x,y)
jax._src.lax_reference.concatenate(operands,dimension)
jax._src.lax_reference.conj(x)
jax._src.lax_reference.conv(lhs,rhs,window_strides,padding)
jax._src.lax_reference.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers)
jax._src.lax_reference.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation)
jax._src.lax_reference.convert_element_type(operand,dtype)
jax._src.lax_reference.digamma(x)
jax._src.lax_reference.div(lhs,rhs)
jax._src.lax_reference.dot_general(lhs,rhs,dimension_numbers)
jax._src.lax_reference.dynamic_slice(operand,start_indices,slice_sizes)
jax._src.lax_reference.dynamic_update_slice(operand,update,start_indices)
jax._src.lax_reference.erf(x)
jax._src.lax_reference.erf_inv(x)
jax._src.lax_reference.erfc(x)
jax._src.lax_reference.lgamma(x)
jax._src.lax_reference.logistic(x)
jax._src.lax_reference.pad(operand,padding_value,padding_config)
jax._src.lax_reference.padtype_to_pads(in_shape,filter_shape,window_strides,padding)
jax._src.lax_reference.population_count(x)
jax._src.lax_reference.reduce(operand,init_value,computation,dimensions)
jax._src.lax_reference.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding,base_dilation)
jax._src.lax_reference.rem(lhs,rhs)
jax._src.lax_reference.reshape(operand,new_sizes,dimensions=None)
jax._src.lax_reference.rev(operand,dimensions)
jax._src.lax_reference.round(x)
jax._src.lax_reference.slice(operand,start_indices,limit_indices,strides=None)
jax._src.lax_reference.sort_key_val(keys,values,dimension=-1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/prng.py----------------------------------------
A:jax._src.prng.ndim->len(impl.key_shape)
A:jax._src.prng._->self._base_array.block_until_ready()
A:jax._src.prng._device->property(op.attrgetter('_base_array._device'))
A:jax._src.prng._committed->property(op.attrgetter('_base_array._committed'))
A:jax._src.prng.sharding->property(op.attrgetter('_base_array.sharding'))
A:jax._src.prng.base_ndim->len(impl.key_shape)
A:jax._src.prng.reshaped_base->jax.numpy.reshape(self._base_array, (*newshape, -1), order=order)
A:jax._src.prng.axis->canonicalize_axis(axis, self.ndim)
A:jax._src.prng.(phys_aval,)->KeyTyRules.physical_avals(aval)
A:jax._src.prng.op_sharding->property(op.attrgetter('_base_array.sharding'))._to_xla_op_sharding(aval.ndim)
A:jax._src.prng.new_op_sharding->jax._src.lib.xla_client.OpSharding()
A:jax._src.prng.tad->list(op_sharding.tile_assignment_dimensions)
A:jax._src.prng.buf.aval->jax.core.ShapedArray(buf.shape, buf.dtype)
A:jax._src.prng.phys_sharding_spec->jax.interpreters.pxla.ShardingSpec(sharding=(*sharding.sharding_spec.sharding, *trailing_sharding), mesh_mapping=sharding.sharding_spec.mesh_mapping)
A:jax._src.prng.phys_sharding->OpShardingSharding(out_sharding._device_assignment, KeyTyRules.physical_op_sharding(aval, out_sharding))
A:jax._src.prng.phys_handler->phys_handler_maker(phys_aval, phys_sharding, committed, is_out_sharding_from_xla)
A:jax._src.prng.dtype->jax._src.dtypes.canonicalize_dtype(np.dtype('int64'))
A:jax._src.prng.slice_sizes_->jax.interpreters.mlir.dense_int_elements((*slice_sizes, *key_shape))
A:jax._src.prng.dimension_numbers->dimension_numbers._replace(offset_dims=(*dimension_numbers.offset_dims, *trailing_offset_dims))._replace(offset_dims=(*dimension_numbers.offset_dims, *trailing_offset_dims))
A:jax._src.prng.gather_lower->partial(lax_internal.slicing._gather_lower, dimension_numbers=dimension_numbers, slice_sizes=slice_sizes, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)
A:jax._src.prng.arr->threefry2x32_p.bind(key1, key2, x[0], x[1]).unsafe_raw_array()
A:jax._src.prng.f->squeeze_vmap(f, sz1 == 1)
A:jax._src.prng.x->jax.core.Primitive('threefry2x32').bind(key1, key2, x[0], x[1])
A:jax._src.prng.y->jax.numpy.squeeze(y, axis=0)
A:jax._src.prng.seeds_arr->jax.numpy.asarray(seeds)
A:jax._src.prng.random_seed_p->jax.core.Primitive('random_seed')
A:jax._src.prng.base_arr->random_fold_in_impl_base(keys.impl, keys.unsafe_raw_array(), msgs, keys.shape)
A:jax._src.prng.seed->iterated_vmap_unary(aval.ndim, impl.seed)
A:jax._src.prng.seed_lowering->jax.interpreters.mlir.lower_fun(seed, multiple_results=False)
A:jax._src.prng.random_split_p->jax.core.Primitive('random_split')
A:jax._src.prng.split->iterated_vmap_unary(aval.ndim, lambda k: impl.split(k, count))
A:jax._src.prng.split_lowering->jax.interpreters.mlir.lower_fun(split, multiple_results=False)
A:jax._src.prng.random_fold_in_p->jax.core.Primitive('random_fold_in')
A:jax._src.prng.shape->jax._src.lax.lax.broadcasting_shape_rule(*args)
A:jax._src.prng.named_shape->jax.core.join_named_shapes(*(a.named_shape for a in args))
A:jax._src.prng.fold_in->iterated_vmap_binary_bcast(keys_aval.shape, msgs_aval.shape, impl.fold_in)
A:jax._src.prng.fold_in_lowering->jax.interpreters.mlir.lower_fun(fold_in, multiple_results=False)
A:jax._src.prng.real_size->jax.lax.psum(1, name)
A:jax._src.prng.axis_index->jax.lax.axis_index(name)
A:jax._src.prng.keys->threefry_split(key, nblocks + 1)
A:jax._src.prng.random_bits_p->jax.core.Primitive('random_bits')
A:jax._src.prng.out_dtype->jax._src.dtypes.dtype(f'uint{bit_width}')
A:jax._src.prng.bits->jax.lax.reshape(bits, (max_count * 32 // bit_width,), (1, 0))
A:jax._src.prng.bits_lowering->jax.interpreters.mlir.lower_fun(bits, multiple_results=False)
A:jax._src.prng.ctx_new->ctx.replace(avals_in=[keys_aval_to_base_arr_aval(aval)])
A:jax._src.prng.out->jax.numpy.concatenate(x)
A:jax._src.prng.random_wrap_p->jax.core.Primitive('random_wrap')
A:jax._src.prng.random_unwrap_p->jax.core.Primitive('random_unwrap')
A:jax._src.prng.k1->convert(lax.shift_right_logical(seed, lax_internal._const(seed, 32)))
A:jax._src.prng.k2->convert(jnp.bitwise_and(seed, np.uint32(4294967295)))
A:jax._src.prng.nbits->numpy.array(jnp.iinfo(dtype).bits, dtype)
A:jax._src.prng.d->jax.lax.convert_element_type(d, dtype)
A:jax._src.prng.aval->jax.core.UnshapedArray(jnp.dtype(jnp.uint32))
A:jax._src.prng.rotate_left->_make_rotate_left(np.uint32)
A:jax._src.prng.v[1]->rotate_left(v[1], rot)
A:jax._src.prng.(x, _, _)->jax.lax.fori_loop(0, 5, rolled_loop_step, (x, rotate_list(ks), rotations))
A:jax._src.prng.rank->len(aval_out.shape)
A:jax._src.prng.zeros->jax.interpreters.mlir.full_like_aval(0, aval_out)
A:jax._src.prng.threefry2x32_p->jax.core.Primitive('threefry2x32')
A:jax._src.prng.counts->jax.lax.iota(np.uint32, num * 2)
A:jax._src.prng.size->prod(shape)
A:jax._src.prng.(max_count, r)->divmod(bit_width * size, 32)
A:jax._src.prng.(nblocks, rem)->divmod(max_count, jnp.iinfo(np.uint32).max)
A:jax._src.prng.blocks->vmap(threefry_2x32, in_axes=(0, None))(subkeys, lax.iota(np.uint32, jnp.iinfo(np.uint32).max))
A:jax._src.prng.last->threefry_2x32(last_key, lax.iota(np.uint32, rem))
A:jax._src.prng.threefry_prng_impl->PRNGImpl(key_shape=(2,), seed=threefry_seed, split=threefry_split, random_bits=threefry_random_bits, fold_in=threefry_fold_in, tag='fry')
A:jax._src.prng.halfkey->threefry_seed(seed)
A:jax._src.prng.(_, bits)->jax.lax.rng_bit_generator(key, shape, dtype=UINT_DTYPES[bit_width])
A:jax._src.prng.rbg_prng_impl->PRNGImpl(key_shape=(4,), seed=_rbg_seed, split=_rbg_split, random_bits=_rbg_random_bits, fold_in=_rbg_fold_in, tag='rbg')
A:jax._src.prng.(_, keys)->jax.lax.rng_bit_generator(key, (10 * num, 4), dtype='uint32')
A:jax._src.prng.(_, random_bits)->jax.lax.rng_bit_generator(_rbg_seed(data), (10, 4), dtype='uint32')
A:jax._src.prng.unsafe_rbg_prng_impl->PRNGImpl(key_shape=(4,), seed=_rbg_seed, split=_unsafe_rbg_split, random_bits=_rbg_random_bits, fold_in=_unsafe_rbg_fold_in, tag='urbg')
jax._src.prng.KeyTy(self,impl)
jax._src.prng.KeyTy.__eq__(self,other)
jax._src.prng.KeyTy.__hash__(self)->int
jax._src.prng.KeyTy.__init__(self,impl)
jax._src.prng.KeyTy.__repr__(self)->str
jax._src.prng.KeyTy.name(self)->str
jax._src.prng.KeyTyRules
jax._src.prng.KeyTyRules.aval_to_ir_types(aval)
jax._src.prng.KeyTyRules.broadcast_in_dim_mlir(ctx,x,*dyn_shape,shape,broadcast_dimensions)
jax._src.prng.KeyTyRules.dynamic_slice_mlir(ctx,x,start_indices,slice_sizes)
jax._src.prng.KeyTyRules.dynamic_update_slice_mlir(ctx,x,update,*start_indices)
jax._src.prng.KeyTyRules.empty_mlir(ctx)
jax._src.prng.KeyTyRules.gather_mlir(ctx,x,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.prng.KeyTyRules.global_sharded_result_handler(aval,out_sharding,committed,is_out_sharding_from_xla)
jax._src.prng.KeyTyRules.local_sharded_result_handler(aval,sharding,indices)
jax._src.prng.KeyTyRules.physical_avals(aval)
jax._src.prng.KeyTyRules.physical_op_sharding(aval,sharding)
jax._src.prng.KeyTyRules.result_handler(sticky_device,aval)
jax._src.prng.KeyTyRules.slice_mlir(ctx,x,start_indices,limit_indices,strides)
jax._src.prng.KeyTyRules.transpose_mlir(ctx,x,*,permutation)
jax._src.prng.PRNGImpl(NamedTuple)
jax._src.prng.PRNGImpl.__hash__(self)->int
jax._src.prng.PRNGImpl.__str__(self)->str
jax._src.prng.PRNGImpl.pprint(self)
jax._src.prng.PRNGKeyArray(self,impl,key_data:Any)
jax._src.prng.PRNGKeyArray.T(self)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.__getitem__(self,_)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.__init__(self,impl,key_data:Any)
jax._src.prng.PRNGKeyArray.__iter__(self)->Iterator['PRNGKeyArray']
jax._src.prng.PRNGKeyArray.__len__(self)
jax._src.prng.PRNGKeyArray.__repr__(self)
jax._src.prng.PRNGKeyArray._is_scalar(self)
jax._src.prng.PRNGKeyArray.block_until_ready(self)
jax._src.prng.PRNGKeyArray.broadcast_to(self,shape)
jax._src.prng.PRNGKeyArray.concatenate(self,key_arrs,axis,dtype=None)
jax._src.prng.PRNGKeyArray.dtype(self)
jax._src.prng.PRNGKeyArray.expand_dims(self,dimensions:Sequence[int])
jax._src.prng.PRNGKeyArray.flatten(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.ndim(self)
jax._src.prng.PRNGKeyArray.pprint(self)
jax._src.prng.PRNGKeyArray.ravel(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.reshape(self,newshape,order=None)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.shape(self)
jax._src.prng.PRNGKeyArray.squeeze(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.swapaxes(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.take(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.transpose(self,*_,**__)->'PRNGKeyArray'
jax._src.prng.PRNGKeyArray.unsafe_raw_array(self)
jax._src.prng.PRNGKeyArrayMeta(abc.ABCMeta)
jax._src.prng.PRNGKeyArrayMeta.__instancecheck__(self,instance)
jax._src.prng._bit_stats(bits)
jax._src.prng._check_prng_key_data(impl,key_data:jnp.ndarray)
jax._src.prng._is_threefry_prng_key(key:jnp.ndarray)->bool
jax._src.prng._make_rotate_left(dtype)
jax._src.prng._rbg_fold_in(key:jnp.ndarray,data:jnp.ndarray)->jnp.ndarray
jax._src.prng._rbg_random_bits(key:jnp.ndarray,bit_width:int,shape:Sequence[int])->jnp.ndarray
jax._src.prng._rbg_seed(seed:jnp.ndarray)->jnp.ndarray
jax._src.prng._rbg_split(key:jnp.ndarray,num:int)->jnp.ndarray
jax._src.prng._threefry2x32_abstract_eval(*args)
jax._src.prng._threefry2x32_gpu_lowering(threefry2x32_lowering,ctx,k1,k2,x1,x2)
jax._src.prng._threefry2x32_lowering(key1,key2,x1,x2,use_rolled_loops=True)
jax._src.prng._threefry_fold_in(key,data)
jax._src.prng._threefry_random_bits(key:jnp.ndarray,bit_width,shape)
jax._src.prng._threefry_split(key,num)->jnp.ndarray
jax._src.prng._unsafe_rbg_fold_in(key:jnp.ndarray,data:jnp.ndarray)->jnp.ndarray
jax._src.prng._unsafe_rbg_split(key:jnp.ndarray,num:int)->jnp.ndarray
jax._src.prng.apply_round(v,rot)
jax._src.prng.base_arr_shape_to_keys_shape(impl,base_arr_shape)
jax._src.prng.device_put_key_array(x:PRNGKeyArray,device)
jax._src.prng.iterated_vmap_binary_bcast(shape1,shape2,f)
jax._src.prng.iterated_vmap_unary(n,f)
jax._src.prng.key_array_constant_handler(x,canonicalize_dtypes)
jax._src.prng.key_array_shard_arg_handler(x:PRNGKeyArray,devices,indices,mode)
jax._src.prng.keys_aval_to_base_arr_aval(keys_aval)
jax._src.prng.keys_shaped_array(impl,shape)
jax._src.prng.random_bits(keys,bit_width,shape)
jax._src.prng.random_bits_abstract_eval(keys_aval,*,bit_width,shape)
jax._src.prng.random_bits_impl(keys,*,bit_width,shape)
jax._src.prng.random_bits_impl_base(impl,base_arr,keys_ndim,*,bit_width,shape)
jax._src.prng.random_bits_lowering(ctx,keys,*,bit_width,shape)
jax._src.prng.random_fold_in(keys,msgs)
jax._src.prng.random_fold_in_abstract_eval(keys_aval,msgs_aval)
jax._src.prng.random_fold_in_impl(keys,msgs)
jax._src.prng.random_fold_in_impl_base(impl,base_arr,msgs,keys_shape)
jax._src.prng.random_fold_in_lowering(ctx,keys,msgs)
jax._src.prng.random_seed(seeds,impl)
jax._src.prng.random_seed_abstract_eval(seeds_aval,*,impl)
jax._src.prng.random_seed_impl(seeds,*,impl)
jax._src.prng.random_seed_impl_base(seeds,*,impl)
jax._src.prng.random_seed_lowering(ctx,seeds,*,impl)
jax._src.prng.random_split(keys,count)
jax._src.prng.random_split_abstract_eval(keys_aval,*,count)
jax._src.prng.random_split_impl(keys,*,count)
jax._src.prng.random_split_impl_base(impl,base_arr,keys_ndim,*,count)
jax._src.prng.random_split_lowering(ctx,keys,*,count)
jax._src.prng.random_unwrap(keys)
jax._src.prng.random_unwrap_abstract_eval(keys_aval)
jax._src.prng.random_unwrap_impl(keys)
jax._src.prng.random_unwrap_lowering(ctx,keys)
jax._src.prng.random_wrap(base_arr,*,impl)
jax._src.prng.random_wrap_abstract_eval(base_arr_aval,*,impl)
jax._src.prng.random_wrap_batch_rule(batched_args,batch_dims,*,impl)
jax._src.prng.random_wrap_impl(base_arr,*,impl)
jax._src.prng.random_wrap_lowering(ctx,base_arr,*,impl)
jax._src.prng.rolled_loop_step(i,state)
jax._src.prng.rotate_list(xs)
jax._src.prng.seed_with_impl(impl:PRNGImpl,seed:int)->PRNGKeyArray
jax._src.prng.squeeze_vmap(f,left)
jax._src.prng.threefry_2x32(keypair,count)
jax._src.prng.threefry_fold_in(key:jnp.ndarray,data:jnp.ndarray)->jnp.ndarray
jax._src.prng.threefry_random_bits(key:jnp.ndarray,bit_width,shape)
jax._src.prng.threefry_seed(seed:jnp.ndarray)->jnp.ndarray
jax._src.prng.threefry_split(key:jnp.ndarray,num:int)->jnp.ndarray


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/config.py----------------------------------------
A:jax._src.config.val->getattr(_thread_local_state, name, unset)
A:jax._src.config.self.FLAGS->NameSpace(self.read, self.update)
A:jax._src.config.self._contextmanager_flags->set()
A:jax._src.config.hook->self._update_hooks.get(name, None)
A:jax._src.config.update_hook->kwargs.pop('update_hook', None)
A:jax._src.config.jax_argv->itertools.takewhile(lambda a: a != '--', sys.argv)
A:jax._src.config.name->name.lower().lower()
A:jax._src.config.default->os.getenv(name.upper(), default)
A:jax._src.config.tls->jax._src.lib.jax_jit.thread_local_state()
A:jax._src.config.no_default->NoDefault()
A:jax._src.config.prev_val->getattr(_thread_local_state, self._name, unset)
A:jax._src.config._thread_local_state->threading.local()
A:jax._src.config.unset->_Unset()
A:jax._src.config.config->Config()
A:jax._src.config.gs->jax._src.lib.jax_jit.global_state()
A:jax._src.config.gs.extra_jit_context->context._replace(**kw)
A:jax._src.config.self.canonicalize->functools.lru_cache(128)(lambda x: x)
A:jax._src.config._thread_local_state_cache->_ThreadLocalStateCache()
A:jax._src.config.tmp->context._replace(**kw)
A:jax._src.config.tls.extra_jit_context->_ThreadLocalStateCache().canonicalize(tmp)
A:jax._src.config.jax2tf_associative_scan_reductions->Config().define_bool_state(name='jax2tf_associative_scan_reductions', default=False, help='JAX has two separate lowering rules for the cumulative reduction primitives (cumsum, cumprod, cummax, cummin). On CPUs and GPUs it uses a lax.associative_scan, while for TPUs it uses the HLO ReduceWindow. The latter has a slow implementation on CPUs and GPUs. By default, jax2tf uses the TPU lowering. Set this flag to True to use the associative scan lowering usage, and only if it makes a difference for your application. See the jax2tf README.md for more details.')
A:jax._src.config.jax2tf_default_experimental_native_lowering->Config().define_bool_state(name='jax2tf_default_experimental_native_lowering', default=bool_env('JAX2TF_DEFAULT_EXPERIMENTAL_NATIVE_LOWERING', False), help='DO NOT USE, highly experimental. Sets the default value of the experimental_native_lowering parameter to jax2tf.convert.')
A:jax._src.config.jax_platforms->Config().define_string_state(name='jax_platforms', default=None, help='Comma-separated list of platform names specifying which platforms jax should initialize. If any of the platforms in this list are not successfully initialized, an exception will be raised and the program will be aborted. The first platform in the list will be the default platform. For example, config.jax_platforms=cpu,tpu means that CPU and TPU backends will be initialized, and the CPU backend will be used unless otherwise specified. If TPU initialization fails, it will raise an exception. By default, jax will try to initialize all available platforms and will default to GPU or TPU if available, and fallback to CPU otherwise.')
A:jax._src.config.enable_checks->Config().define_bool_state(name='jax_enable_checks', default=False, help='Turn on invariant checking for JAX internals. Makes things slower.')
A:jax._src.config.check_tracer_leaks->Config().define_bool_state(name='jax_check_tracer_leaks', default=False, help='Turn on checking for leaked tracers as soon as a trace completes. Enabling leak checking may have performance impacts: some caching is disabled, and other overheads may be added. Additionally, be aware that some Python debuggers can cause false positives, so it is recommended to disable any debuggers while leak checking is enabled.')
A:jax._src.config.checking_leaks->functools.partial(check_tracer_leaks, True)
A:jax._src.config.debug_nans->Config().define_bool_state(name='jax_debug_nans', default=False, help='Add nan checks to every operation. When a nan is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the nan.')
A:jax._src.config.debug_infs->Config().define_bool_state(name='jax_debug_infs', default=False, help='Add inf checks to every operation. When an inf is detected on the output of a jit-compiled computation, call into the un-compiled version in an attempt to more precisely identify the operation which produced the inf.')
A:jax._src.config.log_compiles->Config().define_bool_state(name='jax_log_compiles', default=False, help='Log a message each time every time `jit` or `pmap` compiles an XLA computation. Logging is performed with `absl.logging`. When this option is set, the log level is WARNING; otherwise the level is DEBUG.')
A:jax._src.config.parallel_functions_output_gda->Config().define_bool_state(name='jax_parallel_functions_output_gda', default=False, help='If True, pjit will output GDAs.')
A:jax._src.config.jax_array->Config().define_bool_state(name='jax_array', default=False, upgrade=True, update_global_hook=_update_jax_array_global, update_thread_local_hook=_update_jax_array_thread_local, help='If True, new pjit behavior will be enabled and `jax.Array` will be used.')
A:jax._src.config.distributed_debug->Config().define_bool_state(name='jax_distributed_debug', default=False, help='Enable logging useful for debugging multi-process distributed computations. Logging is performed with `absl.logging` at WARNING level.')
A:jax._src.config.enable_custom_prng->Config().define_bool_state(name='jax_enable_custom_prng', default=False, upgrade=True, help='Enables an internal upgrade that allows one to define custom pseudo-random number generator implementations.')
A:jax._src.config.default_prng_impl->Config().define_enum_state(name='jax_default_prng_impl', enum_values=['threefry2x32', 'rbg', 'unsafe_rbg'], default='threefry2x32', help='Select the default PRNG implementation, used when one is not explicitly provided at seeding time.')
A:jax._src.config.enable_custom_vjp_by_custom_transpose->Config().define_bool_state(name='jax_enable_custom_vjp_by_custom_transpose', default=False, upgrade=True, help='Enables an internal upgrade that implements `jax.custom_vjp` by reduction to `jax.custom_jvp` and `jax.custom_transpose`.')
A:jax._src.config.raise_persistent_cache_errors->Config().define_bool_state(name='jax_raise_persistent_cache_errors', default=False, help='If true, exceptions raised when reading or writing to the persistent compilation cache will be allowed through, halting program execution if not manually caught. If false, exceptions are caught and raised as warnings, allowing program execution to continue. Defaults to false so cache bugs or intermittent issues are non-fatal.')
A:jax._src.config.hlo_source_file_canonicalization_regex->Config().define_string_state(name='jax_hlo_source_file_canonicalization_regex', default=None, help='Used to canonicalize the source_path metadata of HLO instructions by removing the given regex. If set, re.sub() is called on each source_file with the given regex, and all matches are removed. This can be used to avoid spurious cache misses when using the persistent compilation cache, which includes HLO metadata in the cache key.')
A:jax._src.config.numpy_dtype_promotion->Config().define_enum_state(name='jax_numpy_dtype_promotion', enum_values=['standard', 'strict'], default='standard', help='Specify the rules used for implicit type promotion in operations between arrays. Options are "standard" or "strict"; in strict-mode, binary operations between arrays of differing strongly-specified dtypes will result in an error.', update_global_hook=lambda val: _update_global_jit_state(numpy_dtype_promotion=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(numpy_dtype_promotion=val))
A:jax._src.config.enable_x64->Config().define_bool_state(name='jax_enable_x64', default=False, help='Enable 64-bit types to be used', update_global_hook=_update_x64_global, update_thread_local_hook=_update_x64_thread_local)
A:jax._src.config.default_device->Config().define_string_or_object_state(name='jax_default_device', default=None, help='Configure the default device for JAX operations. Set to a Device object (e.g. ``jax.devices("cpu")[0]``) to use that Device as the default device for JAX operations and jit\'d function calls (there is no effect on multi-device computations, e.g. pmapped function calls). Set to None to use the system default device. See :ref:`faq-data-placement` for more information on device placement.', update_global_hook=_update_default_device_global, update_thread_local_hook=_update_default_device_thread_local, validate_new_val_hook=_validate_default_device)
A:jax._src.config.disable_jit->Config().define_bool_state(name='jax_disable_jit', default=False, help='Disable JIT compilation and just call original Python.', update_global_hook=_update_disable_jit_global, update_thread_local_hook=_update_disable_jit_thread_local)
A:jax._src.config.numpy_rank_promotion->Config().define_enum_state(name='jax_numpy_rank_promotion', enum_values=['allow', 'warn', 'raise'], default='allow', help='Control NumPy-style automatic rank promotion broadcasting ("allow", "warn", or "raise").', update_global_hook=lambda val: _update_global_jit_state(numpy_rank_promotion=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(numpy_rank_promotion=val))
A:jax._src.config.default_matmul_precision->Config().define_enum_state(name='jax_default_matmul_precision', enum_values=['bfloat16', 'tensorfloat32', 'float32'], default=None, help="Control the default matmul and conv precision for 32bit inputs.\n\nSome platforms, like TPU, offer configurable precision levels for matrix multiplication and convolution computations, trading off accuracy for speed. The precision can be controlled for each operation; for example, see the :func:`jax.lax.conv_general_dilated` and :func:`jax.lax.dot` docstrings. But it can be useful to control the default behavior obtained when an operation is not given a specific precision.\n\nThis option can be used to control the default precision level for computations involved in matrix multiplication and convolution on 32bit inputs. The levels roughly describe the precision at which scalar products are computed. The 'bfloat16' option is the fastest and least precise; 'float32' is similar to full float32 precision; 'tensorfloat32' is intermediate.\n\n", update_global_hook=lambda val: _update_global_jit_state(default_matmul_precision=val), update_thread_local_hook=lambda val: update_thread_local_jit_state(default_matmul_precision=val))
A:jax._src.config.traceback_filtering->Config().define_enum_state(name='jax_traceback_filtering', enum_values=['off', 'tracebackhide', 'remove_frames', 'auto'], default='auto', help='Controls how JAX filters internal frames out of tracebacks.\n\nValid values are:\n * "off": disables traceback filtering.\n * "auto": use "tracebackhide" if running under a sufficiently new IPython, or "remove_frames" otherwise.\n * "tracebackhide": adds "__tracebackhide__" annotations to  hidden stack frames, which some traceback printers support.\n * "remove_frames": removes hidden frames from tracebacks, and adds  the unfiltered traceback as a __cause__ of the exception.\n')
A:jax._src.config.bcoo_cusparse_lowering->Config().define_bool_state(name='jax_bcoo_cusparse_lowering', default=True, help='Enables lowering BCOO ops to cuSparse.')
A:jax._src.config.state->jax._src.lib.transfer_guard_lib.thread_local_state()
A:jax._src.config.transfer_guard_host_to_device->Config().define_enum_state(name='jax_transfer_guard_host_to_device', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for host-to-device transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'host_to_device', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'host_to_device', val))
A:jax._src.config.transfer_guard_device_to_device->Config().define_enum_state(name='jax_transfer_guard_device_to_device', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for device-to-device transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'device_to_device', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'device_to_device', val))
A:jax._src.config.transfer_guard_device_to_host->Config().define_enum_state(name='jax_transfer_guard_device_to_host', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for device-to-host transfers. Default is "allow".', update_global_hook=lambda val: _update_transfer_guard(transfer_guard_lib.global_state(), 'device_to_host', val), update_thread_local_hook=lambda val: _update_transfer_guard(transfer_guard_lib.thread_local_state(), 'device_to_host', val))
A:jax._src.config._transfer_guard->Config().define_enum_state(name='jax_transfer_guard', enum_values=['allow', 'log', 'disallow', 'log_explicit', 'disallow_explicit'], default=None, help='Select the transfer guard level for all transfers. This option is set-only; the transfer guard level for a specific direction should be read using the per-transfer direction option. Default is "allow".', update_global_hook=_update_all_transfer_guard_global)
jax._src._config_module.Config(self)
jax._src._config_module.Config.DEFINE_bool(self,name,default,*args,**kwargs)
jax._src._config_module.Config.DEFINE_enum(self,name,default,*args,**kwargs)
jax._src._config_module.Config.DEFINE_integer(self,name,default,*args,**kwargs)
jax._src._config_module.Config.DEFINE_string(self,name,default,*args,**kwargs)
jax._src._config_module.Config._config_module_with_absl(self)
jax._src._config_module.Config._read(self,name)
jax._src._config_module.Config._trace_context(self)
jax._src._config_module.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs,update_hook=None)
jax._src._config_module.Config.check_exists(self,name)
jax._src._config_module.Config.complete_absl__config_module(self,absl_flags)
jax._src._config_module.Config.define_bool_state(self,name:str,default:bool,help:str,*,update_global_hook:Optional[Callable[[bool],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[bool]],None]]=None,upgrade:bool=False,extra_description:str='')
jax._src._config_module.Config.define_enum_state(self,name:str,enum_values:List[str],default:Optional[str],help:str,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)
jax._src._config_module.Config.define_string_or_object_state(self,name:str,default:Any,help:str,update_global_hook:Optional[Callable[[Any],None]]=None,update_thread_local_hook:Optional[Callable[[Any],None]]=None,validate_new_val_hook:Optional[Callable[[Any],None]]=None)
jax._src._config_module.Config.define_string_state(self,name:str,default:Optional[str],help:str,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)
jax._src._config_module.Config.parse_flags_with_absl(self)
jax._src._config_module.Config.read(self,name)
jax._src._config_module.Config.update(self,name,val)
jax._src._config_module.NameSpace(self,getter,setter)
jax._src._config_module.NameSpace.__getattr__(self,name)
jax._src._config_module.NameSpace.__setattr__(self,name,val)
jax._src._config_module.NoDefault
jax._src._config_module._GlobalExtraJitContext(NamedTuple)
jax._src._config_module._StateContextManager(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src._config_module._StateContextManager._add_hooks(self,update_global_hook,update_thread_local_hook)
jax._src._config_module._ThreadLocalExtraJitContext(NamedTuple)
jax._src._config_module._ThreadLocalStateCache(self)
jax._src._config_module._Unset
jax._src._config_module._update_all_transfer_guard_global(val)
jax._src._config_module._update_default_device_global(val)
jax._src._config_module._update_default_device_thread_local(val)
jax._src._config_module._update_disable_jit_global(val)
jax._src._config_module._update_disable_jit_thread_local(val)
jax._src._config_module._update_global_jit_state(**kw)
jax._src._config_module._update_jax_array_global(val)
jax._src._config_module._update_jax_array_thread_local(val)
jax._src._config_module._update_transfer_guard(state,key,val)
jax._src._config_module._update_x64_global(val)
jax._src._config_module._update_x64_thread_local(val)
jax._src._config_module._validate_default_device(val)
jax._src._config_module.bool_env(varname:str,default:bool)->bool
jax._src._config_module.explicit_device_get_scope()->Iterator[None]
jax._src._config_module.explicit_device_put_scope()->Iterator[None]
jax._src._config_module.int_env(varname:str,default:int)->int
jax._src._config_module.update_thread_local_jit_state(**kw)
jax._src.config.Config(self)
jax._src.config.Config.DEFINE_bool(self,name,default,*args,**kwargs)
jax._src.config.Config.DEFINE_enum(self,name,default,*args,**kwargs)
jax._src.config.Config.DEFINE_integer(self,name,default,*args,**kwargs)
jax._src.config.Config.DEFINE_string(self,name,default,*args,**kwargs)
jax._src.config.Config.__init__(self)
jax._src.config.Config._read(self,name)
jax._src.config.Config._trace_context(self)
jax._src.config.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs,update_hook=None)
jax._src.config.Config.check_exists(self,name)
jax._src.config.Config.complete_absl_config(self,absl_flags)
jax._src.config.Config.config_with_absl(self)
jax._src.config.Config.define_bool_state(self,name:str,default:bool,help:str,*,update_global_hook:Optional[Callable[[bool],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[bool]],None]]=None,upgrade:bool=False,extra_description:str='')
jax._src.config.Config.define_enum_state(self,name:str,enum_values:List[str],default:Optional[str],help:str,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)
jax._src.config.Config.define_string_or_object_state(self,name:str,default:Any,help:str,update_global_hook:Optional[Callable[[Any],None]]=None,update_thread_local_hook:Optional[Callable[[Any],None]]=None,validate_new_val_hook:Optional[Callable[[Any],None]]=None)
jax._src.config.Config.define_string_state(self,name:str,default:Optional[str],help:str,update_global_hook:Optional[Callable[[str],None]]=None,update_thread_local_hook:Optional[Callable[[Optional[str]],None]]=None)
jax._src.config.Config.parse_flags_with_absl(self)
jax._src.config.Config.read(self,name)
jax._src.config.Config.update(self,name,val)
jax._src.config.NameSpace(self,getter,setter)
jax._src.config.NameSpace.__getattr__(self,name)
jax._src.config.NameSpace.__init__(self,getter,setter)
jax._src.config.NameSpace.__setattr__(self,name,val)
jax._src.config.NoDefault
jax._src.config._GlobalExtraJitContext(NamedTuple)
jax._src.config._StateContextManager(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src.config._StateContextManager.__init__(self,name,help,update_thread_local_hook,validate_new_val_hook:Optional[Callable[[Any],None]]=None,extra_description:str='',default_value:Any=no_default)
jax._src.config._StateContextManager._add_hooks(self,update_global_hook,update_thread_local_hook)
jax._src.config._ThreadLocalExtraJitContext(NamedTuple)
jax._src.config._ThreadLocalStateCache(self)
jax._src.config._ThreadLocalStateCache.__init__(self)
jax._src.config._Unset
jax._src.config._update_all_transfer_guard_global(val)
jax._src.config._update_default_device_global(val)
jax._src.config._update_default_device_thread_local(val)
jax._src.config._update_disable_jit_global(val)
jax._src.config._update_disable_jit_thread_local(val)
jax._src.config._update_global_jit_state(**kw)
jax._src.config._update_jax_array_global(val)
jax._src.config._update_jax_array_thread_local(val)
jax._src.config._update_transfer_guard(state,key,val)
jax._src.config._update_x64_global(val)
jax._src.config._update_x64_thread_local(val)
jax._src.config._validate_default_device(val)
jax._src.config.bool_env(varname:str,default:bool)->bool
jax._src.config.explicit_device_get_scope()->Iterator[None]
jax._src.config.explicit_device_put_scope()->Iterator[None]
jax._src.config.int_env(varname:str,default:int)->int
jax._src.config.transfer_guard(new_val:str)->Iterator[None]
jax._src.config.update_thread_local_jit_state(**kw)
jax.transfer_guard(new_val:str)->Iterator[None]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/tree_util.py----------------------------------------
A:jax._src.tree_util.T->TypeVar('T')
A:jax._src.tree_util.U->TypeVar('U')
A:jax._src.tree_util.lst->list(iterable)
A:jax._src.tree_util._Children->TypeVar('_Children', bound=Iterable[Any])
A:jax._src.tree_util._AuxData->TypeVar('_AuxData', bound=Hashable)
A:jax._src.tree_util._registry[nodetype]->_RegistryEntry(flatten_func, unflatten_func)
A:jax._src.tree_util.(leaves, treedef)->tree_flatten(tree, is_leaf)
A:jax._src.tree_util.(flat, treedef)->tree_flatten(pytree_to_transpose)
A:jax._src.tree_util.expected_treedef->outer_treedef.compose(inner_treedef)
A:jax._src.tree_util.flat->iter(flat)
A:jax._src.tree_util.transposed_lol->zip(*lol)
A:jax._src.tree_util.subtrees->map(partial(tree_unflatten, outer_treedef), transposed_lol)
A:jax._src.tree_util._RegistryEntry->collections.namedtuple('_RegistryEntry', ['to_iter', 'from_iter'])
A:jax._src.tree_util.handler->_keypath_registry.get(type(pytree))
A:jax._src.tree_util.(children, metadata)->_keypath_registry.get(type(pytree)).to_iter(tree)
A:jax._src.tree_util.children->iter(tree)
A:jax._src.tree_util.no_initializer->object()
A:jax._src.tree_util.func->_HashableCallableShim(original_func)
A:jax._src.tree_util.out->super().__new__(klass, func, *args, **kw)
A:jax._src.tree_util.(children, meta)->_keypath_registry.get(type(pytree)).to_iter(pytree)
A:jax._src.tree_util.num_children->len(treedef_children(tree_structure(pytree)))
A:jax._src.tree_util.(prefix_tree_children, prefix_tree_meta)->flatten_one_level(prefix_tree)
A:jax._src.tree_util.(full_tree_children, full_tree_meta)->flatten_one_level(full_tree)
A:jax._src.tree_util.prefix_tree_keys->_child_keys(prefix_tree)
A:jax._src.tree_util.full_tree_keys->_child_keys(full_tree)
A:jax._src.tree_util.diff->set(prefix_tree_keys).symmetric_difference(set(full_tree_keys))
A:jax._src.tree_util.prefix_tree_meta_str->str(prefix_tree_meta)
A:jax._src.tree_util.full_tree_meta_str->str(full_tree_meta)
A:jax._src.tree_util.metadata_diff->textwrap.indent('\n'.join(difflib.ndiff(prefix_tree_meta_str.splitlines(), full_tree_meta_str.splitlines())), prefix='    ')
jax._src.tree_util.AttributeKeyPathEntry(KeyPathEntry)
jax._src.tree_util.AttributeKeyPathEntry.pprint(self)->str
jax._src.tree_util.FlattenedKeyPathEntry(KeyPathEntry)
jax._src.tree_util.FlattenedKeyPathEntry.pprint(self)->str
jax._src.tree_util.GetitemKeyPathEntry(KeyPathEntry)
jax._src.tree_util.GetitemKeyPathEntry.pprint(self)->str
jax._src.tree_util.KeyPath(NamedTuple)
jax._src.tree_util.KeyPath.__add__(self,other)
jax._src.tree_util.KeyPath.pprint(self)->str
jax._src.tree_util.KeyPathEntry(NamedTuple)
jax._src.tree_util.KeyPathEntry.pprint(self)->str
jax._src.tree_util.Partial(klass,func,*args,**kw)
jax._src.tree_util.Partial.__new__(klass,func,*args,**kw)
jax._src.tree_util._HashableCallableShim(self,fun)
jax._src.tree_util._HashableCallableShim.__eq__(self,other)
jax._src.tree_util._HashableCallableShim.__hash__(self)
jax._src.tree_util._HashableCallableShim.__init__(self,fun)
jax._src.tree_util.__getattr__(name)
jax._src.tree_util._child_keys(pytree:Any)->List[KeyPathEntry]
jax._src.tree_util._deprecate(f)
jax._src.tree_util._prefix_error(key_path:KeyPath,prefix_tree:Any,full_tree:Any,is_leaf:Optional[Callable[[Any],bool]]=None)->Iterable[Callable[[str], ValueError]]
jax._src.tree_util._replace_nones(sentinel,tree)
jax._src.tree_util.all_leaves(iterable,is_leaf:Optional[Callable[[Any],bool]]=None)
jax._src.tree_util.broadcast_prefix(prefix_tree:Any,full_tree:Any,is_leaf:Optional[Callable[[Any],bool]]=None)->List[Any]
jax._src.tree_util.build_tree(treedef,xs)
jax._src.tree_util.flatten_one_level(pytree:Any)->Tuple[List[Any], Hashable]
jax._src.tree_util.prefix_errors(prefix_tree:Any,full_tree:Any,is_leaf:Optional[Callable[[Any],bool]]=None)->List[Callable[[str], ValueError]]
jax._src.tree_util.register_keypaths(ty:Type,handler:Callable[[Any],List[KeyPathEntry]])->None
jax._src.tree_util.register_pytree_node(nodetype:Type[T],flatten_func:Callable[[T],Tuple[_Children,_AuxData]],unflatten_func:Callable[[_AuxData,_Children],T])
jax._src.tree_util.register_pytree_node_class(cls)
jax._src.tree_util.tree_all(tree)
jax._src.tree_util.tree_flatten(tree,is_leaf:Optional[Callable[[Any],bool]]=None)
jax._src.tree_util.tree_leaves(tree,is_leaf:Optional[Callable[[Any],bool]]=None)
jax._src.tree_util.tree_map(f:Callable[...,Any],tree:Any,*rest:Any,is_leaf:Optional[Callable[[Any],bool]]=None)->Any
jax._src.tree_util.tree_reduce(function:Callable[[T,Any],T],tree:Any,initializer:Any=no_initializer)->T
jax._src.tree_util.tree_structure(tree,is_leaf:Optional[Callable[[Any],bool]]=None)
jax._src.tree_util.tree_transpose(outer_treedef,inner_treedef,pytree_to_transpose)
jax._src.tree_util.tree_unflatten(treedef,leaves)
jax._src.tree_util.treedef_children(treedef)
jax._src.tree_util.treedef_is_leaf(treedef)
jax._src.tree_util.treedef_is_strict_leaf(treedef)
jax._src.tree_util.treedef_tuple(treedefs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/pretty_printer.py----------------------------------------
A:jax._src.pretty_printer.ipython->get_ipython()
A:jax._src.pretty_printer.CAN_USE_COLOR->_can_use_color()
A:jax._src.pretty_printer._nil->_NilDoc()
A:jax._src.pretty_printer.self.children->list(children)
A:jax._src.pretty_printer.Color->enum.Enum('_Color', ['BLACK', 'RED', 'GREEN', 'YELLOW', 'BLUE', 'MAGENTA', 'CYAN', 'WHITE', 'RESET'])
A:jax._src.pretty_printer.Intensity->enum.Enum('_Intensity', ['DIM', 'NORMAL', 'BRIGHT'])
A:jax._src.pretty_printer._BreakMode->enum.Enum('_BreakMode', ['FLAT', 'BREAK'])
A:jax._src.pretty_printer.(i, m, doc)->agenda.pop()
A:jax._src.pretty_printer.doc->agenda.pop()
A:jax._src.pretty_printer.maxlen->max((l.width for l in lines))
A:jax._src.pretty_printer.default_colors->_ColorState(Color.RESET, Color.RESET, Intensity.NORMAL)
A:jax._src.pretty_printer.annotation_colors->_ColorState(Color.RESET, Color.RESET, Intensity.DIM)
A:jax._src.pretty_printer.(i, m, doc, color)->agenda.pop()
A:jax._src.pretty_printer.(color_state, color_str)->_update_color(use_color, color_state, default_colors)
A:jax._src.pretty_printer.color->_ColorState(doc.foreground or color.foreground, doc.background or color.background, doc.intensity or color.intensity)
A:jax._src.pretty_printer.lines->_align_annotations(lines)
A:jax._src.pretty_printer.out->'\n'.join((l.text if l.annotations is None else f'{l.text}{annotation_prefix}{l.annotations}' for l in lines))
A:jax._src.pretty_printer.docs->list(docs)
A:jax._src.pretty_printer.type_annotation->partial(color, intensity=Intensity.NORMAL, foreground=Color.MAGENTA)
A:jax._src.pretty_printer.keyword->partial(color, intensity=Intensity.BRIGHT, foreground=Color.BLUE)
jax._src.pretty_printer.Doc(abc.ABC)
jax._src.pretty_printer.Doc.__add__(self,other:'Doc')->'Doc'
jax._src.pretty_printer.Doc.__str__(self)
jax._src.pretty_printer.Doc.format(self,width:int=80,use_color:Optional[bool]=None,annotation_prefix='#')->str
jax._src.pretty_printer._BreakDoc(self,text:str)
jax._src.pretty_printer._BreakDoc.__init__(self,text:str)
jax._src.pretty_printer._BreakDoc.__repr__(self)
jax._src.pretty_printer._ColorDoc(self,child:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer._ColorDoc.__init__(self,child:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer._ColorState(NamedTuple)
jax._src.pretty_printer._ConcatDoc(self,children:Sequence[Doc])
jax._src.pretty_printer._ConcatDoc.__init__(self,children:Sequence[Doc])
jax._src.pretty_printer._ConcatDoc.__repr__(self)
jax._src.pretty_printer._GroupDoc(self,child:Doc)
jax._src.pretty_printer._GroupDoc.__init__(self,child:Doc)
jax._src.pretty_printer._GroupDoc.__repr__(self)
jax._src.pretty_printer._Line(NamedTuple)
jax._src.pretty_printer._NestDoc(self,n:int,child:Doc)
jax._src.pretty_printer._NestDoc.__init__(self,n:int,child:Doc)
jax._src.pretty_printer._NestDoc.__repr__(self)
jax._src.pretty_printer._NilDoc(Doc)
jax._src.pretty_printer._NilDoc.__repr__(self)
jax._src.pretty_printer._State(NamedTuple)
jax._src.pretty_printer._TextDoc(self,text:str,annotation:Optional[str]=None)
jax._src.pretty_printer._TextDoc.__init__(self,text:str,annotation:Optional[str]=None)
jax._src.pretty_printer._TextDoc.__repr__(self)
jax._src.pretty_printer._align_annotations(lines)
jax._src.pretty_printer._can_use_color()->bool
jax._src.pretty_printer._fits(doc:Doc,width:int,agenda:List[Tuple[int,_BreakMode,Doc]])->bool
jax._src.pretty_printer._format(doc:Doc,width:int,*,use_color,annotation_prefix)->str
jax._src.pretty_printer._sparse(doc:Doc)->bool
jax._src.pretty_printer._update_color(use_color:bool,state:_ColorState,update:_ColorState)->Tuple[_ColorState, str]
jax._src.pretty_printer.brk(text:str='')->Doc
jax._src.pretty_printer.color(doc:Doc,*,foreground:Optional[Color]=None,background:Optional[Color]=None,intensity:Optional[Intensity]=None)
jax._src.pretty_printer.concat(docs:Sequence[Doc])->Doc
jax._src.pretty_printer.group(doc:Doc)->Doc
jax._src.pretty_printer.join(sep:Doc,docs:Sequence[Doc])->Doc
jax._src.pretty_printer.nest(n:int,doc:Doc)->Doc
jax._src.pretty_printer.nil()->Doc
jax._src.pretty_printer.text(s:str,annotation:Optional[str]=None)->Doc


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/dlpack.py----------------------------------------
A:jax._src.dlpack.SUPPORTED_DTYPES->frozenset({jnp.int8, jnp.int16, jnp.int32, jnp.int64, jnp.uint8, jnp.uint16, jnp.uint32, jnp.uint64, jnp.float16, jnp.bfloat16, jnp.float32, jnp.float64, jnp.complex64, jnp.complex128})
A:jax._src.dlpack.cpu_backend->jax._src.lib.xla_bridge.get_backend('cpu')
A:jax._src.dlpack.gpu_backend->jax._src.lib.xla_bridge.get_backend('cuda')
A:jax._src.dlpack.buf->jax._src.lib.xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, cpu_backend, gpu_backend)
A:jax._src.dlpack.xla_shape->jax._src.lib.xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, cpu_backend, gpu_backend).xla_shape()
A:jax._src.dlpack.aval->jax.core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
jax._src.dlpack.from_dlpack(dlpack)
jax._src.dlpack.to_dlpack(x:device_array.DeviceArrayProtocol,take_ownership:bool=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/ad_util.py----------------------------------------
jax._src.ad_util.Zero(self,aval)
jax._src.ad_util.Zero.__init__(self,aval)
jax._src.ad_util.Zero.__repr__(self)
jax._src.ad_util.Zero.from_value(val)
jax._src.ad_util._stop_gradient_impl(x)
jax._src.ad_util.add_abstract(xs,ys)
jax._src.ad_util.add_impl(xs,ys)
jax._src.ad_util.add_jaxvals(x,y)
jax._src.ad_util.instantiate(z:Union[Zero,Array])->Array
jax._src.ad_util.zeros_like_aval(aval)
jax._src.ad_util.zeros_like_impl(example)
jax._src.ad_util.zeros_like_jaxval(val)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/profiler.py----------------------------------------
A:jax._src.profiler._profiler_server->jax._src.lib.xla_client.profiler.start_server(port)
A:jax._src.profiler.self.lock->threading.Lock()
A:jax._src.profiler._profile_state->_ProfileState()
A:jax._src.profiler._profile_state.profile_session->jax._src.lib.xla_client.profiler.ProfilerSession()
A:jax._src.profiler.curr_path->os.path.abspath(log_dir)
A:jax._src.profiler.root_trace_folder->os.path.join(curr_path, 'plugins', 'profile')
A:jax._src.profiler.latest_folder->max(trace_folders, key=os.path.getmtime)
A:jax._src.profiler.trace_jsons->glob.glob(os.path.join(latest_folder, '*.trace.json.gz'))
A:jax._src.profiler.trace->json.load(fp)
A:jax._src.profiler.perfetto_trace->os.path.join(latest_folder, filename)
A:jax._src.profiler.orig_directory->os.path.abspath(os.getcwd())
A:jax._src.profiler.(directory, filename)->os.path.split(path)
A:jax._src.profiler.abs_filename->_write_perfetto_trace_file(_profile_state.log_dir)
A:jax._src.profiler.profile->device_memory_profile(backend)
jax._src.profiler.StepTraceAnnotation(self,name:str,**kwargs)
jax._src.profiler.StepTraceAnnotation.__init__(self,name:str,**kwargs)
jax._src.profiler.TraceAnnotation(xla_client.profiler.TraceMe)
jax._src.profiler._PerfettoServer(http.server.SimpleHTTPRequestHandler)
jax._src.profiler._PerfettoServer.do_GET(self)
jax._src.profiler._PerfettoServer.do_POST(self)
jax._src.profiler._PerfettoServer.end_headers(self)
jax._src.profiler._ProfileState(self)
jax._src.profiler._ProfileState.__init__(self)
jax._src.profiler._host_perfetto_trace_file(path)
jax._src.profiler._write_perfetto_trace_file(log_dir)
jax._src.profiler.annotate_function(func:Callable,name:Optional[str]=None,**decorator_kwargs)
jax._src.profiler.device_memory_profile(backend:Optional[str]=None)->bytes
jax._src.profiler.save_device_memory_profile(filename,backend:Optional[str]=None)
jax._src.profiler.start_server(port:int)
jax._src.profiler.start_trace(log_dir,create_perfetto_link:bool=False,create_perfetto_trace:bool=False)
jax._src.profiler.stop_server()
jax._src.profiler.stop_trace()
jax._src.profiler.trace(log_dir,create_perfetto_link=False,create_perfetto_trace=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/array.py----------------------------------------
A:jax._src.array.np_value->fun(*args)
A:jax._src.array.jnp_value->device_put(np_value)
A:jax._src.array.jnp_value.aval->device_put(np_value).aval.update(**aval_state)
A:jax._src.array.db->jax.interpreters.pxla._set_aval(buf)
A:jax._src.array.array_device_ids->set(device_id_to_buffer.keys())
A:jax._src.array.addressable_device_ids->set((d.id for d in addressable_dev))
A:jax._src.array.ss->self.sharding.shard_shape(self.shape)
A:jax._src.array.indices->tuple(self.sharding.devices_indices_map(self.shape).values())
A:jax._src.array.buf_idx->tuple(self.sharding.devices_indices_map(self.shape).values()).index(cidx)
A:jax._src.array.aval->jax.core.ShapedArray(shape, arrays[0].dtype, weak_type=False)
A:jax._src.array.s->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width)
A:jax._src.array.(fun, args, arr_state)->self._value.__reduce__()
A:jax._src.array.array->_single_device_array_from_buf(db, self._committed)
A:jax._src.array.self._npy_value->numpy.asarray(self._arrays[0])
A:jax._src.array.npy_value->numpy.empty(self.shape, self.dtype)
A:jax._src.array.npy_value[s.index]->numpy.asarray(s.data._arrays[0])
A:jax._src.array.device_to_index_map->sharding.devices_indices_map(shape)
A:jax._src.array.xla.pytype_aval_mappings[ArrayImpl]->operator.attrgetter('aval')
A:jax._src.array.api_util._shaped_abstractify_handlers[ArrayImpl]->operator.attrgetter('aval')
A:jax._src.array.x->jax._src.dispatch._copy_device_array_to_device(pxla._set_aval(x._arrays[0]), device)
A:jax._src.array.x_indices->jax._src.dispatch._copy_device_array_to_device(pxla._set_aval(x._arrays[0]), device).sharding.addressable_devices_indices_map(x.shape).values()
jax._src.array.ArrayImpl(self,aval:core.ShapedArray,sharding:Sharding,arrays:Union[Sequence[DeviceArray],Sequence[ArrayImpl]],committed:bool,_skip_checks:bool=False)
jax._src.array.ArrayImpl.__array__(self,dtype=None,context=None)
jax._src.array.ArrayImpl.__bool__(self)
jax._src.array.ArrayImpl.__complex__(self)
jax._src.array.ArrayImpl.__cuda_array_interface__(self)
jax._src.array.ArrayImpl.__dlpack__(self)
jax._src.array.ArrayImpl.__float__(self)
jax._src.array.ArrayImpl.__format__(self,format_spec)
jax._src.array.ArrayImpl.__getitem__(self,idx)
jax._src.array.ArrayImpl.__hex__(self)
jax._src.array.ArrayImpl.__index__(self)
jax._src.array.ArrayImpl.__init__(self,aval:core.ShapedArray,sharding:Sharding,arrays:Union[Sequence[DeviceArray],Sequence[ArrayImpl]],committed:bool,_skip_checks:bool=False)
jax._src.array.ArrayImpl.__int__(self)
jax._src.array.ArrayImpl.__iter__(self)
jax._src.array.ArrayImpl.__len__(self)
jax._src.array.ArrayImpl.__nonzero__(self)
jax._src.array.ArrayImpl.__oct__(self)
jax._src.array.ArrayImpl.__reduce__(self)
jax._src.array.ArrayImpl.__repr__(self)
jax._src.array.ArrayImpl.__str__(self)
jax._src.array.ArrayImpl._check_and_rearrange(self)
jax._src.array.ArrayImpl._check_if_deleted(self)
jax._src.array.ArrayImpl._value(self)->np.ndarray
jax._src.array.ArrayImpl.addressable_data(self,index:int)->ArrayImpl
jax._src.array.ArrayImpl.addressable_shards(self)->Sequence[Shard]
jax._src.array.ArrayImpl.block_until_ready(self)
jax._src.array.ArrayImpl.copy_to_host_async(self)
jax._src.array.ArrayImpl.delete(self)
jax._src.array.ArrayImpl.device(self)->Device
jax._src.array.ArrayImpl.device_buffer(self)->ArrayImpl
jax._src.array.ArrayImpl.device_buffers(self)->Sequence[ArrayImpl]
jax._src.array.ArrayImpl.devices(self)->List[Device]
jax._src.array.ArrayImpl.dtype(self)
jax._src.array.ArrayImpl.is_deleted(self)
jax._src.array.ArrayImpl.is_fully_addressable(self)->bool
jax._src.array.ArrayImpl.is_fully_replicated(self)->bool
jax._src.array.ArrayImpl.item(self)
jax._src.array.ArrayImpl.ndim(self)
jax._src.array.ArrayImpl.shape(self)->Shape
jax._src.array.ArrayImpl.sharding(self)
jax._src.array.ArrayImpl.size(self)
jax._src.array.ArrayImpl.tobytes(self,order='C')
jax._src.array.ArrayImpl.tolist(self)
jax._src.array.ArrayImpl.unsafe_buffer_pointer(self)
jax._src.array.ArrayImpl.weak_type(self)
jax._src.array.Shard(self,device:Device,sharding:Sharding,global_shape:Shape,data:Optional[ArrayImpl]=None)
jax._src.array.Shard.__init__(self,device:Device,sharding:Sharding,global_shape:Shape,data:Optional[ArrayImpl]=None)
jax._src.array.Shard.__repr__(self)
jax._src.array.Shard.index(self)->Index
jax._src.array.Shard.replica_id(self)->int
jax._src.array._array_global_result_handler(global_aval,out_sharding,committed,is_out_sharding_from_xla)
jax._src.array._array_local_result_handler(aval,sharding,indices)
jax._src.array._array_mlir_constant_handler(val,canonicalize_types=True)
jax._src.array._array_pmap_shard_arg(x,devices,indices,mode)
jax._src.array._array_rest_shard_arg(x:ArrayImpl,devices,indices,mode)
jax._src.array._array_shard_arg(x,devices,indices,mode)
jax._src.array._device_put_array(x,device:Optional[Device])
jax._src.array._reconstruct_array(fun,args,arr_state,aval_state)
jax._src.array._single_device_array_from_buf(buf,committed)
jax._src.array.make_array_from_callback(shape:Shape,sharding:Sharding,data_callback:Callable[[Optional[Index]],ArrayLike])->ArrayImpl
jax._src.array.make_array_from_single_device_arrays(shape:Shape,sharding:Sharding,arrays:Sequence[ArrayImpl])->ArrayImpl
jax.make_array_from_callback(shape:Shape,sharding:Sharding,data_callback:Callable[[Optional[Index]],ArrayLike])->ArrayImpl
jax.make_array_from_single_device_arrays(shape:Shape,sharding:Sharding,arrays:Sequence[ArrayImpl])->ArrayImpl


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/cloud_tpu_init.py----------------------------------------
A:jax._src.cloud_tpu_init.worker_id->get_metadata('agent-worker-number')
A:jax._src.cloud_tpu_init.accelerator_type->get_metadata('accelerator-type')
A:jax._src.cloud_tpu_init.api_resp->requests.get(f'{gce_metadata_endpoint}/computeMetadata/v1/instance/attributes/{key}', headers={'Metadata-Flavor': 'Google'})
jax._cloud_tpu_init()
jax._src.cloud_tpu_init.cloud_tpu_init()
jax._src.cloud_tpu_init.get_metadata(key)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/sharding.py----------------------------------------
A:jax._src.sharding.process_index->jax._src.lib.xla_bridge.process_index()
A:jax._src.sharding.op_sharding->cast(xc.OpSharding, self._to_xla_op_sharding(len(global_shape)))
A:jax._src.sharding.(partitions, _)->jax.interpreters.pxla._get_num_ways_dim_sharded(op_sharding)
A:jax._src.sharding.(quotient, remainder)->divmod(s, p)
A:jax._src.sharding.h_index->_hashed_index(index)
A:jax._src.sharding.(self._parsed_pspec, _, _, _)->jax.experimental.pjit._prepare_axis_resources(self.spec, 'MeshPspecSharding spec')
A:jax._src.sharding.self._hash->hash((self._devices, self._op_sharding_hash))
A:jax._src.sharding.array_mapping->get_array_mapping(self._parsed_pspec)
A:jax._src.sharding.sharding_spec->jax.interpreters.pxla.new_mesh_sharding_specs(self.mesh.shape, self.mesh.axis_names)(num_dimensions, array_mapping)
A:jax._src.sharding.proto->_get_replicated_op_sharding()
A:jax._src.sharding.indices->jax.interpreters.pxla.op_sharding_to_indices(self._op_sharding, global_shape, len(self._devices))
A:jax._src.sharding.self._devices->tuple(devices)
A:jax._src.sharding.(num_ways_dim_sharded, _)->jax.interpreters.pxla._get_num_ways_dim_sharded(self._op_sharding)
jax._src.sharding.MeshPspecSharding(self,mesh:pxla.Mesh,spec:pxla.PartitionSpec,_parsed_pspec=None)
jax._src.sharding.MeshPspecSharding.__eq__(self,other)
jax._src.sharding.MeshPspecSharding.__hash__(self)
jax._src.sharding.MeshPspecSharding.__init__(self,mesh:pxla.Mesh,spec:pxla.PartitionSpec,_parsed_pspec=None)
jax._src.sharding.MeshPspecSharding.__repr__(self)
jax._src.sharding.MeshPspecSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding.MeshPspecSharding._from_parsed_pspec(cls,mesh,parsed_pspec)
jax._src.sharding.MeshPspecSharding._preprocess(self)
jax._src.sharding.MeshPspecSharding._to_xla_op_sharding(self,num_dimensions:int,axis_ctx:Optional[Union[mlir.SPMDAxisContext,mlir.ShardingContext]]=None)->xc.OpSharding
jax._src.sharding.MeshPspecSharding.device_set(self)->Set[Device]
jax._src.sharding.MeshPspecSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding.MeshPspecSharding.is_compatible_aval(self,aval_shape:Shape)
jax._src.sharding.OpShardingSharding(self,devices:Sequence[Device],op_sharding:xc.OpSharding)
jax._src.sharding.OpShardingSharding.__eq__(self,other)
jax._src.sharding.OpShardingSharding.__hash__(self)
jax._src.sharding.OpShardingSharding.__init__(self,devices:Sequence[Device],op_sharding:xc.OpSharding)
jax._src.sharding.OpShardingSharding.__repr__(self)
jax._src.sharding.OpShardingSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding.OpShardingSharding._op_sharding_hash(self)
jax._src.sharding.OpShardingSharding._to_xla_op_sharding(self,num_dimensions:int)->xc.OpSharding
jax._src.sharding.OpShardingSharding.device_set(self)->Set[Device]
jax._src.sharding.OpShardingSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding.OpShardingSharding.get_replicated(cls,device_assignment)
jax._src.sharding.OpShardingSharding.is_compatible_aval(self,aval_shape:Shape)
jax._src.sharding.PmapSharding(self,devices:np.ndarray,sharding_spec:pxla.ShardingSpec)
jax._src.sharding.PmapSharding.__eq__(self,other)
jax._src.sharding.PmapSharding.__hash__(self)
jax._src.sharding.PmapSharding.__init__(self,devices:np.ndarray,sharding_spec:pxla.ShardingSpec)
jax._src.sharding.PmapSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding.PmapSharding._to_xla_op_sharding(self,num_dimensions:int)->xc.OpSharding
jax._src.sharding.PmapSharding.device_set(self)->Set[Device]
jax._src.sharding.PmapSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Optional[Index]]
jax._src.sharding.PmapSharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding.Sharding(metaclass=abc.ABCMeta)
jax._src.sharding.Sharding.addressable_devices(self)->Set[Device]
jax._src.sharding.Sharding.addressable_devices_indices_map(self,global_shape:Shape)->Mapping[Device, Optional[Index]]
jax._src.sharding.Sharding.device_indices(self,device:Device,global_shape:Shape)->Optional[Index]
jax._src.sharding.Sharding.device_set(self)->Set[Device]
jax._src.sharding.Sharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Optional[Index]]
jax._src.sharding.Sharding.is_fully_addressable(self)->bool
jax._src.sharding.Sharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding.SingleDeviceSharding(self,device:Device)
jax._src.sharding.SingleDeviceSharding.__eq__(self,other)
jax._src.sharding.SingleDeviceSharding.__hash__(self)
jax._src.sharding.SingleDeviceSharding.__init__(self,device:Device)
jax._src.sharding.SingleDeviceSharding.__repr__(self)
jax._src.sharding.SingleDeviceSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding.SingleDeviceSharding._to_xla_op_sharding(self,num_dimensions:int)->xc.OpSharding
jax._src.sharding.SingleDeviceSharding.device_set(self)->Set[Device]
jax._src.sharding.SingleDeviceSharding.devices_indices_map(self,global_shape:Shape)->Mapping[Device, Index]
jax._src.sharding.XLACompatibleSharding(Sharding,metaclass=abc.ABCMeta)
jax._src.sharding.XLACompatibleSharding._addressable_device_assignment(self)->XLADeviceAssignment
jax._src.sharding.XLACompatibleSharding._device_assignment(self)->XLADeviceAssignment
jax._src.sharding.XLACompatibleSharding._to_xla_op_sharding(self,num_dimensions:int)->xc.OpSharding
jax._src.sharding.XLACompatibleSharding.shard_shape(self,global_shape:Shape)->Shape
jax._src.sharding._check_mesh_resource_axis(mesh,parsed_pspec)
jax._src.sharding._get_replicated_op_sharding()
jax._src.sharding._hash_op_sharding(op:xc.OpSharding)
jax._src.sharding._hashed_index(x)->int
jax._src.sharding.device_replica_id_map(sharding,global_shape:Shape)->Mapping[Device, int]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/distributed.py----------------------------------------
A:jax._src.distributed.(coordinator_address, num_processes, process_id, local_device_ids)->jax._src.clusters.ClusterEnv.auto_detect_unset_distributed_params(coordinator_address, num_processes, process_id, local_device_ids)
A:jax._src.distributed.visible_devices->','.join((str(x) for x in local_device_ids))
A:jax._src.distributed.self.service->jax._src.lib.xla_extension.get_distributed_runtime_service(coordinator_address, num_processes, config.jax_coordination_service)
A:jax._src.distributed.self.client->jax._src.lib.xla_extension.get_distributed_runtime_client(coordinator_address, process_id, config.jax_coordination_service, init_timeout=300)
A:jax._src.distributed.self.preemption_sync_manager->jax._src.lib.xla_extension.create_preemption_sync_manager()
A:jax._src.distributed.global_state->State()
jax._src.distributed.State
jax._src.distributed.State.initialize(self,coordinator_address:Optional[str]=None,num_processes:Optional[int]=None,process_id:Optional[int]=None,local_device_ids:Optional[Union[int,Sequence[int]]]=None)
jax._src.distributed.State.initialize_preemption_sync_manager(self)
jax._src.distributed.State.shutdown(self)
jax._src.distributed.initialize(coordinator_address:Optional[str]=None,num_processes:Optional[int]=None,process_id:Optional[int]=None,local_device_ids:Optional[Union[int,Sequence[int]]]=None)
jax._src.distributed.shutdown()


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/checkify.py----------------------------------------
A:jax._src.checkify.val->getattr(obj, attrname)
A:jax._src.checkify.sentinel->object()
A:jax._src.checkify.err->jax.numpy.broadcast_to(err, batch_shape)
A:jax._src.checkify.init_error->Error(False, 0, {})
A:jax._src.checkify.code->jax.numpy.broadcast_to(code, batch_shape)
A:jax._src.checkify.out_code->jax.lax.select(error.err, error.code, code)
A:jax._src.checkify.out_payload->jax.lax.select(error.err, error.payload, payload)
A:jax._src.checkify.aval->property(lambda self: core.get_aval(self.val))
A:jax._src.checkify.rule->error_checks.get(primitive)
A:jax._src.checkify.(out, self.main.error)->rule(self.main.error, self.main.enabled_errors, *in_vals, **params)
A:jax._src.checkify.out->tree_unflatten(out_tree(), out_flat)
A:jax._src.checkify.e->popattr(self.main, 'error')
A:jax._src.checkify.(f, msgs)->checkify_subtrace(f)
A:jax._src.checkify.params->dict(params, donated_invars=(False, False, False, *params['donated_invars']))
A:jax._src.checkify.(err, code, payload, *out_vals)->prim.bind(fun, jvp, e.err, e.code, e.payload, *in_vals)
A:jax._src.checkify.params_->dict(params, in_axes=(None, None, None, *params['in_axes']), out_axes_thunk=new_out_axes_thunk, donated_invars=(False, False, False, *params['donated_invars']))
A:jax._src.checkify.(errs, codes, payloads, *outs)->primitive.bind(f, e.err, e.code, e.payload, *in_vals, **params_)
A:jax._src.checkify.(err, code, payload)->unbatch_error(err, code, payload)
A:jax._src.checkify.trace->main.with_cur_sublevel()
A:jax._src.checkify.msgs->tuple(e.msgs.items())
A:jax._src.checkify.(fun, msgs1)->checkify_subtrace(fun, self.main, msgs)
A:jax._src.checkify.(jvp, msgs2)->checkify_custom_jvp_subtrace(jvp, self.main, msgs)
A:jax._src.checkify.(fst, out_msgs)->jax.linear_util.merge_linear_aux(msgs1, msgs2)
A:jax._src.checkify.(fwd, msgs2)->checkify_custom_vjp_subtrace(fwd, self.main, msgs)
A:jax._src.checkify.(fun, msgs)->checkify_subtrace(fun)
A:jax._src.checkify.fun->jax.core.jaxpr_as_fun(jaxpr)
A:jax._src.checkify.(err, code, payload, *outvals)->jax.core.jaxpr_as_fun(jaxpr).call_wrapped(init_error.err, init_error.code, init_error.payload, *args)
A:jax._src.checkify.out_tracers->map(trace.full_raise, out)
A:jax._src.checkify.(n, ragged)->divmod(len(args), 2)
A:jax._src.checkify.((err,), (code,), (payload,), primals)->split_list(args[:n], [1, 1, 1])
A:jax._src.checkify.((err_dot,), (code_dot,), (pl_dot,), tangents)->split_list(args[n:], [1, 1, 1])
A:jax._src.checkify.(m, ragged)->divmod(len(outs), 2)
A:jax._src.checkify.f->checkify_traceable(f, tuple(error.msgs.items()), enabled_errors)
A:jax._src.checkify.err_aval->jax.core.raise_to_shaped(core.get_aval(error.err))
A:jax._src.checkify.code_aval->jax.core.raise_to_shaped(core.get_aval(error.code))
A:jax._src.checkify.payload_aval->jax.core.raise_to_shaped(core.get_aval(error.payload))
A:jax._src.checkify.(jaxpr_out, _, literals_out)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, avals_in)
A:jax._src.checkify.assert_p->jax.core.Primitive('assert')
A:jax._src.checkify.CheckEffect->object()
A:jax._src.checkify.functionalization_error->ValueError('Cannot abstractly evaluate a checkify.check which was not functionalized. This probably means you tried to stage (jit/scan/pmap/...) a `check` without functionalizing it through `checkify.checkify`.')
A:jax._src.checkify.error->assert_func(error, any_zero, msg, None)
A:jax._src.checkify.(out_op, token_out, keep_alive)->jax.interpreters.mlir.emit_python_callback(ctx, callback=lambda *a: python_err(msgs, *a), token=ctx.tokens_in.get(CheckEffect)[0], operands=[err, code, payload], operand_avals=list(ctx.avals_in), result_avals=list(ctx.avals_out), has_side_effect=True)
A:jax._src.checkify.size->next((x.shape[dim] for (x, dim) in zip(batched_args, batch_dims) if dim is not batching.not_mapped))
A:jax._src.checkify.any_nans->jax.numpy.any(jnp.isnan(out))
A:jax._src.checkify.operand_dims->numpy.array(operand.shape)
A:jax._src.checkify.upper_bound->jax.lax.broadcast_in_dim(upper_bound, indices.shape, (len(indices.shape) - 1,))
A:jax._src.checkify.flat_idx->jax.numpy.argmin(jnp.logical_not(out_of_bounds))
A:jax._src.checkify.multi_idx->jax.numpy.unravel_index(flat_idx, start_indices.shape)
A:jax._src.checkify.payload->jax.numpy.broadcast_to(payload, batch_shape + (3,))
A:jax._src.checkify.any_zero->jax.numpy.any(jnp.equal(y, 0))
A:jax._src.checkify.lower_oob->jax.numpy.any(jnp.less(indices, 0))
A:jax._src.checkify.upper_oob->jax.numpy.any(jnp.greater(indices, upper_bound.astype(indices.dtype)))
A:jax._src.checkify.out_of_bounds->scatter_oob(operand, indices, updates, dimension_numbers)
A:jax._src.checkify.oob_error->assert_func(error, out_of_bounds, oob_msg, None)
A:jax._src.checkify.error_checks[lax.scatter_p]->partial(scatter_error_check, lax.scatter_p)
A:jax._src.checkify.error_checks[lax.scatter_add_p]->partial(scatter_error_check, lax.scatter_add_p)
A:jax._src.checkify.error_checks[lax.scatter_mul_p]->partial(scatter_error_check, lax.scatter_mul_p)
A:jax._src.checkify.error_checks[lax.scatter_min_p]->partial(scatter_error_check, lax.scatter_min_p)
A:jax._src.checkify.error_checks[lax.scatter_max_p]->partial(scatter_error_check, lax.scatter_max_p)
A:jax._src.checkify.(new_branches, msgs_)->unzip2((checkify_jaxpr(jxpr, error, enabled_errors) for jxpr in branches))
A:jax._src.checkify.(err, code, payload, *outs)->jax.lax.scan_p.bind(*new_in_flat, reverse=reverse, length=length, jaxpr=checked_jaxpr, num_consts=len(consts), num_carry=len(carry) + 3, linear=new_linear, unroll=unroll)
A:jax._src.checkify.(consts, carry, xs)->split_list(in_flat, [num_consts, num_carry])
A:jax._src.checkify.(checked_jaxpr_, msgs_)->checkify_jaxpr(jaxpr, error, enabled_errors)
A:jax._src.checkify.checked_jaxpr->jax.interpreters.partial_eval.move_binders_to_front(checked_jaxpr_, tomove)
A:jax._src.checkify.cond_f->jax.core.jaxpr_as_fun(cond_jaxpr)
A:jax._src.checkify.body_f->jax.core.jaxpr_as_fun(body_jaxpr)
A:jax._src.checkify._->cond_f(*c_consts, *out)
A:jax._src.checkify.new_jaxpr->jaxpr.replace(outvars=jaxpr.outvars[3:])
A:jax._src.checkify.err_args->batch_error(error.err, error.code, error.payload, batch_shape)
A:jax._src.checkify.(err, code, payload, *out)->jax.lax.while_p.bind(*new_in_flat, cond_nconsts=cond_nconsts, cond_jaxpr=compat_cond_jaxpr, body_nconsts=body_nconsts, body_jaxpr=checked_body_jaxpr)
A:jax._src.checkify.error_avals->map(lambda x: core.raise_to_shaped(core.get_aval(x)), batched_err)
A:jax._src.checkify.(new_jaxpr, _, literals_out)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(g), [*error_avals, *jaxpr.in_avals[3:]])
A:jax._src.checkify.(c_consts, b_consts, carry)->split_list(in_flat, [cond_nconsts, body_nconsts])
A:jax._src.checkify.(checked_cond_jaxpr, msgs_cond)->checkify_jaxpr(cond_jaxpr, error, enabled_errors)
A:jax._src.checkify.checked_cond_jaxpr->trivial_batched_jaxpr(checked_cond_jaxpr, batch_shape, err_args)
A:jax._src.checkify.(cond_err, cond_code, cond_payload, _)->jax.core.jaxpr_as_fun(checked_cond_jaxpr)(*err_args, *c_consts, *carry)
A:jax._src.checkify.(checked_body_jaxpr_, msgs_body)->checkify_while_body_jaxpr(cond_jaxpr, body_jaxpr, error, enabled_errors, c_consts)
A:jax._src.checkify.checked_body_jaxpr_->trivial_batched_jaxpr(checked_body_jaxpr_, batch_shape, err_args)
A:jax._src.checkify.checked_body_jaxpr->jax.interpreters.partial_eval.move_binders_to_front(checked_body_jaxpr_, to_move)
A:jax._src.checkify.compat_cond_jaxpr_->ignore_error_output_jaxpr(checked_cond_jaxpr)
A:jax._src.checkify.compat_cond_jaxpr->jax.interpreters.partial_eval.move_binders_to_front(compat_cond_jaxpr_, to_move)
A:jax._src.checkify.(checked_jaxpr, msgs)->checkify_jaxpr(jaxpr, error, enabled_errors)
A:jax._src.checkify.sharding->jax._src.sharding.OpShardingSharding.get_replicated(list(resource_env.physical_mesh.devices.flat))
A:jax._src.checkify.(err, code, payload, *vals_out)->jax.experimental.pjit.pjit_p.bind(*new_vals_in, jaxpr=checked_jaxpr, in_shardings=new_in_shardings, out_shardings=new_out_shardings, resource_env=resource_env, donated_invars=new_donated_invars, name=name, in_positional_semantics=new_positional_sems_in, out_positional_semantics=new_positional_sems_out)
A:jax._src.checkify.error_checks[prim]->partial(nan_error_check, prim)
A:jax._src.checkify.ErrorCategory->enum.Enum('ErrorCategory', ['NAN', 'OOB', 'DIV', 'USER_CHECK'])
A:jax._src.checkify.user_checks->frozenset({ErrorCategory.USER_CHECK})
A:jax._src.checkify.nan_checks->frozenset({ErrorCategory.NAN})
A:jax._src.checkify.index_checks->frozenset({ErrorCategory.OOB})
A:jax._src.checkify.div_checks->frozenset({ErrorCategory.DIV})
A:jax._src.checkify.Out->TypeVar('Out')
A:jax._src.checkify.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax._src.checkify.(f, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax._src.checkify.((err, code, payload, out_flat), msgs)->checkify_flat(f, errors, *args_flat)
jax._src.checkify.CheckifyTrace(self,main:core.MainTrace,sublevel:core.Sublevel,enabled_errors:FrozenSet['ErrorCategory'])
jax._src.checkify.CheckifyTrace.__init__(self,main:core.MainTrace,sublevel:core.Sublevel,enabled_errors:FrozenSet['ErrorCategory'])
jax._src.checkify.CheckifyTrace.post_process_call(self,primitive,tracers,params)
jax._src.checkify.CheckifyTrace.post_process_custom_jvp_call(self,out_tracers,jvp_was_run)
jax._src.checkify.CheckifyTrace.post_process_map(self,primitive,tracers,params)
jax._src.checkify.CheckifyTrace.process_call(self,primitive,f,tracers,params)
jax._src.checkify.CheckifyTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax._src.checkify.CheckifyTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees)
jax._src.checkify.CheckifyTrace.process_map(self,primitive,f,tracers,params)
jax._src.checkify.CheckifyTrace.process_primitive(self,primitive,tracers,params)
jax._src.checkify.CheckifyTrace.sublift(self,tracer)
jax._src.checkify.CheckifyTracer(self,trace,val)
jax._src.checkify.CheckifyTracer.__init__(self,trace,val)
jax._src.checkify.Error(self,err:Bool,code:Int,msgs:Dict[int,str],payload:Optional[Payload]=None)
jax._src.checkify.Error.__init__(self,err:Bool,code:Int,msgs:Dict[int,str],payload:Optional[Payload]=None)
jax._src.checkify.Error.__str__(self)
jax._src.checkify.Error.get(self)->Optional[str]
jax._src.checkify.Error.throw(self)
jax._src.checkify._format_msg(msg,payloads)
jax._src.checkify._reduce_any_error(errs,codes,payloads)
jax._src.checkify.add_nan_check(prim)
jax._src.checkify.assert_abstract_eval(err,code,payload,*,msgs)
jax._src.checkify.assert_batching_rule(batched_args,batch_dims,*,msgs)
jax._src.checkify.assert_discharge_rule(error,enabled_errors,err,code,payload,*,msgs)
jax._src.checkify.assert_func(error:Error,err:Bool,msg:str,payload:Optional[Payload])->Error
jax._src.checkify.assert_impl(err,code,payload,*,msgs)
jax._src.checkify.assert_jvp_rule(primals,_,*,msgs)
jax._src.checkify.assert_lowering_rule(ctx,err,code,payload,*,msgs)
jax._src.checkify.assert_lowering_rule_unsupported(*a,**k)
jax._src.checkify.batch_error(err,code,payload,batch_shape)
jax._src.checkify.check(pred:Bool,msg:str)->None
jax._src.checkify.check_error(error:Error)->None
jax._src.checkify.checkify(fun:Callable[...,Out],errors:FrozenSet[ErrorCategory]=user_checks)->Callable[..., Tuple[Error, Out]]
jax._src.checkify.checkify_custom_jvp_subtrace(main,msgs,*args)
jax._src.checkify.checkify_custom_vjp_subtrace(main,msgs,err,code,payload,*args)
jax._src.checkify.checkify_flat(fun:lu.WrappedFun,enabled_errors:FrozenSet['ErrorCategory'],*args)
jax._src.checkify.checkify_fun_to_jaxpr(f,error,enabled_errors,in_avals)
jax._src.checkify.checkify_jaxpr(jaxpr,error,enabled_errors)
jax._src.checkify.checkify_subtrace(main,msgs,err,code,payload,*args)
jax._src.checkify.checkify_traceable(msgs,enabled_errors,err,code,payload,*args)
jax._src.checkify.checkify_while_body_jaxpr(cond_jaxpr,body_jaxpr,error,enabled_errors,c_consts)
jax._src.checkify.cond_error_check(error,enabled_errors,index,*ops,branches,linear)
jax._src.checkify.div_error_check(error,enabled_errors,x,y)
jax._src.checkify.gather_error_check(error,enabled_errors,operand,start_indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.checkify.ignore_error_output_jaxpr(jaxpr)
jax._src.checkify.is_scalar_pred(pred)->bool
jax._src.checkify.nan_error_check(prim,error,enabled_errors,*in_vals,**params)
jax._src.checkify.pjit_error_check(error,enabled_errors,*vals_in,jaxpr,in_shardings,out_shardings,resource_env,donated_invars,name,in_positional_semantics,out_positional_semantics)
jax._src.checkify.popattr(obj,attrname)
jax._src.checkify.python_err(msgs,err,code,payload)
jax._src.checkify.raise_error(error)
jax._src.checkify.scan_error_check(error,enabled_errors,*in_flat,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.checkify.scatter_error_check(prim,error,enabled_errors,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.checkify.scatter_oob(operand,indices,updates,dnums)
jax._src.checkify.setnewattr(obj,name,val)
jax._src.checkify.summary()->str
jax._src.checkify.trivial_batched_jaxpr(jaxpr,batch_shape,batched_err)
jax._src.checkify.unbatch_error(err,code,payload)
jax._src.checkify.while_loop_error_check(error,enabled_errors,*in_flat,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/errors.py----------------------------------------
jax._src.errors.ConcretizationTypeError(self,tracer:core.Tracer,context:str='')
jax._src.errors.ConcretizationTypeError.__init__(self,tracer:core.Tracer,context:str='')
jax._src.errors.JAXIndexError(_JAXErrorMixin,IndexError)
jax._src.errors.JAXTypeError(_JAXErrorMixin,TypeError)
jax._src.errors.NonConcreteBooleanIndexError(self,tracer:core.Tracer)
jax._src.errors.NonConcreteBooleanIndexError.__init__(self,tracer:core.Tracer)
jax._src.errors.TracerArrayConversionError(self,tracer:core.Tracer)
jax._src.errors.TracerArrayConversionError.__init__(self,tracer:core.Tracer)
jax._src.errors.TracerIntegerConversionError(self,tracer:core.Tracer)
jax._src.errors.TracerIntegerConversionError.__init__(self,tracer:core.Tracer)
jax._src.errors.UnexpectedTracerError(self,msg:str)
jax._src.errors.UnexpectedTracerError.__init__(self,msg:str)
jax._src.errors._JAXErrorMixin(self,message:str)
jax._src.errors._JAXErrorMixin.__init__(self,message:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/test_util.py----------------------------------------
A:jax._src.test_util.arr->numpy.asarray(arr)
A:jax._src.test_util.dtype->numpy.dtype(dtype)
A:jax._src.test_util.result->func(*args, **kwargs)
A:jax._src.test_util.tol1->_normalize_tolerance(tol1)
A:jax._src.test_util.tol2->_normalize_tolerance(tol2)
A:jax._src.test_util.out[k]->max(v, tol1.get(k, 0))
A:jax._src.test_util.assert_close->partial(_assert_numpy_allclose, err_msg=err_msg)
A:jax._src.test_util.device_tags->_get_device_tags()
A:jax._src.test_util.test_name->getattr(test_method, '__name__', '[unknown test]')
A:jax._src.test_util.prev_xla_flags->os.getenv('XLA_FLAGS')
A:jax._src.test_util.flag_value->jax._src.config.config._read(flag_name)
A:jax._src.test_util.NUMPY_SCALAR_SHAPE->_NumpyScalar()
A:jax._src.test_util.PYTHON_SCALAR_SHAPE->_PythonScalar()
A:jax._src.test_util.shape->tuple(shape)
A:jax._src.test_util.shapestr->','.join((str(dim) for dim in shape))
A:jax._src.test_util.vals->numpy.where(zeros, np.array(0, dtype=dtype), vals)
A:jax._src.test_util.x_ravel->numpy.asarray(x).ravel()
A:jax._src.test_util.base_rand->rand_default(rng)
A:jax._src.test_util.dims->_dims_of_shape(shape)
A:jax._src.test_util.r->numpy.random.RandomState(42).rand(*dims)
A:jax._src.test_util.jaxpr->jax._src.api.make_jaxpr(fun)(*args)
A:jax._src.test_util.xs->list(xs)
A:jax._src.test_util.n->len(xs)
A:jax._src.test_util.k->min(n, FLAGS.num_generated_cases)
A:jax._src.test_util.indices->_CACHED_INDICES.get(n)
A:jax._src.test_util.rng->numpy.random.RandomState(42)
A:jax._src.test_util._CACHED_INDICES[n]indices->numpy.random.RandomState(42).permutation(n)
A:jax._src.test_util.seen->set()
A:jax._src.test_util.x->numpy.asarray(x)
A:jax._src.test_util.cases->list(gen(choose_one))
A:jax._src.test_util.names->super().getTestCaseNames(testCaseClass)
A:jax._src.test_util.pattern->re.compile(FLAGS.exclude_test_targets)
A:jax._src.test_util.(flat_args, tree)->tree_flatten(args)
A:jax._src.test_util.args->args_maker()
A:jax._src.test_util.self._original_config[key]->jax._src.config.config._read(key)
A:jax._src.test_util.self._rng->numpy.random.RandomState(zlib.adler32(self._testMethodName.encode()))
A:jax._src.test_util.atol->max(tolerance(_dtype(x), atol), tolerance(_dtype(y), atol))
A:jax._src.test_util.rtol->max(tolerance(_dtype(x), rtol), tolerance(_dtype(y), rtol))
A:jax._src.test_util.y->numpy.asarray(y)
A:jax._src.test_util.expected->textwrap.dedent(expected)
A:jax._src.test_util.what->textwrap.dedent(what)
A:jax._src.test_util.ignore_space_re->re.compile('\\s*\\n\\s*')
A:jax._src.test_util.expected_clean->re.sub(ignore_space_re, '\n', expected.strip())
A:jax._src.test_util.what_clean->re.sub(ignore_space_re, '\n', what.strip())
A:jax._src.test_util.python_ans->fun(*args)
A:jax._src.test_util.python_shapes->tree_map(lambda x: np.shape(x), python_ans)
A:jax._src.test_util.np_shapes->tree_map(lambda x: np.shape(np.asarray(x)), python_ans)
A:jax._src.test_util.cfun->jax._src.api.jit(wrapped_fun)
A:jax._src.test_util.monitored_ans->cfun(*args)
A:jax._src.test_util.compiled_ans->cfun(*args)
A:jax._src.test_util.lax_ans->lax_op(*args)
A:jax._src.test_util.numpy_ans->numpy_reference_op(*args)
A:jax._src.test_util._CPP_JIT_IMPLEMENTATION->functools.partial(api._jit, True)
A:jax._src.test_util._PYTHON_JIT_IMPLEMENTATION->functools.partial(api._jit, False)
A:jax._src.test_util.(axis_names, shape)->unzip2(named_shape)
A:jax._src.test_util.size->prod(mesh_shape)
A:jax._src.test_util.local_devices->list(api.local_devices())
A:jax._src.test_util.mesh_devices->numpy.array(devices[:size]).reshape(mesh_shape)
A:jax._src.test_util.devices->sorted(api.devices(), key=lambda d: d.id)
A:jax._src.test_util.global_mesh->Mesh(mesh_devices, axis_names)
A:jax._src.test_util.null->object()
A:jax._src.test_util.self._value->self._method(obj)
A:jax._src.test_util.supported->supported_dtypes()
A:jax._src.test_util.dtypes->_LazyDtypes()
jax._src.test_util.BufferDonationTestCase(JaxTestCase)
jax._src.test_util.BufferDonationTestCase._assertDeleted(self,x,deleted)
jax._src.test_util.JaxTestCase(parameterized.TestCase)
jax._src.test_util.JaxTestCase._CheckAgainstNumpy(self,numpy_reference_op,lax_op,args_maker,check_dtypes=True,tol=None,atol=None,rtol=None,canonicalize_dtypes=True)
jax._src.test_util.JaxTestCase._CompileAndCheck(self,fun,args_maker,*,check_dtypes=True,rtol=None,atol=None,check_cache_misses=True)
jax._src.test_util.JaxTestCase.assertAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None,canonicalize_dtypes=True,err_msg='')
jax._src.test_util.JaxTestCase.assertArraysAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None,err_msg='')
jax._src.test_util.JaxTestCase.assertArraysEqual(self,x,y,*,check_dtypes=True,err_msg='')
jax._src.test_util.JaxTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax._src.test_util.JaxTestCase.assertMultiLineStrippedEqual(self,expected,what)
jax._src.test_util.JaxTestCase.rng(self)
jax._src.test_util.JaxTestCase.setUp(self)
jax._src.test_util.JaxTestCase.tearDown(self)
jax._src.test_util.JaxTestLoader(absltest.TestLoader)
jax._src.test_util.JaxTestLoader.getTestCaseNames(self,testCaseClass)
jax._src.test_util.ScalarShape
jax._src.test_util.ScalarShape.__getitem__(self,i)
jax._src.test_util.ScalarShape.__len__(self)
jax._src.test_util._LazyDtypes
jax._src.test_util._LazyDtypes.all(self)
jax._src.test_util._LazyDtypes.all_floating(self)
jax._src.test_util._LazyDtypes.all_inexact(self)
jax._src.test_util._LazyDtypes.all_integer(self)
jax._src.test_util._LazyDtypes.all_unsigned(self)
jax._src.test_util._LazyDtypes.boolean(self)
jax._src.test_util._LazyDtypes.complex(self)
jax._src.test_util._LazyDtypes.floating(self)
jax._src.test_util._LazyDtypes.inexact(self)
jax._src.test_util._LazyDtypes.integer(self)
jax._src.test_util._LazyDtypes.numeric(self)
jax._src.test_util._LazyDtypes.supported(self,dtypes)
jax._src.test_util._LazyDtypes.unsigned(self)
jax._src.test_util._NumpyScalar(ScalarShape)
jax._src.test_util._PythonScalar(ScalarShape)
jax._src.test_util._cached_property(self,method)
jax._src.test_util._cached_property.__get__(self,obj,cls)
jax._src.test_util._cached_property.__init__(self,method)
jax._src.test_util._cast_to_shape(value,shape,dtype)
jax._src.test_util._dims_of_shape(shape)
jax._src.test_util._format_shape_dtype_string(shape,dtype)
jax._src.test_util._get_device_tags()
jax._src.test_util._normalize_tolerance(tol)
jax._src.test_util._rand_dtype(rand,shape,dtype,scale=1.0,post=lambdax:x)
jax._src.test_util.assert_dot_precision(expected_precision,fun,*args)
jax._src.test_util.assert_num_jit_and_pmap_compilations(times)
jax._src.test_util.capture_stdout()->Generator[Callable[[], str], None, None]
jax._src.test_util.cases_from_gens(*gens)
jax._src.test_util.cases_from_list(xs)
jax._src.test_util.check_eq(xs,ys,err_msg='')
jax._src.test_util.check_raises(thunk,err_type,msg)
jax._src.test_util.check_raises_regexp(thunk,err_type,pattern)
jax._src.test_util.count_device_put()
jax._src.test_util.count_jit_and_pmap_compiles()
jax._src.test_util.count_primitive_compiles()
jax._src.test_util.create_global_mesh(mesh_shape,axis_names)
jax._src.test_util.dtype_str(dtype)
jax._src.test_util.format_shape_dtype_string(shape,dtype)
jax._src.test_util.format_test_name_suffix(opname,shapes,dtypes)
jax._src.test_util.if_device_under_test(device_type:Union[str,Sequence[str]],if_true,if_false)
jax._src.test_util.ignore_warning(**kw)
jax._src.test_util.is_device_cuda()
jax._src.test_util.is_device_rocm()
jax._src.test_util.is_device_tpu_v4()
jax._src.test_util.is_sequence(x)
jax._src.test_util.is_valid_shape(shape,dtype)
jax._src.test_util.iter_eqns(jaxpr)
jax._src.test_util.join_tolerance(tol1,tol2)
jax._src.test_util.named_cases_from_sampler(gen)
jax._src.test_util.num_float_bits(dtype)
jax._src.test_util.promote_like_jnp(fun,inexact=False)
jax._src.test_util.rand_bool(rng)
jax._src.test_util.rand_default(rng,scale=3)
jax._src.test_util.rand_fullrange(rng,standardize_nans=False)
jax._src.test_util.rand_int(rng,low=0,high=None)
jax._src.test_util.rand_nonzero(rng)
jax._src.test_util.rand_not_small(rng,offset=10.0)
jax._src.test_util.rand_positive(rng)
jax._src.test_util.rand_small(rng)
jax._src.test_util.rand_small_positive(rng)
jax._src.test_util.rand_some_equal(rng)
jax._src.test_util.rand_some_inf(rng)
jax._src.test_util.rand_some_inf_and_nan(rng)
jax._src.test_util.rand_some_nan(rng)
jax._src.test_util.rand_some_zero(rng)
jax._src.test_util.rand_uniform(rng,low=0.0,high=1.0)
jax._src.test_util.rand_unique_int(rng,high=None)
jax._src.test_util.restore_spmd_lowering_flag()
jax._src.test_util.restore_spmd_manual_lowering_flag()
jax._src.test_util.sample_product(*args,**kw)
jax._src.test_util.sample_product_testcases(*args,**kw)
jax._src.test_util.set_host_platform_device_count(nr_devices:int)
jax._src.test_util.set_spmd_lowering_flag(val:bool)
jax._src.test_util.set_spmd_manual_lowering_flag(val:bool)
jax._src.test_util.skip_on_devices(*disabled_devices)
jax._src.test_util.skip_on_flag(flag_name,skip_value)
jax._src.test_util.strict_promotion_if_dtypes_match(dtypes)
jax._src.test_util.supported_dtypes()
jax._src.test_util.to_default_dtype(arr)
jax._src.test_util.with_and_without_mesh(f)
jax._src.test_util.with_config(**kwds)
jax._src.test_util.with_jax_dtype_defaults(func,use_defaults=True)
jax._src.test_util.with_mesh(named_shape:MeshSpec)->Generator[None, None, None]
jax._src.test_util.with_mesh_from_kwargs(f)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/random.py----------------------------------------
A:jax._src.random.impl->default_prng_impl()
A:jax._src.random.key->jax._src.prng.seed_with_impl(impl, seed)
A:jax._src.random.default_impl->default_prng_impl()
A:jax._src.random.(key, wrapped)->_check_prng_key(key)
A:jax._src.random.(keys, _)->_check_prng_key(keys)
A:jax._src.random.shape->jax.core.canonicalize_shape(shape)
A:jax._src.random.shape_->jax.lax.broadcast_shapes(shape.positional, *param_shapes)
A:jax._src.random.(key, _)->_check_prng_key(key)
A:jax._src.random.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.random.minval->jax.lax.broadcast_to_rank(minval, len(shape))
A:jax._src.random.maxval->jax.lax.broadcast_to_rank(maxval, len(shape))
A:jax._src.random.finfo->jax.numpy.finfo(dtype)
A:jax._src.random.bits->_random_bits(key, nbits, shape)
A:jax._src.random.float_bits->jax.lax.bitwise_or(lax.shift_right_logical(bits, np.array(nbits - nmant, lax.dtype(bits))), np.array(1.0, dtype).view(UINT_DTYPES[nbits]))
A:jax._src.random.maxval_out_of_range->jax.lax.gt(maxval, _convert_and_clip_integer(jnp.array(jnp.iinfo(dtype).max, dtype), maxval.dtype))
A:jax._src.random.(k1, k2)->split(key)
A:jax._src.random.span->jax.lax.select(maxval_out_of_range & (maxval > minval), lax.add(span, _lax_const(span, 1)), span)
A:jax._src.random.multiplier->jax.lax.rem(lax.mul(multiplier, multiplier), span)
A:jax._src.random.random_offset->jax.lax.rem(random_offset, span)
A:jax._src.random.axis->canonicalize_axis(axis, arr.ndim)
A:jax._src.random.r->rademacher(keys[1], shape, dtype)
A:jax._src.random.ind->jax.numpy.searchsorted(p_cuml, r)
A:jax._src.random.num_rounds->int(np.ceil(exponent * np.log(max(1, x.size)) / np.log(uint32max)))
A:jax._src.random.(key, subkey)->_split(key)
A:jax._src.random.sort_keys->_random_bits(subkey, 32, x.shape)
A:jax._src.random.(_, x)->jax.lax.sort_key_val(sort_keys, x, axis)
A:jax._src.random.arr->jax.numpy.asarray(a)
A:jax._src.random.n_inputs->jax.core.concrete_or_error(int, a, 'The error occurred in jax.random.choice()')
A:jax._src.random.n_draws->prod(shape)
A:jax._src.random.(p_arr,)->_promote_dtypes_inexact(p)
A:jax._src.random.p_cuml->jax.numpy.cumsum(p_arr)
A:jax._src.random.sqrt2->numpy.array(np.sqrt(2), dtype)
A:jax._src.random.(key_re, key_im)->_split(key)
A:jax._src.random._re->_normal_real(key_re, shape, real_dtype).astype(dtype)
A:jax._src.random._im->_normal_real(key_im, shape, real_dtype).astype(dtype)
A:jax._src.random.lo->numpy.nextafter(np.array(-1.0, dtype), np.array(0.0, dtype), dtype=dtype)
A:jax._src.random.hi->numpy.array(1.0, dtype)
A:jax._src.random.u->uniform(key, shape, dtype, minval=-1.0 + jnp.finfo(dtype).epsneg, maxval=1.0)
A:jax._src.random.(u, s, _)->svd(cov)
A:jax._src.random.(w, v)->eigh(cov)
A:jax._src.random.factor->cholesky(cov)
A:jax._src.random.normal_samples->normal(key, shape + mean.shape[-1:], dtype)
A:jax._src.random.lower->jax.lax.convert_element_type(lower, dtype)
A:jax._src.random.upper->jax.lax.convert_element_type(upper, dtype)
A:jax._src.random.a->jax.numpy.broadcast_to(a, shape)
A:jax._src.random.b->jax.lax.convert_element_type(b, dtype)
A:jax._src.random.p->jax.lax.convert_element_type(p, dtype)
A:jax._src.random.(key_a, key_b)->_split(key)
A:jax._src.random.log_gamma_a->loggamma(key_a, a, shape, dtype)
A:jax._src.random.log_gamma_b->loggamma(key_b, b, shape, dtype)
A:jax._src.random.log_max->jax.lax.max(log_gamma_a, log_gamma_b)
A:jax._src.random.gamma_a_scaled->jax.numpy.exp(log_gamma_a - log_max)
A:jax._src.random.gamma_b_scaled->jax.numpy.exp(log_gamma_b - log_max)
A:jax._src.random.pi->_lax_const(u, np.pi)
A:jax._src.random.alpha->jax.lax.select(boost_mask, alpha, lax.add(alpha, one))
A:jax._src.random.log_gamma_samples->loggamma(key, alpha, shape + np.shape(alpha)[-1:], dtype)
A:jax._src.random.x_max->jax.numpy.max(x, axis, keepdims=True)
A:jax._src.random.unnormalized->jax.numpy.exp(x - lax.stop_gradient(x_max))
A:jax._src.random.zero->jax._src.lax.lax._const(sample, 0)
A:jax._src.random.one->_lax_const(alpha, 1)
A:jax._src.random.minus_one->_lax_const(alpha, -1)
A:jax._src.random.one_over_two->_lax_const(alpha, 0.5)
A:jax._src.random.one_over_three->_lax_const(alpha, 1.0 / 3.0)
A:jax._src.random.squeeze_const->_lax_const(alpha, 0.0331)
A:jax._src.random.boost_mask->jax.lax.ge(alpha, one)
A:jax._src.random.d->jax.core.concrete_or_error(index, d, 'The error occurred in jax.random.ball()')
A:jax._src.random.c->jax.lax.div(one_over_three, lax.sqrt(d))
A:jax._src.random.cond->jax.lax.bitwise_and(lax.ge(U, lax.sub(one, lax.mul(squeeze_const, lax.mul(X, X)))), lax.ge(lax.log(U), lax.add(lax.mul(X, one_over_two), lax.mul(d, lax.add(lax.sub(one, V), lax.log(V))))))
A:jax._src.random.x->uniform(key, shape, dtype, minval=jnp.finfo(dtype).eps, maxval=1.0)
A:jax._src.random.v->uniform(subkey_1, shape, lam.dtype)
A:jax._src.random.(key, x_key, U_key)->_split(key, 3)
A:jax._src.random.(_, x, v)->jax.lax.while_loop(lambda kxv: lax.le(kxv[2], zero), _next_kxv, (x_key, zero, minus_one))
A:jax._src.random.X->jax.lax.mul(x, x)
A:jax._src.random.V->jax.lax.mul(lax.mul(v, v), v)
A:jax._src.random.U->uniform(U_key, (), dtype=dtype)
A:jax._src.random.u_boost->uniform(subkey, (), dtype=dtype)
A:jax._src.random.(_, _, V, _)->jax.lax.while_loop(_cond_fn, _body_fn, (key, zero, one, _lax_const(alpha, 2)))
A:jax._src.random.log_boost->jax.lax.select(boost_mask, zero, lax.mul(lax.log(u_boost), lax.div(one, alpha_orig)))
A:jax._src.random.boost->jax.lax.select(boost_mask, one, lax.pow(u_boost, lax.div(one, alpha_orig)))
A:jax._src.random.z->normal(key, (*shape, n, n), dtype)
A:jax._src.random.samples->jax.lax.map(lambda args: _gamma_one(*args, log_space=log_space), (keys, alphas))
A:jax._src.random.alphas->jax.numpy.broadcast_to(a, shape).flatten()
A:jax._src.random.tiny->jax.lax.full_like(samples, jnp.finfo(samples.dtype).tiny)
A:jax._src.random.grads->vmap(gamma_grad)(alphas, samples)
A:jax._src.random.a_shape->jax.numpy.shape(a)
A:jax._src.random.split_count->prod(a_shape[key.ndim:])
A:jax._src.random.keys->split(key)
A:jax._src.random.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax._src.random.k->jax.lax.floor((2 * a / u_shifted + b) * u + lam + 0.43)
A:jax._src.random.random_gamma_p->jax.core.Primitive('random_gamma')
A:jax._src.random.(rng, subkey)->_split(rng)
A:jax._src.random.k_init->jax.lax.full_like(lam, -1, lam.dtype, shape)
A:jax._src.random.log_rate_init->jax.lax.full_like(lam, 0, np.float32, shape)
A:jax._src.random.log_lam->jax.lax.log(lam)
A:jax._src.random.(key, subkey_0, subkey_1)->_split(key, 3)
A:jax._src.random.s->jax.lax.log(v * inv_alpha / (a / (u_shifted * u_shifted) + b))
A:jax._src.random.k_out->jax.lax.select(accept, k, k_out)
A:jax._src.random.accepted->jax.lax.full_like(lam, False, jnp.bool_, shape)
A:jax._src.random.lam_knuth->jax.lax.select(use_knuth, lam, lax.full_like(lam, 0.0))
A:jax._src.random.lam_rejection->jax.lax.select(use_knuth, lax.full_like(lam, 100000.0), lam)
A:jax._src.random.max_iters->jax._src.dtypes.canonicalize_dtype(dtype).type(jnp.iinfo(dtype).max)
A:jax._src.random.result->jax.lax.select(use_knuth, _poisson_knuth(key, lam_knuth, shape, dtype, max_iters), _poisson_rejection(key, lam_rejection, shape, dtype, max_iters))
A:jax._src.random.lam->jax.lax.convert_element_type(lam, np.float32)
A:jax._src.random.logits_arr->jax.numpy.asarray(logits)
A:jax._src.random.batch_shape->tuple(np.delete(logits_arr.shape, axis))
A:jax._src.random.e->exponential(k2, shape, dtype)
A:jax._src.random.df->jax.lax.convert_element_type(df, dtype)
A:jax._src.random.(key_n, key_g)->_split(key)
A:jax._src.random.n->jax.core.concrete_or_error(index, n, 'The error occurred in jax.random.orthogonal()')
A:jax._src.random.two->_lax_const(n, 2)
A:jax._src.random.half_df->jax.lax.div(df, two)
A:jax._src.random.g->generalized_normal(k1, p, (*shape, d), dtype)
A:jax._src.random.bernoulli_samples->bernoulli(key=key, p=0.5, shape=shape).astype(dtype)
A:jax._src.random.norm_rvs->normal(key=key, shape=shape, dtype=dtype)
A:jax._src.random.params_shapes->jax.lax.broadcast_shapes(np.shape(loc), np.shape(scale))
A:jax._src.random.(maxwell_key, rademacher_key)->_split(key)
A:jax._src.random.maxwell_rvs->maxwell(maxwell_key, shape=shape, dtype=dtype)
A:jax._src.random.random_sign->rademacher(rademacher_key, shape=shape, dtype=dtype)
A:jax._src.random.random_uniform->uniform(key=key, shape=shape, minval=0, maxval=1, dtype=dtype)
A:jax._src.random.(q, r)->jax.numpy.linalg.qr(z)
jax._src.random.PRNGKey(seed:int)->KeyArray
jax._src.random._bernoulli(key,p,shape)->Array
jax._src.random._beta(key,a,b,shape,dtype)->Array
jax._src.random._cauchy(key,shape,dtype)->Array
jax._src.random._check_default_impl_with_no_custom_prng(impl,name)
jax._src.random._check_prng_key(key)
jax._src.random._check_shape(name:str,shape:Union[Shape,NamedShape],*param_shapes)->None
jax._src.random._dirichlet(key,alpha,shape,dtype)->Array
jax._src.random._double_sided_maxwell(key,loc,scale,shape,dtype)->Array
jax._src.random._exponential(key,shape,dtype)->Array
jax._src.random._fold_in(key:KeyArray,data:int)->KeyArray
jax._src.random._gamma(key,a,shape,dtype,log_space=False)->Array
jax._src.random._gamma_batching_rule(batched_args,batch_dims,*,log_space)
jax._src.random._gamma_grad(sample,a,*,log_space)
jax._src.random._gamma_impl(key,a,*,log_space,use_vmap=False)
jax._src.random._gamma_one(key:KeyArray,alpha,log_space)->Array
jax._src.random._gumbel(key,shape,dtype)->Array
jax._src.random._isnan(x:ArrayLike)->Array
jax._src.random._key_data(keys:KeyArray)->Array
jax._src.random._laplace(key,shape,dtype)->Array
jax._src.random._logistic(key,shape,dtype)
jax._src.random._maxwell(key,shape,dtype)->Array
jax._src.random._multivariate_normal(key,mean,cov,shape,dtype,method)->Array
jax._src.random._normal(key,shape,dtype)->Array
jax._src.random._normal_real(key,shape,dtype)->Array
jax._src.random._pareto(key,b,shape,dtype)->Array
jax._src.random._poisson(key,lam,shape,dtype)->Array
jax._src.random._poisson_knuth(key,lam,shape,dtype,max_iters)->Array
jax._src.random._poisson_rejection(key,lam,shape,dtype,max_iters)->Array
jax._src.random._rademacher(key,shape,dtype)->Array
jax._src.random._randint(key,shape,minval,maxval,dtype)->Array
jax._src.random._random_bits(key:prng.PRNGKeyArray,bit_width,shape)->Array
jax._src.random._return_prng_keys(was_wrapped,key)
jax._src.random._shuffle(key,x,axis)->Array
jax._src.random._softmax(x,axis)->Array
jax._src.random._split(key:KeyArray,num:int=2)->KeyArray
jax._src.random._t(key,df,shape,dtype)->Array
jax._src.random._truncated_normal(key,lower,upper,shape,dtype)->Array
jax._src.random._uniform(key,shape,dtype,minval,maxval)->Array
jax._src.random._weibull_min(key,scale,concentration,shape,dtype)->Array
jax._src.random.ball(key:KeyArray,d:int,p:float=2,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)
jax._src.random.bernoulli(key:KeyArray,p:RealArray=np.float32(0.5),shape:Optional[Union[Shape,NamedShape]]=None)->Array
jax._src.random.beta(key:KeyArray,a:RealArray,b:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.categorical(key:KeyArray,logits:RealArray,axis:int=-1,shape:Optional[Shape]=None)->Array
jax._src.random.cauchy(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.choice(key:KeyArray,a:Union[int,ArrayLike],shape:Shape=(),replace:bool=True,p:Optional[RealArray]=None,axis:int=0)->Array
jax._src.random.default_prng_impl()
jax._src.random.dirichlet(key:KeyArray,alpha:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.double_sided_maxwell(key:KeyArray,loc:RealArray,scale:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.exponential(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.fold_in(key:KeyArray,data:int)->KeyArray
jax._src.random.gamma(key:KeyArray,a:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.generalized_normal(key:KeyArray,p:float,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.gumbel(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.key_data(keys:KeyArray)->Array
jax._src.random.laplace(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.loggamma(key:KeyArray,a:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.logistic(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.maxwell(key:KeyArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.multivariate_normal(key:KeyArray,mean:RealArray,cov:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_,method:str='cholesky')->Array
jax._src.random.normal(key:KeyArray,shape:Union[Shape,NamedShape]=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.orthogonal(key:KeyArray,n:int,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.pareto(key:KeyArray,b:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.permutation(key:KeyArray,x:Union[int,ArrayLike],axis:int=0,independent:bool=False)->Array
jax._src.random.poisson(key:KeyArray,lam:RealArray,shape:Optional[Shape]=None,dtype:DTypeLikeInt=dtypes.int_)->Array
jax._src.random.rademacher(key:KeyArray,shape:Shape,dtype:DTypeLikeInt=dtypes.int_)->Array
jax._src.random.randint(key:KeyArray,shape:Shape,minval:IntegerArray,maxval:IntegerArray,dtype:DTypeLikeInt=dtypes.int_)->Array
jax._src.random.rbg_key(seed:int)->KeyArray
jax._src.random.shuffle(key:KeyArray,x:ArrayLike,axis:int=0)->Array
jax._src.random.split(key:KeyArray,num:int=2)->KeyArray
jax._src.random.t(key:KeyArray,df:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.threefry2x32_key(seed:int)->KeyArray
jax._src.random.threefry_2x32(keypair,count)
jax._src.random.truncated_normal(key:KeyArray,lower:RealArray,upper:RealArray,shape:Optional[Union[Shape,NamedShape]]=None,dtype:DTypeLikeFloat=dtypes.float_)->Array
jax._src.random.uniform(key:KeyArray,shape:Union[Shape,NamedShape]=(),dtype:DTypeLikeFloat=dtypes.float_,minval:RealArray=0.0,maxval:RealArray=1.0)->Array
jax._src.random.unsafe_rbg_key(seed:int)->KeyArray
jax._src.random.weibull_min(key:KeyArray,scale:RealArray,concentration:RealArray,shape:Shape=(),dtype:DTypeLikeFloat=dtypes.float_)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/util.py----------------------------------------
A:jax._src.util.T->TypeVar('T')
A:jax._src.util.n->len(args[0])
A:jax._src.util.args->list(args)
A:jax._src.util.lst->list(lst)
A:jax._src.util.sentinel->object()
A:jax._src.util.dct->dict(dct)
A:jax._src.util._unflatten_done->object()
A:jax._src.util.xs_iter->iter(xs)
A:jax._src.util.end_nodes->_remove_duplicates(end_nodes)
A:jax._src.util.stack->list(end_nodes)
A:jax._src.util.node->childless_nodes.pop()
A:jax._src.util.visited->set()
A:jax._src.util.seen->set()
A:jax._src.util.sides->list(map(predicate, xs))
A:jax._src.util.memoize->cache(max_size=None)
A:jax._src.util.CacheInfo->namedtuple('CacheInfo', ['hits', 'misses', 'maxsize', 'currsize'])
A:jax._src.util.lock->threading.Lock()
A:jax._src.util.kwargs_key->tuple(kwargs.items())
A:jax._src.util.tctx->jax.config.config._trace_context()
A:jax._src.util.result->call(weak_arg, *args, **kwargs)
A:jax._src.util.del_k->next(iter(cache))
A:jax._src.util.attr->getattr(module, key)
A:jax._src.util.name_stack->name_stack.extend(name).extend(name)
A:jax._src.util.axis->operator.index(axis)
A:jax._src.util.name->getattr(wrapped, '__name__', '<unnamed function>')
A:jax._src.util.fun.__annotations__->getattr(wrapped, '__annotations__', {})
A:jax._src.util.fun.__name__->namestr.format(fun=name)
A:jax._src.util.fun.__module__->getattr(wrapped, '__module__', '<unknown module>')
A:jax._src.util.fun.__doc__->docstr.format(fun=name, doc=doc, **kwargs)
A:jax._src.util.fun.__qualname__->getattr(wrapped, '__qualname__', fun.__name__)
A:jax._src.util.pos->operator.index(axis)
A:jax._src.util.self.elts_set->set()
A:jax._src.util.self.hash->hash(x)
jax._src.util.Hashable(self,val)
jax._src.util.Hashable.__eq__(self,other)
jax._src.util.Hashable.__hash__(self)
jax._src.util.Hashable.__init__(self,val)
jax._src.util.HashableFunction(self,f,closure)
jax._src.util.HashableFunction.__eq__(self,other)
jax._src.util.HashableFunction.__hash__(self)
jax._src.util.HashableFunction.__init__(self,f,closure)
jax._src.util.HashableFunction.__repr__(self)
jax._src.util.HashableWrapper(self,x)
jax._src.util.HashableWrapper.__eq__(self,other)
jax._src.util.HashableWrapper.__hash__(self)
jax._src.util.HashableWrapper.__init__(self,x)
jax._src.util.OrderedSet(self)
jax._src.util.OrderedSet.__contains__(self,elt:T)->bool
jax._src.util.OrderedSet.__init__(self)
jax._src.util.OrderedSet.__iter__(self)->Iterator[T]
jax._src.util.OrderedSet.__len__(self)->int
jax._src.util.OrderedSet.add(self,elt:T)->None
jax._src.util.OrderedSet.update(self,elts:Seq[T])->None
jax._src.util.Unhashable(self,val)
jax._src.util.Unhashable.__eq__(self,other)
jax._src.util.Unhashable.__init__(self,val)
jax._src.util.WrapKwArgs(self,val)
jax._src.util.WrapKwArgs.__eq__(self,other)
jax._src.util.WrapKwArgs.__hash__(self)
jax._src.util.WrapKwArgs.__init__(self,val)
jax._src.util._remove_duplicates(node_list)
jax._src.util.as_hashable_function(closure)
jax._src.util.assert_unreachable(x)
jax._src.util.cache(max_size=4096)
jax._src.util.canonicalize_axis(axis,num_dims)->int
jax._src.util.ceil_of_ratio(x,y)
jax._src.util.check_toposort(nodes)
jax._src.util.concatenate(xs:Iterable[Sequence[T]])->List[T]
jax._src.util.curry(f)
jax._src.util.distributed_debug_log(*pairs)
jax._src.util.extend_name_stack(stack,name:str)
jax._src.util.get_module_functions(module)
jax._src.util.maybe_named_axis(axis,if_pos,if_named)
jax._src.util.merge_lists(bs:Sequence[bool],l0:Sequence[T],l1:Sequence[T])->List[T]
jax._src.util.moveaxis(x,src,dst)
jax._src.util.new_name_stack(name:str='')
jax._src.util.partition_list(bs:Sequence[bool],l:Sequence[T])->Tuple[List[T], List[T]]
jax._src.util.prod(xs)
jax._src.util.safe_map(f,*args)
jax._src.util.safe_zip(*args)
jax._src.util.split_dict(dct,names)
jax._src.util.split_list(args:Sequence[T],ns:Sequence[int])->List[List[T]]
jax._src.util.split_merge(predicate,xs)
jax._src.util.subvals(lst,replace)
jax._src.util.toposort(end_nodes)
jax._src.util.tuple_delete(t,idx)
jax._src.util.tuple_insert(t,idx,val)
jax._src.util.unflatten(xs:Iterable[T],ns:Sequence[int])->List[List[T]]
jax._src.util.unzip2(xys)
jax._src.util.unzip3(xyzs)
jax._src.util.weakref_lru_cache(call:Callable,maxsize=2048)
jax._src.util.wrap_name(name,transform_name)
jax._src.util.wraps(wrapped,fun,namestr='{fun}',docstr='{doc}',**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/custom_derivatives.py----------------------------------------
A:jax._src.custom_derivatives.ba->inspect.signature(fun).bind(*args, **kwargs)
A:jax._src.custom_derivatives.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(rule, ans_avals)
A:jax._src.custom_derivatives.py_args->tree_unflatten(in_tree, args)
A:jax._src.custom_derivatives.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax._src.custom_derivatives.ReturnValue->TypeVar('ReturnValue')
A:jax._src.custom_derivatives.primal_out->self(*primals)
A:jax._src.custom_derivatives.zeros->_zeros_like_pytree(primal_out)
A:jax._src.custom_derivatives.tangent_out->tree_map(_sum_tangents, primal_out, *all_tangents_out)
A:jax._src.custom_derivatives.primal_name->getattr(self.fun, '__name__', str(self.fun))
A:jax._src.custom_derivatives.jvp_name->getattr(self.jvp, '__name__', str(self.jvp))
A:jax._src.custom_derivatives.args->map(core.full_lower, args)
A:jax._src.custom_derivatives.nondiff_argnums->set(self.nondiff_argnums)
A:jax._src.custom_derivatives.(f_, dyn_args)->argnums_partial(lu.wrap_init(self.fun), dyn_argnums, args, require_static_args_hashable=False)
A:jax._src.custom_derivatives.jvp->jax.linear_util.wrap_init(self.jvp)
A:jax._src.custom_derivatives.(args_flat, in_tree)->tree_flatten(args)
A:jax._src.custom_derivatives.(flat_fun, out_type1)->_flatten_fun_nokwargs(f_, in_tree)
A:jax._src.custom_derivatives.(flat_jvp, out_type2)->_flatten_jvp(jvp, primal_name, jvp_name, in_tree, out_type1)
A:jax._src.custom_derivatives.out_flat->jax.core.eval_jaxpr(jaxpr, consts, *all_args)
A:jax._src.custom_derivatives.(_, (out_tree, _))->jax.linear_util.merge_linear_aux(out_type, out_trees)
A:jax._src.custom_derivatives.extra_args->tuple((arg.val for arg in extra_args))
A:jax._src.custom_derivatives.(primals_in, tangents_in)->split_list(args, [len(args) // 2])
A:jax._src.custom_derivatives.py_primals->tree_unflatten(in_tree, primals_in)
A:jax._src.custom_derivatives.py_tangents->tree_unflatten(in_tree, tangents_in)
A:jax._src.custom_derivatives.(primals_out, out_tree)->tree_flatten(py_primals_out)
A:jax._src.custom_derivatives.(tangents_out, out_tree2)->tree_flatten(py_tangents_out)
A:jax._src.custom_derivatives.out_type_->maybe_out_type()
A:jax._src.custom_derivatives.ty_tree->tree_unflatten(out_tree, [a.str_short() for a in primal_avals])
A:jax._src.custom_derivatives.ty_tree_->tree_unflatten(out_tree_, [a.str_short() for a in primal_avals_])
A:jax._src.custom_derivatives.top_trace->jax.core.find_top_trace(args)
A:jax._src.custom_derivatives.(fun, env_trace_todo1)->process_env_traces(fun, self, top_trace and top_trace.level, False)
A:jax._src.custom_derivatives.(jvp, env_trace_todo2)->process_env_traces(jvp, self, top_trace and top_trace.level, True)
A:jax._src.custom_derivatives.tracers->map(top_trace.full_raise, args)
A:jax._src.custom_derivatives.outs->map(trace.full_raise, outs)
A:jax._src.custom_derivatives.(_, env_trace_todo)->jax.linear_util.merge_linear_aux(env_trace_todo1, env_trace_todo2)
A:jax._src.custom_derivatives.new_params->dict(params)
A:jax._src.custom_derivatives.call_jaxpr->dict(params).pop('call_jaxpr')
A:jax._src.custom_derivatives.num_consts->len(hoisted_consts)
A:jax._src.custom_derivatives.jvp_jaxpr_thunk->dict(params).pop('jvp_jaxpr_thunk')
A:jax._src.custom_derivatives.fun->custom_jvp(fun)
A:jax._src.custom_derivatives.(jvp_jaxpr, jvp_consts)->jvp_jaxpr_thunk()
A:jax._src.custom_derivatives.(n, ragged)->divmod(len(xs), 2)
A:jax._src.custom_derivatives.ans->max(tracers, key=lambda x: x._trace.level)
A:jax._src.custom_derivatives.trace->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel()
A:jax._src.custom_derivatives.(outs, cur_todo)->primitive.post_process(trace, outs, jvp_was_run)
A:jax._src.custom_derivatives.todos_list->list(todos)
A:jax._src.custom_derivatives.custom_jvp_call_p->CustomJVPCallPrimitive('custom_jvp_call')
A:jax._src.custom_derivatives.args_->map(mlir.wrap_singleton_ir_values, args)
A:jax._src.custom_derivatives.consts->merge(closure_consts, hoisted_consts)
A:jax._src.custom_derivatives.(out, tokens)->jax.interpreters.mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr, ctx.tokens_in, consts, *args_)
A:jax._src.custom_derivatives.fwd_name->getattr(self.fwd, '__name__', str(self.fwd))
A:jax._src.custom_derivatives.(fwd, _)->argnums_partial(lu.wrap_init(self.fwd), dyn_argnums, args, require_static_args_hashable=False)
A:jax._src.custom_derivatives.bwd->list(todos).pop()(bwd)
A:jax._src.custom_derivatives.(flat_fun, out_type)->_flatten_fun_nokwargs(f_, in_tree)
A:jax._src.custom_derivatives.(flat_fwd, out_trees)->_flatten_fwd(fwd, primal_name, fwd_name, in_tree, out_type)
A:jax._src.custom_derivatives.flat_bwd->_flatten_bwd(bwd, in_tree, in_avals, out_trees)
A:jax._src.custom_derivatives.(res, res_tree)->tree_flatten(res)
A:jax._src.custom_derivatives.(out_tree, res_tree)->out_trees()
A:jax._src.custom_derivatives.(res, cts_out)->split_list(args, [res_tree.num_leaves])
A:jax._src.custom_derivatives.py_res->tree_unflatten(res_tree, res)
A:jax._src.custom_derivatives.py_cts_out->tree_unflatten(out_tree, cts_out)
A:jax._src.custom_derivatives.zero->object()
A:jax._src.custom_derivatives.dummy->tree_unflatten(in_tree, [object()] * in_tree.num_leaves)
A:jax._src.custom_derivatives.(_, in_tree2)->tree_flatten(py_cts_in)
A:jax._src.custom_derivatives.(fwd, env_trace_todo2)->process_env_traces_fwd(fwd, top_trace and top_trace.level, out_trees)
A:jax._src.custom_derivatives.bwd_->jax.linear_util.wrap_init(lambda *args: bwd.call_wrapped(*args))
A:jax._src.custom_derivatives.(fst, env_trace_todo)->jax.linear_util.merge_linear_aux(env_trace_todo1, env_trace_todo2)
A:jax._src.custom_derivatives.custom_vjp_call_p->CustomVJPCallPrimitive('custom_vjp_call')
A:jax._src.custom_derivatives.(outs, cur_todo, bwd_xform)->max(tracers, key=lambda x: x._trace.level)._trace.main.with_cur_sublevel().post_process_custom_vjp_call_fwd(outs, out_trees)
A:jax._src.custom_derivatives.custom_vjp_call_jaxpr_p->jax.core.AxisPrimitive('custom_vjp_call_jaxpr')
A:jax._src.custom_derivatives.(_, args)->split_list(primals, [num_consts])
A:jax._src.custom_derivatives.(consts_dot, args_dot)->split_list(tangents, [num_consts])
A:jax._src.custom_derivatives.(fwd_jaxpr, fwd_consts)->fwd_jaxpr_thunk()
A:jax._src.custom_derivatives.args_dot->map(ad.replace_float0s, args, args_dot)
A:jax._src.custom_derivatives.res_and_primals_out->jax.core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)
A:jax._src.custom_derivatives.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax._src.custom_derivatives.tangents_out->map(ad.recast_to_float0, primals_out, tangents_out)
A:jax._src.custom_derivatives.(_, args_batched)->split_list(in_batched, [num_consts])
A:jax._src.custom_derivatives.(batched_fun_jaxpr, out_batched)->jax.interpreters.batching.batch_jaxpr(fun_jaxpr, axis_size, in_batched, False, axis_name, main_type)
A:jax._src.custom_derivatives.fwd_jaxpr->jax.core.ClosedJaxpr(*fwd_jaxpr_thunk())
A:jax._src.custom_derivatives.(batched_fwd_jaxpr, out_batched)->jax.interpreters.batching.batch_jaxpr(fwd_jaxpr, axis_size, args_batched, False, axis_name, main_type)
A:jax._src.custom_derivatives.batched_bwd->jax.interpreters.batching.batch_custom_vjp_bwd(bwd, axis_name, axis_size, fwd_out_dims, fwd_args_batched, main_type, spmd_axis_name)
A:jax._src.custom_derivatives.batched_outs->jax.core.AxisPrimitive('custom_vjp_call_jaxpr').bind(*args, fun_jaxpr=batched_fun_jaxpr, fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd, out_trees=out_trees, num_consts=num_consts)
A:jax._src.custom_derivatives.batching.axis_primitive_batchers[custom_vjp_call_jaxpr_p]->partial(_custom_vjp_call_jaxpr_vmap, None)
A:jax._src.custom_derivatives.(ans, _)->fun(*args, **kwargs)
A:jax._src.custom_derivatives.(ans, rule)->fun(*args, **kwargs)
A:jax._src.custom_derivatives.(ans_flat, out_tree)->tree_flatten((ans,))
A:jax._src.custom_derivatives.(rule, in_tree)->flatten_fun_nokwargs(lu.wrap_init(rule), out_tree)
A:jax._src.custom_derivatives.(cts_flat, out_tree_)->tree_flatten((cts,))
A:jax._src.custom_derivatives.cts_out->jax.core.Primitive('linear_call').bind(*t_consts, *f_consts, *operands_res, *cts, callee=transpose, transpose=callee, num_callee_consts=len(t_consts), num_transpose_consts=len(f_consts), num_res=len(operands_res))
A:jax._src.custom_derivatives.(flat_args, in_tree)->tree_flatten(example_args)
A:jax._src.custom_derivatives.in_avals->tuple(map(abstractify, flat_args))
A:jax._src.custom_derivatives.x->jax.core.full_lower(x)
A:jax._src.custom_derivatives.vspace->jax.core.full_lower(x).aval.at_least_vspace()
A:jax._src.custom_derivatives.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.custom_derivatives.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, in_avals)
A:jax._src.custom_derivatives.out_tree->out_tree()
A:jax._src.custom_derivatives.((closure_consts, hoisted_consts), merge)->partition_list(_maybe_perturbed, consts)
A:jax._src.custom_derivatives.(args, hoisted_consts)->split_list(args_hconsts, [num_args])
A:jax._src.custom_derivatives.(all_args, in_tree2)->tree_flatten(tuple(args))
A:jax._src.custom_derivatives.(operands_res, res_tree)->tree_flatten(residual_args)
A:jax._src.custom_derivatives.(operands_lin, lin_tree)->tree_flatten(linear_args)
A:jax._src.custom_derivatives.f_in_tree->treedef_tuple((res_tree, lin_tree))
A:jax._src.custom_derivatives.(f, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), f_in_tree)
A:jax._src.custom_derivatives.res_avals->map(abstractify, operands_res)
A:jax._src.custom_derivatives.lin_avals->map(abstractify, operands_lin)
A:jax._src.custom_derivatives.(f_jaxpr, f_consts)->_initial_style_jaxpr(f, (*res_avals, *lin_avals))
A:jax._src.custom_derivatives.f_jaxpr->_close_jaxpr(f_jaxpr)
A:jax._src.custom_derivatives.out_avals->tree_map(core.get_aval, args)
A:jax._src.custom_derivatives.t_in_tree->treedef_tuple((res_tree, out_tree()))
A:jax._src.custom_derivatives.(t, t_out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun_transpose), t_in_tree)
A:jax._src.custom_derivatives.(t_jaxpr, t_consts)->_initial_style_jaxpr(t, (*res_avals, *out_avals))
A:jax._src.custom_derivatives.t_jaxpr->_close_jaxpr(t_jaxpr)
A:jax._src.custom_derivatives.out->unreachable_p.bind(*args_flat, out_avals=out_avals_flat, exc_type=exc_type, message=message)
A:jax._src.custom_derivatives.(consts, _, operands_res, operands_lin)->split_list(args, [num_callee_consts, num_transpose_consts, num_res])
A:jax._src.custom_derivatives.(f_consts, t_consts, operands_res, operands_lin)->split_list(args, [num_callee_consts, num_transpose_consts, num_res])
A:jax._src.custom_derivatives.(_, _, cts_avals)->split_list(transpose.in_avals, [num_transpose_consts, num_res])
A:jax._src.custom_derivatives.linear_call_p->jax.core.Primitive('linear_call')
A:jax._src.custom_derivatives.(out_avals_flat, out_tree)->tree_flatten(out_avals)
A:jax._src.custom_derivatives.disallow_jvp->partial(unreachable, exc_type=TypeError, message="can't apply forward-mode autodiff (jvp) to a custom_vjp function.")
A:jax._src.custom_derivatives.(outs, residuals)->fwd(*primals)
A:jax._src.custom_derivatives.tan_out_types->tree_map(lambda o: core.get_aval(o).at_least_vspace(), outs)
A:jax._src.custom_derivatives.tan_fn->custom_transpose(partial(disallow_jvp, out_avals=tan_out_types))
A:jax._src.custom_derivatives.custom_jvp_call_jaxpr_p->jax.core.Primitive('custom_jvp_call_jaxpr')
jax._src.custom_derivatives.CustomJVPCallPrimitive(core.Primitive)
jax._src.custom_derivatives.CustomJVPCallPrimitive.bind(self,fun,jvp,*args)
jax._src.custom_derivatives.CustomJVPCallPrimitive.get_bind_params(self,params)
jax._src.custom_derivatives.CustomJVPCallPrimitive.impl(self,fun,_,*args)
jax._src.custom_derivatives.CustomJVPCallPrimitive.post_process(self,trace,out_tracers,jvp_was_run:bool)
jax._src.custom_derivatives.CustomVJPCallPrimitive(core.CallPrimitive)
jax._src.custom_derivatives.CustomVJPCallPrimitive.bind(self,fun,fwd,bwd,*args,out_trees)
jax._src.custom_derivatives.CustomVJPCallPrimitive.impl(self,fun,fwd,bwd,*args,out_trees)
jax._src.custom_derivatives.CustomVJPCallPrimitive.post_process(self,trace,out_tracers,params)
jax._src.custom_derivatives.Residuals(self,jaxpr,in_tree,out_tree,consts)
jax._src.custom_derivatives.Residuals.__init__(self,jaxpr,in_tree,out_tree,consts)
jax._src.custom_derivatives.Residuals.__iter__(self)
jax._src.custom_derivatives.Residuals.tree_flatten(self)
jax._src.custom_derivatives.Residuals.tree_unflatten(cls,aux,consts)
jax._src.custom_derivatives._add_args(f,extra_args)
jax._src.custom_derivatives._add_args_(extra_args,*args,**kwargs)
jax._src.custom_derivatives._apply_bwd_transform(todos,bwd)
jax._src.custom_derivatives._apply_todos(todos,outs)
jax._src.custom_derivatives._check_for_tracers(x)
jax._src.custom_derivatives._close_jaxpr(jaxpr)
jax._src.custom_derivatives._closure_convert_for_avals(fun,in_tree,in_avals)
jax._src.custom_derivatives._custom_jvp_call_mlir_translation(ctx,*args,call_jaxpr,jvp_jaxpr_thunk,num_consts)
jax._src.custom_derivatives._custom_jvp_call_transpose(params,jaxpr,args,ct,_,reduce_axes)
jax._src.custom_derivatives._custom_jvp_call_typecheck(*in_avals,call_jaxpr,jvp_jaxpr_thunk,num_consts)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_abstract_eval(*_,fun_jaxpr,**__)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_impl(*args,fun_jaxpr,**_)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_jvp(primals,tangents,*,fun_jaxpr:core.ClosedJaxpr,fwd_jaxpr_thunk:Callable[[],Tuple[core.Jaxpr,Sequence[Any]]],bwd:lu.WrappedFun,out_trees:Callable,num_consts:int)
jax._src.custom_derivatives._custom_vjp_call_jaxpr_vmap(spmd_axis_name,axis_size,axis_name,main_type,args,in_dims,*,fun_jaxpr:core.ClosedJaxpr,fwd_jaxpr_thunk:Callable[[],Tuple[core.Jaxpr,Sequence[Any]]],bwd:lu.WrappedFun,out_trees:Callable,num_consts:int)
jax._src.custom_derivatives._flatten_bwd(in_tree,in_avals,out_trees,*args)
jax._src.custom_derivatives._flatten_fun_nokwargs(in_tree,*args_flat)
jax._src.custom_derivatives._flatten_fwd(primal_name,fwd_name,in_tree,maybe_out_type,*args)
jax._src.custom_derivatives._flatten_jvp(primal_name,jvp_name,in_tree,maybe_out_type,*args)
jax._src.custom_derivatives._initial_style_jaxpr(fun,in_avals)
jax._src.custom_derivatives._initial_style_staging()->bool
jax._src.custom_derivatives._linear_call_abstract_eval(*args,**kwargs)
jax._src.custom_derivatives._linear_call_impl(*args,callee,transpose,num_callee_consts,num_transpose_consts,num_res)
jax._src.custom_derivatives._linear_call_transpose_rule(cts,*args,callee,transpose,num_callee_consts,num_transpose_consts,num_res)
jax._src.custom_derivatives._maybe_perturbed(x:Any)->bool
jax._src.custom_derivatives._resolve_kwargs(fun,args,kwargs)
jax._src.custom_derivatives._stop_gradient(x)
jax._src.custom_derivatives._sum_tangents(_,x,*xs)
jax._src.custom_derivatives._zeros_like_pytree(x)
jax._src.custom_derivatives.abstractify(x)
jax._src.custom_derivatives.closure_convert(fun,*example_args)
jax._src.custom_derivatives.custom_gradient(fun)
jax._src.custom_derivatives.custom_jvp(self,fun:Callable[...,ReturnValue],nondiff_argnums:Tuple[int,...]=())
jax._src.custom_derivatives.custom_jvp.__init__(self,fun:Callable[...,ReturnValue],nondiff_argnums:Tuple[int,...]=())
jax._src.custom_derivatives.custom_jvp.defjvp(self,jvp:Callable[...,Tuple[ReturnValue,ReturnValue]])->Callable[..., Tuple[ReturnValue, ReturnValue]]
jax._src.custom_derivatives.custom_jvp.defjvps(self,*jvps:Optional[Callable[...,ReturnValue]])
jax._src.custom_derivatives.custom_vjp(self,fun:Callable[...,ReturnValue],nondiff_argnums:Tuple[int,...]=())
jax._src.custom_derivatives.custom_vjp.__init__(self,fun:Callable[...,ReturnValue],nondiff_argnums:Tuple[int,...]=())
jax._src.custom_derivatives.custom_vjp.defvjp(self,fwd:Callable[...,Tuple[ReturnValue,Any]],bwd:Callable[...,Tuple[Any,...]])->None
jax._src.custom_derivatives.custom_vjp_by_custom_transpose(fun,fwd,bwd)
jax._src.custom_derivatives.linear_call(fun:Callable,fun_transpose:Callable,residual_args,linear_args)
jax._src.custom_derivatives.partition_list(choice,lst)
jax._src.custom_derivatives.process_env_traces(primitive,level:int,jvp_was_run:bool,*args)
jax._src.custom_derivatives.process_env_traces_fwd(level:int,out_trees,*args)
jax._src.custom_derivatives.unreachable(*args,out_avals=None,exc_type=TypeError,message='unreachable')
jax._src.custom_derivatives.unreachable_impl(*_,out_avals,exc_type,message)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/callback.py----------------------------------------
A:jax._src.callback.pure_callback_p->jax.core.Primitive('pure_callback')
A:jax._src.callback.axis_size->next((a.shape[0] for (a, d) in zip(args, dims) if d is not batching.not_mapped))
A:jax._src.callback.result_avals->jax.tree_util.tree_map(lambda x: core.ShapedArray(x.shape, x.dtype), result_shape_dtypes)
A:jax._src.callback.outvals->lax_map(_batch_fun, batched_args)
A:jax._src.callback.(unbatched_args, batched_args)->jax._src.util.partition_list(is_batched, new_args)
A:jax._src.callback.merged_args->jax._src.util.merge_lists(is_batched, unbatched_args, batched_args)
A:jax._src.callback.(result, _, keepalive)->jax.interpreters.mlir.emit_python_callback(ctx, _callback, None, list(args), ctx.avals_in, ctx.avals_out, False, sharding=None)
A:jax._src.callback.dt->numpy.dtype(shape_dtype.dtype)
A:jax._src.callback.(args, kwargs)->jax.tree_util.tree_unflatten(in_tree, flat_args)
A:jax._src.callback.(flat_args, in_tree)->jax.tree_util.tree_flatten((args, kwargs))
A:jax._src.callback.(flat_result_avals, out_tree)->jax.tree_util.tree_flatten(result_avals)
A:jax._src.callback.out_flat->jax.core.Primitive('pure_callback').bind(*flat_args, callback=_flat_callback, result_avals=tuple(flat_result_avals), vectorized=vectorized)
jax._src.callback._check_shape_dtype(shape_dtype)
jax._src.callback.pure_callback(callback:Callable[...,Any],result_shape_dtypes:Any,*args:Any,vectorized:bool=False,**kwargs:Any)
jax._src.callback.pure_callback_abstract_eval(*avals,callback:Callable[...,Any],result_avals,vectorized:bool)
jax._src.callback.pure_callback_batching_rule(args,dims,*,callback,vectorized:bool,result_avals:Sequence[core.ShapedArray])
jax._src.callback.pure_callback_impl(*args,result_avals,callback:Callable[...,Any],vectorized:bool)
jax._src.callback.pure_callback_jvp_rule(*args,**kwargs)
jax._src.callback.pure_callback_lowering(ctx,*args,callback,**params)
jax._src.callback.pure_callback_transpose_rule(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/abstract_arrays.py----------------------------------------
A:jax._src.abstract_arrays.dtype->jax._src.dtypes._scalar_type_to_dtype(t, x)
A:jax._src.abstract_arrays.(dtype, weak_type)->jax._src.dtypes._lattice_result_type(x)
A:jax._src.abstract_arrays.aval->jax.core.ShapedArray((), dtype, weak_type=True)
A:jax._src.abstract_arrays.core.pytype_aval_mappings[t]->partial(_make_concrete_python_scalar, t)
A:jax._src.abstract_arrays.ad_util.jaxval_zeros_likers[t]->partial(_zeros_like_python_scalar, t)
jax._src.abstract_arrays._make_concrete_python_scalar(t,x)
jax._src.abstract_arrays._zeros_like_python_scalar(t,x)
jax._src.abstract_arrays.canonical_concrete_aval(val,weak_type=None)
jax._src.abstract_arrays.make_shaped_array(x)
jax._src.abstract_arrays.zeros_like_array(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/public_test_util.py----------------------------------------
A:jax._src.public_test_util.tol->_default_tolerance.copy()
A:jax._src.public_test_util.dtype->_dtype(x)
A:jax._src.public_test_util.atol->_merge_tolerance(atol, default_gradient_tolerance)
A:jax._src.public_test_util.rtol->_merge_tolerance(rtol, default_gradient_tolerance)
A:jax._src.public_test_util.assert_close->partial(_assert_numpy_close, atol=atol, rtol=rtol, err_msg=err_msg)
A:jax._src.public_test_util.add->partial(tree_map, lambda x, y: np.add(x, y, dtype=_dtype(x)))
A:jax._src.public_test_util.sub->partial(tree_map, lambda x, y: np.subtract(x, y, dtype=_dtype(x)))
A:jax._src.public_test_util.safe_sub->partial(tree_map, lambda x, y: _safe_subtract(x, y, dtype=_dtype(x)))
A:jax._src.public_test_util.conj->partial(tree_map, lambda x: np.conj(x, dtype=_dtype(x)))
A:jax._src.public_test_util.shape->numpy.shape(x)
A:jax._src.public_test_util.delta->scalar_mul(tangents, eps)
A:jax._src.public_test_util.f_pos->f(*add(primals, delta))
A:jax._src.public_test_util.f_neg->f(*sub(primals, delta))
A:jax._src.public_test_util.out->default.copy()
A:jax._src.public_test_util.rng->numpy.random.RandomState(0)
A:jax._src.public_test_util.tangent->tree_map(_rand_like, args)
A:jax._src.public_test_util.(v_out, t_out)->f_jvp(args, tangent)
A:jax._src.public_test_util.v_out_expected->f(*args)
A:jax._src.public_test_util.t_out_expected->numerical_jvp(f, args, tangent, eps=eps)
A:jax._src.public_test_util._rand_like->partial(rand_like, np.random.RandomState(0))
A:jax._src.public_test_util.(v_out, vjpfun)->f_vjp(*args)
A:jax._src.public_test_util.tangent_out->numerical_jvp(f, args, tangent, eps=eps)
A:jax._src.public_test_util.cotangent->tree_map(_rand_like, v_out)
A:jax._src.public_test_util.cotangent_out->conj(vjpfun(conj(cotangent)))
A:jax._src.public_test_util.ip->inner_prod(tangent, cotangent_out)
A:jax._src.public_test_util.ip_expected->inner_prod(tangent_out, cotangent)
A:jax._src.public_test_util.args->tuple(args)
A:jax._src.public_test_util._check_jvp->partial(check_jvp, atol=atol, rtol=rtol, eps=eps)
A:jax._src.public_test_util._check_vjp->partial(check_vjp, atol=atol, rtol=rtol, eps=eps)
A:jax._src.public_test_util.(out_primal_py, vjp_py)->jax._src.api.vjp(f, *args)
jax._src.public_test_util._assert_numpy_allclose(a,b,atol=None,rtol=None,err_msg='')
jax._src.public_test_util._assert_numpy_close(a,b,atol=None,rtol=None,err_msg='')
jax._src.public_test_util._check_dtypes_match(xs,ys)
jax._src.public_test_util._dtype(x)
jax._src.public_test_util._merge_tolerance(tol,default)
jax._src.public_test_util._safe_subtract(x,y,*,dtype)
jax._src.public_test_util.check_close(xs,ys,atol=None,rtol=None,err_msg='')
jax._src.public_test_util.check_grads(f,args,order,modes=('fwd','rev'),atol=None,rtol=None,eps=None)
jax._src.public_test_util.check_jvp(f,f_jvp,args,atol=None,rtol=None,eps=EPS,err_msg='')
jax._src.public_test_util.check_vjp(f,f_vjp,args,atol=None,rtol=None,eps=EPS,err_msg='')
jax._src.public_test_util.default_tolerance()
jax._src.public_test_util.device_under_test()
jax._src.public_test_util.inner_prod(xs,ys)
jax._src.public_test_util.numerical_jvp(f,primals,tangents,eps=EPS)
jax._src.public_test_util.rand_like(rng,x)
jax._src.public_test_util.scalar_mul(xs,a)
jax._src.public_test_util.tolerance(dtype,tol=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/typing.py----------------------------------------
jax._src.typing.HasDTypeAttribute(Protocol)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/image/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/image/scale.py----------------------------------------
A:jax._src.image.scale.out->jax.numpy.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)
A:jax._src.image.scale.dtype->jax.numpy.result_type(scale, translation)
A:jax._src.image.scale.weights->jax.numpy.where(jnp.abs(total_weight_sum) > 1000.0 * float(np.finfo(np.float32).eps), jnp.divide(weights, jnp.where(total_weight_sum != 0, total_weight_sum, 1)), 0)
A:jax._src.image.scale.total_weight_sum->jax.numpy.sum(weights, axis=0, keepdims=True)
A:jax._src.image.scale.in_indices->list(range(len(output_shape)))
A:jax._src.image.scale.out_indices->list(range(len(output_shape)))
A:jax._src.image.scale.d->canonicalize_axis(d, x.ndim)
A:jax._src.image.scale.w->compute_weight_mat(m, n, scale[i], translation[i], kernel, antialias).astype(x.dtype)
A:jax._src.image.scale.shape->jax.core.canonicalize_shape(shape)
A:jax._src.image.scale.method->ResizeMethod.from_string(method)
A:jax._src.image.scale.(image,)->_promote_dtypes_inexact(image)
A:jax._src.image.scale.(scale, translation)->_promote_dtypes_inexact(scale, translation)
A:jax._src.image.scale.spatial_dims->tuple((i for i in range(len(shape)) if not core.symbolic_equal_dim(image.shape[i], shape[i])))
A:jax._src.image.scale.offsets->jax.numpy.floor(offsets.astype(np.float32)).astype(np.int32)
jax._src.image.scale.ResizeMethod(enum.Enum)
jax._src.image.scale.ResizeMethod.from_string(s:str)
jax._src.image.scale._fill_keys_cubic_kernel(x)
jax._src.image.scale._fill_lanczos_kernel(radius,x)
jax._src.image.scale._fill_triangle_kernel(x)
jax._src.image.scale._resize(image,shape:core.Shape,method:Union[str,ResizeMethod],antialias:bool,precision)
jax._src.image.scale._resize_nearest(x,output_shape:core.Shape)
jax._src.image.scale._scale_and_translate(x,output_shape:core.Shape,spatial_dims:Sequence[int],scale,translation,kernel,antialias:bool,precision)
jax._src.image.scale.compute_weight_mat(input_size:core.DimSize,output_size:core.DimSize,scale,translation,kernel:Callable,antialias:bool)
jax._src.image.scale.resize(image,shape:core.Shape,method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)
jax._src.image.scale.scale_and_translate(image,shape:core.Shape,spatial_dims:Sequence[int],scale,translation,method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/clusters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/clusters/slurm_cluster.py----------------------------------------
A:jax._src.clusters.slurm_cluster.ind->next((i for (i, ch) in enumerate(node_list) if ch in delims), len(node_list))
A:jax._src.clusters.slurm_cluster.ind2->next((i for (i, ch) in enumerate(suffix) if ch in delims2), None)
jax._src.clusters.SlurmCluster(ClusterEnv)
jax._src.clusters.SlurmCluster.get_coordinator_address(cls)->str
jax._src.clusters.SlurmCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.SlurmCluster.get_process_count(cls)->int
jax._src.clusters.SlurmCluster.get_process_id(cls)->int
jax._src.clusters.SlurmCluster.is_env_present(cls)->bool
jax._src.clusters.slurm_cluster.SlurmCluster(ClusterEnv)
jax._src.clusters.slurm_cluster.SlurmCluster.get_coordinator_address(cls)->str
jax._src.clusters.slurm_cluster.SlurmCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.slurm_cluster.SlurmCluster.get_process_count(cls)->int
jax._src.clusters.slurm_cluster.SlurmCluster.get_process_id(cls)->int
jax._src.clusters.slurm_cluster.SlurmCluster.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/clusters/cluster.py----------------------------------------
A:jax._src.clusters.cluster.env->next((env for env in cls._cluster_types if env.is_env_present()), None)
A:jax._src.clusters.cluster.coordinator_address->next((env for env in cls._cluster_types if env.is_env_present()), None).get_coordinator_address()
A:jax._src.clusters.cluster.num_processes->next((env for env in cls._cluster_types if env.is_env_present()), None).get_process_count()
A:jax._src.clusters.cluster.process_id->next((env for env in cls._cluster_types if env.is_env_present()), None).get_process_id()
jax._src.clusters.ClusterEnv
jax._src.clusters.ClusterEnv.__init_subclass__(cls,**kwargs)
jax._src.clusters.ClusterEnv.auto_detect_unset_distributed_params(cls,coordinator_address:Optional[str],num_processes:Optional[int],process_id:Optional[int],local_device_ids:Optional[Sequence[int]])->Tuple[Optional[str], Optional[int], Optional[int], Optional[Sequence[int]]]
jax._src.clusters.ClusterEnv.get_coordinator_address(cls)->str
jax._src.clusters.ClusterEnv.get_local_process_id(cls)->Optional[int]
jax._src.clusters.ClusterEnv.get_process_count(cls)->int
jax._src.clusters.ClusterEnv.get_process_id(cls)->int
jax._src.clusters.ClusterEnv.is_env_present(cls)->bool
jax._src.clusters.cluster.ClusterEnv
jax._src.clusters.cluster.ClusterEnv.__init_subclass__(cls,**kwargs)
jax._src.clusters.cluster.ClusterEnv.auto_detect_unset_distributed_params(cls,coordinator_address:Optional[str],num_processes:Optional[int],process_id:Optional[int],local_device_ids:Optional[Sequence[int]])->Tuple[Optional[str], Optional[int], Optional[int], Optional[Sequence[int]]]
jax._src.clusters.cluster.ClusterEnv.get_coordinator_address(cls)->str
jax._src.clusters.cluster.ClusterEnv.get_local_process_id(cls)->Optional[int]
jax._src.clusters.cluster.ClusterEnv.get_process_count(cls)->int
jax._src.clusters.cluster.ClusterEnv.get_process_id(cls)->int
jax._src.clusters.cluster.ClusterEnv.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/clusters/cloud_tpu_cluster.py----------------------------------------
jax._src.clusters.TpuCluster(ClusterEnv)
jax._src.clusters.TpuCluster._get_worker_endpoints()->str
jax._src.clusters.TpuCluster.get_coordinator_address(cls)->str
jax._src.clusters.TpuCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.TpuCluster.get_process_count(cls)->int
jax._src.clusters.TpuCluster.get_process_id(cls)->int
jax._src.clusters.TpuCluster.is_env_present(cls)->bool
jax._src.clusters.cloud_tpu_cluster.TpuCluster(ClusterEnv)
jax._src.clusters.cloud_tpu_cluster.TpuCluster._get_worker_endpoints()->str
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_coordinator_address(cls)->str
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_local_process_id(cls)->Optional[int]
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_process_count(cls)->int
jax._src.clusters.cloud_tpu_cluster.TpuCluster.get_process_id(cls)->int
jax._src.clusters.cloud_tpu_cluster.TpuCluster.is_env_present(cls)->bool


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/fft.py----------------------------------------
A:jax._src.scipy.fft.(N_arr, k)->_promote_dtypes_complex(N, k)
A:jax._src.scipy.fft.v0->jax.lax.slice_in_dim(x, None, None, 2, axis)
A:jax._src.scipy.fft.v1->jax.lax.rev(lax.slice_in_dim(x, 1, None, 2, axis), (axis,))
A:jax._src.scipy.fft.factor->jax.lax.expand_dims(factor, [a for a in range(out.ndim) if a != axis])
A:jax._src.scipy.fft.axis->canonicalize_axis(axis, x.ndim)
A:jax._src.scipy.fft.x->dctn(x, axes=axes_block, norm=norm)
A:jax._src.scipy.fft.v->_dct_interleave(_dct_interleave(x, axis1), axis2)
A:jax._src.scipy.fft.V->jax.numpy.fft.fftn(v, axes=axes)
A:jax._src.scipy.fft.k->jax.lax.expand_dims(jnp.arange(N, dtype=V.real.dtype), [a for a in range(x.ndim) if a != axis])
A:jax._src.scipy.fft.out->_dct_ortho_norm(out, axis)
A:jax._src.scipy.fft.(axis1, axis2)->map(partial(canonicalize_axis, num_dims=x.ndim), axes)
A:jax._src.scipy.fft.k1->jax.lax.expand_dims(jnp.arange(N1, dtype=V.dtype), [a for a in range(x.ndim) if a != axis1])
A:jax._src.scipy.fft.k2->jax.lax.expand_dims(jnp.arange(N2, dtype=V.dtype), [a for a in range(x.ndim) if a != axis2])
A:jax._src.scipy.fft.axes->range(x.ndim)
jax._src.scipy.fft._W4(N:int,k:Array)->Array
jax._src.scipy.fft._dct2(x:Array,axes:Sequence[int],norm:Optional[str])->Array
jax._src.scipy.fft._dct_interleave(x:Array,axis:int)->Array
jax._src.scipy.fft._dct_ortho_norm(out:Array,axis:int)->Array
jax._src.scipy.fft.dct(x:Array,type:int=2,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.scipy.fft.dctn(x:Array,type:int=2,s:Optional[Sequence[int]]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/ndimage.py----------------------------------------
A:jax._src.scipy.ndimage.index->jax._src.numpy.lax_numpy.floor(coordinate).astype(jnp.int32)
A:jax._src.scipy.ndimage.weight->coordinate.dtype.type(1)
A:jax._src.scipy.ndimage.lower->jax._src.numpy.lax_numpy.floor(coordinate)
A:jax._src.scipy.ndimage.input_arr->jax._src.numpy.lax_numpy.asarray(input)
A:jax._src.scipy.ndimage.cval->jax._src.numpy.lax_numpy.asarray(cval, input_arr.dtype)
A:jax._src.scipy.ndimage.index_fixer->_INDEX_FIXERS.get(mode)
A:jax._src.scipy.ndimage.interp_nodes->interp_fun(coordinate)
A:jax._src.scipy.ndimage.fixed_index->index_fixer(index, size)
A:jax._src.scipy.ndimage.valid->is_valid(index, size)
A:jax._src.scipy.ndimage.(indices, validities, weights)->jax._src.util.unzip3(items)
A:jax._src.scipy.ndimage.all_valid->functools.reduce(operator.and_, validities)
A:jax._src.scipy.ndimage.contribution->jax._src.numpy.lax_numpy.where(all_valid, input_arr[indices], cval)
A:jax._src.scipy.ndimage.result->_round_half_away_from_zero(result)
jax._src.scipy.ndimage._linear_indices_and_weights(coordinate:ArrayLike)->List[Tuple[Array, int]]
jax._src.scipy.ndimage._map_coordinates(input:ArrayLike,coordinates:Sequence[ArrayLike],order:int,mode:str,cval:ArrayLike)->Array
jax._src.scipy.ndimage._mirror_index_fixer(index:Array,size:int)->Array
jax._src.scipy.ndimage._nearest_indices_and_weights(coordinate:Array)->List[Tuple[Array, int]]
jax._src.scipy.ndimage._nonempty_prod(arrs:Sequence[Array])->Array
jax._src.scipy.ndimage._nonempty_sum(arrs:Sequence[Array])->Array
jax._src.scipy.ndimage._reflect_index_fixer(index:Array,size:int)->Array
jax._src.scipy.ndimage._round_half_away_from_zero(a:Array)->Array
jax._src.scipy.ndimage.map_coordinates(input:ArrayLike,coordinates:Sequence[ArrayLike],order:int,mode:str='constant',cval:ArrayLike=0.0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/special.py----------------------------------------
A:jax._src.scipy.special.(x,)->_promote_args_inexact('exp1', x)
A:jax._src.scipy.special.(x, y)->_promote_args_inexact('xlog1py', x, y)
A:jax._src.scipy.special.(a, b, x)->_promote_args_inexact('betainc', a, b, x)
A:jax._src.scipy.special.(a, x)->_promote_args_inexact('gammaincc', a, x)
A:jax._src.scipy.special.(a, b)->_promote_args_inexact('logsumexp', a, b)
A:jax._src.scipy.special.a->jax.numpy.where(b != 0, a, -jnp.inf)
A:jax._src.scipy.special.(a,)->_promote_args_inexact('logsumexp', a)
A:jax._src.scipy.special.(pos_dims, dims)->_reduction_dims(a, axis)
A:jax._src.scipy.special.amax->jax.lax.stop_gradient(lax.select(jnp.isfinite(amax), amax, lax.full_like(amax, 0)))
A:jax._src.scipy.special.out->jax.numpy.where(sign < 0, jnp.array(np.nan, dtype=out.dtype), out)
A:jax._src.scipy.special.sign->jax.lax.stop_gradient(jnp.sign(sumexp))
A:jax._src.scipy.special.expsub->jax.lax.mul(expsub, b)
A:jax._src.scipy.special.sumexp->jax.numpy.sum(expsub, axis=dims, keepdims=keepdims)
A:jax._src.scipy.special.safe_x->jax.numpy.where(x_ok, x, 1.0)
A:jax._src.scipy.special.safe_y->jax.numpy.where(x_ok, y, 1.0)
A:jax._src.scipy.special.d->jax.lax.while_loop(cond, body, init)
A:jax._src.scipy.special.(a, d_)->_promote_args_inexact('multigammaln', a, d)
A:jax._src.scipy.special.constant->jax.lax.mul(lax.mul(lax.mul(_lax_const(a, 0.25), d_), lax.sub(d_, _lax_const(a, 1))), lax.log(_lax_const(a, np.pi)))
A:jax._src.scipy.special.b->jax.lax.div(jnp.arange(d, dtype=d_.dtype), _lax_const(a, 2))
A:jax._src.scipy.special.res->jax.numpy.sum(gammaln(jnp.expand_dims(a, axis=-1) - jnp.expand_dims(b, axis=tuple(range(a.ndim)))), axis=-1)
A:jax._src.scipy.special.(s, a)->_promote_args_inexact('zeta', x, q)
A:jax._src.scipy.special.k->jax.numpy.expand_dims(np.arange(N, dtype=N.dtype), tuple(range(a.ndim)))
A:jax._src.scipy.special.S->jax.numpy.sum((a_ + k) ** (-s_), -1)
A:jax._src.scipy.special.I->jax.lax.div((a + N) ** (dtype(1) - s), s - dtype(1))
A:jax._src.scipy.special.m->jax.core.concrete_or_error(int, m, 'Argument m of lpmn.')
A:jax._src.scipy.special.T1->jax.numpy.clip(T1, a_max=jnp.finfo(dtype).max)
A:jax._src.scipy.special.coefs->numpy.expand_dims(np.array(_BERNOULLI_COEFS[:T1.shape[-1]], dtype=dtype), tuple(range(a.ndim)))
A:jax._src.scipy.special.(n, x)->_promote_args_inexact('expn', n, x)
A:jax._src.scipy.special.shape->jax.numpy.shape(p)
A:jax._src.scipy.special._LOGNDTR_FLOAT64_LOWER->numpy.array(-20, np.float64)
A:jax._src.scipy.special._LOGNDTR_FLOAT32_LOWER->numpy.array(-10, np.float32)
A:jax._src.scipy.special._LOGNDTR_FLOAT64_UPPER->numpy.array(8, np.float64)
A:jax._src.scipy.special._LOGNDTR_FLOAT32_UPPER->numpy.array(5, np.float32)
A:jax._src.scipy.special.x->jax.numpy.array(x)
A:jax._src.scipy.special.dtype->jax.lax.dtype(z)
A:jax._src.scipy.special.z->jax.lax.sqrt(dtype(-2.0) * lax.log(sanitized_mcp))
A:jax._src.scipy.special.y->jax.numpy.cumprod(jnp.broadcast_to(jnp.sqrt(1.0 - x * x), (l_max, x.shape[0])), axis=0)
A:jax._src.scipy.special.p0->list(reversed([-59.96335010141079, 98.00107541859997, -56.67628574690703, 13.931260938727968, -1.2391658386738125]))
A:jax._src.scipy.special.q0->list(reversed([1.0, 1.9544885833814176, 4.676279128988815, 86.36024213908905, -225.46268785411937, 200.26021238006066, -82.03722561683334, 15.90562251262117, -1.1833162112133]))
A:jax._src.scipy.special.p1->list(reversed([4.0554489230596245, 31.525109459989388, 57.16281922464213, 44.08050738932008, 14.684956192885803, 2.1866330685079025, -0.1402560791713545, -0.03504246268278482, -0.0008574567851546854]))
A:jax._src.scipy.special.q1->list(reversed([1.0, 15.779988325646675, 45.39076351288792, 41.3172038254672, 15.04253856929075, 2.504649462083094, -0.14218292285478779, -0.03808064076915783, -0.0009332594808954574]))
A:jax._src.scipy.special.p2->list(reversed([3.2377489177694603, 6.915228890689842, 3.9388102529247444, 1.3330346081580755, 0.20148538954917908, 0.012371663481782003, 0.00030158155350823543, 2.6580697468673755e-06, 6.239745391849833e-09]))
A:jax._src.scipy.special.q2->list(reversed([1.0, 6.02427039364742, 3.6798356385616087, 1.3770209948908132, 0.21623699359449663, 0.013420400608854318, 0.00032801446468212774, 2.8924786474538068e-06, 6.790194080099813e-09]))
A:jax._src.scipy.special.coeffs->numpy.array(coeffs, dtype)
A:jax._src.scipy.special.maybe_complement_p->jax.numpy.where(p > dtype(-np.expm1(-2.0)), dtype(1.0) - p, p)
A:jax._src.scipy.special.sanitized_mcp->jax.numpy.where(maybe_complement_p <= dtype(0.0), jnp.full(shape, dtype(0.5)), maybe_complement_p)
A:jax._src.scipy.special.ww->jax.lax.square(w)
A:jax._src.scipy.special.infinity->jax.numpy.full(shape, dtype(np.inf))
A:jax._src.scipy.special.x_nan_replaced->jax.numpy.where(p <= dtype(0.0), -infinity, jnp.where(p >= dtype(1.0), infinity, x))
A:jax._src.scipy.special.ans->log_ndtr(x, series_order=series_order)
A:jax._src.scipy.special.t_out->jax.lax.mul(t, lax.exp(lax.sub(_norm_logpdf(x), ans)))
A:jax._src.scipy.special.x_2->jax.lax.square(x)
A:jax._src.scipy.special.even_sum->jax.numpy.zeros_like(x)
A:jax._src.scipy.special.odd_sum->jax.numpy.zeros_like(x)
A:jax._src.scipy.special._norm_logpdf_constant->numpy.log(np.sqrt(2 * np.pi))
A:jax._src.scipy.special.neg_half->_lax_const(x, -0.5)
A:jax._src.scipy.special.log_normalizer->_lax_const(x, _norm_logpdf_constant)
A:jax._src.scipy.special.(m_mat, l_mat)->jax.numpy.meshgrid(jnp.arange(num_m, dtype=x.dtype), jnp.arange(num_l, dtype=x.dtype), indexing='ij')
A:jax._src.scipy.special.d0->jax.numpy.sqrt((4.0 * c0 - 1.0) / (c0 - c1))
A:jax._src.scipy.special.d1->jax.numpy.sqrt((c2 + 1.0) * (c3 - c1) / ((c2 - 3.0) * (c0 - c1)))
A:jax._src.scipy.special.d0_mask_indices->jax.numpy.triu_indices(l_max + 1, 1)
A:jax._src.scipy.special.d1_mask_indices->jax.numpy.triu_indices(l_max + 1, 2)
A:jax._src.scipy.special.d_zeros->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype)
A:jax._src.scipy.special.d0_mask->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype).at[d0_mask_indices].set(d0[d0_mask_indices])
A:jax._src.scipy.special.d1_mask->jax.numpy.zeros((l_max + 1, l_max + 1), dtype=dtype).at[d1_mask_indices].set(d1[d1_mask_indices])
A:jax._src.scipy.special.mask->(i + j - k == 0).astype(dtype)
A:jax._src.scipy.special.d0_mask_3d->jax.numpy.einsum('jk,ijk->ijk', d0_mask, mask)
A:jax._src.scipy.special.d1_mask_3d->jax.numpy.einsum('jk,ijk->ijk', d1_mask, mask)
A:jax._src.scipy.special.l_vec->jax.numpy.arange(num_l, dtype=p.dtype)
A:jax._src.scipy.special.update_p_p1->jax.numpy.einsum('i,ij->ij', coeff, p_p1)
A:jax._src.scipy.special.p_mm2_lm1->p_mm2_lm1.at[0, 3:num_l, :].set(update_p_p2).at[0, 3:num_l, :].set(update_p_p2)
A:jax._src.scipy.special.update_p_p2->jax.numpy.einsum('i,ij->ij', coeff, p_p2)
A:jax._src.scipy.special.coeff_zeros->jax.numpy.zeros((num_m, num_l), dtype=x.dtype)
A:jax._src.scipy.special.upper_0_indices->jax.numpy.triu_indices(num_m, 0, num_l)
A:jax._src.scipy.special.zero_vec->jax.numpy.zeros((num_l,), dtype=x.dtype)
A:jax._src.scipy.special.a0_masked->a0_masked.at[1, :].set(zero_vec).at[1, :].set(zero_vec)
A:jax._src.scipy.special.c0_masked->c0_masked.at[1, :].set(zero_vec).at[1, :].set(zero_vec)
A:jax._src.scipy.special.d0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(d0[upper_0_indices])
A:jax._src.scipy.special.e0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(e0[upper_0_indices])
A:jax._src.scipy.special.f0_masked->jax.numpy.zeros((num_m, num_l), dtype=x.dtype).at[upper_0_indices].set(f0[upper_0_indices])
A:jax._src.scipy.special.g0->jax.numpy.einsum('i,ij->ij', (l_vec + 1) * l_vec, p[0, :, :])
A:jax._src.scipy.special.p_derivative_m0->jax.numpy.einsum('j,ij->ij', 0.5 / jnp.sqrt(1 - x * x), g0)
A:jax._src.scipy.special.p_derivative->p_derivative.at[1, 0, :].set(0).at[1, 0, :].set(0)
A:jax._src.scipy.special.p->jax.lax.fori_loop(lower=2, upper=l_max + 1, body_fun=body_fun, init_val=p)
A:jax._src.scipy.special.a_idx->jax.numpy.arange(1, l_max + 1, dtype=x.dtype)
A:jax._src.scipy.special.b_idx->jax.numpy.arange(l_max, dtype=x.dtype)
A:jax._src.scipy.special.f_a->jax.numpy.cumprod(1.0 - 2.0 * a_idx)
A:jax._src.scipy.special.f_b->jax.numpy.sqrt(2.0 * b_idx + 3.0)
A:jax._src.scipy.special.diag_indices->jax.numpy.diag_indices(l_max + 1)
A:jax._src.scipy.special.p_offdiag->jax.numpy.einsum('ij,ij->ij', jnp.einsum('i,j->ij', f_b, x), p[jnp.diag_indices(l_max)])
A:jax._src.scipy.special.(d0_mask_3d, d1_mask_3d)->_gen_recurrence_mask(l_max, is_normalized=is_normalized, dtype=x.dtype)
A:jax._src.scipy.special.n->jax.core.concrete_or_error(int, n, 'Argument n of lpmn.')
A:jax._src.scipy.special.p_vals->_gen_associated_legendre(l_max, z, is_normalized)
A:jax._src.scipy.special.p_derivatives->_gen_derivatives(p_vals, z, is_normalized)
A:jax._src.scipy.special.cos_colatitude->jax.numpy.cos(phi)
A:jax._src.scipy.special.legendre->_gen_associated_legendre(n_max, cos_colatitude, True)
A:jax._src.scipy.special.legendre_val->_gen_associated_legendre(n_max, cos_colatitude, True).at[abs(m), n, jnp.arange(len(n))].get(mode='clip')
A:jax._src.scipy.special.vandermonde->jax.lax.complex(jnp.cos(angle), jnp.sin(angle))
A:jax._src.scipy.special.harmonics->jax.numpy.where(m < 0, (-1.0) ** abs(m) * jnp.conjugate(harmonics), harmonics)
A:jax._src.scipy.special.phi->jax.numpy.array([phi])
A:jax._src.scipy.special.n_max->jax.core.concrete_or_error(int, n_max, 'The `n_max` argument of `jnp.scipy.special.sph_harm` must be statically specified to use `sph_harm` within JAX transformations.')
A:jax._src.scipy.special.one->_c(x, 1)
A:jax._src.scipy.special.ret->jax.numpy.piecewise(x, conds, vals)
A:jax._src.scipy.special.zero->_c(x, 0)
A:jax._src.scipy.special.psi->jax.lax.fori_loop(_c(n, 1), n, lambda i, psi: psi + one / i, psi)
A:jax._src.scipy.special.n1->jax.numpy.where(n == _c(n, 1), n + n, n)
A:jax._src.scipy.special.init->dict(k=_c(n, 1), pkm2=one, qkm2=x, pkm1=one, qkm1=x + n, ans=one / (x + n), t=_c(x, jnp.inf), r=zero, x=x)
A:jax._src.scipy.special.d['t']->jax.numpy.where(nz, abs((d['ans'] - r) / r), one)
A:jax._src.scipy.special.BIG->_c(x, 1.4411518807585587e+17)
A:jax._src.scipy.special.yk->jax.numpy.where(odd, one, x)
A:jax._src.scipy.special.xk->jax.numpy.where(odd, n + (k - _c(k, 1)) / _c(k, 2), k / _c(k, 2))
A:jax._src.scipy.special.d['r']r->jax.numpy.where(nz, pk / qk, d['r'])
A:jax._src.scipy.special.d['ans']->jax.numpy.where(nz, r, d['ans'])
A:jax._src.scipy.special.d[key]->jax.numpy.where(is_big, d[key] / BIG, d[key])
jax._src.scipy.special._double_factorial(n)
jax._src.scipy.special._eval_expint_k(A,B,x)
jax._src.scipy.special._expi_pos(x)
jax._src.scipy.special._expint1(x)
jax._src.scipy.special._expint2(x)
jax._src.scipy.special._expint3(x)
jax._src.scipy.special._expint4(x)
jax._src.scipy.special._expint5(x)
jax._src.scipy.special._expint6(x)
jax._src.scipy.special._expint7(x)
jax._src.scipy.special._expn1(n,x)
jax._src.scipy.special._expn2(n,x)
jax._src.scipy.special._expn3(n,x)
jax._src.scipy.special._gen_associated_legendre(l_max:int,x:jnp.ndarray,is_normalized:bool)->jnp.ndarray
jax._src.scipy.special._gen_derivatives(p:jnp.ndarray,x:jnp.ndarray,is_normalized:bool)->jnp.ndarray
jax._src.scipy.special._gen_recurrence_mask(l_max:int,is_normalized:bool,dtype:Any)->Tuple[jnp.ndarray, jnp.ndarray]
jax._src.scipy.special._log_ndtr_asymptotic_series(x,series_order)
jax._src.scipy.special._log_ndtr_jvp(series_order,primals,tangents)
jax._src.scipy.special._log_ndtr_lower(x,series_order)
jax._src.scipy.special._ndtr(x)
jax._src.scipy.special._ndtri(p)
jax._src.scipy.special._norm_logpdf(x)
jax._src.scipy.special._polygamma(n,x)
jax._src.scipy.special._sph_harm(m:jnp.ndarray,n:jnp.ndarray,theta:jnp.ndarray,phi:jnp.ndarray,n_max:int)->jnp.ndarray
jax._src.scipy.special.betainc(a,b,x)
jax._src.scipy.special.betaln(x,y)
jax._src.scipy.special.digamma(x)
jax._src.scipy.special.entr(x)
jax._src.scipy.special.erf(x)
jax._src.scipy.special.erfc(x)
jax._src.scipy.special.erfinv(x)
jax._src.scipy.special.exp1(x,module='scipy.special')
jax._src.scipy.special.expi(x)
jax._src.scipy.special.expi_jvp(primals,tangents)
jax._src.scipy.special.expit(x)
jax._src.scipy.special.expn(n,x)
jax._src.scipy.special.expn_jvp(n,primals,tangents)
jax._src.scipy.special.gammainc(a,x)
jax._src.scipy.special.gammaincc(a,x)
jax._src.scipy.special.gammaln(x)
jax._src.scipy.special.i0(x)
jax._src.scipy.special.i0e(x)
jax._src.scipy.special.i1(x)
jax._src.scipy.special.i1e(x)
jax._src.scipy.special.log_ndtr(x,series_order=3)
jax._src.scipy.special.logit(x)
jax._src.scipy.special.logsumexp(a,axis=None,b=None,keepdims=False,return_sign=False)
jax._src.scipy.special.lpmn(m:int,n:int,z:jnp.ndarray)->Tuple[jnp.ndarray, jnp.ndarray]
jax._src.scipy.special.lpmn_values(m:int,n:int,z:jnp.ndarray,is_normalized:bool)->jnp.ndarray
jax._src.scipy.special.multigammaln(a,d)
jax._src.scipy.special.ndtr(x)
jax._src.scipy.special.ndtri(p)
jax._src.scipy.special.polygamma(n,x)
jax._src.scipy.special.sph_harm(m:jnp.ndarray,n:jnp.ndarray,theta:jnp.ndarray,phi:jnp.ndarray,n_max:Optional[int]=None)->jnp.ndarray
jax._src.scipy.special.xlog1py(x,y)
jax._src.scipy.special.xlogy(x,y)
jax._src.scipy.special.zeta(x,q=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/linalg.py----------------------------------------
A:jax._src.scipy.linalg._no_chkfinite_doc->textwrap.dedent('\nDoes not support the Scipy argument ``check_finite=True``,\nbecause compiled JAX code cannot perform checks of array values at runtime.\n')
A:jax._src.scipy.linalg.(a,)->_promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.scipy.linalg.l->jax._src.lax.linalg.cholesky(a if lower else jnp.conj(_T(a)), symmetrize_input=False)
A:jax._src.scipy.linalg.(c, b)->_promote_dtypes_inexact(jnp.asarray(c), jnp.asarray(b))
A:jax._src.scipy.linalg.b->jax._src.lax.linalg.triangular_solve(c, b, left_side=True, lower=lower, transpose_a=lower, conjugate_a=lower)
A:jax._src.scipy.linalg.(v, w)->jax._src.lax.linalg.eigh(a, lower=lower)
A:jax._src.scipy.linalg.a->a.T.conj().T.conj()
A:jax._src.scipy.linalg.(lu, pivots, _)->jax._src.lax.linalg.lu(a)
A:jax._src.scipy.linalg.perm->jax._src.lax.linalg.lu_pivots_to_permutation(pivots, m)
A:jax._src.scipy.linalg.(lu, _, permutation)->jax._src.lax.linalg.lu(a)
A:jax._src.scipy.linalg.dtype->jax.lax.dtype(acc)
A:jax._src.scipy.linalg.(m, n)->jax._src.numpy.lax_numpy.shape(a)
A:jax._src.scipy.linalg.p->jax._src.numpy.lax_numpy.real(jnp.array(permutation[None, :] == jnp.arange(m, dtype=permutation.dtype)[:, None], dtype=dtype))
A:jax._src.scipy.linalg.k->min(m, n)
A:jax._src.scipy.linalg.(q, r)->jax._src.lax.linalg.qr(a, full_matrices=full_matrices)
A:jax._src.scipy.linalg.(a, b)->_promote_dtypes_inexact(jnp.asarray(a), jnp.asarray(b))
A:jax._src.scipy.linalg.factors->cho_factor(lax.stop_gradient(a), lower=lower)
A:jax._src.scipy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: lax_linalg._matvec_multiply(a, x), solve=lambda _, x: cho_solve(factors, x), symmetric=True)
A:jax._src.scipy.linalg.out->jax._src.lax.linalg.triangular_solve(a, b, left_side=True, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.scipy.linalg._expm_description->textwrap.dedent('\nIn addition to the original NumPy argument(s) listed below,\nalso supports the optional boolean argument ``upper_triangular``\nto specify whether the ``A`` matrix is upper triangular, and the optional\nargument ``max_squarings`` to specify the max number of squarings allowed\nin the scaling-and-squaring approximation method. Return nan if the actual\nnumber of squarings required is more than ``max_squarings``.\n\nThe number of required squarings = max(0, ceil(log2(norm(A)) - c)\nwhere norm() denotes the L1 norm, and\n\n- c=2.42 for float64 or complex128,\n- c=1.97 for float32 or complex64\n')
A:jax._src.scipy.linalg.(P, Q, n_squarings)->_calc_P_Q(A)
A:jax._src.scipy.linalg.R->jax.lax.cond(n_squarings > max_squarings, _nan, _compute, (A, P, Q))
A:jax._src.scipy.linalg.A->jax._src.numpy.lax_numpy.asarray(A)
A:jax._src.scipy.linalg.A_L1->jax._src.numpy.linalg.norm(A, 1)
A:jax._src.scipy.linalg.n_squarings->jax._src.numpy.lax_numpy.maximum(0, jnp.floor(jnp.log2(A_L1 / maxnorm)))
A:jax._src.scipy.linalg.conds->jax._src.numpy.lax_numpy.array([0.4258730016922831, 1.880152677804762], dtype=A_L1.dtype)
A:jax._src.scipy.linalg.idx->jax._src.numpy.lax_numpy.digitize(A_L1, conds)
A:jax._src.scipy.linalg.(U, V)->jax.lax.switch(idx, [_pade3, _pade5, _pade7], A)
A:jax._src.scipy.linalg.(res, _)->jax.lax.scan(_scan_f, R, jnp.arange(max_squarings, dtype=n_squarings.dtype))
A:jax._src.scipy.linalg.ident->jax._src.numpy.lax_numpy.eye(M, N, dtype=A.dtype)
A:jax._src.scipy.linalg.A2->_precise_dot(A, A)
A:jax._src.scipy.linalg.U->jax.lax.fori_loop(0, n, j_loop, U)
A:jax._src.scipy.linalg.A4->_precise_dot(A2, A2)
A:jax._src.scipy.linalg.A6->_precise_dot(A4, A2)
A:jax._src.scipy.linalg.A8->_precise_dot(A6, A2)
A:jax._src.scipy.linalg._expm_frechet_description->textwrap.dedent("\nDoes not currently support the Scipy argument ``jax.numpy.asarray_chkfinite``,\nbecause `jax.numpy.asarray_chkfinite` does not exist at the moment. Does not\nsupport the ``method='blockEnlarge'`` argument.\n")
A:jax._src.scipy.linalg.E->jax._src.numpy.lax_numpy.asarray(E)
A:jax._src.scipy.linalg.bound_fun->partial(expm, upper_triangular=False, max_squarings=16)
A:jax._src.scipy.linalg.(expm_A, expm_frechet_AE)->jvp(bound_fun, (A,), (E,))
A:jax._src.scipy.linalg.arrs->cast(Tuple[ArrayLike], jnp._promote_dtypes(*arrs))
A:jax._src.scipy.linalg.acc->jax.lax.concatenate([acc, a], dimension=0)
A:jax._src.scipy.linalg.zeros->jax._src.numpy.lax_numpy.zeros(x.shape, dtype=jnp.int32)
A:jax._src.scipy.linalg.ones->jax._src.numpy.lax_numpy.ones(x.shape, dtype=jnp.int32)
A:jax._src.scipy.linalg.count->jax._src.numpy.lax_numpy.where(q <= pivmin, count + 1, count)
A:jax._src.scipy.linalg.q->jax._src.numpy.lax_numpy.where(q <= pivmin, jnp.minimum(q, -pivmin), q)
A:jax._src.scipy.linalg.(q, count)->sturm_step(start + j, q, count)
A:jax._src.scipy.linalg.(i, q, count)->unrolled_steps((i, q, count))
A:jax._src.scipy.linalg.(_, _, count)->jax.lax.while_loop(cond, unrolled_steps, (i, q, count))
A:jax._src.scipy.linalg.alpha->jax._src.numpy.lax_numpy.real(alpha)
A:jax._src.scipy.linalg.beta->jax._src.numpy.lax_numpy.asarray(e)
A:jax._src.scipy.linalg.beta_sq->jax._src.numpy.lax_numpy.square(beta)
A:jax._src.scipy.linalg.beta_abs->jax._src.numpy.lax_numpy.abs(beta)
A:jax._src.scipy.linalg.off_diag_abs_row_sum->jax._src.numpy.lax_numpy.concatenate([beta_abs[:1], beta_abs[:-1] + beta_abs[1:], beta_abs[-1:]], axis=0)
A:jax._src.scipy.linalg.lambda_est_max->jax._src.numpy.lax_numpy.amax(alpha + off_diag_abs_row_sum)
A:jax._src.scipy.linalg.lambda_est_min->jax._src.numpy.lax_numpy.amin(alpha - off_diag_abs_row_sum)
A:jax._src.scipy.linalg.t_norm->jax._src.numpy.lax_numpy.maximum(jnp.abs(lambda_est_min), jnp.abs(lambda_est_max))
A:jax._src.scipy.linalg.finfo->numpy.finfo(alpha.dtype)
A:jax._src.scipy.linalg.one->numpy.ones([], dtype=alpha.dtype)
A:jax._src.scipy.linalg.safemin->numpy.maximum(one / finfo.max, (one + finfo.eps) * finfo.tiny)
A:jax._src.scipy.linalg.alpha0_perturbation->jax._src.numpy.lax_numpy.broadcast_to(alpha0_perturbation, target_shape)
A:jax._src.scipy.linalg.abs_tol->jax._src.numpy.lax_numpy.maximum(tol, abs_tol)
A:jax._src.scipy.linalg.target_counts->jax._src.numpy.lax_numpy.arange(select_range[0], select_range[1] + 1, dtype=jnp.int32)
A:jax._src.scipy.linalg.target_shape->jax._src.numpy.lax_numpy.shape(target_counts)
A:jax._src.scipy.linalg.lower->jax._src.numpy.lax_numpy.where(counts <= target_counts, mid, lower)
A:jax._src.scipy.linalg.upper->jax._src.numpy.lax_numpy.where(counts > target_counts, mid, upper)
A:jax._src.scipy.linalg.pivmin->jax._src.numpy.lax_numpy.broadcast_to(pivmin, target_shape)
A:jax._src.scipy.linalg.counts->_sturm(alpha, beta_sq, pivmin, alpha0_perturbation, mid)
A:jax._src.scipy.linalg.(_, _, mid, _)->jax.lax.while_loop(cond, body, (0, lower, mid, upper))
A:jax._src.scipy.linalg.(unitary, posdef, _, _)->jax._src.lax.qdwh.qdwh(a, is_hermitian=False, eps=eps)
A:jax._src.scipy.linalg.posdef->posdef.T.conj().T.conj()
A:jax._src.scipy.linalg.unitary->unitary.T.conj().T.conj()
A:jax._src.scipy.linalg.(u_svd, s_svd, vh_svd)->jax._src.lax.linalg.svd(a, full_matrices=False)
A:jax._src.scipy.linalg.s_svd->s_svd.astype(u_svd.dtype).astype(u_svd.dtype)
A:jax._src.scipy.linalg.(unitary, _)->polar(a, method, eps, max_iterations)
A:jax._src.scipy.linalg.diag->jax._src.numpy.lax_numpy.sqrt(jnp.diag(T))
A:jax._src.scipy.linalg.s->jax.lax.fori_loop(i + 1, j, lambda k, val: val + U[i, k] * U[k, j], 0.0)
A:jax._src.scipy.linalg.value->jax._src.numpy.lax_numpy.where(T[i, j] == s, 0.0, (T[i, j] - s) / (diag[i] + diag[j]))
A:jax._src.scipy.linalg.(_, U)->jax.lax.fori_loop(0, j, i_loop, (j, U))
A:jax._src.scipy.linalg.(T, Z)->jax.lax.cond(jnp.abs(T[m, m - 1]) > eps * (jnp.abs(T[m - 1, m - 1]) + jnp.abs(T[m, m])), _update_T_Z, lambda m, T, Z: (T, Z), m, T, Z)
A:jax._src.scipy.linalg.sqrt_T->_sqrtm_triu(T)
A:jax._src.scipy.linalg.T->T.at[m, m - 1].set(0.0).at[m, m - 1].set(0.0)
A:jax._src.scipy.linalg.Z->jax.lax.dynamic_update_slice_in_dim(Z, Z_cols @ G.conj().T, m - 1, axis=1)
A:jax._src.scipy.linalg.r->jax._src.numpy.linalg.norm(jnp.array([mu[0], T[m, m - 1]])).astype(T.dtype)
A:jax._src.scipy.linalg.G->jax._src.numpy.lax_numpy.array([[c.conj(), s], [-s, c]], dtype=T.dtype)
A:jax._src.scipy.linalg.T_rows->jax.lax.dynamic_slice_in_dim(T, m - 1, 2, axis=0)
A:jax._src.scipy.linalg.T_rows_new->jax._src.numpy.lax_numpy.where(~col_mask, T_rows, G_dot_T_zeroed_cols)
A:jax._src.scipy.linalg.T_cols->jax.lax.dynamic_slice_in_dim(T, m - 1, 2, axis=1)
A:jax._src.scipy.linalg.T_cols_new->jax._src.numpy.lax_numpy.where(~row_mask, T_cols, T_zeroed_rows_dot_GH)
A:jax._src.scipy.linalg.Z_cols->jax.lax.dynamic_slice_in_dim(Z, m - 1, 2, axis=1)
jax._src.scipy.linalg._calc_P_Q(A:ArrayLike)->Tuple[Array, Array, Array]
jax._src.scipy.linalg._cho_solve(c:ArrayLike,b:ArrayLike,lower:bool)->Array
jax._src.scipy.linalg._cholesky(a:ArrayLike,lower:bool)->Array
jax._src.scipy.linalg._eigh(a:ArrayLike,b:Optional[ArrayLike],lower:bool,eigvals_only:bool,eigvals:None,type:int)->Union[Array, Tuple[Array, Array]]
jax._src.scipy.linalg._lu(a:ArrayLike,permute_l:bool)->Union[Tuple[Array, Array], Tuple[Array, Array, Array]]
jax._src.scipy.linalg._pade13(A:Array)->Tuple[Array, Array]
jax._src.scipy.linalg._pade3(A:Array)->Tuple[Array, Array]
jax._src.scipy.linalg._pade5(A:Array)->Tuple[Array, Array]
jax._src.scipy.linalg._pade7(A:Array)->Tuple[Array, Array]
jax._src.scipy.linalg._pade9(A:Array)->Tuple[Array, Array]
jax._src.scipy.linalg._precise_dot(A:ArrayLike,B:ArrayLike)->Array
jax._src.scipy.linalg._qr(a:ArrayLike,mode:str,pivoting:bool)->Union[Tuple[Array], Tuple[Array, Array]]
jax._src.scipy.linalg._schur(a:Array,output:str)->Tuple[Array, Array]
jax._src.scipy.linalg._solve(a:ArrayLike,b:ArrayLike,assume_a:str,lower:bool)->Array
jax._src.scipy.linalg._solve_P_Q(P:ArrayLike,Q:ArrayLike,upper_triangular:bool=False)->Array
jax._src.scipy.linalg._solve_triangular(a:ArrayLike,b:ArrayLike,trans:Union[int,str],lower:bool,unit_diagonal:bool)->Array
jax._src.scipy.linalg._sqrtm(A:ArrayLike)->Array
jax._src.scipy.linalg._sqrtm_triu(T:Array)->Array
jax._src.scipy.linalg._squaring(R:Array,n_squarings:Array,max_squarings:int)->Array
jax._src.scipy.linalg._svd(a:ArrayLike,*,full_matrices:bool,compute_uv:bool)->Union[Array, Tuple[Array, Array, Array]]
jax._src.scipy.linalg.block_diag(*arrs:ArrayLike)->Array
jax._src.scipy.linalg.cho_factor(a:ArrayLike,lower:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Tuple[Array, bool]
jax._src.scipy.linalg.cho_solve(c_and_lower:Tuple[ArrayLike,bool],b:ArrayLike,overwrite_b:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.cholesky(a:ArrayLike,lower:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.det(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.eigh(a:ArrayLike,b:Optional[ArrayLike]=None,lower:bool=True,eigvals_only:bool=False,overwrite_a:bool=False,overwrite_b:bool=False,turbo:bool=True,eigvals:None=None,type:int=1,check_finite:bool=True)->Union[Array, Tuple[Array, Array]]
jax._src.scipy.linalg.eigh_tridiagonal(d:ArrayLike,e:ArrayLike,*,eigvals_only:bool=False,select:str='a',select_range:Optional[Tuple[float,float]]=None,tol:Optional[float]=None)->Array
jax._src.scipy.linalg.expm(A:ArrayLike,*,upper_triangular:bool=False,max_squarings:int=16)->Array
jax._src.scipy.linalg.expm_frechet(A:ArrayLike,E:ArrayLike,*,method:Optional[str]=None,compute_expm:bool=True)->Union[Array, Tuple[Array, Array]]
jax._src.scipy.linalg.inv(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.lu(a:ArrayLike,permute_l:bool=False,overwrite_a:bool=False,check_finite:bool=True)->Union[Tuple[Array, Array], Tuple[Array, Array, Array]]
jax._src.scipy.linalg.lu_factor(a:ArrayLike,overwrite_a:bool=False,check_finite:bool=True)->Tuple[Array, Array]
jax._src.scipy.linalg.lu_solve(lu_and_piv:Tuple[Array,ArrayLike],b:ArrayLike,trans:int=0,overwrite_b:bool=False,check_finite:bool=True)->Array
jax._src.scipy.linalg.polar(a:ArrayLike,side:str='right',*,method:str='qdwh',eps:Optional[float]=None,max_iterations:Optional[int]=None)->Tuple[Array, Array]
jax._src.scipy.linalg.polar_unitary(a:ArrayLike,*,method:str='qdwh',eps:Optional[float]=None,max_iterations:Optional[int]=None)->Tuple[Array, Array]
jax._src.scipy.linalg.qr(a:ArrayLike,overwrite_a:bool=False,lwork:Any=None,mode:str='full',pivoting:bool=False,check_finite:bool=True)->Union[Tuple[Array], Tuple[Array, Array]]
jax._src.scipy.linalg.rsf2csf(T:ArrayLike,Z:ArrayLike,check_finite:bool=True)->Tuple[Array, Array]
jax._src.scipy.linalg.schur(a:ArrayLike,output:str='real')->Tuple[Array, Array]
jax._src.scipy.linalg.solve(a:ArrayLike,b:ArrayLike,sym_pos:bool=False,lower:bool=False,overwrite_a:bool=False,overwrite_b:bool=False,debug:bool=False,check_finite:bool=True,assume_a:str='gen')->Array
jax._src.scipy.linalg.solve_triangular(a:ArrayLike,b:ArrayLike,trans:Union[int,str]=0,lower:bool=False,unit_diagonal:bool=False,overwrite_b:bool=False,debug:Any=None,check_finite:bool=True)->Array
jax._src.scipy.linalg.sqrtm(A:ArrayLike,blocksize:int=1)->Array
jax._src.scipy.linalg.svd(a:ArrayLike,full_matrices:bool=True,compute_uv:bool=True,overwrite_a:bool=False,check_finite:bool=True,lapack_driver:str='gesdd')->Union[Array, Tuple[Array, Array, Array]]
jax._src.scipy.linalg.tril(m:ArrayLike,k:int=0)->Array
jax._src.scipy.linalg.triu(m:ArrayLike,k:int=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/signal.py----------------------------------------
A:jax._src.scipy.signal.(in1, in2)->_promote_dtypes_inexact(in1, in2)
A:jax._src.scipy.signal.no_swap->all((s1 >= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.swap->all((s1 <= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.in2->jax._src.numpy.lax_numpy.flip(in2)
A:jax._src.scipy.signal.strides->tuple((1 for s in shape))
A:jax._src.scipy.signal.result->jax._src.numpy.lax_numpy.moveaxis(result, -1, axis)
A:jax._src.scipy.signal.same_shape->all((s1 == s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax._src.scipy.signal.(data,)->_promote_dtypes_inexact(jnp.asarray(data))
A:jax._src.scipy.signal.bp->numpy.sort(np.unique(np.r_[0, bp, N]))
A:jax._src.scipy.signal.data->data.at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST)).at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST))
A:jax._src.scipy.signal.sl->slice(bp[m], bp[m + 1])
A:jax._src.scipy.signal.(coef, *_)->jax._src.numpy.linalg.lstsq(A, data[sl])
A:jax._src.scipy.signal.x->jax._src.numpy.lax_numpy.moveaxis(x, -1, time_axis)
A:jax._src.scipy.signal.batch_shape->tuple(batch_shape)
A:jax._src.scipy.signal.(result,)->_promote_dtypes_complex(result)
A:jax._src.scipy.signal.left_end->jax.lax.slice_in_dim(x, 0, 1, axis=axis)
A:jax._src.scipy.signal.left_ext->jax._src.numpy.lax_numpy.flip(lax.slice_in_dim(x, 1, n + 1, axis=axis), axis=axis)
A:jax._src.scipy.signal.right_end->jax.lax.slice_in_dim(x, -1, None, axis=axis)
A:jax._src.scipy.signal.right_ext->jax._src.numpy.lax_numpy.flip(lax.slice_in_dim(x, -(n + 1), -1, axis=axis), axis=axis)
A:jax._src.scipy.signal.ext->jax._src.numpy.lax_numpy.concatenate((2 * left_end - left_ext, x, 2 * right_end - right_ext), axis=axis)
A:jax._src.scipy.signal.axis->canonicalize_axis(axis, x.ndim)
A:jax._src.scipy.signal.(x,)->_promote_dtypes_inexact(x)
A:jax._src.scipy.signal.outershape->jax._src.numpy.lax_numpy.broadcast_shapes(tuple_delete(x.shape, axis), tuple_delete(y.shape, axis))
A:jax._src.scipy.signal.(x, y)->_promote_dtypes_inexact(x, y)
A:jax._src.scipy.signal.result_dtype->jax._src.dtypes.to_complex_dtype(x.dtype)
A:jax._src.scipy.signal.nperseg->jax.core.concrete_or_error(int, nperseg or n_default, 'nperseg: segment length of STFT')
A:jax._src.scipy.signal.(win, nperseg)->jax._src.third_party.scipy.signal_helper._triage_segments(window, nperseg, input_length=x.shape[axis], dtype=x.dtype)
A:jax._src.scipy.signal.noverlap->jax.core.concrete_or_error(int, noverlap or nperseg // 2, 'noverlap of STFT')
A:jax._src.scipy.signal.nfft->jax.core.concrete_or_error(int, nfft, 'nfft of STFT')
A:jax._src.scipy.signal.shape->tuple_insert(outershape, min([x.shape[axis], y.shape[axis]]), axis)
A:jax._src.scipy.signal.y->jax._src.numpy.lax_numpy.concatenate((y, jnp.zeros_like(x, shape=(*y.shape[:-1], nadd))), axis=-1)
A:jax._src.scipy.signal.pad_shape->list(y.shape)
A:jax._src.scipy.signal.detrend_func->partial(detrend, type=detrend_type, axis=-1)
A:jax._src.scipy.signal.d->detrend_type(d)
A:jax._src.scipy.signal.scale->jax._src.numpy.lax_numpy.sqrt(scale)
A:jax._src.scipy.signal.(scale,)->_promote_dtypes_complex(scale)
A:jax._src.scipy.signal.freqs->jax.numpy.fft.rfftfreq(nfft, 1 / fs).astype(freq_dtype)
A:jax._src.scipy.signal.result_y->_fft_helper(y, win, detrend_func, nperseg, noverlap, nfft, sides)
A:jax._src.scipy.signal.(freqs, _, Pxy)->_spectral_helper(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode='psd')
A:jax._src.scipy.signal.bias->jax._src.third_party.scipy.signal_helper._median_bias(Pxy.shape[-1]).astype(Pxy.dtype)
A:jax._src.scipy.signal.Pxy->jax._src.numpy.lax_numpy.reshape(Pxy, Pxy.shape[:-1])
A:jax._src.scipy.signal.(freqs, Pxx)->csd(x, None, fs=fs, window=window, nperseg=nperseg, noverlap=noverlap, nfft=nfft, detrend=detrend, return_onesided=return_onesided, scaling=scaling, axis=axis, average=average)
A:jax._src.scipy.signal.step_size->jax.core.concrete_or_error(int, step_size, 'step_size for overlap_and_add')
A:jax._src.scipy.signal.flat_batchsize->numpy.prod(batch_shape, dtype=np.int64)
A:jax._src.scipy.signal.freq_axis->canonicalize_axis(freq_axis, Zxx.ndim)
A:jax._src.scipy.signal.time_axis->canonicalize_axis(time_axis, Zxx.ndim)
A:jax._src.scipy.signal.Zxx->jax._src.numpy.lax_numpy.transpose(Zxx, outer_idxs + (freq_axis, time_axis))
A:jax._src.scipy.signal.outer_idxs->tuple((idx for idx in range(Zxx.ndim) if idx not in {time_axis, freq_axis}))
A:jax._src.scipy.signal.win->win.reshape((1,) * (xsubs.ndim - 2) + win.shape + (1,)).reshape((1,) * (xsubs.ndim - 2) + win.shape + (1,))
A:jax._src.scipy.signal.win_squared->jax._src.numpy.lax_numpy.repeat(win * win, xsubs.shape[-1], axis=-1)
A:jax._src.scipy.signal.norm->_overlap_and_add(win_squared.swapaxes(-2, -1), nstep)
jax._src.scipy.signal._convolve_nd(in1,in2,mode,*,precision)
jax._src.scipy.signal._fft_helper(x,win,detrend_func,nperseg,noverlap,nfft,sides)
jax._src.scipy.signal._overlap_and_add(x,step_size)
jax._src.scipy.signal._spectral_helper(x,y,fs=1.0,window='hann',nperseg=None,noverlap=None,nfft=None,detrend_type='constant',return_onesided=True,scaling='density',axis=-1,mode='psd',boundary=None,padded=False)
jax._src.scipy.signal.convolve(in1,in2,mode='full',method='auto',precision=None)
jax._src.scipy.signal.convolve2d(in1,in2,mode='full',boundary='fill',fillvalue=0,precision=None)
jax._src.scipy.signal.correlate(in1,in2,mode='full',method='auto',precision=None)
jax._src.scipy.signal.correlate2d(in1,in2,mode='full',boundary='fill',fillvalue=0,precision=None)
jax._src.scipy.signal.csd(x,y,fs=1.0,window='hann',nperseg=None,noverlap=None,nfft=None,detrend='constant',return_onesided=True,scaling='density',axis=-1,average='mean')
jax._src.scipy.signal.detrend(data,axis=-1,type='linear',bp=0,overwrite_data=None)
jax._src.scipy.signal.istft(Zxx,fs=1.0,window='hann',nperseg=None,noverlap=None,nfft=None,input_onesided=True,boundary=True,time_axis=-1,freq_axis=-2)
jax._src.scipy.signal.odd_ext(x,n,axis=-1)
jax._src.scipy.signal.stft(x,fs=1.0,window='hann',nperseg=256,noverlap=None,nfft=None,detrend=False,return_onesided=True,boundary='zeros',padded=True,axis=-1)
jax._src.scipy.signal.welch(x,fs=1.0,window='hann',nperseg=None,noverlap=None,nfft=None,detrend='constant',return_onesided=True,scaling='density',axis=-1,average='mean')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/cluster/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/cluster/vq.py----------------------------------------
A:jax._src.scipy.cluster.vq._no_chkfinite_doc->textwrap.dedent('\nDoes not support the Scipy argument ``check_finite=True``,\nbecause compiled JAX code cannot perform checks of array values at runtime\n')
A:jax._src.scipy.cluster.vq.(obs, code_book)->_promote_dtypes_inexact(obs, code_book)
A:jax._src.scipy.cluster.vq.dist->vmap(lambda ob: norm(ob[None] - code_book, axis=-1))(obs)
A:jax._src.scipy.cluster.vq.code->argmin(dist, axis=-1)
A:jax._src.scipy.cluster.vq.dist_min->vmap(operator.getitem)(dist, code)
jax._src.scipy.cluster.vq.vq(obs,code_book,check_finite=True)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/sparse/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/sparse/linalg.py----------------------------------------
A:jax._src.scipy.sparse.linalg._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg._vdot->partial(jnp.vdot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg._einsum->partial(jnp.einsum, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.sparse.linalg.result->_vdot(x.real, y.real)
A:jax._src.scipy.sparse.linalg.xs->tree_leaves(x)
A:jax._src.scipy.sparse.linalg._add->partial(tree_map, operator.add)
A:jax._src.scipy.sparse.linalg._sub->partial(tree_map, operator.sub)
A:jax._src.scipy.sparse.linalg._dot_tree->partial(tree_map, _dot)
A:jax._src.scipy.sparse.linalg.bs->_vdot_real_tree(b, b)
A:jax._src.scipy.sparse.linalg.atol2->jax.numpy.maximum(jnp.square(tol) * bs, jnp.square(atol))
A:jax._src.scipy.sparse.linalg.Ap->A(p)
A:jax._src.scipy.sparse.linalg.x_->tree_map(partial(jnp.where, exit_early), _add(x, _mul(alpha_, phat)), _add(x, _add(_mul(alpha_, phat), _mul(omega_, shat))))
A:jax._src.scipy.sparse.linalg.r_->tree_map(partial(jnp.where, exit_early), s, _sub(s, _mul(omega_, t)))
A:jax._src.scipy.sparse.linalg.z_->M(r_)
A:jax._src.scipy.sparse.linalg.gamma_->_vdot_real_tree(r_, z_).astype(dtype)
A:jax._src.scipy.sparse.linalg.p_->_add(r, _mul(beta, _sub(p, _mul(omega, q))))
A:jax._src.scipy.sparse.linalg.r0->_sub(b, A(x0))
A:jax._src.scipy.sparse.linalg.p0z0->M(r0)
A:jax._src.scipy.sparse.linalg.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.scipy.sparse.linalg.gamma0->_vdot_real_tree(r0, z0).astype(dtype)
A:jax._src.scipy.sparse.linalg.(x_final, *_)->jax.lax.while_loop(cond_fun, body_fun, initial_value)
A:jax._src.scipy.sparse.linalg.rs->_vdot_real_tree(r, r)
A:jax._src.scipy.sparse.linalg.rho_->_vdot_tree(rhat, r)
A:jax._src.scipy.sparse.linalg.phat->M(p_)
A:jax._src.scipy.sparse.linalg.q_->A(phat)
A:jax._src.scipy.sparse.linalg.s->_sub(r, _mul(alpha_, q_))
A:jax._src.scipy.sparse.linalg.shat->M(s)
A:jax._src.scipy.sparse.linalg.t->A(shat)
A:jax._src.scipy.sparse.linalg.k_->jax.numpy.where(rho_ == 0, -10, k_)
A:jax._src.scipy.sparse.linalg.rho0alpha0omega0->jax._src.lax.lax._convert_element_type(1, *dtypes._lattice_result_type(*tree_leaves(b)))
A:jax._src.scipy.sparse.linalg.x0->tree_map(jnp.zeros_like, b)
A:jax._src.scipy.sparse.linalg.(b, x0)->device_put((b, x0))
A:jax._src.scipy.sparse.linalg.size->sum((bi.size for bi in tree_leaves(b)))
A:jax._src.scipy.sparse.linalg.A->_normalize_matvec(A)
A:jax._src.scipy.sparse.linalg.M->_normalize_matvec(M)
A:jax._src.scipy.sparse.linalg.isolve_solve->partial(_isolve_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)
A:jax._src.scipy.sparse.linalg.x->jax.lax.custom_linear_solve(A, b, solve=_solve, transpose_solve=_solve)
A:jax._src.scipy.sparse.linalg.norm->jax.numpy.where(use_norm, norm, 0.0)
A:jax._src.scipy.sparse.linalg.(dtype, weak_type)->jax._src.dtypes._lattice_result_type(*tree_leaves(b))
A:jax._src.scipy.sparse.linalg.norm_cast->jax._src.lax.lax._convert_element_type(norm, dtype, weak_type)
A:jax._src.scipy.sparse.linalg.normalized_x->tree_map(lambda y: jnp.where(use_norm, y / norm_cast, 0.0), x)
A:jax._src.scipy.sparse.linalg.v_proj->tree_map(lambda X, y: _einsum('...n,...->n', X.conj(), y), A, v)
A:jax._src.scipy.sparse.linalg.r->jax.lax.rsqrt(1 + abs(t) ** 2).astype(t.dtype)
A:jax._src.scipy.sparse.linalg.h->h.at[k + 1].set(v_norm_1.astype(dtype)).at[k + 1].set(v_norm_1.astype(dtype))
A:jax._src.scipy.sparse.linalg.Qh->tree_map(lambda X: _dot(X, h), Q)
A:jax._src.scipy.sparse.linalg.q->_sub(q, Qh)
A:jax._src.scipy.sparse.linalg.(_, qnorm)->_safe_normalize(q)
A:jax._src.scipy.sparse.linalg.(_, _, q, qnorm_scaled)->jax.lax.while_loop(qnorm_cond, qnorm, init)
A:jax._src.scipy.sparse.linalg.(_, rnorm)->_safe_normalize(r)
A:jax._src.scipy.sparse.linalg.(k, q, r, qnorm_scaled)->body_function((0, q, r, xnorm_scaled))
A:jax._src.scipy.sparse.linalg.(k, q, r, _)->jax.lax.while_loop(cond_function, body_function, (k, q, r, qnorm_scaled))
A:jax._src.scipy.sparse.linalg.v->M(A(v))
A:jax._src.scipy.sparse.linalg.(_, v_norm_0)->_safe_normalize(v)
A:jax._src.scipy.sparse.linalg.(v, h)->_iterative_classical_gram_schmidt(V, v, v_norm_0, max_iterations=2)
A:jax._src.scipy.sparse.linalg.(unit_v, v_norm_1)->_safe_normalize(v, thresh=tol)
A:jax._src.scipy.sparse.linalg.V->tree_map(lambda x: jnp.pad(x[..., None], ((0, 0),) * x.ndim + ((0, restart),)), unit_residual)
A:jax._src.scipy.sparse.linalg.H->jax._src.lax.lax._convert_element_type(jnp.eye(restart, restart + 1, dtype=dtype), weak_type=weak_type)
A:jax._src.scipy.sparse.linalg.cs->jax.numpy.where(b_zero, 1, jnp.where(a_lt_b, r * t, r))
A:jax._src.scipy.sparse.linalg.sn->jax.numpy.where(b_zero, 0, jnp.where(a_lt_b, r, r * t))
A:jax._src.scipy.sparse.linalg.R_row->_rotate_vectors(R_row, k, *givens_factors)
A:jax._src.scipy.sparse.linalg.givens_factors->_givens_rotation(R_row[k], R_row[k + 1])
A:jax._src.scipy.sparse.linalg.givens->jax.numpy.zeros((restart, 2), dtype=dtype)
A:jax._src.scipy.sparse.linalg.R->R.at[k, :].set(R_row).at[k, :].set(R_row)
A:jax._src.scipy.sparse.linalg.beta_vec->jax.numpy.zeros_like(H, shape=(restart + 1,)).at[0].set(residual_norm.astype(dtype))
A:jax._src.scipy.sparse.linalg.(V, H, _)->_kth_arnoldi_iteration(k, A, M, V, R)
A:jax._src.scipy.sparse.linalg.(R_row, givens)->_apply_givens_rotations(H[k, :], givens, k)
A:jax._src.scipy.sparse.linalg.err->abs(beta_vec[k + 1])
A:jax._src.scipy.sparse.linalg.carry->jax.lax.while_loop(loop_cond, arnoldi_qr_step, carry)
A:jax._src.scipy.sparse.linalg.y->_lstsq(H.T, beta_vec)
A:jax._src.scipy.sparse.linalg.dx->tree_map(lambda X: _dot(X[..., :-1], y), V)
A:jax._src.scipy.sparse.linalg.residual->M(_sub(b, A(x0)))
A:jax._src.scipy.sparse.linalg.(unit_residual, residual_norm)->_safe_normalize(residual)
A:jax._src.scipy.sparse.linalg.a2->_dot(a.T.conj(), a)
A:jax._src.scipy.sparse.linalg.b2->_dot(a.T.conj(), b)
A:jax._src.scipy.sparse.linalg.(V, H, breakdown)->_kth_arnoldi_iteration(k, A, M, V, H)
A:jax._src.scipy.sparse.linalg.(V, H, _, _)->jax.lax.while_loop(loop_cond, arnoldi_process, carry)
A:jax._src.scipy.sparse.linalg.(x, unit_residual, residual_norm)->gmres_func(A, b, x, unit_residual, residual_norm, ptol, restart, M)
A:jax._src.scipy.sparse.linalg.(x_final, k, _, err)->jax.lax.while_loop(cond_fun, body_fun, initialization)
A:jax._src.scipy.sparse.linalg.restart->min(restart, size)
A:jax._src.scipy.sparse.linalg.b_norm->_norm(b)
A:jax._src.scipy.sparse.linalg.atol->jax.numpy.maximum(tol * b_norm, atol)
A:jax._src.scipy.sparse.linalg.Mb->M(b)
A:jax._src.scipy.sparse.linalg.Mb_norm->_norm(Mb)
A:jax._src.scipy.sparse.linalg.failed->jax.numpy.isnan(_norm(x))
A:jax._src.scipy.sparse.linalg.info->jax.numpy.where(failed, x=-1, y=0)
jax._src.scipy.sparse.linalg._apply_givens_rotations(H_row,givens,k)
jax._src.scipy.sparse.linalg._bicgstab_solve(A,b,x0=None,*,maxiter,tol=1e-05,atol=0.0,M=_identity)
jax._src.scipy.sparse.linalg._cg_solve(A,b,x0=None,*,maxiter,tol=1e-05,atol=0.0,M=_identity)
jax._src.scipy.sparse.linalg._div(tree,scalar)
jax._src.scipy.sparse.linalg._givens_rotation(a,b)
jax._src.scipy.sparse.linalg._gmres_batched(A,b,x0,unit_residual,residual_norm,ptol,restart,M)
jax._src.scipy.sparse.linalg._gmres_incremental(A,b,x0,unit_residual,residual_norm,ptol,restart,M)
jax._src.scipy.sparse.linalg._gmres_solve(A,b,x0,atol,ptol,restart,maxiter,M,gmres_func)
jax._src.scipy.sparse.linalg._identity(x)
jax._src.scipy.sparse.linalg._isolve(_isolve_solve,A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None,check_symmetric=False)
jax._src.scipy.sparse.linalg._iterative_classical_gram_schmidt(Q,x,xnorm,max_iterations=2)
jax._src.scipy.sparse.linalg._kth_arnoldi_iteration(k,A,M,V,H)
jax._src.scipy.sparse.linalg._lstsq(a,b)
jax._src.scipy.sparse.linalg._mul(scalar,tree)
jax._src.scipy.sparse.linalg._norm(x)
jax._src.scipy.sparse.linalg._normalize_matvec(f)
jax._src.scipy.sparse.linalg._project_on_columns(A,v)
jax._src.scipy.sparse.linalg._rotate_vectors(H,i,cs,sn)
jax._src.scipy.sparse.linalg._safe_normalize(x,thresh=None)
jax._src.scipy.sparse.linalg._shapes(pytree)
jax._src.scipy.sparse.linalg._vdot_real_part(x,y)
jax._src.scipy.sparse.linalg._vdot_real_tree(x,y)
jax._src.scipy.sparse.linalg._vdot_tree(x,y)
jax._src.scipy.sparse.linalg.bicgstab(A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None)
jax._src.scipy.sparse.linalg.cg(A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None)
jax._src.scipy.sparse.linalg.gmres(A,b,x0=None,*,tol=1e-05,atol=0.0,restart=20,maxiter=None,M=None,solve_method='batched')


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/beta.py----------------------------------------
A:jax._src.scipy.stats.beta.(x, a, b, loc, scale)->_promote_args_inexact('beta.logpdf', x, a, b, loc, scale)
A:jax._src.scipy.stats.beta.one->_lax_const(x, 1)
A:jax._src.scipy.stats.beta.shape_term->jax.lax.neg(betaln(a, b))
A:jax._src.scipy.stats.beta.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.beta.log_linear_term->jax.lax.add(xlogy(lax.sub(a, one), y), xlog1py(lax.sub(b, one), lax.neg(y)))
A:jax._src.scipy.stats.beta.log_probs->jax.lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
jax._src.scipy.stats.beta.logpdf(x,a,b,loc=0,scale=1)
jax._src.scipy.stats.beta.pdf(x,a,b,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/multivariate_normal.py----------------------------------------
A:jax._src.scipy.stats.multivariate_normal.(x, mean, cov)->_promote_dtypes_inexact(x, mean, cov)
A:jax._src.scipy.stats.multivariate_normal.L->jax.lax.linalg.cholesky(cov)
A:jax._src.scipy.stats.multivariate_normal.y->jax.numpy.vectorize(partial(lax.linalg.triangular_solve, lower=True, transpose_a=True), signature='(n,n),(n)->(n)')(L, x - mean)
jax._src.scipy.stats.multivariate_normal.logpdf(x,mean,cov,allow_singular=None)
jax._src.scipy.stats.multivariate_normal.pdf(x,mean,cov)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/chi2.py----------------------------------------
A:jax._src.scipy.stats.chi2.(x, df, loc, scale)->_promote_args_inexact('chi2.logpdf', x, df, loc, scale)
A:jax._src.scipy.stats.chi2.one->_lax_const(x, 1)
A:jax._src.scipy.stats.chi2.two->_lax_const(x, 2)
A:jax._src.scipy.stats.chi2.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.chi2.df_on_two->jax.lax.div(df, two)
A:jax._src.scipy.stats.chi2.kernel->jax.lax.sub(lax.mul(lax.sub(df_on_two, one), lax.log(y)), lax.div(y, two))
A:jax._src.scipy.stats.chi2.nrml_cnst->jax.lax.neg(lax.add(lax.lgamma(df_on_two), lax.div(lax.mul(lax.log(two), df), two)))
A:jax._src.scipy.stats.chi2.log_probs->jax.lax.add(lax.sub(nrml_cnst, lax.log(scale)), kernel)
jax._src.scipy.stats.chi2.logpdf(x,df,loc=0,scale=1)
jax._src.scipy.stats.chi2.pdf(x,df,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/cauchy.py----------------------------------------
A:jax._src.scipy.stats.cauchy.(x, loc, scale)->_promote_args_inexact('cauchy.logpdf', x, loc, scale)
A:jax._src.scipy.stats.cauchy.pi->_lax_const(x, np.pi)
A:jax._src.scipy.stats.cauchy.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.cauchy.normalize_term->jax.lax.log(lax.mul(pi, scale))
jax._src.scipy.stats.cauchy.logpdf(x,loc=0,scale=1)
jax._src.scipy.stats.cauchy.pdf(x,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/kde.py----------------------------------------
A:jax._src.scipy.stats.kde.dataset->jax.numpy.atleast_2d(dataset)
A:jax._src.scipy.stats.kde.(dataset, weights)->_promote_dtypes_inexact(dataset, weights)
A:jax._src.scipy.stats.kde.weights->jax.numpy.full(n, 1.0 / n, dtype=dataset.dtype)
A:jax._src.scipy.stats.kde.(dataset,)->_promote_dtypes_inexact(dataset)
A:jax._src.scipy.stats.kde.neff->self._setattr('neff', 1 / jnp.sum(weights ** 2))
A:jax._src.scipy.stats.kde.factor->bw_method(self)
A:jax._src.scipy.stats.kde.data_covariance->jax.numpy.atleast_2d(jnp.cov(dataset, rowvar=1, bias=False, aweights=weights))
A:jax._src.scipy.stats.kde.data_inv_cov->jax.numpy.linalg.inv(data_covariance)
A:jax._src.scipy.stats.kde.kde->cls.__new__(cls)
A:jax._src.scipy.stats.kde.points->jax.numpy.dot(points, whitening)
A:jax._src.scipy.stats.kde.result->_gaussian_kernel_eval(True, self.dataset.T, self.weights[:, None], x.T, self.inv_cov)
A:jax._src.scipy.stats.kde.mean->jax.numpy.atleast_1d(jnp.squeeze(mean))
A:jax._src.scipy.stats.kde.cov->jax.numpy.atleast_2d(cov)
A:jax._src.scipy.stats.kde.chol->jax.scipy.linalg.cho_factor(self.covariance + other.covariance)
A:jax._src.scipy.stats.kde.sigma->jax.numpy.squeeze(jnp.sqrt(self.covariance))
A:jax._src.scipy.stats.kde.low->jax.numpy.squeeze((low - self.dataset) / sigma)
A:jax._src.scipy.stats.kde.high->jax.numpy.squeeze((high - self.dataset) / sigma)
A:jax._src.scipy.stats.kde.(ind_key, eps_key)->jax.random.split(key)
A:jax._src.scipy.stats.kde.ind->jax.random.choice(ind_key, self.n, shape=shape, p=self.weights)
A:jax._src.scipy.stats.kde.x->self._reshape_points(x)
A:jax._src.scipy.stats.kde.alpha->jax.scipy.linalg.cho_solve(chol, diff)
A:jax._src.scipy.stats.kde.(points, values, xi, precision)->_promote_dtypes_inexact(points, values, xi, precision)
A:jax._src.scipy.stats.kde.whitening->jax.scipy.linalg.cholesky(precision, lower=True)
A:jax._src.scipy.stats.kde.xi->jax.numpy.dot(xi, whitening)
A:jax._src.scipy.stats.kde.mapped_kernel->vmap(reduced_kernel)
jax._src.scipy.stats.kde._gaussian_kernel_convolve(chol,norm,target,weights,mean)
jax._src.scipy.stats.kde._gaussian_kernel_eval(in_log,points,values,xi,precision)
jax._src.scipy.stats.kde.gaussian_kde(self,dataset,bw_method=None,weights=None)
jax._src.scipy.stats.kde.gaussian_kde.__init__(self,dataset,bw_method=None,weights=None)
jax._src.scipy.stats.kde.gaussian_kde._reshape_points(self,points)
jax._src.scipy.stats.kde.gaussian_kde._setattr(self,name,value)
jax._src.scipy.stats.kde.gaussian_kde.d(self)
jax._src.scipy.stats.kde.gaussian_kde.evaluate(self,points)
jax._src.scipy.stats.kde.gaussian_kde.integrate_box(self,low_bounds,high_bounds,maxpts=None)
jax._src.scipy.stats.kde.gaussian_kde.integrate_box_1d(self,low,high)
jax._src.scipy.stats.kde.gaussian_kde.integrate_gaussian(self,mean,cov)
jax._src.scipy.stats.kde.gaussian_kde.integrate_kde(self,other)
jax._src.scipy.stats.kde.gaussian_kde.logpdf(self,x)
jax._src.scipy.stats.kde.gaussian_kde.n(self)
jax._src.scipy.stats.kde.gaussian_kde.pdf(self,x)
jax._src.scipy.stats.kde.gaussian_kde.resample(self,key,shape=())
jax._src.scipy.stats.kde.gaussian_kde.set_bandwidth(self,bw_method=None)
jax._src.scipy.stats.kde.gaussian_kde.tree_flatten(self)
jax._src.scipy.stats.kde.gaussian_kde.tree_unflatten(cls,aux_data,children)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/bernoulli.py----------------------------------------
A:jax._src.scipy.stats.bernoulli.(k, p, loc)->jax._src.numpy.lax_numpy._promote_args_inexact('bernoulli.logpmf', k, p, loc)
A:jax._src.scipy.stats.bernoulli.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.bernoulli.one->_lax_const(k, 1)
A:jax._src.scipy.stats.bernoulli.x->jax.lax.sub(k, loc)
jax._src.scipy.stats.bernoulli.logpmf(k,p,loc=0)
jax._src.scipy.stats.bernoulli.pmf(k,p,loc=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/logistic.py----------------------------------------
A:jax._src.scipy.stats.logistic.(x,)->_promote_args_inexact('logistic.logpdf', x)
A:jax._src.scipy.stats.logistic.two->_lax_const(x, 2)
A:jax._src.scipy.stats.logistic.half_x->jax.lax.div(x, two)
jax._src.scipy.stats.logistic.cdf(x)
jax._src.scipy.stats.logistic.isf(x)
jax._src.scipy.stats.logistic.logpdf(x)
jax._src.scipy.stats.logistic.pdf(x)
jax._src.scipy.stats.logistic.ppf(x)
jax._src.scipy.stats.logistic.sf(x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/expon.py----------------------------------------
A:jax._src.scipy.stats.expon.(x, loc, scale)->_promote_args_inexact('expon.logpdf', x, loc, scale)
A:jax._src.scipy.stats.expon.log_scale->jax.lax.log(scale)
A:jax._src.scipy.stats.expon.linear_term->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.expon.log_probs->jax.lax.neg(lax.add(linear_term, log_scale))
jax._src.scipy.stats.expon.logpdf(x,loc=0,scale=1)
jax._src.scipy.stats.expon.pdf(x,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/nbinom.py----------------------------------------
A:jax._src.scipy.stats.nbinom.(k, n, p, loc)->_promote_args_inexact('nbinom.logpmf', k, n, p, loc)
A:jax._src.scipy.stats.nbinom.one->_lax_const(k, 1)
A:jax._src.scipy.stats.nbinom.y->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.nbinom.comb_term->jax.lax.sub(lax.sub(gammaln(lax.add(y, n)), gammaln(n)), gammaln(lax.add(y, one)))
A:jax._src.scipy.stats.nbinom.log_linear_term->jax.lax.add(xlogy(n, p), xlogy(y, lax.sub(one, p)))
A:jax._src.scipy.stats.nbinom.log_probs->jax.lax.add(comb_term, log_linear_term)
jax._src.scipy.stats.nbinom.logpmf(k,n,p,loc=0)
jax._src.scipy.stats.nbinom.pmf(k,n,p,loc=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/poisson.py----------------------------------------
A:jax._src.scipy.stats.poisson.(k, mu, loc)->jax._src.numpy.lax_numpy._promote_args_inexact('poisson.logpmf', k, mu, loc)
A:jax._src.scipy.stats.poisson.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.poisson.x->jax.lax.sub(k, loc)
A:jax._src.scipy.stats.poisson.p->gammaincc(jnp.floor(1 + x), mu)
jax._src.scipy.stats.poisson.cdf(k,mu,loc=0)
jax._src.scipy.stats.poisson.logpmf(k,mu,loc=0)
jax._src.scipy.stats.poisson.pmf(k,mu,loc=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/multinomial.py----------------------------------------
A:jax._src.scipy.stats.multinomial.(p,)->_promote_args_inexact('multinomial.logpmf', p)
A:jax._src.scipy.stats.multinomial.(x, n)->_promote_args_numeric('multinomial.logpmf', x, n)
A:jax._src.scipy.stats.multinomial.x->x.astype(p.dtype).astype(p.dtype)
A:jax._src.scipy.stats.multinomial.n->n.astype(p.dtype).astype(p.dtype)
jax._src.scipy.stats.multinomial.logpmf(x,n,p)
jax._src.scipy.stats.multinomial.pmf(x,n,p)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/betabinom.py----------------------------------------
A:jax._src.scipy.stats.betabinom.(k, n, a, b, loc)->_promote_args_inexact('betabinom.logpmf', k, n, a, b, loc)
A:jax._src.scipy.stats.betabinom.y->jax.lax.sub(lax.floor(k), loc)
A:jax._src.scipy.stats.betabinom.one->_lax_const(y, 1)
A:jax._src.scipy.stats.betabinom.zero->_lax_const(y, 0)
A:jax._src.scipy.stats.betabinom.combiln->jax.lax.neg(lax.add(lax.log1p(n), betaln(lax.add(lax.sub(n, y), one), lax.add(y, one))))
A:jax._src.scipy.stats.betabinom.beta_lns->jax.lax.sub(betaln(lax.add(y, a), lax.add(lax.sub(n, y), b)), betaln(a, b))
A:jax._src.scipy.stats.betabinom.log_probs->where(y_cond, -inf, log_probs)
A:jax._src.scipy.stats.betabinom.y_cond->logical_or(lax.lt(y, lax.neg(loc)), lax.gt(y, lax.sub(n, loc)))
A:jax._src.scipy.stats.betabinom.n_a_b_cond->logical_or(logical_or(lax.lt(n, one), lax.lt(a, zero)), lax.lt(b, zero))
A:jax._src.scipy.stats.betabinom.logpmf->_wraps(osp_stats.betabinom.logpmf, update_doc=False)(logpmf)
A:jax._src.scipy.stats.betabinom.pmf->_wraps(osp_stats.betabinom.pmf, update_doc=False)(pmf)
jax._src.scipy.stats.betabinom.logpmf(k,n,a,b,loc=0)
jax._src.scipy.stats.betabinom.pmf(k,n,a,b,loc=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/t.py----------------------------------------
A:jax._src.scipy.stats.t.(x, df, loc, scale)->_promote_args_inexact('t.logpdf', x, df, loc, scale)
A:jax._src.scipy.stats.t.two->_lax_const(x, 2)
A:jax._src.scipy.stats.t.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.t.df_over_two->jax.lax.div(df, two)
A:jax._src.scipy.stats.t.df_plus_one_over_two->jax.lax.add(df_over_two, _lax_const(x, 0.5))
A:jax._src.scipy.stats.t.normalize_term_const->jax.lax.mul(lax.mul(scale, scale), _lax_const(x, np.pi))
A:jax._src.scipy.stats.t.normalize_term_tmp->jax.lax.div(lax.log(lax.mul(normalize_term_const, df)), two)
A:jax._src.scipy.stats.t.normalize_term->jax.lax.sub(lax.add(lax.lgamma(df_over_two), normalize_term_tmp), lax.lgamma(df_plus_one_over_two))
A:jax._src.scipy.stats.t.quadratic->jax.lax.div(lax.mul(scaled_x, scaled_x), df)
jax._src.scipy.stats.t.logpdf(x,df,loc=0,scale=1)
jax._src.scipy.stats.t.pdf(x,df,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/laplace.py----------------------------------------
A:jax._src.scipy.stats.laplace.(x, loc, scale)->_promote_args_inexact('laplace.cdf', x, loc, scale)
A:jax._src.scipy.stats.laplace.two->_lax_const(x, 2)
A:jax._src.scipy.stats.laplace.linear_term->jax.lax.div(lax.abs(lax.sub(x, loc)), scale)
A:jax._src.scipy.stats.laplace.half->_lax_const(x, 0.5)
A:jax._src.scipy.stats.laplace.one->_lax_const(x, 1)
A:jax._src.scipy.stats.laplace.zero->_lax_const(x, 0)
A:jax._src.scipy.stats.laplace.diff->jax.lax.div(lax.sub(x, loc), scale)
jax._src.scipy.stats.laplace.cdf(x,loc=0,scale=1)
jax._src.scipy.stats.laplace.logpdf(x,loc=0,scale=1)
jax._src.scipy.stats.laplace.pdf(x,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/geom.py----------------------------------------
A:jax._src.scipy.stats.geom.(k, p, loc)->jax._src.numpy.lax_numpy._promote_args_inexact('geom.logpmf', k, p, loc)
A:jax._src.scipy.stats.geom.zero->_lax_const(k, 0)
A:jax._src.scipy.stats.geom.one->_lax_const(k, 1)
A:jax._src.scipy.stats.geom.x->jax.lax.sub(k, loc)
jax._src.scipy.stats.geom.logpmf(k,p,loc=0)
jax._src.scipy.stats.geom.pmf(k,p,loc=0)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/gennorm.py----------------------------------------
A:jax._src.scipy.stats.gennorm.(x, p)->_promote_args_inexact('gennorm.cdf', x, p)
jax._src.scipy.stats.gennorm.cdf(x,p)
jax._src.scipy.stats.gennorm.logpdf(x,p)
jax._src.scipy.stats.gennorm.pdf(x,p)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/pareto.py----------------------------------------
A:jax._src.scipy.stats.pareto.(x, b, loc, scale)->_promote_args_inexact('pareto.logpdf', x, b, loc, scale)
A:jax._src.scipy.stats.pareto.one->_lax_const(x, 1)
A:jax._src.scipy.stats.pareto.scaled_x->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.pareto.normalize_term->jax.lax.log(lax.div(scale, b))
A:jax._src.scipy.stats.pareto.log_probs->jax.lax.neg(lax.add(normalize_term, lax.mul(lax.add(b, one), lax.log(scaled_x))))
jax._src.scipy.stats.pareto.logpdf(x,b,loc=0,scale=1)
jax._src.scipy.stats.pareto.pdf(x,b,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/norm.py----------------------------------------
A:jax._src.scipy.stats.norm.(x, loc, scale)->_promote_args_inexact('norm.logcdf', x, loc, scale)
A:jax._src.scipy.stats.norm.scale_sqrd->jax.lax.square(scale)
A:jax._src.scipy.stats.norm.log_normalizer->jax.lax.log(lax.mul(_lax_const(x, 2 * np.pi), scale_sqrd))
A:jax._src.scipy.stats.norm.quadratic->jax.lax.div(lax.square(lax.sub(x, loc)), scale_sqrd)
jax._src.scipy.stats.norm.cdf(x,loc=0,scale=1)
jax._src.scipy.stats.norm.logcdf(x,loc=0,scale=1)
jax._src.scipy.stats.norm.logpdf(x,loc=0,scale=1)
jax._src.scipy.stats.norm.pdf(x,loc=0,scale=1)
jax._src.scipy.stats.norm.ppf(q,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/dirichlet.py----------------------------------------
A:jax._src.scipy.stats.dirichlet.x_sum->jax._src.numpy.lax_numpy.sum(x, axis=0)
A:jax._src.scipy.stats.dirichlet.(x, alpha)->_promote_dtypes_inexact(x, alpha)
A:jax._src.scipy.stats.dirichlet.one->_lax_const(x, 1)
A:jax._src.scipy.stats.dirichlet.x->jax._src.numpy.lax_numpy.concatenate([x, lax.sub(one, x.sum(0, keepdims=True))], axis=0)
A:jax._src.scipy.stats.dirichlet.alpha->jax.lax.broadcast_in_dim(alpha, alpha.shape + (1,) * (x.ndim - 1), (0,))
A:jax._src.scipy.stats.dirichlet.log_probs->jax.lax.sub(jnp.sum(xlogy(lax.sub(alpha, one), x), axis=0), normalize_term)
jax._src.scipy.stats.dirichlet._is_simplex(x)
jax._src.scipy.stats.dirichlet.logpdf(x,alpha)
jax._src.scipy.stats.dirichlet.pdf(x,alpha)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/gamma.py----------------------------------------
A:jax._src.scipy.stats.gamma.(x, a, loc, scale)->_promote_args_inexact('gamma.logpdf', x, a, loc, scale)
A:jax._src.scipy.stats.gamma.one->_lax_const(x, 1)
A:jax._src.scipy.stats.gamma.y->jax.lax.div(lax.sub(x, loc), scale)
A:jax._src.scipy.stats.gamma.log_linear_term->jax.lax.sub(xlogy(lax.sub(a, one), y), y)
A:jax._src.scipy.stats.gamma.shape_terms->jax.lax.add(gammaln(a), lax.log(scale))
A:jax._src.scipy.stats.gamma.log_probs->jax.lax.sub(log_linear_term, shape_terms)
jax._src.scipy.stats.gamma.logpdf(x,a,loc=0,scale=1)
jax._src.scipy.stats.gamma.pdf(x,a,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/stats/uniform.py----------------------------------------
A:jax._src.scipy.stats.uniform.(x, loc, scale)->_promote_args_inexact('uniform.logpdf', x, loc, scale)
A:jax._src.scipy.stats.uniform.log_probs->jax.lax.neg(lax.log(scale))
jax._src.scipy.stats.uniform.logpdf(x,loc=0,scale=1)
jax._src.scipy.stats.uniform.pdf(x,loc=0,scale=1)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/optimize/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/optimize/bfgs.py----------------------------------------
A:jax._src.scipy.optimize.bfgs._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.bfgs._einsum->partial(jnp.einsum, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.bfgs.initial_H->jax.numpy.eye(d, dtype=x0.dtype)
A:jax._src.scipy.optimize.bfgs.(f_0, g_0)->jax.value_and_grad(fun)(x0)
A:jax._src.scipy.optimize.bfgs.state->state._replace(status=status)._replace(status=status)
A:jax._src.scipy.optimize.bfgs.line_search_results->line_search(fun, state.x_k, p_k, old_fval=state.f_k, old_old_fval=state.old_old_fval, gfk=state.g_k, maxiter=line_search_maxiter)
A:jax._src.scipy.optimize.bfgs.rho_k->jax.numpy.reciprocal(_dot(y_k, s_k))
A:jax._src.scipy.optimize.bfgs.H_kp1->jax.numpy.where(jnp.isfinite(rho_k), H_kp1, state.H_k)
A:jax._src.scipy.optimize.bfgs.status->jax.numpy.where(state.converged, 0, jnp.where(state.k == maxiter, 1, jnp.where(state.failed, 2 + state.line_search_status, -1)))
jax._src.scipy.optimize.bfgs._BFGSResults(NamedTuple)
jax._src.scipy.optimize.bfgs.minimize_bfgs(fun:Callable,x0:jnp.ndarray,maxiter:Optional[int]=None,norm=jnp.inf,gtol:float=1e-05,line_search_maxiter:int=10)->_BFGSResults


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/optimize/line_search.py----------------------------------------
A:jax._src.scipy.optimize.line_search._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize.line_search.d1->jax.numpy.array([[dc ** 2, -db ** 2], [-dc ** 3, db ** 3]])
A:jax._src.scipy.optimize.line_search.keys->new_dict.keys()
A:jax._src.scipy.optimize.line_search.out->dict()
A:jax._src.scipy.optimize.line_search.out[key]->jax.numpy.where(replace_bit, new_dict[key], original_dict[key])
A:jax._src.scipy.optimize.line_search.state->jax.lax.while_loop(lambda state: ~state.done & (state.i <= maxiter) & ~state.failed, body, state)
A:jax._src.scipy.optimize.line_search.a->jax.numpy.minimum(state.a_hi, state.a_lo)
A:jax._src.scipy.optimize.line_search.b->jax.numpy.maximum(state.a_hi, state.a_lo)
A:jax._src.scipy.optimize.line_search.threshold->jax.numpy.where(jnp.finfo(dalpha).bits < 64, 1e-05, 1e-10)
A:jax._src.scipy.optimize.line_search.a_j_cubic->_cubicmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi, state.a_rec, state.phi_rec)
A:jax._src.scipy.optimize.line_search.a_j_quad->_quadmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi)
A:jax._src.scipy.optimize.line_search.a_j->jax.numpy.where(use_bisection, a_j_bisection, a_j)
A:jax._src.scipy.optimize.line_search.(phi_j, dphi_j, g_j)->restricted_func_and_grad(a_j)
A:jax._src.scipy.optimize.line_search.phi_j->phi_j.astype(state.phi_lo.dtype).astype(state.phi_lo.dtype)
A:jax._src.scipy.optimize.line_search.dphi_j->dphi_j.astype(state.dphi_lo.dtype).astype(state.dphi_lo.dtype)
A:jax._src.scipy.optimize.line_search.g_j->g_j.astype(state.g_star.dtype).astype(state.g_star.dtype)
A:jax._src.scipy.optimize.line_search.(xk, pk)->_promote_dtypes_inexact(xk, pk)
A:jax._src.scipy.optimize.line_search.t->jax.numpy.array(t, dtype=pk.dtype)
A:jax._src.scipy.optimize.line_search.(phi, g)->jax.value_and_grad(f)(xk + t * pk)
A:jax._src.scipy.optimize.line_search.dphi->jax.numpy.real(_dot(g, pk))
A:jax._src.scipy.optimize.line_search.(phi_0, dphi_0, gfk)->restricted_func_and_grad(0)
A:jax._src.scipy.optimize.line_search.dphi_0->jax.numpy.real(_dot(gfk, pk))
A:jax._src.scipy.optimize.line_search.start_value->jax.numpy.where(candidate_start_value > 1, 1.0, candidate_start_value)
A:jax._src.scipy.optimize.line_search.a_i->jax.numpy.where(state.i == 1, start_value, state.a_i1 * 2.0)
A:jax._src.scipy.optimize.line_search.(phi_i, dphi_i, g_i)->restricted_func_and_grad(a_i)
A:jax._src.scipy.optimize.line_search.zoom1->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, state.a_i1, state.phi_i1, state.dphi_i1, a_i, phi_i, dphi_i, gfk, ~star_to_zoom1)
A:jax._src.scipy.optimize.line_search.zoom2->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, a_i, phi_i, dphi_i, state.a_i1, state.phi_i1, state.dphi_i1, gfk, ~star_to_zoom2)
A:jax._src.scipy.optimize.line_search.status->jax.numpy.where(state.failed, jnp.array(1), jnp.where(state.i > maxiter, jnp.array(3), jnp.array(0)))
A:jax._src.scipy.optimize.line_search.alpha_k->jax.numpy.where((jnp.finfo(alpha_k).bits != 64) & (jnp.abs(alpha_k) < 1e-08), jnp.sign(alpha_k) * 1e-08, alpha_k)
A:jax._src.scipy.optimize.line_search.results->_LineSearchResults(failed=state.failed | ~state.done, nit=state.i - 1, nfev=state.nfev, ngev=state.ngev, k=state.i, a_k=alpha_k, f_k=state.phi_star, g_k=state.g_star, status=status)
jax._src.scipy.optimize.line_search._LineSearchResults(NamedTuple)
jax._src.scipy.optimize.line_search._LineSearchState(NamedTuple)
jax._src.scipy.optimize.line_search._ZoomState(NamedTuple)
jax._src.scipy.optimize.line_search._binary_replace(replace_bit,original_dict,new_dict,keys=None)
jax._src.scipy.optimize.line_search._cubicmin(a,fa,fpa,b,fb,c,fc)
jax._src.scipy.optimize.line_search._quadmin(a,fa,fpa,b,fb)
jax._src.scipy.optimize.line_search._zoom(restricted_func_and_grad,wolfe_one,wolfe_two,a_lo,phi_lo,dphi_lo,a_hi,phi_hi,dphi_hi,g_0,pass_through)
jax._src.scipy.optimize.line_search.line_search(f,xk,pk,old_fval=None,old_old_fval=None,gfk=None,c1=0.0001,c2=0.9,maxiter=20)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/optimize/_lbfgs.py----------------------------------------
A:jax._src.scipy.optimize._lbfgs._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax._src.scipy.optimize._lbfgs.d->len(x0)
A:jax._src.scipy.optimize._lbfgs.dtype->jax.numpy.dtype(x0)
A:jax._src.scipy.optimize._lbfgs.(f_0, g_0)->jax.value_and_grad(fun)(x0)
A:jax._src.scipy.optimize._lbfgs.state_initial->LBFGSResults(converged=False, failed=False, k=0, nfev=1, ngev=1, x_k=x0, f_k=f_0, g_k=g_0, s_history=jnp.zeros((maxcor, d), dtype=dtype), y_history=jnp.zeros((maxcor, d), dtype=dtype), rho_history=jnp.zeros((maxcor,), dtype=dtype), gamma=1.0, status=0, ls_status=0)
A:jax._src.scipy.optimize._lbfgs.p_k->_two_loop_recursion(state)
A:jax._src.scipy.optimize._lbfgs.ls_results->line_search(f=fun, xk=state.x_k, pk=p_k, old_fval=state.f_k, gfk=state.g_k, maxiter=maxls)
A:jax._src.scipy.optimize._lbfgs.rho_k_inv->jax.numpy.real(_dot(y_k, s_k))
A:jax._src.scipy.optimize._lbfgs.rho_k->jax.numpy.reciprocal(rho_k_inv).astype(y_k.dtype)
A:jax._src.scipy.optimize._lbfgs.status->jax.numpy.where(ls_results.failed, 5, status)
A:jax._src.scipy.optimize._lbfgs.state->state._replace(converged=converged, failed=(status > 0) & ~converged, k=state.k + 1, nfev=state.nfev + ls_results.nfev, ngev=state.ngev + ls_results.ngev, x_k=x_kp1.astype(state.x_k.dtype), f_k=f_kp1.astype(state.f_k.dtype), g_k=g_kp1.astype(state.g_k.dtype), s_history=_update_history_vectors(history=state.s_history, new=s_k), y_history=_update_history_vectors(history=state.y_history, new=y_k), rho_history=_update_history_scalars(history=state.rho_history, new=rho_k), gamma=gamma, status=jnp.where(converged, 0, status), ls_status=ls_results.status)._replace(converged=converged, failed=(status > 0) & ~converged, k=state.k + 1, nfev=state.nfev + ls_results.nfev, ngev=state.ngev + ls_results.ngev, x_k=x_kp1.astype(state.x_k.dtype), f_k=f_kp1.astype(state.f_k.dtype), g_k=g_kp1.astype(state.g_k.dtype), s_history=_update_history_vectors(history=state.s_history, new=s_k), y_history=_update_history_vectors(history=state.y_history, new=y_k), rho_history=_update_history_scalars(history=state.rho_history, new=rho_k), gamma=gamma, status=jnp.where(converged, 0, status), ls_status=ls_results.status)
A:jax._src.scipy.optimize._lbfgs.his_size->len(state.rho_history)
A:jax._src.scipy.optimize._lbfgs.curr_size->jax.numpy.where(state.k < his_size, state.k, his_size)
A:jax._src.scipy.optimize._lbfgs.a_his->jax.numpy.zeros_like(state.rho_history)
A:jax._src.scipy.optimize._lbfgs._a_his->_a_his.at[i].set(a_i).at[i].set(a_i)
A:jax._src.scipy.optimize._lbfgs.(q, a_his)->jax.lax.fori_loop(0, curr_size, body_fun1, (q, a_his))
A:jax._src.scipy.optimize._lbfgs.q->jax.lax.fori_loop(0, curr_size, body_fun2, q)
jax._src.scipy.optimize._lbfgs.LBFGSResults(NamedTuple)
jax._src.scipy.optimize._lbfgs._minimize_lbfgs(fun:Callable,x0:Array,maxiter:Optional[float]=None,norm=jnp.inf,maxcor:int=10,ftol:float=2.220446049250313e-09,gtol:float=1e-05,maxfun:Optional[float]=None,maxgrad:Optional[float]=None,maxls:int=20)
jax._src.scipy.optimize._lbfgs._two_loop_recursion(state:LBFGSResults)
jax._src.scipy.optimize._lbfgs._update_history_scalars(history,new)
jax._src.scipy.optimize._lbfgs._update_history_vectors(history,new)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/optimize/minimize.py----------------------------------------
A:jax._src.scipy.optimize.minimize.results->_minimize_lbfgs(fun_with_args, x0, **options)
jax._src.scipy.optimize.minimize.OptimizeResults(NamedTuple)
jax._src.scipy.optimize.minimize.minimize(fun:Callable,x0:jnp.ndarray,args:Tuple=(),*,method:str,tol:Optional[float]=None,options:Optional[Mapping[str,Any]]=None)->OptimizeResults


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/scipy/interpolate/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lib/__init__.py----------------------------------------
A:jax._src.lib.__init__.version_regex->re.compile('[0-9]+(?:\\.[0-9]+)*')
A:jax._src.lib.__init__.m->re.compile('[0-9]+(?:\\.[0-9]+)*').match(v)
A:jax._src.lib.__init__._jax_version->_parse_version(jax_version)
A:jax._src.lib.__init__._minimum_jaxlib_version->_parse_version(minimum_jaxlib_version)
A:jax._src.lib.__init__._jaxlib_version->_parse_version(jaxlib_version)
A:jax._src.lib.__init__.version->check_jaxlib_version(jax_version=jax.version.__version__, jaxlib_version=jaxlib.version.__version__, minimum_jaxlib_version=jax.version._minimum_jaxlib_version)
A:jax._src.lib.__init__.xla_extension_version->getattr(xla_client, '_version', 0)
A:jax._src.lib.__init__.cuda_path->os.path.join(os.path.dirname(jaxlib.__file__), 'cuda')
jax._src.lib.__init__.check_jaxlib_version(jax_version:str,jaxlib_version:str,minimum_jaxlib_version:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lib/xla_bridge.py----------------------------------------
A:jax._src.lib.xla_bridge.compile_options->jax._src.lib.xla_client.CompileOptions()
A:jax._src.lib.xla_bridge.device_assignment->jax._src.lib.xla_client.DeviceAssignment.create(device_assignment)
A:jax._src.lib.xla_bridge.t->threading.Timer(timer_secs, _log_warning)
A:jax._src.lib.xla_bridge.client->jax._src.lib.xla_client.make_tpu_client()
A:jax._src.lib.xla_bridge._backend_lock->threading.Lock()
A:jax._src.lib.xla_bridge.visible_devices->getattr(FLAGS, visible_devices_flag, 'all')
A:jax._src.lib.xla_bridge.platforms->_alias_to_platforms.get(platform, None)
A:jax._src.lib.xla_bridge.b->backends()
A:jax._src.lib.xla_bridge.jax_platforms->jax.config.config.jax_platforms.split(',')
A:jax._src.lib.xla_bridge.priorities->range(len(platforms), 0, -1)
A:jax._src.lib.xla_bridge.platforms_and_priorites->zip(platforms, priorities)
A:jax._src.lib.xla_bridge.backend->backends().get(platform, None)
A:jax._src.lib.xla_bridge._backends_errors[platform]->str(err)
A:jax._src.lib.xla_bridge.(factory, unused_priority)->_backend_factories.get(platform, (None, None))
A:jax._src.lib.xla_bridge.bs->backends()
A:jax._src.lib.xla_bridge.platform->canonicalize_platform(platform)
A:jax._src.lib.xla_bridge.process_index->get_backend(backend).process_index()
jax._src.lib.xla_bridge._clear_backends()
jax._src.lib.xla_bridge._get_backend_uncached(platform=None)
jax._src.lib.xla_bridge._init_backend(platform)
jax._src.lib.xla_bridge._make_tpu_driver_client()
jax._src.lib.xla_bridge.backends()
jax._src.lib.xla_bridge.canonicalize_platform(platform:str)->str
jax._src.lib.xla_bridge.default_backend()->str
jax._src.lib.xla_bridge.device_count(backend:Optional[Union[str,XlaBackend]]=None)->int
jax._src.lib.xla_bridge.devices(backend:Optional[Union[str,XlaBackend]]=None)->List[xla_client.Device]
jax._src.lib.xla_bridge.expand_platform_alias(platform:str)->List[str]
jax._src.lib.xla_bridge.get_backend(platform=None)
jax._src.lib.xla_bridge.get_compile_options(num_replicas:int,num_partitions:int,device_assignment=None,use_spmd_partitioning:bool=True,use_auto_spmd_partitioning:bool=False,auto_spmd_partitioning_mesh_shape=[],auto_spmd_partitioning_mesh_ids=[])->xla_client.CompileOptions
jax._src.lib.xla_bridge.get_device_backend(device=None)
jax._src.lib.xla_bridge.host_count(backend=None)
jax._src.lib.xla_bridge.host_id(backend=None)
jax._src.lib.xla_bridge.host_ids(backend=None)
jax._src.lib.xla_bridge.is_gpu(platform)
jax._src.lib.xla_bridge.is_known_platform(platform:str)
jax._src.lib.xla_bridge.local_device_count(backend:Optional[Union[str,XlaBackend]]=None)->int
jax._src.lib.xla_bridge.local_devices(process_index:Optional[int]=None,backend:Optional[Union[str,XlaBackend]]=None,host_id:Optional[int]=None)->List[xla_client.Device]
jax._src.lib.xla_bridge.make_gpu_client(*,platform_name,visible_devices_flag)
jax._src.lib.xla_bridge.process_count(backend:Optional[Union[str,XlaBackend]]=None)->int
jax._src.lib.xla_bridge.process_index(backend:Optional[Union[str,XlaBackend]]=None)->int
jax._src.lib.xla_bridge.register_backend_factory(name,factory,*,priority=0)
jax._src.lib.xla_bridge.tpu_client_timer_callback(timer_secs:float)
jax._src.lib.xla_bridge.tpu_driver_client_timer_callback(timer_secs:float)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lib/mlir/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lib/mlir/dialects/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/setops.py----------------------------------------
A:jax._src.numpy.setops.ar1_flat->ravel(ar1)
A:jax._src.numpy.setops.ar2_flat->ravel(ar2)
A:jax._src.numpy.setops.ar1->ravel(ar1)
A:jax._src.numpy.setops.size->jax.core.concrete_or_error(operator.index, size, 'The error arose for the size argument of jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.arr1->unique(arr1, size=size and arr1.size)
A:jax._src.numpy.setops.fill_value->asarray(fill_value, dtype=result.dtype)
A:jax._src.numpy.setops.mask->zeros(size, dtype=bool)
A:jax._src.numpy.setops.ar2->ravel(ar2)
A:jax._src.numpy.setops.aux->where(isnan(aux), _lax_const(aux, np.nan), aux)
A:jax._src.numpy.setops.flag->concatenate((array([True]), aux[1:] != aux[:-1], array([True])))
A:jax._src.numpy.setops.ar->jax.core.concrete_or_error(None, ar, 'The error arose for the first argument of jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.iota->jax.lax.broadcasted_iota(np.int64, np.shape(ar), dimension=0)
A:jax._src.numpy.setops.(aux, indices)->jax.lax.sort_key_val(ar, iota)
A:jax._src.numpy.setops.(ar1, ind1)->unique(ar1, return_index=True)
A:jax._src.numpy.setops.(ar2, ind2)->unique(ar2, return_index=True)
A:jax._src.numpy.setops.(aux, mask, aux_sort_indices)->_intersect1d_sorted_mask(ar1, ar2, return_indices)
A:jax._src.numpy.setops.(aux, mask)->_intersect1d_sorted_mask(ar1, ar2, return_indices)
A:jax._src.numpy.setops.result->moveaxis(result, 0, axis)
A:jax._src.numpy.setops.perm->lexsort(aux.reshape(size, _prod(out_shape)).T[::-1])
A:jax._src.numpy.setops.(aux, mask, perm)->_unique_sorted_mask(ar, axis)
A:jax._src.numpy.setops.ind->jax.core.concrete_or_error(None, mask, 'The error arose in jnp.unique(). ' + UNIQUE_SIZE_HINT)
A:jax._src.numpy.setops.valid->jax.lax.expand_dims(arange(size) < mask.sum(), tuple(range(1, result.ndim)))
A:jax._src.numpy.setops.inv_idx->zeros(ar.shape[axis], dtype=int)
A:jax._src.numpy.setops.idx->idx.at[1:].set(where(idx[1:], idx[1:], mask.size)).at[1:].set(where(idx[1:], idx[1:], mask.size))
A:jax._src.numpy.setops.arr->arr.flatten().flatten()
jax._src.numpy.setops._in1d(ar1:ArrayLike,ar2:ArrayLike,invert:bool)->Array
jax._src.numpy.setops._intersect1d_sorted_mask(ar1:ArrayLike,ar2:ArrayLike,return_indices:bool=False)->Tuple[Array, ...]
jax._src.numpy.setops._unique(ar:Array,axis:int,return_index:bool=False,return_inverse:bool=False,return_counts:bool=False,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None,return_true_size:bool=False)->Union[Array, Tuple[Array, ...]]
jax._src.numpy.setops._unique_sorted_mask(ar:Array,axis:int)->Tuple[Array, Array, Array]
jax._src.numpy.setops.in1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,invert:bool=False)->Array
jax._src.numpy.setops.intersect1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,return_indices:bool=False)->Union[Array, Tuple[Array, Array, Array]]
jax._src.numpy.setops.isin(element:ArrayLike,test_elements:ArrayLike,assume_unique:bool=False,invert:bool=False)->Array
jax._src.numpy.setops.setdiff1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)->Array
jax._src.numpy.setops.setxor1d(ar1:ArrayLike,ar2:ArrayLike,assume_unique:bool=False)->Array
jax._src.numpy.setops.union1d(ar1:ArrayLike,ar2:ArrayLike,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)->Array
jax._src.numpy.setops.unique(ar:ArrayLike,return_index:bool=False,return_inverse:bool=False,return_counts:bool=False,axis:Optional[int]=None,*,size:Optional[int]=None,fill_value:Optional[ArrayLike]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/ndarray.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/fft.py----------------------------------------
A:jax._src.numpy.fft.arr->jax._src.numpy.lax_numpy.asarray(a)
A:jax._src.numpy.fft.s->tuple(map(operator.index, s))
A:jax._src.numpy.fft.axes->tuple(range(x.ndim))
A:jax._src.numpy.fft.in_s->list(arr.shape)
A:jax._src.numpy.fft.transformed->jax._src.numpy.lax_numpy.moveaxis(transformed, axes, orig_axes)
A:jax._src.numpy.fft.conj_a->jax._src.numpy.lax_numpy.conj(a)
A:jax._src.numpy.fft.output->_fft_core_1d('ihfft', xla_client.FftType.RFFT, arr, n=n, axis=axis, norm=norm)
A:jax._src.numpy.fft.dtype->jax.dtypes.canonicalize_dtype(jnp.float_)
A:jax._src.numpy.fft.k->jax._src.numpy.lax_numpy.arange(0, (n - 1) // 2 + 1, dtype=dtype)
A:jax._src.numpy.fft.x->jax._src.numpy.lax_numpy.asarray(x)
jax._src.numpy.fft._axis_check_1d(func_name:str,axis:Optional[int])
jax._src.numpy.fft._fft_core(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,s:Optional[Shape],axes:Optional[Sequence[int]],norm:Optional[str])->Array
jax._src.numpy.fft._fft_core_1d(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,n:Optional[int],axis:Optional[int],norm:Optional[str])->Array
jax._src.numpy.fft._fft_core_2d(func_name:str,fft_type:xla_client.FftType,a:ArrayLike,s:Optional[Shape],axes:Sequence[int],norm:Optional[str])->Array
jax._src.numpy.fft._fft_norm(s:Array,func_name:str,norm:str)->Array
jax._src.numpy.fft.fft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.fft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.fftfreq(n:int,d:ArrayLike=1.0)->Array
jax._src.numpy.fft.fftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.fftshift(x:ArrayLike,axes:Union[None,int,Sequence[int]]=None)->Array
jax._src.numpy.fft.hfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.ifftshift(x:ArrayLike,axes:Union[None,int,Sequence[int]]=None)->Array
jax._src.numpy.fft.ihfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.irfftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfft(a:ArrayLike,n:Optional[int]=None,axis:int=-1,norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfft2(a:ArrayLike,s:Optional[Shape]=None,axes:Sequence[int]=(-2,-1),norm:Optional[str]=None)->Array
jax._src.numpy.fft.rfftfreq(n:int,d:ArrayLike=1.0)->Array
jax._src.numpy.fft.rfftn(a:ArrayLike,s:Optional[Shape]=None,axes:Optional[Sequence[int]]=None,norm:Optional[str]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/polynomial.py----------------------------------------
A:jax._src.numpy.polynomial.A->A.at[0, :].set(-p[1:] / p[0]).at[0, :].set(-p[1:] / p[0])
A:jax._src.numpy.polynomial.p->_where(len(p) == num_leading_zeros, 1.0, p)
A:jax._src.numpy.polynomial.roots->_roots_no_zeros(roll(p, -num_leading_zeros))
A:jax._src.numpy.polynomial.p_arr->atleast_1d(*_promote_dtypes_inexact(p))
A:jax._src.numpy.polynomial.num_leading_zeros->jax.core.concrete_or_error(int, num_leading_zeros, 'The error occurred in the jnp.roots() function. To use this within a JIT-compiled context, pass strip_zeros=False, but be aware that leading zeros will be result in some returned roots being set to NaN.')
A:jax._src.numpy.polynomial.deg->jax.core.concrete_or_error(int, deg, 'deg must be int')
A:jax._src.numpy.polynomial.rcond->jax.core.concrete_or_error(float, rcond, 'rcond must be float')
A:jax._src.numpy.polynomial.lhs->vander(x, order)
A:jax._src.numpy.polynomial.(w,)->_promote_dtypes_inexact(w)
A:jax._src.numpy.polynomial.scale->sqrt((lhs * lhs).sum(axis=0))
A:jax._src.numpy.polynomial.(c, resids, rank, s)->jax._src.numpy.linalg.lstsq(lhs, rhs, rcond)
A:jax._src.numpy.polynomial.Vbase->jax._src.numpy.linalg.inv(dot(lhs.T, lhs))
A:jax._src.numpy.polynomial.(seq_of_zeros,)->_promote_dtypes_inexact(seq_of_zeros)
A:jax._src.numpy.polynomial.seq_of_zeros->jax._src.numpy.linalg.eigvals(seq_of_zeros)
A:jax._src.numpy.polynomial.a->convolve(a, array([1, -seq_of_zeros[k]], dtype=dt), mode='full')
A:jax._src.numpy.polynomial.(p, x)->_promote_dtypes_inexact(p, x)
A:jax._src.numpy.polynomial.shape->jax.lax.broadcast_shapes(p.shape[1:], x.shape)
A:jax._src.numpy.polynomial.y->jax.lax.full_like(x, 0, shape=shape, dtype=x.dtype)
A:jax._src.numpy.polynomial.(y, _)->jax.lax.scan(lambda y, p: (y * x + p, None), y, p, unroll=unroll)
A:jax._src.numpy.polynomial.(a1, a2)->_promote_dtypes(a1, a2)
A:jax._src.numpy.polynomial.m->jax.core.concrete_or_error(operator.index, m, "'m' argument of jnp.polyder")
A:jax._src.numpy.polynomial.(p, k_arr)->_promote_dtypes_inexact(p, k)
A:jax._src.numpy.polynomial.k_arr->full((m,), k_arr[0])
A:jax._src.numpy.polynomial.(p,)->_promote_dtypes_inexact(p)
A:jax._src.numpy.polynomial.coeff->(arange(m, len(p), dtype=p.dtype)[np.newaxis] - arange(m, dtype=p.dtype)[:, np.newaxis]).prod(0)
A:jax._src.numpy.polynomial.(a1_arr, a2_arr)->_promote_dtypes_inexact(a1, a2)
A:jax._src.numpy.polynomial.a1_arr->asarray([0], dtype=a2_arr.dtype)
A:jax._src.numpy.polynomial.a2_arr->asarray([0], dtype=a1_arr.dtype)
A:jax._src.numpy.polynomial.(u_arr, v_arr)->_promote_dtypes_inexact(u, v)
A:jax._src.numpy.polynomial.q->q.at[k].set(d).at[k].set(d)
A:jax._src.numpy.polynomial.u_arr->trim_zeros_tol(u_arr, tol=sqrt(finfo(u_arr.dtype).eps), trim='f')
jax._src.numpy.polynomial._roots_no_zeros(p:Array)->Array
jax._src.numpy.polynomial._roots_with_zeros(p:Array,num_leading_zeros:int)->Array
jax._src.numpy.polynomial.poly(seq_of_zeros:Array)->Array
jax._src.numpy.polynomial.polyadd(a1:Array,a2:Array)->Array
jax._src.numpy.polynomial.polyder(p:Array,m:int=1)->Array
jax._src.numpy.polynomial.polydiv(u:ArrayLike,v:ArrayLike,*,trim_leading_zeros:bool=False)->Tuple[Array, Array]
jax._src.numpy.polynomial.polyfit(x:Array,y:Array,deg:int,rcond:Optional[float]=None,full:bool=False,w:Optional[Array]=None,cov:bool=False)->Union[Array, Tuple[Array, ...]]
jax._src.numpy.polynomial.polyint(p:Array,m:int=1,k:Optional[int]=None)->Array
jax._src.numpy.polynomial.polymul(a1:ArrayLike,a2:ArrayLike,*,trim_leading_zeros:bool=False)->Array
jax._src.numpy.polynomial.polysub(a1:Array,a2:Array)->Array
jax._src.numpy.polynomial.polyval(p:Array,x:Array,*,unroll:int=16)->Array
jax._src.numpy.polynomial.roots(p:ArrayLike,*,strip_zeros:bool=True)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/ufuncs.py----------------------------------------
A:jax._src.numpy.ufuncs.fn->jit(fn, inline=True)
A:jax._src.numpy.ufuncs.doc->dedent('\n\n'.join(lax_fn.__doc__.split('\n\n')[1:])).strip()
A:jax._src.numpy.ufuncs.(x1, x2)->_promote_dtypes_inexact(x1, x2)
A:jax._src.numpy.ufuncs.rx->jax.lax.real(x1)
A:jax._src.numpy.ufuncs.ry->jax.lax.real(x2)
A:jax._src.numpy.ufuncs.fabs->_one_to_one_unop(np.fabs, lax.abs, True)
A:jax._src.numpy.ufuncs.bitwise_not->_one_to_one_unop(np.bitwise_not, lax.bitwise_not)
A:jax._src.numpy.ufuncs.invert->_one_to_one_unop(np.invert, lax.bitwise_not)
A:jax._src.numpy.ufuncs.negative->_one_to_one_unop(np.negative, lax.neg)
A:jax._src.numpy.ufuncs.positive->_one_to_one_unop(np.positive, lambda x: x)
A:jax._src.numpy.ufuncs.floor->_one_to_one_unop(np.floor, lax.floor, True)
A:jax._src.numpy.ufuncs.ceil->_one_to_one_unop(np.ceil, lax.ceil, True)
A:jax._src.numpy.ufuncs.exp->_one_to_one_unop(np.exp, lax.exp, True)
A:jax._src.numpy.ufuncs.log->_one_to_one_unop(np.log, lax.log, True)
A:jax._src.numpy.ufuncs.expm1->_one_to_one_unop(np.expm1, lax.expm1, True)
A:jax._src.numpy.ufuncs.log1p->_one_to_one_unop(np.log1p, lax.log1p, True)
A:jax._src.numpy.ufuncs.sin->_one_to_one_unop(np.sin, lax.sin, True)
A:jax._src.numpy.ufuncs.cos->_one_to_one_unop(np.cos, lax.cos, True)
A:jax._src.numpy.ufuncs.tan->_one_to_one_unop(np.tan, lax.tan, True)
A:jax._src.numpy.ufuncs.arcsin->_one_to_one_unop(np.arcsin, lax.asin, True)
A:jax._src.numpy.ufuncs.arccos->_one_to_one_unop(np.arccos, lax.acos, True)
A:jax._src.numpy.ufuncs.arctan->_one_to_one_unop(np.arctan, lax.atan, True)
A:jax._src.numpy.ufuncs.sinh->_one_to_one_unop(np.sinh, lax.sinh, True)
A:jax._src.numpy.ufuncs.cosh->_one_to_one_unop(np.cosh, lax.cosh, True)
A:jax._src.numpy.ufuncs.arcsinh->_one_to_one_unop(np.arcsinh, lax.asinh, True)
A:jax._src.numpy.ufuncs.tanh->_one_to_one_unop(np.tanh, lax.tanh, True)
A:jax._src.numpy.ufuncs.arctanh->_one_to_one_unop(np.arctanh, lax.atanh, True)
A:jax._src.numpy.ufuncs.sqrt->_one_to_one_unop(np.sqrt, lax.sqrt, True)
A:jax._src.numpy.ufuncs.cbrt->_one_to_one_unop(np.cbrt, lax.cbrt, True)
A:jax._src.numpy.ufuncs.add->_maybe_bool_binop(np.add, lax.add, lax.bitwise_or)
A:jax._src.numpy.ufuncs.bitwise_and->_one_to_one_binop(np.bitwise_and, lax.bitwise_and)
A:jax._src.numpy.ufuncs.bitwise_or->_one_to_one_binop(np.bitwise_or, lax.bitwise_or)
A:jax._src.numpy.ufuncs.bitwise_xor->_one_to_one_binop(np.bitwise_xor, lax.bitwise_xor)
A:jax._src.numpy.ufuncs.left_shift->_one_to_one_binop(np.left_shift, lax.shift_left, promote_to_numeric=True)
A:jax._src.numpy.ufuncs.equal->_one_to_one_binop(np.equal, lax.eq)
A:jax._src.numpy.ufuncs.multiply->_maybe_bool_binop(np.multiply, lax.mul, lax.bitwise_and)
A:jax._src.numpy.ufuncs.not_equal->_one_to_one_binop(np.not_equal, lax.ne)
A:jax._src.numpy.ufuncs.subtract->_one_to_one_binop(np.subtract, lax.sub)
A:jax._src.numpy.ufuncs.arctan2->_one_to_one_binop(np.arctan2, lax.atan2, True)
A:jax._src.numpy.ufuncs.minimum->_one_to_one_binop(np.minimum, lax.min)
A:jax._src.numpy.ufuncs.maximum->_one_to_one_binop(np.maximum, lax.max)
A:jax._src.numpy.ufuncs.float_power->_one_to_one_binop(np.float_power, lax.pow, True)
A:jax._src.numpy.ufuncs.nextafter->_one_to_one_binop(np.nextafter, lax.nextafter, True, True)
A:jax._src.numpy.ufuncs.greater_equal->_comparison_op(np.greater_equal, lax.ge)
A:jax._src.numpy.ufuncs.greater->_comparison_op(np.greater, lax.gt)
A:jax._src.numpy.ufuncs.less_equal->_comparison_op(np.less_equal, lax.le)
A:jax._src.numpy.ufuncs.less->_comparison_op(np.less, lax.lt)
A:jax._src.numpy.ufuncs.logical_and->_logical_op(np.logical_and, lax.bitwise_and)
A:jax._src.numpy.ufuncs.logical_not->_logical_op(np.logical_not, lax.bitwise_not)
A:jax._src.numpy.ufuncs.logical_or->_logical_op(np.logical_or, lax.bitwise_or)
A:jax._src.numpy.ufuncs.logical_xor->_logical_op(np.logical_xor, lax.bitwise_xor)
A:jax._src.numpy.ufuncs.out->jax.lax.add(amax, lax.div(lax.log1p(exp2(delta)), _constant_like(x1, np.log(2))))
A:jax._src.numpy.ufuncs.dt->jax._src.dtypes.dtype(x)
A:jax._src.numpy.ufuncs.abs->_wraps(np.abs, module='numpy')(absolute)
A:jax._src.numpy.ufuncs.dtype->jax._src.dtypes.dtype(x)
A:jax._src.numpy.ufuncs.re->jax.lax.real(x)
A:jax._src.numpy.ufuncs.quotient->jax.lax.div(x1, x2)
A:jax._src.numpy.ufuncs.select->logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
A:jax._src.numpy.ufuncs.x1r->jax.lax.real(x1)
A:jax._src.numpy.ufuncs.x1i->jax.lax.imag(x1)
A:jax._src.numpy.ufuncs.x2r->jax.lax.real(x2)
A:jax._src.numpy.ufuncs.x2i->jax.lax.imag(x2)
A:jax._src.numpy.ufuncs.which->jax.lax.ge(lax.abs(x2r), lax.abs(x2i))
A:jax._src.numpy.ufuncs.rat1->_where(which, lax.full_like(x2i, 1), lax.div(x2r, x2i))
A:jax._src.numpy.ufuncs.rat2->_where(which, lax.div(x2i, x2r), _lax_const(x2i, 1))
A:jax._src.numpy.ufuncs.mod->_wraps(np.mod, module='numpy')(remainder)
A:jax._src.numpy.ufuncs.div->jax.lax.select(ind, div - _constant_like(div, 1), div)
A:jax._src.numpy.ufuncs.ind->jax.lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
A:jax._src.numpy.ufuncs.zero->_lax_const(x1, 0)
A:jax._src.numpy.ufuncs.one->_constant_like(x2, 1)
A:jax._src.numpy.ufuncs.acc->_where(lax.bitwise_and(x2, one), lax.mul(acc, x1), acc)
A:jax._src.numpy.ufuncs.x1->jax.lax.abs(x1)
A:jax._src.numpy.ufuncs.x2->jax.lax.abs(x2)
A:jax._src.numpy.ufuncs.(x1,)->_promote_dtypes_numeric(x1)
A:jax._src.numpy.ufuncs.amax->jax.lax.max(x1, x2)
A:jax._src.numpy.ufuncs.delta->jax.lax.sub(lax.add(x1, x2), lax.mul(amax, _constant_like(amax, 2)))
A:jax._src.numpy.ufuncs.a->_constant_like(x, _a)
A:jax._src.numpy.ufuncs.two_a->_constant_like(x, 2 * _a)
A:jax._src.numpy.ufuncs.rem->jax.lax.select(lax.lt(rem, zero), lax.add(rem, two_a), rem)
A:jax._src.numpy.ufuncs.(x1, x2, t1, t2)->_promote_args_inexact('logaddexp2_jvp', x1, x2, t1, t2)
A:jax._src.numpy.ufuncs.primal_out->logaddexp2(x1, x2)
A:jax._src.numpy.ufuncs.tangent_out->jax.lax.add(lax.mul(t1, exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))), lax.mul(t2, exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))
A:jax._src.numpy.ufuncs.(x,)->_promote_dtypes_inexact(x)
A:jax._src.numpy.ufuncs.x->_where(overflow_cond, lax.sign(x1) * lax.full_like(x, np.inf), x)
A:jax._src.numpy.ufuncs.info->jax._src.dtypes.finfo(dtype)
A:jax._src.numpy.ufuncs.x1_dtype->jax._src.dtypes.dtype(x1)
A:jax._src.numpy.ufuncs.x2_dtype->jax._src.dtypes.dtype(x2)
A:jax._src.numpy.ufuncs.(x, e)->_normalize_float(x1)
A:jax._src.numpy.ufuncs.m->_where(cond, m / (1 << info.nmant), m)
A:jax._src.numpy.ufuncs.trunc_mod->jax.lax.rem(x1, x2)
A:jax._src.numpy.ufuncs.trunc_mod_not_zero->jax.lax.ne(trunc_mod, zero)
A:jax._src.numpy.ufuncs.do_plus->jax.lax.bitwise_and(lax.ne(lax.lt(trunc_mod, zero), lax.lt(x2, zero)), trunc_mod_not_zero)
A:jax._src.numpy.ufuncs.whole->_where(lax.ge(x, lax_internal._zero(x)), floor(x), ceil(x))
A:jax._src.numpy.ufuncs.im->jax.lax.imag(x)
A:jax._src.numpy.ufuncs.isposinf->_wraps(np.isposinf, skip_params=['out'])(lambda x, out=None: _isposneginf(np.inf, x, out))
A:jax._src.numpy.ufuncs.isneginf->_wraps(np.isneginf, skip_params=['out'])(lambda x, out=None: _isposneginf(-np.inf, x, out))
A:jax._src.numpy.ufuncs.eq_zero->jax.lax.eq(x, _lax_const(x, 0))
A:jax._src.numpy.ufuncs.pi_x->jax.lax.mul(_lax_const(x, np.pi), x)
A:jax._src.numpy.ufuncs.safe_pi_x->_where(eq_zero, _lax_const(x, 1), pi_x)
jax._src.numpy.ufuncs._comparison_op(numpy_fn,lax_fn)
jax._src.numpy.ufuncs._constant_like(x,const)
jax._src.numpy.ufuncs._float_divmod(x1,x2)
jax._src.numpy.ufuncs._isposneginf(infinity,x,out)
jax._src.numpy.ufuncs._logaddexp2_jvp(primals,tangents)
jax._src.numpy.ufuncs._logaddexp_jvp(primals,tangents)
jax._src.numpy.ufuncs._logical_op(np_op,bitwise_op)
jax._src.numpy.ufuncs._maybe_bool_binop(numpy_fn,lax_fn,bool_lax_fn,lax_doc=False)
jax._src.numpy.ufuncs._normalize_float(x)
jax._src.numpy.ufuncs._one_to_one_binop(numpy_fn,lax_fn,promote_to_inexact=False,lax_doc=False,promote_to_numeric=False)
jax._src.numpy.ufuncs._one_to_one_unop(numpy_fn,lax_fn,promote_to_inexact=False,lax_doc=False)
jax._src.numpy.ufuncs._power(x1,x2)
jax._src.numpy.ufuncs._replace_inf(x)
jax._src.numpy.ufuncs._sinc_maclaurin(k,x)
jax._src.numpy.ufuncs._sinc_maclaurin_jvp(k,primals,tangents)
jax._src.numpy.ufuncs._wrap_between(x,_a)
jax._src.numpy.ufuncs.absolute(x)
jax._src.numpy.ufuncs.arccosh(x)
jax._src.numpy.ufuncs.conjugate(x)
jax._src.numpy.ufuncs.copysign(x1,x2)
jax._src.numpy.ufuncs.deg2rad(x)
jax._src.numpy.ufuncs.divmod(x1,x2)
jax._src.numpy.ufuncs.exp2(x)
jax._src.numpy.ufuncs.floor_divide(x1,x2)
jax._src.numpy.ufuncs.fmod(x1,x2)
jax._src.numpy.ufuncs.frexp(x)
jax._src.numpy.ufuncs.heaviside(x1,x2)
jax._src.numpy.ufuncs.hypot(x1,x2)
jax._src.numpy.ufuncs.imag(val)
jax._src.numpy.ufuncs.isfinite(x)
jax._src.numpy.ufuncs.isinf(x)
jax._src.numpy.ufuncs.isnan(x)
jax._src.numpy.ufuncs.ldexp(x1,x2)
jax._src.numpy.ufuncs.log10(x)
jax._src.numpy.ufuncs.log2(x)
jax._src.numpy.ufuncs.logaddexp(x1,x2)
jax._src.numpy.ufuncs.logaddexp2(x1,x2)
jax._src.numpy.ufuncs.modf(x,out=None)
jax._src.numpy.ufuncs.power(x1,x2)
jax._src.numpy.ufuncs.rad2deg(x)
jax._src.numpy.ufuncs.real(val)
jax._src.numpy.ufuncs.reciprocal(x)
jax._src.numpy.ufuncs.remainder(x1,x2)
jax._src.numpy.ufuncs.right_shift(x1,x2)
jax._src.numpy.ufuncs.rint(x)
jax._src.numpy.ufuncs.sign(x)
jax._src.numpy.ufuncs.signbit(x)
jax._src.numpy.ufuncs.sinc(x)
jax._src.numpy.ufuncs.square(x)
jax._src.numpy.ufuncs.true_divide(x1,x2)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/linalg.py----------------------------------------
A:jax._src.numpy.linalg.(a,)->_promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.numpy.linalg.(w, v)->jax._src.lax.linalg.eig(a, compute_left_eigenvectors=False)
A:jax._src.numpy.linalg.s->jax._src.numpy.lax_numpy.where(s > cutoff, s, jnp.inf).astype(u.dtype)
A:jax._src.numpy.linalg.sign->jax._src.numpy.lax_numpy.asarray(-2 * (parity % 2) + 1, dtype=dtype)
A:jax._src.numpy.linalg.idxs->jax.lax.rev(idxs, dimensions=[s.ndim - 1])
A:jax._src.numpy.linalg.(s, idxs, sign)->jax.lax.sort((s, idxs, sign), dimension=-1, num_keys=1)
A:jax._src.numpy.linalg.u->jax._src.numpy.lax_numpy.take_along_axis(w, idxs[..., None, :], axis=-1)
A:jax._src.numpy.linalg.vh->_H(u * sign[..., None, :].astype(u.dtype))
A:jax._src.numpy.linalg.(arr,)->_promote_dtypes_inexact(jnp.asarray(a))
A:jax._src.numpy.linalg.n->abs(n)
A:jax._src.numpy.linalg.arr->jax._src.numpy.lax_numpy.asarray(a)
A:jax._src.numpy.linalg.(n, bit)->divmod(n, 2)
A:jax._src.numpy.linalg.(M,)->_promote_dtypes_inexact(jnp.asarray(M))
A:jax._src.numpy.linalg.S->svd(M, full_matrices=False, compute_uv=False)
A:jax._src.numpy.linalg.tol->jax._src.numpy.lax_numpy.expand_dims(tol, np.ndim(tol))
A:jax._src.numpy.linalg.dtype->jax.lax.dtype(a)
A:jax._src.numpy.linalg.(lu, pivot, _)->jax._src.lax.linalg.lu(a)
A:jax._src.numpy.linalg.diag->jax._src.numpy.lax_numpy.diagonal(lu, axis1=-2, axis2=-1)
A:jax._src.numpy.linalg.is_zero->jax._src.numpy.lax_numpy.any(diag == jnp.array(0, dtype=dtype), axis=-1)
A:jax._src.numpy.linalg.iota->jax.lax.expand_dims(jnp.arange(a_shape[-1], dtype=pivots.dtype), range(pivots.ndim - 1))
A:jax._src.numpy.linalg.parity->jax._src.numpy.lax_numpy.count_nonzero(pivots != iota, axis=-1)
A:jax._src.numpy.linalg.logdet->jax._src.numpy.lax_numpy.where(is_zero, jnp.array(-jnp.inf, dtype=dtype), jnp.sum(jnp.log(jnp.abs(diag)).astype(dtype), axis=-1))
A:jax._src.numpy.linalg.(a, taus)->jax._src.lax.linalg.geqrf(a)
A:jax._src.numpy.linalg.log_abs_det->jax._src.numpy.lax_numpy.trace(jnp.log(jnp.abs(a)), axis1=-2, axis2=-1)
A:jax._src.numpy.linalg.sign_diag->jax._src.numpy.lax_numpy.prod(jnp.sign(jnp.diagonal(a, axis1=-2, axis2=-1)), axis=-1)
A:jax._src.numpy.linalg.sign_taus->jax._src.numpy.lax_numpy.prod(jnp.where(taus[..., :n - 1] != 0, -1, 1), axis=-1).astype(sign_diag.dtype)
A:jax._src.numpy.linalg.a_shape->jax._src.numpy.lax_numpy.shape(a)
A:jax._src.numpy.linalg.(sign, ans)->slogdet(x)
A:jax._src.numpy.linalg.ans_dot->jax._src.numpy.lax_numpy.real(ans_dot)
A:jax._src.numpy.linalg.sign_dot->jax._src.numpy.lax_numpy.zeros_like(sign)
A:jax._src.numpy.linalg.(b,)->_promote_dtypes_inexact(jnp.asarray(b))
A:jax._src.numpy.linalg.b_shape->jax._src.numpy.lax_numpy.shape(b)
A:jax._src.numpy.linalg.a_ndims->len(a_shape)
A:jax._src.numpy.linalg.(lu, pivots, permutation)->jax._src.lax.linalg.lu(a)
A:jax._src.numpy.linalg.batch_dims->jax.lax.broadcast_shapes(lu.shape[:-2], b.shape[:-2])
A:jax._src.numpy.linalg.x->x.ravel().ravel()
A:jax._src.numpy.linalg.lu->lu.at[..., -1, -1].set(1.0 / partial_det[..., -2]).at[..., -1, -1].set(1.0 / partial_det[..., -2])
A:jax._src.numpy.linalg.permutation->jax._src.numpy.lax_numpy.broadcast_to(permutation, (*batch_dims, a_shape[-1]))
A:jax._src.numpy.linalg.iotas->jax._src.numpy.lax_numpy.ix_(*(lax.iota(jnp.int32, b) for b in (*batch_dims, 1)))
A:jax._src.numpy.linalg.d->jax._src.numpy.lax_numpy.tile(d[..., None, None], d.ndim * (1,) + x.shape[-2:])
A:jax._src.numpy.linalg.(sign, logdet)->slogdet(a)
A:jax._src.numpy.linalg.(y, z)->_cofactor_solve(x, g)
A:jax._src.numpy.linalg.(v, w)->jax._src.lax.linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input)
A:jax._src.numpy.linalg.(w, _)->eigh(a, UPLO)
A:jax._src.numpy.linalg.max_rows_cols->max(arr.shape[-2:])
A:jax._src.numpy.linalg.rcond->jax._src.numpy.lax_numpy.where(rcond < 0, jnp.finfo(dtype).eps, rcond)
A:jax._src.numpy.linalg.(u, s, vh)->svd(arr, full_matrices=False)
A:jax._src.numpy.linalg.res->jax._src.numpy.lax_numpy.matmul(_T(vh), jnp.divide(_T(u), s[..., jnp.newaxis]))
A:jax._src.numpy.linalg.p->pinv(a, rcond=rcond)
A:jax._src.numpy.linalg.I_n->jax.lax.expand_dims(jnp.eye(m, dtype=a.dtype), range(a.ndim - 2))
A:jax._src.numpy.linalg.I_m->jax.lax.expand_dims(jnp.eye(n, dtype=a.dtype), range(a.ndim - 2))
A:jax._src.numpy.linalg.(x,)->_promote_dtypes_inexact(jnp.asarray(x))
A:jax._src.numpy.linalg.x_shape->jax._src.numpy.lax_numpy.shape(x)
A:jax._src.numpy.linalg.ndim->len(x_shape)
A:jax._src.numpy.linalg.axis->tuple((canonicalize_axis(x, ndim) for x in axis))
A:jax._src.numpy.linalg.num_axes->len(axis)
A:jax._src.numpy.linalg.abs_x->jax._src.numpy.lax_numpy.abs(x)
A:jax._src.numpy.linalg.ord_arr->jax._src.lax.lax._const(abs_x, ord)
A:jax._src.numpy.linalg.ord_inv->jax._src.lax.lax._const(abs_x, 1.0 / ord_arr)
A:jax._src.numpy.linalg.out->jax._src.numpy.lax_numpy.sum(abs_x ** ord_arr, axis=axis, keepdims=keepdims)
A:jax._src.numpy.linalg.(row_axis, col_axis)->cast(Tuple[int, ...], axis)
A:jax._src.numpy.linalg.y->jax._src.numpy.lax_numpy.expand_dims(y, axis)
A:jax._src.numpy.linalg.(q, r)->jax._src.lax.linalg.qr(a, full_matrices=full_matrices)
A:jax._src.numpy.linalg.(a, b)->_promote_dtypes_inexact(a, b)
A:jax._src.numpy.linalg.(u, s, vt)->svd(a, full_matrices=False)
A:jax._src.numpy.linalg.rank->mask.sum()
A:jax._src.numpy.linalg.safe_s->jax._src.numpy.lax_numpy.where(mask, s, 1).astype(a.dtype)
A:jax._src.numpy.linalg.uTb->jax._src.numpy.lax_numpy.matmul(u.conj().T, b, precision=lax.Precision.HIGHEST)
A:jax._src.numpy.linalg.resid->jax._src.numpy.lax_numpy.asarray([])
A:jax._src.numpy.linalg.b_estimate->jax._src.numpy.lax_numpy.matmul(a, x, precision=lax.Precision.HIGHEST)
A:jax._src.numpy.linalg._jit_lstsq->jit(partial(_lstsq, numpy_resid=False))
jax._src.numpy.linalg._H(x:ArrayLike)->Array
jax._src.numpy.linalg._T(x:ArrayLike)->Array
jax._src.numpy.linalg._cofactor_solve(a:ArrayLike,b:ArrayLike)->Tuple[Array, Array]
jax._src.numpy.linalg._det_2x2(a:Array)->Array
jax._src.numpy.linalg._det_3x3(a:Array)->Array
jax._src.numpy.linalg._det_jvp(primals,tangents)
jax._src.numpy.linalg._lstsq(a:ArrayLike,b:ArrayLike,rcond:Optional[float],*,numpy_resid:bool=False)->Tuple[Array, Array, Array, Array]
jax._src.numpy.linalg._pinv_jvp(rcond,primals,tangents)
jax._src.numpy.linalg._slogdet_jvp(primals,tangents)
jax._src.numpy.linalg._slogdet_lu(a:Array)->Tuple[Array, Array]
jax._src.numpy.linalg._slogdet_qr(a:Array)->Tuple[Array, Array]
jax._src.numpy.linalg.cholesky(a:ArrayLike)->Array
jax._src.numpy.linalg.det(a:ArrayLike)->Array
jax._src.numpy.linalg.eig(a:ArrayLike)->Tuple[Array, Array]
jax._src.numpy.linalg.eigh(a:ArrayLike,UPLO:Optional[str]=None,symmetrize_input:bool=True)->Tuple[Array, Array]
jax._src.numpy.linalg.eigvals(a:ArrayLike)->Array
jax._src.numpy.linalg.eigvalsh(a:ArrayLike,UPLO:Optional[str]='L')->Array
jax._src.numpy.linalg.inv(a:ArrayLike)->Array
jax._src.numpy.linalg.lstsq(a:ArrayLike,b:ArrayLike,rcond:Optional[float]=None,*,numpy_resid:bool=False)->Tuple[Array, Array, Array, Array]
jax._src.numpy.linalg.matrix_power(a:ArrayLike,n:int)->Array
jax._src.numpy.linalg.matrix_rank(M:ArrayLike,tol:Optional[ArrayLike]=None)->Array
jax._src.numpy.linalg.norm(x:ArrayLike,ord:Union[int,str,None]=None,axis:Union[None,Tuple[int,...],int]=None,keepdims:bool=False)->Array
jax._src.numpy.linalg.pinv(a:ArrayLike,rcond:Optional[ArrayLike]=None)->Array
jax._src.numpy.linalg.qr(a:ArrayLike,mode:str='reduced')->Union[Array, Tuple[Array, Array]]
jax._src.numpy.linalg.slogdet(a:ArrayLike,*,method:Optional[str]=None)->Tuple[Array, Array]
jax._src.numpy.linalg.solve(a:ArrayLike,b:ArrayLike)->Array
jax._src.numpy.linalg.svd(a:ArrayLike,full_matrices:bool=True,compute_uv:bool=True,hermitian:bool=False)->Union[Array, Tuple[Array, Array, Array]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/util.py----------------------------------------
A:jax._src.numpy.util._T->TypeVar('_T')
A:jax._src.numpy.util._parameter_break->re.compile('\n(?=[A-Za-z_])')
A:jax._src.numpy.util._section_break->re.compile('\\n(?=[^\\n]{3,15}\\n-{3,15})', re.MULTILINE)
A:jax._src.numpy.util._numpy_signature_re->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE)
A:jax._src.numpy.util._versionadded->re.compile('^\\s+\\.\\.\\s+versionadded::', re.MULTILINE)
A:jax._src.numpy.util._docreference->re.compile(':doc:`(.*?)\\s*<.*?>`')
A:jax._src.numpy.util.docstr->getattr(fun, '__doc__', None)
A:jax._src.numpy.util.match->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE).match(body)
A:jax._src.numpy.util.signature->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*?\\)$', re.MULTILINE).match(body).group()
A:jax._src.numpy.util.(firstline, _, body)->textwrap.dedent(body.lstrip('\n')).partition('\n')
A:jax._src.numpy.util.body->textwrap.dedent(body.lstrip('\n'))
A:jax._src.numpy.util.(summary, _, body)->textwrap.dedent(body.lstrip('\n')).lstrip('\n').partition('\n')
A:jax._src.numpy.util.section_list->re.compile('\\n(?=[^\\n]{3,15}\\n-{3,15})', re.MULTILINE).split(body)
A:jax._src.numpy.util.(title, underline, content)->textwrap.dedent(body.lstrip('\n')).split('\n', 2)
A:jax._src.numpy.util.parameters->_parse_parameters(parsed.sections['Parameters'])
A:jax._src.numpy.util.name->getattr(fun, '__name__', getattr(op, '__name__', str(op)))
A:jax._src.numpy.util.parsed->_parse_numpydoc(docstr)
A:jax._src.numpy.util.code->getattr(getattr(op, '__wrapped__', op), '__code__', None)
A:jax._src.numpy.util.value->getattr(fun, attr)
A:jax._src.numpy.util._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.numpy.util.(dtype, weak_type)->jax._src.dtypes._lattice_result_type(arr)
A:jax._src.numpy.util.res_shape->jax._src.lax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.util.result_rank->len(lax.broadcast_shapes(*shapes))
A:jax._src.numpy.util.(to_dtype, weak_type)->jax._src.dtypes._lattice_result_type(*args)
A:jax._src.numpy.util.to_dtype->jax._src.dtypes.canonicalize_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_inexact->jax._src.dtypes.to_inexact_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_numeric->jax._src.dtypes.to_numeric_dtype(to_dtype)
A:jax._src.numpy.util.to_dtype_complex->jax._src.dtypes.to_complex_dtype(to_dtype)
A:jax._src.numpy.util.(pos, arg)->next(((i, arg) for (i, arg) in enumerate(args) if not _arraylike(arg)))
A:jax._src.numpy.util.result_shape->jax._src.lax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.util.shape->jax.core.canonicalize_shape(shape)
A:jax._src.numpy.util.arr_shape->numpy.shape(arr)
A:jax._src.numpy.util.compatible->all((core.symbolic_equal_one_of_dim(arr_d, [1, shape_d]) for (arr_d, shape_d) in safe_zip(arr_shape, shape_tail)))
A:jax._src.numpy.util.(diff,)->numpy.where(tuple((not core.symbolic_equal_dim(arr_d, shape_d) for (arr_d, shape_d) in safe_zip(arr_shape, shape_tail))))
A:jax._src.numpy.util.kept_dims->tuple(np.delete(np.arange(len(shape)), new_dims))
A:jax._src.numpy.util.condition->jax._src.lax.lax.ne(condition, lax_internal._zero(condition))
A:jax._src.numpy.util.(x, y)->_promote_dtypes(x, y)
A:jax._src.numpy.util.(condition_arr, x_arr, y_arr)->_broadcast_arrays(condition, x, y)
A:jax._src.numpy.util.is_always_empty->jax.core.is_empty_shape(x_arr.shape)
jax._src.numpy.util.ParsedDoc(NamedTuple)
jax._src.numpy.util._arraylike(x:ArrayLike)->bool
jax._src.numpy.util._asarray(arr:ArrayLike)->Array
jax._src.numpy.util._broadcast_arrays(*args:ArrayLike)->List[Array]
jax._src.numpy.util._broadcast_to(arr:ArrayLike,shape:Shape)->Array
jax._src.numpy.util._check_arraylike(fun_name:str,*args:Any)
jax._src.numpy.util._check_no_float0s(fun_name:str,*args:Any)
jax._src.numpy.util._complex_elem_type(dtype:DTypeLike)->DType
jax._src.numpy.util._parse_extra_params(extra_params:str)->Dict[str, str]
jax._src.numpy.util._parse_numpydoc(docstr:Optional[str])->ParsedDoc
jax._src.numpy.util._parse_parameters(body:str)->Dict[str, str]
jax._src.numpy.util._promote_args(fun_name:str,*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_args_inexact(fun_name:str,*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_args_numeric(fun_name:str,*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_dtypes(*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_dtypes_complex(*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_dtypes_inexact(*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_dtypes_numeric(*args:ArrayLike)->List[Array]
jax._src.numpy.util._promote_shapes(fun_name:str,*args:ArrayLike)->List[Array]
jax._src.numpy.util._rank_promotion_warning_or_error(fun_name:str,shapes:Sequence[Shape])
jax._src.numpy.util._stackable(*args:Any)->bool
jax._src.numpy.util._where(condition:ArrayLike,x:Optional[ArrayLike]=None,y:Optional[ArrayLike]=None)->Array
jax._src.numpy.util._wraps(fun:Optional[Callable[...,Any]],update_doc:bool=True,lax_description:str='',sections:Sequence[str]=('Parameters','Returns','References'),skip_params:Sequence[str]=(),extra_params:Optional[str]=None,module:Optional[str]=None)->Callable[[_T], _T]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/reductions.py----------------------------------------
A:jax._src.numpy.reductions.element->element.__jax_array__().__jax_array__()
A:jax._src.numpy.reductions.a->jax.lax.convert_element_type(a, dtype)
A:jax._src.numpy.reductions.source->_canonicalize_axis(source, np.ndim(a))
A:jax._src.numpy.reductions.destination->_canonicalize_axis(destination, np.ndim(a))
A:jax._src.numpy.reductions.axis->_canonicalize_axis(axis, num_dims)
A:jax._src.numpy.reductions.(pos_dims, dims)->_reduction_dims(a, axis)
A:jax._src.numpy.reductions.shape->numpy.shape(a)
A:jax._src.numpy.reductions.result_dtype->jax._src.dtypes.canonicalize_dtype(result_dtype)
A:jax._src.numpy.reductions.computation_dtype->_upcast_f16(result_dtype)
A:jax._src.numpy.reductions.init_val->_reduction_init_val(a, init_val)
A:jax._src.numpy.reductions.result->_where(normalizer_mask, np.nan, result)
A:jax._src.numpy.reductions.canon_axis->tuple((_canonicalize_axis_allow_named(x, np.ndim(a)) for x in axis))
A:jax._src.numpy.reductions.canon_pos_axis->tuple((x for x in canon_axis if isinstance(x, int)))
A:jax._src.numpy.reductions.a_dtype->jax._src.dtypes.canonicalize_dtype(dtypes.dtype(a))
A:jax._src.numpy.reductions.a_shape->list(np.shape(a))
A:jax._src.numpy.reductions.normalizer->sum(lax_internal.bitwise_not(lax_internal._isnan(a)), axis=axis, keepdims=keepdims, where=where)
A:jax._src.numpy.reductions.dtype->jax._src.dtypes.canonicalize_dtype(dtypes.int_)
A:jax._src.numpy.reductions.(a,)->_promote_dtypes_inexact(a)
A:jax._src.numpy.reductions.avg->mean(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.weights_sum->_broadcast_to(weights_sum, avg.shape)
A:jax._src.numpy.reductions.(a, weights)->_promote_dtypes_inexact(a, weights)
A:jax._src.numpy.reductions.a_ndim->len(a_shape)
A:jax._src.numpy.reductions.weights_shape->numpy.shape(weights)
A:jax._src.numpy.reductions.weights->_moveaxis(weights, -1, axis)
A:jax._src.numpy.reductions.(computation_dtype, dtype)->_var_promote_types(dtypes.dtype(a), dtype)
A:jax._src.numpy.reductions.a_mean->nanmean(a, axis, dtype=computation_dtype, keepdims=True, where=where)
A:jax._src.numpy.reductions.centered->jax.lax.square(centered)
A:jax._src.numpy.reductions.out->jax.lax.div(result, lax.convert_element_type(divisor, result.dtype))
A:jax._src.numpy.reductions.x->amax(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.y->amin(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.reductions.nansum.__doc__->nansum.__doc__.replace('\n\n\n', '\n\n')
A:jax._src.numpy.reductions.nan_mask->jax._src.lax.lax.bitwise_not(lax_internal._isnan(a))
A:jax._src.numpy.reductions.td->jax.lax.div(nansum(a, axis, dtype=dtype, keepdims=keepdims, where=where), normalizer)
A:jax._src.numpy.reductions.normalizer_mask->jax.lax.le(normalizer, 0)
A:jax._src.numpy.reductions.divisor->_where(normalizer_mask, 1, normalizer)
A:jax._src.numpy.reductions.num_dims->len(a_shape)
A:jax._src.numpy.reductions.cumsum->_make_cumulative_reduction(np.cumsum, lax.cumsum, fill_nan=False)
A:jax._src.numpy.reductions.cumprod->_make_cumulative_reduction(np.cumprod, lax.cumprod, fill_nan=False)
A:jax._src.numpy.reductions.nancumsum->_make_cumulative_reduction(np.nancumsum, lax.cumsum, fill_nan=True, fill_value=0)
A:jax._src.numpy.reductions.nancumprod->_make_cumulative_reduction(np.nancumprod, lax.cumprod, fill_nan=True, fill_value=1)
jax._src.numpy.reductions._asarray(a)
jax._src.numpy.reductions._average(a,axis:Optional[Union[int,Tuple[int,...]]]=None,weights=None,returned=False,keepdims=False)
jax._src.numpy.reductions._axis_size(a,axis)
jax._src.numpy.reductions._canonicalize_axis_allow_named(x,rank)
jax._src.numpy.reductions._cast_to_bool(operand)
jax._src.numpy.reductions._cast_to_numeric(operand)
jax._src.numpy.reductions._ensure_optional_axes(x)
jax._src.numpy.reductions._isscalar(element)
jax._src.numpy.reductions._make_cumulative_reduction(np_reduction,reduction,fill_nan=False,fill_value=0)
jax._src.numpy.reductions._mean(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=False,*,where=None)
jax._src.numpy.reductions._moveaxis(a,source:int,destination:int)
jax._src.numpy.reductions._nan_reduction(a,name,jnp_reduction,init_val,nan_if_all_nan,axis=None,keepdims=None,**kwargs)
jax._src.numpy.reductions._ptp(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=False)
jax._src.numpy.reductions._reduce_all(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,*,where=None)
jax._src.numpy.reductions._reduce_any(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,*,where=None)
jax._src.numpy.reductions._reduce_max(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions._reduce_min(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions._reduce_prod(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None,promote_integers=True)
jax._src.numpy.reductions._reduce_sum(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None,promote_integers=True)
jax._src.numpy.reductions._reduction(a,name,np_fun,op,init_val,has_identity=True,preproc=None,bool_op=None,upcast_f16_for_computation=False,axis=None,dtype=None,out=None,keepdims=False,initial=None,where_=None,parallel_reduce=None,promote_integers=False)
jax._src.numpy.reductions._reduction_dims(a,axis)
jax._src.numpy.reductions._reduction_init_val(a,init_val)
jax._src.numpy.reductions._std(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)
jax._src.numpy.reductions._upcast_f16(dtype)
jax._src.numpy.reductions._var(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)
jax._src.numpy.reductions._var_promote_types(a_dtype,dtype)
jax._src.numpy.reductions.all(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,*,where=None)
jax._src.numpy.reductions.any(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,*,where=None)
jax._src.numpy.reductions.average(a,axis:Optional[Union[int,Tuple[int,...]]]=None,weights=None,returned=False,keepdims=False)
jax._src.numpy.reductions.count_nonzero(a,axis:Optional[Union[int,Tuple[int,...]]]=None,keepdims=False)
jax._src.numpy.reductions.max(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.mean(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=False,*,where=None)
jax._src.numpy.reductions.min(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.nanmax(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.nanmean(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=False,where=None)
jax._src.numpy.reductions.nanmin(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.nanprod(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.nanstd(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,where=None)
jax._src.numpy.reductions.nansum(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None)
jax._src.numpy.reductions.nanvar(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,where=None)
jax._src.numpy.reductions.prod(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None,promote_integers=True)
jax._src.numpy.reductions.ptp(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,keepdims=False)
jax._src.numpy.reductions.std(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)
jax._src.numpy.reductions.sum(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,keepdims=None,initial=None,where=None,promote_integers=True)
jax._src.numpy.reductions.var(a,axis:Optional[Union[int,Tuple[int,...]]]=None,dtype=None,out=None,ddof=0,keepdims=False,*,where=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py----------------------------------------
A:jax._src.numpy.lax_numpy._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.numpy.lax_numpy.meta->_ScalarMeta(np_scalar_type.__name__, (object,), {'dtype': np.dtype(np_scalar_type)})
A:jax._src.numpy.lax_numpy.bool_->_make_scalar_type(np.bool_)
A:jax._src.numpy.lax_numpy.uint8->_make_scalar_type(np.uint8)
A:jax._src.numpy.lax_numpy.uint16->_make_scalar_type(np.uint16)
A:jax._src.numpy.lax_numpy.uint32->_make_scalar_type(np.uint32)
A:jax._src.numpy.lax_numpy.uint64->_make_scalar_type(np.uint64)
A:jax._src.numpy.lax_numpy.int8->_make_scalar_type(np.int8)
A:jax._src.numpy.lax_numpy.int16->_make_scalar_type(np.int16)
A:jax._src.numpy.lax_numpy.int32->_make_scalar_type(np.int32)
A:jax._src.numpy.lax_numpy.int64->_make_scalar_type(np.int64)
A:jax._src.numpy.lax_numpy.bfloat16->_make_scalar_type(dtypes.bfloat16)
A:jax._src.numpy.lax_numpy.float16->_make_scalar_type(np.float16)
A:jax._src.numpy.lax_numpy.float32single->_make_scalar_type(np.float32)
A:jax._src.numpy.lax_numpy.float64double->_make_scalar_type(np.float64)
A:jax._src.numpy.lax_numpy.complex64csingle->_make_scalar_type(np.complex64)
A:jax._src.numpy.lax_numpy.complex128cdouble->_make_scalar_type(np.complex128)
A:jax._src.numpy.lax_numpy.dtype->jax._src.dtypes.canonicalize_dtype(float_)
A:jax._src.numpy.lax_numpy.val_dtype->jax._src.dtypes.canonicalize_dtype(val.dtype)
A:jax._src.numpy.lax_numpy.min_val->_lax_const(val, _max(iinfo(dtype).min, iinfo(val_dtype).min))
A:jax._src.numpy.lax_numpy.max_val->_lax_const(val, _min(iinfo(dtype).max, iinfo(val_dtype).max))
A:jax._src.numpy.lax_numpy.out->round(number, decimals=ndigits or 0)
A:jax._src.numpy.lax_numpy.element->element.__jax_array__().__jax_array__()
A:jax._src.numpy.lax_numpy.(y,)->_promote_dtypes_inexact(y)
A:jax._src.numpy.lax_numpy.(y, x)->_promote_dtypes_inexact(y, x)
A:jax._src.numpy.lax_numpy.dx->list(varargs)
A:jax._src.numpy.lax_numpy.y->atleast_2d(y)
A:jax._src.numpy.lax_numpy.(x, y)->_promote_dtypes_inexact(x, y)
A:jax._src.numpy.lax_numpy.out_order->slice(None, None, -1)
A:jax._src.numpy.lax_numpy.result->reshape(result, keepdim)
A:jax._src.numpy.lax_numpy.a->jax.lax.sort(a, dimension=axis)
A:jax._src.numpy.lax_numpy.bins->jax.core.concrete_or_error(operator.index, bins, 'bins argument of histogram_bin_edges')
A:jax._src.numpy.lax_numpy.range->asarray(range, dtype=dtype)
A:jax._src.numpy.lax_numpy.weights->numpy.array(1, dtype=int_)
A:jax._src.numpy.lax_numpy.(a, weights)->map(ravel, _promote_dtypes_inexact(a, weights))
A:jax._src.numpy.lax_numpy.bin_edges->histogram_bin_edges(sample[:, i], bins[i], range_i, weights)
A:jax._src.numpy.lax_numpy.bin_idx->where(sample[:, i] == bin_edges[-1], bin_idx - 1, bin_idx)
A:jax._src.numpy.lax_numpy.bin_widths->diff(bin_edges)
A:jax._src.numpy.lax_numpy.N->len(dimensions)
A:jax._src.numpy.lax_numpy.x_edgesy_edges->asarray(bins)
A:jax._src.numpy.lax_numpy.sample->transpose(asarray([x, y]))
A:jax._src.numpy.lax_numpy.(hist, edges)->histogramdd(sample, bins, range, weights, density)
A:jax._src.numpy.lax_numpy.(sample,)->_promote_dtypes_inexact(sample)
A:jax._src.numpy.lax_numpy.(sample, weights)->_promote_dtypes_inexact(sample, weights)
A:jax._src.numpy.lax_numpy.(N, D)->shape(sample)
A:jax._src.numpy.lax_numpy.num_bins->len(bins)
A:jax._src.numpy.lax_numpy.nbins->numpy.empty(D, int)
A:jax._src.numpy.lax_numpy.dedges[i]->diff(bin_edges_by_dim[i])
A:jax._src.numpy.lax_numpy.xy->ravel_multi_index(bin_idx_by_dim, nbins, mode='clip')
A:jax._src.numpy.lax_numpy.hist->hist.astype(sample.dtype).astype(sample.dtype)
A:jax._src.numpy.lax_numpy.axes->tuple((_canonicalize_axis(i, ndim(a)) for i in axes))
A:jax._src.numpy.lax_numpy.ax1->_canonicalize_axis(ax1, ndim(m))
A:jax._src.numpy.lax_numpy.ax2->_canonicalize_axis(ax2, ndim(m))
A:jax._src.numpy.lax_numpy.perm->tuple((names.index(name) for name in result_names))
A:jax._src.numpy.lax_numpy.axis->_canonicalize_axis(axis, ndim(a))
A:jax._src.numpy.lax_numpy.i->array(i)
A:jax._src.numpy.lax_numpy.re->jax.lax.convert_element_type(re, dtype)
A:jax._src.numpy.lax_numpy.im->jax.lax.convert_element_type(im, dtype)
A:jax._src.numpy.lax_numpy.n->jax.core.concrete_or_error(operator.index, n, "'n' argument of jnp.diag_indices()")
A:jax._src.numpy.lax_numpy.shape->jax.core.canonicalize_shape(shape, context='shape argument of jnp.fromfunction()')
A:jax._src.numpy.lax_numpy.prepend->broadcast_to(prepend, tuple(shape))
A:jax._src.numpy.lax_numpy.append->broadcast_to(append, tuple(shape))
A:jax._src.numpy.lax_numpy.slice1[axis]->slice(1, None)
A:jax._src.numpy.lax_numpy.slice2[axis]->slice(None, -1)
A:jax._src.numpy.lax_numpy.slice1_tuple->tuple(slice1)
A:jax._src.numpy.lax_numpy.slice2_tuple->tuple(slice2)
A:jax._src.numpy.lax_numpy.ary->asarray(ary)
A:jax._src.numpy.lax_numpy.sliced->jax.lax.squeeze(sliced, removed)
A:jax._src.numpy.lax_numpy.a_grad->concatenate((sliced(1, 2) - sliced(0, 1), (sliced(2, None) - sliced(None, -2)) * 0.5, sliced(-1, None) - sliced(-2, -1)), axis)
A:jax._src.numpy.lax_numpy.axis_tuple->tuple((_canonicalize_axis(i, a.ndim) for i in axis))
A:jax._src.numpy.lax_numpy.len_axes->len(axis_tuple)
A:jax._src.numpy.lax_numpy.newshape->_compute_newshape(a, args[0] if len(args) == 1 else args)
A:jax._src.numpy.lax_numpy.dims->list(range(ndim(a)))
A:jax._src.numpy.lax_numpy.strides->(np.cumprod(a.shape[::-1])[::-1] // a.shape).astype(int_)
A:jax._src.numpy.lax_numpy.(indices, out_indices[i])->divmod(indices, s)
A:jax._src.numpy.lax_numpy.new_shape->_ensure_index_tuple(new_shape)
A:jax._src.numpy.lax_numpy.new_size->_prod(new_shape)
A:jax._src.numpy.lax_numpy.repeats->broadcast_to(repeats, [a.shape[axis]])
A:jax._src.numpy.lax_numpy.a_shape->shape(a)
A:jax._src.numpy.lax_numpy.source->tuple((_canonicalize_axis(i, ndim(a)) for i in source))
A:jax._src.numpy.lax_numpy.destination->tuple((_canonicalize_axis(i, ndim(a)) for i in destination))
A:jax._src.numpy.lax_numpy.(a, b)->_promote_dtypes(a, b)
A:jax._src.numpy.lax_numpy.rtol->jax.lax.convert_element_type(rtol, dtype)
A:jax._src.numpy.lax_numpy.atol->jax.lax.convert_element_type(atol, dtype)
A:jax._src.numpy.lax_numpy.a_inf->isinf(a)
A:jax._src.numpy.lax_numpy.b_inf->isinf(b)
A:jax._src.numpy.lax_numpy.any_inf->logical_or(a_inf, b_inf)
A:jax._src.numpy.lax_numpy.both_inf->logical_and(a_inf, b_inf)
A:jax._src.numpy.lax_numpy.same_value->jax.lax.eq(a, b)
A:jax._src.numpy.lax_numpy.same_inf->logical_and(both_inf, same_value)
A:jax._src.numpy.lax_numpy.a_nan->isnan(a)
A:jax._src.numpy.lax_numpy.b_nan->isnan(b)
A:jax._src.numpy.lax_numpy.any_nan->logical_or(a_nan, b_nan)
A:jax._src.numpy.lax_numpy.both_nan->logical_and(a_nan, b_nan)
A:jax._src.numpy.lax_numpy.(x, xp)->_promote_dtypes_inexact(x, xp)
A:jax._src.numpy.lax_numpy.(fp,)->_promote_dtypes_inexact(fp)
A:jax._src.numpy.lax_numpy.period->abs(period)
A:jax._src.numpy.lax_numpy.(xp, fp)->jax.lax.sort_key_val(xp, fp)
A:jax._src.numpy.lax_numpy.xp->concatenate([xp[-1:] - period, xp, xp[:1] + period])
A:jax._src.numpy.lax_numpy.fp->concatenate([fp[-1:], fp, fp[:1]])
A:jax._src.numpy.lax_numpy.epsilon->numpy.spacing(np.finfo(xp.dtype).eps)
A:jax._src.numpy.lax_numpy.f->where(x > xp[-1], fp[-1] if right is None else right, f)
A:jax._src.numpy.lax_numpy.choices->_promote_dtypes(default, *choicelist)
A:jax._src.numpy.lax_numpy.output->where(cond, choice, output)
A:jax._src.numpy.lax_numpy.minlength->jax.core.concrete_or_error(operator.index, minlength, "The error occurred because of argument 'minlength' of jnp.bincount.")
A:jax._src.numpy.lax_numpy.x->remainder(x, a_shape[i] or 1)
A:jax._src.numpy.lax_numpy.length->jax.core.concrete_or_error(operator.index, length, "The error occurred because of argument 'length' of jnp.bincount.")
A:jax._src.numpy.lax_numpy.broadcast_arrays->_wraps(np.broadcast_arrays, lax_description='The JAX version does not necessarily return a view of the input.\n')(_broadcast_arrays)
A:jax._src.numpy.lax_numpy.broadcast_to->_wraps(np.broadcast_to, lax_description='The JAX version does not necessarily return a view of the input.\n')(_broadcast_to)
A:jax._src.numpy.lax_numpy.indices_or_sections->jax.core.concrete_or_error(np.int64, indices_or_sections, f'in jax.numpy.{op} argument 1')
A:jax._src.numpy.lax_numpy.split_indices->numpy.concatenate([np.arange(r + 1, dtype=np.int64) * (part_size + 1), np.arange(indices_or_sections - r, dtype=np.int64) * part_size + ((r + 1) * (part_size + 1) - 1)])
A:jax._src.numpy.lax_numpy.(part_size, r)->_divmod(size, indices_or_sections)
A:jax._src.numpy.lax_numpy.vsplit->_split_on_axis('vsplit', axis=0)
A:jax._src.numpy.lax_numpy.hsplit->_split_on_axis('hsplit', axis=1)
A:jax._src.numpy.lax_numpy.dsplit->_split_on_axis('dsplit', axis=2)
A:jax._src.numpy.lax_numpy.decimals->jax.core.concrete_or_error(operator.index, decimals, "'decimals' argument of jnp.round")
A:jax._src.numpy.lax_numpy.factor->_lax_const(x, 10 ** decimals)
A:jax._src.numpy.lax_numpy.zero->_lax_const(x, 0)
A:jax._src.numpy.lax_numpy.info->finfo(dtypes.canonicalize_dtype(dtype))
A:jax._src.numpy.lax_numpy.size->jax.core.concrete_or_error(operator.index, size, 'The size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.')
A:jax._src.numpy.lax_numpy.flat_indices->cumsum(bincount(cumsum(mask), length=size))
A:jax._src.numpy.lax_numpy.p->asarray(p)
A:jax._src.numpy.lax_numpy.dd->diff(p, axis=axis)
A:jax._src.numpy.lax_numpy.ddmod->where((ddmod == -interval) & (dd > 0), interval, ddmod)
A:jax._src.numpy.lax_numpy.ph_correct->where(abs(dd) < discont, 0, ddmod - dd)
A:jax._src.numpy.lax_numpy.up->concatenate((lax.slice_in_dim(p, 0, 1, axis=axis), lax.slice_in_dim(p, 1, None, axis=axis) + cumsum(ph_correct, axis=axis)), axis=axis)
A:jax._src.numpy.lax_numpy.nd->ndim(a)
A:jax._src.numpy.lax_numpy.constant_values->kwargs.get('constant_values', 0)
A:jax._src.numpy.lax_numpy.array->asarray(array)
A:jax._src.numpy.lax_numpy.(repeats, (left_remainder, right_remainder))->_divmod(pad_width[i], size)
A:jax._src.numpy.lax_numpy.edge->jax.lax.slice_in_dim(x, -1, None, axis=i)
A:jax._src.numpy.lax_numpy.curr_pad->_min(padding, n - offset)
A:jax._src.numpy.lax_numpy.edge_before->jax.lax.slice_in_dim(array, 0, 1, axis=axis)
A:jax._src.numpy.lax_numpy.pad_before->empty_like(array, shape=shape_before)
A:jax._src.numpy.lax_numpy.edge_after->jax.lax.slice_in_dim(array, -1, None, axis=axis)
A:jax._src.numpy.lax_numpy.pad_after->empty_like(array, shape=shape_after)
A:jax._src.numpy.lax_numpy.ramp_before->jax._src.lax.lax._convert_element_type(ramp_before, weak_type=dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.ramp_after->flip(ramp_after, axis)
A:jax._src.numpy.lax_numpy.stat_before->jax._src.lax.lax._convert_element_type(stat_before, array.dtype, dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.length_before->_min(length_before, array_length)
A:jax._src.numpy.lax_numpy.length_after->_min(length_after, array_length)
A:jax._src.numpy.lax_numpy.slice_before->jax.lax.slice_in_dim(array, 0, length_before, axis=i)
A:jax._src.numpy.lax_numpy.slice_after->jax.lax.slice_in_dim(array, -length_after, None, axis=i)
A:jax._src.numpy.lax_numpy.stat_after->jax._src.lax.lax._convert_element_type(stat_after, array.dtype, dtypes.is_weakly_typed(array))
A:jax._src.numpy.lax_numpy.pad_width->_broadcast_to_pairs(pad_width, ndim(array), 'pad_width')
A:jax._src.numpy.lax_numpy.padded->apply_along_axis(func, axis, padded, pad_width[axis], axis, kwargs)
A:jax._src.numpy.lax_numpy.nvals->numpy.asarray(tree_map(lambda x: core.concrete_or_error(np.array, x, context=f'{name} argument of jnp.pad'), nvals))
A:jax._src.numpy.lax_numpy.end_values->kwargs.get('end_values', 0)
A:jax._src.numpy.lax_numpy.stat_length->kwargs.get('stat_length', None)
A:jax._src.numpy.lax_numpy.reflect_type->kwargs.get('reflect_type', 'even')
A:jax._src.numpy.lax_numpy.shape0->shape(arrays[0])
A:jax._src.numpy.lax_numpy.reps->tuple((operator.index(rep) if core.is_constant_dim(rep) else rep for rep in reps))
A:jax._src.numpy.lax_numpy.arr->arr.astype(uint8).astype(uint8)
A:jax._src.numpy.lax_numpy.arrays->_promote_dtypes(*arrays)
A:jax._src.numpy.lax_numpy.arrs->jax.vmap(atleast_3d)(tup)
A:jax._src.numpy.lax_numpy.(a, *choices)->broadcast_arrays(a, *choices)
A:jax._src.numpy.lax_numpy.m->ndim(x)
A:jax._src.numpy.lax_numpy.(xs, depths)->unzip2([_block(x) for x in xs])
A:jax._src.numpy.lax_numpy.rank->ndim(arr)
A:jax._src.numpy.lax_numpy.(out, _)->_block(arrays)
A:jax._src.numpy.lax_numpy._->jax._src.dtypes.coerce_to_array(object, dtype)
A:jax._src.numpy.lax_numpy.object->tree_map(lambda leaf: leaf.__jax_array__() if hasattr(leaf, '__jax_array__') else leaf, object)
A:jax._src.numpy.lax_numpy.leaves->tree_leaves(object)
A:jax._src.numpy.lax_numpy.view->memoryview(object)
A:jax._src.numpy.lax_numpy.out_array->jax.lax.expand_dims(out_array, range(ndmin - ndim(out_array)))
A:jax._src.numpy.lax_numpy.eq->equal(a1, a2)
A:jax._src.numpy.lax_numpy.function->jax.vmap(function, in_axes=tuple(in_axes[::-1]))
A:jax._src.numpy.lax_numpy.k->operator.index(k)
A:jax._src.numpy.lax_numpy.require->partial(core.concrete_or_error, None)
A:jax._src.numpy.lax_numpy.start_dtype->_dtype(start)
A:jax._src.numpy.lax_numpy.start->jax.core.concrete_or_error(operator.index, start, "'start' argument of jnp.rollaxis()")
A:jax._src.numpy.lax_numpy.stop->asarray(stop, dtype=computation_dtype)
A:jax._src.numpy.lax_numpy.num->jax.core.concrete_or_error(operator.index, num, "'num' argument of jnp.geomspace")
A:jax._src.numpy.lax_numpy.computation_dtype->jax._src.dtypes.to_inexact_dtype(dtype)
A:jax._src.numpy.lax_numpy.bounds_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax._src.numpy.lax_numpy.broadcast_start->broadcast_to(start, bounds_shape)
A:jax._src.numpy.lax_numpy.broadcast_stop->broadcast_to(stop, bounds_shape)
A:jax._src.numpy.lax_numpy.step->step.astype(computation_dtype).astype(computation_dtype)
A:jax._src.numpy.lax_numpy.empty_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax._src.numpy.lax_numpy.lin->linspace(start, stop, num, endpoint=endpoint, retstep=False, dtype=None, axis=axis)
A:jax._src.numpy.lax_numpy.signflip->signflip.astype(computation_dtype).astype(computation_dtype)
A:jax._src.numpy.lax_numpy.res->argmin(a, axis=axis, keepdims=keepdims)
A:jax._src.numpy.lax_numpy.(x,)->_promote_args_inexact('i0', x)
A:jax._src.numpy.lax_numpy.dimensions->list(range(nd))
A:jax._src.numpy.lax_numpy.idx->jax.lax.iota(working_dtype, len(x))
A:jax._src.numpy.lax_numpy.result_shape->list(a.shape)
A:jax._src.numpy.lax_numpy.total_repeat_length->numpy.sum(repeats)
A:jax._src.numpy.lax_numpy.exclusive_repeats->roll(repeats, shift=1).at[0].set(0)
A:jax._src.numpy.lax_numpy.scatter_indices->cumsum(exclusive_repeats)
A:jax._src.numpy.lax_numpy.block_split_indicators->block_split_indicators.at[scatter_indices].add(1).at[scatter_indices].add(1)
A:jax._src.numpy.lax_numpy.m_shape->shape(m)
A:jax._src.numpy.lax_numpy.mask->numpy.ones(arr.shape[axis], dtype=bool)
A:jax._src.numpy.lax_numpy.default_int->jax._src.dtypes.canonicalize_dtype(np.int_)
A:jax._src.numpy.lax_numpy.tril_indices->_wrap_indices_function(np.tril_indices)
A:jax._src.numpy.lax_numpy.triu_indices->_wrap_indices_function(np.triu_indices)
A:jax._src.numpy.lax_numpy.mask_indices->_wrap_indices_function(np.mask_indices)
A:jax._src.numpy.lax_numpy.ndim->len(shape)
A:jax._src.numpy.lax_numpy.offset->jax.core.concrete_or_error(operator.index, offset, "'offset' argument of jnp.diagonal()")
A:jax._src.numpy.lax_numpy.diag_size->_max(0, _min(a_shape[axis1] + _min(offset, 0), a_shape[axis2] - _max(offset, 0)))
A:jax._src.numpy.lax_numpy.j->arange(_abs(offset), _abs(offset) + diag_size)
A:jax._src.numpy.lax_numpy.v_shape->shape(v)
A:jax._src.numpy.lax_numpy.v->ravel(v)
A:jax._src.numpy.lax_numpy.v_length->len(v)
A:jax._src.numpy.lax_numpy.filt->jax.core.concrete_or_error(asarray, filt, 'Error arose in the `filt` argument of trim_zeros_tol()')
A:jax._src.numpy.lax_numpy.obj->jax.core.concrete_or_error(np.asarray, obj, "'obj' array argument of jnp.delete()")
A:jax._src.numpy.lax_numpy.indices->argmax(cumsum(concatenate([zeros_like(condlist[:1]), condlist], 0), 0), 0)
A:jax._src.numpy.lax_numpy.values->moveaxis(values, 0, axis)
A:jax._src.numpy.lax_numpy.out_shape->jax.lax.broadcast_shapes(idx_shape, arr_shape)
A:jax._src.numpy.lax_numpy.values_ind->argmax(cumsum(concatenate([zeros_like(condlist[:1]), condlist], 0), 0), 0).at[argsort(indices)].add(arange(n_insert, dtype=indices.dtype))
A:jax._src.numpy.lax_numpy.arr_mask->ones(n_input + n_insert, dtype=bool).at[values_ind].set(False)
A:jax._src.numpy.lax_numpy.num_dims->ndim(arr)
A:jax._src.numpy.lax_numpy.func->jax.vmap(func, in_axes=0, out_axes=0)
A:jax._src.numpy.lax_numpy.b->expand_dims(b, range(ndim(a) - ndim(b)))
A:jax._src.numpy.lax_numpy.num_batch_dims->_max(len(a_batch_dims), len(b_batch_dims))
A:jax._src.numpy.lax_numpy.a_ndim->ndim(a)
A:jax._src.numpy.lax_numpy.b_ndim->ndim(b)
A:jax._src.numpy.lax_numpy.ty->next(iter(non_constant_dim_types))
A:jax._src.numpy.lax_numpy.contract_path->_poly_einsum_handlers.get(ty, _default_poly_einsum_handler)
A:jax._src.numpy.lax_numpy.(operands, contractions)->contract_path(*operands, einsum_call=True, use_blas=True, optimize=optimize)
A:jax._src.numpy.lax_numpy.contractions->tuple(((a, frozenset(b), c) for (a, b, c, *_) in contractions))
A:jax._src.numpy.lax_numpy.dummy->collections.namedtuple('dummy', ['shape', 'dtype'])
A:jax._src.numpy.lax_numpy.(out_dummies, contractions)->opt_einsum.contract_path(*dummies, **kwargs)
A:jax._src.numpy.lax_numpy.operands->list(_promote_dtypes(*operands))
A:jax._src.numpy.lax_numpy.operand->jax.lax.transpose(operand, perm)
A:jax._src.numpy.lax_numpy.names->names.replace(name, '', count - 1).replace(name, '', count - 1)
A:jax._src.numpy.lax_numpy.eye->jax._src.lax.lax._delta(operand.dtype, operand.shape, axes)
A:jax._src.numpy.lax_numpy.(sqez_axes, keep_axes)->partition_list(keep, list(range(operand.ndim)))
A:jax._src.numpy.lax_numpy.contracted_names->sorted(contracted_names_set)
A:jax._src.numpy.lax_numpy.(input_str, result_names)->einstr.split('->')
A:jax._src.numpy.lax_numpy.input_names->input_str.split(',')
A:jax._src.numpy.lax_numpy.counts->jax.lax.expand_dims(counts, tuple(range(q_ndim)))
A:jax._src.numpy.lax_numpy.(operand, names)->sum_repeats(operand, names, counts, result_names)
A:jax._src.numpy.lax_numpy.(lhs, rhs)->map(operands.pop, operand_indices)
A:jax._src.numpy.lax_numpy.(lhs, lhs_names)->sum_repeats(lhs, lhs_names, lhs_counts, result_names + rhs_names)
A:jax._src.numpy.lax_numpy.(rhs, rhs_names)->sum_repeats(rhs, rhs_names, rhs_counts, result_names + lhs_names)
A:jax._src.numpy.lax_numpy.lhs_counts->collections.Counter(lhs_names)
A:jax._src.numpy.lax_numpy.rhs_counts->collections.Counter(rhs_names)
A:jax._src.numpy.lax_numpy.(lhs_batch, rhs_batch)->unzip2(((lhs_names.find(n), rhs_names.find(n)) for n in batch_names))
A:jax._src.numpy.lax_numpy.batch_names_str->''.join(batch_names)
A:jax._src.numpy.lax_numpy.(lhs_cont, rhs_cont)->unzip2(((lhs_names.index(n), rhs_names.index(n)) for n in contracted_names))
A:jax._src.numpy.lax_numpy.remaining_lhs_names->_removechars(lhs_names, deleted_names)
A:jax._src.numpy.lax_numpy.remaining_rhs_names->_removechars(rhs_names, deleted_names)
A:jax._src.numpy.lax_numpy.c->jax.lax.complex(real_part, complex_part)
A:jax._src.numpy.lax_numpy.a_reshaped->expand_dims(a, range(1, 2 * ndim(a), 2))
A:jax._src.numpy.lax_numpy.b_reshaped->expand_dims(b, range(0, 2 * ndim(b), 2))
A:jax._src.numpy.lax_numpy.iota->jax.lax.broadcasted_iota(index_dtype, gather_index_shape, j)
A:jax._src.numpy.lax_numpy.nan_mask->isnan(a)
A:jax._src.numpy.lax_numpy.keys->tuple(keys)
A:jax._src.numpy.lax_numpy.axis_num->_canonicalize_axis(axis, ndim(a))
A:jax._src.numpy.lax_numpy.(_, perm)->jax.lax.sort_key_val(a, iota, dimension=axis_num)
A:jax._src.numpy.lax_numpy.shift->asarray(shift)
A:jax._src.numpy.lax_numpy.b_shape->jax.lax.broadcast_shapes(shift.shape, axis.shape, (1,))
A:jax._src.numpy.lax_numpy.bits->expand_dims(bits, tuple(range(a.ndim - 1)))
A:jax._src.numpy.lax_numpy.packed->(a << bits).sum(-1).astype('uint8')
A:jax._src.numpy.lax_numpy.unpacked->(a[..., None] & expand_dims(bits, tuple(range(a.ndim))) > 0).astype('uint8')
A:jax._src.numpy.lax_numpy.axis_idx->_canonicalize_axis(axis, ndim(a))
A:jax._src.numpy.lax_numpy.index_dims->len(shape(indices))
A:jax._src.numpy.lax_numpy.slice_sizes->list(a_shape)
A:jax._src.numpy.lax_numpy.dnums->jax.lax.GatherDimensionNumbers(offset_dims=tuple(range(q_ndim, len(a_shape) + q_ndim if keepdims else len(a_shape) + q_ndim - 1)), collapsed_slice_dims=() if keepdims else (axis,), start_index_map=(axis,))
A:jax._src.numpy.lax_numpy.axis_size_val->jax.lax.convert_element_type(core.dimension_as_value(axis_size), _dtype(index))
A:jax._src.numpy.lax_numpy.index_dtype->dtype(int64 if use_64bit_index else int32)
A:jax._src.numpy.lax_numpy.lst->list(tup)
A:jax._src.numpy.lax_numpy.use_64bit_index->_any([not core.is_constant_dim(d) or d >= 1 << 31 for d in x_shape])
A:jax._src.numpy.lax_numpy.arr_shape->replace(arr.shape, 1)
A:jax._src.numpy.lax_numpy.gather_indices_arr->jax.lax.concatenate(gather_indices, dimension=j)
A:jax._src.numpy.lax_numpy.aval->jax.core.get_aval(idx)
A:jax._src.numpy.lax_numpy.(treedef, static_idx, dynamic_idx)->_split_index_for_jit(idx, arr.shape)
A:jax._src.numpy.lax_numpy.indexer->_index_to_gather(shape(arr), idx)
A:jax._src.numpy.lax_numpy.fill_value->fill_value.item().item()
A:jax._src.numpy.lax_numpy._Indexer->collections.namedtuple('_Indexer', ['slice_shape', 'gather_slice_shape', 'gather_indices', 'dnums', 'unique_indices', 'indices_are_sorted', 'reversed_y_dims', 'newaxis_dims'])
A:jax._src.numpy.lax_numpy.(leaves, treedef)->tree_flatten(idx)
A:jax._src.numpy.lax_numpy.(advanced_indexes, idx_advanced_axes, x_advanced_axes)->zip(*advanced_pairs)
A:jax._src.numpy.lax_numpy.advanced_axes_are_contiguous->numpy.all(np.diff(idx_advanced_axes) == 1)
A:jax._src.numpy.lax_numpy.advanced_indexes->broadcast_arrays(*advanced_indexes)
A:jax._src.numpy.lax_numpy.start_dim->len(gather_indices_shape)
A:jax._src.numpy.lax_numpy.abstract_i->jax.core.get_aval(i)
A:jax._src.numpy.lax_numpy.(start, limit, stride, needs_rev)->_static_idx(slice(start, stop, step), x_shape[x_axis])
A:jax._src.numpy.lax_numpy.gather_indices_array->jax.lax.concatenate([lax.broadcast_in_dim(g, gather_indices_shape, tuple(range(i, i + g.ndim))) for (g, i) in gather_indices], last_dim)
A:jax._src.numpy.lax_numpy.last_dim->len(gather_indices_shape)
A:jax._src.numpy.lax_numpy.total_dims->_sum((_ndim(e) if _is_boolean_index(e) else 1 for e in idx if e is not None and e is not Ellipsis))
A:jax._src.numpy.lax_numpy.num_ellipsis->_sum((e is Ellipsis for e in idx))
A:jax._src.numpy.lax_numpy.i_shape->_shape(i)
A:jax._src.numpy.lax_numpy.len_without_none->_sum((1 for e in idx if e is not None and e is not Ellipsis))
A:jax._src.numpy.lax_numpy.ellipsis_index->next(ellipses, None)
A:jax._src.numpy.lax_numpy.(start, stop, step)->jax.lax.iota(working_dtype, len(x)).indices(size)
A:jax._src.numpy.lax_numpy.blackman->_wrap_numpy_nullary_function(np.blackman)
A:jax._src.numpy.lax_numpy.bartlett->_wrap_numpy_nullary_function(np.bartlett)
A:jax._src.numpy.lax_numpy.hamming->_wrap_numpy_nullary_function(np.hamming)
A:jax._src.numpy.lax_numpy.hanning->_wrap_numpy_nullary_function(np.hanning)
A:jax._src.numpy.lax_numpy.kaiser->_wrap_numpy_nullary_function(np.kaiser)
A:jax._src.numpy.lax_numpy.(x1, x2)->_promote_dtypes(x1, x2)
A:jax._src.numpy.lax_numpy.(gcd, _)->jax.lax.while_loop(_gcd_cond_fn, _gcd_body_fn, (abs(x1), abs(x2)))
A:jax._src.numpy.lax_numpy.d->diag(c)
A:jax._src.numpy.lax_numpy.condition->asarray(condition).astype(bool)
A:jax._src.numpy.lax_numpy.(m, y)->_promote_args_inexact('cov', m, y)
A:jax._src.numpy.lax_numpy.(m,)->_promote_args_inexact('cov', m)
A:jax._src.numpy.lax_numpy.X->concatenate((X, y), axis=0)
A:jax._src.numpy.lax_numpy.w->asarray(abs(fweights))
A:jax._src.numpy.lax_numpy.aweights->abs(aweights)
A:jax._src.numpy.lax_numpy.(avg, w_sum)->average(X, axis=1, weights=w, returned=True)
A:jax._src.numpy.lax_numpy.stddev->sqrt(real(d)).astype(c.dtype)
A:jax._src.numpy.lax_numpy.real_part->clip(real(c), -1, 1)
A:jax._src.numpy.lax_numpy.complex_part->clip(imag(c), -1, 1)
A:jax._src.numpy.lax_numpy.(a,)->_promote_dtypes_inexact(a)
A:jax._src.numpy.lax_numpy.keepdim->tuple(keepdim)
A:jax._src.numpy.lax_numpy.do_not_touch_shape->tuple((x for (idx, x) in enumerate(shape(a)) if idx not in axis))
A:jax._src.numpy.lax_numpy.touch_shape->tuple((x for (idx, x) in enumerate(shape(a)) if idx in axis))
A:jax._src.numpy.lax_numpy.q_shape->shape(q)
A:jax._src.numpy.lax_numpy.q_ndim->ndim(q)
A:jax._src.numpy.lax_numpy.q->true_divide(q, float32(100.0))
A:jax._src.numpy.lax_numpy.low->jax.lax.convert_element_type(low, int64)
A:jax._src.numpy.lax_numpy.high->jax.lax.convert_element_type(high, int64)
A:jax._src.numpy.lax_numpy.high_weight->jax.lax.broadcast_in_dim(high_weight, high_value.shape, broadcast_dimensions=(0,))
A:jax._src.numpy.lax_numpy.low_weight->jax.lax.broadcast_in_dim(low_weight, low_value.shape, broadcast_dimensions=(0,))
A:jax._src.numpy.lax_numpy.low_value->jax.lax.gather(a, low[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax._src.numpy.lax_numpy.high_value->jax.lax.gather(a, high[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax._src.numpy.lax_numpy.pred->jax.lax.le(high_weight, _lax_const(high_weight, 0.5))
A:jax._src.numpy.lax_numpy.go_left->op(query, sorted_arr[mid])
A:jax._src.numpy.lax_numpy.n_levels->int(np.ceil(np.log2(len(sorted_arr) + 1)))
A:jax._src.numpy.lax_numpy.query_flat->query.ravel()
A:jax._src.numpy.lax_numpy.(a, v)->_promote_dtypes(a, v)
A:jax._src.numpy.lax_numpy.right->jax.core.concrete_or_error(bool, right, 'right argument of jnp.digitize()')
A:jax._src.numpy.lax_numpy.condlist->array(condlist, dtype=bool_)
A:jax._src.numpy.lax_numpy.funcs->dict(funcs)
A:jax._src.numpy.lax_numpy.(q,)->_promote_dtypes_inexact(q)
A:jax._src.numpy.lax_numpy.arr_dtype->_dtype(arr)
A:jax._src.numpy.lax_numpy.arr_bytes->arr_bytes.reshape(arr_bytes.shape[:-2] + (-1,)).reshape(arr_bytes.shape[:-2] + (-1,))
A:jax._src.numpy.lax_numpy.shifts->jax.lax.expand_dims(arange(0, nbits_in, nbits_out, dtype=dt_in), tuple(range(arr_bytes.ndim)))
A:jax._src.numpy.lax_numpy.other->other.__jax_array__().__jax_array__()
A:jax._src.numpy.lax_numpy.argpartition->_not_implemented(np.argpartition)
A:jax._src.numpy.lax_numpy.(num_chunks, tail)->divmod(x.shape[0], size)
jax._src.numpy.lax_numpy._IndexUpdateHelper(self,array)
jax._src.numpy.lax_numpy._IndexUpdateHelper.__getitem__(self,index)
jax._src.numpy.lax_numpy._IndexUpdateHelper.__init__(self,array)
jax._src.numpy.lax_numpy._IndexUpdateHelper.__repr__(self)
jax._src.numpy.lax_numpy._IndexUpdateRef(self,array,index)
jax._src.numpy.lax_numpy._IndexUpdateRef.__init__(self,array,index)
jax._src.numpy.lax_numpy._IndexUpdateRef.__repr__(self)
jax._src.numpy.lax_numpy._IndexUpdateRef.add(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.apply(self,func,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.divide(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.get(self,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.max(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.min(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.multiply(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.power(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._IndexUpdateRef.set(self,values,indices_are_sorted=False,unique_indices=False,mode=None)
jax._src.numpy.lax_numpy._ScalarMeta(self,x)
jax._src.numpy.lax_numpy._ScalarMeta.__call__(self,x)
jax._src.numpy.lax_numpy._ScalarMeta.__eq__(self,other)
jax._src.numpy.lax_numpy._ScalarMeta.__hash__(self)
jax._src.numpy.lax_numpy._ScalarMeta.__instancecheck__(self,instance)
jax._src.numpy.lax_numpy._ScalarMeta.__ne__(self,other)
jax._src.numpy.lax_numpy.__array_module__(self,types)
jax._src.numpy.lax_numpy._argmax(a,axis:Optional[int]=None,out=None,keepdims=False)
jax._src.numpy.lax_numpy._argmin(a,axis:Optional[int]=None,out=None,keepdims=False)
jax._src.numpy.lax_numpy._astype(arr,dtype)
jax._src.numpy.lax_numpy._atleast_nd(x,n)
jax._src.numpy.lax_numpy._block(xs)
jax._src.numpy.lax_numpy._broadcast_to_pairs(nvals,nd,name)
jax._src.numpy.lax_numpy._canonicalize_tuple_index(arr_ndim,idx,array_name='array')
jax._src.numpy.lax_numpy._check_no_padding(axis_padding,mode)
jax._src.numpy.lax_numpy._chunk_iter(x,size)
jax._src.numpy.lax_numpy._clip(number,min=None,max=None,out=None)
jax._src.numpy.lax_numpy._compress_method(a,condition,axis=None,out=None)
jax._src.numpy.lax_numpy._compute_newshape(a,newshape)
jax._src.numpy.lax_numpy._concatenate_array(arr,axis:Optional[int],dtype=None)
jax._src.numpy.lax_numpy._conv(x,y,mode,op,precision)
jax._src.numpy.lax_numpy._convert_and_clip_integer(val,dtype)
jax._src.numpy.lax_numpy._convert_to_array_if_dtype_fails(x)
jax._src.numpy.lax_numpy._copy(self)
jax._src.numpy.lax_numpy._deepcopy(self,memo)
jax._src.numpy.lax_numpy._default_poly_einsum_handler(*operands,**kwargs)
jax._src.numpy.lax_numpy._defer_to_unrecognized_arg(opchar,binary_op,swap=False)
jax._src.numpy.lax_numpy._diag(v,k)
jax._src.numpy.lax_numpy._einsum(operands:Sequence,contractions:Sequence[Tuple[Tuple[int,...],FrozenSet[str],str]],precision)
jax._src.numpy.lax_numpy._eliminate_deprecated_list_indexing(idx)
jax._src.numpy.lax_numpy._expand_bool_indices(idx,shape)
jax._src.numpy.lax_numpy._flip(m,axis:Optional[Union[int,Tuple[int,...]]]=None)
jax._src.numpy.lax_numpy._gather(arr,treedef,static_idx,dynamic_idx,indices_are_sorted,unique_indices,mode,fill_value)
jax._src.numpy.lax_numpy._gcd_body_fn(xs)
jax._src.numpy.lax_numpy._gcd_cond_fn(xs)
jax._src.numpy.lax_numpy._geomspace(start,stop,num=50,endpoint=True,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy._index_to_gather(x_shape,idx,normalize_indices=True)
jax._src.numpy.lax_numpy._int(aval)
jax._src.numpy.lax_numpy._is_advanced_int_indexer(idx)
jax._src.numpy.lax_numpy._is_boolean_index(i)
jax._src.numpy.lax_numpy._is_int_arraylike(x)
jax._src.numpy.lax_numpy._is_scalar(x)
jax._src.numpy.lax_numpy._is_slice_element_none_or_constant(elt)
jax._src.numpy.lax_numpy._itemsize(arr)
jax._src.numpy.lax_numpy._jnp_dtype(obj,align=False,copy=False)
jax._src.numpy.lax_numpy._linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy._logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy._make_scalar_type(np_scalar_type)
jax._src.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax._src.numpy.lax_numpy._moveaxis(a,source:Tuple[int,...],destination:Tuple[int,...])
jax._src.numpy.lax_numpy._movechars(s,src,dst)
jax._src.numpy.lax_numpy._multi_slice(arr,start_indices:Tuple[Tuple[int,...]],limit_indices:Tuple[Tuple[int,...]],removed_dims:Tuple[Tuple[int,...]])
jax._src.numpy.lax_numpy._nanargmax(a,axis:Optional[int]=None,keepdims:bool=False)
jax._src.numpy.lax_numpy._nanargmin(a,axis:Optional[int]=None,keepdims:bool=False)
jax._src.numpy.lax_numpy._nbytes(arr)
jax._src.numpy.lax_numpy._normalize_index(index,axis_size)
jax._src.numpy.lax_numpy._not_implemented(fun,module=None)
jax._src.numpy.lax_numpy._notimplemented_flat(self)
jax._src.numpy.lax_numpy._operator_round(number,ndigits=None)
jax._src.numpy.lax_numpy._pad(array,pad_width,mode,constant_values,stat_length,end_values,reflect_type)
jax._src.numpy.lax_numpy._pad_constant(array,pad_width,constant_values)
jax._src.numpy.lax_numpy._pad_edge(array,pad_width)
jax._src.numpy.lax_numpy._pad_empty(array,pad_width)
jax._src.numpy.lax_numpy._pad_func(array,pad_width,func,**kwargs)
jax._src.numpy.lax_numpy._pad_linear_ramp(array,pad_width,end_values)
jax._src.numpy.lax_numpy._pad_stats(array,pad_width,stat_length,stat_func)
jax._src.numpy.lax_numpy._pad_symmetric_or_reflect(array,pad_width,mode,reflect_type)
jax._src.numpy.lax_numpy._pad_wrap(array,pad_width)
jax._src.numpy.lax_numpy._piecewise(x,condlist,consts,funcs,*args,**kw)
jax._src.numpy.lax_numpy._quantile(a,q,axis,interpolation,keepdims,squash_nans)
jax._src.numpy.lax_numpy._removechars(s,chars)
jax._src.numpy.lax_numpy._reshape(a,*args,order='C')
jax._src.numpy.lax_numpy._result_dtype(op,*args)
jax._src.numpy.lax_numpy._rewriting_take(arr,idx,indices_are_sorted=False,unique_indices=False,mode=None,fill_value=None)
jax._src.numpy.lax_numpy._roll(a,shift,axis)
jax._src.numpy.lax_numpy._searchsorted_via_scan(sorted_arr,query,side,dtype)
jax._src.numpy.lax_numpy._searchsorted_via_sort(sorted_arr,query,side,dtype)
jax._src.numpy.lax_numpy._set_device_array_attributes(device_array)
jax._src.numpy.lax_numpy._set_device_array_base_attributes(device_array,include=None,exclude=None)
jax._src.numpy.lax_numpy._set_shaped_array_attributes(shaped_array)
jax._src.numpy.lax_numpy._should_unpack_list_index(x)
jax._src.numpy.lax_numpy._split(op,ary,indices_or_sections,axis=0)
jax._src.numpy.lax_numpy._split_index_for_jit(idx,shape)
jax._src.numpy.lax_numpy._split_on_axis(op,axis)
jax._src.numpy.lax_numpy._squeeze(a,axis)
jax._src.numpy.lax_numpy._static_idx(idx:slice,size:core.DimSize)
jax._src.numpy.lax_numpy._take(a,indices,axis:Optional[int]=None,out=None,mode=None,unique_indices=False,indices_are_sorted=False,fill_value=None)
jax._src.numpy.lax_numpy._transpose(a,*args)
jax._src.numpy.lax_numpy._unimplemented_setitem(self,i,x)
jax._src.numpy.lax_numpy._unstack(x)
jax._src.numpy.lax_numpy._view(arr,dtype=None,type=None)
jax._src.numpy.lax_numpy._wrap_indices_function(f)
jax._src.numpy.lax_numpy._wrap_numpy_nullary_function(f)
jax._src.numpy.lax_numpy.allclose(a,b,rtol=1e-05,atol=1e-08,equal_nan=False)
jax._src.numpy.lax_numpy.angle(z,deg=False)
jax._src.numpy.lax_numpy.append(arr,values,axis:Optional[int]=None)
jax._src.numpy.lax_numpy.apply_along_axis(func1d,axis:int,arr,*args,**kwargs)
jax._src.numpy.lax_numpy.apply_over_axes(func,a,axes)
jax._src.numpy.lax_numpy.arange(start:core.DimSize,stop:Optional[core.DimSize]=None,step:Optional[core.DimSize]=None,dtype=None)
jax._src.numpy.lax_numpy.argmax(a,axis:Optional[int]=None,out=None,keepdims=None)
jax._src.numpy.lax_numpy.argmin(a,axis:Optional[int]=None,out=None,keepdims=None)
jax._src.numpy.lax_numpy.argsort(a,axis:Optional[int]=-1,kind='stable',order=None)
jax._src.numpy.lax_numpy.argwhere(a,*,size=None,fill_value=None)
jax._src.numpy.lax_numpy.array(object:Any,dtype:Optional[DTypeLike]=None,copy:bool=True,order:str='K',ndmin:int=0)->Array
jax._src.numpy.lax_numpy.array_equal(a1,a2,equal_nan=False)
jax._src.numpy.lax_numpy.array_equiv(a1,a2)
jax._src.numpy.lax_numpy.array_split(ary,indices_or_sections,axis:int=0)
jax._src.numpy.lax_numpy.asarray(a:Any,dtype:Optional[DTypeLike]=None,order:Any=None)->Array
jax._src.numpy.lax_numpy.atleast_1d(*arys)
jax._src.numpy.lax_numpy.atleast_2d(*arys)
jax._src.numpy.lax_numpy.atleast_3d(*arys)
jax._src.numpy.lax_numpy.bincount(x,weights=None,minlength=0,*,length=None)
jax._src.numpy.lax_numpy.block(arrays)
jax._src.numpy.lax_numpy.broadcast_shapes(*shapes)
jax._src.numpy.lax_numpy.canonicalize_shape(shape:Any,context:str='')->core.Shape
jax._src.numpy.lax_numpy.choose(a,choices,out=None,mode='raise')
jax._src.numpy.lax_numpy.clip(a,a_min=None,a_max=None,out=None)
jax._src.numpy.lax_numpy.column_stack(tup)
jax._src.numpy.lax_numpy.compress(condition,a,axis:Optional[int]=None,out=None)
jax._src.numpy.lax_numpy.concatenate(arrays,axis:Optional[int]=0,dtype=None)
jax._src.numpy.lax_numpy.convolve(a,v,mode='full',*,precision=None)
jax._src.numpy.lax_numpy.copy(a,order=None)
jax._src.numpy.lax_numpy.corrcoef(x,y=None,rowvar=True)
jax._src.numpy.lax_numpy.correlate(a,v,mode='valid',*,precision=None)
jax._src.numpy.lax_numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax._src.numpy.lax_numpy.cross(a,b,axisa:int=-1,axisb:int=-1,axisc:int=-1,axis:Optional[int]=None)
jax._src.numpy.lax_numpy.delete(arr,obj,axis=None)
jax._src.numpy.lax_numpy.diag(v,k=0)
jax._src.numpy.lax_numpy.diag_indices(n,ndim=2)
jax._src.numpy.lax_numpy.diag_indices_from(arr)
jax._src.numpy.lax_numpy.diagflat(v,k=0)
jax._src.numpy.lax_numpy.diagonal(a,offset=0,axis1:int=0,axis2:int=1)
jax._src.numpy.lax_numpy.diff(a,n=1,axis:int=-1,prepend=None,append=None)
jax._src.numpy.lax_numpy.digitize(x,bins,right=False)
jax._src.numpy.lax_numpy.dot(a,b,*,precision=None)
jax._src.numpy.lax_numpy.dstack(tup,dtype=None)
jax._src.numpy.lax_numpy.ediff1d(ary,to_end=None,to_begin=None)
jax._src.numpy.lax_numpy.einsum(*operands,out=None,optimize='optimal',precision=None,_use_xeinsum=False)
jax._src.numpy.lax_numpy.einsum_path(subscripts,*operands,optimize='greedy')
jax._src.numpy.lax_numpy.empty(shape,dtype=None)
jax._src.numpy.lax_numpy.empty_like(prototype,dtype=None,shape=None)
jax._src.numpy.lax_numpy.expand_dims(a,axis:Union[int,Sequence[int]])
jax._src.numpy.lax_numpy.extract(condition,arr)
jax._src.numpy.lax_numpy.eye(N,M=None,k=0,dtype=None)
jax._src.numpy.lax_numpy.fix(x,out=None)
jax._src.numpy.lax_numpy.flatnonzero(a,*,size=None,fill_value=None)
jax._src.numpy.lax_numpy.flip(m,axis:Optional[Union[int,Tuple[int,...]]]=None)
jax._src.numpy.lax_numpy.fliplr(m)
jax._src.numpy.lax_numpy.flipud(m)
jax._src.numpy.lax_numpy.fmax(x1,x2)
jax._src.numpy.lax_numpy.fmin(x1,x2)
jax._src.numpy.lax_numpy.from_dlpack(x)
jax._src.numpy.lax_numpy.frombuffer(buffer,dtype=float,count=-1,offset=0)
jax._src.numpy.lax_numpy.fromfile(*args,**kwargs)
jax._src.numpy.lax_numpy.fromfunction(function,shape,*,dtype=float,**kwargs)
jax._src.numpy.lax_numpy.fromiter(*args,**kwargs)
jax._src.numpy.lax_numpy.fromstring(string,dtype=float,count=-1,*,sep)
jax._src.numpy.lax_numpy.full(shape,fill_value,dtype=None)
jax._src.numpy.lax_numpy.full_like(a,fill_value,dtype=None,shape=None)
jax._src.numpy.lax_numpy.gcd(x1,x2)
jax._src.numpy.lax_numpy.geomspace(start,stop,num=50,endpoint=True,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy.gradient(f,*varargs,axis:Optional[Union[int,Tuple[int,...]]]=None,edge_order=None)
jax._src.numpy.lax_numpy.histogram(a,bins=10,range=None,weights=None,density=None)
jax._src.numpy.lax_numpy.histogram2d(x,y,bins=10,range=None,weights=None,density=None)
jax._src.numpy.lax_numpy.histogram_bin_edges(a,bins=10,range=None,weights=None)
jax._src.numpy.lax_numpy.histogramdd(sample,bins=10,range=None,weights=None,density=None)
jax._src.numpy.lax_numpy.hstack(tup,dtype=None)
jax._src.numpy.lax_numpy.i0(x)
jax._src.numpy.lax_numpy.identity(n,dtype=None)
jax._src.numpy.lax_numpy.indices(dimensions,dtype=int32,sparse=False)
jax._src.numpy.lax_numpy.inner(a,b,*,precision=None)
jax._src.numpy.lax_numpy.insert(arr,obj,values,axis=None)
jax._src.numpy.lax_numpy.interp(x,xp,fp,left=None,right=None,period=None)
jax._src.numpy.lax_numpy.isclose(a,b,rtol=1e-05,atol=1e-08,equal_nan=False)
jax._src.numpy.lax_numpy.iscomplex(x)
jax._src.numpy.lax_numpy.iscomplexobj(x)
jax._src.numpy.lax_numpy.isreal(x)
jax._src.numpy.lax_numpy.isrealobj(x)
jax._src.numpy.lax_numpy.isscalar(element)
jax._src.numpy.lax_numpy.issubdtype(arg1,arg2)
jax._src.numpy.lax_numpy.ix_(*args)
jax._src.numpy.lax_numpy.kron(a,b)
jax._src.numpy.lax_numpy.lcm(x1,x2)
jax._src.numpy.lax_numpy.lexsort(keys,axis=-1)
jax._src.numpy.lax_numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy.load(*args,**kwargs)
jax._src.numpy.lax_numpy.logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis:int=0)
jax._src.numpy.lax_numpy.matmul(a,b,*,precision=None)
jax._src.numpy.lax_numpy.median(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,keepdims=False)
jax._src.numpy.lax_numpy.meshgrid(*xi,copy=True,sparse=False,indexing='xy')
jax._src.numpy.lax_numpy.moveaxis(a,source:Union[int,Sequence[int]],destination:Union[int,Sequence[int]])
jax._src.numpy.lax_numpy.msort(a)
jax._src.numpy.lax_numpy.nan_to_num(x,copy=True,nan=0.0,posinf=None,neginf=None)
jax._src.numpy.lax_numpy.nanargmax(a,axis:Optional[int]=None,out:Any=None,keepdims:Optional[bool]=None)
jax._src.numpy.lax_numpy.nanargmin(a,axis:Optional[int]=None,out:Any=None,keepdims:Optional[bool]=None)
jax._src.numpy.lax_numpy.nanmedian(a,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,keepdims=False)
jax._src.numpy.lax_numpy.nanpercentile(a,q,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,method='linear',keepdims=False,interpolation=None)
jax._src.numpy.lax_numpy.nanquantile(a,q,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,method='linear',keepdims=False,interpolation=None)
jax._src.numpy.lax_numpy.nonzero(a,*,size=None,fill_value=None)
jax._src.numpy.lax_numpy.ones(shape,dtype=None)
jax._src.numpy.lax_numpy.ones_like(a,dtype=None,shape=None)
jax._src.numpy.lax_numpy.outer(a,b,out=None)
jax._src.numpy.lax_numpy.packbits(a,axis:Optional[int]=None,bitorder='big')
jax._src.numpy.lax_numpy.pad(array,pad_width,mode='constant',**kwargs)
jax._src.numpy.lax_numpy.percentile(a,q,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,method='linear',keepdims=False,interpolation=None)
jax._src.numpy.lax_numpy.piecewise(x,condlist,funclist,*args,**kw)
jax._src.numpy.lax_numpy.quantile(a,q,axis:Optional[Union[int,Tuple[int,...]]]=None,out=None,overwrite_input=False,method='linear',keepdims=False,interpolation=None)
jax._src.numpy.lax_numpy.ravel(a,order='C')
jax._src.numpy.lax_numpy.ravel_multi_index(multi_index,dims,mode='raise',order='C')
jax._src.numpy.lax_numpy.repeat(a,repeats,axis:Optional[int]=None,*,total_repeat_length=None)
jax._src.numpy.lax_numpy.reshape(a,newshape,order='C')
jax._src.numpy.lax_numpy.resize(a,new_shape)
jax._src.numpy.lax_numpy.result_type(*args)
jax._src.numpy.lax_numpy.roll(a,shift,axis:Optional[Union[int,Sequence[int]]]=None)
jax._src.numpy.lax_numpy.rollaxis(a,axis:int,start=0)
jax._src.numpy.lax_numpy.rot90(m,k=1,axes=(0,1))
jax._src.numpy.lax_numpy.round(a,decimals=0,out=None)
jax._src.numpy.lax_numpy.searchsorted(a,v,side='left',sorter=None,*,method='scan')
jax._src.numpy.lax_numpy.select(condlist,choicelist,default=0)
jax._src.numpy.lax_numpy.sort(a,axis:Optional[int]=-1,kind='quicksort',order=None)
jax._src.numpy.lax_numpy.sort_complex(a)
jax._src.numpy.lax_numpy.split(ary,indices_or_sections,axis:int=0)
jax._src.numpy.lax_numpy.squeeze(a,axis:Optional[Union[int,Tuple[int,...]]]=None)
jax._src.numpy.lax_numpy.stack(arrays,axis:int=0,out=None,dtype=None)
jax._src.numpy.lax_numpy.swapaxes(a,axis1:int,axis2:int)
jax._src.numpy.lax_numpy.take(a,indices,axis:Optional[int]=None,out=None,mode=None,unique_indices=False,indices_are_sorted=False,fill_value=None)
jax._src.numpy.lax_numpy.take_along_axis(arr,indices,axis:Optional[int],mode:Optional[Union[str,lax.GatherScatterMode]]=None)
jax._src.numpy.lax_numpy.tensordot(a,b,axes=2,*,precision=None)
jax._src.numpy.lax_numpy.tile(A,reps)
jax._src.numpy.lax_numpy.trace(a,offset=0,axis1:int=0,axis2:int=1,dtype=None,out=None)
jax._src.numpy.lax_numpy.transpose(a,axes=None)
jax._src.numpy.lax_numpy.trapz(y,x=None,dx=1.0,axis:int=-1)
jax._src.numpy.lax_numpy.tri(N,M=None,k=0,dtype=None)
jax._src.numpy.lax_numpy.tril(m,k=0)
jax._src.numpy.lax_numpy.tril_indices_from(arr,k=0)
jax._src.numpy.lax_numpy.trim_zeros(filt,trim='fb')
jax._src.numpy.lax_numpy.trim_zeros_tol(filt,tol,trim='fb')
jax._src.numpy.lax_numpy.triu(m,k=0)
jax._src.numpy.lax_numpy.triu_indices_from(arr,k=0)
jax._src.numpy.lax_numpy.trunc(x)
jax._src.numpy.lax_numpy.unpackbits(a,axis:Optional[int]=None,count=None,bitorder='big')
jax._src.numpy.lax_numpy.unravel_index(indices,shape)
jax._src.numpy.lax_numpy.unwrap(p,discont=None,axis:int=-1,period=2*pi)
jax._src.numpy.lax_numpy.vander(x,N=None,increasing=False)
jax._src.numpy.lax_numpy.vdot(a,b,*,precision=None)
jax._src.numpy.lax_numpy.vstack(tup,dtype=None)
jax._src.numpy.lax_numpy.where(condition,x=None,y=None,*,size=None,fill_value=None)
jax._src.numpy.lax_numpy.zeros(shape,dtype=None)
jax._src.numpy.lax_numpy.zeros_like(a,dtype=None,shape=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/vectorize.py----------------------------------------
A:jax._src.numpy.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax._src.numpy.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax._src.numpy.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax._src.numpy.vectorize.num_core_dims->len(core_dims)
A:jax._src.numpy.vectorize.broadcast_shape->jax.lax.broadcast_shapes(*shapes)
A:jax._src.numpy.vectorize.out->func(*args)
A:jax._src.numpy.vectorize.out_shapes->map(jnp.shape, out if isinstance(out, tuple) else [out])
A:jax._src.numpy.vectorize.sizes->dict(dim_sizes)
A:jax._src.numpy.vectorize.args->tuple(map(jnp.asarray, args))
A:jax._src.numpy.vectorize.error_context->'on vectorized function with excluded={!r} and signature={!r}'.format(excluded, signature)
A:jax._src.numpy.vectorize.(excluded_func, args)->_apply_excluded(pyfunc, excluded, args)
A:jax._src.numpy.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax._src.numpy.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims, error_context)
A:jax._src.numpy.vectorize.checked_func->_check_output_dims(excluded_func, dim_sizes, output_core_dims, error_context)
A:jax._src.numpy.vectorize.core_shape->tuple((dim_sizes[dim] for dim in core_dims))
A:jax._src.numpy.vectorize.vec_arg->jax._src.numpy.lax_numpy.broadcast_to(arg, vec_shape)
A:jax._src.numpy.vectorize.in_axes->tuple((0 if c > 0 else None for c in vmap_counts))
A:jax._src.numpy.vectorize.vectorized_func->jax._src.api.vmap(vectorized_func, in_axes)
jax._src.numpy.vectorize._apply_excluded(func,excluded,args)
jax._src.numpy.vectorize._check_output_dims(func:Callable,dim_sizes:Dict[str,int],expected_output_core_dims:List[CoreDims],error_context:str='')->Callable
jax._src.numpy.vectorize._parse_gufunc_signature(signature:str)->Tuple[List[CoreDims], List[CoreDims]]
jax._src.numpy.vectorize._parse_input_dimensions(args:Tuple[NDArray,...],input_core_dims:List[CoreDims],error_context:str='')->Tuple[Tuple[int, ...], Dict[str, int]]
jax._src.numpy.vectorize._update_dim_sizes(dim_sizes:Dict[str,int],shape:Tuple[int,...],core_dims:CoreDims,error_context:str='',*,is_input:bool)
jax._src.numpy.vectorize.vectorize(pyfunc,*,excluded=frozenset(),signature=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/numpy/index_tricks.py----------------------------------------
A:jax._src.numpy.index_tricks.stop->jax.core.concrete_or_error(None, s.stop, f'slice stop of jnp.{op_name}')
A:jax._src.numpy.index_tricks.newobj->transpose(newobj, shape_obj)
A:jax._src.numpy.index_tricks.output->meshgrid(*output, indexing='ij', sparse=self.sparse)
A:jax._src.numpy.index_tricks.mgrid->_Mgrid()
A:jax._src.numpy.index_tricks.ogrid->_Ogrid()
A:jax._src.numpy.index_tricks.vec->directive.split(',')
A:jax._src.numpy.index_tricks.k->len(vec)
A:jax._src.numpy.index_tricks.params->list(map(int, vec))
A:jax._src.numpy.index_tricks.shape_obj->tuple(shape_obj[num_lshifts:] + shape_obj[:num_lshifts])
A:jax._src.numpy.index_tricks.res->expand_dims(res, matrix)
A:jax._src.numpy.index_tricks.r_->RClass()
A:jax._src.numpy.index_tricks.c_->CClass()
jax._src.numpy.index_tricks.CClass(_AxisConcat)
jax._src.numpy.index_tricks.RClass(_AxisConcat)
jax._src.numpy.index_tricks._AxisConcat(abc.ABC)
jax._src.numpy.index_tricks._AxisConcat.__getitem__(self,key)
jax._src.numpy.index_tricks._AxisConcat.__len__(self)
jax._src.numpy.index_tricks._IndexGrid(abc.ABC)
jax._src.numpy.index_tricks._IndexGrid.__getitem__(self,key)
jax._src.numpy.index_tricks._Mgrid(_IndexGrid)
jax._src.numpy.index_tricks._Ogrid(_IndexGrid)
jax._src.numpy.index_tricks._make_1d_grid_from_slice(s:slice,op_name:str)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/colab_debugger.py----------------------------------------
A:jax._src.debugger.colab_debugger.self._view->jax._src.debugger.colab_lib.dynamic(colab_lib.div())
A:jax._src.debugger.colab_debugger.is_dark_mode->google.colab.output.eval_js('document.documentElement.matches("[theme=dark]");')
A:jax._src.debugger.colab_debugger.lexer->pygments.lexers.get_lexer_by_name('python')
A:jax._src.debugger.colab_debugger.formatter->pygments.formatters.HtmlFormatter(full=False, hl_lines=highlights, linenos=True, linenostart=linenostart, style=code_style)
A:jax._src.debugger.colab_debugger.css_->pygments.formatters.HtmlFormatter(full=False, hl_lines=highlights, linenos=True, linenostart=linenostart, style=code_style).get_style_defs()
A:jax._src.debugger.colab_debugger.code->pygments.highlight(code, lexer, formatter)
A:jax._src.debugger.colab_debugger.(code_, css_)->self._highlight_code(self._code, highlights, linenostart)
A:jax._src.debugger.colab_debugger.uuid_->uuid.uuid4()
A:jax._src.debugger.colab_debugger.code_div->jax._src.debugger.colab_lib.div(colab_lib.css(css_), code_, id=f'code-{uuid_}', style=colab_lib.style({'max-height': '500px', 'overflow-y': 'scroll', 'background-color': 'var(--colab-border-color)', 'padding': '5px 5px 5px 5px'}))
A:jax._src.debugger.colab_debugger.self._header->jax._src.debugger.colab_lib.dynamic(colab_lib.div(colab_lib.span('Breakpoint'), style=colab_lib.style({'background-color': 'var(--colab-secondary-surface-color)', 'color': 'var(--colab-primary-text-color)', 'padding': '5px 5px 5px 5px', 'font-weight': 'bold'})))
A:jax._src.debugger.colab_debugger.self._code_view->CodeViewer('', highlights=[])
A:jax._src.debugger.colab_debugger.filename->self.frame.filename.strip()
A:jax._src.debugger.colab_debugger.self._file_cache[filename]->fp.read()
A:jax._src.debugger.colab_debugger.source->'\n'.join(frame.source)
A:jax._src.debugger.colab_debugger.highlight->min(frame.offset + 1, len(frame.source) - 1)
A:jax._src.debugger.colab_debugger.self._interaction_log->jax._src.debugger.colab_lib.dynamic(colab_lib.div())
A:jax._src.debugger.colab_debugger.self._frame_preview->FramePreview(frame)
A:jax._src.debugger.colab_debugger.self._debugger_view->DebuggerView(self.current_frame())
jax._src.debugger.colab_debugger.CodeViewer(self,code_:str,highlights:List[int],linenostart:int=1)
jax._src.debugger.colab_debugger.CodeViewer.__init__(self,code_:str,highlights:List[int],linenostart:int=1)
jax._src.debugger.colab_debugger.CodeViewer._highlight_code(self,code:str,highlights,linenostart:int)
jax._src.debugger.colab_debugger.CodeViewer.append(self,child)
jax._src.debugger.colab_debugger.CodeViewer.clear(self)
jax._src.debugger.colab_debugger.CodeViewer.render(self)
jax._src.debugger.colab_debugger.CodeViewer.update(self,elem)
jax._src.debugger.colab_debugger.CodeViewer.update_code(self,code_,highlights,*,linenostart:int=1)
jax._src.debugger.colab_debugger.ColabDebugger(self,frames:List[debugger_core.DebuggerFrame],thread_id:int)
jax._src.debugger.colab_debugger.ColabDebugger.__init__(self,frames:List[debugger_core.DebuggerFrame],thread_id:int)
jax._src.debugger.colab_debugger.ColabDebugger.do_down(self,arg)
jax._src.debugger.colab_debugger.ColabDebugger.do_up(self,arg)
jax._src.debugger.colab_debugger.ColabDebugger.run(self)
jax._src.debugger.colab_debugger.DebuggerView(self,frame,*,log_color='')
jax._src.debugger.colab_debugger.DebuggerView.__init__(self,frame,*,log_color='')
jax._src.debugger.colab_debugger.DebuggerView.append(self,child)
jax._src.debugger.colab_debugger.DebuggerView.clear(self)
jax._src.debugger.colab_debugger.DebuggerView.flush(self)
jax._src.debugger.colab_debugger.DebuggerView.isatty(self)
jax._src.debugger.colab_debugger.DebuggerView.read(self)
jax._src.debugger.colab_debugger.DebuggerView.readline(self)
jax._src.debugger.colab_debugger.DebuggerView.render(self)
jax._src.debugger.colab_debugger.DebuggerView.update(self,elem)
jax._src.debugger.colab_debugger.DebuggerView.update_frame(self,frame)
jax._src.debugger.colab_debugger.DebuggerView.write(self,text)
jax._src.debugger.colab_debugger.FramePreview(self,frame)
jax._src.debugger.colab_debugger.FramePreview.__init__(self,frame)
jax._src.debugger.colab_debugger.FramePreview.append(self,child)
jax._src.debugger.colab_debugger.FramePreview.clear(self)
jax._src.debugger.colab_debugger.FramePreview.render(self)
jax._src.debugger.colab_debugger.FramePreview.update(self,elem)
jax._src.debugger.colab_debugger.FramePreview.update_frame(self,frame)
jax._src.debugger.colab_debugger._run_debugger(frames,thread_id,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/web_debugger.py----------------------------------------
A:jax._src.debugger.web_debugger.web_pdb_version->tuple(map(int, web_pdb.__version__.split('.')))
A:jax._src.debugger.web_debugger._web_consoles[host, port]->web_pdb.WebConsole(host, port, self)
A:jax._src.debugger.web_debugger._web_console._debugger->weakref.proxy(self)
A:jax._src.debugger.web_debugger.current_frame->self.current_frame()
A:jax._src.debugger.web_debugger.globals->'\n'.join([f'{key} = {value}' for (key, value) in sorted(current_frame.globals.items())])
A:jax._src.debugger.web_debugger.locals->'\n'.join([f'{key} = {value}' for (key, value) in sorted(current_frame.locals.items())])
jax._src.debugger.web_debugger.WebDebugger(self,frames:List[debugger_core.DebuggerFrame],thread_id,completekey:str='tab',host:str='',port:int=5555)
jax._src.debugger.web_debugger.WebDebugger.__init__(self,frames:List[debugger_core.DebuggerFrame],thread_id,completekey:str='tab',host:str='',port:int=5555)
jax._src.debugger.web_debugger.WebDebugger.get_current_frame_data(self)
jax._src.debugger.web_debugger.WebDebugger.get_globals(self)
jax._src.debugger.web_debugger.WebDebugger.get_locals(self)
jax._src.debugger.web_debugger.WebDebugger.run(self)
jax._src.debugger.web_debugger.run_debugger(frames:List[debugger_core.DebuggerFrame],thread_id:Optional[int],**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/cli_debugger.py----------------------------------------
A:jax._src.debugger.cli_debugger.msg->traceback.format_exception_only(*exc_info)[-1].strip()
jax._src.debugger.cli_debugger.CliDebugger(self,frames:List[DebuggerFrame],thread_id,stdin:Optional[IO[str]]=None,stdout:Optional[IO[str]]=None,completekey:str='tab')
jax._src.debugger.cli_debugger.CliDebugger.__init__(self,frames:List[DebuggerFrame],thread_id,stdin:Optional[IO[str]]=None,stdout:Optional[IO[str]]=None,completekey:str='tab')
jax._src.debugger.cli_debugger.CliDebugger._error_message(self)
jax._src.debugger.cli_debugger.CliDebugger.current_frame(self)
jax._src.debugger.cli_debugger.CliDebugger.default(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_continue(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_down(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_list(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_p(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_pp(self,arg)
jax._src.debugger.cli_debugger.CliDebugger.do_quit(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_up(self,_)
jax._src.debugger.cli_debugger.CliDebugger.do_where(self,_)
jax._src.debugger.cli_debugger.CliDebugger.evaluate(self,expr)
jax._src.debugger.cli_debugger.CliDebugger.print_backtrace(self)
jax._src.debugger.cli_debugger.CliDebugger.print_context(self,num_lines=2)
jax._src.debugger.cli_debugger.CliDebugger.run(self)
jax._src.debugger.cli_debugger.run_debugger(frames:List[DebuggerFrame],thread_id:Optional[int],**kwargs:Any)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/colab_lib.py----------------------------------------
A:jax._src.debugger.colab_lib.self._uuid->str(uuid.uuid4())
A:jax._src.debugger.colab_lib.self._root_elem->div(id=self.tag)
A:jax._src.debugger.colab_lib.children->'\n'.join([str(c) for c in self.children])
A:jax._src.debugger.colab_lib.code->functools.partial(_make_elem, 'code')
A:jax._src.debugger.colab_lib.div->functools.partial(_make_elem, 'div')
A:jax._src.debugger.colab_lib.li->functools.partial(_make_elem, 'li')
A:jax._src.debugger.colab_lib.ol->functools.partial(_make_elem, 'ol')
A:jax._src.debugger.colab_lib.pre->functools.partial(_make_elem, 'pre')
A:jax._src.debugger.colab_lib.progress->functools.partial(_make_elem, 'progress')
A:jax._src.debugger.colab_lib.span->functools.partial(_make_elem, 'span')
jax._src.debugger.colab_lib.DOMElement(metaclass=abc.ABCMeta)
jax._src.debugger.colab_lib.DOMElement.render(self)
jax._src.debugger.colab_lib.DynamicDOMElement(DOMElement)
jax._src.debugger.colab_lib.DynamicDOMElement.append(self,child:DOMElement)
jax._src.debugger.colab_lib.DynamicDOMElement.clear(self)
jax._src.debugger.colab_lib.DynamicDOMElement.render(self)
jax._src.debugger.colab_lib.DynamicDOMElement.update(self,elem:DOMElement)
jax._src.debugger.colab_lib.DynamicDiv(DynamicDOMElement)
jax._src.debugger.colab_lib.DynamicDiv.__post_init__(self)
jax._src.debugger.colab_lib.DynamicDiv.append(self,child:DOMElement)
jax._src.debugger.colab_lib.DynamicDiv.clear(self)
jax._src.debugger.colab_lib.DynamicDiv.render(self)
jax._src.debugger.colab_lib.DynamicDiv.tag(self)
jax._src.debugger.colab_lib.DynamicDiv.update(self,elem:DOMElement)
jax._src.debugger.colab_lib.StaticDOMElement(DOMElement)
jax._src.debugger.colab_lib.StaticDOMElement.__repr__(self)
jax._src.debugger.colab_lib.StaticDOMElement.__str__(self)
jax._src.debugger.colab_lib.StaticDOMElement.append(self,child:DOMElement)->DOMElement
jax._src.debugger.colab_lib.StaticDOMElement.attr(self,key:str)->str
jax._src.debugger.colab_lib.StaticDOMElement.html(self)
jax._src.debugger.colab_lib.StaticDOMElement.render(self)
jax._src.debugger.colab_lib.StaticDOMElement.replace(self,**kwargs)->DOMElement
jax._src.debugger.colab_lib._make_elem(tag:str,*children:Element,**attrs)->StaticDOMElement
jax._src.debugger.colab_lib._style_dict_to_str(style_dict:Dict[str,Any])->str
jax._src.debugger.colab_lib.css(text:str)->StaticDOMElement
jax._src.debugger.colab_lib.dynamic(elem:StaticDOMElement)->DynamicDiv
jax._src.debugger.colab_lib.style(*args,**kwargs)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/debugger/core.py----------------------------------------
A:jax._src.debugger.core.cant_flatten->_CantFlatten()
A:jax._src.debugger.core.(flat_locals, locals_tree)->_safe_flatten_dict(self.locals)
A:jax._src.debugger.core.(flat_globals, globals_tree)->_safe_flatten_dict(self.globals)
A:jax._src.debugger.core.(invalid_vars, valid_vars)->jax._src.util.partition_list(is_valid, flat_vars)
A:jax._src.debugger.core.flat_vars->jax._src.util.merge_lists(is_valid, invalid_vars, valid_vars)
A:jax._src.debugger.core.(flat_locals, flat_globals)->jax._src.util.split_list(flat_vars, [num_locals])
A:jax._src.debugger.core.locals_->jax.tree_util.tree_unflatten(locals_tree, flat_locals).to_dict()
A:jax._src.debugger.core.globals_->jax.tree_util.tree_unflatten(globals_tree, flat_globals).to_dict()
A:jax._src.debugger.core.(_, start)->inspect.getsourcelines(frame_info.frame)
A:jax._src.debugger.core.source->inspect.getsource(frame_info.frame).split('\n')
A:jax._src.debugger.core.debuggers->sorted(_debugger_registry.values(), key=lambda x: -x[0])
A:jax._src.debugger.core.debug_lock->threading.Lock()
A:jax._src.debugger.core.frame_infos->inspect.stack()
A:jax._src.debugger.core.(flat_args, frames_tree)->jax.tree_util.tree_flatten(frames)
A:jax._src.debugger.core.frames->jax.tree_util.tree_unflatten(frames_tree, flat_args)
A:jax._src.debugger.core.thread_id->threading.get_ident()
A:jax._src.debugger.core.debugger->get_debugger(backend=backend)
jax._src.debugger.breakpoint(*,backend:Optional[str]=None,filter_frames:bool=True,num_frames:Optional[int]=None,ordered:bool=False,**kwargs)
jax._src.debugger.core.Debugger(self,frames:List[DebuggerFrame],thread_id:Optional[int],**kwargs:Any)
jax._src.debugger.core.Debugger.__call__(self,frames:List[DebuggerFrame],thread_id:Optional[int],**kwargs:Any)
jax._src.debugger.core.DebuggerFrame
jax._src.debugger.core.DebuggerFrame.from_frameinfo(cls,frame_info)->DebuggerFrame
jax._src.debugger.core.DebuggerFrame.tree_flatten(self)
jax._src.debugger.core.DebuggerFrame.tree_unflatten(cls,info,valid_vars)
jax._src.debugger.core._CantFlatten
jax._src.debugger.core._DictWrapper(self,keys,values)
jax._src.debugger.core._DictWrapper.__init__(self,keys,values)
jax._src.debugger.core._DictWrapper.to_dict(self)
jax._src.debugger.core._DictWrapper.tree_flatten(self)
jax._src.debugger.core._DictWrapper.tree_unflatten(cls,keys,values)
jax._src.debugger.core._safe_flatten_dict(dct:dict[Any,Any])->tuple[list[Any], tree_util.PyTreeDef]
jax._src.debugger.core.breakpoint(*,backend:Optional[str]=None,filter_frames:bool=True,num_frames:Optional[int]=None,ordered:bool=False,**kwargs)
jax._src.debugger.core.get_debugger(backend:Optional[str]=None)->Debugger
jax._src.debugger.core.register_debugger(name:str,debugger:Debugger,priority:int)->None


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/scipy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/scipy/signal_helper.py----------------------------------------
A:jax._src.third_party.scipy.signal_helper.win->jax._src.numpy.lax_numpy.asarray(window)
A:jax._src.third_party.scipy.signal_helper.ii_2->jax._src.numpy.lax_numpy.arange(2.0, n, 2)
jax._src.third_party.scipy.signal_helper._median_bias(n)
jax._src.third_party.scipy.signal_helper._triage_segments(window,nperseg,input_length,dtype)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/scipy/linalg.py----------------------------------------
A:jax._src.third_party.scipy.linalg.minden->jax._src.numpy.lax_numpy.where(minden == 0.0, tol, minden)
A:jax._src.third_party.scipy.linalg.(_, F, minden)->jax.lax.fori_loop(1, N - p + 1, _inner_loop, (p, *F_minden))
A:jax._src.third_party.scipy.linalg.s->jax._src.numpy.lax_numpy.where(den != 0, s / den, s)
A:jax._src.third_party.scipy.linalg.F->F.astype(T.dtype.char).astype(T.dtype.char)
A:jax._src.third_party.scipy.linalg.A->jax._src.numpy.lax_numpy.asarray(A)
A:jax._src.third_party.scipy.linalg.(T, Z)->rsf2csf(T, Z)
A:jax._src.third_party.scipy.linalg.(F, minden)->_algorithm_11_1_1(F, T)
A:jax._src.third_party.scipy.linalg.err->jax._src.numpy.lax_numpy.where(jnp.any(jnp.isinf(F)), jnp.inf, jnp.minimum(1, jnp.maximum(tol, tol / minden * norm(jnp.triu(T, 1), 1))))
jax._src.third_party.scipy.linalg._algorithm_11_1_1(F:Array,T:Array)->Tuple[Array, Array]
jax._src.third_party.scipy.linalg.funm(A:ArrayLike,func:Callable[[Array],Array],disp:bool=True)->Tuple[Array, Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/scipy/interpolate.py----------------------------------------
A:jax._src.third_party.scipy.interpolate.p->broadcast_arrays(*points)
A:jax._src.third_party.scipy.interpolate.points->points.reshape(-1, ndim).reshape(-1, ndim)
A:jax._src.third_party.scipy.interpolate.(values,)->_promote_dtypes_inexact(values)
A:jax._src.third_party.scipy.interpolate.fill_value->asarray(fill_value)
A:jax._src.third_party.scipy.interpolate.self.grid->tuple((asarray(p) for p in points))
A:jax._src.third_party.scipy.interpolate.ndim->len(self.grid)
A:jax._src.third_party.scipy.interpolate.xi->xi.reshape(-1, xi_shape[-1]).reshape(-1, xi_shape[-1])
A:jax._src.third_party.scipy.interpolate.(indices, norm_distances, out_of_bounds)->self._find_indices(xi.T)
A:jax._src.third_party.scipy.interpolate.result->where(out_of_bounds.reshape(bc_shp), self.fill_value, result)
A:jax._src.third_party.scipy.interpolate.edges->product(*[[i, i + 1] for i in indices])
A:jax._src.third_party.scipy.interpolate.values->asarray(0.0)
A:jax._src.third_party.scipy.interpolate.weight->asarray(1.0)
A:jax._src.third_party.scipy.interpolate.out_of_bounds->zeros((xi.shape[1],), dtype=bool)
A:jax._src.third_party.scipy.interpolate.i->where(i > g.size - 2, g.size - 2, i)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator(self,points,values,method='linear',bounds_error=False,fill_value=nan)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator.__init__(self,points,values,method='linear',bounds_error=False,fill_value=nan)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._evaluate_linear(self,indices,norm_distances)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._evaluate_nearest(self,indices,norm_distances)
jax._src.third_party.scipy.interpolate.RegularGridInterpolator._find_indices(self,xi)
jax._src.third_party.scipy.interpolate._ndim_coords_from_arrays(points,ndim=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/numpy/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/third_party/numpy/linalg.py----------------------------------------
A:jax._src.third_party.numpy.linalg.s->numpy.empty((n, n), dtype=np.intp)
A:jax._src.third_party.numpy.linalg.invx->jax._src.numpy.linalg.inv(x)
A:jax._src.third_party.numpy.linalg.orig_nan_check->jax._src.numpy.lax_numpy.full_like(r, ~jnp.isnan(r).any())
A:jax._src.third_party.numpy.linalg.nan_mask->jax._src.numpy.lax_numpy.logical_and(jnp.isnan(r), ~jnp.isnan(x).any(axis=(-2, -1)))
A:jax._src.third_party.numpy.linalg.r->jax._src.numpy.lax_numpy.where(orig_nan_check, jnp.where(nan_mask, jnp.inf, r), r)
A:jax._src.third_party.numpy.linalg.a->a.reshape(-1, prod).reshape(-1, prod)
A:jax._src.third_party.numpy.linalg.ia->jax._src.numpy.linalg.inv(a)
A:jax._src.third_party.numpy.linalg.b->b.ravel().ravel()
A:jax._src.third_party.numpy.linalg.allaxes->list(range(0, an))
A:jax._src.third_party.numpy.linalg.res->res.reshape(Q).reshape(Q)
A:jax._src.third_party.numpy.linalg.n->len(arrays)
A:jax._src.third_party.numpy.linalg.arrays[0]->jax._src.numpy.lax_numpy.atleast_2d(arrays[0])
A:jax._src.third_party.numpy.linalg.result->_multi_dot(arrays, order, 0, n - 1, precision)
A:jax._src.third_party.numpy.linalg.order->_multi_dot_matrix_chain_order(arrays)
A:jax._src.third_party.numpy.linalg.m->numpy.zeros((n, n), dtype=np.double)
jax._src.third_party.numpy.linalg._assert2d(*arrays)
jax._src.third_party.numpy.linalg._assertNdSquareness(*arrays)
jax._src.third_party.numpy.linalg._assertNoEmpty2d(*arrays)
jax._src.third_party.numpy.linalg._assertRankAtLeast2(*arrays)
jax._src.third_party.numpy.linalg._isEmpty2d(arr)
jax._src.third_party.numpy.linalg._multi_dot(arrays,order,i,j,precision)
jax._src.third_party.numpy.linalg._multi_dot_matrix_chain_order(arrays,return_costs=False)
jax._src.third_party.numpy.linalg._multi_dot_three(A,B,C,precision)
jax._src.third_party.numpy.linalg.cond(x,p=None)
jax._src.third_party.numpy.linalg.multi_dot(arrays,*,precision=None)
jax._src.third_party.numpy.linalg.tensorinv(a,ind=2)
jax._src.third_party.numpy.linalg.tensorsolve(a,b,axes=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/nn/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/nn/functions.py----------------------------------------
A:jax._src.nn.functions.safe_x->jax.numpy.where(x > 0, 0.0, x)
A:jax._src.nn.functions.x->jax.numpy.asarray(x)
A:jax._src.nn.functions.sqrt_2_over_pi->numpy.sqrt(2 / np.pi).astype(x.dtype)
A:jax._src.nn.functions.sqrt_2->numpy.sqrt(2).astype(x.dtype)
A:jax._src.nn.functions.(x1, x2)->jax.numpy.split(x, 2, axis)
A:jax._src.nn.functions.x_max->jax.numpy.max(x, axis, where=where, initial=initial, keepdims=True)
A:jax._src.nn.functions.shifted_logsumexp->jax.numpy.log(jnp.sum(jnp.exp(shifted), axis, where=where, keepdims=True))
A:jax._src.nn.functions.unnormalized->jax.numpy.exp(x - lax.stop_gradient(x_max))
A:jax._src.nn.functions.mean->jax.numpy.mean(x, axis, keepdims=True, where=where)
A:jax._src.nn.functions.num_classes->jax.core.concrete_or_error(int, num_classes, 'The error arose in jax.nn.one_hot argument `num_classes`.')
A:jax._src.nn.functions.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.nn.functions.output_pos_axis->jax._src.util.canonicalize_axis(axis, x.ndim + 1)
A:jax._src.nn.functions.axis_size->jax.lax.psum(1, axis)
A:jax._src.nn.functions.axis_idx->jax.lax.axis_index(axis)
A:jax._src.nn.functions.axis->operator.index(axis)
A:jax._src.nn.functions.lhs->jax.lax.expand_dims(x, (axis,))
A:jax._src.nn.functions.rhs->jax.lax.broadcasted_iota(x.dtype, rhs_shape, output_pos_axis)
jax._src.nn.functions._one_hot(x:Array,num_classes:int,*,dtype:Any,axis:Union[int,AxisName])->Array
jax._src.nn.functions.celu(x:Array,alpha:Array=1.0)->Array
jax._src.nn.functions.elu(x:Array,alpha:Array=1.0)->Array
jax._src.nn.functions.gelu(x:Array,approximate:bool=True)->Array
jax._src.nn.functions.glu(x:Array,axis:int=-1)->Array
jax._src.nn.functions.hard_sigmoid(x:Array)->Array
jax._src.nn.functions.hard_silu(x:Array)->Array
jax._src.nn.functions.hard_tanh(x:Array)->Array
jax._src.nn.functions.leaky_relu(x:Array,negative_slope:Array=0.01)->Array
jax._src.nn.functions.log_sigmoid(x:Array)->Array
jax._src.nn.functions.log_softmax(x:Array,axis:Optional[Union[int,Tuple[int,...]]]=-1,where:Optional[Array]=None,initial:Optional[Array]=None)->Array
jax._src.nn.functions.normalize(x:Array,axis:Optional[Union[int,Tuple[int,...]]]=-1,mean:Optional[Array]=None,variance:Optional[Array]=None,epsilon:Array=1e-05,where:Optional[Array]=None)->Array
jax._src.nn.functions.one_hot(x:Array,num_classes:int,*,dtype:Any=jnp.float_,axis:Union[int,AxisName]=-1)->Array
jax._src.nn.functions.relu(x:Array)->Array
jax._src.nn.functions.relu6(x:Array)->Array
jax._src.nn.functions.selu(x:Array)->Array
jax._src.nn.functions.sigmoid(x:Array)->Array
jax._src.nn.functions.silu(x:Array)->Array
jax._src.nn.functions.soft_sign(x:Array)->Array
jax._src.nn.functions.softmax(x:Array,axis:Optional[Union[int,Tuple[int,...]]]=-1,where:Optional[Array]=None,initial:Optional[Array]=None)->Array
jax._src.nn.functions.softplus(x:Array)->Array
jax._src.nn.functions.standardize(x:Array,axis:Optional[Union[int,Tuple[int,...]]]=-1,mean:Optional[Array]=None,variance:Optional[Array]=None,epsilon:Array=1e-05,where:Optional[Array]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/nn/initializers.py----------------------------------------
A:jax._src.nn.initializers.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.nn.initializers.in_size->int(np.prod([shape[i] for i in in_axis]))
A:jax._src.nn.initializers.out_size->int(np.prod([shape[i] for i in out_axis]))
A:jax._src.nn.initializers.batch_size->int(np.prod([shape[i] for i in batch_axis]))
A:jax._src.nn.initializers.(key_r, key_theta)->jax.random.split(key)
A:jax._src.nn.initializers.r->jax.numpy.sqrt(-jnp.log(1 - t))
A:jax._src.nn.initializers.named_shape->jax.core.as_named_shape(shape)
A:jax._src.nn.initializers.(fan_in, fan_out)->_compute_fans(named_shape, in_axis, out_axis, batch_axis)
A:jax._src.nn.initializers.variance->jax.numpy.array(scale / denominator, dtype=dtype)
A:jax._src.nn.initializers.A->jax.random.normal(key, matrix_shape, dtype)
A:jax._src.nn.initializers.(Q, R)->jax.numpy.linalg.qr(A)
A:jax._src.nn.initializers.diag_sign->jax.lax.broadcast_to_rank(jnp.sign(jnp.diag(R)), rank=Q.ndim)
A:jax._src.nn.initializers.Q->jax.numpy.moveaxis(Q, -1, column_axis)
A:jax._src.nn.initializers.ortho_init->orthogonal(scale=scale, column_axis=column_axis, dtype=dtype)
A:jax._src.nn.initializers.ortho_matrix->ortho_init(key, shape[-2:])
A:jax._src.nn.initializers.W->jax.numpy.zeros(shape, dtype=dtype)
jax._src.nn.initializers.Initializer(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)
jax._src.nn.initializers.Initializer.__call__(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)
jax._src.nn.initializers._complex_truncated_normal(key:KeyArray,upper:Array,shape:Union[Sequence[int],core.NamedShape],dtype:DTypeLikeInexact)->Array
jax._src.nn.initializers._complex_uniform(key:KeyArray,shape:Union[Sequence[int],core.NamedShape],dtype:DTypeLikeInexact)->Array
jax._src.nn.initializers._compute_fans(shape:core.NamedShape,in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Union[int,Sequence[int]]=())->Tuple[Array, Array]
jax._src.nn.initializers.constant(value:Array,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.delta_orthogonal(scale:RealNumeric=1.0,column_axis:int=-1,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.glorot_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.glorot_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.he_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.he_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.lecun_normal(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.lecun_uniform(in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.normal(stddev:RealNumeric=0.01,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.ones(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)->Array
jax._src.nn.initializers.orthogonal(scale:RealNumeric=1.0,column_axis:int=-1,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.uniform(scale:RealNumeric=0.01,dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.variance_scaling(scale:RealNumeric,mode:Union[Literal['fan_in'],Literal['fan_out'],Literal['fan_avg']],distribution:Union[Literal['truncated_normal'],Literal['normal'],Literal['uniform']],in_axis:Union[int,Sequence[int]]=-2,out_axis:Union[int,Sequence[int]]=-1,batch_axis:Sequence[int]=(),dtype:DTypeLikeInexact=jnp.float_)->Initializer
jax._src.nn.initializers.zeros(key:KeyArray,shape:core.Shape,dtype:DTypeLikeInexact=jnp.float_)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/ops/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/ops/scatter.py----------------------------------------
A:jax._src.ops.scatter.x->jax._src.numpy.lax_numpy.asarray(x)
A:jax._src.ops.scatter.y->jax.lax.rev(y, indexer.reversed_y_dims)
A:jax._src.ops.scatter.(treedef, static_idx, dynamic_idx)->jax._src.numpy.lax_numpy._split_index_for_jit(idx, x.shape)
A:jax._src.ops.scatter.dtype->jax.lax.dtype(x)
A:jax._src.ops.scatter.weak_type->jax._src.dtypes.is_weakly_typed(x)
A:jax._src.ops.scatter.idx->jax._src.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)
A:jax._src.ops.scatter.indexer->jax._src.numpy.lax_numpy._index_to_gather(jnp.shape(x), idx, normalize_indices=normalize_indices)
A:jax._src.ops.scatter.(x, y)->jax._src.numpy.lax_numpy._promote_dtypes(x, y)
A:jax._src.ops.scatter.dnums->jax.lax.ScatterDimensionNumbers(update_window_dims=indexer.dnums.offset_dims, inserted_window_dims=indexer.dnums.collapsed_slice_dims, scatter_dims_to_operand_dims=indexer.dnums.start_index_map)
A:jax._src.ops.scatter.out->_scatter_update(out, np.index_exp[lax.div(jnp.arange(segment_ids.shape[0]), bucket_size), segment_ids[None, :]], data, scatter_op, indices_are_sorted, unique_indices, normalize_indices=False, mode=mode)
A:jax._src.ops.scatter.data->jax._src.numpy.lax_numpy.asarray(data)
A:jax._src.ops.scatter.segment_ids->jax._src.numpy.lax_numpy.asarray(segment_ids)
A:jax._src.ops.scatter.num_segments->jax.core.concrete_or_error(int, num_segments, 'segment_sum() `num_segments` argument.')
A:jax._src.ops.scatter.num_buckets->jax._src.util.ceil_of_ratio(segment_ids.size, bucket_size)
jax._src.ops.scatter._get_identity(op,dtype)
jax._src.ops.scatter._scatter_impl(x,y,scatter_op,treedef,static_idx,dynamic_idx,indices_are_sorted,unique_indices,mode,normalize_indices)
jax._src.ops.scatter._scatter_update(x,idx,y,scatter_op,indices_are_sorted,unique_indices,mode=None,normalize_indices=True)
jax._src.ops.scatter._segment_update(name:str,data:Array,segment_ids:Array,scatter_op:Callable,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,reducer:Optional[Callable]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_max(data:Array,segment_ids:Array,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_min(data:Array,segment_ids:Array,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_prod(data:Array,segment_ids:Array,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array
jax._src.ops.scatter.segment_sum(data:Array,segment_ids:Array,num_segments:Optional[int]=None,indices_are_sorted:bool=False,unique_indices:bool=False,bucket_size:Optional[int]=None,mode:Optional[lax.GatherScatterMode]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/state/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/state/discharge.py----------------------------------------
A:jax._src.state.discharge.eval_jaxpr->jax.linear_util.wrap_init(partial(_eval_jaxpr_discharge_state, jaxpr, consts))
A:jax._src.state.discharge.(new_jaxpr, _, new_consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, in_avals)
A:jax._src.state.discharge.env->Environment({})
A:jax._src.state.discharge.invals->map(env.read, eqn.invars)
A:jax._src.state.discharge.(new_invals, ans)->_discharge_rules[eqn.primitive](in_avals, *invals, **eqn.params)
A:jax._src.state.discharge.(subfuns, bind_params)->eqn.primitive.get_bind_params(eqn.params)
A:jax._src.state.discharge.ans->_addupdate_discharge(x, val, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.out_vals->map(env.read, jaxpr.outvars)
A:jax._src.state.discharge.ref_vals->map(env.read, [v for v in jaxpr.invars if type(v.aval) is ShapedArrayRef])
A:jax._src.state.discharge.y->_get_discharge(x, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.indexer->tuple([next(idx_) if b else slice(None) for b in indexed_dims])
A:jax._src.state.discharge.idx_->iter(idx)
A:jax._src.state.discharge.(z, x_new)->_swap_discharge(x, val, non_slice_idx, indexed_dims)
A:jax._src.state.discharge.z->_prepend_gather(x, idx, indexed_dims)
A:jax._src.state.discharge.x_new->_prepend_scatter(x, idx, indexed_dims, val)
A:jax._src.state.discharge.out->jax.lax.dynamic_slice(x, starts, sizes)
A:jax._src.state.discharge.num_outs->len(jaxpr.outvars)
A:jax._src.state.discharge.(discharged_jaxpr, discharged_consts)->discharge_state(jaxpr, consts)
A:jax._src.state.discharge.discharged_closed_jaxpr->jax.core.ClosedJaxpr(discharged_jaxpr, discharged_consts)
A:jax._src.state.discharge.fun->jax.linear_util.wrap_init(core.jaxpr_as_fun(discharged_closed_jaxpr))
A:jax._src.state.discharge.out_and_ref_vals->jax.core.closed_call_p.bind(fun, *args, call_jaxpr=discharged_closed_jaxpr)
A:jax._src.state.discharge.(out_vals, ref_vals)->split_list(out_and_ref_vals, [num_outs])
A:jax._src.state.discharge.ref_vals_iter->iter(ref_vals)
A:jax._src.state.discharge.new_invals->tuple((next(ref_vals_iter) if isinstance(aval, ShapedArrayRef) else None for aval in in_avals))
jax._src.state.discharge.DischargeRule(self,in_avals:Sequence[core.AbstractValue],*args:Any,**params:Any)
jax._src.state.discharge.DischargeRule.__call__(self,in_avals:Sequence[core.AbstractValue],*args:Any,**params:Any)
jax._src.state.discharge.Environment
jax._src.state.discharge.Environment.read(self,v:core.Atom)->Any
jax._src.state.discharge.Environment.write(self,v:core.Var,val:Any)->None
jax._src.state.discharge._addupdate_discharge(x,val,idx,indexed_dims)
jax._src.state.discharge._addupdate_discharge_rule(_:Sequence[core.AbstractValue],x,val,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge._closed_call_discharge_rule(in_avals:Sequence[core.AbstractValue],*args,call_jaxpr:core.ClosedJaxpr)
jax._src.state.discharge._dynamic_index(x,idx,indexed_dims)
jax._src.state.discharge._dynamic_update_index(x,idx,val,indexed_dims)
jax._src.state.discharge._eval_jaxpr_discharge_state(jaxpr:core.Jaxpr,consts:Sequence[Any],*args:Any)
jax._src.state.discharge._get_discharge(x,idx,indexed_dims)
jax._src.state.discharge._get_discharge_rule(_:Sequence[core.AbstractValue],x,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge._has_refs(eqn:core.JaxprEqn)
jax._src.state.discharge._indexer(idx,indexed_dims)
jax._src.state.discharge._prepend_gather(x,idx,indexed_dims)
jax._src.state.discharge._prepend_scatter(x,idx,indexed_dims,val,*,add=False)
jax._src.state.discharge._swap_discharge(x,val,idx,indexed_dims)
jax._src.state.discharge._swap_discharge_rule(_:Sequence[core.AbstractValue],x,val,*non_slice_idx,indexed_dims:Sequence[bool])
jax._src.state.discharge.discharge_state(jaxpr:core.Jaxpr,consts:Sequence[Any])->Tuple[core.Jaxpr, List[Any]]
jax._src.state.discharge.register_discharge_rule(prim:core.Primitive)
jax._src.state.discharge_state(jaxpr:core.Jaxpr,consts:Sequence[Any])->Tuple[core.Jaxpr, List[Any]]
jax._src.state.register_discharge_rule(prim:core.Primitive)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/state/types.py----------------------------------------
A:jax._src.state.types.ndim->property(lambda self: len(self.shape))
A:jax._src.state.types.size->property(lambda self: prod(self.shape))
A:jax._src.state.types.a->jax.core.ShapedArray(self.shape, self.dtype)
jax._src.state.AccumEffect(RefEffect)
jax._src.state.AccumEffect.__str__(self)
jax._src.state.ReadEffect(RefEffect)
jax._src.state.ReadEffect.__str__(self)
jax._src.state.ShapedArrayRef(self,shape,dtype)
jax._src.state.ShapedArrayRef.__eq__(self,other)
jax._src.state.ShapedArrayRef.__hash__(self)
jax._src.state.ShapedArrayRef.__repr__(self)->str
jax._src.state.ShapedArrayRef._getitem(self,tracer,idx)->Array
jax._src.state.ShapedArrayRef._setitem(self,tracer,idx,value)->None
jax._src.state.ShapedArrayRef.at_least_vspace(self)
jax._src.state.ShapedArrayRef.get(tracer,idx=())
jax._src.state.ShapedArrayRef.join(self,other)
jax._src.state.ShapedArrayRef.set(tracer,value,idx=())
jax._src.state.WriteEffect(RefEffect)
jax._src.state.WriteEffect.__str__(self)
jax._src.state.types.AccumEffect(RefEffect)
jax._src.state.types.AccumEffect.__str__(self)
jax._src.state.types.ReadEffect(RefEffect)
jax._src.state.types.ReadEffect.__str__(self)
jax._src.state.types.RefEffect(self,ref_aval:ShapedArrayRef)
jax._src.state.types.RefEffect.__eq__(self,other)
jax._src.state.types.RefEffect.__hash__(self)
jax._src.state.types.RefEffect.__init__(self,ref_aval:ShapedArrayRef)
jax._src.state.types.RefEffect.replace(self,*,ref_aval:Optional[ShapedArrayRef]=None)
jax._src.state.types.ShapedArrayRef(self,shape,dtype)
jax._src.state.types.ShapedArrayRef.__eq__(self,other)
jax._src.state.types.ShapedArrayRef.__hash__(self)
jax._src.state.types.ShapedArrayRef.__init__(self,shape,dtype)
jax._src.state.types.ShapedArrayRef.__repr__(self)->str
jax._src.state.types.ShapedArrayRef._getitem(self,tracer,idx)->Array
jax._src.state.types.ShapedArrayRef._setitem(self,tracer,idx,value)->None
jax._src.state.types.ShapedArrayRef.at_least_vspace(self)
jax._src.state.types.ShapedArrayRef.get(tracer,idx=())
jax._src.state.types.ShapedArrayRef.join(self,other)
jax._src.state.types.ShapedArrayRef.set(tracer,value,idx=())
jax._src.state.types.WriteEffect(RefEffect)
jax._src.state.types.WriteEffect.__str__(self)
jax._src.state.types._map_ref(size,axis,aval)
jax._src.state.types._unmap_ref(size,axis_name,axis,aval)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/state/primitives.py----------------------------------------
A:jax._src.state.primitives.T->TypeVar('T')
A:jax._src.state.primitives.get_p->jax.core.Primitive('get')
A:jax._src.state.primitives.(_, non_slice_idx)->partition_list(indexed_dims_, idx)
A:jax._src.state.primitives.(idx, indexed_dims)->_unpack_idx(idx, ref.ndim)
A:jax._src.state.primitives.swap_p->jax.core.Primitive('swap')
A:jax._src.state.primitives.addupdate_p->jax.core.Primitive('addupdate')
A:jax._src.state.primitives.idx_shapes->tuple((i.shape for i in idx))
A:jax._src.state.primitives.shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.val_aval->jax.core.raise_to_shaped(val_aval)
A:jax._src.state.primitives.expected_output_shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.slice_shape->_get_slice_output_shape(ref_aval.shape, idx_shapes, indexed_dims)
A:jax._src.state.primitives.pp_ref->partial(pp.color, intensity=pp.Intensity.NORMAL, foreground=pp.Color.GREEN)
A:jax._src.state.primitives.idx_iter->iter(non_slice_idx)
A:jax._src.state.primitives.idx->_pp_idx(context, idx, eqn.params['indexed_dims'])
A:jax._src.state.primitives.lhs->jax.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.state.primitives.x_i->jax._src.pretty_printer.concat([pp.text(core.pp_var(x, context)), pp.text('['), idx, pp.text(']')])
A:jax._src.state.primitives.y->jax.core.pp_vars([y], context, print_shapes=settings.print_shapes)
A:jax._src.state.primitives.x_tangent->jax._src.ad_util.instantiate(x_tangent)
A:jax._src.state.primitives.x_bar->jax.core.Primitive('swap').bind(ref, ad_util.instantiate(g), *idx, **params)
A:jax._src.state.primitives.g->jax.core.Primitive('get').bind(ref, *idx, **params)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[get_p]->partial(_state_partial_eval_custom, get_p)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[swap_p]->partial(_state_partial_eval_custom, swap_p)
A:jax._src.state.primitives.pe.partial_eval_jaxpr_custom_rules[addupdate_p]->partial(_state_partial_eval_custom, addupdate_p)
A:jax._src.state.primitives.num_idxs_to_left->sum(indexed_dims[:ref_dim])
A:jax._src.state.primitives.idx_is_batched->any((i_dim is not batching.not_mapped for i_dim in idx_dims))
A:jax._src.state.primitives.idxs->tuple_insert(idxs, idx_place, iota)
A:jax._src.state.primitives.indexed_dims->tuple_insert(indexed_dims, ref_dim, True)
A:jax._src.state.primitives.idx_place->[i for (i, i_dim) in enumerate(indexed_dims) if i_dim].index(ref_dim)
A:jax._src.state.primitives.iota->jax.lax.broadcasted_iota(jnp.dtype('int32'), idxs_shape, 0)
A:jax._src.state.primitives.bdim_out->_output_bdim(indexed_dims, ref_dim, idxs_shape)
A:jax._src.state.primitives.val->jax.interpreters.batching.moveaxis(val, val_dim, 0)
jax._src.state.primitives.Ref(Protocol)
jax._src.state.primitives.Ref.ndim(self)->int
jax._src.state.primitives.Ref.shape(self)->Tuple[int, ...]
jax._src.state.primitives._addupdate_abstract_eval(ref_aval:ShapedArrayRef,val_aval:core.AbstractValue,*idx:core.ShapedArray,indexed_dims:Tuple[bool])
jax._src.state.primitives._addupdate_impl(ref:Ref,value:Array,*idx:int)
jax._src.state.primitives._addupdate_pp_rule(eqn,context,settings)
jax._src.state.primitives._addupdate_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._get_abstract_eval(ref_aval:ShapedArrayRef,*idx,indexed_dims)
jax._src.state.primitives._get_impl(ref:Ref,*idx:int,**_)
jax._src.state.primitives._get_jvp(primals:List[Any],tangents:List[Any],**params:Any)
jax._src.state.primitives._get_pp_rule(eqn,context,settings)
jax._src.state.primitives._get_slice_output_shape(in_shape:Tuple[int,...],idx_shapes:Tuple[Tuple[int,...],...],indexed_dims:Tuple[bool,...])->Tuple[int, ...]
jax._src.state.primitives._get_transpose(g,ref,*idx,**params)
jax._src.state.primitives._get_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._output_bdim(indexed_dims:Tuple[bool,...],ref_dim:int,idxs_shape:Tuple[int,...])
jax._src.state.primitives._pp_idx(context,non_slice_idx,indexed_dims)
jax._src.state.primitives._state_partial_eval_custom(prim,saveable,unks_in,inst_in,eqn)
jax._src.state.primitives._swap_abstract_eval(ref_aval:ShapedArrayRef,val_aval:core.AbstractValue,*idx:core.ShapedArray,indexed_dims:Tuple[bool])
jax._src.state.primitives._swap_impl(ref:Ref,value:Array,*idx:int,**_)
jax._src.state.primitives._swap_jvp(primals:List[Any],tangents:List[Any],**params:Any)
jax._src.state.primitives._swap_pp_rule(eqn,context,settings)
jax._src.state.primitives._swap_transpose(g,ref,x,*idx,**params)
jax._src.state.primitives._swap_vmap(batched_args,batched_dims,*,indexed_dims)
jax._src.state.primitives._unpack_idx(idx:Indexer,ndim:int)->Tuple[Tuple[int, ...], Tuple[bool, ...]]
jax._src.state.primitives.addupdate_jvp_rule(primals:List[Any],tangents:List[Any],**params:Any)
jax._src.state.primitives.addupdate_transpose(cts_in,ref,x,*idx,**params)
jax._src.state.primitives.ref_addupdate(ref:Ref,idx:Tuple[int,...],x:Array)->None
jax._src.state.primitives.ref_get(ref:Ref,idx:Tuple[Union[int,slice],...])->Array
jax._src.state.primitives.ref_set(ref:Ref,idx:Tuple[int,...],value:Array)->None
jax._src.state.primitives.ref_swap(ref:Ref,idx:Tuple[int,...],value:Array)->Array
jax._src.state.ref_addupdate(ref:Ref,idx:Tuple[int,...],x:Array)->None
jax._src.state.ref_get(ref:Ref,idx:Tuple[Union[int,slice],...])->Array
jax._src.state.ref_set(ref:Ref,idx:Tuple[int,...],value:Array)->None
jax._src.state.ref_swap(ref:Ref,idx:Tuple[int,...],value:Array)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/ann.py----------------------------------------
A:jax._src.lax.ann.dims->list(operand.shape)
A:jax._src.lax.ann.c->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt'))
A:jax._src.lax.ann.p0->jax.interpreters.xla.parameter(c, 0, xc.Shape.scalar_shape(op_type))
A:jax._src.lax.ann.p1->jax.interpreters.xla.parameter(c, 1, xc.Shape.scalar_shape(op_type))
A:jax._src.lax.ann.cmp_result->jax._src.lib.xla_client.ops.Lt(p0, p1)
A:jax._src.lax.ann.op_shape->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt')).get_shape(operand)
A:jax._src.lax.ann.op_dims->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt')).get_shape(operand).dimensions()
A:jax._src.lax.ann.op_type->jax._src.lib.xla_client.XlaBuilder('top_k_{}_comparator'.format('gt' if is_max_k else 'lt')).get_shape(operand).element_type()
A:jax._src.lax.ann.comparator->_comparator_builder(op_type, is_max_k)
A:jax._src.lax.ann.init_val_literal->_get_init_val_literal(op_type, is_max_k)
A:jax._src.lax.ann.iota->jax._src.lib.xla_client.ops.Iota(c, xc.Shape.array_shape(np.dtype(np.int32), op_dims), reduction_dimension)
A:jax._src.lax.ann.init_val->jax._src.lib.xla_client.ops.Constant(c, init_val_literal)
A:jax._src.lax.ann.init_arg->jax._src.lib.xla_client.ops.Constant(c, np.int32(-1))
A:jax._src.lax.ann.out->jax._src.lib.xla_client.ops.ApproxTopKFallback(c, [operand, iota], [init_val, init_arg], k, reduction_dimension, comparator, recall_target, aggregate_to_topk, reduction_input_size_override)
A:jax._src.lax.ann.(val_out, arg_out)->approx_min_k(operand, k, reduction_dimension, recall_target, reduction_input_size_override, aggregate_to_topk)
A:jax._src.lax.ann.tangent_out->jax._src.ad_util.Zero.from_value(val_out)
A:jax._src.lax.ann.rank->len(arg_shape)
A:jax._src.lax.ann.idx->tuple((arg_out if i == reduction_dimension else iotas[i] for i in range(rank)))
A:jax._src.lax.ann.approx_top_k_p->jax.core.Primitive('approx_top_k')
jax._src.lax.ann._approx_top_k_abstract_eval(operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_batch_rule(batch_operands,batch_axes,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_fallback_translation(ctx,avals_in,avals_out,operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_jvp(primals,tangents,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._approx_top_k_tpu_translation(ctx,avals_in,avals_out,operand,*,k,reduction_dimension,recall_target,is_max_k,reduction_input_size_override,aggregate_to_topk)
jax._src.lax.ann._comparator_builder(op_type,is_max_k)
jax._src.lax.ann._get_init_val_literal(op_type,is_max_k)
jax._src.lax.ann.approx_max_k(operand:Array,k:int,reduction_dimension:int=-1,recall_target:float=0.95,reduction_input_size_override:int=-1,aggregate_to_topk:bool=True)->Tuple[Array, Array]
jax._src.lax.ann.approx_min_k(operand:Array,k:int,reduction_dimension:int=-1,recall_target:float=0.95,reduction_input_size_override:int=-1,aggregate_to_topk:bool=True)->Tuple[Array, Array]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/fft.py----------------------------------------
A:jax._src.lax.fft.typ->_str_to_fft_type(fft_type)
A:jax._src.lax.fft.(x,)->_promote_dtypes_complex(x)
A:jax._src.lax.fft.fft_lengths->tuple(fft_lengths)
A:jax._src.lax.fft.dtype->_real_dtype(x.dtype)
A:jax._src.lax.fft.y->fft(x, xla_client.FftType.FFT, fft_lengths)
A:jax._src.lax.fft.dummy_primal->ShapeDtypeStruct(dummy_shape, _real_dtype(t.dtype))
A:jax._src.lax.fft.transpose->linear_transpose(partial(_naive_rfft, fft_lengths=fft_lengths), dummy_primal)
A:jax._src.lax.fft.(result,)->transpose(t)
A:jax._src.lax.fft.x->jax.interpreters.batching.moveaxis(x, bd, 0)
A:jax._src.lax.fft.full->partial(lax.full_like, t, dtype=x.dtype)
A:jax._src.lax.fft.mask->jax.lax.concatenate([full(1.0, shape=(1,)), full(2.0, shape=(n - 2 + is_odd,)), full(1.0, shape=(1 - is_odd,))], dimension=0)
A:jax._src.lax.fft.result->fft(t, fft_type, fft_lengths)
A:jax._src.lax.fft.fft_p->Primitive('fft')
jax._src.lax.fft._fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax._src.lax.fft._fft_impl(x,fft_type,fft_lengths)
jax._src.lax.fft._fft_lowering(ctx,x,*,fft_type,fft_lengths)
jax._src.lax.fft._fft_lowering_cpu(ctx,x,*,fft_type,fft_lengths)
jax._src.lax.fft._fft_transpose_rule(t,operand,fft_type,fft_lengths)
jax._src.lax.fft._irfft_transpose(t,fft_lengths)
jax._src.lax.fft._naive_rfft(x,fft_lengths)
jax._src.lax.fft._rfft_transpose(t,fft_lengths)
jax._src.lax.fft._str_to_fft_type(s:str)->xla_client.FftType
jax._src.lax.fft.fft(x,fft_type:Union[xla_client.FftType,str],fft_lengths:Sequence[int])
jax._src.lax.fft.fft_abstract_eval(x,fft_type,fft_lengths)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/utils.py----------------------------------------
A:jax._src.lax.utils.prim->jax.core.Primitive(name)
A:jax._src.lax.utils.weak_type->weak_type_rule(*avals, **kwargs)
A:jax._src.lax.utils.least_specialized->_max(map(type, avals), key=operator.attrgetter('array_abstraction_level'))
A:jax._src.lax.utils.out->jax.core.Primitive(name).impl(*[x.val for x in avals], **kwargs)
A:jax._src.lax.utils.shape->shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.weak_types->weak_type_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_vals->jax.core.Primitive(name).impl(*[x.val for x in avals], **kwargs)
A:jax._src.lax.utils.out_shapes->shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_dtypes->dtype_rule(*avals, **kwargs)
A:jax._src.lax.utils.out_named_shapes->named_shape_rule(*avals, **kwargs)
A:jax._src.lax.utils.xla_opname->''.join((term.capitalize() for term in prim.name.split('_')))
A:jax._src.lax.utils.op->getattr(xops, xla_opname)
jax._src.lax.utils._argnum_weak_type(*argnums)
jax._src.lax.utils._standard_weak_type_rule(*avals,**kwargs)
jax._src.lax.utils.standard_abstract_eval(prim,shape_rule,dtype_rule,weak_type_rule,named_shape_rule,*avals,**kwargs)
jax._src.lax.utils.standard_multi_result_abstract_eval(prim,shape_rule,dtype_rule,weak_type_rule,named_shape_rule,*avals,**kwargs)
jax._src.lax.utils.standard_named_shape_rule(*avals,**kwargs)
jax._src.lax.utils.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None,weak_type_rule=None,named_shape_rule=None)
jax._src.lax.utils.standard_translate(prim)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/convolution.py----------------------------------------
A:jax._src.lax.convolution.dnums->jax._src.lib.mlir.dialects.mhlo.ConvDimensionNumbers.get(input_batch_dimension=lhs_spec[0], input_feature_dimension=lhs_spec[1], input_spatial_dimensions=list(lhs_spec[2:]), kernel_output_feature_dimension=rhs_spec[0], kernel_input_feature_dimension=rhs_spec[1], kernel_spatial_dimensions=list(rhs_spec[2:]), output_batch_dimension=out_spec[0], output_feature_dimension=out_spec[1], output_spatial_dimensions=list(out_spec[2:]))
A:jax._src.lax.convolution.padding->list(map(np.sum, padding))
A:jax._src.lax.convolution.pad_a->int(np.ceil(pad_len / 2))
A:jax._src.lax.convolution.x->numpy.flip(x, axis)
A:jax._src.lax.convolution.ndims->len(lhs.shape)
A:jax._src.lax.convolution.dn->conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)
A:jax._src.lax.convolution.k_shape->numpy.take(rhs.shape, dn.rhs_spec)
A:jax._src.lax.convolution.effective_k_size->map(lambda k, r: (k - 1) * r + 1, k_sdims, rhs_dilation)
A:jax._src.lax.convolution.rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[1], rhs)
A:jax._src.lax.convolution.(quot, rem)->divmod(lhs_feature_count, feature_group_count)
A:jax._src.lax.convolution.lhs_trans->numpy.take(lhs_shape, lhs_perm)
A:jax._src.lax.convolution.rhs_trans->numpy.take(rhs_shape, rhs_perm)
A:jax._src.lax.convolution.out_trans->tuple((lhs_trans[0], rhs_trans[0]) + tuple(out_space))
A:jax._src.lax.convolution.input_dtype->jax._src.lax.lax.naryop_dtype_rule(lax._input_dtype, [lax._any, lax._any], 'conv_general_dilated', lhs, rhs)
A:jax._src.lax.convolution.(lhs_sdims, rhs_sdims, out_sdims)->map(_conv_sdims, dimension_numbers)
A:jax._src.lax.convolution.t_rhs_spec->_conv_spec_transpose(rhs_spec)
A:jax._src.lax.convolution.trans_dimension_numbers->ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
A:jax._src.lax.convolution.revd_weights->jax._src.lax.lax.rev(rhs, rhs_sdims)
A:jax._src.lax.convolution.out->_reshape_axis_into(out_spec[1], out_spec[1] + 1, out)
A:jax._src.lax.convolution.(lhs_trans, rhs_trans, out_trans)->map(_conv_spec_transpose, dimension_numbers)
A:jax._src.lax.convolution.shape->list(x.shape)
A:jax._src.lax.convolution.new_lhs->_reshape_axis_into(lhs_spec[0], lhs_spec[0], new_lhs)
A:jax._src.lax.convolution.new_rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[0], new_rhs)
A:jax._src.lax.convolution.conv_general_dilated_p->jax._src.lax.lax.standard_primitive(_conv_general_dilated_shape_rule, _conv_general_dilated_dtype_rule, 'conv_general_dilated')
A:jax._src.lax.convolution.k1->mul(lax.add(x_re, x_im), y_re)
A:jax._src.lax.convolution.k2->mul(x_re, lax.sub(y_im, y_re))
A:jax._src.lax.convolution.k3->mul(x_im, lax.add(y_re, y_im))
A:jax._src.lax.convolution.preferred_element_type->_real_dtype(preferred_element_type)
A:jax._src.lax.convolution.complex_conv->jax.interpreters.mlir.lower_fun(partial(_complex_mul, partial(conv_general_dilated, window_strides=window_strides, padding=padding, lhs_dilation=lhs_dilation, rhs_dilation=rhs_dilation, dimension_numbers=dimension_numbers, feature_group_count=feature_group_count, batch_group_count=batch_group_count, precision=precision, preferred_element_type=preferred_element_type)), multiple_results=False)
A:jax._src.lax.convolution.window_reversal->jax.interpreters.mlir.dense_bool_elements([False] * num_spatial_dims)
A:jax._src.lax.convolution.new_shape->list(np.delete(x.shape, src))
A:jax._src.lax.convolution.(size2, ragged)->divmod(shape[src], size1)
A:jax._src.lax.convolution.pads->jax._src.lax.lax.padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
A:jax._src.lax.convolution.lhs_padded->numpy.add(lhs_shape[2:], np.sum(np.array(pads).reshape(-1, 2), axis=1))
A:jax._src.lax.convolution.out_space->numpy.sum([unpad_out_space, padding], axis=0).tolist()
A:jax._src.lax.convolution.(lhs_perm, rhs_perm, out_perm)->map(getperm, dimension_numbers, charpairs)
A:jax._src.lax.convolution.iota->tuple(range(len(lhs_shape)))
A:jax._src.lax.convolution.(lhs_spec, rhs_spec, out_spec)->conv_general_permutations(dimension_numbers)
A:jax._src.lax.convolution.spatial->sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
A:jax._src.lax.convolution.proto->jax._src.lib.xla_client.ConvolutionDimensionNumbers()
A:jax._src.lax.convolution.lhs_dilated_shape->jax._src.lax.lax._dilate_shape(in_shape, lhs_dilation)
A:jax._src.lax.convolution.rhs_dilated_shape->jax._src.lax.lax._dilate_shape(window_dimensions, rhs_dilation)
A:jax._src.lax.convolution.out_dilated_shape->jax._src.lax.lax._dilate_shape(out_shape, window_strides)
A:jax._src.lax.convolution.(pads_lo, _)->jax._src.util.unzip2(padding)
A:jax._src.lax.convolution.pads_from_lhs->jax.core.diff_shape(out_dilated_shape, lhs_dilated_shape)
A:jax._src.lax.convolution.pads_from_rhs->jax.core.diff_shape(core.diff_shape(rhs_dilated_shape, pads_lo), (1,) * len(pads_lo))
A:jax._src.lax.convolution.pads_hi->jax.core.sum_shapes(pads_from_lhs, pads_from_rhs)
jax._src.lax.convolution.ConvDimensionNumbers(NamedTuple)
jax._src.lax.convolution._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax._src.lax.convolution._complex_mul(mul,x,y)
jax._src.lax.convolution._conv_general_dilated_batch_rule(batched_args,batch_dims,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,preferred_element_type,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_dtype_rule(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,preferred_element_type,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_lower(ctx,lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,preferred_element_type,expand_complex_convolutions=False,**unused_kwargs)
jax._src.lax.convolution._conv_general_dilated_shape_rule(lhs:core.ShapedArray,rhs:core.ShapedArray,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,**unused_kwargs)->Tuple[int, ...]
jax._src.lax.convolution._conv_general_dilated_transpose_lhs(g,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,lhs_shape,rhs_shape,precision,preferred_element_type)
jax._src.lax.convolution._conv_general_dilated_transpose_rhs(g,lhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,lhs_shape,rhs_shape,precision,preferred_element_type)
jax._src.lax.convolution._conv_general_proto(dimension_numbers)
jax._src.lax.convolution._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)->List[Tuple[int, int]]
jax._src.lax.convolution._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._src.lax.convolution._conv_transpose_padding(k,s,padding)
jax._src.lax.convolution._flip_axes(x,axes)
jax._src.lax.convolution._reshape_axis_into(src,dst,x)
jax._src.lax.convolution._reshape_axis_out_of(src,size1,x)
jax._src.lax.convolution.conv(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:str,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DType]=None)->Array
jax._src.lax.convolution.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)->ConvDimensionNumbers
jax._src.lax.convolution.conv_general_dilated(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,feature_group_count:int=1,batch_group_count:int=1,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DType]=None)->Array
jax._src.lax.convolution.conv_general_permutations(dimension_numbers)
jax._src.lax.convolution.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax._src.lax.convolution.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads,batch_group_count=1)
jax._src.lax.convolution.conv_transpose(lhs:Array,rhs:Array,strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,transpose_kernel:bool=False,precision:lax.PrecisionLike=None,preferred_element_type:Optional[DType]=None)->Array
jax._src.lax.convolution.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax._src.lax.convolution.conv_with_general_padding(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]],rhs_dilation:Optional[Sequence[int]],precision:lax.PrecisionLike=None,preferred_element_type:Optional[DType]=None)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/linalg.py----------------------------------------
A:jax._src.lax.linalg.TFun->TypeVar('TFun', bound=Callable[..., Any])
A:jax._src.lax.linalg.sig->inspect.signature(f)
A:jax._src.lax.linalg.x->jax._src.lib.mlir.dialects.mhlo.BroadcastInDimOp(ir.RankedTensorType.get(out_shape, x_type.element_type), x, bcast_dims(x_type.shape))
A:jax._src.lax.linalg.(v, w)->jax.interpreters.xla.apply_primitive(eigh_p, operand, lower=lower, sort_eigenvalues=sort_eigenvalues)
A:jax._src.lax.linalg.permutation->jax.lax.broadcasted_iota(jnp.int32, batch_dims + (m,), len(batch_dims))
A:jax._src.lax.linalg.(lu, pivots, permutation)->Primitive('lu').bind(a)
A:jax._src.lax.linalg.(q, r)->Primitive('qr').bind(x, full_matrices=False)
A:jax._src.lax.linalg.result->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax._src.lax.linalg.b->min(r - k, block_size)
A:jax._src.lax.linalg.out->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve').bind(a, b, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.lax.linalg.out_shape->list(lax_internal.broadcast_shapes(tuple(which_type.shape), tuple(x_type.shape), tuple(y_type.shape)))
A:jax._src.lax.linalg.(lu_, _, permutation)->lu(lax.stop_gradient(a))
A:jax._src.lax.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: _matvec_multiply(a, x), solve=lambda _, x: lu_solve(lu_, permutation, x, trans=0), transpose_solve=lambda _, x: lu_solve(lu_, permutation, x, trans=1))
A:jax._src.lax.linalg.t->f(c, *args, **kwargs)
A:jax._src.lax.linalg.L->jax._src.numpy.lax_numpy.tril(cholesky_p.bind(x))
A:jax._src.lax.linalg.l->jax.lax.pad(jnp.tril(lu[..., :, :k], -1), zero, l_padding)
A:jax._src.lax.linalg.tmp->triangular_solve(L, sigma_dot, left_side=False, transpose_a=True, conjugate_a=True, lower=True)
A:jax._src.lax.linalg.L_dot->jax.lax.batch_matmul(L, phi(triangular_solve(L, tmp, left_side=True, transpose_a=False, lower=True)), precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.cholesky_p->standard_unop(_float | _complex, 'cholesky')
A:jax._src.lax.linalg.(result, info)->potrf_impl(operand_aval.dtype, operand, lower=True)
A:jax._src.lax.linalg.ok->jax.interpreters.mlir.compare_mhlo(info, mlir.full_like_aval(0, ShapedArray(batch_dims, np.dtype(np.int32))), 'EQ', 'SIGNED')
A:jax._src.lax.linalg.dtype->jax._src.dtypes.canonicalize_dtype(dtype)
A:jax._src.lax.linalg.vlvr->raise_to_shaped(operand).update(shape=batch_dims + (n, n), dtype=dtype)
A:jax._src.lax.linalg.w->w_real.astype(a.dtype)
A:jax._src.lax.linalg.(w, vl, vr, info)->jax._src.lib.lapack.geev_mhlo(operand_aval.dtype, operand, jobvl=compute_left_eigenvectors, jobvr=compute_right_eigenvectors)
A:jax._src.lax.linalg.vl->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, vl, _nan_like_mhlo(aval))
A:jax._src.lax.linalg.vr->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, vr, _nan_like_mhlo(aval))
A:jax._src.lax.linalg.(l, v)->eig(a, compute_left_eigenvectors=False)
A:jax._src.lax.linalg.eig_p->Primitive('eig')
A:jax._src.lax.linalg.(w, v)->jax.interpreters.xla.apply_primitive(eigh_jacobi_p, operand, lower=lower, sort_eigenvalues=sort_eigenvalues)
A:jax._src.lax.linalg.v->jax._src.numpy.lax_numpy.empty(batch_shape + (0, 0), dtype=a.dtype)
A:jax._src.lax.linalg.eigh_jacobi_p->Primitive('eigh_jacobi')
A:jax._src.lax.linalg.(v, w, info)->syevd_impl(operand_aval.dtype, operand, lower=lower)
A:jax._src.lax.linalg.zeros->jax.interpreters.mlir.full_like_aval(0, ShapedArray(batch_dims, np.dtype(np.int32)))
A:jax._src.lax.linalg.(eig_vals, eig_vecs)->eigh_qdwh(x)
A:jax._src.lax.linalg.mask->jax._src.numpy.lax_numpy.logical_not(jnp.tri(n, k=-1, dtype=bool))
A:jax._src.lax.linalg.re->jax.lax.select(mask, lax.real(x), _T(lax.real(x)))
A:jax._src.lax.linalg.im_mask->jax._src.numpy.lax_numpy.logical_not(jnp.tri(n, k=0, dtype=bool))
A:jax._src.lax.linalg.im->jax.lax.select(mask, im, -_T(im))
A:jax._src.lax.linalg.(v, w_real)->Primitive('eigh').bind(symmetrize(a), lower=lower, sort_eigenvalues=sort_eigenvalues)
A:jax._src.lax.linalg.eye_n->jax._src.numpy.lax_numpy.eye(a.shape[-1], dtype=a.dtype)
A:jax._src.lax.linalg.dot->partial(lax.dot if g_a.ndim == 2 else lax.batch_matmul, precision=lax.Precision.HIGHEST)
A:jax._src.lax.linalg.vdag_adot_v->dot(dot(_H(v), a_dot), v)
A:jax._src.lax.linalg.dv->dot(v, jnp.multiply(Fmat, vdag_adot_v))
A:jax._src.lax.linalg.dw->jax._src.numpy.lax_numpy.real(jnp.diagonal(vdag_adot_v, axis1=-2, axis2=-1))
A:jax._src.lax.linalg.eigh_p->Primitive('eigh')
A:jax._src.lax.linalg.triangular_solve_dtype_rule->partial(naryop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex), 'triangular_solve')
A:jax._src.lax.linalg.g_a->jax.lax.neg(g_a)
A:jax._src.lax.linalg.cotangent_b->triangular_solve(a, cotangent, left_side=left_side, lower=lower, transpose_a=not transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.lax.linalg.y->jax._src.lib.mlir.dialects.mhlo.BroadcastInDimOp(ir.RankedTensorType.get(out_shape, y_type.element_type), y, bcast_dims(y_type.shape))
A:jax._src.lax.linalg.y_flat->jax._src.lib.mlir.dialects.mhlo.BroadcastInDimOp(ir.RankedTensorType.get(out_shape, y_type.element_type), y, bcast_dims(y_type.shape)).reshape(y.shape[:-3] + (y.shape[-3] * y.shape[-2], y.shape[-1]))
A:jax._src.lax.linalg.out_flat->triangular_solve(x, y_flat, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax._src.lax.linalg.size->max(m, n)
A:jax._src.lax.linalg.triangular_solve_p->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve')
A:jax._src.lax.linalg.a->_broadcasting_select_mhlo(ok, a, _nan_like_mhlo(a_aval))
A:jax._src.lax.linalg.alpha->jax.interpreters.mlir.ir_constant(np.array(1, dtype=a_aval.dtype))
A:jax._src.lax.linalg.batch->prod(batch_dims)
A:jax._src.lax.linalg.iotas->jax._src.numpy.lax_numpy.ix_(*(lax.iota(jnp.int32, b) for b in batch_dims + (1,)))
A:jax._src.lax.linalg.(result, _)->jax.lax.fori_loop(np.array(0, np.int32), np.array(k, np.int32), _lu_pivots_body_fn, (permutation, swaps))
A:jax._src.lax.linalg.pivots->raise_to_shaped(pivots)
A:jax._src.lax.linalg.permutations->raise_to_shaped(pivots).update(shape=batch_dims + (permutation_size,))
A:jax._src.lax.linalg.lu_pivots_to_permutation_p->Primitive('lu_pivots_to_permutation')
A:jax._src.lax.linalg.m_idx->jax._src.numpy.lax_numpy.arange(m)
A:jax._src.lax.linalg.n_idx->jax._src.numpy.lax_numpy.arange(n)
A:jax._src.lax.linalg.magnitude->jax._src.numpy.lax_numpy.abs(a[:, k])
A:jax._src.lax.linalg.i->jax._src.numpy.lax_numpy.argmax(jnp.where(m_idx >= k, magnitude, -jnp.inf))
A:jax._src.lax.linalg.pivot->raise_to_shaped(operand).update(shape=batch_dims + (min(m, n),), dtype=jnp.int32)
A:jax._src.lax.linalg.perm->raise_to_shaped(operand).update(shape=batch_dims + (m,), dtype=jnp.int32)
A:jax._src.lax.linalg.r->jax._src.numpy.lax_numpy.triu(r)
A:jax._src.lax.linalg.(block_pivot, block_perm, lu_block)->_lu_unblocked(a[k:, k:k + b])
A:jax._src.lax.linalg.fn->jax._src.api.vmap(fn)
A:jax._src.lax.linalg.(lu, pivot, perm)->jax.interpreters.xla.apply_primitive(lu_p, operand)
A:jax._src.lax.linalg.operand->raise_to_shaped(operand)
A:jax._src.lax.linalg.a_shape->jax._src.numpy.lax_numpy.shape(a)
A:jax._src.lax.linalg.k->min(m, n)
A:jax._src.lax.linalg.ndims->len(a_shape)
A:jax._src.lax.linalg.zero->xops.Constant(c, np.array(0, aval.dtype))
A:jax._src.lax.linalg.u_eye->jax.lax.pad(jnp.eye(n - k, n - k, dtype=dtype), zero, ((k, 0, 0), (k, 0, 0)))
A:jax._src.lax.linalg.la->triangular_solve(l, x, left_side=True, transpose_a=False, lower=True, unit_diagonal=True)
A:jax._src.lax.linalg.lau->triangular_solve(u, la, left_side=False, transpose_a=False, lower=False)
A:jax._src.lax.linalg.l_dot->jax._src.numpy.lax_numpy.matmul(l, jnp.tril(lau, -1))
A:jax._src.lax.linalg.u_dot->jax._src.numpy.lax_numpy.matmul(jnp.triu(lau), u)
A:jax._src.lax.linalg.(lu, pivot, info)->getrf_impl(operand_aval.dtype, operand)
A:jax._src.lax.linalg.lu->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, lu, _nan_like_mhlo(out_aval))
A:jax._src.lax.linalg.sub_ctx->ctx.replace(primitive=None, avals_in=[pivot_aval], avals_out=[perm_aval])
A:jax._src.lax.linalg.perm_fn->jax.interpreters.mlir.lower_fun(lambda x: lu_pivots_to_permutation(x, m), multiple_results=False)
A:jax._src.lax.linalg.(perm,)->perm_fn(sub_ctx, pivot)
A:jax._src.lax.linalg.lu_p->Primitive('lu')
A:jax._src.lax.linalg.(a_out, taus)->batched_geqrf_impl(a_aval.dtype, a)
A:jax._src.lax.linalg.taus->_broadcasting_select_mhlo(ok_taus, taus, _nan_like_mhlo(taus_aval))
A:jax._src.lax.linalg.(a_out, taus, info_geqrf)->geqrf_impl(a_aval.dtype, a)
A:jax._src.lax.linalg.a_out->_broadcasting_select_mhlo(ok_a, a_out, _nan_like_mhlo(a_aval))
A:jax._src.lax.linalg.geqrf_p->Primitive('geqrf')
A:jax._src.lax.linalg.(a, info_orgqr)->orgqr_impl(a_aval.dtype, a, taus)
A:jax._src.lax.linalg.orgqr_p->Primitive('orgqr')
A:jax._src.lax.linalg.q->orgqr(r, taus)
A:jax._src.lax.linalg.dx_rinv->triangular_solve(r, dx)
A:jax._src.lax.linalg.qt_dx_rinv->jax._src.numpy.lax_numpy.matmul(_H(q), dx_rinv)
A:jax._src.lax.linalg.qt_dx_rinv_lower->jax._src.numpy.lax_numpy.tril(qt_dx_rinv, -1)
A:jax._src.lax.linalg.I->jax.lax.expand_dims(jnp.eye(n, dtype=A.dtype), range(V.ndim - 2))
A:jax._src.lax.linalg.dr->jax._src.numpy.lax_numpy.matmul(qt_dx_rinv - do, r)
A:jax._src.lax.linalg.(r, taus)->geqrf(a)
A:jax._src.lax.linalg.qr_p->Primitive('qr')
A:jax._src.lax.linalg.iota_shape->jax._src.lib.xla_client.Shape.array_shape(xla.dtype_to_primitive_type(np.dtype(np.int32)), aval.shape)
A:jax._src.lax.linalg.s->fn(a)
A:jax._src.lax.linalg.u->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, u, _nan_like_mhlo(u_aval))
A:jax._src.lax.linalg.vt->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, vt, _nan_like_mhlo(vt_aval))
A:jax._src.lax.linalg.(s, U, Vt)->Primitive('svd').bind(A, full_matrices=False, compute_uv=True)
A:jax._src.lax.linalg.dS->jax._src.numpy.lax_numpy.matmul(jnp.matmul(Ut, dA), V)
A:jax._src.lax.linalg.ds->jax._src.numpy.lax_numpy.real(jnp.diagonal(dS, 0, -2, -1))
A:jax._src.lax.linalg.s_diffs_zeros->jax.lax.expand_dims(s_diffs_zeros, range(s_diffs.ndim - 2))
A:jax._src.lax.linalg.s_zeros->(s == 0).astype(s.dtype)
A:jax._src.lax.linalg.s_inv_mat->jax._src.numpy.lax_numpy.vectorize(jnp.diag, signature='(k)->(k,k)')(s_inv)
A:jax._src.lax.linalg.dU->jax._src.numpy.lax_numpy.matmul(U, F.astype(A.dtype) * (dSS + _H(dSS)) + dUdV_diag)
A:jax._src.lax.linalg.dV->jax._src.numpy.lax_numpy.matmul(V, F.astype(A.dtype) * (SdS + _H(SdS)))
A:jax._src.lax.linalg.(s, u, vt, info)->gesvd_impl(operand_aval.dtype, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax._src.lax.linalg.(u, s, vh)->fn(a)
A:jax._src.lax.linalg.outs->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax._src.lax.linalg.svd_p->Primitive('svd')
A:jax._src.lax.linalg.tridiagonal_solve_p->Primitive('tridiagonal_solve')
A:jax._src.lax.linalg.(_, tu_)->jax.lax.scan(lambda tu_, x: double(fwd1, (tu_, x)), du[0] / d[0], (d, du, dl), unroll=32)
A:jax._src.lax.linalg.(_, b_)->jax.lax.scan(lambda b_, x: double(fwd2, (b_, x)), b[0] / d[0], (b, d, prepend_zero(tu_), dl), unroll=32)
A:jax._src.lax.linalg.(_, x_)->jax.lax.scan(lambda x_, x: double(bwd1, (x_, x)), b_[-1], (b_[::-1], tu_[::-1]), unroll=32)
A:jax._src.lax.linalg.T->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, T, _nan_like_mhlo(ctx.avals_out[0]))
A:jax._src.lax.linalg.vs->_broadcasting_select_mhlo(mhlo.BroadcastInDimOp(ir.RankedTensorType.get(batch_dims + (1, 1), ir.IntegerType.get_signless(1)), ok, mlir.dense_int_elements(range(len(batch_dims)))).result, vs, _nan_like_mhlo(ctx.avals_out[1]))
A:jax._src.lax.linalg.gees_result->jax._src.lib.lapack.gees_mhlo(operand_aval.dtype, operand, jobvs=compute_schur_vectors, sort=sort_eig_vals, select=select_callable)
A:jax._src.lax.linalg.schur_p->Primitive('schur')
A:jax._src.lax.linalg.which->jax._src.lib.mlir.dialects.mhlo.BroadcastInDimOp(ir.RankedTensorType.get(out_shape, which_type.element_type), which, bcast_dims(which_type.shape))
jax._src.lax.linalg._H(x:Array)->Array
jax._src.lax.linalg._T(x:Array)->Array
jax._src.lax.linalg._broadcasting_select_mhlo(which,x,y)
jax._src.lax.linalg._check_solve_shapes(a:Array,b:Array)
jax._src.lax.linalg._cholesky_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._cholesky_cpu_gpu_lowering(potrf_impl,ctx,operand)
jax._src.lax.linalg._cholesky_jvp_rule(primals,tangents)
jax._src.lax.linalg._cholesky_lowering(ctx,x)
jax._src.lax.linalg._eig_cpu_lowering(ctx,operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg._eigh_abstract_eval(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_batching_rule(batched_args,batch_dims,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_cpu_gpu_lowering(syevd_impl,ctx,operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_impl(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jacobi_abstract_eval(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jacobi_impl(operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jacobi_translation_rule(ctx,avals_in,avals_out,operand,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_jvp_rule(primals,tangents,*,lower,sort_eigenvalues)
jax._src.lax.linalg._eigh_tpu_impl(x,*,lower,sort_eigenvalues)
jax._src.lax.linalg._empty_svd(a,*,full_matrices,compute_uv)
jax._src.lax.linalg._eye_like_xla(c,aval)
jax._src.lax.linalg._generic_lu_pivots_to_permutation(swaps,permutation_size)
jax._src.lax.linalg._geqrf_abstract_eval(operand)
jax._src.lax.linalg._geqrf_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._geqrf_cpu_gpu_lowering(geqrf_impl,batched_geqrf_impl,ctx,a)
jax._src.lax.linalg._geqrf_translation_rule(ctx,avals_in,avals_out,operand)
jax._src.lax.linalg._lu_abstract_eval(operand)
jax._src.lax.linalg._lu_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._lu_blocked(a,block_size=128)
jax._src.lax.linalg._lu_cpu_gpu_lowering(getrf_impl,ctx,operand)
jax._src.lax.linalg._lu_impl(operand)
jax._src.lax.linalg._lu_jvp_rule(primals,tangents)
jax._src.lax.linalg._lu_pivots_body_fn(i,permutation_and_swaps)
jax._src.lax.linalg._lu_pivots_to_permutation_abstract_eval(pivots,*,permutation_size)
jax._src.lax.linalg._lu_pivots_to_permutation_batching_rule(batched_args,batch_dims,*,permutation_size)
jax._src.lax.linalg._lu_pivots_to_permutation_gpu_lowering(lowering,ctx,pivots,*,permutation_size)
jax._src.lax.linalg._lu_python(x)
jax._src.lax.linalg._lu_solve(lu:Array,permutation:Array,b:Array,trans:int)->Array
jax._src.lax.linalg._lu_solve_core(lu:Array,permutation:Array,b:Array,trans:int)->Array
jax._src.lax.linalg._lu_tpu_translation_rule(ctx,avals_in,avals_out,operand)
jax._src.lax.linalg._lu_unblocked(a)
jax._src.lax.linalg._matvec_multiply(a:Array,b:Array)->Array
jax._src.lax.linalg._nan_like_mhlo(aval)
jax._src.lax.linalg._orgqr_abstract_eval(a,taus)
jax._src.lax.linalg._orgqr_batching_rule(batched_args,batch_dims)
jax._src.lax.linalg._orgqr_cpu_gpu_lowering(orgqr_impl,ctx,a,taus)
jax._src.lax.linalg._orgqr_translation_rule(ctx,avals_in,avals_out,a,taus)
jax._src.lax.linalg._qr_abstract_eval(operand,*,full_matrices)
jax._src.lax.linalg._qr_batching_rule(batched_args,batch_dims,*,full_matrices)
jax._src.lax.linalg._qr_impl(operand,*,full_matrices)
jax._src.lax.linalg._qr_lowering(a,*,full_matrices)
jax._src.lax.linalg._qr_translation_rule(ctx,avals_in,avals_out,operand,*,full_matrices)
jax._src.lax.linalg._schur_abstract_eval(operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_batching_rule(batched_args,batch_dims,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_cpu_lowering(ctx,operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_impl(operand,*,compute_schur_vectors,sort_eig_vals,select_callable)
jax._src.lax.linalg._schur_jvp_rule(primals,tangents,*,compute_schur_vectors,sort_eig_vals)
jax._src.lax.linalg._schur_lowering(ctx,*args,**kwargs)
jax._src.lax.linalg._solve(a:Array,b:Array)->Array
jax._src.lax.linalg._svd_abstract_eval(operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_batching_rule(batched_args,batch_dims,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_cpu_gpu_lowering(gesvd_impl,ctx,operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_impl(operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_jvp_rule(primals,tangents,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_tpu(a,*,full_matrices,compute_uv)
jax._src.lax.linalg._svd_tpu_lowering_rule(ctx,operand,*,full_matrices,compute_uv)
jax._src.lax.linalg._triangular_solve_cpu_lower(ctx,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_gpu_lower(trsm_impl,ctx,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._triangular_solve_lowering(ctx,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg._tridiagonal_solve_gpu_lowering(lowering,ctx,dl,d,du,b,*,m,n,ldb,t)
jax._src.lax.linalg._tridiagonal_solve_jax(dl,d,du,b,**kw)
jax._src.lax.linalg._unpack_tuple(f:TFun,n:int)->TFun
jax._src.lax.linalg._warn_on_positional_kwargs(f:TFun)->TFun
jax._src.lax.linalg._zeros_like_xla(c,aval)
jax._src.lax.linalg.cholesky(x:Array,*,symmetrize_input:bool=True)->Array
jax._src.lax.linalg.eig(x:ArrayLike,*,compute_left_eigenvectors:bool=True,compute_right_eigenvectors:bool=True)->List[Array]
jax._src.lax.linalg.eig_abstract_eval(operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_batching_rule(batched_args,batch_dims,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_impl(operand,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_jvp_rule(primals,tangents,*,compute_left_eigenvectors,compute_right_eigenvectors)
jax._src.lax.linalg.eig_lower(*args,**kw)
jax._src.lax.linalg.eigh(x:Array,*,lower:bool=True,symmetrize_input:bool=True,sort_eigenvalues:bool=True)->Tuple[Array, Array]
jax._src.lax.linalg.eigh_jacobi(x:ArrayLike,*,lower:bool=True,sort_eigenvalues:bool=True)->Tuple[Array, Array]
jax._src.lax.linalg.geqrf(a:ArrayLike)->Tuple[Array, Array]
jax._src.lax.linalg.lu(x:ArrayLike)->Tuple[Array, Array, Array]
jax._src.lax.linalg.lu_pivots_to_permutation(pivots:ArrayLike,permutation_size:int)->Array
jax._src.lax.linalg.lu_solve(lu:ArrayLike,permutation:ArrayLike,b:ArrayLike,trans:int=0)->Array
jax._src.lax.linalg.orgqr(a:ArrayLike,taus:ArrayLike)->Array
jax._src.lax.linalg.qr(x:ArrayLike,*,full_matrices:bool=True)->Tuple[Array, Array]
jax._src.lax.linalg.qr_jvp_rule(primals,tangents,*,full_matrices)
jax._src.lax.linalg.schur(x:ArrayLike,*,compute_schur_vectors:bool=True,sort_eig_vals:bool=False,select_callable:Optional[Callable[...,Any]]=None)->Tuple[Array, Array]
jax._src.lax.linalg.svd(x:ArrayLike,*,full_matrices:bool=True,compute_uv:bool=True)->Union[Array, Tuple[Array, Array, Array]]
jax._src.lax.linalg.symmetrize(x:Array)->Array
jax._src.lax.linalg.triangular_solve(a:ArrayLike,b:ArrayLike,*,left_side:bool=False,lower:bool=False,transpose_a:bool=False,conjugate_a:bool=False,unit_diagonal:bool=False)->Array
jax._src.lax.linalg.triangular_solve_batching_rule(batched_args,batch_dims,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg.triangular_solve_jvp_rule_a(g_a,ans,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg.triangular_solve_shape_rule(a,b,*,left_side=False,**unused_kwargs)
jax._src.lax.linalg.triangular_solve_transpose_rule(cotangent,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax._src.lax.linalg.tridiagonal_solve(dl:Array,d:Array,du:Array,b:Array)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/parallel.py----------------------------------------
A:jax._src.lax.parallel.(leaves, treedef)->jax.tree_util.tree_flatten(x)
A:jax._src.lax.parallel.axis_index_groups->_canonicalize_axis_index_groups(axis_index_groups)
A:jax._src.lax.parallel.out_flat->jax.core.AxisPrimitive('pmin').bind(*leaves, axes=axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.x->jax._src.lib.mlir.dialects.mhlo.BroadcastInDimOp(mlir.aval_to_ir_type(x_aval.update(shape=new_shape)), x, mlir.dense_int_elements(broadcast_dimensions))
A:jax._src.lax.parallel.n->psum(1, axis_name=axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.idx->jax._src.lax.lax.expand_dims(idx, (-1,))
A:jax._src.lax.parallel.validx->jax._src.numpy.lax_numpy.where(val == x, idx, dtypes.iinfo(dtypes.dtype(idx)).max)
A:jax._src.lax.parallel.axis_space->range(sum((len(group) for group in axis_index_groups)))
A:jax._src.lax.parallel.group_size->len(replica_groups[0])
A:jax._src.lax.parallel.result->jax.core.AxisPrimitive('pgather').bind(src_last_batched, idx, axes=axes)
A:jax._src.lax.parallel.pos_contract->unzip2(((lhs_subs.index(n), rhs_subs.index(n)) for n in subs_contract))
A:jax._src.lax.parallel.pos_batch->unzip2(((lhs_subs.index(n), rhs_subs.index(n)) for n in subs_batch))
A:jax._src.lax.parallel.(in_spec, out_spec)->spec.split('->')
A:jax._src.lax.parallel.(all_in_subs, all_in_named)->unzip2(XeinsumSpecParser(in_spec).parse_args())
A:jax._src.lax.parallel.((out_subs, out_named),)->XeinsumSpecParser(out_spec).parse_args()
A:jax._src.lax.parallel.xs->list(operands)
A:jax._src.lax.parallel.other_named->set().union(*[named for (i, named) in enumerate(all_in_named) if i != idx])
A:jax._src.lax.parallel.other_subs->set().union(*[subs for (i, subs) in enumerate(all_in_subs) if i != idx])
A:jax._src.lax.parallel.subs_reduce->list(set(in_subs) - {*out_subs, *other_subs})
A:jax._src.lax.parallel.named_reduce_axes->list(set(in_named) - {*out_named, *other_named})
A:jax._src.lax.parallel.xs[idx]->psum(xs[idx], axis_name=subs_reduce_axes + named_reduce_axes)
A:jax._src.lax.parallel.named_contract->list((set(lhs_named) & set(rhs_named)) - set(out_named))
A:jax._src.lax.parallel.end->self.spec.index(',', self.pos, end)
A:jax._src.lax.parallel.(subscript, cont)->self.parse_subscript()
A:jax._src.lax.parallel.axis_name->self.parse_axis_name()
A:jax._src.lax.parallel.(cont, result)->self.parse_arg()
A:jax._src.lax.parallel.result[pname]->sum(((name,) if isinstance(name, int) else subst(name) for name in axis_name), ())
A:jax._src.lax.parallel.(unmapped_axes, unmapped_vals_in)->transform_unmapped(0, unmapped_vals_in)
A:jax._src.lax.parallel.unmapped_vals_out->prim.bind(*unmapped_vals_in, axes=unmapped_axes, axis_index_groups=None)
A:jax._src.lax.parallel.(mapped_axes, mapped_vals_in)->transform_mapped(0, mapped_vals_in)
A:jax._src.lax.parallel.mapped_vals_out->prim.bind(*mapped_vals_in, axes=mapped_axes, axis_index_groups=None)
A:jax._src.lax.parallel.vals_out->_reduction_with_positional_batcher(prim, vals_in, dims_in, axis_index_groups, lambda d, d_vals_in: (tuple((axis for axis in axes if axis != frame_name)), [if_unmapped(v, axis_size) for v in d_vals_in]), lambda d, d_vals_in: (tuple((axis + (axis >= d) if isinstance(axis, int) else axis if axis != frame_name else d for axis in axes)), d_vals_in))
A:jax._src.lax.parallel.replica_groups->_replica_groups(ctx.module_context.axis_env, axis_name, axis_index_groups)
A:jax._src.lax.parallel.pos_axes->tuple((axis for axis in axes if isinstance(axis, int)))
A:jax._src.lax.parallel.len_0->len(axis_index_groups[0])
A:jax._src.lax.parallel.reducer->jax.interpreters.mlir.lower_fun(pos_fn, multiple_results=False)
A:jax._src.lax.parallel.aval_out->aval.update(shape=np.delete(np.array(aval.shape, dtype=np.int64), positional_axes))
A:jax._src.lax.parallel.reducer_ctx->ctx.replace(primitive=None, avals_in=[scalar_aval] * 2, avals_out=[scalar_aval])
A:jax._src.lax.parallel.args->map(_positional_reduce, ctx.avals_in, args)
A:jax._src.lax.parallel.is_manual->isinstance(axis_context, mlir.SPMDAxisContext)
A:jax._src.lax.parallel.channel->ctx.module_context.new_channel()
A:jax._src.lax.parallel.other_args->dict(channel_handle=mhlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE), use_global_device_ids=ir.BoolAttr.get(True))
A:jax._src.lax.parallel.op->jax._src.lib.mlir.dialects.mhlo.ReduceScatterOp(mlir.aval_to_ir_type(x_aval.update(shape=scatter_out_shape)), x, scatter_dimension=mlir.i64_attr(scatter_dimension), replica_groups=_replica_groups_mhlo(replica_groups), channel_handle=None)
A:jax._src.lax.parallel.scalar_aval->jax.core.raise_to_shaped(x).update(shape=())
A:jax._src.lax.parallel.scalar_type->jax.interpreters.mlir.aval_to_ir_type(scalar_aval)
A:jax._src.lax.parallel.reducer_block->jax._src.lib.mlir.dialects.mhlo.ReduceScatterOp(mlir.aval_to_ir_type(x_aval.update(shape=scatter_out_shape)), x, scatter_dimension=mlir.i64_attr(scatter_dimension), replica_groups=_replica_groups_mhlo(replica_groups), channel_handle=None).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.parallel.lower_reducer->jax.interpreters.mlir.lower_fun(prim.bind, multiple_results=False)
A:jax._src.lax.parallel.out_nodes->lower_reducer(reducer_ctx, *([a] for a in reducer_block.arguments))
A:jax._src.lax.parallel.cts->map(broadcast_positional, cts, args)
A:jax._src.lax.parallel.(nonzero_out_cts, treedef)->jax.tree_util.tree_flatten(cts)
A:jax._src.lax.parallel.nonzero_in_cts->jax.core.AxisPrimitive('psum').bind(*nonzero_out_cts, axes=tuple(named_axes), axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.psum_p->jax.core.AxisPrimitive('psum')
A:jax._src.lax.parallel.batching.primitive_batchers[psum_p]->partial(_reduction_batcher, psum_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[psum_p]->partial(_batched_reduction_collective, psum_p, lambda v, axis_size: axis_size * v)
A:jax._src.lax.parallel.core.axis_substitution_rules[psum_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.size->prod([core.axis_frame(name).size for name in named_axes])
A:jax._src.lax.parallel.pmax_p->jax.core.AxisPrimitive('pmax')
A:jax._src.lax.parallel.batching.primitive_batchers[pmax_p]->partial(_reduction_batcher, pmax_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[pmax_p]->partial(_batched_reduction_collective, pmax_p, lambda v, axis_size: v)
A:jax._src.lax.parallel.core.axis_substitution_rules[pmax_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.pmin_p->jax.core.AxisPrimitive('pmin')
A:jax._src.lax.parallel.batching.primitive_batchers[pmin_p]->partial(_reduction_batcher, pmin_p)
A:jax._src.lax.parallel.batching.axis_primitive_batchers[pmin_p]->partial(_batched_reduction_collective, pmin_p, lambda v, axis_size: v)
A:jax._src.lax.parallel.core.axis_substitution_rules[pmin_p]->partial(_subst_all_names_in_param, 'axes')
A:jax._src.lax.parallel.(srcs, dsts)->unzip2(perm)
A:jax._src.lax.parallel.full_perm->full_perm.reshape((-1, 2)).reshape((-1, 2))
A:jax._src.lax.parallel.grp->list(sorted(grp))
A:jax._src.lax.parallel.inverse_perm->list(zip(dsts, srcs))
A:jax._src.lax.parallel.remaining_axes->tuple((axis for axis in axis_name if axis != frame_name))
A:jax._src.lax.parallel.perm_indices->numpy.zeros(axis_size, dtype=int)
A:jax._src.lax.parallel.ppermute_p->jax.core.AxisPrimitive('ppermute')
A:jax._src.lax.parallel.batching.primitive_batchers[ppermute_p]->partial(_collective_batcher, ppermute_p)
A:jax._src.lax.parallel.core.axis_substitution_rules[ppermute_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.new_shape->list(x_aval.shape)
A:jax._src.lax.parallel.cur_device_id->axis_index(axis_name)
A:jax._src.lax.parallel.flat_groups->numpy.array(axis_index_groups).flatten()
A:jax._src.lax.parallel.full->all_gather(x, axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.sliced->jax._src.lax.slicing.dynamic_slice_in_dim(full, tile_base_idx, tile_size, split_axis + 1)
A:jax._src.lax.parallel.split_count->len(replica_groups[0])
A:jax._src.lax.parallel.lowering->jax.interpreters.mlir.lower_fun(_all_gather_via_psum, multiple_results=False)
A:jax._src.lax.parallel.pos->self.parse_axis_name().index(frame_name)
A:jax._src.lax.parallel.x_concat->_foldaxis(concat_axis, _moveaxis(d, concat_axis, x))
A:jax._src.lax.parallel.input_aval->raise_to_shaped(x)
A:jax._src.lax.parallel.shape->list(src.shape)
A:jax._src.lax.parallel.all_to_all_p->jax.core.AxisPrimitive('all_to_all')
A:jax._src.lax.parallel.core.axis_substitution_rules[all_to_all_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.axis_size->psum(1, axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.bind->partial(reduce_scatter_p.bind, axis_name=axis_name, scatter_dimension=scatter_dimension, axis_index_groups=axis_index_groups, axis_size=axis_size, tiled=tiled)
A:jax._src.lax.parallel.out->jax.core.AxisPrimitive('pdot').bind(x, y, axis_name=axis_name, pos_contract=pos_contract, pos_batch=pos_batch, precision=precision)
A:jax._src.lax.parallel.index->_index_in_group(axis_name, axis_index_groups)
A:jax._src.lax.parallel.outs->jax._src.lax.lax.squeeze(outs, [scatter_dimension])
A:jax._src.lax.parallel.sums->psum(outs, axis_name, axis_index_groups=axis_index_groups)
A:jax._src.lax.parallel.x_aval->jax.core.raise_to_shaped(x)
A:jax._src.lax.parallel.out_shape->list(np.shape(x))
A:jax._src.lax.parallel.y->_foldaxis(all_gather_dimension, y)
A:jax._src.lax.parallel.all_gather_p->jax.core.AxisPrimitive('all_gather')
A:jax._src.lax.parallel.core.axis_substitution_rules[all_gather_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.scatter_out_shape->list(x_aval.shape)
A:jax._src.lax.parallel.reduce_scatter_p->jax.core.AxisPrimitive('reduce_scatter')
A:jax._src.lax.parallel.axis_pos->list(axis_env.names).index(axis_name)
A:jax._src.lax.parallel.div->jax.interpreters.mlir.ir_constant(np.array(nreplicas * prod(axis_env.sizes[axis_pos + 1:]), dtype=np.uint32))
A:jax._src.lax.parallel.mod->jax.interpreters.mlir.ir_constant(np.array(axis_env.sizes[axis_pos], dtype=np.uint32))
A:jax._src.lax.parallel.unsigned_index->jax._src.lib.mlir.dialects.mhlo.RemOp(mhlo.DivOp(mhlo.ReplicaIdOp(), div), mod)
A:jax._src.lax.parallel.frame->jax.core.axis_frame(name)
A:jax._src.lax.parallel.axis_index_p->jax.core.Primitive('axis_index')
A:jax._src.lax.parallel.core.axis_substitution_rules[axis_index_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.trace->jax.core.axis_frame(name).main_trace.with_cur_sublevel()
A:jax._src.lax.parallel.pdot_p->jax.core.AxisPrimitive('pdot')
A:jax._src.lax.parallel.core.axis_substitution_rules[pdot_p]->partial(_subst_all_names_in_param, 'axis_name')
A:jax._src.lax.parallel.common_named_shape->jax.core.join_named_shapes(x.named_shape, y.named_shape)
A:jax._src.lax.parallel.remaining_axis_names->tuple((n for n in axis_name if n != frame_name))
A:jax._src.lax.parallel.((pos_contract, pos_batch), result_batch_dim)->jax._src.lax.lax._dot_general_batch_dim_nums((x.ndim, y.ndim), dims_in, [pos_contract, pos_batch])
A:jax._src.lax.parallel.local_out->jax._src.lax.lax.dot_general(x, y, dimension_numbers=(pos_contract, pos_batch), precision=precision, preferred_element_type=None)
A:jax._src.lax.parallel.src_axes_front->moveaxis(src, axes, range(len(axes)))
A:jax._src.lax.parallel.src_one_axis_front->moveaxis(src, axes, range(len(axes))).reshape((-1,) + non_axes_shape)
A:jax._src.lax.parallel.offset_dims->tuple(range(idx.ndim - 1, idx.ndim + src_one_axis_front.ndim - 2))
A:jax._src.lax.parallel.dnums->jax._src.lax.slicing.GatherDimensionNumbers(offset_dims=offset_dims, collapsed_slice_dims=(0,), start_index_map=(0,))
A:jax._src.lax.parallel.src_last_batched->moveaxis(src, dsrc, -1)
A:jax._src.lax.parallel.new_axes->tuple((dsrc if axis == frame_name else axis + (dsrc <= axis) if isinstance(axis, int) else axis for axis in axes))
A:jax._src.lax.parallel.pgather_p->jax.core.AxisPrimitive('pgather')
A:jax._src.lax.parallel.core.axis_substitution_rules[pgather_p]->partial(_subst_all_names_in_param, 'axes')
jax._src.lax.parallel.XeinsumSpecParser(self,spec:str)
jax._src.lax.parallel.XeinsumSpecParser.__init__(self,spec:str)
jax._src.lax.parallel.XeinsumSpecParser.cur(self)
jax._src.lax.parallel.XeinsumSpecParser.eof(self)
jax._src.lax.parallel.XeinsumSpecParser.maybe_take(self,char:str,on_eof:bool=False)
jax._src.lax.parallel.XeinsumSpecParser.parse_arg(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_args(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_axis_name(self)
jax._src.lax.parallel.XeinsumSpecParser.parse_subscript(self)
jax._src.lax.parallel._all_gather_abstract_eval(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_batched_collective(frame_size,frame_name,_,vals_in,dims_in,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_batcher(vals_in,dims_in,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_impl(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_lowering(ctx,x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_transpose_rule(cts,x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_gather_via_psum(x,*,all_gather_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._all_to_all_abstract_eval(x,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_batched_collective(axis_size,frame_name,_,vals_in,dims_in,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_batcher(vals_in,dims_in,*,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_lowering(ctx,x,*,split_axis,concat_axis,axis_name,axis_index_groups)
jax._src.lax.parallel._all_to_all_transpose_rule(cts,x,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._all_to_all_via_all_gather(x,*,axis_name,split_axis,concat_axis,axis_index_groups)
jax._src.lax.parallel._allreduce_abstract_eval(*args,axes,axis_index_groups)
jax._src.lax.parallel._allreduce_impl(pos_reducer,*args,axes,axis_index_groups)
jax._src.lax.parallel._allreduce_lowering(prim,pos_fn,ctx,*args,axes,axis_index_groups)
jax._src.lax.parallel._axis_index_abstract_eval(*,axis_name)
jax._src.lax.parallel._axis_index_bind(*,axis_name)
jax._src.lax.parallel._axis_index_lowering(ctx,*,axis_name)
jax._src.lax.parallel._axis_index_of_val(x,val,axis_name)
jax._src.lax.parallel._batched_reduction_collective(prim,if_unmapped,axis_size,frame_name,_,vals_in,dims_in,axes,axis_index_groups)
jax._src.lax.parallel._build_axis_index_lowering_mhlo(axis_name,axis_env)
jax._src.lax.parallel._canonicalize_axis_index_groups(axis_index_groups)
jax._src.lax.parallel._collective_batcher(prim,args,dims,**params)
jax._src.lax.parallel._expand(dim,size,index,tiled,x)
jax._src.lax.parallel._foldaxis(axis,x)
jax._src.lax.parallel._index_in_group(axis_name,axis_index_groups)
jax._src.lax.parallel._moveaxis(src,dst,x)
jax._src.lax.parallel._pdot_abstract_eval(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_impl(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_lowering(x,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_transpose_lhs(g,y,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_transpose_rhs(g,x,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_vmap_batching_rule(vals_in,dims_in,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pdot_vmap_collective_rule(axis_size,frame_name,_,vals_in,dims_in,*,axis_name,pos_contract,pos_batch,precision)
jax._src.lax.parallel._pgather_abstract_eval(src,idx,*,axes)
jax._src.lax.parallel._pgather_batcher(vals_in,dims_in,*,axes)
jax._src.lax.parallel._pgather_collective_batcher(axis_size,frame_name,_,vals_in,dims_in,*,axes)
jax._src.lax.parallel._pgather_impl(src,idx,*,axes)
jax._src.lax.parallel._pgather_parallel_lowering(ctx,src,idx,*,axes)
jax._src.lax.parallel._ppermute_batcher(axis_size,frame_name,_,vals_in,dims_in,axis_name,perm)
jax._src.lax.parallel._ppermute_lowering(ctx,x,*,axis_name,perm)
jax._src.lax.parallel._ppermute_transpose_rule(t,x,perm,axis_name)
jax._src.lax.parallel._psum_transpose_rule(cts,*args,axes,axis_index_groups)
jax._src.lax.parallel._reduce_scatter_abstract_eval(x,*,axis_name,scatter_dimension,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_lowering(prim,reducer,ctx,x,*,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduce_scatter_via_reducer(x,*,reducer,scatter_dimension,axis_name,axis_index_groups,axis_size,tiled)
jax._src.lax.parallel._reduction_batcher(prim,vals_in,dims_in,*,axes,axis_index_groups)
jax._src.lax.parallel._reduction_with_positional_batcher(prim,vals_in,dims_in,axis_index_groups,transform_unmapped,transform_mapped)
jax._src.lax.parallel._replica_groups(axis_env,axis_name,axis_index_groups)
jax._src.lax.parallel._replica_groups_mhlo(replica_groups:Sequence[Sequence[int]])->ir.DenseIntElementsAttr
jax._src.lax.parallel._splitaxis(axis,factor,x)
jax._src.lax.parallel._subst_all_names_in_param(pname:str,params:core.ParamDict,subst:core.AxisSubst,traverse:bool)->core.ParamDict
jax._src.lax.parallel._validate_reduce_axis_index_groups(axis_index_groups)
jax._src.lax.parallel._vmap_process_axis_index(self,frame)
jax._src.lax.parallel.all_gather(x,axis_name,*,axis_index_groups=None,axis=0,tiled=False)
jax._src.lax.parallel.all_to_all(x,axis_name,split_axis,concat_axis,*,axis_index_groups=None,tiled=False)
jax._src.lax.parallel.axis_index(axis_name)
jax._src.lax.parallel.pargmax(x,axis_name)
jax._src.lax.parallel.pargmin(x,axis_name)
jax._src.lax.parallel.pdot(x,y,axis_name,pos_contract=((),()),pos_batch=((),()),precision=None)
jax._src.lax.parallel.pgather(src,idx,axes:Union[int,AxisName])
jax._src.lax.parallel.pmax(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.pmean(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.pmin(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.ppermute(x,axis_name,perm)
jax._src.lax.parallel.pshuffle(x,axis_name,perm)
jax._src.lax.parallel.psum(x,axis_name,*,axis_index_groups=None)
jax._src.lax.parallel.psum_bind(*args,axes,axis_index_groups)
jax._src.lax.parallel.psum_scatter(x,axis_name,*,scatter_dimension=0,axis_index_groups=None,tiled=False)
jax._src.lax.parallel.pswapaxes(x,axis_name,axis,*,axis_index_groups=None)
jax._src.lax.parallel.xeinsum(spec:str,*operands)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/windowed_reductions.py----------------------------------------
A:jax._src.lax.windowed_reductions.(flat_operands, operand_tree)->jax.tree_util.tree_flatten(operand)
A:jax._src.lax.windowed_reductions.(flat_init_values, init_value_tree)->jax.tree_util.tree_flatten(init_value)
A:jax._src.lax.windowed_reductions.padding->tuple(padding)
A:jax._src.lax.windowed_reductions.monoid_reducer->_get_monoid_window_reducer(computation, flat_init_values)
A:jax._src.lax.windowed_reductions.flat_init_avals->map(lax._abstractify, flat_init_values)
A:jax._src.lax.windowed_reductions.(jaxpr, consts, out_tree)->jax._src.lax.lax._variadic_reduction_jaxpr(computation, tuple(flat_init_avals), init_value_tree)
A:jax._src.lax.windowed_reductions.out_flat->jax.core.Primitive('reduce_window').bind(*flat_operands + flat_init_values, jaxpr=jaxpr, consts=consts, window_dimensions=tuple(window_dimensions), window_strides=tuple(window_strides), padding=padding, base_dilation=tuple(base_dilation), window_dilation=tuple(window_dilation))
A:jax._src.lax.windowed_reductions.aval->jax.core.get_aval(x)
A:jax._src.lax.windowed_reductions.init_value->jax._src.lax.lax._const(operand, 1)
A:jax._src.lax.windowed_reductions.(jaxpr, consts)->jax._src.lax.lax._reduction_jaxpr(lax.mul, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(out,)->jax.core.Primitive('reduce_window').bind(operand, init_value, jaxpr=jaxpr, consts=consts, window_dimensions=tuple(window_dimensions), window_strides=tuple(window_strides), padding=tuple(padding), base_dilation=tuple(base_dilation), window_dilation=tuple(window_dilation))
A:jax._src.lax.windowed_reductions.(select_jaxpr, select_consts)->jax._src.lax.lax._reduction_jaxpr(select, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(scatter_jaxpr, scatter_consts)->jax._src.lax.lax._reduction_jaxpr(scatter, lax._abstractify(init_value))
A:jax._src.lax.windowed_reductions.(operand_avals, init_val_avals)->jax._src.util.split_list(avals, [len(avals) // 2])
A:jax._src.lax.windowed_reductions.out_shape->_common_reduce_window_shape_rule(operand_avals[0], window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.(operands, init_values)->jax._src.util.split_list(args, [len(args) // 2])
A:jax._src.lax.windowed_reductions.(operand_bdims, init_value_bdims)->jax._src.util.split_list(batch_dims, [num_operands])
A:jax._src.lax.windowed_reductions.size->next((a.shape[bdim] for (a, bdim) in zip(batched_args, batch_dims) if bdim is not None))
A:jax._src.lax.windowed_reductions.outs->jax.core.Primitive('reduce_window').bind(*operands + init_values, jaxpr=jaxpr, consts=consts, window_dimensions=window_dimensions, window_strides=window_strides, padding=padding, base_dilation=base_dilation, window_dilation=window_dilation)
A:jax._src.lax.windowed_reductions.reduce_window_p->jax.core.Primitive('reduce_window')
A:jax._src.lax.windowed_reductions.(_, init_value_avals)->jax._src.util.split_list(ctx.avals_in, [len(operands)])
A:jax._src.lax.windowed_reductions.rw->jax._src.lib.mlir.dialects.mhlo.ReduceWindowOp([ir.RankedTensorType.get(out_aval.shape, double_word_type)], pack(operand, tangents), pack(const(dtype, init), const(dtype, 0)), mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), base_dilations=mlir.dense_int_elements(base_dilation), window_dilations=mlir.dense_int_elements(window_dilation), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2)))
A:jax._src.lax.windowed_reductions.reducer->jax._src.lib.mlir.dialects.mhlo.ReduceWindowOp([ir.RankedTensorType.get(out_aval.shape, double_word_type)], pack(operand, tangents), pack(const(dtype, init), const(dtype, 0)), mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), base_dilations=mlir.dense_int_elements(base_dilation), window_dilations=mlir.dense_int_elements(window_dilation), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.windowed_reductions.(out_nodes, _)->jax.interpreters.mlir.jaxpr_subcomp(ctx.module_context, scatter_jaxpr, mlir.TokenSet(), scatter_consts, *([a] for a in scatter.arguments))
A:jax._src.lax.windowed_reductions.pads->jax._src.lax.convolution._conv_general_vjp_lhs_padding(input_shape, window_dimensions, window_strides, cotangent.shape, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.pad_cotangent->jax._src.lax.lax.pad(cotangent, lax._zero(cotangent), padding_config)
A:jax._src.lax.windowed_reductions.result->jax._src.lax.slicing.slice(result, (0,) * len(result.shape), result.shape, base_dilation)
A:jax._src.lax.windowed_reductions.operand->jax._src.lax.lax.pad(operand, select_identity(operand.dtype), tuple(((0, 0, d - 1) for d in base_dilation)))
A:jax._src.lax.windowed_reductions.reduce_window_sum_p->jax._src.lax.lax.standard_primitive(_reduce_window_sum_shape_rule, lax._input_dtype, 'reduce_window_sum')
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_sum_p]->partial(_reduce_window_batch_rule, _reduce_window_sum)
A:jax._src.lax.windowed_reductions.operand_shape->jax._src.lax.lax._dilate_shape(operand_shape, base_dilation)
A:jax._src.lax.windowed_reductions.window_dimensions->jax._src.lax.lax._dilate_shape(window_dimensions, window_dilation)
A:jax._src.lax.windowed_reductions.(pads_lo, pads_hi)->jax._src.util.unzip2(padding)
A:jax._src.lax.windowed_reductions.operand_padded->jax.core.sum_shapes(operand_shape, pads_lo, pads_hi)
A:jax._src.lax.windowed_reductions.reduce_window_max_p->jax._src.lax.lax.standard_primitive(_common_reduce_window_shape_rule, lax._input_dtype, 'reduce_window_max')
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_max_p]->partial(_reduce_window_batch_rule, _reduce_window_max)
A:jax._src.lax.windowed_reductions.reduce_window_min_p->jax._src.lax.lax.standard_primitive(_common_reduce_window_shape_rule, lax._input_dtype, 'reduce_window_min')
A:jax._src.lax.windowed_reductions._reduce_window_min_batch_rule->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax._src.lax.windowed_reductions.batching.primitive_batchers[reduce_window_min_p]->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax._src.lax.windowed_reductions.scalar_aval->operand_aval.update(shape=())
A:jax._src.lax.windowed_reductions.scalar_type->jax._src.lib.mlir.ir.RankedTensorType.get([], double_word_type)
A:jax._src.lax.windowed_reductions.select_and_scatter_p->jax._src.lax.lax.standard_primitive(_select_and_scatter_shape_rule, lax._input_dtype, 'select_and_scatter')
A:jax._src.lax.windowed_reductions.op->jax._src.lib.mlir.dialects.mhlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2)))
A:jax._src.lax.windowed_reductions.select->jax._src.lib.mlir.dialects.mhlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).select.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.windowed_reductions.scatter->jax._src.lib.mlir.dialects.mhlo.SelectAndScatterOp(mlir.aval_to_ir_type(aval_out), operand, source, init_value, window_dimensions=mlir.dense_int_elements(window_dimensions), window_strides=mlir.dense_int_elements(window_strides), padding=ir.DenseIntElementsAttr.get(np.asarray(padding, np.int64), shape=(len(padding), 2))).scatter.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.windowed_reductions.val_out->_select_and_gather_add(source, operand, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.tangent_out->_select_and_gather_add(g_source, operand, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.source_t->_select_and_gather_add(t, operand, select_prim, window_dimensions, window_strides, padding, ones, ones)
A:jax._src.lax.windowed_reductions.source->jax.interpreters.batching.bdim_at_front(source, s_bdim, size)
A:jax._src.lax.windowed_reductions.out->_select_and_gather_add(t, x, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.select_and_scatter_add_p->jax._src.lax.lax.standard_primitive(_select_and_scatter_add_shape_rule, lax._input_dtype, 'select_and_scatter_add')
A:jax._src.lax.windowed_reductions.etype->jax.interpreters.mlir.dtype_to_ir_type(dtype)
A:jax._src.lax.windowed_reductions.word_type->jax.interpreters.mlir.dtype_to_ir_type(word_dtype)
A:jax._src.lax.windowed_reductions.double_word_type->jax.interpreters.mlir.dtype_to_ir_type(double_word_dtype)
A:jax._src.lax.windowed_reductions.a->jax._src.lib.mlir.dialects.mhlo.BitcastConvertOp(ir.RankedTensorType.get(a_dims, word_type), a)
A:jax._src.lax.windowed_reductions.b->jax._src.lib.mlir.dialects.mhlo.ShiftRightLogicalOp(b, _broadcast(const(word_dtype, r_nbits), b_dims))
A:jax._src.lax.windowed_reductions.st->jax._src.lib.mlir.dialects.mhlo.AndOp(t, const(word_dtype, (1 << r_nbits) - 1 << r_nbits))
A:jax._src.lax.windowed_reductions.double_word_typeword_type->jax.interpreters.mlir.dtype_to_ir_type(word_dtype)
A:jax._src.lax.windowed_reductions.which->select_prim.bind(kx, ky)
A:jax._src.lax.windowed_reductions.(_, out)->reduce_window((operand, tangents), (np.array(init, dtype=operand.dtype), np.array(0, dtype=operand.dtype)), reducer, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax._src.lax.windowed_reductions.has_base_dilation->any((d != 1 for d in base_dilation))
A:jax._src.lax.windowed_reductions.t->jax.interpreters.batching.bdim_at_front(t, t_bdim, size)
A:jax._src.lax.windowed_reductions.x->jax.interpreters.batching.bdim_at_front(x, x_bdim, size)
A:jax._src.lax.windowed_reductions.select_and_gather_add_p->jax._src.lax.lax.standard_primitive(_select_and_gather_add_shape_rule, lax._input_dtype, 'select_and_gather_add')
jax._src.lax.windowed_reductions._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._generic_reduce_window_batch_rule(batched_args,batch_dims,*,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._generic_reduce_window_lower(ctx,*args,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._get_monoid_window_reducer(monoid_op:Callable,xs:Sequence[Array])->Optional[Callable]
jax._src.lax.windowed_reductions._reduce_window_abstract_eval_rule(*avals,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_batch_rule(reduce_window,batched_args,bdims,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_chooser_jvp_rule(prim,g,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_lower(reduce_op,init_value,ctx,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_max(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_min(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_prod(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_sum(operand:Array,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions._reduce_window_sum_shape_rule(operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._reduce_window_sum_transpose_rule(cotangent,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add(tangents:Array,operand:Array,select_prim:core.Primitive,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Sequence[int],window_dilation:Sequence[int])->Array
jax._src.lax.windowed_reductions._select_and_gather_add_batching_rule(batched_args,batch_dims,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_lowering(ctx,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation,max_bits=64)
jax._src.lax.windowed_reductions._select_and_gather_add_shape_rule(tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_transpose(t,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_gather_add_using_variadic_reducewindow(tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax._src.lax.windowed_reductions._select_and_scatter(operand:Array,select:Callable,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],source:Array,init_value:Array,scatter:Callable)->Array
jax._src.lax.windowed_reductions._select_and_scatter_add(source:Array,operand:Array,select_prim:core.Primitive,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]])->Array
jax._src.lax.windowed_reductions._select_and_scatter_add_batch_rule(batched_args,batch_dims,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_impl(source,operand,*,select_prim,window_dimensions,window_strides,padding,expand_padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_shape_rule(source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_add_transpose(t,source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_lower(ctx,operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions._select_and_scatter_shape_rule(operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._src.lax.windowed_reductions.reduce_window(operand,init_value,computation:Callable,window_dimensions:core.Shape,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax._src.lax.windowed_reductions.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding,base_dilation=None,window_dilation=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/slicing.py----------------------------------------
A:jax._src.lax.slicing._dtype->partial(dtypes.dtype, canonicalize=True)
A:jax._src.lax.slicing.start_indices->list(start_indices)
A:jax._src.lax.slicing.(dynamic_sizes, static_sizes)->jax._src.lax.lax._extract_tracers_dyn_shape(slice_sizes)
A:jax._src.lax.slicing.static_sizes->jax.core.canonicalize_shape(slice_sizes)
A:jax._src.lax.slicing.CLIP->enum.auto()
A:jax._src.lax.slicing.FILL_OR_DROP->enum.auto()
A:jax._src.lax.slicing.PROMISE_IN_BOUNDS->enum.auto()
A:jax._src.lax.slicing.parsed_mode->GatherScatterMode.from_any(mode)
A:jax._src.lax.slicing.dtype->_dtype(operand)
A:jax._src.lax.slicing.(jaxpr, consts)->jax._src.lax.lax._reduction_jaxpr(_scatter_reduction_computation, lax._abstractify(lax._const(operand, 0)))
A:jax._src.lax.slicing.unused->jax._src.lax.lax.full(scatter_indices.shape[:1], 0, operand.dtype)
A:jax._src.lax.slicing._apply->_scatter_apply_cache.setdefault(func, _apply)
A:jax._src.lax.slicing.indices->jax._src.lax.lax.concatenate([counts, indices], len(count_shape) - 1)
A:jax._src.lax.slicing.max_idx->jax._src.lax.lax.expand_dims(np.array([src.shape[ax] for ax in axes]), tuple(range(indices.ndim - 1)))
A:jax._src.lax.slicing.slice_sizes->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn)
A:jax._src.lax.slicing.offset_dims->tuple(np.add(1, dimension_numbers.offset_dims))
A:jax._src.lax.slicing.dnums->ScatterDimensionNumbers(update_window_dims=update_window_dims, inserted_window_dims=inserted_window_dims, scatter_dims_to_operand_dims=scatter_dims_to_operand_dims)
A:jax._src.lax.slicing.limit_indices->list(operand.shape)
A:jax._src.lax.slicing.axis->int(axis)
A:jax._src.lax.slicing.strides[axis]->int(stride)
A:jax._src.lax.slicing.result->jax.interpreters.mlir.aval_to_ir_types(aval_out)
A:jax._src.lax.slicing.slice_sizes[axis]->jax.core._canonicalize_dimension(slice_size)
A:jax._src.lax.slicing.update->jax._src.lib.mlir.dialects.mhlo.ScatterOp(result, operand, indices, updates, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices)).update_computation.blocks.append(scalar_type, scalar_type)
A:jax._src.lax.slicing.diff->jax.core.diff_shape(limit_indices, start_indices)
A:jax._src.lax.slicing.pads->zip(start_indices, np.subtract(operand_shape, real_limits), np.subtract(strides, 1))
A:jax._src.lax.slicing.real_limits->numpy.add(start_indices, np.where(np.array(t.shape) == 0, 0, np.add(1, np.multiply(np.subtract(t.shape, 1), strides))))
A:jax._src.lax.slicing.new_start_indices->list(start_indices)
A:jax._src.lax.slicing.new_limit_indices->list(limit_indices)
A:jax._src.lax.slicing.new_strides->list(strides)
A:jax._src.lax.slicing.out->scatter_add(zeros, indices, t, scatter_dnums, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode)
A:jax._src.lax.slicing.slice_p->standard_primitive(_slice_shape_rule, _input_dtype, 'slice')
A:jax._src.lax.slicing.tangent_out->scatter_add(masked_g_operand, indices, masked_g_updates, dimension_numbers=dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.zeros->jax._src.lax.lax.full(operand_shape, lax._zero(t))
A:jax._src.lax.slicing.empty_marker->object()
A:jax._src.lax.slicing.size->next((x.shape[ax] for (x, ax) in zip(batched_args, batch_dims) if ax is not None))
A:jax._src.lax.slicing.dims->tuple(range(len(update_shape)))
A:jax._src.lax.slicing.(index, index_bdim)->_batch_dynamic_slice_indices(start_idx, start_idx_bd)
A:jax._src.lax.slicing.(start_indices, dyn)->jax._src.util.split_list(starts_and_dyn_sizes, [x_aval.ndim])
A:jax._src.lax.slicing.shape->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn)
A:jax._src.lax.slicing.aval->jax.core.DShapedArray(shape, x.dtype, False)
A:jax._src.lax.slicing.(out_aval, effects)->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', weak_type_rule=_argnum_weak_type(0)).abstract_eval(x.aval, *(d.aval for d in start_indices), slice_sizes=slice_sizes)
A:jax._src.lax.slicing.out_shape->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn)
A:jax._src.lax.slicing.out_aval->jax.core.DShapedArray(tuple(out_shape), x.aval.dtype, x.aval.weak_type)
A:jax._src.lax.slicing.(x_aval, start_indices_avals, dyn_avals)->jax._src.util.split_list(in_avals, [1, x.ndim])
A:jax._src.lax.slicing.slice_sizes_->jax._src.lax.lax._merge_dyn_shape(slice_sizes, dyn_)
A:jax._src.lax.slicing.dynamic_slice_p->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.val_out->scatter_add(masked_operand, indices, masked_updates, dimension_numbers=dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.g_operand->jax.interpreters.ad.instantiate_zeros(g_operand)
A:jax._src.lax.slicing.g_update->jax.interpreters.ad.instantiate_zeros(g_update)
A:jax._src.lax.slicing.dynamic_update_slice_p->standard_primitive(_dynamic_update_slice_shape_rule, _dynamic_update_slice_dtype_rule, 'dynamic_update_slice')
A:jax._src.lax.slicing.proto->jax._src.lib.xla_client.ScatterDimensionNumbers()
A:jax._src.lax.slicing.expanded_indices_shape->list(indices.shape)
A:jax._src.lax.slicing.output_offset_dim_count->len(offset_dims)
A:jax._src.lax.slicing.indices_shape->iter(expanded_indices_shape)
A:jax._src.lax.slicing.intarray->partial(np.array, dtype=np.int64)
A:jax._src.lax.slicing.operand_dims->jax._src.lax.lax.shape_as_value(operand.shape)
A:jax._src.lax.slicing.mask->scatter(lax._ones(t, dtype=np.bool_), indices, lax.full(updates_shape, False), dimension_numbers=dimension_numbers, indices_are_sorted=indices_are_sorted, unique_indices=True, mode=mode)
A:jax._src.lax.slicing.batch_dims_in_output->numpy.delete(np.arange(output_ndims), dnums.offset_dims)
A:jax._src.lax.slicing.gather_out->gather(operand, indices, dnums, slice_sizes, indices_are_sorted=indices_are_sorted, mode=GatherScatterMode.PROMISE_IN_BOUNDS)
A:jax._src.lax.slicing.scatter_dnums->jax._src.lib.mlir.dialects.mhlo.ScatterDimensionNumbers.get(update_window_dims=list(dnums.update_window_dims), inserted_window_dims=list(dnums.inserted_window_dims), scattered_dims_to_operand_dims=list(dnums.scatter_dims_to_operand_dims), index_vector_dim=len(ctx.avals_in[1].shape) - 1)
A:jax._src.lax.slicing.operand->jax.interpreters.batching.bdim_at_front(operand, operand_bdim, size)
A:jax._src.lax.slicing.collapsed_slice_dims->tuple(np.add(1, dimension_numbers.collapsed_slice_dims))
A:jax._src.lax.slicing.start_index_map->tuple(np.add(1, dimension_numbers.start_index_map))
A:jax._src.lax.slicing.output_shape->_gather_shape_rule(core.ShapedArray(operand.shape[1:], operand.dtype), core.ShapedArray(indices.shape[1:], dtypes.canonicalize_dtype(indices.dtype)), dimension_numbers=dimension_numbers, slice_sizes=slice_sizes, unique_indices=unique_indices, indices_are_sorted=indices_are_sorted, mode=mode, fill_value=fill_value)
A:jax._src.lax.slicing.count_shape->list(indices.shape)
A:jax._src.lax.slicing.counts->jax._src.lax.lax.broadcasted_iota(indices.dtype, tuple(count_shape), 0)
A:jax._src.lax.slicing.gather_p->standard_primitive(_gather_shape_rule, _gather_dtype_rule, 'gather', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.gather_fill_fn->jax.interpreters.mlir.lower_fun(_gather_fill, multiple_results=False)
A:jax._src.lax.slicing.upper_bound->jax._src.lax.lax.broadcast_in_dim(upper_bound, indices.shape, (len(indices.shape) - 1,))
A:jax._src.lax.slicing.g_updates->jax.interpreters.ad.instantiate_zeros(g_updates)
A:jax._src.lax.slicing.gather_dnums->GatherDimensionNumbers(offset_dims=dimension_numbers.update_window_dims, collapsed_slice_dims=dimension_numbers.inserted_window_dims, start_index_map=dimension_numbers.scatter_dims_to_operand_dims)
A:jax._src.lax.slicing.update_t->gather(t, indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes, mode=mode, fill_value=0)
A:jax._src.lax.slicing.operand_t->jax._src.lax.lax.select(mask, t, lax._zeros(t))
A:jax._src.lax.slicing.updates->jax.interpreters.batching.bdim_at_front(updates, updates_bdim, size)
A:jax._src.lax.slicing.inserted_window_dims->tuple(np.add(1, dimension_numbers.inserted_window_dims))
A:jax._src.lax.slicing.scatter_dims_to_operand_dims->tuple(np.add(1, dimension_numbers.scatter_dims_to_operand_dims))
A:jax._src.lax.slicing.update_window_dims->tuple(np.add(1, dimension_numbers.update_window_dims))
A:jax._src.lax.slicing.scatter_add_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-add', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_add_p]->partial(_scatter_batching_rule, scatter_add)
A:jax._src.lax.slicing.scatter_mul_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-mul', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_mul_p]->partial(_scatter_batching_rule, scatter_mul)
A:jax._src.lax.slicing.initial_vals->gather(operand, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.target_vals->gather(val_out, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.num_updates->gather(scatter_add(lax._zeros(operand), indices, lax.select(successful_updates, lax._ones(updates), lax._zeros(updates)), scatter_dnums), indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.num_refs->gather(scatter_add(lax._zeros(operand), indices, lax._ones(updates), scatter_dnums), indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.updates_normalizer->jax._src.lax.lax.select(retained_values, 1.0 / (num_updates + 1), 1.0 / num_updates)
A:jax._src.lax.slicing.updates_coef->jax._src.lax.lax.select(successful_updates, updates_normalizer, lax._zeros(updates))
A:jax._src.lax.slicing.operand_normalizer->jax._src.lax.lax.select(retained_values, 1.0 / (num_updates + 1), lax._zeros(num_updates))
A:jax._src.lax.slicing.target_tangents->gather(g_operand, indices, gather_dnums, np.array(slice_sizes))
A:jax._src.lax.slicing.scatter_min_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-min', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_min_p]->partial(_scatter_batching_rule, scatter_min)
A:jax._src.lax.slicing.ad.primitive_jvps[scatter_min_p]->partial(_scatter_extremal_jvp, scatter_min_p)
A:jax._src.lax.slicing.scatter_max_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-max', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_max_p]->partial(_scatter_batching_rule, scatter_max)
A:jax._src.lax.slicing.ad.primitive_jvps[scatter_max_p]->partial(_scatter_extremal_jvp, scatter_max_p)
A:jax._src.lax.slicing.ids_shape->numpy.array(updates.shape, dtype=np.int64)
A:jax._src.lax.slicing.num_ids->numpy.prod(ids_shape)
A:jax._src.lax.slicing.update_ids->jax._src.lax.lax.add(lax.reshape(lax.iota(id_dtype, num_ids), ids_shape), lax._ones(updates, dtype=id_dtype))
A:jax._src.lax.slicing.scattered_ids->scatter(lax.full(operand.shape, 0, id_dtype), indices, update_ids, dnums, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices, mode=mode)
A:jax._src.lax.slicing.gathered_update_ids->gather(scattered_ids, indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax._src.lax.slicing.masked_operand->jax._src.lax.lax.select(lax.eq(scattered_ids, lax._zeros(scattered_ids)), operand, lax._zeros(operand))
A:jax._src.lax.slicing.masked_updates->jax._src.lax.lax.select(lax.eq(update_ids, gathered_update_ids), updates, lax._zeros(updates))
A:jax._src.lax.slicing.masked_g_operand->jax._src.lax.lax.select(lax.eq(scattered_ids, lax._zeros(scattered_ids)), g_operand, lax._zeros(g_operand))
A:jax._src.lax.slicing.masked_g_updates->jax._src.lax.lax.select(lax.eq(update_ids, gathered_update_ids), g_updates, lax._zeros(g_updates))
A:jax._src.lax.slicing.scatter_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter', weak_type_rule=_argnum_weak_type(0))
A:jax._src.lax.slicing.batching.primitive_batchers[scatter_p]->partial(_scatter_batching_rule, scatter)
A:jax._src.lax.slicing.clip_fn->jax.interpreters.mlir.lower_fun(_clamp_scatter_indices, multiple_results=False)
A:jax._src.lax.slicing.((indices,),)->clip_fn(ctx.replace(avals_out=None), operand, indices, updates, dnums=dimension_numbers)
A:jax._src.lax.slicing.op->jax._src.lib.mlir.dialects.mhlo.ScatterOp(result, operand, indices, updates, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices))
A:jax._src.lax.slicing.scalar_type->jax.interpreters.mlir.aval_to_ir_type(core.ShapedArray((), real_dtype))
A:jax._src.lax.slicing.update_ctx->ctx.module_context.replace(name_stack=util.new_name_stack())
A:jax._src.lax.slicing.(out_nodes, _)->jax.interpreters.mlir.jaxpr_subcomp(update_ctx, update_jaxpr, mlir.TokenSet(), update_consts, (update.arguments[0],), (update.arguments[1],))
A:jax._src.lax.slicing.real_dtype->_real_dtype(aval_out.dtype)
A:jax._src.lax.slicing.operand_type_part->jax.interpreters.mlir.aval_to_ir_types(core.ShapedArray(aval_out.shape, real_dtype))
A:jax._src.lax.slicing.scatter->jax._src.lib.mlir.dialects.mhlo.ScatterOp(operand_type_part, operand_part, indices, updates_part, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices))
A:jax._src.lax.slicing.reducer->jax._src.lib.mlir.dialects.mhlo.ScatterOp(operand_type_part, operand_part, indices, updates_part, scatter_dnums, indices_are_sorted=ir.BoolAttr.get(indices_are_sorted), unique_indices=ir.BoolAttr.get(unique_indices)).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.slicing.real->_scatter(mhlo.RealOp(operand).result, mhlo.RealOp(updates).result)
A:jax._src.lax.slicing.imag->_scatter(mhlo.ImagOp(operand).result, mhlo.ImagOp(updates).result)
A:jax._src.lax.slicing.d->jax._src.lax.lax.convert_element_type(d, _dtype(i))
jax._src.lax.slicing.GatherDimensionNumbers(NamedTuple)
jax._src.lax.slicing.GatherScatterMode(enum.Enum)
jax._src.lax.slicing.GatherScatterMode.from_any(s:Optional[Union[str,'GatherScatterMode']])
jax._src.lax.slicing.ScatterDimensionNumbers(NamedTuple)
jax._src.lax.slicing._batch_dynamic_slice_indices(indices,bdims)
jax._src.lax.slicing._clamp_scatter_indices(operand,indices,updates,*,dnums)
jax._src.lax.slicing._dynamic_slice_batching_rule(batched_args,batch_dims,*,slice_sizes)
jax._src.lax.slicing._dynamic_slice_dtype_rule(operand,*start_indices,slice_sizes)
jax._src.lax.slicing._dynamic_slice_indices(operand:Array,start_indices:Union[Array,Sequence[ArrayLike]])->List[ArrayLike]
jax._src.lax.slicing._dynamic_slice_jvp(primals,tangents,*,slice_sizes)
jax._src.lax.slicing._dynamic_slice_lower(ctx,x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_padding_rule(in_avals,out_avals,x,*starts_and_dyn,slice_sizes)
jax._src.lax.slicing._dynamic_slice_shape_rule(operand,*start_indices,slice_sizes)
jax._src.lax.slicing._dynamic_slice_staging_rule(trace,x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_slice_transpose_rule(t,operand,*start_indices,slice_sizes)
jax._src.lax.slicing._dynamic_slice_typecheck_rule(x,*starts_and_dyn_sizes,slice_sizes)
jax._src.lax.slicing._dynamic_update_slice_batching_rule(batched_args,batch_dims)
jax._src.lax.slicing._dynamic_update_slice_dtype_rule(operand,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_jvp(primals,tangents)
jax._src.lax.slicing._dynamic_update_slice_lower(ctx,x,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_shape_rule(operand,update,*start_indices)
jax._src.lax.slicing._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices)
jax._src.lax.slicing._gather_batching_rule(batched_args,batch_dims,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_dimensions_proto(indices_shape:Sequence[int],dimension_numbers:GatherDimensionNumbers)->xla_client.GatherDimensionNumbers
jax._src.lax.slicing._gather_dtype_rule(operand,indices,*,fill_value,**kwargs)
jax._src.lax.slicing._gather_fill(operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,fill_value,output_shape)
jax._src.lax.slicing._gather_jvp_rule(g,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_lower(ctx,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_pad_rule(in_avals,out_avals,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_shape_rule(operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._gather_transpose_rule(t,operand,indices,*,dimension_numbers,slice_sizes,unique_indices,indices_are_sorted,mode,fill_value)
jax._src.lax.slicing._is_sorted(dims,op_name,name)
jax._src.lax.slicing._no_duplicate_dims(dims,op_name,name)
jax._src.lax.slicing._real_dtype(dtype)
jax._src.lax.slicing._scatter_add_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_add_lower_gpu(ctx,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_add_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_batching_rule(scatter_op,batched_args,batch_dims,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_dimensions_proto(indices_shape:Sequence[int],dimension_numbers:ScatterDimensionNumbers)->xla_client.ScatterDimensionNumbers
jax._src.lax.slicing._scatter_dtype_rule(operand,indices,updates,**kwargs)
jax._src.lax.slicing._scatter_extremal_jvp(scatter_op,primals,tangents,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_lower(ctx,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_mul_jvp_rhs(g,x,i,y,*,dimension_numbers,indices_are_sorted,unique_indices,mode,**kw)
jax._src.lax.slicing._scatter_mul_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_shape_rule(operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._scatter_transpose_rule(t,operand,indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices,mode)
jax._src.lax.slicing._slice_batching_rule(batched_args,batch_dims,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_lower(ctx,x,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_shape_rule(operand,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._slice_transpose_rule(t,operand,*,start_indices,limit_indices,strides)
jax._src.lax.slicing._sorted_dims_in_range(dims,rank,op_name,name)
jax._src.lax.slicing.dynamic_index_in_dim(operand:Array,index:Array,axis:int=0,keepdims:bool=True)->Array
jax._src.lax.slicing.dynamic_slice(operand:Array,start_indices:Union[Array,Sequence[ArrayLike]],slice_sizes:Shape)->Array
jax._src.lax.slicing.dynamic_slice_in_dim(operand:Array,start_index:ArrayLike,slice_size:int,axis:int=0)->Array
jax._src.lax.slicing.dynamic_update_index_in_dim(operand:Array,update:ArrayLike,index:ArrayLike,axis:int)->Array
jax._src.lax.slicing.dynamic_update_slice(operand:Array,update:ArrayLike,start_indices:Union[Array,Sequence[ArrayLike]])->Array
jax._src.lax.slicing.dynamic_update_slice_in_dim(operand:Array,update:ArrayLike,start_index:ArrayLike,axis:int)->Array
jax._src.lax.slicing.gather(operand:ArrayLike,start_indices:ArrayLike,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape,*,unique_indices:bool=False,indices_are_sorted:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None,fill_value=None)->Array
jax._src.lax.slicing.index_in_dim(operand:Array,index:int,axis:int=0,keepdims:bool=True)->Array
jax._src.lax.slicing.index_take(src:Array,idxs:Array,axes:Sequence[int])->Array
jax._src.lax.slicing.scatter(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_add(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_apply(operand:Array,scatter_indices:Array,func:Callable[[Array],Array],dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_max(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_min(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.scatter_mul(operand:ArrayLike,scatter_indices:ArrayLike,updates:ArrayLike,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False,mode:Optional[Union[str,GatherScatterMode]]=None)->Array
jax._src.lax.slicing.slice(operand:ArrayLike,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)->Array
jax._src.lax.slicing.slice_in_dim(operand:Array,start_index:Optional[int],limit_index:Optional[int],stride:int=1,axis:int=0)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/qdwh.py----------------------------------------
A:jax._src.lax.qdwh.y->jax._src.lax.linalg.cholesky(x, symmetrize_input=False)
A:jax._src.lax.qdwh.(q, _)->jax._src.lax.linalg.qr(y, full_matrices=False)
A:jax._src.lax.qdwh.q1->_mask(lax.slice(q, (0, 0), (M, N)), (m, n))
A:jax._src.lax.qdwh.q2->_mask(q2, (n, n)).T.conj()
A:jax._src.lax.qdwh.x->_mask(x, (m, n))
A:jax._src.lax.qdwh.z->jax._src.lax.linalg.triangular_solve(y, z, left_side=True, lower=True, transpose_a=True, conjugate_a=True).T.conj()
A:jax._src.lax.qdwh.eps->float(jnp.finfo(x.dtype).eps)
A:jax._src.lax.qdwh.alpha->(jnp.sqrt(jnp.linalg.norm(x, ord=1)) * jnp.sqrt(jnp.linalg.norm(x, ord=jnp.inf))).astype(x.dtype)
A:jax._src.lax.qdwh.tol_norm->jax.numpy.cbrt(tol_l)
A:jax._src.lax.qdwh.dd->jax.numpy.cbrt(4.0 * (1.0 / l2 - 1.0) / l2)
A:jax._src.lax.qdwh.sqd->jax.numpy.sqrt(1.0 + dd)
A:jax._src.lax.qdwh.a->jax.numpy.real(a)
A:jax._src.lax.qdwh.u->jax.lax.cond(c > 100, true_fn, false_fn, operand=u)
A:jax._src.lax.qdwh.is_unconverged->jax.numpy.logical_or(iterating_l, iterating_u)
A:jax._src.lax.qdwh.(u, _, num_iters, is_unconverged, _)->jax.lax.while_loop(cond_fun=cond_fun, body_fun=body_fun, init_val=(u, l, iter_idx, is_unconverged, is_not_max_iteration))
A:jax._src.lax.qdwh.is_converged->jax.numpy.logical_not(is_unconverged)
A:jax._src.lax.qdwh.is_hermitian->jax.core.concrete_or_error(bool, is_hermitian, 'The `is_hermitian` argument must be statically specified to use `qdwh` within JAX transformations.')
A:jax._src.lax.qdwh.(u, h, num_iters, is_converged)->_qdwh(x, m, n, is_hermitian, max_iterations, eps)
jax._src.lax.qdwh._dynamic_concat(a,b,m,axis=0)
jax._src.lax.qdwh._mask(x,dims,alternative=0)
jax._src.lax.qdwh._pad_in_dim(x,low=0,high=0,interior=0,fill_value=0,axis=0)
jax._src.lax.qdwh._qdwh(x,m,n,is_hermitian,max_iterations,eps)
jax._src.lax.qdwh._use_cholesky(u,m,n,params)
jax._src.lax.qdwh._use_qr(u,m,n,params)
jax._src.lax.qdwh.qdwh(x,*,is_hermitian=False,max_iterations=None,eps=None,dynamic_shape:Optional[Tuple[int,int]]=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/svd.py----------------------------------------
A:jax._src.lax.svd.k->min(m, n)
A:jax._src.lax.svd.s->jax.numpy.zeros(shape=(k,), dtype=a.real.dtype)
A:jax._src.lax.svd.u->jax.numpy.eye(m, k, dtype=a.dtype)
A:jax._src.lax.svd.vh->jax.numpy.eye(k, n, dtype=a.dtype)
A:jax._src.lax.svd.(u, h, _, _)->jax.lax.linalg.qdwh(a, is_hermitian=hermitian, max_iterations=max_iterations)
A:jax._src.lax.svd.(v, s)->jax.lax.linalg.eigh(h)
A:jax._src.lax.svd.s_out->jax.numpy.flip(s)
A:jax._src.lax.svd.v_out->jax.numpy.fliplr(v)
A:jax._src.lax.svd.(u_out, r)->jax.lax.linalg.qr(u_out, full_matrices=False)
A:jax._src.lax.svd.eps->float(jnp.finfo(a.dtype).eps)
A:jax._src.lax.svd.u_out->jax.numpy.hstack((u_out, u_out_null))
A:jax._src.lax.svd.a->a.T.conj().T.conj()
A:jax._src.lax.svd.(q_full, a_full)->jax.lax.linalg.qr(a, full_matrices=True)
A:jax._src.lax.svd.(q, a)->jax.lax.linalg.qr(a, full_matrices=False)
A:jax._src.lax.svd.(u_out, s_out, v_out)->_svd_tall_and_square_input(a, hermitian, compute_uv, max_iterations)
A:jax._src.lax.svd.full_matrices->jax.core.concrete_or_error(bool, full_matrices, 'The `full_matrices` argument must be statically specified to use `svd` within JAX transformations.')
A:jax._src.lax.svd.compute_uv->jax.core.concrete_or_error(bool, compute_uv, 'The `compute_uv` argument must be statically specified to use `svd` within JAX transformations.')
A:jax._src.lax.svd.hermitian->jax.core.concrete_or_error(bool, hermitian, 'The `hermitian` argument must be statically specified to use `qdwh` within JAX transformations.')
A:jax._src.lax.svd.max_iterations->jax.core.concrete_or_error(int, max_iterations, 'The `max_iterations` argument must be statically specified to use `qdwh` within JAX transformations.')
jax._src.lax.svd._qdwh_svd(a:Any,full_matrices:bool,compute_uv:bool=True,hermitian:bool=False,max_iterations:int=10)->Union[Any, Sequence[Any]]
jax._src.lax.svd._svd_tall_and_square_input(a:Any,hermitian:bool,compute_uv:bool,max_iterations:int)->Union[Any, Sequence[Any]]
jax._src.lax.svd._zero_svd(a:Any,full_matrices:bool,compute_uv:bool=True)->Union[Any, Sequence[Any]]
jax._src.lax.svd.svd(a:Any,full_matrices:bool,compute_uv:bool=True,hermitian:bool=False,max_iterations:int=10)->Union[Any, Sequence[Any]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/other.py----------------------------------------
A:jax._src.lax.other.filter_shape->tuple(filter_shape)
A:jax._src.lax.other.dimension_numbers->jax._src.lax.convolution.conv_dimension_numbers(lhs.shape, (1, 1) + filter_shape, dimension_numbers)
A:jax._src.lax.other.spatial_size->prod(filter_shape)
A:jax._src.lax.other.rhs->jax._src.numpy.lax_numpy.moveaxis(rhs, (0, 1), (rhs_spec[0], rhs_spec[1]))
A:jax._src.lax.other.out->jax._src.numpy.lax_numpy.moveaxis(out, (-2, -1), (out_spec[0], out_spec[1]))
A:jax._src.lax.other.c_precision->jax._src.lax.lax.canonicalize_precision(precision)
A:jax._src.lax.other.lhs_precision->type_cast(Optional[lax.PrecisionType], c_precision[0] if isinstance(c_precision, tuple) and len(c_precision) == 2 else c_precision)
A:jax._src.lax.other.patches->conv_general_dilated_patches(lhs=lhs, filter_shape=filter_shape, window_strides=window_strides, padding=padding, lhs_dilation=lhs_dilation, rhs_dilation=rhs_dilation, dimension_numbers=dimension_numbers, precision=lhs_precision)
A:jax._src.lax.other.(lhs_spec, rhs_spec, out_spec)->jax._src.lax.convolution.conv_dimension_numbers(lhs.shape, (1, 1) + tuple(filter_shape), dimension_numbers)
A:jax._src.lax.other.lhs_b_dims->sorted(lhs_b_dims)
jax._src.lax.other.conv_general_dilated_local(lhs:jnp.ndarray,rhs:jnp.ndarray,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],filter_shape:Sequence[int],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:Optional[convolution.ConvGeneralDilatedDimensionNumbers]=None,precision:lax.PrecisionLike=None)->jnp.ndarray
jax._src.lax.other.conv_general_dilated_patches(lhs:lax.Array,filter_shape:Sequence[int],window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:Optional[convolution.ConvGeneralDilatedDimensionNumbers]=None,precision:Optional[lax.PrecisionType]=None,preferred_element_type:Optional[DType]=None)->lax.Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/stack.py----------------------------------------
A:jax._src.lax.stack.elem->jax.tree_util.tree_map(lambda x: lax.dynamic_index_in_dim(x, self._size - 1, 0, keepdims=False), self._data)
A:jax._src.lax.stack.(leaves, treedef)->jax.tree_util.tree_flatten(self._data)
jax._src.lax.stack.Stack(self,size,data)
jax._src.lax.stack.Stack.__init__(self,size,data)
jax._src.lax.stack.Stack.__repr__(self)
jax._src.lax.stack.Stack.create(capacity:int,prototype:Any)->Stack
jax._src.lax.stack.Stack.empty(self)->Any
jax._src.lax.stack.Stack.flatten(self)
jax._src.lax.stack.Stack.pop(self)->Tuple[Any, Stack]
jax._src.lax.stack.Stack.push(self,elem:Any)->Stack
jax._src.lax.stack.Stack.unflatten(treedef,leaves)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/eigh.py----------------------------------------
A:jax._src.lax.eigh.padded->jax.lax.pad(operand, jnp.array(0, operand.dtype), [(0, d, 0) for d in static_slice_sizes])
A:jax._src.lax.eigh.out->jax.lax.dynamic_slice(padded, tuple((jnp.int32(i) for i in start_indices)), static_slice_sizes)
A:jax._src.lax.eigh.operand->jax.lax.dynamic_update_slice(operand, t, start_indices)
A:jax._src.lax.eigh.start_indices->tuple((jnp.int32(i) for i in start_indices))
A:jax._src.lax.eigh.t->_mask(update, update_dims, t)
A:jax._src.lax.eigh.column_norms->_mask(column_norms, (n,), jnp.nan)
A:jax._src.lax.eigh.sort_idxs->jax._src.numpy.lax_numpy.argsort(eig_vals)
A:jax._src.lax.eigh.X->jax._src.numpy.lax_numpy.dot(P, V1)
A:jax._src.lax.eigh.H_norm->jax._src.numpy.linalg.norm(H)
A:jax._src.lax.eigh.(Q, _)->jax._src.numpy.linalg.qr(X, mode='complete')
A:jax._src.lax.eigh.V1->_mask(Q, (n, rank))
A:jax._src.lax.eigh.V2->_slice(Q, (0, rank), (n, n - rank), (N, N))
A:jax._src.lax.eigh.error_matrix->jax._src.numpy.lax_numpy.dot(error_matrix, V1)
A:jax._src.lax.eigh.(V1, V2, error)->body_f_after_matmul(X)
A:jax._src.lax.eigh.one->jax._src.numpy.lax_numpy.ones(1, dtype=jnp.int32)
A:jax._src.lax.eigh.(V1, V2, _, error)->jax.lax.while_loop(cond_f, body_f, (V1, V2, one, error))
A:jax._src.lax.eigh.(U, _, _, _)->jax._src.lax.qdwh.qdwh(H_shift, is_hermitian=True, dynamic_shape=(n, n))
A:jax._src.lax.eigh.rank->jax._src.numpy.lax_numpy.round(jnp.trace(jnp.real(P))).astype(jnp.int32)
A:jax._src.lax.eigh.(V_minus, V_plus)->_projector_subspace(P, H, n, rank)
A:jax._src.lax.eigh.V_minus->jax._src.numpy.lax_numpy.dot(V0, V_minus)
A:jax._src.lax.eigh.V_plus->jax._src.numpy.lax_numpy.dot(V0, V_plus)
A:jax._src.lax.eigh.n->jax._src.numpy.lax_numpy.asarray(n, jnp.int32)
A:jax._src.lax.eigh.agenda->agenda.push(_Subproblem(offset, rank)).push(_Subproblem(offset, rank))
A:jax._src.lax.eigh.eigenvectors->_update_slice(eigenvectors, V_plus, (0, offset + rank), (n, b - rank))
A:jax._src.lax.eigh.H->_mask(H, (n, n), jnp.eye(N, dtype=H.dtype))
A:jax._src.lax.eigh.V->_slice(eigenvectors, (0, offset), (n, b), (N, B))
A:jax._src.lax.eigh.(eig_vecs, eig_vals)->jax.lax.linalg.eigh(H, sort_eigenvalues=False)
A:jax._src.lax.eigh.eig_vecs->jax._src.numpy.lax_numpy.dot(V, eig_vecs)
A:jax._src.lax.eigh.eig_vals->_mask(jnp.real(eig_vals), (n,), jnp.nan)
A:jax._src.lax.eigh.blocks->_update_slice(blocks, H_plus, (offset + rank, 0), (b - rank, b - rank))
A:jax._src.lax.eigh.split_point->jax._src.numpy.lax_numpy.nanmedian(_mask(jnp.diag(jnp.real(H)), (b,), jnp.nan))
A:jax._src.lax.eigh.(H_minus, V_minus, H_plus, V_plus, rank)->split_spectrum(H, b, split_point, V0=V)
A:jax._src.lax.eigh.cutoff->min(N, termination_size)
A:jax._src.lax.eigh.i->min(2 * i, N)
A:jax._src.lax.eigh.buckets->jax._src.numpy.lax_numpy.array(buckets, dtype='int32')
A:jax._src.lax.eigh.((offset, b), agenda)->agenda.push(_Subproblem(offset, rank)).push(_Subproblem(offset, rank)).pop()
A:jax._src.lax.eigh.which->jax._src.numpy.lax_numpy.where(buckets < b, jnp.iinfo(jnp.int32).max, buckets)
A:jax._src.lax.eigh.choice->jax._src.numpy.lax_numpy.argmin(which)
A:jax._src.lax.eigh.(_, blocks, eigenvectors)->jax.lax.while_loop(loop_cond, loop_body, (agenda, blocks, eigenvectors))
A:jax._src.lax.eigh.(eig_vals, eig_vecs)->_eigh_work(H, n, termination_size=termination_size)
jax._src.lax.eigh._Subproblem(NamedTuple)
jax._src.lax.eigh._eigh_work(H,n,termination_size=256)
jax._src.lax.eigh._mask(x,dims,alternative=0)
jax._src.lax.eigh._projector_subspace(P,H,n,rank,maxiter=2)
jax._src.lax.eigh._slice(operand,start_indices,dynamic_slice_sizes,static_slice_sizes,fill_value=0)
jax._src.lax.eigh._update_slice(operand,update,start_indices,update_dims)
jax._src.lax.eigh.eigh(H,*,precision='float32',termination_size=256,n=None,sort_eigenvalues=True)
jax._src.lax.eigh.split_spectrum(H,n,split_point,V0=None)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/lax.py----------------------------------------
A:jax._src.lax.lax.T->TypeVar('T')
A:jax._src.lax.lax.checked->canonicalize_shape(shape)
A:jax._src.lax.lax.ndim->numpy.ndim(array)
A:jax._src.lax.lax.result_shape->broadcast_shapes(np.shape(x), np.shape(y))
A:jax._src.lax.lax.dyn_shape_it->iter(dyn_shape)
A:jax._src.lax.lax.shape->_merge_dyn_shape(shape, dyn_shape)
A:jax._src.lax.lax.source_info->jax._src.source_info_util.current()
A:jax._src.lax.lax.out_tracer->jax.interpreters.partial_eval.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)
A:jax._src.lax.lax.eqn->jax.interpreters.partial_eval.new_eqn_recipe([operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p, dict(shape=shape, broadcast_dimensions=broadcast_dimensions), core.no_effects, source_info_util.current())
A:jax._src.lax.lax.rounding_method->RoundingMethod(rounding_method)
A:jax._src.lax.lax.operand->jax.interpreters.batching.moveaxis(operand, bdim, 0)
A:jax._src.lax.lax.old_dtype->jax._src.dtypes.canonicalize_dtype(operand.dtype)
A:jax._src.lax.lax.old_weak_type->jax._src.dtypes.is_weakly_typed(operand)
A:jax._src.lax.lax.new_dtype->jax._src.dtypes.canonicalize_dtype(new_dtype)
A:jax._src.lax.lax.DEFAULT->_enum_descriptor('default')
A:jax._src.lax.lax.HIGH->_enum_descriptor('high')
A:jax._src.lax.lax.HIGHEST->_enum_descriptor('highest')
A:jax._src.lax.lax.arg0->self._strings.get(arg0, arg0)
A:jax._src.lax.lax.dims->numpy.delete(np.arange(prototype_arg.ndim), new_bdim)
A:jax._src.lax.lax.(dyn_shape, static_shape)->_extract_tracers_dyn_shape(shape)
A:jax._src.lax.lax.new_sizes->tuple(new_sizes)
A:jax._src.lax.lax.same_shape->jax.core.symbolic_equal_shape(np.shape(operand), new_sizes)
A:jax._src.lax.lax.(dyn_shape, static_new_sizes)->_extract_tracers_dyn_shape(new_sizes)
A:jax._src.lax.lax.permutation->tuple((operator.index(d) for d in permutation))
A:jax._src.lax.lax.(flat_operands, operand_tree)->jax.tree_util.tree_flatten(operands)
A:jax._src.lax.lax.(flat_init_values, init_value_tree)->jax.tree_util.tree_flatten(init_values)
A:jax._src.lax.lax.monoid_reducer->_get_monoid_reducer(computation, flat_init_values)
A:jax._src.lax.lax.flat_init_avals->safe_map(_abstractify, flat_init_values)
A:jax._src.lax.lax.(jaxpr, consts, out_tree)->_variadic_reduction_jaxpr(computation, tuple(flat_init_avals), init_value_tree)
A:jax._src.lax.lax.out->lower_comparator(sub_ctx, *[[a] for a in comparator.arguments], num_keys=num_keys)
A:jax._src.lax.lax.result->broadcast_in_dim(cotangent, input_shape, broadcast_dimensions)
A:jax._src.lax.lax.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(flat_comp, tuple(flat_in_avals))
A:jax._src.lax.lax.avals->jax.tree_util.tree_unflatten(aval_tree, flat_avals)
A:jax._src.lax.lax.(flat_in_avals, in_tree)->jax.tree_util.tree_flatten((avals, avals))
A:jax._src.lax.lax.comp->jax.linear_util.wrap_init(computation)
A:jax._src.lax.lax.(flat_comp, out_tree)->jax._src.api_util.flatten_fun_nokwargs(comp, in_tree)
A:jax._src.lax.lax.aval->jax.core.DShapedArray(_merge_dyn_shape(shape, dyn_shape), dtype, False)
A:jax._src.lax.lax.dtype->jax._src.dtypes._scalar_type_to_dtype(int)
A:jax._src.lax.lax.dimension->kwargs.pop('dimension')
A:jax._src.lax.lax.(k, v)->Primitive('sort').bind(keys, values, dimension=dimension, is_stable=is_stable, num_keys=1)
A:jax._src.lax.lax.k->int(k)
A:jax._src.lax.lax.fill_value->_convert_element_type(fill_value, dtype, weak_type)
A:jax._src.lax.lax.scalar_zero->_convert_element_type(0, aval.dtype, aval.weak_type)
A:jax._src.lax.lax.offset->int(offset)
A:jax._src.lax.lax.bool_eye->eq(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)), broadcasted_iota(np.int32, shape, 1))
A:jax._src.lax.lax.axes->frozenset(axes)
A:jax._src.lax.lax.base_shape->tuple(np.take(shape, axes))
A:jax._src.lax.lax.bool_tri->ge(add(broadcasted_iota(np.int32, shape, 0), np.int32(offset)), broadcasted_iota(np.int32, shape, 1))
A:jax._src.lax.lax.exponent_bits->operator.index(exponent_bits)
A:jax._src.lax.lax.mantissa_bits->operator.index(mantissa_bits)
A:jax._src.lax.lax.dimensions->tuple(np.add(1, dimensions))
A:jax._src.lax.lax.dims_set->set(dimensions)
A:jax._src.lax.lax.val->jax._src.dtypes.scalar_type_of(example)(val)
A:jax._src.lax.lax.size->next((x.shape[ax] for (x, ax) in zip(batched_args, batch_dims) if ax is not None))
A:jax._src.lax.lax.batch->tuple(range(lhs.ndim - 2))
A:jax._src.lax.lax.ShapedArray.broadcast->jax.core.aval_method(broadcast)
A:jax._src.lax.lax.ShapedArray.transpose->jax.core.aval_method(transpose)
A:jax._src.lax.lax.ShapedArray.reshape->jax.core.aval_method(reshape)
A:jax._src.lax.lax.n->numpy.prod(input_shape[list(axes)])
A:jax._src.lax.lax.ShapedArray._iter->staticmethod(_iter)
A:jax._src.lax.lax.core.DShapedArray._iter->staticmethod(_iter)
A:jax._src.lax.lax.typename->str(np.dtype(dtype).name)
A:jax._src.lax.lax.dtype_rule->partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name)
A:jax._src.lax.lax.weak_type_rule->partial(_naryop_weak_type_rule, name)
A:jax._src.lax.lax.prim->standard_primitive(shape_rule, dtype_rule, name, weak_type_rule=weak_type_rule)
A:jax._src.lax.lax.standard_unop->partial(unop, _identity)
A:jax._src.lax.lax.typenames->', '.join((t.__name__ for t in types))
A:jax._src.lax.lax.pos->next((i for (i, aval) in enumerate(avals) if aval.dtype == dtypes.float0))
A:jax._src.lax.lax.shape_rule->partial(broadcasting_shape_rule, name)
A:jax._src.lax.lax.standard_naryop->partial(naryop, _input_dtype)
A:jax._src.lax.lax.bcast_dims->tuple(range(len(aval_out.shape) - len(aval_in.shape), len(aval_out.shape)))
A:jax._src.lax.lax.arg->xops.BroadcastInDim(arg, aval_out.shape, bcast_dims)
A:jax._src.lax.lax.x_shape->numpy.shape(x)
A:jax._src.lax.lax.substitute->partial(_substitute_axis_sizes_in_aval, ctx.axis_size_env)
A:jax._src.lax.lax.avals_in->map(substitute, avals_in)
A:jax._src.lax.lax.aval_out->substitute(aval_out)
A:jax._src.lax.lax.broadcasted_args->broadcast_mhlo(aval_out, avals_in, args)
A:jax._src.lax.lax.neg_p->standard_unop(_num, 'neg')
A:jax._src.lax.lax.sign_p->standard_unop(_num, 'sign')
A:jax._src.lax.lax.nextafter_p->standard_naryop([_float, _float], 'nextafter')
A:jax._src.lax.lax.floor_p->standard_unop(_float, 'floor')
A:jax._src.lax.lax.ceil_p->standard_unop(_float, 'ceil')
A:jax._src.lax.lax.round_p->standard_unop(_float, 'round')
A:jax._src.lax.lax.is_finite_p->unop(_fixed_dtype(np.bool_), _float, 'is_finite')
A:jax._src.lax.lax.exp_p->standard_unop(_float | _complex, 'exp')
A:jax._src.lax.lax.log_p->standard_unop(_float | _complex, 'log')
A:jax._src.lax.lax.expm1_p->standard_unop(_float | _complex, 'expm1')
A:jax._src.lax.lax.log1p_p->standard_unop(_float | _complex, 'log1p')
A:jax._src.lax.lax.tanh_p->standard_unop(_float | _complex, 'tanh')
A:jax._src.lax.lax.logistic_p->standard_unop(_float | _complex, 'logistic')
A:jax._src.lax.lax.one->_const(x, 1)
A:jax._src.lax.lax.sin_p->standard_unop(_float | _complex, 'sin')
A:jax._src.lax.lax.cos_p->standard_unop(_float | _complex, 'cos')
A:jax._src.lax.lax.tan_p->standard_unop(_float | _complex, 'tan')
A:jax._src.lax.lax.asin_p->standard_unop(_float | _complex, 'asin')
A:jax._src.lax.lax.rpart->real(result)
A:jax._src.lax.lax.acos_p->standard_unop(_float | _complex, 'acos')
A:jax._src.lax.lax.atan_p->standard_unop(_float | _complex, 'atan')
A:jax._src.lax.lax.atan2_p->standard_naryop([_float | _complex, _float | _complex], 'atan2')
A:jax._src.lax.lax.sinh_p->standard_unop(_float | _complex, 'sinh')
A:jax._src.lax.lax.cosh_p->standard_unop(_float | _complex, 'cosh')
A:jax._src.lax.lax.asinh_p->standard_unop(_float | _complex, 'asinh')
A:jax._src.lax.lax.acosh_p->standard_unop(_float | _complex, 'acosh')
A:jax._src.lax.lax.atanh_p->standard_unop(_float | _complex, 'atanh')
A:jax._src.lax.lax.regularized_incomplete_beta_p->standard_naryop([_float, _float, _float], 'regularized_incomplete_beta')
A:jax._src.lax.lax.partial_x->exp((b - 1) * log1p(-x) + (a - 1) * log(x) - lbeta)
A:jax._src.lax.lax.lgamma_p->standard_unop(_float, 'lgamma')
A:jax._src.lax.lax.digamma_p->standard_unop(_float, 'digamma')
A:jax._src.lax.lax.igamma_p->standard_naryop([_float, _float], 'igamma')
A:jax._src.lax.lax.igamma_grad_a_p->standard_naryop([_float, _float], 'igamma_grad_a')
A:jax._src.lax.lax.igammac_p->standard_naryop([_float, _float], 'igammac')
A:jax._src.lax.lax.random_gamma_grad_p->standard_naryop([_float, _float], 'random_gamma_grad')
A:jax._src.lax.lax.bessel_i0e_p->standard_unop(_float, 'bessel_i0e')
A:jax._src.lax.lax.bessel_i1e_p->standard_unop(_float, 'bessel_i1e')
A:jax._src.lax.lax.safe_x->select(x_is_not_tiny, x, full_like(x, eps))
A:jax._src.lax.lax.dy_dx->select(x_is_not_tiny, dy_dx, full_like(x, 0.5))
A:jax._src.lax.lax.erf_p->standard_unop(_float, 'erf')
A:jax._src.lax.lax.erfc_p->standard_unop(_float, 'erfc')
A:jax._src.lax.lax.erf_inv_p->standard_unop(_float, 'erf_inv')
A:jax._src.lax.lax.real_p->unop(_complex_basetype, _complex, 'real')
A:jax._src.lax.lax.imag_p->unop(_complex_basetype, _complex, 'imag')
A:jax._src.lax.lax.complex_p->naryop(_complex_dtype, [_complex_elem_types, _complex_elem_types], 'complex')
A:jax._src.lax.lax.conj_p->unop(_complex_dtype, _complex_elem_types | _complex, 'conj')
A:jax._src.lax.lax.ad.primitive_jvps[conj_p]->partial(ad.linear_jvp, conj_p)
A:jax._src.lax.lax.abs_p->unop(_complex_basetype, _num, 'abs')
A:jax._src.lax.lax.sqrt_p->standard_unop(_float | _complex, 'sqrt')
A:jax._src.lax.lax.rsqrt_p->standard_unop(_float | _complex, 'rsqrt')
A:jax._src.lax.lax.cbrt_p->standard_unop(_float, 'cbrt')
A:jax._src.lax.lax.pow_p->standard_naryop([_float | _complex, _float | _complex], 'pow')
A:jax._src.lax.lax.integer_pow_p->standard_primitive(_attrgetter('shape'), _integer_pow_dtype_rule, 'integer_pow')
A:jax._src.lax.lax.x->convert_element_type(x, np.float32)
A:jax._src.lax.lax.lowering->jax.interpreters.mlir.cache_lowering(lowering)
A:jax._src.lax.lax.not_p->standard_unop(_bool_or_int, 'not')
A:jax._src.lax.lax.and_p->standard_naryop([_bool_or_int, _bool_or_int], 'and')
A:jax._src.lax.lax.or_p->standard_naryop([_bool_or_int, _bool_or_int], 'or')
A:jax._src.lax.lax.xor_p->standard_naryop([_bool_or_int, _bool_or_int], 'xor')
A:jax._src.lax.lax.population_count_p->standard_unop(_int, 'population_count')
A:jax._src.lax.lax.clz_p->standard_unop(_int, 'clz')
A:jax._src.lax.lax.primal_out->sub(x, y)
A:jax._src.lax.lax.sub_p->standard_naryop([_num, _num], 'sub')
A:jax._src.lax.lax.mul_p->standard_naryop([_num, _num], 'mul')
A:jax._src.lax.lax.div_p->standard_naryop([_num, _num], 'div')
A:jax._src.lax.lax.rem_p->standard_naryop([_int | _float, _int | _float], 'rem')
A:jax._src.lax.lax.y->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim').bind(operand, *dyn_shape, shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax._src.lax.lax.rx->real(x)
A:jax._src.lax.lax.ry->real(y)
A:jax._src.lax.lax.pick_x->select(eq(rx, ry), lax_cmp_pick_x(imag(x), imag(y)), lax_cmp_pick_x(rx, ry))
A:jax._src.lax.lax.shift_left_p->standard_naryop([_int, _int], 'shift_left')
A:jax._src.lax.lax.shift_right_arithmetic_p->standard_naryop([_int, _int], 'shift_right_arithmetic')
A:jax._src.lax.lax.shift_right_logical_p->standard_naryop([_int, _int], 'shift_right_logical')
A:jax._src.lax.lax.(x, y)->broadcast_mhlo(aval_out.update(dtype=x_aval.dtype), ctx.avals_in, (x, y))
A:jax._src.lax.lax.eq_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'eq')
A:jax._src.lax.lax.ne_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'ne')
A:jax._src.lax.lax.ge_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'ge')
A:jax._src.lax.lax.gt_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'gt')
A:jax._src.lax.lax.le_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'le')
A:jax._src.lax.lax.lt_p->naryop(_fixed_dtype(np.bool_), [_ordered, _ordered], 'lt')
A:jax._src.lax.lax.lhs->jax.core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)
A:jax._src.lax.lax.convert_element_type_p->Primitive('convert_element_type')
A:jax._src.lax.lax.aval_in->aval_in.update(dtype=_real_dtype(aval_in.dtype)).update(dtype=_real_dtype(aval_in.dtype))
A:jax._src.lax.lax.bitcast_convert_type_p->standard_primitive(_bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule, 'bitcast_convert_type', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.config->jax._src.lib.xla_client.PrecisionConfig()
A:jax._src.lax.lax.lhs_batch_shape->tuple((lhs.shape[i] for i in lhs_batch))
A:jax._src.lax.lax.rhs_batch_shape->tuple((rhs.shape[i] for i in rhs_batch))
A:jax._src.lax.lax.lhs_contracting_shape->tuple((lhs.shape[i] for i in lhs_contracting))
A:jax._src.lax.lax.rhs_contracting_shape->tuple((rhs.shape[i] for i in rhs_contracting))
A:jax._src.lax.lax.batch_shape->tuple((lhs_shape[i] for i in lhs_batch))
A:jax._src.lax.lax.lhs_contract_or_batch->tuple(sorted(tuple(lhs_contracting) + tuple(lhs_batch)))
A:jax._src.lax.lax.lhs_tensored_shape->tuple_delete(lhs_shape, lhs_contract_or_batch)
A:jax._src.lax.lax.rhs_contract_or_batch->tuple(sorted(tuple(rhs_contracting) + tuple(rhs_batch)))
A:jax._src.lax.lax.rhs_tensored_shape->tuple_delete(rhs_shape, rhs_contract_or_batch)
A:jax._src.lax.lax.idx_->set(idx)
A:jax._src.lax.lax.input_dtype->naryop_dtype_rule(_input_dtype, [_any, _any], 'dot_general', lhs, rhs)
A:jax._src.lax.lax.x_kept->remaining(range(x_ndim), x_contract, x_batch)
A:jax._src.lax.lax.y_kept->remaining(range(y.ndim), y_contract, y_batch)
A:jax._src.lax.lax.(ans_batch, ans_y, _)->ranges_like(x_batch, y_kept, x_kept)
A:jax._src.lax.lax.(ans_batch, _, ans_y)->ranges_like(x_batch, x_kept, y_kept)
A:jax._src.lax.lax.x_contract_sorted_by_y->list(np.take(x_contract, np.argsort(y_contract)))
A:jax._src.lax.lax.out_axes->numpy.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
A:jax._src.lax.lax.(new_dimension_numbers, result_batch_dim)->_dot_general_batch_dim_nums((lhs.ndim, rhs.ndim), batch_dims, dimension_numbers)
A:jax._src.lax.lax.batched_out->dot_general(lhs, rhs, new_dimension_numbers, precision=precision, preferred_element_type=preferred_element_type)
A:jax._src.lax.lax.lhs_contract->bump_dims(lhs_contract, lbd)
A:jax._src.lax.lax.rhs_contract->bump_dims(rhs_contract, rbd)
A:jax._src.lax.lax.other->tuple((d for d in range(rhs_ndim) if d not in rhs_batch and d not in rhs_contract))
A:jax._src.lax.lax.lhs_batch->bump_dims(lhs_batch, lbd)
A:jax._src.lax.lax.rhs_batch->bump_dims(rhs_batch, rbd)
A:jax._src.lax.lax.lhs_->_replace_masked_values(lhs, 0, padded_axes)
A:jax._src.lax.lax.dot_general_p->standard_primitive(_dot_general_shape_rule, _dot_general_dtype_rule, 'dot_general')
A:jax._src.lax.lax.f32->jax.interpreters.mlir.dtype_to_ir_type(np.dtype(np.float32))
A:jax._src.lax.lax.dot_dnums->jax._src.lib.mlir.dialects.mhlo.DotDimensionNumbers.get(lhs_batching_dimensions=list(lhs_batch), rhs_batching_dimensions=list(rhs_batch), lhs_contracting_dimensions=list(lhs_contracting), rhs_contracting_dimensions=list(rhs_contracting))
A:jax._src.lax.lax.operand_ndim->numpy.ndim(operand)
A:jax._src.lax.lax.(out_aval, effects)->Primitive('iota').abstract_eval(dtype=dtype, shape=shape, dimension=dimension)
A:jax._src.lax.lax.out_shape->_ceil_divide(in_shape, window_strides)
A:jax._src.lax.lax.out_aval->jax.core.DShapedArray(tuple(out_shape), dtype, False)
A:jax._src.lax.lax.bdims->tuple(np.delete(broadcast_dimensions, unit_dims))
A:jax._src.lax.lax.new_operand->jax.interpreters.batching.moveaxis(operand, bdim, 0)
A:jax._src.lax.lax.params->dict(dtype=dtype, shape=shape, dimension=dimension)
A:jax._src.lax.lax.y_dot->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim').bind(operand_dot, *dyn_shape, shape=shape, broadcast_dimensions=broadcast_dimensions)
A:jax._src.lax.lax.operand_tracer->trace.instantiate_const(operand)
A:jax._src.lax.lax.dyn_shape_tracers->map(trace.instantiate_const, dyn_shape)
A:jax._src.lax.lax.dyn_shape_tracers_->iter(dyn_shape_tracers)
A:jax._src.lax.lax.broadcast_in_dim_p->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
A:jax._src.lax.lax._clamp_dtype_rule->partial(naryop_dtype_rule, _input_dtype, [_any, _any, _any], 'clamp')
A:jax._src.lax.lax.min->broadcast_in_dim(min, x.shape, [0])
A:jax._src.lax.lax.max->broadcast_in_dim(max, x.shape, [0])
A:jax._src.lax.lax.clamp_p->standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')
A:jax._src.lax.lax.op->jax._src.lib.mlir.dialects.mhlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes))
A:jax._src.lax.lax.concat_size->sum((o.shape[dimension] for o in operands))
A:jax._src.lax.lax.limit_points->numpy.cumsum([shape[dimension] for shape in operand_shapes]).tolist()
A:jax._src.lax.lax.starts->numpy.zeros((len(operands), t.ndim), dtype=int).tolist()
A:jax._src.lax.lax.limits->numpy.tile(t.shape, (len(operands), 1)).tolist()
A:jax._src.lax.lax.concatenate_p->standard_primitive(_concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate')
A:jax._src.lax.lax.op_shape->numpy.shape(operand)
A:jax._src.lax.lax.(lo, hi, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax.lax.unpad_config->safe_zip(np.negative(lo), np.negative(hi), np.zeros_like(interior))
A:jax._src.lax.lax.unpadded->pad(t, np.array(0.0, t.dtype), unpad_config)
A:jax._src.lax.lax.padding_config->list(padding_config)
A:jax._src.lax.lax.mask->pad(full_like(operand, True, np.bool_), False, padding_config)
A:jax._src.lax.lax.broadcasted_padding->broadcast_in_dim(padding_value, x.shape, (operand_bdim,))
A:jax._src.lax.lax.pad_p->standard_primitive(_pad_shape_rule, _pad_dtype_rule, 'pad')
A:jax._src.lax.lax.(low, high, interior)->jax._src.util.unzip3(padding_config)
A:jax._src.lax.lax.squeeze_p->standard_primitive(_squeeze_shape_rule, _squeeze_dtype_rule, 'squeeze')
A:jax._src.lax.lax.d2->next(new, None)
A:jax._src.lax.lax.av->jax.core.DShapedArray(_merge_dyn_shape(new_sizes, dyn), x.dtype, x.weak_type)
A:jax._src.lax.lax.reshape_p->standard_primitive(_reshape_shape_rule, _reshape_dtype_rule, 'reshape')
A:jax._src.lax.lax.rev_p->standard_primitive(_rev_shape_rule, _input_dtype, 'rev')
A:jax._src.lax.lax.transpose_p->standard_primitive(_transpose_shape_rule, _input_dtype, 'transpose')
A:jax._src.lax.lax.zeros->full_like(t, 0)
A:jax._src.lax.lax.which->broadcast_in_dim(which, cases[0].shape, [0])
A:jax._src.lax.lax.out_dot->select_n(which, *case_tangents)
A:jax._src.lax.lax.z->_zeros(next((t for t in case_tangents if type(t) is not ad_util.Zero)))
A:jax._src.lax.lax.pred->jax.interpreters.mlir.compare_mhlo(which, mlir.full_like_aval(offset + mid, which_aval), lt, compare_type)
A:jax._src.lax.lax.select_n_p->standard_primitive(_select_shape_rule, _select_dtype_rule, 'select_n', weak_type_rule=_select_weak_type_rule)
A:jax._src.lax.lax.(operand_avals, init_val_avals)->split_list(avals, [len(avals) // 2])
A:jax._src.lax.lax.(operands, init_values)->jax._src.util.split_list(values, [len(values) // 2])
A:jax._src.lax.lax.(operand_bdims, init_value_bdims)->split_list(batch_dims, [num_operands])
A:jax._src.lax.lax.input_shape->numpy.array(primals[0].shape, dtype=np.int_)
A:jax._src.lax.lax.non_axes->numpy.delete(np.arange(len(input_shape)), axes)
A:jax._src.lax.lax.primals->Primitive('sort').bind(*primals + (iotas[dimension],), dimension=dimension, is_stable=is_stable, num_keys=num_keys)
A:jax._src.lax.lax.tangents->tuple((reshape(t, new_shape, permutation) for t in tangents))
A:jax._src.lax.lax.reducer->jax._src.lib.mlir.dialects.mhlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes)).regions[0].blocks.append(*ir_types + ir_types)
A:jax._src.lax.lax.xs->reducer(*xs1 + xs2)
A:jax._src.lax.lax.(primal_xs, init_values)->split_list(primals, [len(primals) // 2])
A:jax._src.lax.lax.(tangent_xs, tangent_init)->split_list(tangents, [len(tangents) // 2])
A:jax._src.lax.lax.(operand_avals, init_avals)->split_list(avals, [num_operands])
A:jax._src.lax.lax.join->jax.core.join_named_shapes(*(a.named_shape for a in operand_avals))
A:jax._src.lax.lax.reduce_p->jax.core.Primitive('reduce')
A:jax._src.lax.lax.reducer_ctx->ctx.module_context.replace(name_stack=util.new_name_stack())
A:jax._src.lax.lax.(out_nodes, _)->jax.interpreters.mlir.jaxpr_subcomp(reducer_ctx, jaxpr, mlir.TokenSet(), consts, *([a] for a in reducer.arguments))
A:jax._src.lax.lax.broadcast_dimensions->tuple(np.delete(np.arange(len(input_shape)), axes))
A:jax._src.lax.lax.operand_->_replace_masked_values(operand, ident(aval.dtype), padded_axes)
A:jax._src.lax.lax.reduce_sum_p->standard_primitive(_reduce_sum_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_sum'), 'reduce_sum')
A:jax._src.lax.lax.pe.padding_rules[reduce_sum_p]->partial(_reducer_padding, _reduce_sum, _get_sum_identity)
A:jax._src.lax.lax.(primals_out, tangents_out)->_reduce_jvp(reducer, [_const(primals[0], 1)], primals, tangents, axes)
A:jax._src.lax.lax.reduce_prod_p->standard_primitive(_reduce_op_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_prod'), 'reduce_prod')
A:jax._src.lax.lax.pe.padding_rules[reduce_prod_p]->partial(_reducer_padding, _reduce_prod, _get_prod_identity)
A:jax._src.lax.lax.location_indicators->convert_element_type(_eq_meet(operand, reshape(ans, shape)), g.dtype)
A:jax._src.lax.lax.counts->_reduce_sum(location_indicators, axes)
A:jax._src.lax.lax.reduce_max_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_max')
A:jax._src.lax.lax.pe.padding_rules[reduce_max_p]->partial(_reducer_padding, _reduce_max, _get_max_identity)
A:jax._src.lax.lax.reduce_min_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_min')
A:jax._src.lax.lax.pe.padding_rules[reduce_min_p]->partial(_reducer_padding, _reduce_min, _get_min_identity)
A:jax._src.lax.lax.indices->broadcasted_iota(index_dtype, np.shape(operand), axis)
A:jax._src.lax.lax.pick_op_val->bitwise_or(value_comparator(op_val, acc_val), ne(op_val, op_val))
A:jax._src.lax.lax.pick_op_index->bitwise_or(pick_op_val, bitwise_and(eq(op_val, acc_val), lt(op_index, acc_index)))
A:jax._src.lax.lax.res->reduce([operand, indices], [get_identity(operand.dtype), np.array(0, index_dtype)], reducer_fn, axes)
A:jax._src.lax.lax.argmin_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmin', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.argmax_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmax', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_or_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_or', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_and_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_and', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.reduce_xor_p->standard_primitive(_reduce_logical_shape_rule, _input_dtype, 'reduce_xor', weak_type_rule=_strip_weak_type)
A:jax._src.lax.lax.scalar_type->jax.interpreters.mlir.aval_to_ir_type(core.ShapedArray((), dtype))
A:jax._src.lax.lax.reducer_region->jax._src.lib.mlir.dialects.mhlo.ReduceOp([mlir.aval_to_ir_type(aval_out)], [x], mlir.ir_constants(unit_factory(aval_out.dtype)), mlir.dense_int_elements(axes)).regions[0].blocks.append(scalar_type, scalar_type)
A:jax._src.lax.lax.add->reducer(*reducer_region.arguments)
A:jax._src.lax.lax.reduce_precision_p->standard_primitive(_reduce_precision_shape_rule, partial(unop_dtype_rule, _identity, _float, 'reduce_precision'), name='reduce_precision')
A:jax._src.lax.lax.args->tuple((raise_to_shaped(arg) for arg in args))
A:jax._src.lax.lax.shapes->' '.join((str(a.shape) for a in args))
A:jax._src.lax.lax.signed->select(_isnan(x), full_like(signed, signed_nan), signed)
A:jax._src.lax.lax.unsigned->bitcast_convert_type(x, unsigned_dtype)
A:jax._src.lax.lax.signed_nan->convert_element_type(x, np.float32).dtype.type(np.nan).view(signed_dtype)
A:jax._src.lax.lax.flipped->bitcast_convert_type(sub(unsigned_dtype.type(np.iinfo(signed_dtype).max), unsigned), signed_dtype)
A:jax._src.lax.lax.(x_keys, y_keys)->_operands_to_keys(*operands, num_keys=num_keys)
A:jax._src.lax.lax.idx->tuple((primals[-1] if i == dimension else iotas[i] for i in range(len(shape))))
A:jax._src.lax.lax.tangents_out->tuple((t if type(t) is ad_util.Zero else t[idx] for t in tangents))
A:jax._src.lax.lax.(prototype_arg, new_bdim)->next(((a, b) for (a, b) in zip(batched_args, batch_dims) if b is not None))
A:jax._src.lax.lax.sort_p->Primitive('sort')
A:jax._src.lax.lax.sort->jax._src.lib.mlir.dialects.mhlo.SortOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out], mlir.flatten_lowering_ir_args(operands), dimension=mlir.i64_attr(dimension), is_stable=ir.BoolAttr.get(is_stable))
A:jax._src.lax.lax.scalar_types->safe_map(mlir.aval_to_ir_type, scalar_avals)
A:jax._src.lax.lax.comparator->jax._src.lib.mlir.dialects.mhlo.SortOp([mlir.aval_to_ir_type(aval) for aval in ctx.avals_out], mlir.flatten_lowering_ir_args(operands), dimension=mlir.i64_attr(dimension), is_stable=ir.BoolAttr.get(is_stable)).comparator.blocks.append(*util.flatten(zip(scalar_types, scalar_types)))
A:jax._src.lax.lax.lower_comparator->jax.interpreters.mlir.lower_fun(partial(_sort_lt_comparator), multiple_results=False)
A:jax._src.lax.lax.sub_ctx->ctx.replace(primitive=None, avals_in=util.flatten(zip(scalar_avals, scalar_avals)), avals_out=[core.ShapedArray((), np.bool_)])
A:jax._src.lax.lax.primals_out->top_k(operand, k)
A:jax._src.lax.lax.tangent_out->jax._src.lax.slicing.gather(tangent, gather_indices, dnums, slice_sizes)
A:jax._src.lax.lax.rank->len(idx_shape)
A:jax._src.lax.lax._iota->broadcast_in_dim(_iota, gather_index_shape, (i,))
A:jax._src.lax.lax.gather_indices->concatenate(gather_indices, dimension=rank)
A:jax._src.lax.lax.dnums->jax._src.lax.slicing.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=tuple(range(rank)), start_index_map=tuple(range(rank)))
A:jax._src.lax.lax.perm->numpy.arange(operand.ndim)
A:jax._src.lax.lax.(top_k_v, top_k_i)->top_k(transpose(operand, perm), k=k)
A:jax._src.lax.lax.top_k_p->Primitive('top_k')
A:jax._src.lax.lax.create_token_p->Primitive('create_token')
A:jax._src.lax.lax.after_all_p->Primitive('after_all')
A:jax._src.lax.lax.InOutFeedEffect->enum.Enum('InOutFeedEffect', ['Infeed', 'Outfeed'])
A:jax._src.lax.lax.(flat_shapes, treedef)->jax._src.lib.pytree.flatten(shape)
A:jax._src.lax.lax.xs_and_token->Primitive('infeed').bind(token, shapes=tuple(flat_shapes), partitions=partitions)
A:jax._src.lax.lax.infeed_p->Primitive('infeed')
A:jax._src.lax.lax.output_types->safe_map(mlir.aval_to_ir_types, ctx.avals_out[:-1])
A:jax._src.lax.lax.flat_output_types->jax._src.util.flatten(output_types)
A:jax._src.lax.lax.layouts->jax._src.lib.mlir.ir.ArrayAttr.get([ir.ArrayAttr.get([mlir.i64_attr(i) for i in range(len(aval.shape) - 1, -1, -1)]) for aval in shapes])
A:jax._src.lax.lax.infeed->jax._src.lib.mlir.dialects.mhlo.InfeedOp(flat_output_types + [mhlo.TokenType.get()], token, infeed_config=ir.StringAttr.get(''), layout=layouts)
A:jax._src.lax.lax.(flat_xs, _)->jax._src.lib.pytree.flatten(xs)
A:jax._src.lax.lax.outfeed_p->Primitive('outfeed')
A:jax._src.lax.lax.outfeed->jax._src.lib.mlir.dialects.mhlo.OutfeedOp(mlir.aval_to_ir_type(token_aval), mlir.flatten_lowering_ir_args(xs), token, outfeed_config=ir.StringAttr.get(''))
A:jax._src.lax.lax.rng_uniform_p->Primitive('rng_uniform')
A:jax._src.lax.lax.(shape,)->jax.interpreters.mlir.ir_constants(np.array(aval_out.shape, np.int64), canonicalize_types=False)
A:jax._src.lax.lax.key_type->jax._src.lib.mlir.ir.RankedTensorType(key.type)
A:jax._src.lax.lax.u32_type->jax._src.lib.mlir.ir.IntegerType.get_unsigned(32)
A:jax._src.lax.lax.u64_type->jax._src.lib.mlir.ir.IntegerType.get_unsigned(64)
A:jax._src.lax.lax.etype->jax.interpreters.mlir.dtype_to_ir_type(dtype)
A:jax._src.lax.lax.algorithm_attr->_rng_algorithm(algorithm)
A:jax._src.lax.lax.rng_bit_generator_p->Primitive('rng_bit_generator')
A:jax._src.lax.lax.copy_p->jax.core.Primitive('copy')
A:jax._src.lax.lax.iota_p->Primitive('iota')
A:jax._src.lax.lax.pad_sizes->numpy.maximum(0, (out_shape - 1) * window_strides + window_shape - in_shape)
A:jax._src.lax.lax.types->map(np.dtype, ttypes)
A:jax._src.lax.lax.obj_arr->numpy.array(obj)
A:jax._src.lax.lax.x_len->len(x)
A:jax._src.lax.lax.removed->set(itertools.chain(*removed_lists))
A:jax._src.lax.lax.higher_dtype->jax._src.dtypes.promote_types(a_dtype, b_dtype)
A:jax._src.lax.lax.a->convert_element_type(a, b_dtype)
A:jax._src.lax.lax.b->convert_element_type(b, a_dtype)
A:jax._src.lax.lax.np_dtype->numpy.dtype(dtype)
A:jax._src.lax.lax.empty_p->jax.core.Primitive('empty')
A:jax._src.lax.lax.buf.aval->jax.core.ShapedArray(buf.shape, buf.dtype)
jax._src.lax.lax.BIntRules
jax._src.lax.lax.BIntRules.aval_to_ir_types(aval)
jax._src.lax.lax.BIntRules.result_handler(sticky_device,aval)
jax._src.lax.lax.Precision(self,arg0)
jax._src.lax.lax.Precision.__init__(self,arg0)
jax._src.lax.lax.Precision.__str__(self)->str
jax._src.lax.lax.RoundingMethod(enum.IntEnum)
jax._src.lax.lax._abs_jvp_rule(g,ans,x)
jax._src.lax.lax._abstractify(x)
jax._src.lax.lax._add_inverse(r,x,y)
jax._src.lax.lax._add_jvp(primals,tangents)
jax._src.lax.lax._add_transpose(t,x,y)
jax._src.lax.lax._after_all_abstract_eval(*operands)
jax._src.lax.lax._after_all_lowering(ctx,*operands)
jax._src.lax.lax._argminmax_dtype_rule(operand,*,axes,index_dtype)
jax._src.lax.lax._argminmax_shape_rule(operand,*,axes,index_dtype)
jax._src.lax.lax._array_copy(arr:ArrayLike)->Array
jax._src.lax.lax._balanced_eq(x,z,y)
jax._src.lax.lax._bessel_i1e_jvp(g,y,x)
jax._src.lax.lax._bitcast_convert_type_dtype_rule(operand,*,new_dtype)
jax._src.lax.lax._bitcast_convert_type_lower(ctx,operand,*,new_dtype)
jax._src.lax.lax._bitcast_convert_type_shape_rule(operand,*,new_dtype)
jax._src.lax.lax._broadcast_in_dim_abstract_eval(x,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_batch_rule(batched_args,batch_dims,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_fwd_rule(eqn)
jax._src.lax.lax._broadcast_in_dim_jvp_rule(primals,tangents,*,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_lower(ctx,x,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_padding_rule(in_avals,out_avals,x,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_partial_eval(trace,operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_pp_rule(eqn,context,settings)
jax._src.lax.lax._broadcast_in_dim_shape_rule(operand,*,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_staging_rule(trace,x,*dyn,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_transpose_rule(ct,operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_in_dim_typecheck_rule(operand,*dyn_shape,shape,broadcast_dimensions)
jax._src.lax.lax._broadcast_ranks(s1,s2)
jax._src.lax.lax._broadcast_shapes_cached(*shapes:Tuple[int,...])->Tuple[int, ...]
jax._src.lax.lax._broadcast_shapes_uncached(*shapes)
jax._src.lax.lax._broadcast_translate(op,ctx,avals_in,avals_out,*args)
jax._src.lax.lax._ceil_divide(x1,x2)
jax._src.lax.lax._check_same_dtypes(name,ignore_fp_precision,*ttypes)
jax._src.lax.lax._check_shapelike(fun_name,arg_name,obj,non_zero_shape=False)
jax._src.lax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax._src.lax.lax._clamp_batch_rule(batched_args,batch_dims,**params)
jax._src.lax.lax._clamp_shape_rule(min,operand,max)
jax._src.lax.lax._compare_lower_mhlo(direction:str,ctx,x,y)
jax._src.lax.lax._complex_transpose_rule(t,x,y)
jax._src.lax.lax._compute_argminmax(value_comparator,get_identity,operand,*,index_dtype,axes)
jax._src.lax.lax._compute_squeeze_shape(shape,dimensions)
jax._src.lax.lax._concatenate_batch_rule(batched_args,batch_dims,*,dimension)
jax._src.lax.lax._concatenate_dtype_rule(*operands,**kwargs)
jax._src.lax.lax._concatenate_lower(ctx,*xs,dimension)
jax._src.lax.lax._concatenate_pad_rule(in_avals,out_avals,*operands,dimension)
jax._src.lax.lax._concatenate_shape_rule(*operands,**kwargs)
jax._src.lax.lax._concatenate_transpose_rule(t,*operands,dimension)
jax._src.lax.lax._conj_impl(x,**kw)
jax._src.lax.lax._conj_transpose_rule(t,x,*,input_dtype)
jax._src.lax.lax._const(example,val)
jax._src.lax.lax._convert_element_type(operand:ArrayLike,new_dtype:Optional[DTypeLike]=None,weak_type:bool=False)
jax._src.lax.lax._convert_element_type_dtype_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_jvp_rule(tangent,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_lower(ctx,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_shape_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_transpose_rule(ct,operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_element_type_weak_type_rule(operand,*,new_dtype,weak_type)
jax._src.lax.lax._convert_elt_type_folding_rule(consts,eqn)
jax._src.lax.lax._convert_elt_type_fwd_rule(eqn)
jax._src.lax.lax._convert_elt_type_pp_rule(eqn,context,settings)
jax._src.lax.lax._create_token_lowering(ctx,*operands)
jax._src.lax.lax._delta(dtype:DTypeLike,shape:Shape,axes:Sequence[int])->Array
jax._src.lax.lax._dilate_shape(shape,dilation)
jax._src.lax.lax._div_transpose_rule(cotangent,x,y)
jax._src.lax.lax._dot_general_batch_dim_nums(ndims,batch_dims,dimension_numbers)
jax._src.lax.lax._dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_dtype_rule(lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_lower(ctx,lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[np.dtype])
jax._src.lax.lax._dot_general_padding_rule(in_avals,out_avals,lhs,rhs,*,dimension_numbers,**params)
jax._src.lax.lax._dot_general_pp_rule(eqn,context,settings)
jax._src.lax.lax._dot_general_shape_computation(lhs_shape,rhs_shape,dimension_numbers)
jax._src.lax.lax._dot_general_shape_rule(lhs,rhs,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dot_general_transpose_lhs(g,y,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike],swap_ans=False)
jax._src.lax.lax._dot_general_transpose_rhs(g,x,*,dimension_numbers,precision,preferred_element_type:Optional[DTypeLike])
jax._src.lax.lax._dyn_shape_staging_rule(trace,prim,out_aval,*args,**params)
jax._src.lax.lax._empty_lower(ctx,*,dtype)
jax._src.lax.lax._enum_descriptor(self,val)
jax._src.lax.lax._enum_descriptor.__get__(self,_,owner)
jax._src.lax.lax._enum_descriptor.__init__(self,val)
jax._src.lax.lax._eq_meet(a,b)
jax._src.lax.lax._extract_tracers_dyn_shape(shape:Sequence[Union[int,core.Tracer]])->Tuple[List[core.Tracer], List[Optional[int]]]
jax._src.lax.lax._eye(dtype:DTypeLike,shape:Shape,offset:int)->Array
jax._src.lax.lax._float_to_int_for_sort(x)
jax._src.lax.lax._get_bitwise_and_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_bitwise_or_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_max_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_min_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_monoid_reducer(monoid_op:Callable,xs:Sequence[Array])->Optional[Callable]
jax._src.lax.lax._get_prod_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._get_sum_identity(dtype:DTypeLike)->np.ndarray
jax._src.lax.lax._identity(x)
jax._src.lax.lax._infeed_abstract_eval(token,*,shapes,partitions)
jax._src.lax.lax._infeed_lowering(ctx,token,*,shapes,partitions)
jax._src.lax.lax._integer_pow(x,*,y)
jax._src.lax.lax._integer_pow_dtype_rule(x,*,y)
jax._src.lax.lax._integer_pow_jvp(g,x,*,y)
jax._src.lax.lax._integer_pow_lowering(ctx,x,*,y)
jax._src.lax.lax._iota_abstract_eval(*,dtype,shape,dimension)
jax._src.lax.lax._iota_lower(ctx,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_padding_rule(in_avals,out_avals,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_pp_rule(eqn,context,settings)
jax._src.lax.lax._iota_staging_rule(trace,*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._iota_typecheck_rule(*dyn_shape,dtype,shape,dimension)
jax._src.lax.lax._is_array_or_tracer(operand:Any)->bool
jax._src.lax.lax._is_singleton_reshape(old,new)
jax._src.lax.lax._iscomplex(x)->bool
jax._src.lax.lax._isnan(x:ArrayLike)->Array
jax._src.lax.lax._iter(tracer)
jax._src.lax.lax._masked(padded_value,logical_shape,dimensions,value=0)
jax._src.lax.lax._maybe_broadcast(target_shape,x)
jax._src.lax.lax._merge_dyn_shape(static_shape:Sequence[Optional[int]],dyn_shape:Sequence[Any])->Tuple[Union[int, mlir.Value], ...]
jax._src.lax.lax._minmax_complex_lowering(x,y,*,lax_cmp_pick_x)
jax._src.lax.lax._mul_inverse(r,x,y)
jax._src.lax.lax._mul_transpose(ct,x,y)
jax._src.lax.lax._nary_lower_mhlo(op:Callable,ctx,*args:Union[ir.Value,Sequence[ir.Value]],explicit_type=False,**params)
jax._src.lax.lax._naryop_weak_type_rule(name,*avals,**kwargs)
jax._src.lax.lax._operands_to_keys(*operands,num_keys=1)
jax._src.lax.lax._outfeed_abstract_eval(token,*xs,partitions)
jax._src.lax.lax._outfeed_lowering(ctx,token,*xs,partitions)
jax._src.lax.lax._pad_batch_rule(batched_args,batch_dims,*,padding_config)
jax._src.lax.lax._pad_dtype_rule(operand,padding_value,*,padding_config)
jax._src.lax.lax._pad_lower(ctx,x,padding_value,*,padding_config)
jax._src.lax.lax._pad_shape_rule(operand,padding_value,*,padding_config)
jax._src.lax.lax._pad_transpose(t,operand,padding_value,*,padding_config)
jax._src.lax.lax._pow_jvp_lhs(g,ans,x,y)
jax._src.lax.lax._pow_jvp_rhs(g,ans,x,y)
jax._src.lax.lax._precision_config(precision)
jax._src.lax.lax._real_dtype(dtype)
jax._src.lax.lax._reduce_and(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_batch_rule(batched_args,batch_dims,*,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_chooser_jvp_rule(g,ans,operand,*,axes)
jax._src.lax.lax._reduce_chooser_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_dtype_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_jvp(reducer,init_values,primals,tangents,axes)
jax._src.lax.lax._reduce_jvp_rule(primals,tangents,*,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_logical_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_lower(ctx,*values,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_max(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_min(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_named_shape_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_number_dtype_rule(name,operand,*args,**kw)
jax._src.lax.lax._reduce_op_shape_rule(operand,*,axes,input_shape=None)
jax._src.lax.lax._reduce_or(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_precision_lower(ctx,operand,*,exponent_bits,mantissa_bits)
jax._src.lax.lax._reduce_precision_shape_rule(operand,*,exponent_bits,mantissa_bits)
jax._src.lax.lax._reduce_prod(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_prod_jvp_rule(primals,tangents,*,axes)
jax._src.lax.lax._reduce_shape_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_sum(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reduce_sum_shape_rule(operand,*,axes)
jax._src.lax.lax._reduce_sum_transpose_rule(cotangent,operand,*,axes)
jax._src.lax.lax._reduce_weak_type_rule(*avals,computation,jaxpr,consts,dimensions)
jax._src.lax.lax._reduce_xor(operand:ArrayLike,axes:Sequence[int])->Array
jax._src.lax.lax._reducer_padding(traceable,ident,in_avals,out_avals,operand,*,axes)
jax._src.lax.lax._reduction_jaxpr(computation,aval)
jax._src.lax.lax._replace_masked_values(x,val,padded_axes)
jax._src.lax.lax._reshape_batch_rule(batched_args,batch_dims,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_dtype_rule(operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_lower(ctx,x,*dyn_shape,new_sizes,dimensions)
jax._src.lax.lax._reshape_shape_rule(operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_staging_rule(trace,x,*dyn,new_sizes,dimensions)
jax._src.lax.lax._reshape_transpose_rule(t,operand,*,new_sizes,dimensions)
jax._src.lax.lax._reshape_typecheck_rule(operand,*dyn_shape,new_sizes,dimensions)
jax._src.lax.lax._rev_batch_rule(batched_args,batch_dims,*,dimensions)
jax._src.lax.lax._rev_lower(ctx,x,*,dimensions)
jax._src.lax.lax._rev_shape_rule(operand,*,dimensions)
jax._src.lax.lax._rng_algorithm(algorithm:RandomAlgorithm)
jax._src.lax.lax._rng_bit_generator_dtype_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_lowering(ctx,key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_named_shape_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_shape_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_bit_generator_weak_type_rule(key,*,shape,dtype,algorithm)
jax._src.lax.lax._rng_uniform_abstract_eval(a,b,*,shape)
jax._src.lax.lax._rng_uniform_lowering(ctx,a,b,*,shape)
jax._src.lax.lax._round_lower(ctx,x,*,rounding_method)
jax._src.lax.lax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax._src.lax.lax._select_dtype_rule(which,*cases)
jax._src.lax.lax._select_jvp(primals,tangents)
jax._src.lax.lax._select_mhlo_lowering(ctx,which,*cases)
jax._src.lax.lax._select_shape_rule(which,*cases)
jax._src.lax.lax._select_transpose_rule(t,which,*cases)
jax._src.lax.lax._select_weak_type_rule(which,*cases)
jax._src.lax.lax._sign_lower_mhlo(ctx,x)
jax._src.lax.lax._sort_abstract_eval(*args,**kwargs)
jax._src.lax.lax._sort_batch_rule(batched_args,batch_dims,*,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_jvp(primals,tangents,*,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_le_comparator(*operands,num_keys=1)
jax._src.lax.lax._sort_lower(ctx,*operands,dimension,is_stable,num_keys)
jax._src.lax.lax._sort_lt_comparator(*operands,num_keys=1)
jax._src.lax.lax._squeeze_batch_rule(batched_args,batch_dims,*,dimensions)
jax._src.lax.lax._squeeze_dtype_rule(operand,*,dimensions)
jax._src.lax.lax._squeeze_lower(ctx,operand,*,dimensions)
jax._src.lax.lax._squeeze_shape_rule(operand,*,dimensions)
jax._src.lax.lax._squeeze_transpose_rule(t,operand,*,dimensions)
jax._src.lax.lax._stop_gradient_batch_rule(batched_args,batch_dims)
jax._src.lax.lax._stop_gradient_jvp_rule(primals,tangents)
jax._src.lax.lax._sub_jvp(primals,tangents)
jax._src.lax.lax._sub_transpose(t,x,y)
jax._src.lax.lax._substitute_axis_sizes_in_aval(env:Dict[core.Var,ir.Value],a:core.AbstractValue)->core.AbstractValue
jax._src.lax.lax._tan_impl(x)
jax._src.lax.lax._top_k_abstract_eval(operand,*,k)
jax._src.lax.lax._top_k_batch_rule(batched_args,batch_dims,*,k)
jax._src.lax.lax._top_k_jvp(primals,tangents,*,k)
jax._src.lax.lax._top_k_lower(ctx,operand,k)
jax._src.lax.lax._top_k_translation_rule(ctx,avals_in,avals_out,x,*,k)
jax._src.lax.lax._transpose_batch_rule(batched_args,batch_dims,*,permutation)
jax._src.lax.lax._transpose_lower(ctx,x,*,permutation)
jax._src.lax.lax._transpose_shape_rule(operand,*,permutation)
jax._src.lax.lax._tri(dtype:DTypeLike,shape:Shape,offset:int)->Array
jax._src.lax.lax._try_broadcast_shapes(shapes:Sequence[Tuple[int,...]])->Optional[Tuple[int, ...]]
jax._src.lax.lax._unary_reduce_lower(reducer,unit_factory,ctx,x,*,axes)
jax._src.lax.lax._unbroadcast(aval,x)
jax._src.lax.lax._upcast_fp16_for_computation(f)
jax._src.lax.lax._validate_preferred_element_type(input_dtype,preferred_element_type)
jax._src.lax.lax._validate_shapes(shapes:Sequence[Shape])
jax._src.lax.lax._variadic_reduction_jaxpr(computation,flat_avals,aval_tree)
jax._src.lax.lax.abs(x:ArrayLike)->Array
jax._src.lax.lax.acos(x:ArrayLike)->Array
jax._src.lax.lax.acos_impl(x)
jax._src.lax.lax.acosh(x:ArrayLike)->Array
jax._src.lax.lax.add(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.after_all(*operands)
jax._src.lax.lax.argmax(operand:ArrayLike,axis:int,index_dtype:DTypeLike)->Tuple[Array, Array]
jax._src.lax.lax.argmin(operand:ArrayLike,axis:int,index_dtype:DTypeLike)->Tuple[Array, Array]
jax._src.lax.lax.asin(x:ArrayLike)->Array
jax._src.lax.lax.asin_impl(x)
jax._src.lax.lax.asinh(x:ArrayLike)->Array
jax._src.lax.lax.atan(x:ArrayLike)->Array
jax._src.lax.lax.atan2(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.atan_impl(x)
jax._src.lax.lax.atanh(x:ArrayLike)->Array
jax._src.lax.lax.batch_matmul(lhs:Array,rhs:Array,precision:PrecisionLike=None)->Array
jax._src.lax.lax.bessel_i0e(x:ArrayLike)->Array
jax._src.lax.lax.bessel_i1e(x:ArrayLike)->Array
jax._src.lax.lax.betainc(a:ArrayLike,b:ArrayLike,x:ArrayLike)->Array
jax._src.lax.lax.betainc_grad_not_implemented(g,a,b,x)
jax._src.lax.lax.betainc_gradx(g,a,b,x)
jax._src.lax.lax.bitcast_convert_type(operand:ArrayLike,new_dtype:DTypeLike)->Array
jax._src.lax.lax.bitwise_and(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.bitwise_not(x:ArrayLike)->Array
jax._src.lax.lax.bitwise_or(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.bitwise_xor(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.broadcast(operand:ArrayLike,sizes:Sequence[int])->Array
jax._src.lax.lax.broadcast_in_dim(operand:ArrayLike,shape:Shape,broadcast_dimensions:Sequence[int])->Array
jax._src.lax.lax.broadcast_mhlo(aval_out:core.ShapedArray,avals:Sequence[core.ShapedArray],args:Sequence[ir.Value])->Sequence[ir.Value]
jax._src.lax.lax.broadcast_shapes(*shapes)
jax._src.lax.lax.broadcast_to_rank(x:Array,rank:int)->Array
jax._src.lax.lax.broadcasted_iota(dtype:DTypeLike,shape:Shape,dimension:int)->Array
jax._src.lax.lax.broadcasting_shape_rule(name,*avals)
jax._src.lax.lax.canonicalize_precision(precision:PrecisionLike)->Optional[Tuple[PrecisionType, PrecisionType]]
jax._src.lax.lax.cbrt(x:ArrayLike)->Array
jax._src.lax.lax.ceil(x:ArrayLike)->Array
jax._src.lax.lax.clamp(min:ArrayLike,x:ArrayLike,max:ArrayLike)->Array
jax._src.lax.lax.clz(x:ArrayLike)->Array
jax._src.lax.lax.collapse(operand:Array,start_dimension:int,stop_dimension:int)->Array
jax._src.lax.lax.complex(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.concatenate(operands:Union[Array,Sequence[ArrayLike]],dimension:int)->Array
jax._src.lax.lax.conj(x:ArrayLike)->Array
jax._src.lax.lax.convert_element_type(operand:ArrayLike,new_dtype:DTypeLike)->Array
jax._src.lax.lax.cos(x:ArrayLike)->Array
jax._src.lax.lax.cosh(x:ArrayLike)->Array
jax._src.lax.lax.create_token(_=None)
jax._src.lax.lax.digamma(x:ArrayLike)->Array
jax._src.lax.lax.div(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.dot(lhs:Array,rhs:Array,precision:PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.dot_general(lhs:ArrayLike,rhs:ArrayLike,dimension_numbers:DotDimensionNumbers,precision:PrecisionLike=None,preferred_element_type:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.empty(dtype)
jax._src.lax.lax.eq(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.erf(x:ArrayLike)->Array
jax._src.lax.lax.erf_inv(x:ArrayLike)->Array
jax._src.lax.lax.erfc(x:ArrayLike)->Array
jax._src.lax.lax.exp(x:ArrayLike)->Array
jax._src.lax.lax.expand_dims(array:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.expm1(x:ArrayLike)->Array
jax._src.lax.lax.floor(x:ArrayLike)->Array
jax._src.lax.lax.full(shape:Shape,fill_value:ArrayLike,dtype:Optional[DTypeLike]=None)->Array
jax._src.lax.lax.full_like(x:ArrayLike,fill_value:ArrayLike,dtype:Optional[DTypeLike]=None,shape:Optional[Shape]=None)->Array
jax._src.lax.lax.ge(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.gt(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.igamma(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.lax.igamma_grad_a(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.lax.igamma_grada(g,a,x)
jax._src.lax.lax.igamma_gradx(g,a,x)
jax._src.lax.lax.igammac(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.lax.igammac_grada(g,a,x)
jax._src.lax.lax.igammac_gradx(g,a,x)
jax._src.lax.lax.imag(x:ArrayLike)->Array
jax._src.lax.lax.infeed(token,shape=None,partitions=None)
jax._src.lax.lax.integer_pow(x:ArrayLike,y:int)->Array
jax._src.lax.lax.iota(dtype:DTypeLike,size:int)->Array
jax._src.lax.lax.is_finite(x:ArrayLike)->Array
jax._src.lax.lax.le(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.lgamma(x:ArrayLike)->Array
jax._src.lax.lax.log(x:ArrayLike)->Array
jax._src.lax.lax.log1p(x:ArrayLike)->Array
jax._src.lax.lax.logistic(x:ArrayLike)->Array
jax._src.lax.lax.logistic_impl(x)
jax._src.lax.lax.lt(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.max(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.min(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.mul(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.naryop(result_dtype,accepted_dtypes,name)
jax._src.lax.lax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax._src.lax.lax.ne(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.neg(x:ArrayLike)->Array
jax._src.lax.lax.nextafter(x1:ArrayLike,x2:ArrayLike)->Array
jax._src.lax.lax.outfeed(token,xs,partitions=None)
jax._src.lax.lax.pad(operand:ArrayLike,padding_value:ArrayLike,padding_config:Sequence[Tuple[int,int,int]])->Array
jax._src.lax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax._src.lax.lax.population_count(x:ArrayLike)->Array
jax._src.lax.lax.pow(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.precision_attr(precision:PrecisionType)->ir.ArrayAttr
jax._src.lax.lax.random_gamma_grad(a:ArrayLike,x:ArrayLike)->Array
jax._src.lax.lax.ranges_like(*xs)
jax._src.lax.lax.real(x:ArrayLike)->Array
jax._src.lax.lax.reciprocal(x:ArrayLike)->Array
jax._src.lax.lax.reduce(operands:Any,init_values:Any,computation:Callable[[Any,Any],Any],dimensions:Sequence[int])->Any
jax._src.lax.lax.reduce_precision(operand:Union[float,ArrayLike],exponent_bits:int,mantissa_bits:int)->Array
jax._src.lax.lax.rem(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.remaining(original,*removed_lists)
jax._src.lax.lax.reshape(operand:ArrayLike,new_sizes:Shape,dimensions:Optional[Sequence[int]]=None)->Array
jax._src.lax.lax.rev(operand:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.rng_bit_generator(key,shape,dtype=np.uint32,algorithm=RandomAlgorithm.RNG_DEFAULT)
jax._src.lax.lax.rng_uniform(a,b,shape)
jax._src.lax.lax.round(x:ArrayLike,rounding_method:RoundingMethod=RoundingMethod.AWAY_FROM_ZERO)->Array
jax._src.lax.lax.rsqrt(x:ArrayLike)->Array
jax._src.lax.lax.select(pred:ArrayLike,on_true:ArrayLike,on_false:ArrayLike)->Array
jax._src.lax.lax.select_n(which:ArrayLike,*cases:ArrayLike)->Array
jax._src.lax.lax.shape_as_value(shape:core.Shape)
jax._src.lax.lax.shift_left(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.shift_right_arithmetic(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.shift_right_logical(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.sign(x:ArrayLike)->Array
jax._src.lax.lax.sin(x:ArrayLike)->Array
jax._src.lax.lax.sinh(x:ArrayLike)->Array
jax._src.lax.lax.sort(operand:Union[Array,Sequence[Array]],dimension:int=-1,is_stable:bool=True,num_keys:int=1)->Union[Array, Tuple[Array, ...]]
jax._src.lax.lax.sort_key_val(keys:Array,values:ArrayLike,dimension:int=-1,is_stable:bool=True)->Tuple[Array, Array]
jax._src.lax.lax.sqrt(x:ArrayLike)->Array
jax._src.lax.lax.square(x:ArrayLike)->Array
jax._src.lax.lax.squeeze(array:ArrayLike,dimensions:Sequence[int])->Array
jax._src.lax.lax.stop_gradient(x:T)->T
jax._src.lax.lax.sub(x:ArrayLike,y:ArrayLike)->Array
jax._src.lax.lax.tan(x:ArrayLike)->Array
jax._src.lax.lax.tanh(x:ArrayLike)->Array
jax._src.lax.lax.tie_in(x:Any,y:T)->T
jax._src.lax.lax.top_k(operand:ArrayLike,k:int)->Tuple[Array, Array]
jax._src.lax.lax.transpose(operand:ArrayLike,permutation:Sequence[int])->Array
jax._src.lax.lax.tuple_delete(tup,idx)
jax._src.lax.lax.unop(result_dtype,accepted_dtypes,name)
jax._src.lax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax._src.lax.lax.zeros_like_array(x:ArrayLike)->Array
jax._src.lax.lax.zeros_like_shaped_array(aval:ArrayLike)->Array


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/loops.py----------------------------------------
A:jax._src.lax.control_flow.loops.T->TypeVar('T')
A:jax._src.lax.control_flow.loops.new_dtype->jax._src.dtypes.result_type(in_vals[i], out_avals[i])
A:jax._src.lax.control_flow.loops.in_vals[i]->jax._src.lax.lax.convert_element_type(in_vals[i], new_dtype)
A:jax._src.lax.control_flow.loops.Carry->TypeVar('Carry')
A:jax._src.lax.control_flow.loops.X->TypeVar('X')
A:jax._src.lax.control_flow.loops.Y->TypeVar('Y')
A:jax._src.lax.control_flow.loops.(xs_flat, xs_tree)->tree_flatten(xs)
A:jax._src.lax.control_flow.loops.length->int(length)
A:jax._src.lax.control_flow.loops.unique_lengths->set(lengths)
A:jax._src.lax.control_flow.loops.(carry, y)->split_list(out, [num_carry])
A:jax._src.lax.control_flow.loops.stacked_y->tree_map(stack, *maybe_reversed(ys))
A:jax._src.lax.control_flow.loops.(init_flat, init_tree)->tree_flatten(init)
A:jax._src.lax.control_flow.loops.(in_flat, in_tree)->tree_flatten((init, xs))
A:jax._src.lax.control_flow.loops.carry_avals->tuple(_map(_abstractify, init_flat))
A:jax._src.lax.control_flow.loops.(jaxpr, consts, out_tree)->_initial_style_jaxpr(f, in_tree, (*carry_avals, *x_avals), 'scan')
A:jax._src.lax.control_flow.loops.out_tree_children->out_tree.children()
A:jax._src.lax.control_flow.loops.(init_flat, carry_avals, carry_avals_out, init_tree, *rest)->_create_jaxpr(new_init)
A:jax._src.lax.control_flow.loops.(new_init_flat, changed)->_promote_weak_typed_inputs(init_flat, carry_avals, carry_avals_out)
A:jax._src.lax.control_flow.loops.new_init->tree_unflatten(init_tree, new_init_flat)
A:jax._src.lax.control_flow.loops.out->jax.core.AxisPrimitive('while').bind(*cconst + bconst + bconst_dot + init + init_dot, cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr_augmented, body_nconsts=len(bconst) + len(bconst_dot), body_jaxpr=body_jvp_rearranged)
A:jax._src.lax.control_flow.loops.(consts, init, xs)->split_list(args, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.x->_map(partial(_dynamic_index_array, i_), x_avals, xs)
A:jax._src.lax.control_flow.loops.ys->_map(_concatenate, y_avals, ys, ys_rem)
A:jax._src.lax.control_flow.loops.([i], carry, ys)->split_list(vals, [1, num_carry])
A:jax._src.lax.control_flow.loops.out_flat->jax.core.AxisPrimitive('scan').bind(*consts + consts_dot + init + init_dot + xs + xs_dot, reverse=reverse, length=length, jaxpr=jaxpr_jvp_rearranged, num_consts=num_consts + len(consts_dot), num_carry=num_carry + len(init_dot), linear=jaxpr_jvp_linear, unroll=unroll)
A:jax._src.lax.control_flow.loops.(carry_out, y_updates)->split_list(out_flat, [num_carry])
A:jax._src.lax.control_flow.loops.ys_out->_map(partial(_update_array, i_), y_avals, ys, y_updates)
A:jax._src.lax.control_flow.loops.ys_init->_map(partial(_empty_array, length), y_avals)
A:jax._src.lax.control_flow.loops.(_, *outs)->while_loop(cond_fun, body_fun, init_val)
A:jax._src.lax.control_flow.loops.(num_blocks, rem)->divmod(length, unroll)
A:jax._src.lax.control_flow.loops.partition->partial(_partition_leading, num_blocks, block_length)
A:jax._src.lax.control_flow.loops.xs_block->_map(partition, x_avals, xs)
A:jax._src.lax.control_flow.loops.prepend_aval->partial(_prepend_dim_to_aval, block_length)
A:jax._src.lax.control_flow.loops.x_block_avals->_map(prepend_aval, x_avals)
A:jax._src.lax.control_flow.loops.y_block_avals->_map(prepend_aval, y_avals)
A:jax._src.lax.control_flow.loops.f_impl_block->partial(_scan_impl_unrolled, reverse=reverse, length=block_length, num_consts=num_consts, num_carry=num_carry, linear=linear, f_impl=f_impl, x_avals=x_avals, y_avals=y_avals)
A:jax._src.lax.control_flow.loops.outs->jax.core.AxisPrimitive('while').bind(*cconsts + bconsts + new_init, cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr_batched, body_nconsts=body_nconsts, body_jaxpr=body_jaxpr_batched)
A:jax._src.lax.control_flow.loops.(carry, ys_blocks)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.combine->partial(_combine_leading, num_blocks, block_length)
A:jax._src.lax.control_flow.loops.(_, _, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(_, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.f_impl->jax.core.jaxpr_as_fun(jaxpr)
A:jax._src.lax.control_flow.loops.split->partial(_split_leading_dim, length_div)
A:jax._src.lax.control_flow.loops.(xs_rem, xs)->unzip2(_map(split, x_avals, xs))
A:jax._src.lax.control_flow.loops.(xs, xs_rem)->unzip2(_map(split, x_avals, xs))
A:jax._src.lax.control_flow.loops.(carry, ys)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.(carry, ys_rem)->split_list(outs, [num_carry])
A:jax._src.lax.control_flow.loops.(carry_avals, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.ys_avals->_map(partial(_prepend_dim_to_aval, length), y_avals)
A:jax._src.lax.control_flow.loops.(const_nz, init_nz, xs_nz)->split_list(nonzeros, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(jaxpr, nonzeros, instantiate=carry_nz + [False] * num_ys)
A:jax._src.lax.control_flow.loops.carry_nz->_map(operator.or_, carry_nz, nonzeros_out)
A:jax._src.lax.control_flow.loops.all_tangents->split_list(tangents, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts_dot, init_dot, xs_dot)->_map(_prune_zeros, all_tangents)
A:jax._src.lax.control_flow.loops.jaxpr_jvp_rearranged->jax.interpreters.ad.rearrange_binders(jaxpr_jvp, [num_consts, num_carry, num_xs], [len(consts_dot), len(init_dot), len(xs_dot)], [num_carry, num_ys], [len(init_dot), sum(nonzeros_out) - len(init_dot)])
A:jax._src.lax.control_flow.loops.(consts_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.jaxpr_jvp_linear->tuple(consts_linear + [True] * len(consts_dot) + init_linear + [True] * len(init_dot) + xs_linear + [True] * len(xs_dot))
A:jax._src.lax.control_flow.loops.(carry, carry_dot, ys, ys_dot)->split_list(out_flat, [num_carry, len(init_dot), num_ys])
A:jax._src.lax.control_flow.loops.tangents_out_iter->iter(carry_dot + ys_dot)
A:jax._src.lax.control_flow.loops.(const_uk, init_uk, xs_uk)->split_list(unknowns, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_known, jaxpr_unknown, out_uk, res_avals)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr, unknowns, instantiate=carry_uk + [False] * num_ys)
A:jax._src.lax.control_flow.loops.(carry_uk_out, ys_uk)->split_list(unks_out, [num_carry])
A:jax._src.lax.control_flow.loops.carry_uk->_map(operator.or_, carry_uk, carry_uk_out)
A:jax._src.lax.control_flow.loops.num_res->len(res_avals)
A:jax._src.lax.control_flow.loops.jaxpr_unknown->jax.interpreters.partial_eval.move_binders_to_front(jaxpr_unknown, [False] * sum(unknowns) + [pval.is_known() for pval in res_pvals])
A:jax._src.lax.control_flow.loops.(jaxpr_known_, invar_pvals_out, jaxpr_known_consts)->jax.interpreters.partial_eval.trace_to_jaxpr_nounits(lu.wrap_init(core.jaxpr_as_fun(jaxpr_known)), const_pvals + other_pvals, instantiate=[True] * (len(out_uk) - sum(out_uk)) + [False] * num_res)
A:jax._src.lax.control_flow.loops.jaxpr_known->jax.core.ClosedJaxpr(jaxpr_known_, jaxpr.consts)
A:jax._src.lax.control_flow.loops.fwds_known->jax.interpreters.partial_eval._jaxpr_forwarding(jaxpr_known.jaxpr)
A:jax._src.lax.control_flow.loops.out_known->jax.core.AxisPrimitive('while').bind(*in_consts, cond_nconsts=cond_nconsts_known, cond_jaxpr=cond_jaxpr_known, body_nconsts=body_nconsts_known, body_jaxpr=body_jaxpr_known)
A:jax._src.lax.control_flow.loops.out_known_iter->iter(out_known)
A:jax._src.lax.control_flow.loops.(out_known, extensive_res)->split_list(out_known, [len(out_uk) - sum(out_uk)])
A:jax._src.lax.control_flow.loops.intensive_res->_map(newvar, intensive_avals)
A:jax._src.lax.control_flow.loops.extensive_res->_map(newvar, ext_avals)
A:jax._src.lax.control_flow.loops.linear_unknown->tuple([False] * len(intensive_res) + [l for (l, uk) in zip(linear, unknowns) if uk] + [False] * len(extensive_res))
A:jax._src.lax.control_flow.loops.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.loops.eqn->jax.interpreters.partial_eval.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res], out_tracers, scan_p, dict(reverse=reverse, length=length, unroll=unroll, jaxpr=jaxpr_unknown, linear=linear_unknown, num_consts=len(intensive_res) + sum(const_uk), num_carry=sum(carry_uk)), jaxpr_unknown.effects, source)
A:jax._src.lax.control_flow.loops.(consts_lin, init_lin, xs_lin)->split_list(linear, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(consts, _, xs)->split_list(args, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(ires, _)->split_list(consts, [num_ires])
A:jax._src.lax.control_flow.loops.(_, eres)->split_list(xs, [sum(xs_lin)])
A:jax._src.lax.control_flow.loops.(ct_carry, ct_ys)->split_list(cts, [num_carry])
A:jax._src.lax.control_flow.loops.ct_carry->_map(ad.instantiate_zeros_aval, carry_avals, ct_carry)
A:jax._src.lax.control_flow.loops.ct_ys->_map(ad.instantiate_zeros_aval, ys_avals, ct_ys)
A:jax._src.lax.control_flow.loops.ct_consts->_map(ad_util.zeros_like_aval, jaxpr.in_avals[num_ires:num_consts])
A:jax._src.lax.control_flow.loops.jaxpr_trans->_transpose_scan_jaxpr(num_ires, num_consts - num_ires, num_eres, jaxpr, reduce_axes)
A:jax._src.lax.control_flow.loops.(ct_consts, ct_init, ct_xs)->split_list(outs, [num_consts - num_ires, num_carry])
A:jax._src.lax.control_flow.loops.(res1_avals, c_avals, a_avals, res2_avals)->split_list(jaxpr.in_avals, [num_res1, num_c, num_a])
A:jax._src.lax.control_flow.loops.num_b->len(jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.b_avals->list(jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.(res1, c_bar, b_bar, res2)->split_list(res1_cbar_bbar_res2, [num_res1, num_c, num_b])
A:jax._src.lax.control_flow.loops.cbar_abar->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, reduce_axes, False, jaxpr.consts, primals, b_bar)
A:jax._src.lax.control_flow.loops.(_, new_c_bar, a_bar, _)->split_list(cbar_abar, [num_res1, num_c, num_a])
A:jax._src.lax.control_flow.loops.a_bar->_map(ad.instantiate_zeros_aval, a_avals, a_bar)
A:jax._src.lax.control_flow.loops.c_bar->_map(ad.instantiate_zeros_aval, c_avals, _map(ad.add_tangents, c_bar, new_c_bar))
A:jax._src.lax.control_flow.loops.(const_batched, init_batched, xs_batched)->split_list(orig_batched, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_batched, batched_out)->jax.interpreters.batching.batch_jaxpr(jaxpr, axis_size, batched, instantiate=carry_batched + [False] * num_ys, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.carry_batched->_map(operator.or_, carry_batched, carry_batched_out)
A:jax._src.lax.control_flow.loops.(consts_bdims, init_bdims, xs_bdims)->split_list(dims, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.fun->jax.core.jaxpr_as_fun(jaxpr)
A:jax._src.lax.control_flow.loops.([dynamic_length], consts, [i], carry, xs)->split_list(args, [1, num_consts, 1, num_carry])
A:jax._src.lax.control_flow.loops.(new_carry, ys)->split_list(out, [num_carry])
A:jax._src.lax.control_flow.loops.aval->ShapedArray((), dtypes.canonicalize_dtype(dtypes.int_))
A:jax._src.lax.control_flow.loops.(const_avals, carry_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.padded_jaxpr->jax.core.ClosedJaxpr(*pe.pad_jaxpr(jaxpr.jaxpr, jaxpr.consts))
A:jax._src.lax.control_flow.loops.(used_carry_out, used_extensive_out)->split_list(used_outputs, [num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_dce, used_inputs)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr.jaxpr, used_outputs, instantiate=[False] * num_consts + used_carry_out + [False] * num_xs)
A:jax._src.lax.control_flow.loops.(used_consts, used_carry_in, used_extensive_in)->split_list(used_inputs, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.used_carry_out->_map(operator.or_, used_carry_out, used_carry_in)
A:jax._src.lax.control_flow.loops.new_params->dict(eqn.params, num_consts=sum(used_consts), num_carry=sum(used_carry_in), linear=tuple(new_linear), jaxpr=core.ClosedJaxpr(jaxpr_dce, jaxpr.consts))
A:jax._src.lax.control_flow.loops.new_eqn->jax.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, eqn.effects, eqn.source_info)
A:jax._src.lax.control_flow.loops.(const_uk, carry_uk, xs_uk)->split_list(unks_in, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(jaxpr_known_, jaxpr_staged_, unks_out, inst_out, num_res)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=unks_in, in_inst=True, ensure_out_unknowns=carry_uk + [False] * num_ys, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.loops.jaxpr_staged->jax.interpreters.partial_eval.move_binders_to_front(jaxpr_staged, [False] * sum(inst_in) + _map(operator.not_, loop_dep_res))
A:jax._src.lax.control_flow.loops.(jaxpr_known_hoist, jaxpr_known_loop, loop_dep, consts_known_lp_avals)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr_known, [False] * num_const_known + [True] * (num_carry_known + num_xs_known), [True] * (len(unks_out) - sum(unks_out)) + [False] * num_res)
A:jax._src.lax.control_flow.loops.(_, loop_dep_res)->split_list(loop_dep, [len(loop_dep) - num_res])
A:jax._src.lax.control_flow.loops.(intensive_avals, ext_avals_mapped)->partition_list(loop_dep_res, res_avals)
A:jax._src.lax.control_flow.loops.newvar->jax.core.gensym([cond_jaxpr.jaxpr])
A:jax._src.lax.control_flow.loops.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax._src.lax.control_flow.loops.(out_binders_known, _)->partition_list(carry_uk, eqn.outvars)
A:jax._src.lax.control_flow.loops.(_, linear_known_)->split_list(linear_known_, [num_const_known])
A:jax._src.lax.control_flow.loops.params_known->dict(cond_jaxpr=cond_jaxpr_known, body_jaxpr=body_jaxpr_known, cond_nconsts=len(cond_consts_uk) - sum(cond_consts_uk), body_nconsts=len(body_consts_uk) - sum(body_consts_uk))
A:jax._src.lax.control_flow.loops.(consts_known_hoist, ins_known_lp)->split_list(ins_known, [num_const_known])
A:jax._src.lax.control_flow.loops.out_hoist->jax.core.jaxpr_as_fun(jaxpr_known_hoist)(*consts_known_hoist)
A:jax._src.lax.control_flow.loops.(intensive_res, consts_known_lp)->split_list(out_hoist, [num_intensive_res])
A:jax._src.lax.control_flow.loops.out_loop->jax.core.AxisPrimitive('scan').bind(*consts_known_lp, *ins_known_lp, **params_known)
A:jax._src.lax.control_flow.loops.(call_jaxpr_, _, call_jaxpr_consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(known, [v.aval for v in ins_known])
A:jax._src.lax.control_flow.loops.call_jaxpr->jax.core.ClosedJaxpr(call_jaxpr_, call_jaxpr_consts)
A:jax._src.lax.control_flow.loops.eqn_known->jax.interpreters.partial_eval.new_jaxpr_eqn(ins_known, out_binders_known, while_p, params_known, effects_known, eqn.source_info)
A:jax._src.lax.control_flow.loops.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax._src.lax.control_flow.loops.params_staged->dict(eqn.params, jaxpr=jaxpr_staged, num_consts=len(intensive_res) + eqn.params['num_consts'], linear=tuple(linear_staged))
A:jax._src.lax.control_flow.loops.eqn_staged->jax.interpreters.partial_eval.new_jaxpr_eqn([*intensive_res, *eqn.invars, *extensive_res], out_binders_staged, eqn.primitive, params_staged, jaxpr_staged.effects, eqn.source_info)
A:jax._src.lax.control_flow.loops.tc->partial(_typecheck_param, 'scan')
A:jax._src.lax.control_flow.loops.(const_avals, init_avals, x_avals)->split_list(avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(const_avals_jaxpr, init_avals_jaxpr, x_avals_jaxpr)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax._src.lax.control_flow.loops.(carry_avals_jaxpr, y_avals_mapped)->split_list(jaxpr.out_avals, [num_carry])
A:jax._src.lax.control_flow.loops.x_avals_mapped->_map(partial(core.mapped_aval, length, 0), x_avals)
A:jax._src.lax.control_flow.loops.printed_params->dict(eqn.params)
A:jax._src.lax.control_flow.loops.lhs->jax.core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)
A:jax._src.lax.control_flow.loops.avals->_map(core.get_aval, args)
A:jax._src.lax.control_flow.loops.scan_p->jax.core.AxisPrimitive('scan')
A:jax._src.lax.control_flow.loops.core.custom_typechecks[scan_p]->partial(_scan_typecheck, False)
A:jax._src.lax.control_flow.loops.val->body_fun(val)
A:jax._src.lax.control_flow.loops.(init_vals, in_tree)->tree_flatten((init_val,))
A:jax._src.lax.control_flow.loops.init_avals->tuple(_map(_abstractify, init_vals))
A:jax._src.lax.control_flow.loops.(cond_jaxpr, cond_consts, cond_tree)->_initial_style_jaxpr(cond_fun, in_tree, init_avals, 'while_cond')
A:jax._src.lax.control_flow.loops.(body_jaxpr, body_consts, body_tree)->_initial_style_jaxpr(body_fun, in_tree, init_avals, 'while_loop')
A:jax._src.lax.control_flow.loops.(init_vals, init_avals, body_jaxpr, in_tree, *rest)->_create_jaxpr(new_init_val)
A:jax._src.lax.control_flow.loops.(new_init_vals, changed)->_promote_weak_typed_inputs(init_vals, init_avals, body_jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.(new_init_val,)->tree_unflatten(in_tree, new_init_vals)
A:jax._src.lax.control_flow.loops.in_tree_children->in_tree.children()
A:jax._src.lax.control_flow.loops.effects->jax.core.join_effects(cond_jaxpr.effects, body_jaxpr.effects)
A:jax._src.lax.control_flow.loops.joined_effects->jax.core.join_effects(cond_jaxpr.effects, body_jaxpr.effects)
A:jax._src.lax.control_flow.loops.(cconst_bat, bconst_bat, init_bat)->split_list(orig_batched, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cconsts, bconsts, init)->split_list(args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(cconst_dims, bconst_dims, init_dims)->split_list(dims, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(_, carry_bat_out)->jax.interpreters.batching.batch_jaxpr(body_jaxpr, axis_size, bconst_bat + carry_bat, instantiate=carry_bat, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.carry_bat->safe_map(operator.or_, carry_bat, carry_bat_out)
A:jax._src.lax.control_flow.loops.(_, (pred_bat,))->jax.interpreters.batching.batch_jaxpr(cond_jaxpr, axis_size, cconst_bat + carry_bat, instantiate=False, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.(body_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr_axes(body_jaxpr, axis_size, bconst_dims + carry_dims, carry_dims, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr_axes(cond_jaxpr, axis_size, cconst_dims + carry_dims, (None,), axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.loops.cond_rank->len(cond_jaxpr.out_avals[0].shape)
A:jax._src.lax.control_flow.loops.(cconst_nz, bconst_nz, init_nz)->split_list(nonzeros, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(body_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(body_jaxpr, body_nonzeros, instantiate=carry_nz)
A:jax._src.lax.control_flow.loops.(cconst, bconst, init)->split_list(primals, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(_, bconst_dot, init_dot)->split_list(tangents, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.bconst_dot->_prune_zeros(bconst_dot)
A:jax._src.lax.control_flow.loops.init_dot->_prune_zeros(init_dot)
A:jax._src.lax.control_flow.loops.body_jvp_rearranged->jax.interpreters.ad.rearrange_binders(body_jvp, [body_nconsts, num_carry], [len(bconst_dot), len(init_dot)], [num_carry], [len(init_dot)])
A:jax._src.lax.control_flow.loops.cond_jaxpr_augmented->jax.core.ClosedJaxpr(cond_jaxpr_augmented, cond_jaxpr.consts)
A:jax._src.lax.control_flow.loops.(out_carry, out_carry_dot)->split_list(out, [num_carry])
A:jax._src.lax.control_flow.loops.out_tangents_iter->iter(out_carry_dot)
A:jax._src.lax.control_flow.loops.params->dict(cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr, body_nconsts=body_nconsts, body_jaxpr=body_jaxpr)
A:jax._src.lax.control_flow.loops.(cond_consts_uk, body_consts_uk, carry_init_uk)->split_list(unks_in, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.(body_jaxpr_known, _, carry_out_uk, body_res_avals)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(body_jaxpr, body_consts_uk + carry_uk, instantiate=carry_uk)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_known, _, cond_uk, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(cond_jaxpr, cond_consts_uk + carry_uk, instantiate=False)
A:jax._src.lax.control_flow.loops.out_tracers_->trace.default_process_primitive(while_p, tracers, params)
A:jax._src.lax.control_flow.loops.(jaxpr_known_, _, carry_uk_out, _, num_res)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(body_jaxpr.jaxpr, in_unknowns=body_unks_in, in_inst=True, ensure_out_unknowns=carry_uk, ensure_out_inst=True, saveable=ad_checkpoint.nothing_saveable)
A:jax._src.lax.control_flow.loops.body_jaxpr_known->jax.core.ClosedJaxpr(jaxpr_known_, body_jaxpr.consts)
A:jax._src.lax.control_flow.loops.(cond_jaxpr_known_, _, [cond_uk], _, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(cond_jaxpr.jaxpr, cond_unks_in, in_inst=True, ensure_out_unknowns=False, ensure_out_inst=True, saveable=ad_checkpoint.nothing_saveable)
A:jax._src.lax.control_flow.loops.cond_jaxpr_known->jax.core.ClosedJaxpr(cond_jaxpr_known_, cond_jaxpr.consts)
A:jax._src.lax.control_flow.loops.effects_known->jax.core.join_effects(cond_jaxpr_known.effects, body_jaxpr_known.effects)
A:jax._src.lax.control_flow.loops.batched->bool(pred_aval.shape)
A:jax._src.lax.control_flow.loops.pred->cond(args)
A:jax._src.lax.control_flow.loops.args->body(args)
A:jax._src.lax.control_flow.loops.(_, out)->while_loop(new_cond, new_body, (pred, args))
A:jax._src.lax.control_flow.loops.loop_carry_types->_map(mlir.aval_to_ir_types, ctx.avals_in)
A:jax._src.lax.control_flow.loops.num_tokens->len(body_effects)
A:jax._src.lax.control_flow.loops.flat_loop_carry_types->jax._src.util.flatten(loop_carry_types)
A:jax._src.lax.control_flow.loops.flat_args->jax.interpreters.mlir.flatten_lowering_ir_args(args)
A:jax._src.lax.control_flow.loops.while_op->jax._src.lib.mlir.dialects.mhlo.WhileOp(flat_loop_carry_types, flat_args)
A:jax._src.lax.control_flow.loops.cond_block->jax._src.lib.mlir.dialects.mhlo.WhileOp(flat_loop_carry_types, flat_args).regions[0].blocks.append(*flat_loop_carry_types)
A:jax._src.lax.control_flow.loops.name_stack->extend_name_stack(ctx.module_context.name_stack, 'while')
A:jax._src.lax.control_flow.loops.cond_args->jax._src.util.unflatten(flat_cond_args, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(x, _, z)->jax._src.util.split_list(cond_args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.cond_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(name_stack, 'cond'))
A:jax._src.lax.control_flow.loops.(((pred,),), _)->jax.interpreters.mlir.jaxpr_subcomp(cond_ctx, cond_jaxpr.jaxpr, mlir.TokenSet(), _map(mlir.ir_constants, cond_jaxpr.consts), *x + z)
A:jax._src.lax.control_flow.loops.pred_ctx->jax.interpreters.mlir.LoweringRuleContext(module_context=ctx.module_context, primitive=None, avals_in=[pred_aval], avals_out=[pred_aval.update(shape=())], tokens_in=mlir.TokenSet(), tokens_out=None)
A:jax._src.lax.control_flow.loops.(pred,)->jax._src.lax.lax._unary_reduce_lower(mhlo.OrOp, lambda dtype: np.array(False, dtype), pred_ctx, pred, axes=tuple(range(len(pred_aval.shape))))
A:jax._src.lax.control_flow.loops.body_block->jax._src.lib.mlir.dialects.mhlo.WhileOp(flat_loop_carry_types, flat_args).regions[1].blocks.append(*flat_loop_carry_types)
A:jax._src.lax.control_flow.loops.body_args->jax._src.util.unflatten(flat_body_args, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(token_args, body_args)->jax._src.util.split_list(body_args, [num_tokens])
A:jax._src.lax.control_flow.loops.tokens_in->jax.interpreters.mlir.TokenSet(zip(body_effects, token_args))
A:jax._src.lax.control_flow.loops.(x, y, z)->jax._src.util.split_list(body_args, [cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.body_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(name_stack, 'body'))
A:jax._src.lax.control_flow.loops.(new_z, tokens_out)->jax.interpreters.mlir.jaxpr_subcomp(body_ctx, body_jaxpr.jaxpr, tokens_in, _map(mlir.ir_constants, body_jaxpr.consts), *y + z)
A:jax._src.lax.control_flow.loops.body_pred_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(name_stack, 'body_pred'))
A:jax._src.lax.control_flow.loops.(((body_pred,),), _)->jax.interpreters.mlir.jaxpr_subcomp(body_pred_ctx, cond_jaxpr.jaxpr, mlir.TokenSet(), _map(mlir.ir_constants, cond_jaxpr.consts), *x + z)
A:jax._src.lax.control_flow.loops.new_z->_map(partial(_pred_bcast_select_mhlo, pred_aval, body_pred), new_z, z, body_jaxpr.out_avals)
A:jax._src.lax.control_flow.loops.outputs->jax._src.util.unflatten(while_op.results, _map(len, loop_carry_types))
A:jax._src.lax.control_flow.loops.(tokens, _, _, z)->jax._src.util.split_list(outputs, [num_tokens, cond_nconsts, body_nconsts])
A:jax._src.lax.control_flow.loops.while_p->jax.core.AxisPrimitive('while')
A:jax._src.lax.control_flow.loops.x_y_type->jax.interpreters.mlir.aval_to_ir_type(x_y_aval)
A:jax._src.lax.control_flow.loops.bcast_pred_type->jax._src.lib.mlir.ir.RankedTensorType.get(x_y_type.shape, mlir.dtype_to_ir_type(np.dtype(np.bool_)))
A:jax._src.lax.control_flow.loops.body_fun->weakref.ref(body_fun)
A:jax._src.lax.control_flow.loops.lower_dtype->jax._src.dtypes.canonicalize_dtype(lax.dtype(lower))
A:jax._src.lax.control_flow.loops.upper_dtype->jax._src.dtypes.canonicalize_dtype(lax.dtype(upper))
A:jax._src.lax.control_flow.loops.lower_->int(lower)
A:jax._src.lax.control_flow.loops.upper_->int(upper)
A:jax._src.lax.control_flow.loops.((_, result), _)->scan(_fori_scan_body_fun(body_fun), (lower_, init_val), None, length=upper_ - lower_)
A:jax._src.lax.control_flow.loops.(_, _, result)->while_loop(_fori_cond_fun, _fori_body_fun(body_fun), (lower, upper, init_val))
A:jax._src.lax.control_flow.loops.(_, ys)->scan(g, (), xs)
A:jax._src.lax.control_flow.loops.key->jax.interpreters.batching.moveaxis(key, bd, 0)
A:jax._src.lax.control_flow.loops.(stacked_keys, stacked_bits)->map(map_body, key)
A:jax._src.lax.control_flow.loops.(elems_flat, tree)->tree_flatten(elems)
A:jax._src.lax.control_flow.loops.a->tree_unflatten(tree, a_flat)
A:jax._src.lax.control_flow.loops.b->tree_unflatten(tree, b_flat)
A:jax._src.lax.control_flow.loops.c->fn(a, b)
A:jax._src.lax.control_flow.loops.(c_flat, _)->tree_flatten(c)
A:jax._src.lax.control_flow.loops.axis->jax._src.util.canonicalize_axis(axis, elems_flat[0].ndim)
A:jax._src.lax.control_flow.loops.num_elems->int(elems_flat[0].shape[axis])
A:jax._src.lax.control_flow.loops.reduced_elems->combine([slicing.slice_in_dim(elem, 0, -1, stride=2, axis=axis) for elem in elems], [slicing.slice_in_dim(elem, 1, None, stride=2, axis=axis) for elem in elems])
A:jax._src.lax.control_flow.loops.odd_elems->_scan(reduced_elems)
A:jax._src.lax.control_flow.loops.even_elems->combine(odd_elems, [slicing.slice_in_dim(e, 2, None, stride=2, axis=axis) for e in elems])
A:jax._src.lax.control_flow.loops.scans->_scan(elems_flat)
A:jax._src.lax.control_flow.loops.reducer_p->jax._src.lax.lax.standard_primitive(_cumred_shape_rule, partial(_cumred_dtype_rule, name), name)
A:jax._src.lax.control_flow.loops.batching.primitive_batchers[reducer_p]->partial(_cumred_batch_rule, reducer_p)
A:jax._src.lax.control_flow.loops.cumsum_p->_cumulative_reduction_primitive('cumsum', lax.add, windowed_reductions._reduce_window_sum)
A:jax._src.lax.control_flow.loops.cumprod_p->_cumulative_reduction_primitive('cumprod', lax.mul, windowed_reductions._reduce_window_prod)
A:jax._src.lax.control_flow.loops.cummax_p->_cumulative_reduction_primitive('cummax', lax.max, windowed_reductions._reduce_window_max)
A:jax._src.lax.control_flow.loops.cummin_p->_cumulative_reduction_primitive('cummin', lax.min, windowed_reductions._reduce_window_min)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cumprod_p]->partial(_cumulative_jvp_rule, combine_fn=lax.mul)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cummin_p]->partial(_cumulative_jvp_rule, combine_fn=lax.min)
A:jax._src.lax.control_flow.loops.ad.primitive_jvps[cummax_p]->partial(_cumulative_jvp_rule, combine_fn=lax.max)
jax._src.lax.control_flow._scan_impl(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow._scan_impl_block_unrolled(*args,reverse,length,num_consts,num_carry,linear,block_length,f_impl,x_avals,y_avals)
jax._src.lax.control_flow._scan_impl_loop(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow._scan_impl_unrolled(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.associative_scan(fn:Callable,elems,reverse:bool=False,axis:int=0)
jax._src.lax.control_flow.cummax(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cummin(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cumprod(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.cumred_reduce_window_impl(window_reduce:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.cumsum(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.fori_loop(lower,upper,body_fun,init_val)
jax._src.lax.control_flow.loops._combine_leading(sz0,sz1,aval,x)
jax._src.lax.control_flow.loops._concatenate(aval,x1,x2)
jax._src.lax.control_flow.loops._cumred_batch_rule(prim,batched_args,batch_dims,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumred_dtype_rule(name,operand,*args,**kw)
jax._src.lax.control_flow.loops._cumred_shape_rule(x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumsum_transpose_rule(t,operand,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops._cumulative_jvp_rule(primals,tangents,*,axis:int,reverse:bool,combine_fn:Callable)
jax._src.lax.control_flow.loops._cumulative_reduction_primitive(name,reduce_fn,reduce_window_fn)
jax._src.lax.control_flow.loops._dynamic_index_array(i,aval,x)
jax._src.lax.control_flow.loops._empty_array(sz,aval)
jax._src.lax.control_flow.loops._fori_body_fun(body_fun)
jax._src.lax.control_flow.loops._fori_cond_fun(loop_carry)
jax._src.lax.control_flow.loops._fori_scan_body_fun(body_fun)
jax._src.lax.control_flow.loops._index_array(i,aval,x)
jax._src.lax.control_flow.loops._interleave(a,b,axis)
jax._src.lax.control_flow.loops._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax._src.lax.control_flow.loops._maybe_put(x)
jax._src.lax.control_flow.loops._partition_leading(sz0,sz1,aval,x)
jax._src.lax.control_flow.loops._pred_bcast_select_mhlo(pred_aval:core.ShapedArray,pred:ir.Value,xs:Sequence[ir.Value],ys:Sequence[ir.Value],x_y_aval:core.AbstractValue)->Sequence[ir.Value]
jax._src.lax.control_flow.loops._prepend_dim_to_aval(sz,aval)
jax._src.lax.control_flow.loops._promote_weak_typed_inputs(in_vals,in_avals,out_avals)
jax._src.lax.control_flow.loops._rng_bit_generator_batching_rule(batched_args,batch_dims,*,shape,dtype,algorithm)
jax._src.lax.control_flow.loops._scan_abstract_eval(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_batching_rule(axis_size,axis_name,main_type,args,dims,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.lax.control_flow.loops._scan_dce_rule(used_outputs:List[bool],eqn:core.JaxprEqn)->Tuple[List[bool], core.JaxprEqn]
jax._src.lax.control_flow.loops._scan_impl(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_impl_block_unrolled(*args,reverse,length,num_consts,num_carry,linear,block_length,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_impl_loop(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_impl_unrolled(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax._src.lax.control_flow.loops._scan_jvp(primals,tangents,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax._src.lax.control_flow.loops._scan_padding_rule(in_avals,out_avals,*args,jaxpr,**params)
jax._src.lax.control_flow.loops._scan_partial_eval(trace,*tracers,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.loops._scan_pp_rule(eqn,context,settings)
jax._src.lax.control_flow.loops._scan_transpose(reduce_axes,cts,*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._scan_typecheck(bind_time,*in_atoms,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax._src.lax.control_flow.loops._split_leading_dim(i,aval,x)
jax._src.lax.control_flow.loops._stack(aval,vals)
jax._src.lax.control_flow.loops._transpose_scan_jaxpr(num_res1,num_c,num_res2,jaxpr,reduce_axes)
jax._src.lax.control_flow.loops._update_array(i,aval,xs,x)
jax._src.lax.control_flow.loops._while_loop_abstract_eval(*args,cond_jaxpr,body_jaxpr,**kwargs)
jax._src.lax.control_flow.loops._while_loop_batching_rule(axis_size,axis_name,main_type,args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.lax.control_flow.loops._while_loop_jvp(primals,tangents,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax._src.lax.control_flow.loops._while_lowering(ctx,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax._src.lax.control_flow.loops._while_partial_eval(trace:pe.JaxprTrace,*tracers:pe.Tracer,cond_nconsts:int,cond_jaxpr:pe.ClosedJaxpr,body_nconsts:int,body_jaxpr:pe.ClosedJaxpr)->Sequence[pe.Tracer]
jax._src.lax.control_flow.loops._while_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.loops._while_transpose_error(*_,**kwargs)
jax._src.lax.control_flow.loops._while_typecheck(*in_atoms,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax._src.lax.control_flow.loops.associative_scan(fn:Callable,elems,reverse:bool=False,axis:int=0)
jax._src.lax.control_flow.loops.cummax(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cummin(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cumprod(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.cumred_gpu_impl(window_reduce:Callable,reduce_fn:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops.cumred_reduce_window_impl(window_reduce:Callable,x,*,axis:int,reverse:bool)
jax._src.lax.control_flow.loops.cumsum(operand:Array,axis:int=0,reverse:bool=False)->Array
jax._src.lax.control_flow.loops.fori_loop(lower,upper,body_fun,init_val)
jax._src.lax.control_flow.loops.map(f,xs)
jax._src.lax.control_flow.loops.scan(f:Callable[[Carry,X],Tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->Tuple[Carry, Y]
jax._src.lax.control_flow.loops.scan_bind(*args,**params)
jax._src.lax.control_flow.loops.while_loop(cond_fun:Callable[[T],BooleanNumeric],body_fun:Callable[[T],T],init_val:T)->T
jax._src.lax.control_flow.map(f,xs)
jax._src.lax.control_flow.scan(f:Callable[[Carry,X],Tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->Tuple[Carry, Y]
jax._src.lax.control_flow.scan_bind(*args,**params)
jax._src.lax.control_flow.while_loop(cond_fun:Callable[[T],BooleanNumeric],body_fun:Callable[[T],T],init_val:T)->T


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/for_loop.py----------------------------------------
A:jax._src.lax.control_flow.for_loop.S->TypeVar('S')
A:jax._src.lax.control_flow.for_loop.T->TypeVar('T')
A:jax._src.lax.control_flow.for_loop.for_p->jax.core.Primitive('for')
A:jax._src.lax.control_flow.for_loop.(const_avals, const_ref_avals)->partition_list(is_const_ref, all_const_avals)
A:jax._src.lax.control_flow.for_loop.merged_const_avals->merge_lists(is_const_ref, const_avals, const_ref_avals)
A:jax._src.lax.control_flow.for_loop.num_consts->len(merged_const_avals)
A:jax._src.lax.control_flow.for_loop.(all_consts, args)->split_list(consts_args, [num_consts])
A:jax._src.lax.control_flow.for_loop.(consts, const_refs)->partition_list(is_const_ref, all_consts)
A:jax._src.lax.control_flow.for_loop.consts->map(lambda x: ref_get(x, ()), consts)
A:jax._src.lax.control_flow.for_loop.all_consts->merge_lists(is_const_ref, consts, const_refs)
A:jax._src.lax.control_flow.for_loop.(hoisted_jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(_hoist), in_avals)
A:jax._src.lax.control_flow.for_loop.(f, out_tree_thunk)->flatten_fun_nokwargs(lu.wrap_init(f), treedef_tuple((tree_structure(0), state_tree)))
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, [*in_avals, *res_ref_avals])
A:jax._src.lax.control_flow.for_loop.aval->jax.core.raise_to_shaped(core.get_aval(x))
A:jax._src.lax.control_flow.for_loop.vals->for_loop(rest_steps, partial(body, i), vals, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(flat_state, state_tree)->tree_flatten(init_state)
A:jax._src.lax.control_flow.for_loop.state_avals->map(val_to_ref_aval, flat_state)
A:jax._src.lax.control_flow.for_loop.idx_aval->jax.core.ShapedArray((), jnp.dtype('int32'))
A:jax._src.lax.control_flow.for_loop.(jaxpr, consts, out_tree)->_trace_to_jaxpr_with_refs(body, state_tree, [idx_aval, *state_avals])
A:jax._src.lax.control_flow.for_loop.jaxpr->_hoist_consts_to_refs(jaxpr)
A:jax._src.lax.control_flow.for_loop.out_flat->jax._src.lax.control_flow.loops.fori_loop(0, nsteps, fori_body, flat_state)
A:jax._src.lax.control_flow.for_loop.Carry->TypeVar('Carry')
A:jax._src.lax.control_flow.for_loop.X->TypeVar('X')
A:jax._src.lax.control_flow.for_loop.Y->TypeVar('Y')
A:jax._src.lax.control_flow.for_loop.(xs_flat, xs_tree)->tree_flatten(xs)
A:jax._src.lax.control_flow.for_loop.length->int(length)
A:jax._src.lax.control_flow.for_loop.unique_lengths->set(lengths)
A:jax._src.lax.control_flow.for_loop.x_avals->tuple(map(core.ShapedArray, x_shapes, x_dtypes))
A:jax._src.lax.control_flow.for_loop.init_flat->tree_leaves(init)
A:jax._src.lax.control_flow.for_loop.(_, in_tree)->tree_flatten((init, xs))
A:jax._src.lax.control_flow.for_loop.carry_avals->tuple(map(_abstractify, init_flat))
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, out_tree)->_initial_style_jaxpr(f, in_tree, carry_avals + x_avals, 'scan')
A:jax._src.lax.control_flow.for_loop.(jaxpr, out_tree)->_create_jaxpr(init)
A:jax._src.lax.control_flow.for_loop.(_, ys_avals)->tree_unflatten(out_tree, jaxpr.out_avals)
A:jax._src.lax.control_flow.for_loop.ys->tree_map(lambda aval: jnp.zeros([length, *aval.shape], aval.dtype), ys_avals)
A:jax._src.lax.control_flow.for_loop.carry->tree_map(lambda x: x[()], carry_refs)
A:jax._src.lax.control_flow.for_loop.x->tree_map(lambda x: x[i], xs_refs)
A:jax._src.lax.control_flow.for_loop.(carry, y)->f(carry, x)
A:jax._src.lax.control_flow.for_loop.(init, _, ys)->for_loop(length, for_body, (init, xs, ys), reverse=reverse, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.nonlocal_state_effects->jax.core.join_effects(*aval_effects)
A:jax._src.lax.control_flow.for_loop.out_vals->jax.core.Primitive('for').bind(*args, jaxpr=jaxpr, reverse=reverse, which_linear=which_linear, nsteps=nsteps, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, consts)->discharge_state(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.i->jax.numpy.int32(i)
A:jax._src.lax.control_flow.for_loop.state->body(i, state)
A:jax._src.lax.control_flow.for_loop.(_, state)->jax.lax.while_loop(cond, while_body, (i, state))
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, body_consts)->discharge_state(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.(_, out_batched)->jax.interpreters.batching.batch_jaxpr(core.ClosedJaxpr(discharged_jaxpr, body_consts), axis_size, [False] + batched, instantiate=batched, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.for_loop.batched->map(operator.or_, batched, out_batched)
A:jax._src.lax.control_flow.for_loop.(batched_jaxpr_, _)->jax.interpreters.batching.batch_jaxpr(core.ClosedJaxpr(jaxpr, []), axis_size, [False] + batched, [], axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.for_loop.(_, out_nonzero_tangents)->jax.interpreters.ad.jvp_jaxpr(core.ClosedJaxpr(discharged_jaxpr, body_consts), [False] + nonzero_tangents, instantiate=nonzero_tangents)
A:jax._src.lax.control_flow.for_loop.nonzero_tangents->map(operator.or_, nonzero_tangents, out_nonzero_tangents)
A:jax._src.lax.control_flow.for_loop.closed_jaxpr->jax.core.ClosedJaxpr(jaxpr, ())
A:jax._src.lax.control_flow.for_loop.(jvp_jaxpr_, _)->jax.interpreters.ad.jvp_jaxpr(closed_jaxpr, [False] + nonzero_tangents, [])
A:jax._src.lax.control_flow.for_loop.(out_primals, out_tangents)->split_list(out_flat, [len(primals)])
A:jax._src.lax.control_flow.for_loop.out_tangents_iter->iter(out_tangents)
A:jax._src.lax.control_flow.for_loop.loop_var_refs->map(operator.not_, loop_invar_refs)
A:jax._src.lax.control_flow.for_loop.(_, _, loop_var_outputs, _, _)->_partial_eval_jaxpr_custom(jaxpr, loop_var_inputs, _save_everything)
A:jax._src.lax.control_flow.for_loop.num_inputs->len(eqn.invars)
A:jax._src.lax.control_flow.for_loop.(discharged_jaxpr, discharged_consts)->discharge_state(jaxpr, consts)
A:jax._src.lax.control_flow.for_loop.discharged_jaxpr->discharged_jaxpr.replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[]).replace(invars=discharged_jaxpr.constvars + discharged_jaxpr.invars, constvars=[])
A:jax._src.lax.control_flow.for_loop.(_, _, out_unknowns, _, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(discharged_jaxpr, jaxpr_in_unknowns, [True] * len(jaxpr_in_unknowns), in_unknowns, False, _save_everything)
A:jax._src.lax.control_flow.for_loop.out_unknowns->list(out_unknowns)
A:jax._src.lax.control_flow.for_loop.in_unknowns->map(operator.or_, in_unknowns, out_unknowns)
A:jax._src.lax.control_flow.for_loop.tracers->tuple((trace.instantiate_const(t) if uk else t for (t, uk) in zip(tracers, in_unknowns)))
A:jax._src.lax.control_flow.for_loop.(jaxpr_known_resout, jaxpr_unknown_resin_, uk_out, inst_out, num_res)->_partial_eval_jaxpr_custom(jaxpr, [False, *in_unknowns], _save_everything)
A:jax._src.lax.control_flow.for_loop.loop_invar_res->_loop_invariant_outputs(jaxpr_known_resout)
A:jax._src.lax.control_flow.for_loop.(jaxpr_known, res_avals)->_convert_outputs_to_writes(nsteps, jaxpr_known_resout, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.(known_tracers, _)->partition_list(in_unknowns, tracers)
A:jax._src.lax.control_flow.for_loop.empty_res->map(ad_util.zeros_like_aval, res_avals)
A:jax._src.lax.control_flow.for_loop.(known_outputs, residuals)->split_list(out_flat, [len(known_tracers)])
A:jax._src.lax.control_flow.for_loop.residuals->map(trace.new_instantiated_const, residuals)
A:jax._src.lax.control_flow.for_loop.(jaxpr_unknown_resin, used_inputs)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr_unknown_resin_, [], [True] * num_res + [True, *in_unknowns])
A:jax._src.lax.control_flow.for_loop.(used_res, (used_i,), used_refs)->split_list(used_inputs, [num_res, 1])
A:jax._src.lax.control_flow.for_loop.jaxpr_unknown->_convert_inputs_to_reads(nsteps, len(res_avals), jaxpr_unknown_resin, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.used_and_known->map(operator.and_, used_refs, map(operator.not_, in_unknowns))
A:jax._src.lax.control_flow.for_loop.(_, known_used)->partition_list(used_refs, used_and_known)
A:jax._src.lax.control_flow.for_loop.(_, used_tracers)->partition_list(used_refs, tracers)
A:jax._src.lax.control_flow.for_loop.(_, used_which_linear)->partition_list(used_refs, which_linear)
A:jax._src.lax.control_flow.for_loop.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.for_loop.eqn->jax.interpreters.partial_eval.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs, for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps, reverse=reverse, which_linear=which_linear_unknown, unroll=unroll), core.no_effects, source)
A:jax._src.lax.control_flow.for_loop.(_, unknown_outputs)->split_list(res_ref_unknown_outputs, [num_res])
A:jax._src.lax.control_flow.for_loop.(unknown_outputs, _)->partition_list(known_used, unknown_outputs)
A:jax._src.lax.control_flow.for_loop.(jaxpr, nsteps, reverse, which_linear, unroll)->split_dict(eqn.params, ['jaxpr', 'nsteps', 'reverse', 'which_linear', 'unroll'])
A:jax._src.lax.control_flow.for_loop.(_, _, out_unknowns, out_inst, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(discharged_jaxpr, jaxpr_in_unknowns, True, ensure_out_unknowns=in_unknowns, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.for_loop.(jaxpr_known_resout, jaxpr_staged_resin_, _, _, num_res)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr, [False, *in_unknowns], [True, *in_inst], [], [], saveable)
A:jax._src.lax.control_flow.for_loop.(known_invars, _)->partition_list(in_unknowns, eqn.invars)
A:jax._src.lax.control_flow.for_loop.(known_outvars, _)->partition_list(in_unknowns, eqn.outvars)
A:jax._src.lax.control_flow.for_loop.newvar->jax.core.gensym()
A:jax._src.lax.control_flow.for_loop.resvars->map(newvar, res_avals)
A:jax._src.lax.control_flow.for_loop.(call_jaxpr_, _, call_jaxpr_consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(staged, [v.aval for v in [*resvars, *eqn.invars]])
A:jax._src.lax.control_flow.for_loop.call_jaxpr->jax.core.ClosedJaxpr(call_jaxpr_, call_jaxpr_consts)
A:jax._src.lax.control_flow.for_loop.eqn_known->jax.interpreters.partial_eval.new_jaxpr_eqn(known_invars, [*known_outvars, *resvars], core.closed_call_p, dict(call_jaxpr=call_jaxpr), call_jaxpr.effects, eqn.source_info)
A:jax._src.lax.control_flow.for_loop.jaxpr_staged->_convert_inputs_to_reads(nsteps, len(res_avals), jaxpr_staged_resin_, loop_invar_res)
A:jax._src.lax.control_flow.for_loop.params_staged->dict(eqn.params, jaxpr=jaxpr_staged, reverse=reverse, nsteps=nsteps, which_linear=which_linear_unknown, unroll=unroll)
A:jax._src.lax.control_flow.for_loop.(_, ans)->partition_list(out_inst, ans)
A:jax._src.lax.control_flow.for_loop.(_, outvars)->partition_list(out_inst, eqn.outvars)
A:jax._src.lax.control_flow.for_loop.eqn_staged->jax.interpreters.partial_eval.new_jaxpr_eqn([*resvars, *eqn.invars], outvars, core.closed_call_p, dict(call_jaxpr=call_jaxpr), call_jaxpr.effects, eqn.source_info)
A:jax._src.lax.control_flow.for_loop.(orig_refs, residual_refs)->split_list(refs, [len(in_avals) - 1])
A:jax._src.lax.control_flow.for_loop.residual_vals->jax.core.eval_jaxpr(jaxpr, (), i, *orig_refs)
A:jax._src.lax.control_flow.for_loop.(residual_refs, orig_refs)->split_list(refs, [num_res])
A:jax._src.lax.control_flow.for_loop.()->jax.core.eval_jaxpr(jaxpr, (), *residual_vals, i, *orig_refs)
A:jax._src.lax.control_flow.for_loop.(res_val_avals, (i_aval,), orig_ref_avals)->split_list([v.aval for v in jaxpr.invars], [num_res, 1])
A:jax._src.lax.control_flow.for_loop.(jaxpr, _, ())->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(eval_jaxpr, [i_aval, *res_ref_avals, *orig_ref_avals])
A:jax._src.lax.control_flow.for_loop.(res_jaxpr, tangent_jaxpr_, *_)->_partial_eval_jaxpr_custom(jaxpr, [False, *which_linear], _save_everything)
A:jax._src.lax.control_flow.for_loop.res->jax.core.eval_jaxpr(res_jaxpr, (), i, *res_args)
A:jax._src.lax.control_flow.for_loop.(tangent_jaxpr, used)->jax.interpreters.partial_eval.dce_jaxpr(tangent_jaxpr_, [])
A:jax._src.lax.control_flow.for_loop.(used_res, (used_i,), used_ct)->split_list(used, [len(res), 1])
A:jax._src.lax.control_flow.for_loop.(jaxpr_trans, _, _)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(lu.wrap_init(trans), [v.aval for v in jaxpr.invars])
A:jax._src.lax.control_flow.for_loop.jaxpr_transpose->transpose_jaxpr(jaxpr, which_linear)
A:jax._src.lax.control_flow.for_loop.all_outs->jax.core.Primitive('for').bind(*args_, jaxpr=jaxpr_transpose, nsteps=nsteps, reverse=not reverse, which_linear=tuple(which_linear_transpose), unroll=unroll)
jax._src.lax.control_flow.for_loop.Ref(Generic[T])
jax._src.lax.control_flow.for_loop._convert_inputs_to_reads(nsteps:int,num_res:int,jaxpr:core.Jaxpr,loop_invar_res:Sequence[bool])->core.Jaxpr
jax._src.lax.control_flow.for_loop._convert_outputs_to_writes(nsteps:int,jaxpr:core.Jaxpr,loop_invar_res:Sequence[bool])->Tuple[core.Jaxpr, List[core.ShapedArray]]
jax._src.lax.control_flow.for_loop._for_abstract_eval(*avals,jaxpr,**__)
jax._src.lax.control_flow.for_loop._for_discharge_rule(in_avals,*args:Any,jaxpr:core.Jaxpr,reverse:bool,which_linear:Sequence[bool],nsteps:int,unroll:int)->Tuple[Sequence[Optional[Any]], Sequence[Any]]
jax._src.lax.control_flow.for_loop._for_impl(*args,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_impl_unrolled(body,nsteps,unroll,*args)
jax._src.lax.control_flow.for_loop._for_jvp(primals,tangents,*,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_partial_eval(trace:pe.JaxprTrace,*tracers:pe.JaxprTracer,jaxpr:core.Jaxpr,nsteps:int,reverse:bool,which_linear:Tuple[bool,...],unroll:int)->List[pe.JaxprTracer]
jax._src.lax.control_flow.for_loop._for_partial_eval_custom(saveable,in_unknowns,in_inst,eqn)
jax._src.lax.control_flow.for_loop._for_transpose(in_cts,*args,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._for_vmap(axis_size,axis_name,main_type,args,dims,*,jaxpr,nsteps,reverse,which_linear,unroll)
jax._src.lax.control_flow.for_loop._get_ref_state_effects(jaxpr:core.Jaxpr)->List[Set[StateEffect]]
jax._src.lax.control_flow.for_loop._hoist_consts_to_refs(jaxpr:core.Jaxpr)->core.Jaxpr
jax._src.lax.control_flow.for_loop._is_read_only(ref_effects:Set[StateEffect])->bool
jax._src.lax.control_flow.for_loop._loop_invariant_outputs(jaxpr:core.Jaxpr)->List[bool]
jax._src.lax.control_flow.for_loop._partial_eval_jaxpr_custom(jaxpr,in_unknowns,policy)
jax._src.lax.control_flow.for_loop._trace_to_jaxpr_with_refs(f,state_tree:PyTreeDef,state_avals:Sequence[core.AbstractValue])->Tuple[core.Jaxpr, List[Any], PyTreeDef]
jax._src.lax.control_flow.for_loop.discharged_for_loop(nsteps,body,init_state,*,reverse:bool=False)
jax._src.lax.control_flow.for_loop.for_loop(nsteps:Union[int,Sequence[int]],body:Callable[[Array,Ref[S]],None],init_state:S,*,reverse:bool=False,unroll:int=1)->S
jax._src.lax.control_flow.for_loop.scan(f:Callable[[Carry,X],Tuple[Carry,Y]],init:Carry,xs:X,length:Optional[int]=None,reverse:bool=False,unroll:int=1)->Tuple[Carry, Y]
jax._src.lax.control_flow.for_loop.transpose_jaxpr(jaxpr:core.Jaxpr,which_linear:List[bool])->core.Jaxpr
jax._src.lax.control_flow.for_loop.val_to_ref_aval(x)->ShapedArrayRef


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/conditionals.py----------------------------------------
A:jax._src.lax.control_flow.conditionals._no_operand_sentinel->object()
A:jax._src.lax.control_flow.conditionals.index_dtype->jax._src.dtypes.result_type(index)
A:jax._src.lax.control_flow.conditionals.branches->tuple((core.subst_axis_names_jaxpr(jaxpr, subst) for jaxpr in params['branches']))
A:jax._src.lax.control_flow.conditionals.index->jax.core.raise_as_much_as_possible(index)
A:jax._src.lax.control_flow.conditionals.lo->numpy.array(0, np.int32)
A:jax._src.lax.control_flow.conditionals.hi->numpy.array(len(branches) - 1, np.int32)
A:jax._src.lax.control_flow.conditionals.(ops, ops_tree)->tree_flatten(operands)
A:jax._src.lax.control_flow.conditionals.ops_avals->tuple(map(_abstractify, ops))
A:jax._src.lax.control_flow.conditionals.(jaxprs, consts, out_trees)->_initial_style_jaxprs_with_common_consts((true_fun, false_fun), ops_tree, ops_avals, 'cond')
A:jax._src.lax.control_flow.conditionals.joined_effects->jax.core.join_effects(*(branch.effects for branch in branches))
A:jax._src.lax.control_flow.conditionals.out->jax.core.AxisPrimitive('cond').bind(index, *res, *cts, branches=branches_trans, linear=linear_trans)
A:jax._src.lax.control_flow.conditionals.pred_dtype->jax._src.dtypes.result_type(pred)
A:jax._src.lax.control_flow.conditionals.(linear_ops, ops_tree2)->tree_flatten(linear)
A:jax._src.lax.control_flow.conditionals.ba->inspect.signature(_cond_with_per_branch_args).bind(*args, **kwargs)
A:jax._src.lax.control_flow.conditionals.idx->list(range(np.ndim(pred)))
A:jax._src.lax.control_flow.conditionals.pred->jax._src.lax.lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)
A:jax._src.lax.control_flow.conditionals.predicate->jax._src.lax.lax.eq(index, lax._const(index, i))
A:jax._src.lax.control_flow.conditionals.branches_batched->tuple((batching.batch_jaxpr(jaxpr, axis_size, ops_bat, out_bat, axis_name, main_type)[0] for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.branches_jvp->tuple((ad.jvp_jaxpr(jaxpr, ops_nz, instantiate=out_nz)[0] for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.ops_dot->_prune_zeros(ops_dot)
A:jax._src.lax.control_flow.conditionals.ops_lin->tuple(linear)
A:jax._src.lax.control_flow.conditionals.(out_primals, out_tangents)->split_list(out, [len(out_nz)])
A:jax._src.lax.control_flow.conditionals.out_tangents_iter->iter(out_tangents)
A:jax._src.lax.control_flow.conditionals.params->dict(branches=branches_unknown, linear=tuple(linear_unknown))
A:jax._src.lax.control_flow.conditionals.(_, _, out_uks, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(branch_jaxpr, ops_uk, instantiate=False)
A:jax._src.lax.control_flow.conditionals.(branch_jaxpr_known, branch_jaxpr_unknown, _, res_avals)->jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(branch_jaxpr, ops_uk, instantiate=out_uks)
A:jax._src.lax.control_flow.conditionals.(all_res_avals, res_avals_per_branch)->_merge_branch_residuals(branch_res_avals)
A:jax._src.lax.control_flow.conditionals.num_res->len(res_indices)
A:jax._src.lax.control_flow.conditionals.branches_known->_join_cond_outputs(branches_known_, all_res_avals, res_avals_per_branch, num_known_outs)
A:jax._src.lax.control_flow.conditionals.branches_unknown->_join_cond_pe_staged_jaxpr_inputs(branches_unknown, all_res_avals, res_avals_per_branch)
A:jax._src.lax.control_flow.conditionals.out_consts_res->jax.core.AxisPrimitive('cond').bind(*in_consts, branches=branches_known, linear=tuple(linear_known))
A:jax._src.lax.control_flow.conditionals.(out_consts, res)->split_list(out_consts_res, [len(out_consts_res) - num_res])
A:jax._src.lax.control_flow.conditionals.index_tracer->trace.instantiate_const(tracers[0])
A:jax._src.lax.control_flow.conditionals.res_tracers->map(trace.new_instantiated_const, res)
A:jax._src.lax.control_flow.conditionals.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax._src.lax.control_flow.conditionals.eqn->jax.interpreters.partial_eval.new_eqn_recipe([index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params, core.join_effects(*(j.effects for j in branches_unknown)), source)
A:jax._src.lax.control_flow.conditionals.(_, _, unks_out_, _, _)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=ops_uk, in_inst=True, ensure_out_unknowns=False, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.conditionals.unks_out->map(operator.or_, unks_out, unks_out_)
A:jax._src.lax.control_flow.conditionals.(jaxpr_known, jaxpr_staged, _, inst_out, num_res)->jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr.jaxpr, in_unknowns=ops_uk, in_inst=True, ensure_out_unknowns=unks_out, ensure_out_inst=True, saveable=saveable)
A:jax._src.lax.control_flow.conditionals.branches_staged->_join_cond_pe_staged_jaxpr_inputs(branches_staged_, all_res_avals, res_avals_per_branch)
A:jax._src.lax.control_flow.conditionals.newvar->jax.core.gensym([j.jaxpr for j in jaxprs], suffix='_')
A:jax._src.lax.control_flow.conditionals.res_binders->map(newvar, all_res_avals)
A:jax._src.lax.control_flow.conditionals.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax._src.lax.control_flow.conditionals.(out_binders_known, _)->partition_list(unks_out, eqn.outvars)
A:jax._src.lax.control_flow.conditionals.params_known->dict(branches=branches_known, linear=tuple(linear_known))
A:jax._src.lax.control_flow.conditionals.effects_known->jax.core.join_effects(*(b.effects for b in branches_known))
A:jax._src.lax.control_flow.conditionals.eqn_known->jax.interpreters.partial_eval.new_jaxpr_eqn(ins_known, [*out_binders_known, *res_binders], cond_p, params_known, effects_known, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax._src.lax.control_flow.conditionals.params_staged->dict(branches=branches_staged, linear=tuple(linear_staged))
A:jax._src.lax.control_flow.conditionals.effects_staged->jax.core.join_effects(*(b.effects for b in branches_staged))
A:jax._src.lax.control_flow.conditionals.eqn_staged->jax.interpreters.partial_eval.new_jaxpr_eqn([eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged, cond_p, params_staged, effects_staged, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.branch_res_tagged_avals->map(enumerate_equal, branch_res_avals)
A:jax._src.lax.control_flow.conditionals.all_tagged_avals->_ordered_unique(util.concatenate(branch_res_tagged_avals))
A:jax._src.lax.control_flow.conditionals.outs_and_residuals->jax.core.jaxpr_as_fun(jaxpr)(*args)
A:jax._src.lax.control_flow.conditionals.(outs, residuals)->split_list(outs_and_residuals, [num_non_res_outputs])
A:jax._src.lax.control_flow.conditionals.aug_residuals->jax._src.util.subvals(aug_residuals, zip(res_indices, residuals))
A:jax._src.lax.control_flow.conditionals.all_res_vars->map(newvar, all_res_avals)
A:jax._src.lax.control_flow.conditionals.aug_res_vars->list(util.subvals(all_res_vars, zip(res_indices, res_vars)))
A:jax._src.lax.control_flow.conditionals.jaxpr_aug->jax.core.ClosedJaxpr(jaxpr_aug, jaxpr.consts)
A:jax._src.lax.control_flow.conditionals.d->collections.OrderedDict(((x, None) for x in xs))
A:jax._src.lax.control_flow.conditionals.(_, used_inputs_)->jax.interpreters.partial_eval.dce_jaxpr(jaxpr, used_outputs, instantiate=False)
A:jax._src.lax.control_flow.conditionals.used_inputs->map(operator.or_, used_inputs, used_inputs_)
A:jax._src.lax.control_flow.conditionals.new_params->dict(eqn.params, branches=tuple(dce_branches), linear=tuple(dce_linear))
A:jax._src.lax.control_flow.conditionals.new_effects->jax.core.join_effects(*(b.effects for b in dce_branches))
A:jax._src.lax.control_flow.conditionals.new_eqn->jax.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, [True, *used_inputs]) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_effects, eqn.source_info)
A:jax._src.lax.control_flow.conditionals.(res_avals, primal_avals)->split_list(jaxpr.in_avals, [num_res])
A:jax._src.lax.control_flow.conditionals.primal_avals->map(raise_to_shaped, primal_avals)
A:jax._src.lax.control_flow.conditionals.(res, cts_out)->split_list(args, [num_res])
A:jax._src.lax.control_flow.conditionals.cts_in->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, reduce_axes, False, jaxpr.consts, primals, cts_out)
A:jax._src.lax.control_flow.conditionals.(_, cts_in)->split_list(cts_in, [num_res])
A:jax._src.lax.control_flow.conditionals.in_avals->map(raise_to_shaped, branches[0].in_avals)
A:jax._src.lax.control_flow.conditionals.branches_trans->tuple((_transpose_cond_jaxpr(jaxpr, num_res, reduce_axes) for jaxpr in branches))
A:jax._src.lax.control_flow.conditionals.cts->map(ad.instantiate_zeros_aval, branches[0].out_avals, cts)
A:jax._src.lax.control_flow.conditionals.out_iter->iter(out)
A:jax._src.lax.control_flow.conditionals.tc->partial(_typecheck_param, 'cond')
A:jax._src.lax.control_flow.conditionals.jaxpr0_in_avals_str->_avals_short(jaxpr0.in_avals)
A:jax._src.lax.control_flow.conditionals.jaxpr0_out_avals_str->_avals_short(jaxpr0.out_avals)
A:jax._src.lax.control_flow.conditionals.avals->map(core.get_aval, args)
A:jax._src.lax.control_flow.conditionals.cond_p->jax.core.AxisPrimitive('cond')
A:jax._src.lax.control_flow.conditionals.num_tokens->len(ordered_effects)
A:jax._src.lax.control_flow.conditionals.tokens_in->ctx.tokens_in.subset(ordered_effects)
A:jax._src.lax.control_flow.conditionals.flat_output_types->jax._src.util.flatten(output_types)
A:jax._src.lax.control_flow.conditionals.case_op->jax._src.lib.mlir.dialects.mhlo.CaseOp(flat_output_types, index=index, num_branches=len(branches))
A:jax._src.lax.control_flow.conditionals.name_stack->extend_name_stack(ctx.module_context.name_stack, 'cond')
A:jax._src.lax.control_flow.conditionals.branch->jax._src.lib.mlir.dialects.mhlo.CaseOp(flat_output_types, index=index, num_branches=len(branches)).regions[i].blocks.append()
A:jax._src.lax.control_flow.conditionals.sub_ctx->ctx.module_context.replace(name_stack=xla.extend_name_stack(name_stack, f'branch_{i}_fun'))
A:jax._src.lax.control_flow.conditionals.(out_vals, tokens_out)->jax.interpreters.mlir.jaxpr_subcomp(sub_ctx, jaxpr.jaxpr, tokens_in, map(mlir.ir_constants, jaxpr.consts), *map(mlir.wrap_singleton_ir_values, args))
A:jax._src.lax.control_flow.conditionals.tokens_and_outputs->jax._src.util.unflatten(case_op.results, map(len, output_types))
A:jax._src.lax.control_flow.conditionals.(tokens, outputs)->jax._src.util.split_list(tokens_and_outputs, [num_tokens])
jax._src.lax.control_flow.cond(*args,**kwargs)
jax._src.lax.control_flow.cond_bind(*args,branches,linear)
jax._src.lax.control_flow.conditionals._bcast_select(pred,on_true,on_false)
jax._src.lax.control_flow.conditionals._bcast_select_n(pred,*cases)
jax._src.lax.control_flow.conditionals._cond(pred,true_fun:Callable,false_fun:Callable,*operands,operand=_no_operand_sentinel,linear=None)
jax._src.lax.control_flow.conditionals._cond_abstract_eval(*args,branches,**kwargs)
jax._src.lax.control_flow.conditionals._cond_axis_substitution(params,subst,traverse)
jax._src.lax.control_flow.conditionals._cond_batching_rule(axis_size,axis_name,main_type,args,dims,branches,linear)
jax._src.lax.control_flow.conditionals._cond_dce_rule(used_outputs:List[bool],eqn:core.JaxprEqn)->Tuple[List[bool], core.JaxprEqn]
jax._src.lax.control_flow.conditionals._cond_jvp(primals,tangents,branches,linear)
jax._src.lax.control_flow.conditionals._cond_lowering(ctx,index,*args,branches,linear)
jax._src.lax.control_flow.conditionals._cond_partial_eval(trace,*tracers,branches,linear)
jax._src.lax.control_flow.conditionals._cond_partial_eval_custom(saveable,unks_in,inst_in,eqn)
jax._src.lax.control_flow.conditionals._cond_transpose(reduce_axes,cts,*args,branches,linear)
jax._src.lax.control_flow.conditionals._cond_typecheck(*in_atoms,branches,linear)
jax._src.lax.control_flow.conditionals._cond_with_per_branch_args(pred,true_operand,true_fun:Callable,false_operand,false_fun:Callable)
jax._src.lax.control_flow.conditionals._join_cond_outputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr,num_non_res_outputs)
jax._src.lax.control_flow.conditionals._join_cond_pe_staged_jaxpr_inputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr)
jax._src.lax.control_flow.conditionals._merge_branch_residuals(branch_res_avals)
jax._src.lax.control_flow.conditionals._ordered_unique(xs)
jax._src.lax.control_flow.conditionals._transpose_cond_jaxpr(jaxpr,num_res,reduce_axes)
jax._src.lax.control_flow.conditionals.cond(*args,**kwargs)
jax._src.lax.control_flow.conditionals.cond_bind(*args,branches,linear)
jax._src.lax.control_flow.conditionals.switch(index,branches:Sequence[Callable],*operands,operand=_no_operand_sentinel)
jax._src.lax.control_flow.switch(index,branches:Sequence[Callable],*operands,operand=_no_operand_sentinel)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/solves.py----------------------------------------
A:jax._src.lax.control_flow.solves._RootTuple->collections.namedtuple('_RootTuple', 'f, solve, l_and_s')
A:jax._src.lax.control_flow.solves.params_list->split_list(args, list(const_lengths))
A:jax._src.lax.control_flow.solves.(guess_flat, in_args_tree)->tree_flatten((initial_guess,))
A:jax._src.lax.control_flow.solves.guess_avals->tuple(_map(_abstractify, guess_flat))
A:jax._src.lax.control_flow.solves.(f_jaxpr, f_consts, out_tree)->_initial_style_jaxpr(f, in_args_tree, guess_avals)
A:jax._src.lax.control_flow.solves.(in_tree,)->treedef_children(in_args_tree)
A:jax._src.lax.control_flow.solves.(solve_jaxpr, solve_consts, solution_tree)->_initial_style_jaxpr(partial(solve, f), in_args_tree, guess_avals)
A:jax._src.lax.control_flow.solves.(unchecked_zeros, f_jvp)->jax.linearize(f, x)
A:jax._src.lax.control_flow.solves.(l_and_s_jaxpr, l_and_s_consts, out_tree)->_initial_style_jaxpr(linearize_and_solve, treedef_tuple((in_tree,) * 2), guess_avals * 2)
A:jax._src.lax.control_flow.solves.const_lengths->_LinearSolveTuple(*_map(len, all_consts))
A:jax._src.lax.control_flow.solves.jaxprs->_LinearSolveTuple(matvec_jaxpr, vecmat_jaxpr, solve_jaxpr, tr_solve_jaxpr)
A:jax._src.lax.control_flow.solves.solution_flat->_custom_root(const_lengths, jaxprs, *_flatten(all_consts) + guess_flat)
A:jax._src.lax.control_flow.solves.(params, initial_guess)->_split_root_args(args, const_lengths)
A:jax._src.lax.control_flow.solves.solution->jax.core.jaxpr_as_fun(jaxprs.solve)(*params.solve + initial_guess)
A:jax._src.lax.control_flow.solves.(params, _)->_split_linear_solve_args(primals, const_lengths)
A:jax._src.lax.control_flow.solves.sol->_custom_root(const_lengths, jaxprs, *primals)
A:jax._src.lax.control_flow.solves.f_out_vals->len(jaxprs.f.out_avals)
A:jax._src.lax.control_flow.solves.(solution, aux)->split_list(sol, [f_out_vals])
A:jax._src.lax.control_flow.solves.(params_dot, _)->_split_root_args(tangents, const_lengths)
A:jax._src.lax.control_flow.solves.f->jax.core.jaxpr_as_fun(jaxprs.f)
A:jax._src.lax.control_flow.solves.linearize_and_solve->partial(core.jaxpr_as_fun(jaxprs.l_and_s), *params.l_and_s)
A:jax._src.lax.control_flow.solves.(_, rhs)->jax.interpreters.ad.jvp(lu.wrap_init(f_at_solution)).call_wrapped(params.f, params_dot.f)
A:jax._src.lax.control_flow.solves.solution_dot->_map(operator.neg, linearize_and_solve(*solution, *rhs))
A:jax._src.lax.control_flow.solves.transpose_fun->jax.linear_transpose(linear_fun, primals)
A:jax._src.lax.control_flow.solves.(y,)->transpose_fun(x)
A:jax._src.lax.control_flow.solves.actual_shapes->_map(np.shape, tree_leaves(actual))
A:jax._src.lax.control_flow.solves.expected_shapes->_map(np.shape, tree_leaves(expected))
A:jax._src.lax.control_flow.solves.(b_flat, in_args_tree)->tree_flatten((b,))
A:jax._src.lax.control_flow.solves.b_avals->tuple(_map(_abstractify, b_flat))
A:jax._src.lax.control_flow.solves.(tree,)->treedef_children(in_args_tree)
A:jax._src.lax.control_flow.solves.y->fun(x)
A:jax._src.lax.control_flow.solves.(y, aux)->fun(x)
A:jax._src.lax.control_flow.solves.(matvec_jaxpr, matvec_consts, out_tree)->_initial_style_jaxpr(_shape_checked(matvec, 'matvec', False), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.(solve_jaxpr, solve_consts, out_tree)->_initial_style_jaxpr(_shape_checked(partial(solve, matvec), 'solve', has_aux), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.vecmat->_transpose_one_output(matvec, b)
A:jax._src.lax.control_flow.solves.(vecmat_jaxpr, vecmat_consts, out_tree)->_initial_style_jaxpr(vecmat, in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.(tr_solve_jaxpr, tr_solve_consts, out_tree)->_initial_style_jaxpr(_shape_checked(partial(transpose_solve, vecmat), 'transpose_solve', has_aux), in_args_tree, b_avals, 'custom_linear_solve')
A:jax._src.lax.control_flow.solves.out_flat->jax.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(all_consts) + b_flat, const_lengths=const_lengths, jaxprs=jaxprs)
A:jax._src.lax.control_flow.solves.(params, b)->_split_linear_solve_args(args, const_lengths)
A:jax._src.lax.control_flow.solves.x->jax.core.AxisPrimitive('custom_linear_solve').bind(*primals, **kwargs)
A:jax._src.lax.control_flow.solves.zeros->_map(ad_util.Zero.from_value, x)
A:jax._src.lax.control_flow.solves.(_, out_tangent)->jax.interpreters.ad.jvp(lu.wrap_init(func)).call_wrapped(params + list(x), params_dot + zeros)
A:jax._src.lax.control_flow.solves.kwargs->dict(const_lengths=const_lengths, jaxprs=jaxprs)
A:jax._src.lax.control_flow.solves.(params_dot, b_dot)->_split_linear_solve_args(tangents, const_lengths)
A:jax._src.lax.control_flow.solves.num_x_leaves->len(b_dot)
A:jax._src.lax.control_flow.solves.(x_leaves, _)->split_list(x, [num_x_leaves])
A:jax._src.lax.control_flow.solves.matvec_tangents->_tangent_linear_map(core.jaxpr_as_fun(jaxprs.matvec), params.matvec, params_dot.matvec, *x_leaves)
A:jax._src.lax.control_flow.solves.rhs->_map(ad.add_tangents, b_dot, _map(operator.neg, matvec_tangents))
A:jax._src.lax.control_flow.solves.x_dot->jax.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(params) + rhs, **kwargs)
A:jax._src.lax.control_flow.solves.(dx_leaves, daux_leaves)->split_list(x_dot, [num_x_leaves])
A:jax._src.lax.control_flow.solves.daux_leaves->_map(ad_util.Zero.from_value, daux_leaves)
A:jax._src.lax.control_flow.solves.(x_cotangent, _)->split_list(cotangent, [len(b)])
A:jax._src.lax.control_flow.solves.cotangent_b_full->jax.core.AxisPrimitive('custom_linear_solve').bind(*_flatten(params.transpose()) + x_cotangent, const_lengths=const_lengths.transpose(), jaxprs=jaxprs.transpose())
A:jax._src.lax.control_flow.solves.(cotangent_b, _)->split_list(cotangent_b_full, [len(b)])
A:jax._src.lax.control_flow.solves.(params_dims, b_dims)->_split_linear_solve_args(dims, const_lengths)
A:jax._src.lax.control_flow.solves.(params_bat, orig_b_bat)->_split_linear_solve_args(orig_bat, const_lengths)
A:jax._src.lax.control_flow.solves.(solve_jaxpr_batched, solve_x_bat)->jax.interpreters.batching.batch_jaxpr(solve, axis_size, solve_bat + b_bat, instantiate=x_bat, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.(vecmat_jaxpr_batched, vecmat_x_bat)->jax.interpreters.batching.batch_jaxpr(vecmat, axis_size, vecmat_bat + b_bat, instantiate=x_bat, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.x_bat_out->_map(operator.or_, vecmat_x_bat + [True] * num_aux, solve_x_bat)
A:jax._src.lax.control_flow.solves.(matvec_jaxpr_batched, matvec_b_bat)->jax.interpreters.batching.batch_jaxpr(matvec, axis_size, matvec_bat + x_bat_out, instantiate=b_bat, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.b_bat_out->_map(lambda m, s, o: m or s or o, matvec_b_bat, solve_t_b_bat, orig_b_bat)
A:jax._src.lax.control_flow.solves.(solve_t_jaxpr_batched, solve_t_b_aux_bat)->jax.interpreters.batching.batch_jaxpr(solve_t, axis_size, solve_t_bat + x_bat_out, instantiate=b_bat, axis_name=axis_name, main_type=main_type)
A:jax._src.lax.control_flow.solves.(solve_t_b_bat, _)->split_list(solve_t_b_aux_bat, [len(orig_b_bat)])
A:jax._src.lax.control_flow.solves.batched_jaxprs->_LinearSolveTuple(matvec_jaxpr_batched, vecmat_jaxpr_batched, solve_jaxpr_batched, solve_t_jaxpr_batched)
A:jax._src.lax.control_flow.solves.outs->jax.core.AxisPrimitive('custom_linear_solve').bind(*new_params + new_b, const_lengths=const_lengths, jaxprs=batched_jaxprs)
A:jax._src.lax.control_flow.solves.linear_solve_p->jax.core.AxisPrimitive('custom_linear_solve')
jax._src.lax.control_flow._custom_linear_solve_impl(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False,has_aux=False)
jax._src.lax.control_flow.custom_root(f,initial_guess,solve,tangent_solve,has_aux=False)
jax._src.lax.control_flow.solves._LinearSolveTuple(collections.namedtuple('_LinearSolveTuple','matvec,vecmat,solve,transpose_solve'))
jax._src.lax.control_flow.solves._LinearSolveTuple.transpose(self)
jax._src.lax.control_flow.solves._check_shapes(func_name,expected_name,actual,expected)
jax._src.lax.control_flow.solves._custom_linear_solve_impl(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._custom_linear_solve_jvp(primals,tangents,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._custom_root(const_lengths,jaxprs,*args)
jax._src.lax.control_flow.solves._flatten(args)
jax._src.lax.control_flow.solves._linear_solve_abstract_eval(*args,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._linear_solve_batching_rule(axis_size,axis_name,main_type,args,dims,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._linear_solve_transpose_rule(cotangent,*primals,const_lengths,jaxprs)
jax._src.lax.control_flow.solves._root_jvp(const_lengths,jaxprs,primals,tangents)
jax._src.lax.control_flow.solves._split_linear_solve_args(args,const_lengths)
jax._src.lax.control_flow.solves._split_root_args(args,const_lengths)
jax._src.lax.control_flow.solves._tangent_linear_map(func,params,params_dot,*x)
jax._src.lax.control_flow.solves._transpose_one_output(linear_fun,primals)
jax._src.lax.control_flow.solves.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False,has_aux=False)
jax._src.lax.control_flow.solves.custom_root(f,initial_guess,solve,tangent_solve,has_aux=False)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/_src/lax/control_flow/common.py----------------------------------------
A:jax._src.lax.control_flow.common.param_str->str(param)
A:jax._src.lax.control_flow.common.msg->sep.join([msg, param_str])
A:jax._src.lax.control_flow.common.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax._src.lax.control_flow.common.debug->jax.interpreters.partial_eval.debug_info(fun, in_tree, False, primitive_name or '<unknown>')
A:jax._src.lax.control_flow.common.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(traceable, in_avals)
A:jax._src.lax.control_flow.common.(jaxpr, consts, out_tree)->_initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name)
A:jax._src.lax.control_flow.common.closed_jaxpr->jax.core.ClosedJaxpr(pe.convert_constvars_jaxpr(jaxpr), ())
A:jax._src.lax.control_flow.common.(jaxprs, all_consts, all_out_trees)->unzip3((_initial_style_open_jaxpr(fun, in_tree, in_avals, primitive_name) for fun in funs))
A:jax._src.lax.control_flow.common.newvar->jax.core.gensym(jaxprs, suffix='_')
A:jax._src.lax.control_flow.common.prefix->jax._src.util.concatenate(unused_const_vars[:i])
A:jax._src.lax.control_flow.common.suffix->jax._src.util.concatenate(unused_const_vars[i + 1:])
A:jax._src.lax.control_flow.common.consts->jax._src.util.concatenate(all_consts)
A:jax._src.lax.control_flow.common.diff->tree_map(_show_diff, tree_unflatten(tree1, avals1), tree_unflatten(tree2, avals2))
A:jax._src.lax.control_flow.common.actual_tree_children->tree_structure(actual_tree_children[0]).children()
A:jax._src.lax.control_flow.common.actual_tree->tree_structure(actual_tree_children[0])
jax._src.lax.control_flow._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax._src.lax.control_flow._initial_style_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow._initial_style_jaxprs_with_common_consts(funs:Sequence[Callable],in_tree,in_avals,primitive_name:str)
jax._src.lax.control_flow._initial_style_open_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._abstractify(x)
jax._src.lax.control_flow.common._avals_short(avals)
jax._src.lax.control_flow.common._check_tree(func_name,expected_name,actual_tree,expected_tree,has_aux=False)
jax._src.lax.control_flow.common._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax._src.lax.control_flow.common._initial_style_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._initial_style_jaxprs_with_common_consts(funs:Sequence[Callable],in_tree,in_avals,primitive_name:str)
jax._src.lax.control_flow.common._initial_style_open_jaxpr(fun:Callable,in_tree,in_avals,primitive_name:Optional[str]=None)
jax._src.lax.control_flow.common._make_closed_jaxpr(traceable:lu.WrappedFun,in_avals:Sequence[core.AbstractValue])
jax._src.lax.control_flow.common._prune_zeros(ts)
jax._src.lax.control_flow.common._show_diff(array1,array2)
jax._src.lax.control_flow.common._typecheck_param(prim,param,name,msg_required,pred)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/ops/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/ad.py----------------------------------------
A:jax.interpreters.ad.(fun, aux)->jvp_subtrace_aux(fun)
A:jax.interpreters.ad.trace->JVPTrace(main, core.cur_sublevel())
A:jax.interpreters.ad.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.ans_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.(out_primals, out_tangents)->unzip2(((t.primal, t.tangent) for t in ans_tracers))
A:jax.interpreters.ad.has_aux->kwargs.pop('has_aux', False)
A:jax.interpreters.ad.jvpfun->jvp(traceable)
A:jax.interpreters.ad.(jvpfun, aux)->jvp(traceable, has_aux=True)
A:jax.interpreters.ad.(_, in_tree)->tree_flatten(((primals, primals), {}))
A:jax.interpreters.ad.(jvpfun_flat, out_tree)->flatten_fun(jvpfun, in_tree)
A:jax.interpreters.ad.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)
A:jax.interpreters.ad.(out_primals_pvals, out_tangents_pvals)->tree_unflatten(out_tree(), out_pvals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts)->linearize(traceable, *primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts, aux)->linearize(traceable, *primals, has_aux=True)
A:jax.interpreters.ad.cts->tuple((ct for (ct, pval) in zip(cts, pvals) if not pval.is_known()))
A:jax.interpreters.ad.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax.interpreters.ad.vjp_->Partial(partial(unbound_vjp, pvals, jaxpr), consts)
A:jax.interpreters.ad.axes_to_reduce->tuple((axis_name for axis_name in reduce_axes if axis_name in core.get_aval(ct).named_shape and axis_name not in v.aval.named_shape))
A:jax.interpreters.ad.ct->jax.lax.psum(ct, axis_name=axes_to_reduce)
A:jax.interpreters.ad.a->a.update(shape=tuple(shape)).update(shape=tuple(shape))
A:jax.interpreters.ad.invals->map(read_primal, eqn.invars)
A:jax.interpreters.ad.cts_in->bwd.call_wrapped(*res, *cts_out)
A:jax.interpreters.ad.(cts_in,)->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.params->update_params(params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct])
A:jax.interpreters.ad.call_jaxpr->update_params(params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct]).pop('call_jaxpr')
A:jax.interpreters.ad.cts_out->map(instantiate_zeros_aval, out_avals, cts_out)
A:jax.interpreters.ad.cotangents_out->map(read_cotangent, jaxpr.invars)
A:jax.interpreters.ad.tangent_zero->Zero(get_aval(val).at_least_vspace())
A:jax.interpreters.ad.(primals_in, tangents_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax.interpreters.ad.jvp->primitive_jvps.get(primitive)
A:jax.interpreters.ad.(primal_out, tangent_out)->tree_unflatten(out_tree(), result)
A:jax.interpreters.ad.(primals, tangents)->tree_unflatten(in_tree, primals_and_tangents)
A:jax.interpreters.ad.(args, in_tree)->tree_flatten((primals, tangents))
A:jax.interpreters.ad.f_jvp->jvp_subtrace(f, self.main)
A:jax.interpreters.ad.(f_jvp, which_nz_out)->nonzero_tangent_outputs(f_jvp)
A:jax.interpreters.ad.out_ax->out_axes_thunk()
A:jax.interpreters.ad.(f_jvp, out_tree)->traceable(f_jvp, in_tree)
A:jax.interpreters.ad.update_params->call_transpose_param_updaters.get(primitive)
A:jax.interpreters.ad.result->call_primitive.bind(_update_annotation(f_jvp, f.in_type, which_nz), *args, **new_params)
A:jax.interpreters.ad.(out, treedef)->tree_flatten((primals, tangents))
A:jax.interpreters.ad.primals_in->map(core.full_lower, primals_in)
A:jax.interpreters.ad.tangents_in->map(instantiate_zeros, tangents_in)
A:jax.interpreters.ad.outs->jvp_subtrace(f, self.main).call_wrapped(*it.chain(primals_in, tangents_in))
A:jax.interpreters.ad.(primals_out, tangents_out)->split_list(outs, [len(outs) // 2])
A:jax.interpreters.ad.tangents_out->list(tangents_out)
A:jax.interpreters.ad.res_and_primals_out->fwd.call_wrapped(*map(core.full_lower, primals_in))
A:jax.interpreters.ad.(out_tree, res_tree)->out_trees()
A:jax.interpreters.ad.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax.interpreters.ad.(ps_in, ts_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax.interpreters.ad.(res_ps_in, lin_ps_in)->split_list(ps_in, [params['res_tree'].num_leaves])
A:jax.interpreters.ad.(res_ts_in, lin_ts_in)->split_list(ts_in, [params['res_tree'].num_leaves])
A:jax.interpreters.ad.ps_out->prim.bind(call, *ps_in, **params)
A:jax.interpreters.ad.lin_ts_in->map(instantiate_zeros, lin_ts_in)
A:jax.interpreters.ad.ts_out->prim.bind(call, *res_ps_in, *lin_ts_in, **params)
A:jax.interpreters.ad.primal_aval->raise_to_shaped(get_aval(primal), weak_type=False)
A:jax.interpreters.ad.tangent_aval->raise_to_shaped(get_aval(tangent), weak_type=False)
A:jax.interpreters.ad.expected_tangent_dtype->jax.core.primal_dtype_to_tangent_dtype(primal_aval.dtype)
A:jax.interpreters.ad.primitive_jvps[primitive]->partial(zero_jvp, primitive)
A:jax.interpreters.ad.primitive_transposes[primitive]->partial(linear_transpose2, transpose_rule)
A:jax.interpreters.ad.val_out->primitive.bind(*primals, **params)
A:jax.interpreters.ad.tangents->map(instantiate_zeros, tangents)
A:jax.interpreters.ad.primitive_transposes[prim]->partial(bilinear_transpose, lhs_rule, rhs_rule)
A:jax.interpreters.ad.out->rhs_rule(cotangent, x, **kwargs)
A:jax.interpreters.ad.r->primitive.bind(*primals, **params)
A:jax.interpreters.ad.(out_flat, out_tree)->tree_flatten((primals_out, tangents_out))
A:jax.interpreters.ad.(all_args, in_tree_def)->tree_flatten(((), args, ct))
A:jax.interpreters.ad.fun->jax.linear_util.hashable_partial(lu.wrap_init(backward_pass), call_jaxpr, reduce_axes, False)
A:jax.interpreters.ad.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax.interpreters.ad.(res_invars, _)->partition_list(which_lin, call_jaxpr.invars)
A:jax.interpreters.ad.out_flat->primitive.bind(fun, *all_args, **new_params)
A:jax.interpreters.ad.primitive_transposes[core.call_p]->partial(call_transpose, call_p)
A:jax.interpreters.ad.primitive_transposes[core.named_call_p]->partial(call_transpose, core.named_call_p)
A:jax.interpreters.ad.jaxpr_->jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr_)
A:jax.interpreters.ad.(fun, nz_arg_cts)->nonzero_outputs(fun)
A:jax.interpreters.ad.new_params->update_params(new_params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct])
A:jax.interpreters.ad.f->jax.linear_util.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.ad.(f_jvp, out_nonzeros)->f_jvp_traceable(jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)
A:jax.interpreters.ad.avals_in->list(it.chain(jaxpr.in_avals, tangent_avals))
A:jax.interpreters.ad.(jaxpr_out, avals_out, literals_out)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(f_jvp, avals_in)
A:jax.interpreters.ad.num_primals->len(nonzeros)
A:jax.interpreters.ad.primals->list(primals_and_nztangents[:num_primals])
A:jax.interpreters.ad.nonzero_tangents->iter(primals_and_nztangents[num_primals:])
A:jax.interpreters.ad.new_invars->_perm(primals_in, tangents_in, jaxpr.jaxpr.invars)
A:jax.interpreters.ad.new_outvars->_perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)
A:jax.interpreters.ad.new_jaxpr->jax.core.Jaxpr(jaxpr.jaxpr.constvars, new_invars, new_outvars, jaxpr.jaxpr.eqns, jaxpr.jaxpr.effects)
A:jax.interpreters.ad.n->sum(primal_counts)
A:jax.interpreters.ad.primal_groups->split_list(primals, primal_counts[:-1])
A:jax.interpreters.ad.tangent_groups->split_list(tangents, tangent_counts[:-1])
A:jax.interpreters.ad.(res, _)->split_list(invals, [num_res])
jax.interpreters.ad.CustomJVPException(self)
jax.interpreters.ad.CustomJVPException.__init__(self)
jax.interpreters.ad.CustomVJPException(self)
jax.interpreters.ad.CustomVJPException.__init__(self)
jax.interpreters.ad.JVPTrace(Trace)
jax.interpreters.ad.JVPTrace.join(self,xt,yt)
jax.interpreters.ad.JVPTrace.lift(self,val)
jax.interpreters.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.ad.JVPTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.interpreters.ad.JVPTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.interpreters.ad.JVPTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.ad.JVPTrace.process_custom_jvp_call(self,_,__,f_jvp,tracers)
jax.interpreters.ad.JVPTrace.process_custom_transpose(self,prim,call,tracers,**params)
jax.interpreters.ad.JVPTrace.process_custom_vjp_call(self,_,__,fwd,bwd,tracers,*,out_trees)
jax.interpreters.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.ad.JVPTrace.pure(self,val)
jax.interpreters.ad.JVPTrace.sublift(self,val)
jax.interpreters.ad.JVPTracer(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.__init__(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.aval(self)
jax.interpreters.ad.JVPTracer.full_lower(self)
jax.interpreters.ad.UndefinedPrimal(self,aval)
jax.interpreters.ad.UndefinedPrimal.__init__(self,aval)
jax.interpreters.ad.UndefinedPrimal.__repr__(self)
jax.interpreters.ad._closed_call_transpose(params,jaxpr,args,ct,cts_in_avals,reduce_axes)
jax.interpreters.ad._custom_lin_transpose(cts_out,*invals,num_res,bwd,out_avals)
jax.interpreters.ad._interleave(xs,ys)
jax.interpreters.ad._jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax.interpreters.ad._perm(primal_counts,tangent_counts,lst)
jax.interpreters.ad._primal_tangent_shapes_match(primal,tangent)
jax.interpreters.ad._raise_custom_vjp_error_on_jvp(*_,**__)
jax.interpreters.ad._update_annotation(f:lu.WrappedFun,orig_type:Optional[Tuple[Tuple[core.AbstractValue,bool],...]],nonzeros:List[bool])->lu.WrappedFun
jax.interpreters.ad.add_tangents(x,y)
jax.interpreters.ad.backward_pass(jaxpr:core.Jaxpr,reduce_axes,transform_stack,consts,primals_in,cotangents_in)
jax.interpreters.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax.interpreters.ad.call_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax.interpreters.ad.closed_backward_pass(jaxpr:core.ClosedJaxpr,reduce_axes,transform_stack,primals_in,cotangents_in)
jax.interpreters.ad.defbilinear(prim,lhs_rule,rhs_rule)
jax.interpreters.ad.defjvp(primitive,*jvprules)
jax.interpreters.ad.defjvp2(primitive,*jvprules)
jax.interpreters.ad.defjvp_zero(primitive)
jax.interpreters.ad.deflinear(primitive,transpose_rule)
jax.interpreters.ad.deflinear2(primitive,transpose_rule)
jax.interpreters.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax.interpreters.ad.get_primitive_transpose(p)
jax.interpreters.ad.identity(x)
jax.interpreters.ad.instantiate_zeros(tangent)
jax.interpreters.ad.instantiate_zeros_aval(aval,tangent)
jax.interpreters.ad.is_undefined_primal(x)
jax.interpreters.ad.jvp(fun:lu.WrappedFun,has_aux=False,instantiate=True,transform_stack=True)->Any
jax.interpreters.ad.jvp_jaxpr(jaxpr:core.ClosedJaxpr,nonzeros:Sequence[bool],instantiate:Union[bool,Sequence[bool]])->Tuple[core.ClosedJaxpr, List[bool]]
jax.interpreters.ad.jvp_subtrace(main,primals,tangents)
jax.interpreters.ad.jvp_subtrace_aux(main,primals,tangents)
jax.interpreters.ad.jvpfun(instantiate,transform_stack,primals,tangents)
jax.interpreters.ad.linear_jvp(primitive,primals,tangents,**params)
jax.interpreters.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linear_transpose2(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linearize(traceable,*primals,**kwargs)
jax.interpreters.ad.map_transpose(primitive,params,call_jaxpr,args,ct,_,reduce_axes)
jax.interpreters.ad.nonzero_outputs(*args,**kwargs)
jax.interpreters.ad.nonzero_tangent_outputs(*args,**kwargs)
jax.interpreters.ad.rearrange_binders(jaxpr:core.ClosedJaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax.interpreters.ad.recast_to_float0(primal,tangent)
jax.interpreters.ad.replace_float0s(primal,tangent)
jax.interpreters.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.traceable(in_tree,*primals_and_tangents)
jax.interpreters.ad.unpair_pval(pval)
jax.interpreters.ad.vjp(traceable,primals,has_aux=False,reduce_axes=())
jax.interpreters.ad.zero_jvp(primitive,primals,tangents,**params)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/batching.py----------------------------------------
A:jax.interpreters.batching.handler->make_iota_handlers.get(type(axis_size))
A:jax.interpreters.batching.x_->main.with_cur_sublevel().full_raise(x)
A:jax.interpreters.batching.(spec_type, axis_size_type)->vmappables.pop(data_type)
A:jax.interpreters.batching.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax.interpreters.batching.NotMapped->type(None)
A:jax.interpreters.batching.aval->jax.core.mapped_aval(sz, src, x.aval)
A:jax.interpreters.batching.(axis_size,)->jax.core.dedup_referents((x.shape[d] for (x, d) in zip(vals, dims) if d is not not_mapped))
A:jax.interpreters.batching.(vals_in, dims_in)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.used_names->jax.core.used_axis_names(primitive, params)
A:jax.interpreters.batching.frame->self.get_frame(vals_in, dims_in)
A:jax.interpreters.batching.batcher_primitive->self.get_axis_primitive_batcher(primitive, frame)
A:jax.interpreters.batching.(val_out, dim_out)->batched_primitive(vals_in, dims_in, **params)
A:jax.interpreters.batching.batched_primitive->self.get_primitive_batcher(primitive, frame)
A:jax.interpreters.batching.src->jax._src.source_info_util.current()
A:jax.interpreters.batching.params->dict(params, input_shape=operand.shape)
A:jax.interpreters.batching.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.(f_, dims_out)->batch_subtrace(f, self.main, dims)
A:jax.interpreters.batching.f_->_update_annotation(f_, f.in_type, axis_size, self.axis_name, dims)
A:jax.interpreters.batching.vals_out->map_primitive.bind(f, *vals, **new_params)
A:jax.interpreters.batching.(vals, dims, srcs)->unzip3(((t.val, t.batch_dim, t.source_info) for t in out_tracers))
A:jax.interpreters.batching.trace->main.with_cur_sublevel()
A:jax.interpreters.batching.new_in_axes->tuple((in_axis + 1 if both_mapped(in_axis, d) and d <= in_axis else in_axis for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax.interpreters.batching.new_dims->tuple((d - 1 if both_mapped(in_axis, d) and in_axis < d else d for (d, in_axis) in zip(dims, params['in_axes'])))
A:jax.interpreters.batching.(f, dims_out)->batch_subtrace(f, self.main, new_dims)
A:jax.interpreters.batching.new_params->dict(params, in_axes=new_in_axes, out_axes_thunk=new_out_axes_thunk)
A:jax.interpreters.batching.(in_vals, in_dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.(fun, out_dims1)->batch_subtrace(fun, self.main, in_dims)
A:jax.interpreters.batching.(jvp, out_dims2)->batch_custom_jvp_subtrace(jvp, self.main, in_dims)
A:jax.interpreters.batching.out_vals->map(partial(matchaxis, trace.axis_name, axis_size), out_axes, out_axes_dest, out_vals)
A:jax.interpreters.batching.(fst, out_dims)->jax.linear_util.merge_linear_aux(out_dims1, out_dims2)
A:jax.interpreters.batching.(fwd, out_dims2)->batch_subtrace(fwd, self.main, in_dims)
A:jax.interpreters.batching.bwd->batch_custom_vjp_bwd(bwd, self.axis_name, axis_size, out_dims2, in_dims, self.main.trace_type, self.spmd_axis_name)
A:jax.interpreters.batching.(_, res_tree)->out_trees()
A:jax.interpreters.batching.(res_dims, primal_dims)->split_list(dims, [num_res])
A:jax.interpreters.batching.(_, primal_srcs)->split_list(srcs, [num_res])
A:jax.interpreters.batching.f->_batch_jaxpr_outer(f, axis_name, axis_size, in_axes, main_type)
A:jax.interpreters.batching.idx->memoize(lambda : BatchTracer(trace, make_iota(axis_size), 0, source_info_util.current()))
A:jax.interpreters.batching.in_tracers->map(partial(to_elt, trace, idx), in_vals, in_dims)
A:jax.interpreters.batching.shape->list(np.shape(x))
A:jax.interpreters.batching.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.batching.(out_vals, out_dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(f, out_batched)->_batch_jaxpr_inner(f, axis_size, out_axes_dest)
A:jax.interpreters.batching.(jaxpr_out, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(f, avals_in)
A:jax.interpreters.batching.(out_vals, out_axes)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.zero_if_mapped->ZeroIfMapped()
A:jax.interpreters.batching.(out_primals, out_tangents)->split_list(out_vals, [len(out_vals) // 2])
A:jax.interpreters.batching.(out_primal_bds, out_tangent_bds)->split_list(out_dims, [len(out_vals) // 2])
A:jax.interpreters.batching.out_dims->map(_merge_bdims, out_primal_bds, out_tangent_bds)
A:jax.interpreters.batching.out_primals->map(partial(matchaxis, trace.axis_name, size), out_primal_bds, out_dims, out_primals)
A:jax.interpreters.batching.out_tangents->map(partial(matchaxis, trace.axis_name, size), out_tangent_bds, out_dims, out_tangents)
A:jax.interpreters.batching.(bwd, out_dims_thunk)->batch_subtrace(bwd)
A:jax.interpreters.batching.bwd_->_batch_outer(bwd, axis_name, axis_size, in_dims, main_type, spmd_axis_name)
A:jax.interpreters.batching.primitive_batchers[prim]->partial(reducer_batcher, prim)
A:jax.interpreters.batching.(shape, dim)->next(((x.shape, d) for (x, d) in zip(args, dims) if d is not not_mapped))
A:jax.interpreters.batching.out->prim.bind(*args, **params)
A:jax.interpreters.batching.ndim->max((np.ndim(x) for x in args))
A:jax.interpreters.batching.axes->tuple(np.where(np.less(axes, bdim), axes, np.add(axes, 1)))
A:jax.interpreters.batching.bdim_out->int(list(np.delete(np.arange(operand.ndim), axes)).index(bdim))
A:jax.interpreters.batching.broadcast_dims->tuple(np.delete(np.arange(len(shape)), axis))
A:jax.interpreters.batching._->jax.core.get_aval(x)
A:jax.interpreters.batching.x->moveaxis(x, bdx, bdy)
A:jax.interpreters.batching.y->broadcast(y, x.shape[bdx], bdx)
jax.interpreters.batching.BatchTrace(self,*args,axis_name,spmd_axis_name=None)
jax.interpreters.batching.BatchTrace.__init__(self,*args,axis_name,spmd_axis_name=None)
jax.interpreters.batching.BatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax.interpreters.batching.BatchTrace.get_frame(self,vals,dims)->core.AxisEnvFrame
jax.interpreters.batching.BatchTrace.get_primitive_batcher(self,primitive,frame)
jax.interpreters.batching.BatchTrace.lift(self,val)
jax.interpreters.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.post_process_custom_jvp_call(self,out_tracers,jvp_was_run)
jax.interpreters.batching.BatchTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.interpreters.batching.BatchTrace.post_process_custom_vjp_call_fwd(self,out_tracers,out_trees)
jax.interpreters.batching.BatchTrace.post_process_map(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.batching.BatchTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.interpreters.batching.BatchTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,*,out_trees)
jax.interpreters.batching.BatchTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.batching.BatchTrace.pure(self,val)
jax.interpreters.batching.BatchTrace.sublift(self,val)
jax.interpreters.batching.BatchTracer(self,trace,val,batch_dim:Optional[int],source_info:Optional[source_info_util.SourceInfo]=None)
jax.interpreters.batching.BatchTracer.__init__(self,trace,val,batch_dim:Optional[int],source_info:Optional[source_info_util.SourceInfo]=None)
jax.interpreters.batching.BatchTracer._contents(self)
jax.interpreters.batching.BatchTracer._origin_msg(self)
jax.interpreters.batching.BatchTracer.aval(self)
jax.interpreters.batching.BatchTracer.full_lower(self)
jax.interpreters.batching.ZeroIfMapped
jax.interpreters.batching._batch_inner(axis_size,out_dim_dests,main,in_dims,*in_vals)
jax.interpreters.batching._batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,main_type)
jax.interpreters.batching._batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,main_type)
jax.interpreters.batching._batch_jaxpr_inner(axis_size,out_axes_dest,main,in_axes,*in_vals)
jax.interpreters.batching._batch_jaxpr_outer(axis_name,axis_size,in_dims,main_type,*in_vals)
jax.interpreters.batching._batch_outer(axis_name,axis_size,in_dims,main_type,spmd_axis_name,*in_vals)
jax.interpreters.batching._handle_scalar_broadcasting(nd,x,d)
jax.interpreters.batching._main_trace_for_axis_names(main_trace:core.MainTrace,axis_name:Iterable[core.AxisName])->bool
jax.interpreters.batching._match_axes_and_sum(axis_size,axis_name,out_dims_thunk,out_dim_dests,*in_vals)
jax.interpreters.batching._matchaxis_symbolic_zeros(axis_name,sz,name,src,dst,x,sum_match=False)
jax.interpreters.batching._merge_bdims(x,y)
jax.interpreters.batching._update_annotation(f:lu.WrappedFun,orig_type:Optional[core.InputType],axis_size:core.AxisSize,axis_name:core.AxisName,in_dims:Sequence[Optional[int]])->lu.WrappedFun
jax.interpreters.batching.add_batched(batched_args,batch_dims)
jax.interpreters.batching.batch(fun:lu.WrappedFun,axis_name:core.AxisName,axis_size,in_dims,out_dim_dests,main_type:Type[BatchTrace]=BatchTrace,spmd_axis_name:Optional[Hashable]=None)->lu.WrappedFun
jax.interpreters.batching.batch_custom_jvp_subtrace(main,in_dims,*in_vals)
jax.interpreters.batching.batch_custom_vjp_bwd(bwd,axis_name,axis_size,in_dims,out_dim_dests,main_type,spmd_axis_name)
jax.interpreters.batching.batch_jaxpr(closed_jaxpr,axis_size,in_batched,instantiate,axis_name,main_type)
jax.interpreters.batching.batch_jaxpr_axes(closed_jaxpr,axis_size,in_axes,out_axes_dest,axis_name,main_type)
jax.interpreters.batching.batch_subtrace(main,in_dims,*in_vals)
jax.interpreters.batching.bdim_at_front(x,bdim,size)
jax.interpreters.batching.broadcast(x,sz,axis)
jax.interpreters.batching.broadcast_batcher(prim,args,dims,**params)
jax.interpreters.batching.defbroadcasting(prim)
jax.interpreters.batching.defreducer(prim)
jax.interpreters.batching.defvectorized(prim)
jax.interpreters.batching.flatten_fun_for_vmap(in_tree,*args_flat)
jax.interpreters.batching.from_elt(trace:'BatchTrace',axis_size:AxisSize,x:Elt,spec:MapSpec)->Vmappable
jax.interpreters.batching.is_vmappable(x:Any)->bool
jax.interpreters.batching.make_iota(axis_size:AxisSize)->Array
jax.interpreters.batching.matchaxis(axis_name,sz,src,dst,x,sum_match=False)
jax.interpreters.batching.reducer_batcher(prim,batched_args,batch_dims,axes,**params)
jax.interpreters.batching.register_vmappable(data_type:Type,spec_type:Type,axis_size_type:Type,to_elt:Callable,from_elt:Callable,make_iota:Optional[Callable])
jax.interpreters.batching.to_elt(trace:Trace,get_idx:GetIdx,x:Vmappable,spec:MapSpec)->Elt
jax.interpreters.batching.unregister_vmappable(data_type:Type)->None
jax.interpreters.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax.interpreters.batching.vtile(f_flat:lu.WrappedFun,in_axes_flat:Tuple[Optional[int],...],out_axes_flat:Tuple[Optional[int],...],tile_size:Optional[int],axis_name:core.AxisName,main_type:Type[BatchTrace]=BatchTrace)
jax.interpreters.batching.zeros_like_batched(batched_args,batch_dims)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/xla.py----------------------------------------
A:jax.interpreters.xla._scalar_types->jax._src.dtypes.python_scalar_dtypes.keys()
A:jax.interpreters.xla.source_file->re.sub(config.jax_hlo_source_file_canonicalization_regex, '', source_file)
A:jax.interpreters.xla.frame->jax._src.source_info_util.user_frame(source_info)
A:jax.interpreters.xla.proto->jax._src.lib.xla_client.OpSharding()
A:jax.interpreters.xla.proto.tile_assignment_dimensions->list(sharding)
A:jax.interpreters.xla.proto.tile_assignment_devices->list(range(np.product(sharding)))
A:jax.interpreters.xla.typ->type(x)
A:jax.interpreters.xla.handler->canonicalize_dtype_handlers.get(typ)
A:jax.interpreters.xla.aval_fn->pytype_aval_mappings.get(typ)
A:jax.interpreters.xla.pytype_aval_mappings[t]->operator.attrgetter('aval')
A:jax.interpreters.xla.pytype_aval_mappings[core.DArray]->operator.attrgetter('_aval')
A:jax.interpreters.xla.c->jax._src.lib.xla_client.XlaBuilder(f'primitive_computation_{prim.name}')
A:jax.interpreters.xla.counts->itertools.count()
A:jax.interpreters.xla.ctx->TranslationContext(builder=c, platform=platform, axis_env=axis_env, name_stack=new_name_stack())
A:jax.interpreters.xla.ans->f(ctx.builder, *args, **kw)
A:jax.interpreters.xla.num_elements->len(c.get_shape(ans).tuple_shapes())
A:jax.interpreters.xla.mesh_axes->tuple(unsafe_map(partial(axis_read, axis_env), name))
A:jax.interpreters.xla.(trailing_size, ragged)->divmod(axis_env.nreps, prod(axis_env.sizes))
A:jax.interpreters.xla.iota->numpy.arange(prod(mesh_spec)).reshape(mesh_spec)
A:jax.interpreters.xla.groups->numpy.reshape(np.moveaxis(iota, mesh_axes, np.arange(len(mesh_axes))), (prod(np.take(mesh_spec, mesh_axes)), -1))
A:jax.interpreters.xla.ad.primitive_transposes[xla_call_p]->partial(ad.call_transpose, xla_call_p)
A:jax.interpreters.xla.(donated_known, _)->partition_list(unks_in, params_known['donated_invars'])
A:jax.interpreters.xla.new_params_known->dict(params_known, donated_invars=tuple(donated_known))
A:jax.interpreters.xla.(_, donated_staged_)->partition_list(inst_in, params_staged['donated_invars'])
A:jax.interpreters.xla.new_params_staged->dict(params_staged, donated_invars=tuple(donated_staged))
A:jax.interpreters.xla.pe.partial_eval_jaxpr_custom_rules[xla_call_p]->partial(pe.call_partial_eval_custom_rule, 'call_jaxpr', _xla_call_partial_eval_custom_params_updater)
A:jax.interpreters.xla.pe.padding_rules[xla_call_p]->partial(pe.call_padding_rule, xla_call_p)
A:jax.interpreters.xla.lhs->jax.core.pp_vars(eqn.outvars, context, print_shapes=settings.print_shapes)
A:jax.interpreters.xla._backend_specific_translations->defaultdict(dict)
A:jax.interpreters.xla.wrapped->self._wrap_fn(key, value)
A:jax.interpreters.xla.translations->_TranslationRuleAdapter([_translations], _wrap_old_translation)
A:jax.interpreters.xla.retself[key]->_TranslationRuleAdapter(translation_tables, _wrap_old_translation)
A:jax.interpreters.xla.backend_specific_translations->_BackendSpecificTranslationsAdapter()
jax.interpreters.xla.AxisEnv(NamedTuple)
jax.interpreters.xla.TranslationContext
jax.interpreters.xla.TranslationContext.replace(self,**kw)
jax.interpreters.xla._BackendSpecificTranslationsAdapter(defaultdict)
jax.interpreters.xla._BackendSpecificTranslationsAdapter.__missing__(self,key)
jax.interpreters.xla._TranslationRuleAdapter(self,translations,wrap_fn:Callable[[core.Primitive,Callable],TranslationRule])
jax.interpreters.xla._TranslationRuleAdapter.__init__(self,translations,wrap_fn:Callable[[core.Primitive,Callable],TranslationRule])
jax.interpreters.xla._TranslationRuleAdapter.__setitem__(self,key:core.Primitive,value:Callable)
jax.interpreters.xla._axis_groups(mesh_spec,mesh_axes)
jax.interpreters.xla._canonicalize_ndarray_dtype(x)
jax.interpreters.xla._canonicalize_python_scalar_dtype(typ,x)
jax.interpreters.xla._get_canonical_source_file(frame:source_info_util.Frame)
jax.interpreters.xla._make_abstract_python_scalar(typ,val)
jax.interpreters.xla._make_array_shape(a:ShapedArray)->Sequence[XlaShape]
jax.interpreters.xla._pp_xla_call(eqn:core.JaxprEqn,context:core.JaxprPpContext,settings:core.JaxprPpSettings)->List[pp.Doc]
jax.interpreters.xla._wrap_old_translation(prim:core.Primitive,f:Callable)->TranslationRule
jax.interpreters.xla._xla_call_jvp_update_params(params,nz_tangents)
jax.interpreters.xla._xla_call_partial_eval_custom_params_updater(unks_in:Sequence[bool],inst_in:Sequence[bool],kept_outs_known:Sequence[bool],kept_outs_staged:Sequence[bool],num_res:int,params_known:dict,params_staged:dict)->Tuple[dict, dict]
jax.interpreters.xla._xla_call_partial_eval_update_params(params,kept_inputs,num_new_inputs)
jax.interpreters.xla._xla_call_transpose_update_params(params,undef_primals,nonzero_cts)
jax.interpreters.xla.abstractify(x)->core.AbstractValue
jax.interpreters.xla.aval_to_xla_shapes(aval:core.AbstractValue)->Sequence[XlaShape]
jax.interpreters.xla.axis_groups(axis_env:AxisEnv,name)->Tuple[Tuple[int, ...]]
jax.interpreters.xla.axis_read(axis_env,axis_name)
jax.interpreters.xla.canonicalize_dtype(x)
jax.interpreters.xla.check_backend_matches(inner_backend,outer_backend)
jax.interpreters.xla.dtype_to_primitive_type(dtype:np.dtype)->xc.PrimitiveType
jax.interpreters.xla.extend_axis_env(env:AxisEnv,name,size:int)
jax.interpreters.xla.identity(x)
jax.interpreters.xla.jaxpr_collectives(jaxpr)
jax.interpreters.xla.lower_fun(fun:Callable,*,multiple_results:bool,backend=None,new_style:bool=False)->Callable
jax.interpreters.xla.make_op_metadata(primitive:core.Primitive,params:Dict,*,source_info:source_info_util.SourceInfo,name_stack:Union[str,source_info_util.NameStack]='')->xc.OpMetadata
jax.interpreters.xla.parameter(builder,num,shape,name=None,replicated=None)
jax.interpreters.xla.primitive_subcomputation(platform:str,axis_env:'AxisEnv',prim:core.Primitive,avals_in:Sequence[core.AbstractValue],avals_out:Sequence[core.AbstractValue],**params)
jax.interpreters.xla.register_collective_primitive(prim:core.Primitive)
jax.interpreters.xla.register_initial_style_primitive(prim:core.Primitive)
jax.interpreters.xla.register_translation(prim:core.Primitive,rule:TranslationRule,*,platform:Optional[str]=None)->None
jax.interpreters.xla.sharding_to_proto(sharding:SpatialSharding)
jax.interpreters.xla.tuple_sharding_proto(elems)
jax.interpreters.xla.with_sharding(builder,sharding:SpatialSharding,op_fn,*args,**kwargs)
jax.interpreters.xla.with_sharding_proto(builder,sharding_proto,op_fn,*args,**kwargs)
jax.interpreters.xla.xla_destructure(c,ans)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/pxla.py----------------------------------------
A:jax.interpreters.pxla._UNSHARDED_INSTANCE->NoSharding()
A:jax.interpreters.pxla.mesh_shape->list(named_mesh_shape.values())
A:jax.interpreters.pxla.mesh->_get_logical_mesh_ids(self.mesh_shape)
A:jax.interpreters.pxla.proto->MeshPspecSharding(i.mesh.local_mesh, i.spec)._to_xla_op_sharding(aval.ndim)
A:jax.interpreters.pxla.ty->special_axes.get(axis, xc.OpSharding.Type.REPLICATED)
A:jax.interpreters.pxla.proto_mesh->_get_logical_mesh_ids(self.mesh_shape).transpose(mesh_permutation).reshape(new_mesh_shape)
A:jax.interpreters.pxla.proto.tile_assignment_dimensions->list(proto_mesh.shape)
A:jax.interpreters.pxla.proto.tile_assignment_devices->list(raw_mesh.transpose(tad_perm).reshape(tad_shape).flat)
A:jax.interpreters.pxla.indices->spec_to_indices(aval.shape, sharding_spec)
A:jax.interpreters.pxla.(partitions, num_replicas)->_get_num_ways_dim_sharded(op_sharding)
A:jax.interpreters.pxla.(shard_size, ragged)->divmod(axis_size, total_chunks)
A:jax.interpreters.pxla.device_it->iter(op_sharding.tile_assignment_devices)
A:jax.interpreters.pxla.has_unstacked->any((isinstance(s, Unstacked) for s in self.sharding_spec.sharding))
A:jax.interpreters.pxla.op_sharding_proto->sharding_spec_sharding_proto(self)
A:jax.interpreters.pxla.total_chunks->int(np.prod(sharding.chunks))
A:jax.interpreters.pxla.shard_indices->shard_indices.reshape(shard_indices_shape).reshape(shard_indices_shape)
A:jax.interpreters.pxla.num_sharded_dim->len(shard_indices_shape)
A:jax.interpreters.pxla.replica_sizes->tuple((a.replicas for a in self.mesh_mapping if isinstance(a, Replicated)))
A:jax.interpreters.pxla.ShardingSpec.mesh_shape->property(sharding_spec_mesh_shape)
A:jax.interpreters.pxla.arg->jax.interpreters.xla.canonicalize_dtype(arg)
A:jax.interpreters.pxla.x->numpy.zeros(x.shape, dtype=np.dtype(bool))
A:jax.interpreters.pxla.(start_indices, limit_indices, removed_dims)->unzip3((_as_slice_indices(x, idx) for idx in indices))
A:jax.interpreters.pxla.shards->ShardInfo(sharded_avals, out_sharded_avals, global_sharded_avals, num_local_shards, num_global_shards)
A:jax.interpreters.pxla.limit_indices->list(arr.shape)
A:jax.interpreters.pxla.AUTO->_AUTOAxisResource()
A:jax.interpreters.pxla._UNSPECIFIED->_UnspecifiedValue()
A:jax.interpreters.pxla.reverse_map->defaultdict(list)
A:jax.interpreters.pxla.partitions->tuple((tuple(reverse_map[i]) if reverse_map[i] else None for i in range(max_index + 1)))
A:jax.interpreters.pxla.sharded_aval->i.mesh._global_to_local(cast(ArrayMapping, _get_array_mapping(i.spec)), gaval).update(shape=aval.shape[:sharded_dim] + aval.shape[sharded_dim + 1:])
A:jax.interpreters.pxla.sharding_spec->_pmap_sharding_spec(nrep, axis_size, 1, None, aval, in_axis)
A:jax.interpreters.pxla.seen_index_hashes->set()
A:jax.interpreters.pxla.hashed_index->_hashable_index(index)
A:jax.interpreters.pxla.self._one_replica_buffer_indices->_one_replica_buffer_indices(self.indices)
A:jax.interpreters.pxla.npy_value->numpy.empty(self.aval.shape, self.aval.dtype)
A:jax.interpreters.pxla.npy_value[self.indices[i]]->numpy.asarray(self.device_buffers[i])
A:jax.interpreters.pxla.buf_idx->self.indices.index(cidx)
A:jax.interpreters.pxla.aval->i.mesh._global_to_local(cast(ArrayMapping, _get_array_mapping(i.spec)), gaval)
A:jax.interpreters.pxla.devices->list(devices)
A:jax.interpreters.pxla.db->_set_aval(db)
A:jax.interpreters.pxla.candidates->defaultdict(list)
A:jax.interpreters.pxla.arr_indices->tuple(x.sharding.devices_indices_map(x.shape).values())
A:jax.interpreters.pxla.xla.pytype_aval_mappings[sda]->operator.attrgetter('aval')
A:jax.interpreters.pxla.api_util._shaped_abstractify_handlers[sda]->operator.attrgetter('aval')
A:jax.interpreters.pxla.abstract_args->unsafe_map(xla.abstractify, args)
A:jax.interpreters.pxla.(compiled_fun, fingerprint)->parallel_callable(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *abstract_args)
A:jax.interpreters.pxla.emap_info->EmapInfo(backend, devices)
A:jax.interpreters.pxla.t->self.main.with_cur_sublevel()
A:jax.interpreters.pxla.ans->tiling_transform(fun, mesh, [_get_array_mapping(i.spec) for i in in_shardings], [_get_array_mapping(o.spec) for o in out_shardings]).call_wrapped(*in_tracers)
A:jax.interpreters.pxla.out_tracers->map(t.full_raise, ans)
A:jax.interpreters.pxla.(outvals, out_axes_src)->unzip2(((t.val, t.shard_axes) for t in out_tracers))
A:jax.interpreters.pxla.out_axes->out_axes_thunk()
A:jax.interpreters.pxla.out->jax.pmap(lambda _, x: x, in_axes=(0, out_axis_src.get(axis_name)), out_axes=out_axis, devices=None if devices is None else list(devices), backend=backend, donate_argnums=donate_argnums_)(np.arange(axis_size), outval)
A:jax.interpreters.pxla.in_axes->tuple((arg_axis[i] for arg_axis in all_axes))
A:jax.interpreters.pxla.f->jax.pmap(f, in_axes=in_axes, axis_name=name, out_axes=0, backend=info.backend, devices=None if info.devices is None else list(info.devices))
A:jax.interpreters.pxla.(vals, shard_axes)->unzip2([(t.val, t.shard_axes) for t in tracers])
A:jax.interpreters.pxla.(f_mapped, out_shard_axes)->_multi_pmap(partial(primitive.bind, **params), info, names, all_axes)
A:jax.interpreters.pxla.outvals->f_mapped(*vals)
A:jax.interpreters.pxla.fake_primitive->types.SimpleNamespace(multiple_results=False, bind=lambda _: jax.lax.axis_index(frame.name))
A:jax.interpreters.pxla.in_tracers->map(partial(MapTracer, t), vals, shard_axes)
A:jax.interpreters.pxla.(out, outaxes)->unzip2((_match_annot(axis_name, axis_size, v, s, dst) for (v, s, dst) in zip(out, outaxes, out_axes_thunk())))
A:jax.interpreters.pxla.range->jax.lax.iota(np.int32, frame.size)
A:jax.interpreters.pxla.dummy_tracer->MapTracer(self, range, {frame.name: 0})
A:jax.interpreters.pxla.mapped_axes_->set(mapped_axes)
A:jax.interpreters.pxla.shard_axis_out->_moveaxis(np.ndim(val), shard_axis_src, src, dst)
A:jax.interpreters.pxla.src->_moveaxis(np.ndim(val), shard_axis_src, src, dst).pop(axis_name, None)
A:jax.interpreters.pxla.dst->_annot_to_flat(np.ndim(val) + (src is None), shard_axis_out.values(), dst_annotation)
A:jax.interpreters.pxla.outval->jax.interpreters.batching.broadcast(val, axis_size, dst)
A:jax.interpreters.pxla.name->lst.pop(src)
A:jax.interpreters.pxla.shard_axes->dict(self.shard_axes)
A:jax.interpreters.pxla.pmap_computation->lower_parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, avals)
A:jax.interpreters.pxla.pmap_executable->lower_parallel_callable(fun, backend_name, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, avals).compile()
A:jax.interpreters.pxla.jaxpr_replicas->jax._src.dispatch.jaxpr_replicas(jaxpr)
A:jax.interpreters.pxla.sharded_avals->tuple((shard_aval(pci.axis_size, axis, aval) if axis is not None else aval for (axis, aval) in safe_zip(pci.in_axes, pci.avals)))
A:jax.interpreters.pxla.(jaxpr, out_sharded_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_final(fun, global_sharded_avals, pe.debug_info_final(fun, 'pmap'))
A:jax.interpreters.pxla.jaxpr->jax._src.dispatch.apply_outfeed_rewriter(jaxpr)
A:jax.interpreters.pxla.replicas->find_replicas(jaxpr, pci.axis_size, pci.global_axis_size)
A:jax.interpreters.pxla.parts->find_partitions(jaxpr)
A:jax.interpreters.pxla.backend->jax._src.lib.xla_bridge.get_device_backend(mesh.devices.flat[0])
A:jax.interpreters.pxla.global_axis_size->len(devices)
A:jax.interpreters.pxla.pci->ParallelCallableInfo(name, backend, axis_name, axis_size, global_axis_size, devices, in_axes, out_axes_thunk, avals)
A:jax.interpreters.pxla.(jaxpr, consts, replicas, parts, shards)->stage_parallel_callable(pci, fun, global_arg_shapes)
A:jax.interpreters.pxla.axis_env->jax.interpreters.xla.AxisEnv(nreps=mesh.size, names=tuple(global_axis_sizes.keys()), sizes=tuple(global_axis_sizes.values()))
A:jax.interpreters.pxla.name_stack->new_name_stack(wrap_name(fun_name, api_name))
A:jax.interpreters.pxla.closed_jaxpr->jax.core.ClosedJaxpr(jaxpr, consts)
A:jax.interpreters.pxla.tuple_args->jax._src.dispatch.should_tuple_args(len(in_jaxpr_avals), backend.platform)
A:jax.interpreters.pxla.lowering_result->jax.interpreters.mlir.lower_jaxpr_to_module(module_name, closed_jaxpr, unordered_effects, ordered_effects, backend, backend.platform, axis_ctx, name_stack, donated_invars, replicated_args=replicated_args, arg_shardings=in_partitions, result_shardings=out_partitions)
A:jax.interpreters.pxla.module_str->xe.mlir.xla_computation_to_mlir_module(self._hlo)
A:jax.interpreters.pxla.self._executable->MeshExecutable.from_hlo(self._name, self._hlo, **self.compile_args, _allow_propagation_to_outputs=_allow_propagation_to_outputs, _allow_compile_replicated=_allow_compile_replicated)
A:jax.interpreters.pxla.local_devices_str->', '.join(map(str, pci.local_devices))
A:jax.interpreters.pxla.device_assignment->numpy.array(devices).reshape((replicas.num_global_replicas, parts.num_partitions))
A:jax.interpreters.pxla.compile_options->jax._src.lib.xla_bridge.get_compile_options(num_replicas=num_replicas, num_partitions=num_partitions, device_assignment=xla_device_assignment, use_spmd_partitioning=spmd_lowering, use_auto_spmd_partitioning=auto_spmd_lowering)
A:jax.interpreters.pxla.process_index->jax._src.lib.xla_bridge.process_index()
A:jax.interpreters.pxla.local_device_assignment->numpy.array([d for d in device_assignment.flat if d.process_index == process_index])
A:jax.interpreters.pxla.in_shardings->tuple((s for (i, s) in enumerate(in_shardings) if i in kept_var_idx))
A:jax.interpreters.pxla.nouts->len(shards.out_sharded_avals)
A:jax.interpreters.pxla.out_shardings->_out_shardings_for_trivial(jaxpr, consts, in_shardings, device_assignment)
A:jax.interpreters.pxla.handle_outs->global_avals_to_results_handler(global_out_avals, out_shardings, committed, [False] * len(global_out_avals))
A:jax.interpreters.pxla.execute_fun->ExecuteReplicated(compiled, 'parallel computation', pci.backend, handle_args, handle_outs, unordered_effects, ordered_effects, keepalive, bool(host_callbacks), set(range(len(input_indices))))
A:jax.interpreters.pxla.compiled->jax._src.dispatch.compile_or_get_cached(pci.backend, xla_computation, compile_options, host_callbacks)
A:jax.interpreters.pxla.handle_args->InputsHandler(xla_executable.local_devices(), in_shardings, input_indices, InputsHandlerMode.pjit_or_xmap)
A:jax.interpreters.pxla.fingerprint->getattr(compiled, 'fingerprint', None)
A:jax.interpreters.pxla.arg_avals->map(xla.abstractify, kept_args)
A:jax.interpreters.pxla.used_collectives->set(xla.jaxpr_collectives(jaxpr))
A:jax.interpreters.pxla.num_partitions->reconcile_num_partitions(eqn.params['call_jaxpr'], eqn.params['nparts'])
A:jax.interpreters.pxla.(arg_parts, out_parts, num_partitions, local_arg_parts, local_out_parts, local_num_partitions)->_find_partitions(jaxpr)
A:jax.interpreters.pxla.inner_num_parts->_inner_partitions(jaxpr, outer_num_parts)
A:jax.interpreters.pxla.nparts->get_num_partitions(parts)
A:jax.interpreters.pxla.expected_num_parts->_inner_partitions(subjaxpr, expected_num_parts)
A:jax.interpreters.pxla.(result, ragged)->divmod(x, y)
A:jax.interpreters.pxla.self.handler->partial(shard_args, local_devices, input_indices, mode)
A:jax.interpreters.pxla.replicated_aval->i.mesh._global_to_local(cast(ArrayMapping, _get_array_mapping(i.spec)), gaval).update(shape=(axis_size,) + aval.shape)
A:jax.interpreters.pxla.device_buffers->device_put(val, devices, replicate=True)
A:jax.interpreters.pxla.(replication_factor, ragged)->divmod(nrep, axis_size)
A:jax.interpreters.pxla.pspec->partitioned_sharding_spec(npart, parts, sharded_aval)
A:jax.interpreters.pxla.sharded_in_axis->sum((not isinstance(s, NoSharding) for s in pspec.sharding[:map_axis]))
A:jax.interpreters.pxla.self.has_unordered_effects->bool(unordered_effects)
A:jax.interpreters.pxla.self._local_devices->self.xla_executable.local_devices()
A:jax.interpreters.pxla.(out_bufs, sharded_token)->self.xla_executable.execute_sharded_on_local_devices_with_tokens(input_bufs)
A:jax.interpreters.pxla.(token_bufs, out_bufs)->jax._src.util.split_list(out_bufs, [num_output_tokens])
A:jax.interpreters.pxla.out_bufs->self.xla_executable.execute_sharded_on_local_devices(input_bufs)
A:jax.interpreters.pxla.input_bufs->self.in_handler(args)
A:jax.interpreters.pxla.bufs->cast(xc.ShardedBuffer, bufs).get_device_buffers()
A:jax.interpreters.pxla.xla_pmap_p->jax.core.MapPrimitive('xla_pmap')
A:jax.interpreters.pxla.(donated_invars_known, _)->partition_list(unks_in, params_known['donated_invars'])
A:jax.interpreters.pxla.(in_axes_known, _)->partition_list(unks_in, params_known['in_axes'])
A:jax.interpreters.pxla.(_, out_axes_known)->partition_list(kept_outs_known, params_known['out_axes'])
A:jax.interpreters.pxla.new_params_known->dict(params_known, in_axes=tuple(in_axes_known), out_axes=tuple(out_axes_known), donated_invars=tuple(donated_invars_known))
A:jax.interpreters.pxla.(_, donated_invars_staged)->partition_list(inst_in, params_staged['donated_invars'])
A:jax.interpreters.pxla.(_, in_axes_staged)->partition_list(inst_in, params_staged['in_axes'])
A:jax.interpreters.pxla.(_, out_axes_staged)->partition_list(kept_outs_staged, params_staged['out_axes'])
A:jax.interpreters.pxla.new_params_staged->dict(params_staged, in_axes=tuple(in_axes_staged), out_axes=tuple(out_axes_staged), donated_invars=tuple(donated_invars_staged))
A:jax.interpreters.pxla.(new_jaxpr, used_inputs)->jax.interpreters.partial_eval.dce_jaxpr(eqn.params['call_jaxpr'], used_outputs)
A:jax.interpreters.pxla.(_, donated_invars)->partition_list(used_inputs, eqn.params['donated_invars'])
A:jax.interpreters.pxla.(_, in_axes)->partition_list(used_inputs, eqn.params['in_axes'])
A:jax.interpreters.pxla.(_, out_axes)->partition_list(used_outputs, eqn.params['out_axes'])
A:jax.interpreters.pxla.new_params->dict(eqn.params, call_jaxpr=new_jaxpr, donated_invars=tuple(donated_invars), in_axes=tuple(in_axes), out_axes=tuple(out_axes))
A:jax.interpreters.pxla.new_eqn->jax.interpreters.partial_eval.new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax.interpreters.pxla.pe.partial_eval_jaxpr_custom_rules[xla_pmap_p]->partial(pe.call_partial_eval_custom_rule, 'call_jaxpr', _pmap_partial_eval_custom_params_updater, res_aval=_pmap_partial_eval_custom_res_maker)
A:jax.interpreters.pxla.ad.primitive_transposes[xla_pmap_p]->partial(ad.map_transpose, xla_pmap_p)
A:jax.interpreters.pxla.div->jax.interpreters.mlir.ir_constant(np.array(axis_env.nreps // util.prod(axis_env.sizes), np.uint32))
A:jax.interpreters.pxla.mod->jax.interpreters.mlir.ir_constant(np.array(axis_env.sizes[-1], np.uint32))
A:jax.interpreters.pxla.dims->list(aval.shape)
A:jax.interpreters.pxla.zero->jax.interpreters.mlir.ir_constant(np.zeros((), dtype=np.uint32))
A:jax.interpreters.pxla.dims_unsqueezed->list(aval.shape).copy()
A:jax.interpreters.pxla.padded_aval->i.mesh._global_to_local(cast(ArrayMapping, _get_array_mapping(i.spec)), gaval).update(shape=[axis_env.sizes[-1]] + dims)
A:jax.interpreters.pxla.padded->jax.interpreters.mlir.full_like_aval(0, padded_aval)
A:jax.interpreters.pxla.replica_groups->jax.interpreters.mlir.dense_int_elements(xla.axis_groups(axis_env, axis_env.names[-1]))
A:jax.interpreters.pxla.perm->list(range(1, len(dims)))
A:jax.interpreters.pxla.transposed_dims->list(dims)
A:jax.interpreters.pxla.float_zero->jax.interpreters.mlir.full_like_aval(0, padded_aval)
A:jax.interpreters.pxla.new_env->_ThreadLocalOldEnv().stack[-1].with_mesh(self)
A:jax.interpreters.pxla.sub_ctx->ctx.module_context.replace(axis_context=mlir.ReplicaAxisContext(new_env), name_stack=xla.extend_name_stack(ctx.module_context.name_stack, util.wrap_name(name, 'pmap')))
A:jax.interpreters.pxla.(sharded_outs, _)->jax.interpreters.mlir.jaxpr_subcomp(sub_ctx, call_jaxpr, mlir.TokenSet(), (), *in_nodes_sharded)
A:jax.interpreters.pxla.self.devices->list(devices).copy()
A:jax.interpreters.pxla.self.axis_names->tuple(axis_names)
A:jax.interpreters.pxla.self._hash->hash((self.axis_names, tuple(self.devices.flat), self.devices.shape))
A:jax.interpreters.pxla.is_local_device->numpy.vectorize(lambda d: d.process_index == process_index, otypes=[bool])(self.devices)
A:jax.interpreters.pxla.other_axes->tuple_delete(tuple(range(self.devices.ndim)), axis)
A:jax.interpreters.pxla.local_slices->numpy.vectorize(lambda d: d.process_index == process_index, otypes=[bool])(self.devices).any(other_axes, keepdims=False)
A:jax.interpreters.pxla.nonzero_indices->numpy.flatnonzero(local_slices)
A:jax.interpreters.pxla.subcube_indices->tuple(subcube_indices)
A:jax.interpreters.pxla.EMPTY_ENV->ResourceEnv(Mesh(np.empty((), dtype=object), ()), ())
A:jax.interpreters.pxla.thread_resources->_ThreadResourcesLocalState()
A:jax.interpreters.pxla._old_env->_ThreadLocalOldEnv()
A:jax.interpreters.pxla.shape->list(aval.shape)
A:jax.interpreters.pxla.named_shape->dict(aval.named_shape)
A:jax.interpreters.pxla.fun->tiling_transform(fun, mesh, [_get_array_mapping(i.spec) for i in in_shardings], [_get_array_mapping(o.spec) for o in out_shardings])
A:jax.interpreters.pxla.full_to_shard_p->jax.core.Primitive('full_to_shard')
A:jax.interpreters.pxla.manual_axes->list(sorted(manual_axes_set, key=str))
A:jax.interpreters.pxla.replicated_axes->list((axis for axis in mesh.axis_names if axis not in manual_axes_set))
A:jax.interpreters.pxla.raw_mesh->numpy.arange(np.prod(mesh_shape)).reshape(mesh_shape)
A:jax.interpreters.pxla.sharding_proto->mesh_sharding_specs(mesh.shape, mesh.axis_names)(aval_out, axes).sharding_proto()
A:jax.interpreters.pxla.sx->jax.interpreters.mlir.wrap_with_sharding_op(x, manual_proto, unspecified_dims=unspecified_dims)
A:jax.interpreters.pxla.manual_proto->_manual_proto(aval_in, manual_axes, mesh)
A:jax.interpreters.pxla.(result_type,)->jax.interpreters.mlir.aval_to_ir_types(aval_out)
A:jax.interpreters.pxla.shard_to_full_p->jax.core.Primitive('shard_to_full')
A:jax.interpreters.pxla._UNCONSTRAINED_PARTITION->_UnconstrainedPartitionSingleton()
A:jax.interpreters.pxla.first_device_assignment->list(i._device_assignment)
A:jax.interpreters.pxla.arr_device_assignment->list(i._device_assignment)
A:jax.interpreters.pxla.(jaxpr, global_out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_final(fun, global_in_avals, debug_info=pe.debug_info_final(fun, api_name))
A:jax.interpreters.pxla.jaxpr_sharding->list(dispatch.jaxpr_shardings(jaxpr))
A:jax.interpreters.pxla.(backend, device_assignment)->_get_and_check_device_assignment(it.chain(in_shardings, out_shardings, jaxpr_sharding), devices_from_context)
A:jax.interpreters.pxla.committed->bool(devices_from_context or len(device_assignment) > 1 or any((not _is_unspecified(i) for i in in_shardings)) or jaxpr_sharding or any((not _is_unspecified(o) for o in out_shardings)))
A:jax.interpreters.pxla.kept_var_idx->set(range(len(global_in_avals)))
A:jax.interpreters.pxla.(jaxpr, kept_const_idx, kept_var_idx)->jax._src.dispatch._prune_unused_inputs(jaxpr)
A:jax.interpreters.pxla.global_in_avals->tuple((a for (i, a) in enumerate(global_in_avals) if i in kept_var_idx))
A:jax.interpreters.pxla.in_is_global->tuple((g for (i, g) in enumerate(in_is_global) if i in kept_var_idx))
A:jax.interpreters.pxla.donated_invars->tuple((x for (i, x) in enumerate(donated_invars) if i in kept_var_idx))
A:jax.interpreters.pxla.has_outfeed->jax.core.jaxpr_uses_outfeed(jaxpr)
A:jax.interpreters.pxla.nreps->jax._src.dispatch.jaxpr_replicas(jaxpr)
A:jax.interpreters.pxla.axis_ctx->jax.interpreters.mlir.ReplicaAxisContext(axis_env)
A:jax.interpreters.pxla.auto_spmd_lowering->_check_if_any_auto(in_shardings + out_shardings)
A:jax.interpreters.pxla.(jaxpr, out_jaxpr_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_final(fun, in_jaxpr_avals)
A:jax.interpreters.pxla.sharding->MeshPspecSharding(i.mesh.local_mesh, i.spec)
A:jax.interpreters.pxla.index->tuple(sharding.addressable_devices_indices_map(aval.shape).values())
A:jax.interpreters.pxla.(in_op_shardings, out_op_shardings)->jax.experimental.pjit._get_op_sharding_from_executable(xla_executable)
A:jax.interpreters.pxla.(in_pspec, out_pspec)->jax.experimental.pjit._get_pspec_from_executable(xla_executable, mesh)
A:jax.interpreters.pxla.dev->numpy.array(device_assignment)
A:jax.interpreters.pxla.xla_device_assignment->numpy.array(device_assignment).reshape((num_replicas, num_partitions))
A:jax.interpreters.pxla.compile_options.executable_build_options.auto_spmd_partitioning_mesh_shape->list(mesh.shape.values())
A:jax.interpreters.pxla.compile_options.executable_build_options.auto_spmd_partitioning_mesh_ids->_get_logical_mesh_ids(list(mesh.shape.values())).reshape(-1)
A:jax.interpreters.pxla.(in_shardings, input_indices, input_avals)->_get_input_metadata(global_in_avals, in_shardings, in_is_global)
A:jax.interpreters.pxla.unsafe_call->partial(_execute_trivial, jaxpr, consts, handle_ins, handle_outs, kept_var_idx)
A:jax.interpreters.pxla.xla_executable->jax._src.dispatch.compile_or_get_cached(backend, computation, compile_options, host_callbacks)
A:jax.interpreters.pxla.(in_shardings_xla, out_shardings_xla)->_get_mesh_pspec_shardings_from_executable(xla_executable, mesh)
A:jax.interpreters.pxla.(out_shardings, are_out_shardings_from_xla)->unzip2(out_shardings_tuple)
A:jax.interpreters.pxla.(_, out_shardings_xla)->_get_op_sharding_shardings_from_executable(xla_executable, device_assignment, len(global_in_avals), len(global_out_avals))
A:jax.interpreters.pxla.has_unordered_effects->bool(unordered_effects)
A:jax.interpreters.pxla.buffer_counts->jax._src.dispatch.get_buffer_counts(global_out_avals, ordered_effects, has_unordered_effects)
A:jax.interpreters.pxla.(_, indices, _)->_get_input_metadata(global_out_avals, out_shardings, are_global)
A:jax.interpreters.pxla.handle_ins->InputsHandler(local_device_assignment, out_shardings, indices, InputsHandlerMode.pjit_or_xmap)
A:jax.interpreters.pxla.rep->MeshPspecSharding(i.mesh.local_mesh, i.spec).OpShardingSharding(device_assignment, sharding._get_replicated_op_sharding())
A:jax.interpreters.pxla.(parsed_pspec, _, _, _)->_prepare_axis_resources(pspec, 'pspec to array_mapping')
A:jax.interpreters.pxla.(axis, other_axis)->sorted([str(axis), str(other_axis)])
A:jax.interpreters.pxla.typing_rule->custom_resource_typing_rules.get(eqn.primitive, None)
A:jax.interpreters.pxla.chunked->Chunked([])
A:jax.interpreters.pxla.sharding[axis]->Chunked(list(chunked.chunks) + [axis_sizes[name]])
A:jax.interpreters.pxla.mesh_mapping[mesh_axis_pos[name]]->ShardedAxis(next_sharded_axis)
A:jax.interpreters.pxla.aval_shape->list(aval.shape)
A:jax.interpreters.pxla.self.dynamic_axis_env->DynamicAxisEnv()
A:jax.interpreters.pxla._thread_local_state->_ThreadLocalState()
A:jax.interpreters.pxla.val.aval->jax.core.ShapedArray(val.shape, val.dtype)
A:jax.interpreters.pxla.original_func->_original_func(f)
jax.interpreters.pxla.DynamicAxisEnv(list)
jax.interpreters.pxla.DynamicAxisEnv.__contains__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.__getitem__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.nreps(self)
jax.interpreters.pxla.DynamicAxisEnv.sizes(self)
jax.interpreters.pxla.DynamicAxisEnvFrame(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.DynamicAxisEnvFrame.__init__(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.EmapInfo(NamedTuple)
jax.interpreters.pxla.ExecuteReplicated(self,xla_executable,name,backend,in_handler:InputsHandler,out_handler:ResultsHandler,unordered_effects:List[core.Effect],ordered_effects:List[core.Effect],keepalive:Any,has_host_callbacks:bool,kept_var_idx:Set[int])
jax.interpreters.pxla.ExecuteReplicated.__init__(self,xla_executable,name,backend,in_handler:InputsHandler,out_handler:ResultsHandler,unordered_effects:List[core.Effect],ordered_effects:List[core.Effect],keepalive:Any,has_host_callbacks:bool,kept_var_idx:Set[int])
jax.interpreters.pxla.ExecuteReplicated._call_with_tokens(self,input_bufs)
jax.interpreters.pxla.InputsHandler(self,local_devices,in_shardings,input_indices,mode)
jax.interpreters.pxla.InputsHandler.__init__(self,local_devices,in_shardings,input_indices,mode)
jax.interpreters.pxla.InputsHandler.__str__(self)
jax.interpreters.pxla.InputsHandlerMode(enum.Enum)
jax.interpreters.pxla.MapTrace(self,*args,emap_info)
jax.interpreters.pxla.MapTrace.__init__(self,*args,emap_info)
jax.interpreters.pxla.MapTrace.process_axis_index(self,frame)
jax.interpreters.pxla.MapTrace.process_call(self,call_primitive,fun,tracers,params)
jax.interpreters.pxla.MapTrace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.interpreters.pxla.MapTrace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.interpreters.pxla.MapTrace.process_map(self,call_primitive,fun,tracers,params)
jax.interpreters.pxla.MapTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.pxla.MapTrace.pure(self,val)
jax.interpreters.pxla.MapTrace.sublift(self,tracer)
jax.interpreters.pxla.MapTracer(self,trace:MapTrace,val,shard_axes:Dict[core.AxisName,int])
jax.interpreters.pxla.MapTracer.__init__(self,trace:MapTrace,val,shard_axes:Dict[core.AxisName,int])
jax.interpreters.pxla.MapTracer.__str__(self)
jax.interpreters.pxla.MapTracer.aval(self)
jax.interpreters.pxla.MapTracer.full_lower(self)
jax.interpreters.pxla.Mesh(self,devices:Union[np.ndarray,Sequence[xc.Device]],axis_names:Sequence[MeshAxisName])
jax.interpreters.pxla.Mesh.__enter__(self)
jax.interpreters.pxla.Mesh.__eq__(self,other)
jax.interpreters.pxla.Mesh.__exit__(self,exc_type,exc_value,traceback)
jax.interpreters.pxla.Mesh.__hash__(self)
jax.interpreters.pxla.Mesh.__init__(self,devices:Union[np.ndarray,Sequence[xc.Device]],axis_names:Sequence[MeshAxisName])
jax.interpreters.pxla.Mesh.__repr__(self)
jax.interpreters.pxla.Mesh.__setattr__(self,name,value)
jax.interpreters.pxla.Mesh._global_to_local(self,axes:ArrayMapping,aval)
jax.interpreters.pxla.Mesh._local_mesh(self,process_index)
jax.interpreters.pxla.Mesh._local_to_global(self,axes:ArrayMapping,aval)
jax.interpreters.pxla.Mesh.device_ids(self)
jax.interpreters.pxla.Mesh.empty(self)
jax.interpreters.pxla.Mesh.is_multi_process(self)
jax.interpreters.pxla.Mesh.local_devices(self)
jax.interpreters.pxla.Mesh.local_mesh(self)
jax.interpreters.pxla.Mesh.shape(self)
jax.interpreters.pxla.Mesh.size(self)
jax.interpreters.pxla.MeshComputation(self,name:str,hlo:Union[ir.Module,xc.XlaComputation],is_trivial:bool,donated_invars:Sequence[bool],**compile_args)
jax.interpreters.pxla.MeshComputation.__init__(self,name:str,hlo:Union[ir.Module,xc.XlaComputation],is_trivial:bool,donated_invars:Sequence[bool],**compile_args)
jax.interpreters.pxla.MeshComputation.compile(self,_allow_propagation_to_outputs:bool=False,_allow_compile_replicated:bool=True)->MeshExecutable
jax.interpreters.pxla.MeshComputation.hlo(self)->xc.XlaComputation
jax.interpreters.pxla.MeshComputation.mhlo(self)->ir.Module
jax.interpreters.pxla.MeshExecutable(self,xla_executable,unsafe_call,in_avals,in_shardings,out_shardings,auto_spmd_lowering,kept_var_idx)
jax.interpreters.pxla.MeshExecutable.__init__(self,xla_executable,unsafe_call,in_avals,in_shardings,out_shardings,auto_spmd_lowering,kept_var_idx)
jax.interpreters.pxla.MeshExecutable.call(self,*args)
jax.interpreters.pxla.MeshExecutable.from_hlo(name:str,computation:Union[ir.Module,xc.XlaComputation],mesh:Optional[Mesh],global_in_avals:Sequence[ShapedArray],global_out_avals:Sequence[ShapedArray],in_shardings:Sequence[Union[XLACompatibleSharding,_AUTOAxisResource]],out_shardings:Sequence[Union[XLACompatibleSharding,_AUTOAxisResource,_UnspecifiedValue]],spmd_lowering:bool,tuple_args:bool,in_is_global:Sequence[bool],auto_spmd_lowering:bool,_allow_propagation_to_outputs:bool,_allow_compile_replicated:bool,unordered_effects:List[core.Effect],ordered_effects:List[core.Effect],host_callbacks:List[Any],keepalive:Any,kept_var_idx:Set[int],backend:xb.XlaBackend,device_assignment:Sequence[xc.Device],committed:bool,pmap_nreps:int=1)->MeshExecutable
jax.interpreters.pxla.MeshExecutable.from_trivial_jaxpr(jaxpr,consts,global_in_avals,global_out_avals,in_shardings,device_assignment,committed,kept_var_idx,keepalive)->MeshExecutable
jax.interpreters.pxla.MeshExecutable.input_shardings(self)
jax.interpreters.pxla.MeshExecutable.output_shardings(self)
jax.interpreters.pxla.MeshExecutable.xla_extension_executable(self)
jax.interpreters.pxla.OutputType(enum.Enum)
jax.interpreters.pxla.ParallelCallableInfo
jax.interpreters.pxla.ParallelCallableInfo.local_devices(self)
jax.interpreters.pxla.ParallelCallableInfo.out_axes(self)
jax.interpreters.pxla.PartitionInfo(NamedTuple)
jax.interpreters.pxla.PartitionSpec(self,*partitions)
jax.interpreters.pxla.PartitionSpec.__init__(self,*partitions)
jax.interpreters.pxla.PartitionSpec.__reduce__(self)
jax.interpreters.pxla.PartitionSpec.__repr__(self)
jax.interpreters.pxla.PmapComputation(self,hlo:Union[ir.Module,xc.XlaComputation],**compile_args)
jax.interpreters.pxla.PmapComputation.__init__(self,hlo:Union[ir.Module,xc.XlaComputation],**compile_args)
jax.interpreters.pxla.PmapComputation.compile(self)->PmapExecutable
jax.interpreters.pxla.PmapComputation.hlo(self)->xc.XlaComputation
jax.interpreters.pxla.PmapComputation.mhlo(self)->ir.Module
jax.interpreters.pxla.PmapExecutable(self,xla_executable,unsafe_call,fingerprint,in_avals)
jax.interpreters.pxla.PmapExecutable.__init__(self,xla_executable,unsafe_call,fingerprint,in_avals)
jax.interpreters.pxla.PmapExecutable.call(self,*args)
jax.interpreters.pxla.PmapExecutable.from_hlo(xla_computation,pci:ParallelCallableInfo,replicas:ReplicaInfo,parts:PartitionInfo,shards:ShardInfo,tuple_args:bool,unordered_effects:List[core.Effect],ordered_effects:List[core.Effect],host_callbacks:List[Any],keepalive:Any)
jax.interpreters.pxla.PmapExecutable.xla_extension_executable(self)
jax.interpreters.pxla.ReplicaInfo(NamedTuple)
jax.interpreters.pxla.ResourceEnv(NamedTuple)
jax.interpreters.pxla.ResourceEnv.__repr__(self)
jax.interpreters.pxla.ResourceEnv.local_shape(self)
jax.interpreters.pxla.ResourceEnv.loop_resource_axes(self)->Set[ResourceAxisName]
jax.interpreters.pxla.ResourceEnv.physical_resource_axes(self)->Set[ResourceAxisName]
jax.interpreters.pxla.ResourceEnv.resource_axes(self)->Set[ResourceAxisName]
jax.interpreters.pxla.ResourceEnv.shape(self)
jax.interpreters.pxla.ResourceEnv.with_extra_loop(self,loop:_Loop)
jax.interpreters.pxla.ResourceEnv.with_mesh(self,mesh:Mesh)
jax.interpreters.pxla.ResultsHandler(self,handlers,out_shardings,out_avals)
jax.interpreters.pxla.ResultsHandler.__init__(self,handlers,out_shardings,out_avals)
jax.interpreters.pxla.SPMDBatchTrace(batching.BatchTrace)
jax.interpreters.pxla.SPMDBatchTrace.get_axis_primitive_batcher(self,primitive,frame)
jax.interpreters.pxla.ShardInfo(NamedTuple)
jax.interpreters.pxla.TileManual
jax.interpreters.pxla.TileVectorize
jax.interpreters.pxla.WeakRefList(list)
jax.interpreters.pxla._AUTOAxisResource
jax.interpreters.pxla._Loop(NamedTuple)
jax.interpreters.pxla._ShardedDeviceArray(self,aval:ShapedArray,sharding_spec:ShardingSpec,device_buffers:List[xb.xla_client.Buffer],indices:Optional[Tuple[Index,...]]=None)
jax.interpreters.pxla._ShardedDeviceArray.__init__(self,aval:ShapedArray,sharding_spec:ShardingSpec,device_buffers:List[xb.xla_client.Buffer],indices:Optional[Tuple[Index,...]]=None)
jax.interpreters.pxla._ShardedDeviceArray.delete(self)
jax.interpreters.pxla._ShardedDeviceArray.dtype(self)
jax.interpreters.pxla._ShardedDeviceArray.ndim(self)
jax.interpreters.pxla._ShardedDeviceArray.shape(self)
jax.interpreters.pxla._ShardedDeviceArray.size(self)
jax.interpreters.pxla._ThreadLocalOldEnv(self)
jax.interpreters.pxla._ThreadLocalOldEnv.__init__(self)
jax.interpreters.pxla._ThreadLocalState(self)
jax.interpreters.pxla._ThreadLocalState.__init__(self)
jax.interpreters.pxla._ThreadResourcesLocalState(self)
jax.interpreters.pxla._ThreadResourcesLocalState.__init__(self)
jax.interpreters.pxla._UnconstrainedPartitionSingleton
jax.interpreters.pxla._UnconstrainedPartitionSingleton.__str__(self)
jax.interpreters.pxla._UnspecifiedValue
jax.interpreters.pxla._annot_to_flat(ndim:int,mapped_axes:Iterable[int],annotation:Optional[int])->Optional[int]
jax.interpreters.pxla._as_slice_indices(arr:device_array.DeviceArrayProtocol,idx:Index)->Tuple[Tuple[int, ...], Tuple[int, ...], Tuple[int, ...]]
jax.interpreters.pxla._check_gda_or_array_xla_sharding_match(args,in_xla_shardings)
jax.interpreters.pxla._check_if_any_auto(shardings:Iterable[Union[XLACompatibleSharding,_AUTOAxisResource,_UnspecifiedValue]])->bool
jax.interpreters.pxla._create_mesh_pspec_sharding(mesh,pspec,parsed_pspec=None)
jax.interpreters.pxla._create_pmap_sharding_spec(aval,sharded_dim=0,sharded_dim_size=None)
jax.interpreters.pxla._emap_impl(fun:lu.WrappedFun,*args,backend:Optional[str],axis_name:core.AxisName,axis_size:int,global_axis_size:Optional[int],devices:Optional[Sequence[Any]],name:str,in_axes:Sequence[Optional[int]],out_axes_thunk:Callable[[],Sequence[Optional[int]]],donated_invars:Sequence[bool],global_arg_shapes:Sequence[Optional[Tuple[int,...]]])
jax.interpreters.pxla._execute_trivial(jaxpr,consts,in_handler,out_handler,kept_var_idx,*args)
jax.interpreters.pxla._find_partitions(jaxpr)
jax.interpreters.pxla._full_to_shard_abstract_eval(x,axes,mesh,**_)
jax.interpreters.pxla._full_to_shard_lowering(ctx,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:FrozenSet[MeshAxisName])
jax.interpreters.pxla._get_and_check_device_assignment(shardings:Iterable[XLACompatibleSharding],devices:Optional[Sequence[xc.Device]])->Tuple[xla.Backend, Sequence[xc.Device]]
jax.interpreters.pxla._get_array_mapping(pspec:PartitionSpec)->ArrayMappingOrAutoOrUnspecified
jax.interpreters.pxla._get_input_metadata(global_in_avals:Sequence[ShapedArray],in_shardings:Sequence[XLACompatibleSharding],in_is_global:Sequence[bool])->Tuple[Sequence[XLACompatibleSharding], Sequence[Tuple[Optional[Index], ...]], Sequence[ShapedArray]]
jax.interpreters.pxla._get_logical_mesh_ids(mesh_shape)
jax.interpreters.pxla._get_mesh_pspec_shardings_from_executable(xla_executable,mesh)
jax.interpreters.pxla._get_num_ways_dim_sharded(op_sharding:xc.OpSharding)->Tuple[Sequence[int], int]
jax.interpreters.pxla._get_op_sharding_shardings_from_executable(xla_executable,device_assignment,num_in_avals,num_out_avals)
jax.interpreters.pxla._get_pmap_sharding(devices,specs)
jax.interpreters.pxla._get_sharding_specs(shardings:Sequence[XLACompatibleSharding],avals:Sequence[ShapedArray])->Sequence[ShardingSpec]
jax.interpreters.pxla._hashable_index(idx)
jax.interpreters.pxla._inner_partitions(jaxpr,expected_num_parts:Optional[int])
jax.interpreters.pxla._is_auto(x)
jax.interpreters.pxla._is_unspecified(x)
jax.interpreters.pxla._make_sharding_spec(axis_sizes,mesh_axis_pos,num_dimensions,aval_axes)
jax.interpreters.pxla._manual_proto(aval:core.ShapedArray,manual_axes_set:FrozenSet[MeshAxisName],mesh:Mesh)
jax.interpreters.pxla._map_schedule(idx:Tuple[Optional[int],...])->List[Optional[int]]
jax.interpreters.pxla._match_annot(axis_name:core.AxisName,axis_size:int,val:Any,shard_axis_src:Dict[core.AxisName,int],dst_annotation:Optional[int])->Tuple[Any, Dict[core.AxisName, int]]
jax.interpreters.pxla._mhlo_shard(aval,axis_env,xs,in_axis)
jax.interpreters.pxla._mhlo_unshard(aval,axis_env,out_axis,xs,platform)
jax.interpreters.pxla._moveaxis(ndim:int,shard_axes:Dict[core.AxisName,int],src:int,dst:int)->Dict[core.AxisName, int]
jax.interpreters.pxla._multi_pmap(f:Callable,info:EmapInfo,names:List[core.AxisName],all_axes:List[Tuple[Optional[int],...]])->Tuple[Callable, Dict[core.AxisName, int]]
jax.interpreters.pxla._one_replica_buffer_indices(indices:Tuple[Index,...])
jax.interpreters.pxla._op_sharding_to_numpy_indices(op_sharding:xc.OpSharding,shape:Tuple[int,...],num_devices:int)->np.ndarray
jax.interpreters.pxla._original_func(f)
jax.interpreters.pxla._out_shardings_for_trivial(jaxpr:core.Jaxpr,consts:Sequence[Any],in_shardings:Sequence[XLACompatibleSharding],device_assignment:Sequence[xc.Device])->List[XLACompatibleSharding]
jax.interpreters.pxla._pmap_dce_rule(used_outputs,eqn)
jax.interpreters.pxla._pmap_lowering(ctx,*in_nodes,axis_name,axis_size,global_axis_size,devices,name,call_jaxpr,backend=None,in_axes,out_axes,donated_invars,global_arg_shapes)
jax.interpreters.pxla._pmap_partial_eval_custom_params_updater(unks_in,inst_in,kept_outs_known,kept_outs_staged,num_res,params_known,params_staged)
jax.interpreters.pxla._pmap_partial_eval_custom_res_maker(params_known,aval)
jax.interpreters.pxla._pmap_sharding_spec(nrep,axis_size,npart,parts,sharded_aval,map_axis:Optional[int])->ShardingSpec
jax.interpreters.pxla._register_handlers_for_sharded_device_array(sda)
jax.interpreters.pxla._safe_div(x,y)
jax.interpreters.pxla._sanitize_mesh_jaxpr(jaxpr)
jax.interpreters.pxla._sda__getitem__(self,idx)
jax.interpreters.pxla._sda__iter__(self)
jax.interpreters.pxla._sda__reversed__(self)
jax.interpreters.pxla._sda_addressable_shards(self)
jax.interpreters.pxla._sda_block_until_ready(self)
jax.interpreters.pxla._sda_check_if_deleted(self)
jax.interpreters.pxla._sda_copy_to_host_async(self)
jax.interpreters.pxla._sda_one_replica_buffer_indices(self)
jax.interpreters.pxla._sda_sharding(self)
jax.interpreters.pxla._sda_value(self)
jax.interpreters.pxla._set_aval(val)
jax.interpreters.pxla._shard_abstract_array(size,axis:int,x)
jax.interpreters.pxla._shard_arg(arg,devices,arg_indices,mode)
jax.interpreters.pxla._shard_array(x,devices,indices,mode)
jax.interpreters.pxla._shard_device_array(x,devices,indices,mode)
jax.interpreters.pxla._shard_sharded_device_array_slow_path(x,devices,indices,mode)
jax.interpreters.pxla._shard_to_full_abstract_eval(x,axes,mesh,**_)
jax.interpreters.pxla._shard_to_full_lowering(ctx,x,*,axes:ArrayMapping,mesh:Mesh,manual_axes:FrozenSet[MeshAxisName])
jax.interpreters.pxla._shard_token(x,devices,indices,mode)
jax.interpreters.pxla._sharded_device_array_mlir_constant_handler(val,canonicalize_types=True)
jax.interpreters.pxla._shardings_to_mlir_shardings(shardings:Optional[Sequence[PartitionsOrReplicated]])->Optional[Sequence[Optional[xc.OpSharding]]]
jax.interpreters.pxla._unravel_index_mhlo(axis_env)
jax.interpreters.pxla.are_op_shardings_equal(op1,op2)->bool
jax.interpreters.pxla.array_mapping_to_axis_resources(array_mapping:ArrayMapping)
jax.interpreters.pxla.check_multihost_collective_allowlist(jaxpr)
jax.interpreters.pxla.device_put(x,devices:Sequence[xb.xla_client.Device],replicate:bool=False)->List[xb.xla_client.Buffer]
jax.interpreters.pxla.find_partitions(jaxpr)->PartitionInfo
jax.interpreters.pxla.find_replicas(jaxpr,axis_size,global_axis_size)
jax.interpreters.pxla.get_global_aval(local_aval,global_parts:PartitionsOrReplicated,local_parts:PartitionsOrReplicated)
jax.interpreters.pxla.get_local_aval(global_aval,global_parts:PartitionsOrReplicated,local_parts:PartitionsOrReplicated)
jax.interpreters.pxla.get_num_partitions(*partitions)
jax.interpreters.pxla.global_aval_to_result_handler(aval:core.AbstractValue,out_sharding,committed:bool,is_out_sharding_from_xla:bool)->Callable[[Sequence[xb.xla_client.Buffer]], Any]
jax.interpreters.pxla.global_avals_to_results_handler(global_out_avals:Sequence[ShapedArray],shardings:Sequence[XLACompatibleSharding],committed:bool,are_out_shardings_from_xla:Sequence[bool])->ResultsHandler
jax.interpreters.pxla.identity(x)
jax.interpreters.pxla.is_op_sharding_replicated(op:xc.OpSharding)->bool
jax.interpreters.pxla.local_aval_to_result_handler(aval:core.AbstractValue,sharding:XLACompatibleSharding,indices:Optional[Tuple[Index,...]])->Callable[[List[xb.xla_client.Buffer]], Any]
jax.interpreters.pxla.local_avals_to_results_handler(unmapped_local_out_avals:Sequence[Optional[ShapedArray]],local_shardings:Sequence[XLACompatibleSharding])->ResultsHandler
jax.interpreters.pxla.lower_mesh_computation(fun:lu.WrappedFun,api_name:str,fun_name:str,mesh:Mesh,in_shardings:Sequence[Union[MeshPspecSharding,_AUTOAxisResource]],out_shardings:Sequence[Union[MeshPspecSharding,_AUTOAxisResource,_UnspecifiedValue]],donated_invars:Sequence[bool],spmd_lowering:bool,global_in_avals:Sequence[core.ShapedArray],tiling_method:Optional[TilingMethod],in_is_global:Sequence[bool])
jax.interpreters.pxla.lower_parallel_callable(fun:lu.WrappedFun,backend_name:Optional[str],axis_name:core.AxisName,axis_size:int,global_axis_size:Optional[int],devices:Optional[Sequence[xla.Device]],name:str,in_axes:Iterable[Optional[int]],out_axes_thunk:Callable[[],Sequence[Optional[int]]],donated_invars:Sequence[bool],global_arg_shapes:Sequence[Optional[Tuple[int,...]]],avals:Sequence[core.AbstractValue])
jax.interpreters.pxla.lower_sharding_computation(fun:lu.WrappedFun,api_name:str,fun_name:str,in_shardings:Sequence[Union[XLACompatibleSharding,_UnspecifiedValue]],out_shardings:Union[Sequence[Union[XLACompatibleSharding,_UnspecifiedValue]],_UnspecifiedValue],donated_invars:Sequence[bool],global_in_avals:Sequence[core.ShapedArray],in_is_global:Sequence[bool],keep_unused:bool,always_lower:bool,devices_from_context:Optional[Sequence[xc.Device]]=None)
jax.interpreters.pxla.make_sharded_device_array(aval:ShapedArray,sharding_spec:Optional[ShardingSpec],device_buffers:List[Union[Any,xb.xla_client.Buffer]],indices:Optional[Tuple[Index,...]]=None)
jax.interpreters.pxla.maybe_extend_axis_env(*args,**kwargs)
jax.interpreters.pxla.mesh_sharding_specs(axis_sizes,axis_names,allow_uneven_axes=False)
jax.interpreters.pxla.new_mesh_sharding_specs(axis_sizes,axis_names)
jax.interpreters.pxla.op_sharding_to_indices(op_sharding:xc.OpSharding,shape:Tuple[int,...],num_devices:int)->Tuple[Tuple[slice, ...], ...]
jax.interpreters.pxla.parallel_callable(fun:lu.WrappedFun,backend_name:Optional[str],axis_name:core.AxisName,axis_size:int,global_axis_size:Optional[int],devices:Optional[Sequence[Any]],name:str,in_axes:Sequence[Optional[int]],out_axes_thunk:Callable[[],Sequence[Optional[int]]],donated_invars:Sequence[bool],global_arg_shapes:Sequence[Optional[Tuple[int,...]]],*avals)
jax.interpreters.pxla.partitioned_sharding_spec(num_partitions:int,partitions:Optional[Sequence[int]],aval)->ShardingSpec
jax.interpreters.pxla.reconcile_num_partitions(jaxpr,outer_num_parts:Optional[int])
jax.interpreters.pxla.replicate(val,axis_size,nrep,devices=None,backend=None,in_axis=0)
jax.interpreters.pxla.resource_typecheck(jaxpr,resource_env,axis_resources,what_jaxpr_thunk)
jax.interpreters.pxla.sda_array_result_handler(aval:ShapedArray,sharding,indices)
jax.interpreters.pxla.shard_args(devices:Sequence[xb.xla_client.Device],indices:Sequence[Sequence[Index]],mode:InputsHandlerMode,args)->Sequence[Union[xb.ShardedBuffer, Sequence[xb.xla_client.Buffer]]]
jax.interpreters.pxla.shard_aval(size,axis:int,aval)
jax.interpreters.pxla.sharding_spec_indices(self,shape:Tuple[int,...])->np.ndarray
jax.interpreters.pxla.sharding_spec_mesh_shape(self)
jax.interpreters.pxla.sharding_spec_repr(self)
jax.interpreters.pxla.sharding_spec_sharding_proto(self,special_axes:Mapping[int,OpShardingType]={})
jax.interpreters.pxla.show_axes(axes)
jax.interpreters.pxla.spec_to_indices(shape:Tuple[int,...],spec:ShardingSpec)->Tuple[Index, ...]
jax.interpreters.pxla.stage_parallel_callable(pci:ParallelCallableInfo,fun:lu.WrappedFun,global_arg_shapes:Sequence[Optional[Tuple[int,...]]])
jax.interpreters.pxla.tile_aval_nd(axis_sizes,in_axes:ArrayMapping,aval)
jax.interpreters.pxla.untile_aval_nd(axis_sizes,out_axes:ArrayMapping,aval)
jax.interpreters.pxla.use_cpp_class(cpp_cls)
jax.interpreters.pxla.use_cpp_method(f)
jax.interpreters.pxla.vtile_by_mesh(fun:lu.WrappedFun,mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping])
jax.interpreters.pxla.vtile_manual(manual_axes:FrozenSet[MeshAxisName],mesh:Mesh,in_axes:Sequence[ArrayMapping],out_axes:Sequence[ArrayMapping],*args)
jax.interpreters.pxla.xla_pmap_impl(fun:lu.WrappedFun,*args,backend:Optional[str],axis_name:core.AxisName,axis_size:int,global_axis_size:Optional[int],devices:Optional[Sequence[Any]],name:str,in_axes:Sequence[Optional[int]],out_axes_thunk:Callable[[],Sequence[Optional[int]]],donated_invars:Sequence[bool],global_arg_shapes:Sequence[Optional[Tuple[int,...]]])


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/partial_eval.py----------------------------------------
A:jax.interpreters.partial_eval.expl_names_->set(expl_names)
A:jax.interpreters.partial_eval.known->self.pval.get_known()
A:jax.interpreters.partial_eval.aval->get_aval(x)
A:jax.interpreters.partial_eval.const->self._new_const(aval, t).pval.get_known()
A:jax.interpreters.partial_eval.tracers->map(self.instantiate_const_abstracted, tracers)
A:jax.interpreters.partial_eval.(out_aval, effects)->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.name_stack->self._current_truncated_name_stack()
A:jax.interpreters.partial_eval.source->jax._src.source_info_util.current().replace(name_stack=name_stack)
A:jax.interpreters.partial_eval.eqn->new_jaxpr_eqn([*constvars, *invars], outvars, prim, dict(call_jaxpr=closed_call_jaxpr, transpose_jaxpr_thunk=transpose_jaxpr_thunk, out_types=out_types, res_tree=res_tree, lin_tree=lin_tree, out_tree=out_tree), closed_call_jaxpr.effects, source_info_util.current())
A:jax.interpreters.partial_eval.out_tracer->JaxprTracer(self, PartialVal.unknown(out_aval), None)
A:jax.interpreters.partial_eval.out_tracer.recipe->new_eqn_recipe(tracers, [out_tracer], primitive, params, effects, source)
A:jax.interpreters.partial_eval.rule->dce_rules.get(eqn.primitive, _default_dce_rule)
A:jax.interpreters.partial_eval.(in_knowns, in_avals, in_consts)->partition_pvals([t.pval for t in tracers])
A:jax.interpreters.partial_eval.f_->trace_to_subjaxpr_nounits_fwd(f, self.main, False)
A:jax.interpreters.partial_eval.(f_, aux)->trace_to_subjaxpr_nounits_dyn(f, self.main, tuple(in_knowns), f.in_type, False)
A:jax.interpreters.partial_eval.f->jax.linear_util.annotate(f, tuple(((raise_to_shaped(t.aval), True) for t in explicit_tracers)))
A:jax.interpreters.partial_eval.const_params->dict(const_params, in_axes=tuple(const_in_axes), out_axes_thunk=const_out_axes_thunk)
A:jax.interpreters.partial_eval.out->primitive.bind(f, *in_consts, **const_params)
A:jax.interpreters.partial_eval.(fwds, out_knowns, out_type, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.(out_consts, non_fwd_res_)->split_list(out, [sum(out_knowns)])
A:jax.interpreters.partial_eval.cin_consts_full[idx]->next(in_consts_)
A:jax.interpreters.partial_eval.non_fwd_res->iter(non_fwd_res_)
A:jax.interpreters.partial_eval.sentinel->object()
A:jax.interpreters.partial_eval.res_tracers->map(self.new_instantiated_const, res)
A:jax.interpreters.partial_eval.env_tracers->map(self.full_raise, env)
A:jax.interpreters.partial_eval.staged_params->dict(staged_params, in_axes=staged_in_axes, out_axes=tuple(staged_out_axes), call_jaxpr=call_jaxpr)
A:jax.interpreters.partial_eval.(unk_in_axes, const_in_axes)->partition_list(in_knowns, params['in_axes'])
A:jax.interpreters.partial_eval.(f, aux)->partial_eval_wrapper_nounits(f, tuple(in_knowns), tuple(in_avals))
A:jax.interpreters.partial_eval.(out_knowns, _, jaxpr, _)->aux()
A:jax.interpreters.partial_eval.(_, out_axes)->partition_list(out_knowns, out_axes_thunk())
A:jax.interpreters.partial_eval.(out_knowns, out_avals_mapped, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.(out_consts, res)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.call_jaxpr->convert_constvars_jaxpr(jaxpr)
A:jax.interpreters.partial_eval.out_axes->params['out_axes_thunk']()
A:jax.interpreters.partial_eval.(staged_out_axes, _)->partition_list(out_knowns, out_axes)
A:jax.interpreters.partial_eval.const_tracers->map(trace.new_instantiated_const, res)
A:jax.interpreters.partial_eval.(jaxpr, res, env)->tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.interpreters.partial_eval.(out_knowns, out_avals, out_consts)->partition_pvals(out_pvals)
A:jax.interpreters.partial_eval.trace->main.with_cur_sublevel()
A:jax.interpreters.partial_eval.new_params->dict(params, call_jaxpr=padded_jaxpr)
A:jax.interpreters.partial_eval.(out_knowns, out_avals_mapped, out_consts)->partition_pvals(out_pvals)
A:jax.interpreters.partial_eval.staged_out_axes->tuple(out_axes_unknown)
A:jax.interpreters.partial_eval.(out_axes_unknown, out_axes_known)->partition_list(out_knowns, out_axes)
A:jax.interpreters.partial_eval.(res_ts, lin_ts)->split_list(tracers, [params['res_tree'].num_leaves])
A:jax.interpreters.partial_eval.lin_all_known->all((t.is_known() for t in lin_ts))
A:jax.interpreters.partial_eval.in_tracers->map(trace.new_arg, pvals)
A:jax.interpreters.partial_eval.(in_knowns, in_avals, ())->partition_pvals([t.pval for t in tracers])
A:jax.interpreters.partial_eval.out_flat->trace_to_subjaxpr_nounits(fwd, self.main, True).call_wrapped()
A:jax.interpreters.partial_eval.(out_knowns, out_avals, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.closed_jaxpr->jax.core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())
A:jax.interpreters.partial_eval.fwd_->trace_to_subjaxpr_nounits(fwd, self.main, True)
A:jax.interpreters.partial_eval.(fwd_, aux)->partial_eval_wrapper_nounits(fwd_, tuple(in_knowns), tuple(in_avals))
A:jax.interpreters.partial_eval.(_, res)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.converted_jaxpr->Jaxpr(constvars=jaxpr.constvars + env_vars, invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns, effects=jaxpr.effects)
A:jax.interpreters.partial_eval.(in_avals, which_explicit)->unzip2(in_type)
A:jax.interpreters.partial_eval.constval->next(in_consts_iter)
A:jax.interpreters.partial_eval.in_consts_full[d.val]->JaxprTracer(trace, PartialVal.unknown(in_avals[d.val]), ConstVar(constval.shape[i]))
A:jax.interpreters.partial_eval.in_consts_full[idx]->JaxprTracer(trace, PartialVal.unknown(aval), ConstVar(constval))
A:jax.interpreters.partial_eval.in_knowns_iter->iter(in_knowns)
A:jax.interpreters.partial_eval.tracer->self._new_const(aval, t)
A:jax.interpreters.partial_eval.in_args->merge_lists(in_knowns, in_tracers, in_consts)
A:jax.interpreters.partial_eval.out_tracers->map(partial(instantiate_const_at, trace), instantiate, out_tracers)
A:jax.interpreters.partial_eval.res->tuple([c for (c, fwd) in zip(res, fwds) if fwd is None])
A:jax.interpreters.partial_eval.jaxpr->Jaxpr(constvars, self.invars, expl_outvars, self.eqns, self.effects)
A:jax.interpreters.partial_eval.(_, avals_out, _)->trace_to_jaxpr_dynamic(lu.wrap_init(fun, params), avals, debug_info)
A:jax.interpreters.partial_eval.current_name_stack->jax._src.source_info_util.current_name_stack()
A:jax.interpreters.partial_eval.fun->inspect.unwrap(fun)
A:jax.interpreters.partial_eval.(jaxpr, (out_pvals, consts, env))->inspect.unwrap(fun).call_wrapped(pvals)
A:jax.interpreters.partial_eval.(jaxpr, out_consts, env)->tracers_to_jaxpr(in_tracers, out_tracers_)
A:jax.interpreters.partial_eval.FreeVar->namedtuple('FreeVar', ['val'])
A:jax.interpreters.partial_eval.ConstVar->namedtuple('ConstVar', ['val'])
A:jax.interpreters.partial_eval.LambdaBinding->namedtuple('LambdaBinding', [])
A:jax.interpreters.partial_eval.gensym->jax.core.gensym()
A:jax.interpreters.partial_eval.var->self.frame.tracer_to_var.get(id(tracer))
A:jax.interpreters.partial_eval.var_->t_to_var.setdefault(id(t), var)
A:jax.interpreters.partial_eval.processed_eqn_ids->set()
A:jax.interpreters.partial_eval.in_atoms->map(get_atom, r.in_tracers)
A:jax.interpreters.partial_eval.varconstid_to_var[id(r.val)]->newvar(t)
A:jax.interpreters.partial_eval.(env_vars, env_vals)->unzip2(env.items())
A:jax.interpreters.partial_eval.(const_vars, const_vals)->unzip2(consts.items())
A:jax.interpreters.partial_eval.effects->jax.core.join_effects(*(eqn.effects for eqn in eqns))
A:jax.interpreters.partial_eval.lifted_jaxpr->Jaxpr(constvars=tuple(constvars), invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns, effects=jaxpr.effects)
A:jax.interpreters.partial_eval.(constvars, invars)->split_list(jaxpr.invars, [n])
A:jax.interpreters.partial_eval.(env_vars, invars)->split_list(jaxpr.invars, [num_env_vars])
A:jax.interpreters.partial_eval.known_vals_in->iter(known_vals_in)
A:jax.interpreters.partial_eval.(jaxpr_unknown_, out_pvals, residuals)->trace_to_jaxpr_nounits(f, in_pvals, instantiate=instantiate)
A:jax.interpreters.partial_eval.jaxpr_unknown->convert_constvars_jaxpr(jaxpr_unknown_)
A:jax.interpreters.partial_eval.(jaxpr_known, _, consts_known)->trace_to_jaxpr_dynamic(lu.wrap_init(fun), known_avals)
A:jax.interpreters.partial_eval.closed_jaxpr_known->ClosedJaxpr(jaxpr_known, consts_known)
A:jax.interpreters.partial_eval.closed_jaxpr_unknown->ClosedJaxpr(jaxpr_unknown, ())
A:jax.interpreters.partial_eval.(unks_in, inst_in)->unzip2(map(read, eqn.invars))
A:jax.interpreters.partial_eval.(eqn1, eqn2, unks_out, inst_out, res)->rule(saveable, unks_in, inst_in, eqn)
A:jax.interpreters.partial_eval.inputs->map(ensure_instantiated, inst_in, eqn.invars)
A:jax.interpreters.partial_eval.(out_unknowns, out_inst)->unzip2(map(read, jaxpr.outvars))
A:jax.interpreters.partial_eval.out_unknowns->map(op.or_, out_unknowns, ensure_out_unknowns)
A:jax.interpreters.partial_eval.out_inst->map(op.or_, out_inst, ensure_out_inst)
A:jax.interpreters.partial_eval.(ins_known, _)->partition_list(unks_in, eqn.invars)
A:jax.interpreters.partial_eval.(outs_known, _)->partition_list(out_unknowns, jaxpr.outvars)
A:jax.interpreters.partial_eval.known_effects->jax.core.join_effects(*(eqn.effects for eqn in known_eqns))
A:jax.interpreters.partial_eval.jaxpr_known->Jaxpr(jaxpr.constvars, ins_known, [*outs_known, *residuals], known_eqns, known_effects)
A:jax.interpreters.partial_eval.(_, ins_staged)->partition_list(inst_in, eqn.invars)
A:jax.interpreters.partial_eval.(_, outs_staged)->partition_list(out_inst, jaxpr.outvars)
A:jax.interpreters.partial_eval.staged_effects->jax.core.join_effects(*(eqn.effects for eqn in staged_eqns))
A:jax.interpreters.partial_eval.jaxpr_staged->Jaxpr(jaxpr.constvars, [*residuals, *ins_staged], outs_staged, staged_eqns, staged_effects)
A:jax.interpreters.partial_eval.(jaxpr_known, jaxpr_staged, unks_out, inst_out, num_res)->partial_eval_jaxpr_custom(jaxpr, unks_in, inst_in, False, False, saveable)
A:jax.interpreters.partial_eval.(out_binders_known, _)->partition_list(unks_out, eqn.outvars)
A:jax.interpreters.partial_eval.(_, out_binders_staged)->partition_list(inst_out, eqn.outvars)
A:jax.interpreters.partial_eval.newvar->jax.core.gensym([jaxpr_known, jaxpr_staged])
A:jax.interpreters.partial_eval.(params_known, params_staged)->params_updater(unks_in, inst_in, map(op.not_, unks_out), inst_out, num_res, params_known, params_staged)
A:jax.interpreters.partial_eval.eqn_known->new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals], eqn.primitive, params_known, jaxpr_known.effects, eqn.source_info)
A:jax.interpreters.partial_eval.eqn_staged->new_jaxpr_eqn([*residuals, *ins_staged], out_binders_staged, eqn.primitive, params_staged, jaxpr_staged.effects, eqn.source_info)
A:jax.interpreters.partial_eval.partial_eval_jaxpr_custom_rules[core.call_p]->partial(call_partial_eval_custom_rule, 'call_jaxpr', lambda _, __, ___, ____, _____, x, y: (x, y))
A:jax.interpreters.partial_eval.partial_eval_jaxpr_custom_rules[core.closed_call_p]->partial(closed_call_partial_eval_custom_rule, 'call_jaxpr')
A:jax.interpreters.partial_eval.partial_eval_jaxpr_custom_rules[core.named_call_p]->partial(call_partial_eval_custom_rule, 'call_jaxpr', lambda _, __, ___, ____, _____, x, y: (x, y))
A:jax.interpreters.partial_eval.(fwd_vars, _)->forwarding_rules[eqn.primitive](eqn)
A:jax.interpreters.partial_eval.jaxpr_->convert_constvars_jaxpr(jaxpr)
A:jax.interpreters.partial_eval.(new_jaxpr_, used_inputs_)->dce_jaxpr(jaxpr_, used_outputs)
A:jax.interpreters.partial_eval.(used_consts, used_inputs)->split_list(used_inputs_, [len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.new_jaxpr->Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns, jaxpr.effects)
A:jax.interpreters.partial_eval.used_outs->map(read, eqn.outvars)
A:jax.interpreters.partial_eval.(used_ins, new_eqn)->rule(used_outs, eqn)
A:jax.interpreters.partial_eval.used_inputs->map(op.or_, instantiate, used_inputs)
A:jax.interpreters.partial_eval.(new_jaxpr, used_inputs)->dce_jaxpr(jaxpr, used_outputs)
A:jax.interpreters.partial_eval.update_params->call_param_updaters.get(map_primitive)
A:jax.interpreters.partial_eval.new_eqn->new_jaxpr_eqn([v for (v, used) in zip(eqn.invars, used_inputs) if used], [v for (v, used) in zip(eqn.outvars, used_outputs) if used], eqn.primitive, new_params, new_jaxpr.effects, eqn.source_info)
A:jax.interpreters.partial_eval.new_invars->_move_to_front(closed_jaxpr.jaxpr.invars, to_move)
A:jax.interpreters.partial_eval.new_closed_jaxpr->jax.core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)
A:jax.interpreters.partial_eval.(invar_pos, progenitor_eqns)->self._trace.frame.find_progenitors(self)
A:jax.interpreters.partial_eval.val->JaxprStackFrame().constvar_to_val.get(frame.tracer_to_var.get(id(self)))
A:jax.interpreters.partial_eval.api_util._shaped_abstractify_handlers[DynamicJaxprTracer]->operator.attrgetter('aval')
A:jax.interpreters.partial_eval.self.gensym->jax.core.gensym()
A:jax.interpreters.partial_eval.self.effects->set()
A:jax.interpreters.partial_eval.(constvars, constvals)->unzip2(self.constvar_to_val.items())
A:jax.interpreters.partial_eval.(jaxpr, constvals)->_inline_literals(jaxpr, constvals)
A:jax.interpreters.partial_eval.(jaxpr, out_type)->_add_implicit_outputs(jaxpr)
A:jax.interpreters.partial_eval.(consts_out, new_eqn)->const_fold_rules[eqn.primitive](consts_in, eqn)
A:jax.interpreters.partial_eval.(fwd_vars, new_eqn)->forwarding_rules[eqn.primitive](eqn)
A:jax.interpreters.partial_eval.(new_constvars, new_constvals)->unzip2(consts.items())
A:jax.interpreters.partial_eval.self.frame.tracer_to_var[id(tracer)]var->self.frame.newvar(aval)
A:jax.interpreters.partial_eval.varself.frame.tracer_to_var[id(tracer)]->self.frame.newvar(tracer.aval)
A:jax.interpreters.partial_eval.(out_avals, effects)->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.source_info->jax._src.source_info_util.current()
A:jax.interpreters.partial_eval.invars->map(self.getvar, tracers)
A:jax.interpreters.partial_eval.outvars->map(self.makevar, out_tracers)
A:jax.interpreters.partial_eval.implicit_tracers->_extract_implicit_args(self, f.in_type, explicit_tracers)
A:jax.interpreters.partial_eval.dbg->debug_info_final(fun, traced_for)
A:jax.interpreters.partial_eval.(jaxpr, out_type, consts)->trace_to_subjaxpr_dynamic2(fun, main, debug_info)
A:jax.interpreters.partial_eval.constvars->map(self.getvar, map(self.instantiate_const, call_consts))
A:jax.interpreters.partial_eval.(jaxpr, reduced_out_avals, consts)->trace_to_subjaxpr_dynamic(f, self.main, reduced_in_avals, debug_info=debug_info_final(f, map_primitive.name))
A:jax.interpreters.partial_eval.(fun_jaxpr, out_avals, consts)->trace_to_subjaxpr_dynamic(fun, self.main, in_avals)
A:jax.interpreters.partial_eval.closed_fun_jaxpr->jax.core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())
A:jax.interpreters.partial_eval.main_->ref(self.main)
A:jax.interpreters.partial_eval.jvp_jaxpr_thunk->_memoize(lambda : trace_to_subjaxpr_dynamic(jvp, main_(), 2 * in_avals)[::2])
A:jax.interpreters.partial_eval.fwd_jaxpr_thunk->_memoize(lambda : trace_to_subjaxpr_dynamic(fwd, main_(), in_avals)[::2])
A:jax.interpreters.partial_eval.(tracers_res, tracers_lin)->split_list(tracers, [res_tree.num_leaves])
A:jax.interpreters.partial_eval.(call_jaxpr, out_avals, call_consts)->trace_to_subjaxpr_dynamic(call, self.main, in_avals_p)
A:jax.interpreters.partial_eval.closed_call_jaxpr->jax.core.ClosedJaxpr(convert_constvars_jaxpr(call_jaxpr), ())
A:jax.interpreters.partial_eval.(transpose_flat, in_tree2)->flatten_fun_nokwargs(lu.wrap_init(transpose), treedef_tuple((res_tree, out_tree)))
A:jax.interpreters.partial_eval.transpose_jaxpr_thunk->_memoize(lambda : trace_to_subjaxpr_dynamic(transpose_flat, main_(), in_avals_t)[::2])
A:jax.interpreters.partial_eval.core.thread_local_state.trace_state->saved_state.pop()
A:jax.interpreters.partial_eval.func_src_info->fun_sourceinfo(fn)
A:jax.interpreters.partial_eval.arg_info->partial(arg_info_pytree, fn, in_tree, has_kwargs)
A:jax.interpreters.partial_eval.(args, kwargs)->tree_unflatten(in_tree, dummy_args)
A:jax.interpreters.partial_eval.ba->inspect.signature(fn).bind(*args, **kwargs)
A:jax.interpreters.partial_eval.(jaxpr, out_avals, consts)->trace_to_subjaxpr_dynamic(fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)
A:jax.interpreters.partial_eval.frame->JaxprStackFrame()
A:jax.interpreters.partial_eval.ans->inspect.unwrap(fun).call_wrapped(*in_tracers_)
A:jax.interpreters.partial_eval.(jaxpr, consts)->JaxprStackFrame().to_jaxpr(out_tracers)
A:jax.interpreters.partial_eval.(in_avals, keep_inputs)->unzip2(fun.in_type)
A:jax.interpreters.partial_eval.partial_specs->_canonicalize_specs(ndims, axes_specs)
A:jax.interpreters.partial_eval.specs->_complete_specs(args, partial_specs)
A:jax.interpreters.partial_eval.(idxs, implicit_types)->_collect_implicit(args, specs)
A:jax.interpreters.partial_eval.d->sizes.setdefault(name, x.shape[i])
A:jax.interpreters.partial_eval.spec->dict(spec)
A:jax.interpreters.partial_eval.spec[i]->named_tracers.get(id(d), TracerAsName(d))
A:jax.interpreters.partial_eval.counter->itertools.count()
A:jax.interpreters.partial_eval.idxs[name]->DBIdx(next(counter))
A:jax.interpreters.partial_eval.offset->len(implicit_types)
A:jax.interpreters.partial_eval.out_type->tuple(zip(out_avals, kept_outs))
A:jax.interpreters.partial_eval.self.tracer->main.with_cur_sublevel().with_cur_sublevel().full_raise(tracer)
A:jax.interpreters.partial_eval.explicit_tracers_->iter(explicit_tracers)
A:jax.interpreters.partial_eval.tracers[d1.val]->main.with_cur_sublevel().instantiate_const(d2)
A:jax.interpreters.partial_eval.a->a.update(shape=tuple(shape)).update(shape=tuple(shape))
A:jax.interpreters.partial_eval.s1_size->functools.reduce(op.mul, s1, 1)
A:jax.interpreters.partial_eval.s2_size->functools.reduce(op.mul, s2, 1)
A:jax.interpreters.partial_eval.(q, r)->divmod(s1_size, s2_size)
A:jax.interpreters.partial_eval.core._SPECIAL_DIMENSION_HANDLERS[DynamicJaxprTracer]->DimensionHandlerTracer()
A:jax.interpreters.partial_eval.eval_padded->jax.linear_util.wrap_init(partial(_eval_jaxpr_padded, jaxpr, consts))
A:jax.interpreters.partial_eval.(padded_jaxpr, _, padded_consts)->trace_to_jaxpr_dynamic(eval_padded, in_avals)
A:jax.interpreters.partial_eval.outs->rule(in_avals, out_avals, *map(read, eqn.invars), **eqn.params)
A:jax.interpreters.partial_eval.padding_rules[prim]->partial(_trivial_padding_rule, prim)
A:jax.interpreters.partial_eval.(padded_jaxpr, padded_consts)->pad_jaxpr(call_jaxpr, ())
A:jax.interpreters.partial_eval.(subfuns, bind_params)->prim.get_bind_params(new_params)
A:jax.interpreters.partial_eval.(jaxpr, consts, env)->tracers_to_jaxpr(in_tracers, out_tracers)
jax.interpreters.partial_eval.BoundedAxisSize(NamedTuple)
jax.interpreters.partial_eval.DebugInfo(NamedTuple)
jax.interpreters.partial_eval.DimensionHandlerTracer(core.DimensionHandler)
jax.interpreters.partial_eval.DimensionHandlerTracer.as_value(self,d:core.DimSize)
jax.interpreters.partial_eval.DimensionHandlerTracer.divide_shape_sizes(self,s1:core.Shape,s2:core.Shape)->core.DimSize
jax.interpreters.partial_eval.DimensionHandlerTracer.greater_equal(self,d1:core.DimSize,d2:core.DimSize)
jax.interpreters.partial_eval.DimensionHandlerTracer.is_constant(self,d:core.DimSize)->bool
jax.interpreters.partial_eval.DimensionHandlerTracer.stride(self,d:core.DimSize,window_size:core.DimSize,window_stride:core.DimSize)->core.DimSize
jax.interpreters.partial_eval.DimensionHandlerTracer.symbolic_equal(self,d1:core.DimSize,d2:core.DimSize)->bool
jax.interpreters.partial_eval.DynamicJaxprTrace(core.Trace)
jax.interpreters.partial_eval.DynamicJaxprTrace._lift_tracers_in_aval(self,aval)
jax.interpreters.partial_eval.DynamicJaxprTrace._new_const(self,aval,c)
jax.interpreters.partial_eval.DynamicJaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.frame(self)
jax.interpreters.partial_eval.DynamicJaxprTrace.getvar(self,tracer)
jax.interpreters.partial_eval.DynamicJaxprTrace.instantiate_const(self,val)
jax.interpreters.partial_eval.DynamicJaxprTrace.makevar(self,tracer)
jax.interpreters.partial_eval.DynamicJaxprTrace.new_arg(self,aval)
jax.interpreters.partial_eval.DynamicJaxprTrace.new_const(self,c)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_call(self,call_primitive,f,explicit_tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_custom_transpose(self,prim,call,tracers,transpose,out_types,lin_tree,res_tree,out_tree)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.sublift(self,t)
jax.interpreters.partial_eval.DynamicJaxprTracer(self,trace,aval,line_info=None)
jax.interpreters.partial_eval.DynamicJaxprTracer.__init__(self,trace,aval,line_info=None)
jax.interpreters.partial_eval.DynamicJaxprTracer._assert_live(self)->None
jax.interpreters.partial_eval.DynamicJaxprTracer._contents(self)
jax.interpreters.partial_eval.DynamicJaxprTracer._origin_msg(self)
jax.interpreters.partial_eval.DynamicJaxprTracer.full_lower(self)
jax.interpreters.partial_eval.DynamicJaxprTracer.get_referent(self)
jax.interpreters.partial_eval.JaxprEqnRecipe(NamedTuple)
jax.interpreters.partial_eval.JaxprStackFrame(self)
jax.interpreters.partial_eval.JaxprStackFrame.__init__(self)
jax.interpreters.partial_eval.JaxprStackFrame.add_eqn(self,eqn:core.JaxprEqn)
jax.interpreters.partial_eval.JaxprStackFrame.find_progenitors(self,tracer)
jax.interpreters.partial_eval.JaxprStackFrame.newvar(self,aval)
jax.interpreters.partial_eval.JaxprStackFrame.to_jaxpr(self,out_tracers)
jax.interpreters.partial_eval.JaxprStackFrame.to_jaxpr2(self,out_tracers)
jax.interpreters.partial_eval.JaxprTrace(self,*args,name_stack:source_info_util.NameStack)
jax.interpreters.partial_eval.JaxprTrace.__init__(self,*args,name_stack:source_info_util.NameStack)
jax.interpreters.partial_eval.JaxprTrace._current_truncated_name_stack(self)
jax.interpreters.partial_eval.JaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.instantiate_const(self,tracer)->Tracer
jax.interpreters.partial_eval.JaxprTrace.instantiate_const_abstracted(self,tracer)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.lift(self,val:Tracer)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.new_arg(self,pval:PartialVal)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.new_const(self,val)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_const(self,val)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_literal(self,val)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.post_process_call(self,primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.post_process_custom_jvp_call(self,out_tracers,_)
jax.interpreters.partial_eval.JaxprTrace.post_process_custom_vjp_call(self,out_tracers,_)
jax.interpreters.partial_eval.JaxprTrace.post_process_map(self,primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_call(self,primitive,f,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.interpreters.partial_eval.JaxprTrace.process_custom_transpose(self,prim,call,tracers,**params)
jax.interpreters.partial_eval.JaxprTrace.process_custom_vjp_call(self,prim,f,fwd,bwd,tracers,out_trees)
jax.interpreters.partial_eval.JaxprTrace.process_map(self,primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.pure(self,val:Any)->JaxprTracer
jax.interpreters.partial_eval.JaxprTrace.sublift(self,val:JaxprTracer)->JaxprTracer
jax.interpreters.partial_eval.JaxprTracer(self,trace:JaxprTrace,pval:PartialVal,recipe:Optional[JaxprTracerRecipe])
jax.interpreters.partial_eval.JaxprTracer.__init__(self,trace:JaxprTrace,pval:PartialVal,recipe:Optional[JaxprTracerRecipe])
jax.interpreters.partial_eval.JaxprTracer.__repr__(self)
jax.interpreters.partial_eval.JaxprTracer.aval(self)->AbstractValue
jax.interpreters.partial_eval.JaxprTracer.full_lower(self)
jax.interpreters.partial_eval.JaxprTracer.get_referent(self)
jax.interpreters.partial_eval.JaxprTracer.is_known(self)
jax.interpreters.partial_eval.JaxprTracer.parents(self)->Sequence[JaxprTracer]
jax.interpreters.partial_eval.PartialVal(cls,xs:Tuple[Optional[AbstractValue],core.Value])
jax.interpreters.partial_eval.PartialVal.__new__(cls,xs:Tuple[Optional[AbstractValue],core.Value])
jax.interpreters.partial_eval.PartialVal.get_aval(self)->AbstractValue
jax.interpreters.partial_eval.PartialVal.get_known(self)->Optional[core.Value]
jax.interpreters.partial_eval.PartialVal.is_known(self)->bool
jax.interpreters.partial_eval.PartialVal.known(cls,const:core.Value)->PartialVal
jax.interpreters.partial_eval.PartialVal.unknown(cls,aval:AbstractValue)->PartialVal
jax.interpreters.partial_eval.TracerAsName(self,tracer)
jax.interpreters.partial_eval.TracerAsName.__eq__(self,other)
jax.interpreters.partial_eval.TracerAsName.__hash__(self)
jax.interpreters.partial_eval.TracerAsName.__init__(self,tracer)
jax.interpreters.partial_eval.WrapperForWeakRef(self,val)
jax.interpreters.partial_eval.WrapperForWeakRef.__init__(self,val)
jax.interpreters.partial_eval._add_implicit_outputs(jaxpr:Jaxpr)->Tuple[Jaxpr, OutputType]
jax.interpreters.partial_eval._arg_type(idxs:Dict[AbstractedAxisName,DBIdx],x:Any,spec:Dict[int,AbstractedAxisName])->AbstractValue
jax.interpreters.partial_eval._canonicalize_specs(ndims:Sequence[int],specs:Optional[Sequence[AbstractedAxesSpec]])->List[Dict[int, AbstractedAxisName]]
jax.interpreters.partial_eval._closed_call_param_updater(params,_,__)
jax.interpreters.partial_eval._collect_implicit(args:Sequence[Any],specs:List[Dict[int,AbstractedAxisName]])->Tuple[Dict[AbstractedAxisName, DBIdx], List[AbstractValue]]
jax.interpreters.partial_eval._complete_specs(args:Sequence[Any],partial_specs:List[Dict[int,AbstractedAxisName]])->List[Dict[int, AbstractedAxisName]]
jax.interpreters.partial_eval._const_folding_and_forwarding(jaxpr,constvals)
jax.interpreters.partial_eval._dce_jaxpr(jaxpr:Jaxpr,used_outputs:Tuple[bool,...],instantiate:Tuple[bool,...])->Tuple[Jaxpr, List[bool]]
jax.interpreters.partial_eval._default_dce_rule(used_outs:List[bool],eqn:JaxprEqn)->Tuple[List[bool], JaxprEqn]
jax.interpreters.partial_eval._default_res_aval_updater(params:Dict[str,Any],aval:AbstractValue)->AbstractValue
jax.interpreters.partial_eval._eval_jaxpr_padded(jaxpr:Jaxpr,consts:List[Const],*args:DynamicJaxprTracer)->List[Union[Const, DynamicJaxprTracer]]
jax.interpreters.partial_eval._extract_implicit_args(trace:DynamicJaxprTrace,in_type:Sequence[Tuple[AbstractValue,bool]],explicit_tracers:Sequence[DynamicJaxprTracer])->Sequence[DynamicJaxprTracer]
jax.interpreters.partial_eval._in_avals_from_tracers(tracers:List[DynamicJaxprTracer])->List[AbstractValue]
jax.interpreters.partial_eval._inline_literals(jaxpr,constvals)
jax.interpreters.partial_eval._input_type_to_tracers(new_arg:Callable[[AbstractValue],Tracer],in_avals:Sequence[AbstractValue])->Sequence[Tracer]
jax.interpreters.partial_eval._is_bint_axis_size(d:Union[int,core.DArray,core.Var])->bool
jax.interpreters.partial_eval._jaxpr_forwarding(jaxpr:Jaxpr)->List[Optional[int]]
jax.interpreters.partial_eval._memoize(thunk)
jax.interpreters.partial_eval._move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:Tuple[bool,...])->ClosedJaxpr
jax.interpreters.partial_eval._move_to_front(lst:Sequence,to_move:Sequence[bool])->Sequence
jax.interpreters.partial_eval._partial_eval_jaxpr_custom_cached(jaxpr:Jaxpr,in_unknowns:Tuple[bool,...],in_inst:Tuple[bool,...],ensure_out_unknowns:Tuple[bool,...],ensure_out_inst:Tuple[bool,...],saveable:Callable[...,bool])->Tuple[Jaxpr, Jaxpr, List[bool], List[bool], int]
jax.interpreters.partial_eval._partial_eval_jaxpr_nounits(jaxpr,in_unknowns,instantiate)
jax.interpreters.partial_eval._substitute_axis_sizes(env:Dict,aval:AbstractValue)->AbstractValue
jax.interpreters.partial_eval._substitute_vars_in_type(consts:Dict[Var,Literal],env:Dict[Var,Var],a:AbstractValue)->AbstractValue
jax.interpreters.partial_eval._trace_to_subjaxpr_nounits(main,instantiate,in_pvals)
jax.interpreters.partial_eval._trivial_padding_rule(prim,_,__,*args,**params)
jax.interpreters.partial_eval._trivial_padding_rule_multi(prim,_,__,*args,**params)
jax.interpreters.partial_eval._update_annotation_known(f:lu.WrappedFun,orig_type:Optional[InputType],in_knowns:List[bool])->lu.WrappedFun
jax.interpreters.partial_eval.abstract_eval_fun(fun,*avals,debug_info=None,**params)
jax.interpreters.partial_eval.arg_info_flattened(flat_pos:List[int])->str
jax.interpreters.partial_eval.arg_info_pytree(fn:Callable,in_tree:PyTreeDef,has_kwargs:bool,flat_pos:List[int])->str
jax.interpreters.partial_eval.call_padding_rule(prim,in_avals,out_avals,*args,call_jaxpr,**params)
jax.interpreters.partial_eval.call_partial_eval_custom_rule(jaxpr_param_name:str,params_updater:ParamsUpdater,saveable:Callable[...,bool],unks_in:List[bool],inst_in:List[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater)->Tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], List[Var]]
jax.interpreters.partial_eval.close_jaxpr(jaxpr:Jaxpr)->ClosedJaxpr
jax.interpreters.partial_eval.closed_call_partial_eval_custom_rule(jaxpr_param_name:str,saveable:Callable[...,bool],unks_in:List[bool],inst_in:List[bool],eqn:JaxprEqn,*,res_aval:ResAvalUpdater=_default_res_aval_updater)->Tuple[JaxprEqn, JaxprEqn, Sequence[bool], Sequence[bool], List[Var]]
jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr:Jaxpr)->Jaxpr
jax.interpreters.partial_eval.convert_envvars_to_constvars(jaxpr:Jaxpr,num_env_vars:int)->Jaxpr
jax.interpreters.partial_eval.convert_invars_to_constvars(jaxpr:Jaxpr,n:int)->Jaxpr
jax.interpreters.partial_eval.dce_jaxpr(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:Union[bool,Sequence[bool]]=False)->Tuple[Jaxpr, List[bool]]
jax.interpreters.partial_eval.dce_jaxpr_call_rule(used_outputs:List[bool],eqn:JaxprEqn)->Tuple[List[bool], Optional[JaxprEqn]]
jax.interpreters.partial_eval.dce_jaxpr_closed_call_rule(used_outputs:List[bool],eqn:JaxprEqn)->Tuple[List[bool], JaxprEqn]
jax.interpreters.partial_eval.dce_jaxpr_consts(jaxpr:Jaxpr,used_outputs:Sequence[bool],instantiate:Union[bool,Sequence[bool]]=False)->Tuple[Jaxpr, List[bool], List[bool]]
jax.interpreters.partial_eval.debug_info(fn:Callable,in_tree:Optional[PyTreeDef],has_kwargs:bool,traced_for:str)->DebugInfo
jax.interpreters.partial_eval.debug_info_final(fn:lu.WrappedFun,traced_for:str)->DebugInfo
jax.interpreters.partial_eval.def_trivial_padding(prim:Primitive)->None
jax.interpreters.partial_eval.extend_jaxpr_stack(main,frame)
jax.interpreters.partial_eval.fun_sourceinfo(fun:Callable)
jax.interpreters.partial_eval.identity(x)
jax.interpreters.partial_eval.infer_lambda_input_type(axes_specs:Optional[Sequence[AbstractedAxesSpec]],args:Sequence[Any])->InputType
jax.interpreters.partial_eval.instantiate_const_at(trace:JaxprTrace,instantiate:bool,tracer)
jax.interpreters.partial_eval.move_binders_to_back(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax.interpreters.partial_eval.move_binders_to_front(closed_jaxpr:ClosedJaxpr,to_move:Sequence[bool])->ClosedJaxpr
jax.interpreters.partial_eval.new_eqn_recipe(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer],primitive:Primitive,params:Dict[str,Any],effects:core.Effects,source_info:source_info_util.SourceInfo)->JaxprEqnRecipe
jax.interpreters.partial_eval.pad_jaxpr(jaxpr:Jaxpr,consts:Sequence[Const])->Tuple[Jaxpr, List[Const]]
jax.interpreters.partial_eval.partial_eval_jaxpr_custom(jaxpr:Jaxpr,in_unknowns:Sequence[bool],in_inst:Union[bool,Sequence[bool]],ensure_out_unknowns:Union[bool,Sequence[bool]],ensure_out_inst:Union[bool,Sequence[bool]],saveable:Callable[...,bool])->Tuple[Jaxpr, Jaxpr, List[bool], List[bool], int]
jax.interpreters.partial_eval.partial_eval_jaxpr_custom_rule_not_implemented(name:str,saveable:Callable[...,bool],unks_in:Sequence[bool],inst_in:Sequence[bool],eqn:JaxprEqn)->PartialEvalCustomResult
jax.interpreters.partial_eval.partial_eval_jaxpr_nounits(jaxpr:ClosedJaxpr,unknowns:Sequence[bool],instantiate:Union[bool,Sequence[bool]])->Tuple[ClosedJaxpr, ClosedJaxpr, List[bool], List[AbstractValue]]
jax.interpreters.partial_eval.partial_eval_wrapper_nounits(in_knowns:Sequence[bool],in_avals:Sequence[AbstractValue],*in_consts:Any)
jax.interpreters.partial_eval.partition_pvals(pvals:List[PartialVal])->Tuple[List[bool], List[AbstractValue], List[Any]]
jax.interpreters.partial_eval.recipe_to_eqn(getvar:Callable[[JaxprTracer],Atom],recipe:JaxprEqnRecipe)->core.JaxprEqn
jax.interpreters.partial_eval.trace_to_jaxpr(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:Union[bool,Sequence[bool]]=False)->Tuple[Jaxpr, List[PartialVal], List[core.Value]]
jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:Optional[DebugInfo]=None,*,keep_inputs:Optional[List[bool]]=None)
jax.interpreters.partial_eval.trace_to_jaxpr_dynamic2(fun:lu.WrappedFun,debug_info:Optional[DebugInfo]=None)->Tuple[Jaxpr, OutputType, List[Any]]
jax.interpreters.partial_eval.trace_to_jaxpr_final(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue],debug_info:Optional[DebugInfo]=None,keep_inputs:Optional[Sequence[bool]]=None)
jax.interpreters.partial_eval.trace_to_jaxpr_final2(fun:lu.WrappedFun,debug_info:Optional[DebugInfo]=None)->Tuple[Jaxpr, OutputType, List[Any]]
jax.interpreters.partial_eval.trace_to_jaxpr_nounits(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:Union[bool,Sequence[bool]]=False)->Tuple[Jaxpr, List[PartialVal], List[core.Value]]
jax.interpreters.partial_eval.trace_to_subjaxpr(main:core.MainTrace,instantiate:Union[bool,Sequence[bool]],pvals:Sequence[PartialVal])
jax.interpreters.partial_eval.trace_to_subjaxpr_dynamic(fun:lu.WrappedFun,main:core.MainTrace,in_avals:Sequence[AbstractValue],*,keep_inputs:Optional[Sequence[bool]]=None,debug_info:Optional[DebugInfo]=None)
jax.interpreters.partial_eval.trace_to_subjaxpr_dynamic2(fun:lu.WrappedFun,main:core.MainTrace,debug_info:Optional[DebugInfo]=None)->Tuple[Jaxpr, OutputType, List[Any]]
jax.interpreters.partial_eval.trace_to_subjaxpr_dynamic2_memoized(fun:lu.WrappedFun,main:core.MainTrace,traced_for:str)
jax.interpreters.partial_eval.trace_to_subjaxpr_nounits(main:core.MainTrace,instantiate:Union[bool,Sequence[bool]],in_pvals:Sequence[PartialVal])
jax.interpreters.partial_eval.trace_to_subjaxpr_nounits_dyn(main:core.MainTrace,in_knowns:Sequence[bool],in_type:InputType,instantiate:Union[bool,Sequence[bool]],*in_consts:Any)
jax.interpreters.partial_eval.trace_to_subjaxpr_nounits_fwd(main:core.MainTrace,instantiate:Union[bool,Sequence[bool]],in_pvals:Sequence[PartialVal])
jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers:Sequence[JaxprTracer],out_tracers:Sequence[JaxprTracer])->Tuple[Jaxpr, Tuple[Any, ...], Tuple[Any, ...]]


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/interpreters/mlir.py----------------------------------------
A:jax.interpreters.mlir.T->typing.TypeVar('T')
A:jax.interpreters.mlir.a->numpy.array(0 if a.item() == 0 else 255, np.uint8)
A:jax.interpreters.mlir.int1d->aval_to_ir_type(core.ShapedArray((1,), np.int32))
A:jax.interpreters.mlir.(d, *ds)->map(lower_dim, sizes)
A:jax.interpreters.mlir.ctx_new->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks).replace(**ctx_override_kwargs)
A:jax.interpreters.mlir.out->handler(val, canonicalize_types)
A:jax.interpreters.mlir.dtype->numpy.dtype(dtype)
A:jax.interpreters.mlir.types->aval_to_ir_types(aval)
A:jax.interpreters.mlir.handler->_constant_handlers.get(t)
A:jax.interpreters.mlir.values->ir_constants(val, canonicalize_types=canonicalize_types)
A:jax.interpreters.mlir.x->numpy.ascontiguousarray(x)
A:jax.interpreters.mlir.element_type->dtype_to_ir_type(x.dtype)
A:jax.interpreters.mlir.attr->jax._src.lib.mlir.ir.DenseElementsAttr.get(x, type=element_type, shape=shape)
A:jax.interpreters.mlir.(zero_stride_axes,)->numpy.where(np.equal(0, val.strides))
A:jax.interpreters.mlir.(other_axes,)->numpy.where(np.not_equal(0, val.strides))
A:jax.interpreters.mlir.collapsed_val->numpy.asarray(collapsed_val, dtypes.canonicalize_dtype(collapsed_val.dtype))
A:jax.interpreters.mlir.frame->jax._src.source_info_util.user_frame(source_info)
A:jax.interpreters.mlir.loc->_source_info_to_location(eqn.primitive, eqn.params, source_info, name_stack=ctx.name_stack)
A:jax.interpreters.mlir.context->jax._src.lib.mlir.ir.Context()
A:jax.interpreters.mlir._platform_specific_lowerings->collections.defaultdict(dict)
A:jax.interpreters.mlir._module_name_regex->re.compile('[^\\w.-]')
A:jax.interpreters.mlir.tile_rank->len(sharding.tile_assignment_dimensions)
A:jax.interpreters.mlir.platform->jax._src.lib.xla_bridge.canonicalize_platform(platform)
A:jax.interpreters.mlir.(out_aval,)->out_aval.dtype._rules.physical_avals(out_aval)
A:jax.interpreters.mlir.(input_output_aliases, donated_args)->_set_up_aliases(in_avals, out_avals, donated_args)
A:jax.interpreters.mlir.channel_iter->itertools.count(1)
A:jax.interpreters.mlir.ctx->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks)
A:jax.interpreters.mlir.module_name->re.compile('[^\\w.-]').sub('_', module_name)
A:jax.interpreters.mlir.ctx.module.operation.attributes['sym_name']->jax._src.lib.mlir.ir.StringAttr.get(module_name)
A:jax.interpreters.mlir.output->io.BytesIO()
A:jax.interpreters.mlir.avals_in->map(aval, eqn.invars)
A:jax.interpreters.mlir.avals_out->map(strip_metadata, avals_out)
A:jax.interpreters.mlir.donations->collections.defaultdict(collections.deque)
A:jax.interpreters.mlir.out_donated_args->list(donated_args)
A:jax.interpreters.mlir.input_id->donations[aval].popleft()
A:jax.interpreters.mlir.self._tokens->collections.OrderedDict(*args, **kwargs)
A:jax.interpreters.mlir.aval->jax.core.ShapedArray((), np.dtype(np.bool_))
A:jax.interpreters.mlir.input_types->map(aval_to_ir_types, ctx.avals_in)
A:jax.interpreters.mlir.output_types->map(aval_to_ir_types, ctx.avals_out)
A:jax.interpreters.mlir.num_tokens->len(effects)
A:jax.interpreters.mlir.flat_input_types->jax._src.util.flatten(input_types)
A:jax.interpreters.mlir.flat_output_types->jax._src.util.flatten(output_types)
A:jax.interpreters.mlir.ftype->jax._src.lib.mlir.ir.FunctionType.get(flat_input_types, flat_output_types)
A:jax.interpreters.mlir.func_op->lower_jaxpr_to_fun(ctx, fn_name, call_jaxpr, effects)
A:jax.interpreters.mlir.func_op.attributes['sym_visibility']->jax._src.lib.mlir.ir.StringAttr.get('private')
A:jax.interpreters.mlir.ir_arg_shardings->jax._src.util.flatten([[sharding] * len(types) for (sharding, types) in zip(arg_shardings, input_types)])
A:jax.interpreters.mlir.ir_result_shardings->jax._src.util.flatten([[sharding] * len(types) for (sharding, types) in zip(result_shardings, output_types)])
A:jax.interpreters.mlir.attrs['mhlo.is_same_data_across_replicas']->jax._src.lib.mlir.ir.UnitAttr.get()
A:jax.interpreters.mlir.attrs['mhlo.sharding']->jax._src.lib.mlir.ir.StringAttr.get(sharding.SerializeToString())
A:jax.interpreters.mlir.output_ids->jax._src.util.unflatten(list(range(len(flat_output_types))), map(len, output_types))
A:jax.interpreters.mlir.attrs['tf.aliasing_output']->i32_attr(alias)
A:jax.interpreters.mlir.func_op.arg_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DictAttr.get(attrs) for attrs in arg_attrs])
A:jax.interpreters.mlir.func_op.result_attrs->jax._src.lib.mlir.ir.ArrayAttr.get([ir.DictAttr.get({} if sharding is None else {'mhlo.sharding': ir.StringAttr.get(sharding.SerializeToString())}) for sharding in ir_result_shardings])
A:jax.interpreters.mlir.entry_block->lower_jaxpr_to_fun(ctx, fn_name, call_jaxpr, effects).add_entry_block()
A:jax.interpreters.mlir.unflattened_args->jax._src.util.unflatten(entry_block.arguments, map(len, input_types))
A:jax.interpreters.mlir.(token_args, unflattened_args)->jax._src.util.split_list(unflattened_args, [len(ctx.tokens_in)])
A:jax.interpreters.mlir.tokens_in->tokens.update_tokens(tokens_out).subset(effects)
A:jax.interpreters.mlir.callee_name_stack->jax.interpreters.xla.extend_name_stack(ctx.name_stack, util.wrap_name(name, 'jit'))
A:jax.interpreters.mlir.(out_vals, tokens_out)->jaxpr_subcomp(ctx.replace(name_stack=callee_name_stack), jaxpr.jaxpr, tokens_in, map(ir_constants, jaxpr.consts), *args)
A:jax.interpreters.mlir.flat_outputs->jax._src.util.flatten(outs)
A:jax.interpreters.mlir.sub_ctx->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks).replace(tokens_in=TokenSet(zip(ctx.tokens_in.effects(), token_args)))
A:jax.interpreters.mlir.outs->lowering_rule(sub_ctx, *_unwrap_singleton_ir_values(unflattened_args))
A:jax.interpreters.mlir.env[v]->tuple(node)
A:jax.interpreters.mlir.in_nodes->map(read, eqn.invars)
A:jax.interpreters.mlir.source_info->eqn.source_info.replace(name_stack=ctx.name_stack + eqn.source_info.name_stack)
A:jax.interpreters.mlir.rule->xla_fallback_lowering(eqn.primitive)
A:jax.interpreters.mlir.rule_ctx->rule_ctx.replace(axis_size_env=axis_size_env).replace(axis_size_env=axis_size_env)
A:jax.interpreters.mlir.ans->rule(rule_ctx, *map(_unwrap_singleton_ir_values, in_nodes), **eqn.params)
A:jax.interpreters.mlir.tokens->tokens.update_tokens(tokens_out).update_tokens(tokens_out)
A:jax.interpreters.mlir.out_nodes->jax._src.util.unflatten(call.results, map(len, output_types))
A:jax.interpreters.mlir.wrapped_fun->jax.linear_util.annotate(wrapped_fun, (*implicit_args, *explicit_args))
A:jax.interpreters.mlir.i32_aval->jax.core.ShapedArray((), np.dtype('int32'))
A:jax.interpreters.mlir.(jaxpr, _, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, ctx.avals_in)
A:jax.interpreters.mlir.(out, tokens)->jaxpr_subcomp(ctx.module_context, jaxpr, ctx.tokens_in, _ir_consts(consts), *map(wrap_singleton_ir_values, args))
A:jax.interpreters.mlir.call_jaxpr->jax.core.ClosedJaxpr(call_jaxpr, ())
A:jax.interpreters.mlir.effects->tokens.update_tokens(tokens_out).subset(effects).effects()
A:jax.interpreters.mlir.call->jax._src.lib.mlir.dialects.func.CallOp(flat_output_types, ir.FlatSymbolRefAttr.get(func.name.value), flatten_lowering_ir_args(args))
A:jax.interpreters.mlir.(tokens, out_nodes)->jax._src.util.split_list(out_nodes, [len(effects)])
A:jax.interpreters.mlir.tokens_out->tokens.update_tokens(tokens_out).subset(effects).update_tokens(TokenSet(zip(effects, tokens)))
A:jax.interpreters.mlir.(out_nodes, tokens)->_call_lowering(name, name, call_jaxpr, backend, ctx.module_context, ctx.avals_in, ctx.avals_out, ctx.tokens_in, *args)
A:jax.interpreters.mlir.zero->ir_constant(np.array(value, aval.dtype))
A:jax.interpreters.mlir.tensor_type->jax._src.lib.mlir.ir.RankedTensorType(x.type)
A:jax.interpreters.mlir.real_eq->compare_mhlo(rx, ry, 'EQ', 'FLOAT')
A:jax.interpreters.mlir.real_cmp->compare_mhlo(rx, ry, cmp, 'FLOAT')
A:jax.interpreters.mlir.imag_cmp->compare_mhlo(mhlo.ImagOp(x).result, mhlo.ImagOp(y).result, cmp, 'FLOAT')
A:jax.interpreters.mlir.min_mhlo->partial(_minmax_mhlo, mhlo.MinOp, 'LT')
A:jax.interpreters.mlir.max_mhlo->partial(_minmax_mhlo, mhlo.MaxOp, 'GT')
A:jax.interpreters.mlir.op->typing.cast(func_dialect.FuncOp, op)
A:jax.interpreters.mlir.op.attributes['mhlo.sharding']->jax._src.lib.mlir.ir.StringAttr.get(sharding_proto.SerializeToString())
A:jax.interpreters.mlir.wrap_with_full_to_shard_op->partial(_wrap_with_spmd_op, 'SPMDFullToShardShape')
A:jax.interpreters.mlir.wrap_with_shard_to_full_op->partial(_wrap_with_spmd_op, 'SPMDShardToFullShape')
A:jax.interpreters.mlir.func->_emit_lowering_rule_as_fun(partial(f, **params), ctx)
A:jax.interpreters.mlir.module_str->jax._src.lib.xla_client._xla.mlir.xla_computation_to_mlir_module(xla_computation)
A:jax.interpreters.mlir.dst_symtab->jax._src.lib.mlir.ir.SymbolTable(dst_module.operation)
A:jax.interpreters.mlir.n->len(dst_module.body.operations)
A:jax.interpreters.mlir.op.attributes['sym_visibility']->jax._src.lib.mlir.ir.StringAttr.get('private')
A:jax.interpreters.mlir.xla_computation->jax.interpreters.xla.primitive_subcomputation(module_ctx.platform, axis_env, prim, ctx.avals_in, ctx.avals_out, **params)
A:jax.interpreters.mlir.xla_module->xla_computation_to_mhlo_module(xla_computation)
A:jax.interpreters.mlir.callee_name->merge_mhlo_modules(module_ctx.module, f'xla_fallback_{prim.name}', xla_module)
A:jax.interpreters.mlir.channel_handle->jax._src.lib.mlir.dialects.mhlo.ChannelHandle.get(channel, RECV_FROM_HOST_TYPE)
A:jax.interpreters.mlir.send_op->jax._src.lib.mlir.dialects.mhlo.SendOp(mhlo.TokenType.get(), [operand], token, channel_handle, is_host_transfer=ir.BoolAttr.get(True))
A:jax.interpreters.mlir.dtype_str->_dtype_to_xla_type_string(out_aval.dtype)
A:jax.interpreters.mlir.send_op.attributes['mhlo.frontend_attributes']->jax._src.lib.mlir.ir.DictAttr.get(dict(_xla_host_transfer_handler_name=ir.StringAttr.get(str(name)), _xla_host_transfer_original_type=ir.StringAttr.get(dtype_str), _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))
A:jax.interpreters.mlir.recv_op->jax._src.lib.mlir.dialects.mhlo.RecvOp([aval_to_ir_type(out_aval), mhlo.TokenType.get()], token, channel_handle, is_host_transfer=ir.BoolAttr.get(True))
A:jax.interpreters.mlir.recv_op.attributes['mhlo.frontend_attributes']->jax._src.lib.mlir.ir.DictAttr.get(dict(_xla_host_transfer_handler_name=ir.StringAttr.get(str(name)), _xla_host_transfer_original_type=ir.StringAttr.get(dtype_str), _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))
A:jax.interpreters.mlir.send_channel->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks).module_context.new_channel()
A:jax.interpreters.mlir.dummy_send_aval->jax.core.ShapedArray((1,), np.float32)
A:jax.interpreters.mlir.dummy_send_val->ir_constant(np.zeros(1, np.float32))
A:jax.interpreters.mlir.token->send_to_host(channel, token, operand, operand_aval, callback.__name__, sharding=sharding)
A:jax.interpreters.mlir.channel->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks).module_context.new_channel()
A:jax.interpreters.mlir.recv_channel->ModuleContext(backend_or_name, platform, axis_context, name_stack, keepalives, channel_iter, host_callbacks).module_context.new_channel()
A:jax.interpreters.mlir.dummy_recv_aval->jax.core.ShapedArray((1,), np.float32)
A:jax.interpreters.mlir.(token, _)->receive_from_host(recv_channel, token, dummy_recv_aval, callback.__name__, sharding=sharding)
A:jax.interpreters.mlir.(token, out)->receive_from_host(channel, token, result_aval, callback.__name__, sharding=sharding)
A:jax.interpreters.mlir.opaque->backend.make_python_callback_from_host_send_and_recv(_wrapped_callback, operand_shapes, result_shapes, send_channels, recv_channels)
A:jax.interpreters.mlir.result_shapes->jax._src.util.flatten([xla.aval_to_xla_shapes(result_aval) for result_aval in result_avals])
A:jax.interpreters.mlir.operand_shapes->jax._src.util.flatten([xla.aval_to_xla_shapes(op_aval) for op_aval in operand_avals])
A:jax.interpreters.mlir.out_vals->callback(*args)
A:jax.interpreters.mlir.result_types->jax._src.util.flatten([aval_to_ir_types(aval) for aval in result_avals])
A:jax.interpreters.mlir.(callback_descriptor, keepalive)->backend.get_emit_python_callback_descriptor(_wrapped_callback, operand_shapes, result_shapes)
A:jax.interpreters.mlir.descriptor_operand->ir_constant(callback_descriptor, canonicalize_types=False)
A:jax.interpreters.mlir.result_type->jax._src.lib.mlir.ir.TupleType.get_tuple(result_types)
A:jax.interpreters.mlir.result->jax._src.lib.mlir.dialects.mhlo.CustomCallOp([result_type], callback_operands, call_target_name=ir.StringAttr.get(call_target_name), has_side_effect=ir.BoolAttr.get(has_side_effect), api_version=i32_attr(2), called_computations=ir.ArrayAttr.get([]), backend_config=ir.StringAttr.get(str(callback_descriptor)), operand_layouts=None, result_layouts=None)
A:jax.interpreters.mlir.lowering_result->lower_jaxpr_to_module(name, closed_jaxpr, backend_or_name=backend_or_name, unordered_effects=[], ordered_effects=[], name_stack=source_info_util.NameStack(), donated_args=[False] * len(closed_jaxpr.jaxpr.invars), axis_context=axis_context, platform=platform)
jax.interpreters.mlir.ConstantHandler(self,val:Any,canonicalize_types:bool)
jax.interpreters.mlir.ConstantHandler.__call__(self,val:Any,canonicalize_types:bool)
jax.interpreters.mlir.LoweringResult(NamedTuple)
jax.interpreters.mlir.LoweringRuleContext
jax.interpreters.mlir.LoweringRuleContext.replace(self,**kw)
jax.interpreters.mlir.LoweringRuleContext.set_tokens_out(self,tokens_out:TokenSet)
jax.interpreters.mlir.ModuleContext(self,backend_or_name:Optional[Union[str,xb.XlaBackend]],platform:str,axis_context:AxisContext,name_stack:NameStack,keepalives:List[Any],channel_iterator:Iterator[int],host_callbacks:List[Any],context:Optional[ir.Context]=None,module:Optional[ir.Module]=None,ip:Optional[ir.InsertionPoint]=None,symbol_table:Optional[ir.SymbolTable]=None,cached_primitive_lowerings:Optional[Dict[Any,func_dialect.FuncOp]]=None,cached_call_jaxpr_lowerings:Optional[Dict[Any,func_dialect.FuncOp]]=None)
jax.interpreters.mlir.ModuleContext.__init__(self,backend_or_name:Optional[Union[str,xb.XlaBackend]],platform:str,axis_context:AxisContext,name_stack:NameStack,keepalives:List[Any],channel_iterator:Iterator[int],host_callbacks:List[Any],context:Optional[ir.Context]=None,module:Optional[ir.Module]=None,ip:Optional[ir.InsertionPoint]=None,symbol_table:Optional[ir.SymbolTable]=None,cached_primitive_lowerings:Optional[Dict[Any,func_dialect.FuncOp]]=None,cached_call_jaxpr_lowerings:Optional[Dict[Any,func_dialect.FuncOp]]=None)
jax.interpreters.mlir.ModuleContext.add_host_callback(self,host_callback:Any)->None
jax.interpreters.mlir.ModuleContext.add_keepalive(self,keepalive:Any)->None
jax.interpreters.mlir.ModuleContext.axis_env(self)->xla.AxisEnv
jax.interpreters.mlir.ModuleContext.backend(self)->xb.XlaBackend
jax.interpreters.mlir.ModuleContext.new_channel(self)->int
jax.interpreters.mlir.ModuleContext.replace(self,**kw)
jax.interpreters.mlir.ReplicaAxisContext
jax.interpreters.mlir.SPMDAxisContext
jax.interpreters.mlir.SPMDAxisContext.axis_env(self)
jax.interpreters.mlir.SPMDAxisContext.extend_manual(self,axes:FrozenSet[MeshAxisName])->SPMDAxisContext
jax.interpreters.mlir.SPMDAxisContext.unsafe_axis_env(self)
jax.interpreters.mlir.ShardingContext
jax.interpreters.mlir.ShardingContext.axis_env(self)
jax.interpreters.mlir.TokenSet(self,*args,**kwargs)
jax.interpreters.mlir.TokenSet.__init__(self,*args,**kwargs)
jax.interpreters.mlir.TokenSet.__len__(self)
jax.interpreters.mlir.TokenSet.create(cls,effects:Sequence[core.Effect])->TokenSet
jax.interpreters.mlir.TokenSet.effects(self)->Sequence[core.Effect]
jax.interpreters.mlir.TokenSet.get(self,effect:core.Effect)->Token
jax.interpreters.mlir.TokenSet.items(self)->Sequence[Tuple[core.Effect, Token]]
jax.interpreters.mlir.TokenSet.subset(self,effects:Sequence[core.Effect])->TokenSet
jax.interpreters.mlir.TokenSet.tokens(self)->Sequence[Token]
jax.interpreters.mlir.TokenSet.update_tokens(self,tokens:TokenSet)->TokenSet
jax.interpreters.mlir._array_ir_types(aval:Union[core.ShapedArray,core.DShapedArray])->Sequence[ir.Type]
jax.interpreters.mlir._call_lowering(fn_name,stack_name,call_jaxpr,backend,ctx,avals_in,avals_out,tokens_in,*args)
jax.interpreters.mlir._device_array_constant_handler(val,canonicalize_types)
jax.interpreters.mlir._dtype_to_xla_type_string(dtype:np.dtype)->str
jax.interpreters.mlir._dynamic_array_ir_types(aval:core.ShapedArray)->Sequence[ir.Type]
jax.interpreters.mlir._emit_lowering_rule_as_fun(lowering_rule,ctx:LoweringRuleContext)->func_dialect.FuncOp
jax.interpreters.mlir._emit_tpu_python_callback(backend:xb.XlaBackend,ctx:LoweringRuleContext,callback,token:Optional[Any],operands:List[ir.Value],operand_avals:List[core.ShapedArray],operand_shapes:List[xc.Shape],result_avals:List[core.ShapedArray],result_shapes:List[xc.Shape],*,sharding:Optional[xc.OpSharding]=None)->Tuple[List[ir.Value], Any, Any]
jax.interpreters.mlir._ir_consts(consts)
jax.interpreters.mlir._lower_jaxpr_to_fun_cached(ctx,fn_name,call_jaxpr,effects)
jax.interpreters.mlir._minmax_mhlo(op,cmp,x,y)
jax.interpreters.mlir._named_call_lowering(ctx,*args,name,backend=None,call_jaxpr)
jax.interpreters.mlir._ndarray_constant_handler(val:np.ndarray,canonicalize_types)->Sequence[ir.Value]
jax.interpreters.mlir._numpy_array_constant(x:np.ndarray,canonicalize_types)->Sequence[ir.Value]
jax.interpreters.mlir._python_scalar_handler(dtype,val,canonicalize_dtypes)
jax.interpreters.mlir._set_up_aliases(avals_in,avals_out,donated_args)
jax.interpreters.mlir._source_info_to_location(primitive:core.Primitive,params:Dict,source_info:source_info_util.SourceInfo,name_stack:Union[str,source_info_util.NameStack]='')->ir.Location
jax.interpreters.mlir._unwrap_singleton_ir_values(x)
jax.interpreters.mlir._wrap_with_spmd_op(name:str,result_type:ir.Type,x:ir.Value,sharding_proto:xc.OpSharding,unspecified_dims:Optional[Set[int]]=None)
jax.interpreters.mlir._xla_call_lower(ctx,*args,backend=None,name,call_jaxpr,donated_invars,inline=None,device=None,keep_unused=None)
jax.interpreters.mlir.add_jaxvals_lowering(ctx,x,y)
jax.interpreters.mlir.aval_to_ir_type(aval:core.AbstractValue)->ir.Type
jax.interpreters.mlir.aval_to_ir_types(aval:core.AbstractValue)->Sequence[ir.Type]
jax.interpreters.mlir.build_xla_computation_helper(closed_jaxpr:core.ClosedJaxpr,*,name:str,platform:str,backend_or_name:str,axis_context:AxisContext)->xc.XlaComputation
jax.interpreters.mlir.cache_lowering(f)
jax.interpreters.mlir.compare_mhlo(x,y,direction:str,comparison_type:Optional[str]=None)
jax.interpreters.mlir.convert_mhlo(x,aval_in,aval_out)
jax.interpreters.mlir.create_token()->Token
jax.interpreters.mlir.delegate_lowering(ctx,lowering_fun,*args,**ctx_override_kwargs)
jax.interpreters.mlir.dense_bool_elements(xs:Sequence[bool])->ir.DenseElementsAttr
jax.interpreters.mlir.dense_int_elements(xs)->ir.DenseIntElementsAttr
jax.interpreters.mlir.dtype_to_ir_type(dtype:Union[np.dtype,np.generic])->ir.Type
jax.interpreters.mlir.dummy_token()->Sequence[ir.Value]
jax.interpreters.mlir.dummy_token_type()->Sequence[ir.Type]
jax.interpreters.mlir.emit_python_callback(ctx:LoweringRuleContext,callback,token:Optional[Any],operands:List[ir.Value],operand_avals:List[core.ShapedArray],result_avals:List[core.ShapedArray],has_side_effect:bool,*,sharding:Optional[xc.OpSharding]=None)->Tuple[List[ir.Value], Any, Any]
jax.interpreters.mlir.flatten_lowering_ir_args(xs:Sequence[Union[ir.Value,Sequence[ir.Value]]])->Sequence[Sequence[ir.Value]]
jax.interpreters.mlir.full_like_aval(value,aval:core.ShapedArray)->ir.Value
jax.interpreters.mlir.get_constant_handler(type_:type)->ConstantHandler
jax.interpreters.mlir.i32_attr(i)
jax.interpreters.mlir.i64_attr(i)
jax.interpreters.mlir.ir_constant(val:Any,canonicalize_types:bool=True)->ir.Value
jax.interpreters.mlir.ir_constants(val:Any,canonicalize_types:bool=True)->Sequence[ir.Value]
jax.interpreters.mlir.jaxpr_subcomp(ctx:ModuleContext,jaxpr:core.Jaxpr,tokens:TokenSet,consts:Sequence[Sequence[ir.Value]],*args:Sequence[ir.Value])->Tuple[Sequence[Sequence[ir.Value]], TokenSet]
jax.interpreters.mlir.lower_fun(fun:Callable,multiple_results:bool=True)->Callable
jax.interpreters.mlir.lower_jaxpr_to_fun(ctx:ModuleContext,name:str,jaxpr:core.ClosedJaxpr,effects:Sequence[core.Effect],*,create_tokens:bool=False,public:bool=False,replace_tokens_with_dummy:bool=False,replicated_args:Optional[Sequence[bool]]=None,arg_shardings:Optional[Sequence[Optional[xc.OpSharding]]]=None,result_shardings:Optional[Sequence[Optional[xc.OpSharding]]]=None,use_sharding_annotations:bool=True,input_output_aliases:Optional[Sequence[Optional[int]]]=None,num_output_tokens:int=0)->func_dialect.FuncOp
jax.interpreters.mlir.lower_jaxpr_to_module(module_name:str,jaxpr:core.ClosedJaxpr,unordered_effects:List[core.Effect],ordered_effects:List[core.Effect],backend_or_name:Optional[Union[str,xb.XlaBackend]],platform:str,axis_context:AxisContext,name_stack:NameStack,donated_args:Sequence[bool],replicated_args:Optional[Sequence[bool]]=None,arg_shardings:Optional[Sequence[Optional[xc.OpSharding]]]=None,result_shardings:Optional[Sequence[Optional[xc.OpSharding]]]=None)->LoweringResult
jax.interpreters.mlir.make_ir_context()->ir.Context
jax.interpreters.mlir.merge_mhlo_modules(dst_module:ir.Module,sym_name:str,src_module:ir.Module)->str
jax.interpreters.mlir.module_to_bytecode(module:ir.Module)->bytes
jax.interpreters.mlir.module_to_string(module:ir.Module)->str
jax.interpreters.mlir.receive_from_host(channel:int,token:mhlo.TokenType,out_aval:core.ShapedArray,name:str,*,sharding:Optional[xc.OpSharding]=None)->ir.Value
jax.interpreters.mlir.register_constant_handler(type_:type,handler_fun:ConstantHandler)
jax.interpreters.mlir.register_lowering(prim:core.Primitive,rule:LoweringRule,platform:Optional[str]=None)
jax.interpreters.mlir.send_to_host(channel:int,token:mhlo.TokenType,operand:Any,aval:core.ShapedArray,name:str,*,sharding:Optional[xc.OpSharding]=None)->ir.Value
jax.interpreters.mlir.set_sharding(op,sharding_proto:xc.OpSharding)
jax.interpreters.mlir.shape_tensor(sizes:Sequence[Union[int,ir.RankedTensorType]])->ir.RankedTensorType
jax.interpreters.mlir.sharded_aval(aval:core.ShapedArray,sharding:Optional[xc.OpSharding])->core.ShapedArray
jax.interpreters.mlir.token_type()->Sequence[ir.Type]
jax.interpreters.mlir.wrap_singleton_ir_values(x:Union[ir.Value,Sequence[ir.Value]])->Sequence[ir.Value]
jax.interpreters.mlir.wrap_with_sharding_op(x:ir.Value,sharding_proto:xc.OpSharding,unspecified_dims:Optional[Set[int]]=None)
jax.interpreters.mlir.xla_computation_to_mhlo_module(xla_computation:xc.XlaComputation)->ir.Module
jax.interpreters.mlir.xla_fallback_lowering(prim:core.Primitive)
jax.interpreters.mlir.zeros_like_lowering(ctx,x)


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/lax/__init__.py----------------------------------------


----------------------------------------/dataset/nuaa/anaconda3/envs/jax0.3.23/lib/python3.9/site-packages/jax/lax/linalg.py----------------------------------------

