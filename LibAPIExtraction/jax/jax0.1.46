
----------------------------------------/home/zhang/Packages/jax/jax0.1.46/flatten_util.py----------------------------------------
A:jax.flatten_util.(leaves, treedef)->tree_flatten(pytree)
A:jax.flatten_util.(flat, unravel_list)->vjp(ravel_list, *leaves)
A:jax.flatten_util.pytree_args->unravel_inputs(flat_in)
jax.flatten_util.ravel_fun(unravel_inputs,flat_in,**kwargs)
jax.flatten_util.ravel_list(*lst)
jax.flatten_util.ravel_pytree(pytree)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/util.py----------------------------------------
A:jax.util.n->len(args[0])
A:jax.util.args->list(args)
A:jax.util.dct->dict(dct)
A:jax.util.wrapped->functools.partial(fun, *args, **kwargs)
A:jax.util.stack->list(end_nodes)
A:jax.util.node->childless_nodes.pop()
A:jax.util.visited->set()
A:jax.util.sides->list(map(predicate, xs))
A:jax.util.memoize->fastcache.clru_cache(maxsize=None)
A:jax.util.module_fns->set()
A:jax.util.attr->getattr(module, key)
jax.util.Hashable(self,val)
jax.util.Hashable.__eq__(self,other)
jax.util.Hashable.__hash__(self)
jax.util.Hashable.__init__(self,val)
jax.util.WrapHashably(self,val)
jax.util.WrapHashably.__eq__(self,other)
jax.util.WrapHashably.__hash__(self)
jax.util.WrapHashably.__init__(self,val)
jax.util.cache(max_size=4096)
jax.util.check_toposort(nodes)
jax.util.concatenate(xs)
jax.util.curry(f)
jax.util.get_module_functions(module)
jax.util.partial(fun,*args,**kwargs)
jax.util.partialmethod(functools.partial)
jax.util.partialmethod.__get__(self,instance,owner)
jax.util.prod(xs)
jax.util.safe_map(f,*args)
jax.util.safe_zip(*args)
jax.util.split_dict(dct,names)
jax.util.split_list(args,ns)
jax.util.split_merge(predicate,xs)
jax.util.toposort(end_nodes)
jax.util.unzip2(xys)
jax.util.unzip3(xyzs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/abstract_arrays.py----------------------------------------
A:jax.abstract_arrays.fname->getattr(fun, '__name__', fun)
A:jax.abstract_arrays.self.dtype->numpy.dtype(xla_bridge.canonicalize_dtype(onp.result_type(val)))
A:jax.abstract_arrays._bool_nonzero->concretization_function_error(bool)
A:jax.abstract_arrays._float->concretization_function_error(float)
A:jax.abstract_arrays._int->concretization_function_error(int)
A:jax.abstract_arrays._long->concretization_function_error(long)
A:jax.abstract_arrays._complex->concretization_function_error(complex)
A:jax.abstract_arrays._hex->concretization_function_error(hex)
A:jax.abstract_arrays._oct->concretization_function_error(oct)
A:jax.abstract_arrays.ndim->property(lambda self: len(self.shape))
A:jax.abstract_arrays.size->property(lambda self: prod(self.shape))
A:jax.abstract_arrays.shapestr->','.join(map(str, self.shape))
A:jax.abstract_arrays.self.shape->numpy.shape(val)
A:jax.abstract_arrays.dtype->lib.xla_bridge.canonicalize_dtype(onp.result_type(x))
jax.abstract_arrays.ConcreteArray(self,val)
jax.abstract_arrays.ConcreteArray.__eq__(self,other)
jax.abstract_arrays.ConcreteArray.__hash__(self)
jax.abstract_arrays.ConcreteArray.__init__(self,val)
jax.abstract_arrays.ConcreteArray.at_least_vspace(self)
jax.abstract_arrays.ConcreteArray.join(self,other)
jax.abstract_arrays.ConcreteArray.str_short(self)
jax.abstract_arrays.ShapedArray(self,shape,dtype)
jax.abstract_arrays.ShapedArray.__eq__(self,other)
jax.abstract_arrays.ShapedArray.__hash__(self)
jax.abstract_arrays.ShapedArray.__init__(self,shape,dtype)
jax.abstract_arrays.ShapedArray.__len__(self)
jax.abstract_arrays.ShapedArray._len(self,ignored_tracer)
jax.abstract_arrays.ShapedArray.at_least_vspace(self)
jax.abstract_arrays.ShapedArray.join(self,other)
jax.abstract_arrays.ShapedArray.str_short(self)
jax.abstract_arrays.UnshapedArray(self,dtype)
jax.abstract_arrays.UnshapedArray.__eq__(self,other)
jax.abstract_arrays.UnshapedArray.__hash__(self)
jax.abstract_arrays.UnshapedArray.__init__(self,dtype)
jax.abstract_arrays.UnshapedArray.__ne__(self,other)
jax.abstract_arrays.UnshapedArray.__repr__(self)
jax.abstract_arrays.UnshapedArray.at_least_vspace(self)
jax.abstract_arrays.UnshapedArray.join(self,other)
jax.abstract_arrays.UnshapedArray.str_short(self)
jax.abstract_arrays.concretization_err_msg(fun)
jax.abstract_arrays.concretization_function_error(fun)
jax.abstract_arrays.make_shaped_array(x)
jax.abstract_arrays.raise_to_shaped(aval)
jax.abstract_arrays.zeros_like_array(x)
jax.abstract_arrays.zeros_like_shaped_array(aval)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax_reference.py----------------------------------------
A:jax.lax_reference.quotient->numpy.floor_divide(lhs, rhs)
A:jax.lax_reference.select->numpy.logical_and(onp.sign(lhs) != onp.sign(rhs), onp.remainder(lhs, rhs) != 0)
A:jax.lax_reference.pads->padtype_to_pads(op.shape, dims, strides, padding)
A:jax.lax_reference.(lhs_perm, rhs_perm, out_perm)->_conv_general_permutations(dimension_numbers)
A:jax.lax_reference.padding->padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:], onp.take(rhs.shape, rhs_perm)[2:], window_strides, padding)
A:jax.lax_reference.trans_lhs->transpose(lhs, lhs_perm)
A:jax.lax_reference.trans_rhs->transpose(rhs, rhs_perm)
A:jax.lax_reference.out->numpy.zeros(operand.shape[:2] + tuple(outspace), operand.dtype)
A:jax.lax_reference.new_id->itertools.count()
A:jax.lax_reference.shared_id->next(new_id)
A:jax.lax_reference.out_axis_ids->filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)
A:jax.lax_reference.inshape->tuple((1 if i not in broadcast_dimensions else d for (i, d) in enumerate(shape)))
A:jax.lax_reference.dimensions->frozenset(dimensions)
A:jax.lax_reference.(lo, hi, interior)->zip(*padding_config)
A:jax.lax_reference.outshape->numpy.add(onp.add(onp.add(lo, hi), operand.shape), onp.multiply(interior, onp.subtract(operand.shape, 1)))
A:jax.lax_reference.lhs_slices->tuple((_slice(None, None, step) for step in factors))
A:jax.lax_reference.rhs_slices->tuple((_slice(l if l < 0 else 0, -h if h < 0 else None) for (l, h) in zip(lo, hi)))
A:jax.lax_reference.strides->numpy.ones(len(start_indices)).astype(int)
A:jax.lax_reference.slices->tuple((_slice(abs(lo) if lo < 0 else 0, hi % dim if hi < 0 else None) for ((lo, hi), dim) in zip(pads, onp.shape(arr))))
A:jax.lax_reference.idx->tuple((_slice(start, start + size) for (start, size) in zip(start_indices, slice_sizes)))
A:jax.lax_reference.updated_operand->numpy.copy(operand)
A:jax.lax_reference.reducer->_make_reducer(computation, init_value)
A:jax.lax_reference.view->numpy.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)
A:jax.lax_reference.idxs->list(onp.ix_(*[onp.arange(d) for d in keys.shape]))
A:jax.lax_reference.idxs[dimension]->numpy.argsort(keys, axis=dimension)
A:jax.lax_reference.(view, view_axes, rhs_axes, out_axes)->_conv_view(lhs, rhs.shape, window_strides, pads, 0.0)
A:jax.lax_reference.out_shape->numpy.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
A:jax.lax_reference.lhs->_pad(lhs, [(0, 0)] * 2 + list(pads), pad_value)
A:jax.lax_reference.dim->len(filter_shape)
A:jax.lax_reference.out_strides->numpy.multiply(window_strides, lhs.strides[2:])
A:jax.lax_reference.view_axes->list(range(view.ndim))
A:jax.lax_reference.outspace->numpy.add(operand.shape[2:], onp.multiply(onp.subtract(factors, 1), onp.subtract(operand.shape[2:], 1)))
A:jax.lax_reference.monoid_record->_monoids.get(getattr(py_binop, '__name__'))
A:jax.lax_reference.MonoidRecord->collections.namedtuple('MonoidRecord', ['reducer', 'identity'])
A:jax.lax_reference.result->numpy.full(onp.delete(onp.shape(operand), axis), init_val, dtype=onp.asarray(operand).dtype)
A:jax.lax_reference.out_idx->tuple(onp.delete(idx, axis))
A:jax.lax_reference.result[out_idx]->py_binop(result[out_idx], operand[idx])
jax.lax_reference._conv(lhs,rhs,window_strides,pads)
jax.lax_reference._conv_general_permutations(dimension_numbers)
jax.lax_reference._conv_view(lhs,rhs_shape,window_strides,pads,pad_value)
jax.lax_reference._dilate(operand,factors)
jax.lax_reference._get_max_identity(dt)
jax.lax_reference._get_min_identity(dt)
jax.lax_reference._identity_getter(op)
jax.lax_reference._make_reducer(py_binop,init_val)
jax.lax_reference._pad(arr,pads,pad_value)
jax.lax_reference._reducer_from_pyfunc(py_binop,init_val)
jax.lax_reference.bitcast_convert_type(operand,dtype)
jax.lax_reference.broadcast(operand,sizes)
jax.lax_reference.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.lax_reference.clamp(min,operand,max)
jax.lax_reference.complex(x,y)
jax.lax_reference.concatenate(operands,dimension)
jax.lax_reference.conj(x)
jax.lax_reference.conv(lhs,rhs,window_strides,padding)
jax.lax_reference.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers)
jax.lax_reference.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation)
jax.lax_reference.convert_element_type(operand,dtype)
jax.lax_reference.div(lhs,rhs)
jax.lax_reference.dot_general(lhs,rhs,dimension_numbers)
jax.lax_reference.dynamic_slice(operand,start_indices,slice_sizes)
jax.lax_reference.dynamic_update_slice(operand,update,start_indices)
jax.lax_reference.pad(operand,padding_value,padding_config)
jax.lax_reference.padtype_to_pads(in_shape,filter_shape,window_strides,padding)
jax.lax_reference.reduce(operand,init_value,computation,dimensions)
jax.lax_reference.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.lax_reference.rem(lhs,rhs)
jax.lax_reference.reshape(operand,new_sizes,dimensions=None)
jax.lax_reference.rev(operand,dimensions)
jax.lax_reference.slice(operand,start_indices,limit_indices,strides=None)
jax.lax_reference.sort_key_val(keys,values,dimension=-1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/test_util.py----------------------------------------
A:jax.test_util.atol->max(atol, 0.5)
A:jax.test_util.rtol->max(rtol, 0.1)
A:jax.test_util.close->partial(numpy_close, atol=atol, rtol=rtol)
A:jax.test_util.add->partial(tree_multimap, onp.add)
A:jax.test_util.sub->partial(tree_multimap, onp.subtract)
A:jax.test_util.conj->partial(tree_map, onp.conj)
A:jax.test_util.shape->numpy.shape(x)
A:jax.test_util.dtype->_dtype(x)
A:jax.test_util.delta->scalar_mul(tangents, eps)
A:jax.test_util.f_pos->f(*add(primals, delta))
A:jax.test_util.f_neg->f(*sub(primals, delta))
A:jax.test_util.rng->numpy.random.RandomState(42)
A:jax.test_util.tangent->tree_map(_rand_like, args)
A:jax.test_util.(v_out, t_out)->f_jvp(args, tangent)
A:jax.test_util.v_out_expected->f(*args)
A:jax.test_util.t_out_expected->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util._rand_like->partial(rand_like, onp.random.RandomState(0))
A:jax.test_util.(v_out, vjpfun)->f_vjp(*args)
A:jax.test_util.tangent_out->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util.cotangent->tree_map(_rand_like, v_out)
A:jax.test_util.cotangent_out->conj(vjpfun(conj(cotangent)))
A:jax.test_util.ip->inner_prod(tangent, cotangent_out)
A:jax.test_util.ip_expected->inner_prod(tangent_out, cotangent)
A:jax.test_util.args->args_maker()
A:jax.test_util._check_jvp->partial(check_jvp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util._check_vjp->partial(check_vjp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util.(out_primal_py, vjp_py)->api.vjp(f, *args)
A:jax.test_util.device->device_under_test()
A:jax.test_util.test_name->getattr(test_method, '__name__', '[unknown test]')
A:jax.test_util.flag_value->getattr(FLAGS, flag_name)
A:jax.test_util.NUMPY_SCALAR_SHAPE->_NumpyScalar()
A:jax.test_util.PYTHON_SCALAR_SHAPE->_PythonScalar()
A:jax.test_util.shapestr->','.join((str(dim) for dim in shape))
A:jax.test_util.vals->numpy.where(zeros, 0, vals)
A:jax.test_util.x_ravel->numpy.asarray(x).ravel()
A:jax.test_util.base_rand->rand_default()
A:jax.test_util.dims->_dims_of_shape(shape)
A:jax.test_util.xs->list(xs)
A:jax.test_util.k->min(len(xs), FLAGS.num_generated_cases)
A:jax.test_util.indices->numpy.random.RandomState(42).choice(onp.arange(len(xs)), k, replace=False)
A:jax.test_util.msg->'Arguments x and y not equal to tolerance atol={}, rtol={}:\nx:\n{}\ny:\n{}\n'.format(atol, rtol, x, y)
A:jax.test_util.x_dtype->c128_to_c64(onp.asarray(x).dtype)
A:jax.test_util.y_dtype->c128_to_c64(onp.asarray(y).dtype)
A:jax.test_util.x->numpy.asarray(x)
A:jax.test_util.y->numpy.asarray(y)
A:jax.test_util.python_ans->fun(*args)
A:jax.test_util.cfun->api.jit(wrapped_fun)
A:jax.test_util.monitored_ans->cfun(*args)
A:jax.test_util.compiled_ans->cfun(*args)
A:jax.test_util.numpy_ans->numpy_reference_op(*args)
A:jax.test_util.lax_ans->lax_op(*args)
jax.test_util.JaxTestCase(parameterized.TestCase)
jax.test_util.JaxTestCase._CheckAgainstNumpy(self,numpy_reference_op,lax_op,args_maker,check_dtypes=False,tol=1e-05)
jax.test_util.JaxTestCase._CompileAndCheck(self,fun,args_maker,check_dtypes,rtol=None,atol=None)
jax.test_util.JaxTestCase.assertAllClose(self,x,y,check_dtypes,atol=None,rtol=None)
jax.test_util.JaxTestCase.assertArraysAllClose(self,x,y,check_dtypes,atol=None,rtol=None)
jax.test_util.JaxTestCase.assertDtypesMatch(self,x,y)
jax.test_util.ScalarShape(object)
jax.test_util.ScalarShape.__len__(self)
jax.test_util._NumpyScalar(ScalarShape)
jax.test_util._PythonScalar(ScalarShape)
jax.test_util._cast_to_shape(value,shape,dtype)
jax.test_util._dims_of_shape(shape)
jax.test_util._rand_dtype(rand,shape,dtype,scale=1.0,post=lambdax:x)
jax.test_util.cases_from_gens(*gens)
jax.test_util.cases_from_list(xs)
jax.test_util.check_close(xs,ys,atol=ATOL,rtol=RTOL)
jax.test_util.check_eq(xs,ys)
jax.test_util.check_grads(f,args,order,modes=['fwd','rev'],atol=None,rtol=None,eps=None)
jax.test_util.check_jvp(f,f_jvp,args,atol=ATOL,rtol=RTOL,eps=EPS)
jax.test_util.check_raises(thunk,err_type,msg)
jax.test_util.check_raises_regexp(thunk,err_type,pattern)
jax.test_util.check_vjp(f,f_vjp,args,atol=ATOL,rtol=RTOL,eps=EPS)
jax.test_util.device_under_test()
jax.test_util.dtype_str(dtype)
jax.test_util.format_shape_dtype_string(shape,dtype)
jax.test_util.format_test_name_suffix(opname,shapes,dtypes)
jax.test_util.inner_prod(xs,ys)
jax.test_util.is_sequence(x)
jax.test_util.numerical_jvp(f,primals,tangents,eps=EPS)
jax.test_util.numpy_close(a,b,atol=ATOL,rtol=RTOL,equal_nan=False)
jax.test_util.numpy_eq(x,y)
jax.test_util.rand_bool()
jax.test_util.rand_default()
jax.test_util.rand_int(low,high=None)
jax.test_util.rand_like(rng,x)
jax.test_util.rand_nonzero()
jax.test_util.rand_not_small()
jax.test_util.rand_positive()
jax.test_util.rand_small()
jax.test_util.rand_small_positive()
jax.test_util.rand_some_equal()
jax.test_util.rand_some_inf()
jax.test_util.rand_some_inf_and_nan()
jax.test_util.rand_some_zero()
jax.test_util.rand_uniform(low=0.0,high=1.0)
jax.test_util.scalar_mul(xs,a)
jax.test_util.skip_on_devices(*disabled_devices)
jax.test_util.skip_on_flag(flag_name,skip_value)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/core.py----------------------------------------
A:jax.core.self.constvars->list(constvars)
A:jax.core.self.freevars->list(freevars)
A:jax.core.self.invars->list(invars)
A:jax.core.self.outvars->list(outvars)
A:jax.core.self.eqns->list(eqns)
A:jax.core.self.literals->list(literals)
A:jax.core.self.in_avals->list(in_avals)
A:jax.core.self.out_avals->list(out_avals)
A:jax.core.JaxprEqn->namedtuple('JaxprEqn', ['eqn_id', 'invars', 'outvars', 'primitive', 'bound_subjaxprs', 'params'])
A:jax.core.self.hash->hash((val.item(), val.dtype))
A:jax.core.literalable_types->set()
A:jax.core.top_trace->find_top_trace(args)
A:jax.core.tracers->map(top_trace.full_raise, args)
A:jax.core.out_tracer->find_top_trace(args).process_primitive(self, tracers, kwargs)
A:jax.core.in_vals->map(read, eqn.invars)
A:jax.core.subfuns->map(lu.wrap_init, subfuns)
A:jax.core.ans->max(tracers, key=lambda x: x.trace.level)
A:jax.core.attr->getattr(self.aval, name)
A:jax.core.t->ref(sublevel)
A:jax.core.aval_property->namedtuple('aval_property', ['fget'])
A:jax.core.aval_method->namedtuple('aval_method', ['fun'])
A:jax.core.self.trace_stack->TraceStack()
A:jax.core.trace_state->TraceState()
A:jax.core.level->TraceState().trace_stack.next_level(bottom)
A:jax.core.master->MasterTrace(level, trace_type)
A:jax.core.sublevel->Sublevel(len(trace_state.substack))
A:jax.core.bot->Bot()
A:jax.core.abstract_unit->AbstractUnit()
A:jax.core.unit->Unit()
A:jax.core.unitvar->UnitVar()
A:jax.core.identity_p->Primitive('id')
A:jax.core.outs->map(full_lower, top_trace.process_call(primitive, f, tracers, params))
A:jax.core.params->dict(params_tuple)
A:jax.core.trace->type(ans.trace)(ans.trace.master, cur_sublevel())
A:jax.core.(outs, cur_todo)->type(ans.trace)(ans.trace.master, cur_sublevel()).post_process_call(primitive, outs, params)
A:jax.core.params_tuple->tuple(params.items())
A:jax.core.(f, env_trace_todo)->process_env_traces(f, primitive, level, params_tuple)
A:jax.core.call_p->Primitive('call')
A:jax.core.call->partial(call_bind, call_p)
A:jax.core.env->set()
A:jax.core.read->partial(read_env, env)
A:jax.core.write->partial(write_env, env)
A:jax.core.lhs->print_vars(eqn.outvars)
A:jax.core.pp_subexpr->pp('')
jax.core.AbstractUnit(AbstractValue)
jax.core.AbstractUnit._eq(self,self_traced,other)
jax.core.AbstractUnit.join(self,other)
jax.core.AbstractValue(object)
jax.core.AbstractValue.__repr__(self)
jax.core.AbstractValue.at_least_vspace(self)
jax.core.Bot(AbstractValue)
jax.core.Jaxpr(self,constvars,freevars,invars,outvars,eqns)
jax.core.Jaxpr.__init__(self,constvars,freevars,invars,outvars,eqns)
jax.core.Jaxpr.__str__(self)
jax.core.Literal(self,val)
jax.core.Literal.__eq__(self,other)
jax.core.Literal.__hash__(self)
jax.core.Literal.__init__(self,val)
jax.core.Literal.__repr__(self)
jax.core.MasterTrace(self,level,trace_type)
jax.core.MasterTrace.__eq__(self,other)
jax.core.MasterTrace.__hash__(self)
jax.core.MasterTrace.__init__(self,level,trace_type)
jax.core.MasterTrace.__repr__(self)
jax.core.Primitive(self,name)
jax.core.Primitive.__init__(self,name)
jax.core.Primitive.__repr__(self)
jax.core.Primitive.abstract_eval(self,*args,**kwargs)
jax.core.Primitive.bind(self,*args,**kwargs)
jax.core.Primitive.def_abstract_eval(self,abstract_eval)
jax.core.Primitive.def_custom_bind(self,bind)
jax.core.Primitive.def_impl(self,impl)
jax.core.Primitive.impl(self,*args,**kwargs)
jax.core.Sublevel(int)
jax.core.Trace(self,master,sublevel)
jax.core.Trace.__init__(self,master,sublevel)
jax.core.Trace.__repr__(self)
jax.core.Trace.full_raise(self,val)
jax.core.Trace.lift(self,tracer)
jax.core.Trace.pure(self,val)
jax.core.Trace.sublift(self,tracer)
jax.core.TraceStack(self)
jax.core.TraceStack.__init__(self)
jax.core.TraceStack.__repr__(self)
jax.core.TraceStack.next_level(self,bottom)
jax.core.TraceStack.pop(self,bottom)
jax.core.TraceStack.push(self,val,bottom)
jax.core.TraceState(self)
jax.core.TraceState.__init__(self)
jax.core.Tracer(self,trace)
jax.core.Tracer.__abs__(self)
jax.core.Tracer.__add__(self,other)
jax.core.Tracer.__and__(self,other)
jax.core.Tracer.__array__(self)
jax.core.Tracer.__bool__(self)
jax.core.Tracer.__complex__(self)
jax.core.Tracer.__div__(self,other)
jax.core.Tracer.__divmod__(self,other)
jax.core.Tracer.__eq__(self,other)
jax.core.Tracer.__float__(self)
jax.core.Tracer.__floordiv__(self,other)
jax.core.Tracer.__ge__(self,other)
jax.core.Tracer.__getattr__(self,name)
jax.core.Tracer.__getitem__(self,idx)
jax.core.Tracer.__gt__(self,other)
jax.core.Tracer.__hex__(self)
jax.core.Tracer.__init__(self,trace)
jax.core.Tracer.__int__(self)
jax.core.Tracer.__invert__(self)
jax.core.Tracer.__iter__(self)
jax.core.Tracer.__le__(self,other)
jax.core.Tracer.__len__(self)
jax.core.Tracer.__long__(self)
jax.core.Tracer.__lshift__(self,other)
jax.core.Tracer.__lt__(self,other)
jax.core.Tracer.__matmul__(self,other)
jax.core.Tracer.__mod__(self,other)
jax.core.Tracer.__mul__(self,other)
jax.core.Tracer.__ne__(self,other)
jax.core.Tracer.__neg__(self)
jax.core.Tracer.__nonzero__(self)
jax.core.Tracer.__oct__(self)
jax.core.Tracer.__or__(self,other)
jax.core.Tracer.__pow__(self,other)
jax.core.Tracer.__radd__(self,other)
jax.core.Tracer.__rand__(self,other)
jax.core.Tracer.__rdiv__(self,other)
jax.core.Tracer.__rdivmod__(self,other)
jax.core.Tracer.__repr__(self)
jax.core.Tracer.__rfloordiv__(self,other)
jax.core.Tracer.__rmatmul__(self,other)
jax.core.Tracer.__rmod__(self,other)
jax.core.Tracer.__rmul__(self,other)
jax.core.Tracer.__ror__(self,other)
jax.core.Tracer.__rpow__(self,other)
jax.core.Tracer.__rshift__(self,other)
jax.core.Tracer.__rsub__(self,other)
jax.core.Tracer.__rtruediv__(self,other)
jax.core.Tracer.__rxor__(self,other)
jax.core.Tracer.__setitem__(self,idx,val)
jax.core.Tracer.__sub__(self,other)
jax.core.Tracer.__truediv__(self,other)
jax.core.Tracer.__xor__(self,other)
jax.core.Tracer.aval(self)
jax.core.TypedJaxpr(self,jaxpr,literals,in_avals,out_avals)
jax.core.TypedJaxpr.__init__(self,jaxpr,literals,in_avals,out_avals)
jax.core.TypedJaxpr.__iter__(self)
jax.core.TypedJaxpr.__str__(self)
jax.core.Unit(object)
jax.core.Unit.__repr__(self)
jax.core.UnitVar(object)
jax.core.UnitVar.__repr__(self)
jax.core.apply_todos(todos,outs)
jax.core.call_bind(primitive,f,*args,**params)
jax.core.call_impl(f,*args,**params)
jax.core.check_jaxpr(jaxpr)
jax.core.concrete_aval(x)
jax.core.cur_sublevel()
jax.core.eval_jaxpr(jaxpr,consts,freevar_vals,*args)
jax.core.find_top_trace(xs)
jax.core.full_lower(val)
jax.core.get_aval(x)
jax.core.jaxpr_as_fun(typed_jaxpr,*args)
jax.core.lattice_join(x,y)
jax.core.new_jaxpr_eqn(*args)
jax.core.new_master(trace_type,bottom=False)
jax.core.new_sublevel()
jax.core.pp_jaxpr(jaxpr)
jax.core.process_env_traces(primitive,level,params_tuple,*args)
jax.core.valid_jaxtype(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/ad_util.py----------------------------------------
A:jax.ad_util.add_jaxvals_p->Primitive('add_any')
A:jax.ad_util.zeros_like_p->Primitive('zeros_like')
A:jax.ad_util.zero->Zero()
jax.ad_util.Zero(object)
jax.ad_util.Zero.__repr__(self)
jax.ad_util.add_abstract(xs,ys)
jax.ad_util.add_impl(xs,ys)
jax.ad_util.add_jaxvals(x,y)
jax.ad_util.zeros_like_aval(aval)
jax.ad_util.zeros_like_impl(example)
jax.ad_util.zeros_like_impl_jaxtuple(xs)
jax.ad_util.zeros_like_jaxval(val)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/config.py----------------------------------------
A:jax.config.self.FLAGS->NameSpace(self.read)
A:jax.config.config->Config()
jax.config.Config(self)
jax.config.Config.DEFINE_bool(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_enum(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_integer(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_string(self,name,default,*args,**kwargs)
jax.config.Config.__init__(self)
jax.config.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs)
jax.config.Config.check_exists(self,name)
jax.config.Config.complete_absl_config(self,absl_flags)
jax.config.Config.config_with_absl(self)
jax.config.Config.parse_flags_with_absl(self)
jax.config.Config.read(self,name)
jax.config.Config.update(self,name,val)
jax.config.NameSpace(self,getter)
jax.config.NameSpace.__getattr__(self,name)
jax.config.NameSpace.__init__(self,getter)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/api.py----------------------------------------
A:jax.api._thread_local_state->_ThreadLocalState()
A:jax.api.f->lu.wrap_init(fun)
A:jax.api.(f, dyn_args)->_argnums_partial(f, dyn_argnums, args)
A:jax.api.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax.api.(flat_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(self.fun), in_tree)
A:jax.api.out->interpreters.partial_eval.abstract_eval_fun(fun.call_wrapped, *map(abstractify, args_flat))
A:jax.api.f_jitted.__name__->jitted_name.format(f_jitted.__name__, static_argnums)
A:jax.api.aval->interpreters.xla.abstractify(x)
A:jax.api.(names, sizes)->zip(*axis_env)
A:jax.api.wrapped->lu.wrap_init(fun, kwargs)
A:jax.api.(jax_args, in_tree)->tree_flatten((args, kwargs))
A:jax.api.(jaxtree_fun, out_tree)->pytree_fun_to_jaxtupletree_fun(wrapped, in_trees)
A:jax.api.pvals->map(pv_like, jax_args)
A:jax.api.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(jaxtree_fun, pvals)
A:jax.api.axis_env_->make_axis_env(xla.jaxpr_replicas(jaxpr))
A:jax.api.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, holomorphic=holomorphic)
A:jax.api.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax.api.((_, aux), g)->value_and_grad_f(*args, **kwargs)
A:jax.api.(f_partial, dyn_args)->_argnums_partial(f, argnums, args)
A:jax.api.(ans, vjp_py)->vjp(f_partial, *dyn_args)
A:jax.api.(ans, vjp_py, aux)->vjp(f_partial, *dyn_args, has_aux=True)
A:jax.api.dtype->numpy.result_type(*leaves)
A:jax.api.g->vjp_py(onp.ones((), dtype=dtype))
A:jax.api.pushfwd->partial(jvp, fun, primals)
A:jax.api.(y, jac)->vmap(pushfwd, out_axes=(None, -1))(_std_basis(dyn_args))
A:jax.api.(y, pullback)->vjp(f_partial, *dyn_args)
A:jax.api.jac->tree_map(partial(_unravel_array_into_pytree, y, 0), jac)
A:jax.api.(leaves, _)->tree_flatten(pytree)
A:jax.api.ndim->sum(map(onp.size, leaves))
A:jax.api.flat_basis->numpy.eye(ndim, dtype=dtype)
A:jax.api.(leaves, treedef)->tree_flatten(pytree)
A:jax.api.parts->_split(arr, onp.cumsum(map(onp.size, leaves[:-1])), axis)
A:jax.api.out_flat->interpreters.parallel.papply(flat_fun, axis_name, args_flat, axis_size)
A:jax.api.dummy->tree_unflatten(treedef, [object()] * treedef.num_leaves)
A:jax.api._none_proxy->_NoneProxy()
A:jax.api.(args, in_tree)->tree_flatten((args, kwargs))
A:jax.api.axis_size->_pmap_axis_size(args_flat)
A:jax.api.f_pmapped.__name__->namestr(f_pmapped.__name__, axis_name)
A:jax.api.(chunk_size, leftover)->divmod(axis_size, pxla.unmapped_device_count())
A:jax.api.soft_mapped_fun->interpreters.pxla.split_axis(flat_fun, axis_name, chunk_size)
A:jax.api.reshaped_outs->interpreters.pxla.xla_pmap(soft_mapped_fun, *reshaped_args, axis_name=axis_name, axis_size=num_chunks, devices=None, backend=backend)
A:jax.api.axis_name->_TempAxisName()
A:jax.api.(f, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax.api.(f, out_axes)->interpreters.parallel.papply_transform(f, axis_name, axis_size)
A:jax.api.outs->self.prim.bind(*it.chain(consts, args_flat), jaxpr=jaxpr, in_tree=in_tree, out_tree=out_tree(), num_consts=len(consts))
A:jax.api.(in_specs, in_shapes_tree)->tree_flatten(in_shapes)
A:jax.api.(out_specs, out_shapes_tree)->tree_flatten(out_shape)
A:jax.api.in_specs->map(partial(_remap_ids, unique_ids), in_specs)
A:jax.api.out_specs->map(partial(_remap_ids, unique_ids), out_specs)
A:jax.api.unique_ids->collections.defaultdict(object)
A:jax.api.in_shapes->map(masking.parse_spec, in_shapes)
A:jax.api.padded_env->_bind_shapes(in_shapes, [x.shape for x in args_flat])
A:jax.api.(outs, out_shapes_)->interpreters.masking.mask_fun(flat_fun, logical_env, padded_env, args_flat, in_shapes)
A:jax.api.out_shapes->map(masking.parse_spec, out_shapes)
A:jax.api.(in_shapes, in_tree)->tree_flatten(in_shapes)
A:jax.api.(out_shapes, out_tree)->tree_flatten(out_shape)
A:jax.api.(flat_fun, out_tree_)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.api.out_shapes_->interpreters.masking.shapecheck(flat_fun, in_shapes)
A:jax.api.fun->lu.wrap_init(fun)
A:jax.api.(ps_flat, tree_def)->tree_flatten(primals)
A:jax.api.(ts_flat, tree_def_2)->tree_flatten(tangents)
A:jax.api.(out_primals, out_tangents)->interpreters.ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
A:jax.api.(primals_flat, in_tree)->tree_flatten(primals)
A:jax.api.(out_primals, out_pvals, jaxpr, consts)->interpreters.ad.linearize(jaxtree_fun, *primals_flat)
A:jax.api.out_tree->out_tree()
A:jax.api.out_primal_py->tree_unflatten(out_tree, out_primal)
A:jax.api.primal_avals->list(map(core.get_aval, primals_flat))
A:jax.api.lifted_jvp->partial(lift_linearized, jaxpr, primal_avals, consts, (in_tree, out_tree), out_pvals)
A:jax.api.tangent_avals->list(map(core.get_aval, tangents))
A:jax.api.has_aux->kwargs.pop('has_aux', False)
A:jax.api.(out_primal, out_vjp)->interpreters.ad.vjp(flat_fun, primals_flat)
A:jax.api.(flat_fun, out_aux_trees)->flatten_fun_nokwargs2(fun, in_tree)
A:jax.api.(out_primal, out_vjp, aux)->interpreters.ad.vjp(flat_fun, primals_flat, has_aux=True)
A:jax.api.(out_tree, aux_tree)->out_aux_trees()
A:jax.api.vjp_py->partial(apply_flat_fun_nokwargs, out_vjp, (out_tree, in_tree))
A:jax.api.(jaxpr, _, _)->interpreters.partial_eval.trace_to_jaxpr(jaxtree_fun, pvals)
A:jax.api.jaxpr_maker.__name__->'make_jaxpr({})'.format(jaxpr_maker.__name__)
A:jax.api.dyn_argnums->tuple(dyn_argnums)
A:jax.api.fixed_args->tuple([core.unit if i in dyn_argnums else _wrap_hashably(arg) for (i, arg) in enumerate(args)])
A:jax.api.dyn_args->tuple((args[i] for i in dyn_argnums))
A:jax.api.name->getattr(fun, '__name__', '<unnamed custom_transforms primitive>')
A:jax.api.fun_p->core.Primitive(name)
A:jax.api.(consts, args)->split_list(args, [params['num_consts']])
A:jax.api.(_, args_flat)->split_list(primals, [num_consts])
A:jax.api.(consts_dot, args_dot_flat)->split_list(tangents, [num_consts])
A:jax.api.args->tree_unflatten(params['in_tree'], args_flat)
A:jax.api.args_dot->tree_unflatten(in_tree, args_dot_flat)
A:jax.api.(out, out_dot)->custom_jvp(args, args_dot)
A:jax.api.(out_flat, out_tree)->tree_flatten(out)
A:jax.api.(out_dot_flat, out_tree2)->tree_flatten(out_dot)
A:jax.api.ans->fun(*primals)
A:jax.api.(consts, args_flat)->split_list(consts_and_args, [num_consts])
A:jax.api.(out, vjp)->custom_vjp(*args)
A:jax.api.cts->tree_unflatten(out_tree, cts_flat)
A:jax.api.(args_cts_flat, in_tree2)->tree_flatten(vjp(cts))
A:jax.api.(ans, _)->fun(*args, **kwargs)
A:jax.api.primal_fun->custom_transforms(primal_fun)
A:jax.api.new_fun->custom_transforms(fun)
A:jax.api.(y, jacs)->vmap(pushfwd, out_axes=(None, 0))(_elementwise_std_basis(tangents))
A:jax.api.(flat_tangents, _)->tree_flatten(tangents)
A:jax.api.out_tangent->sum([t * jac for (t, jac) in zip(flat_tangents, jacs)])
A:jax.api.arity->len(leaves)
A:jax.api.dims->map(onp.size, leaves)
A:jax.api.basis_array->numpy.stack([onp.concatenate([onp.ones(dims[j], dtype) if i == j else onp.zeros(dims[j], dtype) for j in range(arity)]) for i in range(arity)])
A:jax.api.id_name->next(id_names)
A:jax.api.(jax_args, in_trees)->unzip2(map(pytree_to_jaxtupletree, args))
A:jax.api.graphviz_maker.__name__->'make_graphviz({})'.format(graphviz_maker.__name__)
A:jax.api.(fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax.api.solution->solve_impl(params)
A:jax.api.(unchecked_zeros, f_jvp)->vjp(func, solution, params)
A:jax.api.grad_solution->tree_map(lambda x: -x, tangent_solve(lambda p: f_jvp(p)[0], f_jvp(grad_params)[1]))
jax.CustomTransformsFunction(self,fun,prim)
jax.CustomTransformsFunction.__repr__(self)
jax.ShapeDtypeStruct(self,shape,dtype)
jax._NoneProxy(object)
jax._TempAxisName(object)
jax._TempAxisName.__repr__(self)
jax._ThreadLocalState(self)
jax._argnums_partial(f,dyn_argnums,args)
jax._argnums_partial_(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax._bind_shapes(shape_exprs,shapes)
jax._check_args(args)
jax._check_callable(fun)
jax._check_custom_transforms_type(name,fun)
jax._check_inexact_input_vjp(x)
jax._check_real_input_jacfwd(x)
jax._check_real_output_jacrev(x)
jax._check_scalar(x)
jax._custom_implicit_solve(solve,tangent_solve)
jax._device_get(x)
jax._dtype(x)
jax._elementwise_std_basis(pytree)
jax._flatten_axes(treedef,axis_tree)
jax._make_graphviz(fun)
jax._papply(fun)
jax._parallelize(fun)
jax._pmap_axis_size(xs)
jax._remap_ids(names,shape_spec)
jax._replace_nones(tuptree)
jax._reshape_merge(x)
jax._reshape_split(num_chunks,x)
jax._shape_spec_consistent(spec,expr)
jax._split(x,indices,axis)
jax._std_basis(pytree)
jax._unravel_array_into_pytree(pytree,axis,arr)
jax._valid_jaxtype(arg)
jax._wrap_hashably(arg)
jax.api.CustomTransformsFunction(self,fun,prim)
jax.api.CustomTransformsFunction.__init__(self,fun,prim)
jax.api.CustomTransformsFunction.__repr__(self)
jax.api.ShapeDtypeStruct(self,shape,dtype)
jax.api.ShapeDtypeStruct.__init__(self,shape,dtype)
jax.api._NoneProxy(object)
jax.api._TempAxisName(object)
jax.api._TempAxisName.__repr__(self)
jax.api._ThreadLocalState(self)
jax.api._ThreadLocalState.__init__(self)
jax.api._argnums_partial(f,dyn_argnums,args)
jax.api._argnums_partial_(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax.api._bind_shapes(shape_exprs,shapes)
jax.api._check_args(args)
jax.api._check_callable(fun)
jax.api._check_custom_transforms_type(name,fun)
jax.api._check_inexact_input_vjp(x)
jax.api._check_real_input_jacfwd(x)
jax.api._check_real_output_jacrev(x)
jax.api._check_scalar(x)
jax.api._custom_implicit_solve(solve,tangent_solve)
jax.api._device_get(x)
jax.api._dtype(x)
jax.api._elementwise_std_basis(pytree)
jax.api._flatten_axes(treedef,axis_tree)
jax.api._make_graphviz(fun)
jax.api._papply(fun)
jax.api._parallelize(fun)
jax.api._pmap_axis_size(xs)
jax.api._remap_ids(names,shape_spec)
jax.api._replace_nones(tuptree)
jax.api._reshape_merge(x)
jax.api._reshape_split(num_chunks,x)
jax.api._shape_spec_consistent(spec,expr)
jax.api._split(x,indices,axis)
jax.api._std_basis(pytree)
jax.api._unravel_array_into_pytree(pytree,axis,arr)
jax.api._valid_jaxtype(arg)
jax.api._wrap_hashably(arg)
jax.api.custom_gradient(fun)
jax.api.custom_transforms(fun)
jax.api.defjvp(fun,*jvprules)
jax.api.defjvp_all(fun,custom_jvp)
jax.api.defvjp(fun,*vjprules)
jax.api.defvjp_all(fun,custom_vjp)
jax.api.device_get(x)
jax.api.device_put(x,device_num=0,backend=None)
jax.api.disable_jit()
jax.api.eval_shape(fun,*args,**kwargs)
jax.api.grad(fun,argnums=0,has_aux=False,holomorphic=False)
jax.api.hessian(fun,argnums=0,holomorphic=False)
jax.api.jacfwd(fun,argnums=0,holomorphic=False)
jax.api.jacrev(fun,argnums=0,holomorphic=False)
jax.api.jarrett(fun)
jax.api.jit(fun,static_argnums=(),device_assignment=None,backend=None)
jax.api.jvp(fun,primals,tangents)
jax.api.lift_linearized(jaxpr,primal_avals,consts,io_tree,out_pvals,*py_args)
jax.api.linearize(fun,*primals)
jax.api.make_jaxpr(fun)
jax.api.mask(fun,in_shapes,out_shape)
jax.api.pmap(fun,axis_name=None,devices=None,backend=None)
jax.api.shapecheck(in_shapes,out_shape,fun)
jax.api.soft_pmap(fun,axis_name=None,backend=None)
jax.api.value_and_grad(fun,argnums=0,has_aux=False,holomorphic=False)
jax.api.vjp(fun,*primals,**kwargs)
jax.api.vmap(fun,in_axes=0,out_axes=0)
jax.api.xla_computation(fun,static_argnums=(),axis_env=None,backend=None)
jax.custom_gradient(fun)
jax.custom_transforms(fun)
jax.defjvp(fun,*jvprules)
jax.defjvp_all(fun,custom_jvp)
jax.defvjp(fun,*vjprules)
jax.defvjp_all(fun,custom_vjp)
jax.device_get(x)
jax.device_put(x,device_num=0,backend=None)
jax.disable_jit()
jax.eval_shape(fun,*args,**kwargs)
jax.grad(fun,argnums=0,has_aux=False,holomorphic=False)
jax.hessian(fun,argnums=0,holomorphic=False)
jax.jacfwd(fun,argnums=0,holomorphic=False)
jax.jacrev(fun,argnums=0,holomorphic=False)
jax.jarrett(fun)
jax.jit(fun,static_argnums=(),device_assignment=None,backend=None)
jax.jvp(fun,primals,tangents)
jax.lift_linearized(jaxpr,primal_avals,consts,io_tree,out_pvals,*py_args)
jax.linearize(fun,*primals)
jax.make_jaxpr(fun)
jax.mask(fun,in_shapes,out_shape)
jax.pmap(fun,axis_name=None,devices=None,backend=None)
jax.shapecheck(in_shapes,out_shape,fun)
jax.soft_pmap(fun,axis_name=None,backend=None)
jax.value_and_grad(fun,argnums=0,has_aux=False,holomorphic=False)
jax.vjp(fun,*primals,**kwargs)
jax.vmap(fun,in_axes=0,out_axes=0)
jax.xla_computation(fun,static_argnums=(),axis_env=None,backend=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/linear_util.py----------------------------------------
A:jax.linear_util.store->Store()
A:jax.linear_util._EMPTY_STORE_VALUE->EmptyStoreValue()
A:jax.linear_util.(ans, aux)->f(*init_args + rest)
A:jax.linear_util.gen->gen(*gen_args + tuple(args), **kwargs)
A:jax.linear_util.(args, kwargs)->next(gen)
A:jax.linear_util.ans->gen(*gen_args + tuple(args), **kwargs).send(ans)
A:jax.linear_util.(gen, out_store)->stack.pop()
A:jax.linear_util.transformation_stack->map(transform_to_str, enumerate(self.transforms))
A:jax.linear_util.out_store->Store()
A:jax.linear_util.(ans, f_prev)->cached_fun_body(f, args)
jax.linear_util.EmptyStoreValue(object)
jax.linear_util.Store(self)
jax.linear_util.Store.__init__(self)
jax.linear_util.Store.__nonzero__(self)
jax.linear_util.Store.store(self,val)
jax.linear_util.Store.val(self)
jax.linear_util.StoreException(Exception)
jax.linear_util.WrappedFun(self,f,transforms,stores,params)
jax.linear_util.WrappedFun.__eq__(self,other)
jax.linear_util.WrappedFun.__hash__(self)
jax.linear_util.WrappedFun.__init__(self,f,transforms,stores,params)
jax.linear_util.WrappedFun.__name__(self)
jax.linear_util.WrappedFun.__repr__(self)
jax.linear_util.WrappedFun.call_wrapped(self,*args,**kwargs)
jax.linear_util.WrappedFun.populate_stores(self,other)
jax.linear_util.WrappedFun.wrap(self,gen,gen_args,out_store)
jax.linear_util.cache(call,max_size=4096)
jax.linear_util.fun_name(f)
jax.linear_util.staged(f,*init_args)
jax.linear_util.thunk(f)
jax.linear_util.transformation(gen,fun,*transformation_args)
jax.linear_util.transformation_with_aux(gen,fun,*transformation_args)
jax.linear_util.wrap_init(f,params={})


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/random.py----------------------------------------
A:jax.random.k1->convert(lax.shift_right_logical(seed, 32))
A:jax.random.k2->convert(lax.bitwise_and(seed, 4294967295))
A:jax.random.nbits->numpy.array(onp.iinfo(dtype).bits, dtype)
A:jax.random.d->lax.sub(alpha, one_over_three)
A:jax.random.rotate_left->_make_rotate_left(lax.dtype(count))
A:jax.random.v[1]->rotate_left(v[1], rot)
A:jax.random.x->normal(subkey, (), dtype=dtype)
A:jax.random.(x, _, _)->lax.fori_loop(0, 5, step, (x, rotate_list(ks), rotations))
A:jax.random.out->np.concatenate(x)
A:jax.random.counts->lax.tie_in(key, lax.iota(onp.uint32, max_count))
A:jax.random.key2->lax.tie_in(key, PRNGKey(data))
A:jax.random.bits->_random_bits(key, nbits, shape)
A:jax.random.shape->tuple(map(int, shape))
A:jax.random.dtype->jax.lib.xla_bridge.canonicalize_dtype(dtype)
A:jax.random.minval->lax.convert_element_type(minval, dtype)
A:jax.random.maxval->lax.max(lax.add(minval, onp.array(1, dtype)), maxval)
A:jax.random.finfo->numpy.finfo(dtype)
A:jax.random.float_bits->lax.bitwise_or(lax.shift_right_logical(bits, onp.array(nbits - nmant, lax.dtype(bits))), onp.array(1.0, dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
A:jax.random.(k1, k2)->split(key)
A:jax.random.span->lax.convert_element_type(maxval - minval, unsigned_dtype)
A:jax.random.multiplier->lax.rem(lax.mul(multiplier, multiplier), span)
A:jax.random.random_offset->lax.rem(random_offset, span)
A:jax.random.num_rounds->int(onp.ceil(exponent * onp.log(x.size) / onp.log(uint32max)))
A:jax.random.(key, subkey)->split(key)
A:jax.random.sort_keys->_random_bits(subkey, 32, x.shape)
A:jax.random.(_, x)->lax.sort_key_val(sort_keys, x, axis)
A:jax.random.lo->numpy.nextafter(onp.array(-1.0, dtype), 0.0, dtype=dtype)
A:jax.random.hi->numpy.array(1.0, dtype)
A:jax.random.u->uniform(key, shape, dtype, minval=-1.0 + np.finfo(dtype).epsneg, maxval=1.0)
A:jax.random.sqrt2->numpy.array(onp.sqrt(2), dtype)
A:jax.random.a->np.broadcast_to(a, shape)
A:jax.random.b->np.broadcast_to(b, shape)
A:jax.random.p->np.broadcast_to(p, shape)
A:jax.random.(key_a, key_b)->split(key)
A:jax.random.gamma_a->gamma(key_a, a, shape, dtype)
A:jax.random.gamma_b->gamma(key_b, b, shape, dtype)
A:jax.random.pi->_constant_like(u, onp.pi)
A:jax.random.alpha->lax.select(lax.ge(alpha, one), alpha, lax.add(alpha, one))
A:jax.random.gamma_samples->gamma(key, alpha, shape + alpha.shape[-1:], dtype)
A:jax.random.zero->_constant_like(alpha, 0)
A:jax.random.one->_constant_like(alpha, 1)
A:jax.random.minus_one->_constant_like(alpha, -1)
A:jax.random.one_over_two->_constant_like(alpha, 0.5)
A:jax.random.one_over_three->_constant_like(alpha, 1.0 / 3.0)
A:jax.random.squeeze_const->_constant_like(alpha, 0.0331)
A:jax.random.boost->lax.select(lax.ge(alpha, one), one, lax.pow(uniform(subkey, (), dtype=dtype), lax.div(one, alpha)))
A:jax.random.c->lax.div(one_over_three, lax.pow(d, one_over_two))
A:jax.random.cond->lax.bitwise_and(lax.ge(U, lax.sub(one, lax.mul(squeeze_const, lax.mul(X, X)))), lax.ge(lax.log(U), lax.add(lax.mul(X, one_over_two), lax.mul(d, lax.add(lax.sub(one, V), lax.log(V))))))
A:jax.random.v->np.log(alpha)
A:jax.random.(key, x_key, U_key)->split(key, 3)
A:jax.random.(_, x, v)->lax.while_loop(lambda kxv: lax.le(kxv[2], zero), _next_kxv, (x_key, zero, minus_one))
A:jax.random.X->lax.mul(x, x)
A:jax.random.V->lax.mul(lax.mul(v, v), v)
A:jax.random.U->uniform(U_key, (), dtype=dtype)
A:jax.random.(_, _, V, _)->lax.while_loop(_cond_fn, _body_fn, (key, zero, one, _constant_like(alpha, 2)))
A:jax.random.z->lax.mul(lax.mul(d, V), boost)
A:jax.random.sqrt_8a->np.sqrt(8 * alpha)
A:jax.random.log_z_div_a->np.log(z / alpha)
A:jax.random.sign->np.where(z < alpha, 1.0, -1.0)
A:jax.random.z_div_a->np.divide(z, alpha)
A:jax.random.grad->np.exp(p / np.maximum(q, 0.01))
A:jax.random.(_, _, grad, flag)->lax.while_loop(lambda zagf: ~zagf[3], _case4, (z, alpha, grad, flag))
A:jax.random.samples->vmap(_gamma_one)(keys, alphas)
A:jax.random.alphas->np.reshape(a, -1)
A:jax.random.grads->vmap(_gamma_grad_one)(samples, alphas)
A:jax.random.keys->split(key, onp.size(alphas))
A:jax.random.e->exponential(key, shape, dtype)
A:jax.random.df->lax.convert_element_type(df, dtype)
A:jax.random.(key_n, key_g)->split(key)
A:jax.random.n->normal(key_n, shape, dtype)
A:jax.random.two->_constant_like(n, 2)
A:jax.random.half_df->lax.div(df, two)
A:jax.random.g->gamma(key_n, half_df, shape, dtype)
jax.random.PRNGKey(seed)
jax.random._bernoulli(key,p,shape)
jax.random._beta(key,a,b,shape,dtype)
jax.random._bit_stats(bits)
jax.random._cauchy(key,shape,dtype)
jax.random._check_shape(name,shape)
jax.random._dirichlet(key,alpha,shape,dtype)
jax.random._exponential(key,shape,dtype)
jax.random._fold_in(key,data)
jax.random._gamma(key,a,shape,dtype)
jax.random._gamma_grad(sample,a)
jax.random._gamma_grad_one(z,alpha)
jax.random._gamma_impl(key,a)
jax.random._gamma_one(key,alpha)
jax.random._gumbel(key,shape,dtype)
jax.random._is_prng_key(key)
jax.random._laplace(key,shape,dtype)
jax.random._logistic(key,shape,dtype)
jax.random._make_rotate_left(dtype)
jax.random._normal(key,shape,dtype)
jax.random._pareto(key,b,shape,dtype)
jax.random._randint(key,shape,minval,maxval,dtype)
jax.random._random_bits(key,bit_width,shape)
jax.random._shuffle(key,x,axis)
jax.random._split(key,num)
jax.random._t(key,df,shape,dtype)
jax.random._truncated_normal(key,lower,upper,shape,dtype)
jax.random._uniform(key,shape,dtype,minval,maxval)
jax.random.bernoulli(key,p=onp.float32(0.5),shape=())
jax.random.beta(key,a,b,shape=(),dtype=onp.float64)
jax.random.cauchy(key,shape=(),dtype=onp.float64)
jax.random.dirichlet(key,alpha,shape=(),dtype=onp.float64)
jax.random.exponential(key,shape=(),dtype=onp.float64)
jax.random.fold_in(key,data)
jax.random.gamma(key,a,shape=(),dtype=onp.float64)
jax.random.gumbel(key,shape=(),dtype=onp.float64)
jax.random.laplace(key,shape=(),dtype=onp.float64)
jax.random.logistic(key,shape=(),dtype=onp.float64)
jax.random.normal(key,shape=(),dtype=onp.float64)
jax.random.pareto(key,b,shape=(),dtype=onp.float64)
jax.random.randint(key,shape,minval,maxval,dtype=onp.int64)
jax.random.shuffle(key,x,axis=0)
jax.random.split(key,num=2)
jax.random.t(key,df,shape=(),dtype=onp.float64)
jax.random.threefry_2x32(keypair,count)
jax.random.truncated_normal(key,lower,upper,shape=(),dtype=onp.float64)
jax.random.uniform(key,shape=(),dtype=onp.float64,minval=0.0,maxval=1.0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/tree_util.py----------------------------------------
A:jax.tree_util.(leaves, treedef)->lib.pytree.flatten(tree)
A:jax.tree_util.(flat, treedef)->tree_flatten(pytree_to_transpose)
A:jax.tree_util.expected_treedef->outer_treedef.compose(inner_treedef)
A:jax.tree_util.flat->iter(flat)
A:jax.tree_util.transposed_lol->zip(*lol)
A:jax.tree_util.subtrees->map(partial(tree_unflatten, outer_treedef), transposed_lol)
A:jax.tree_util.(_, treedef)->lib.pytree.flatten(tree)
jax.tree_util.Partial(functools.partial)
jax.tree_util.build_tree(treedef,xs)
jax.tree_util.process_pytree(process_node,tree)
jax.tree_util.tree_all(tree)
jax.tree_util.tree_leaves(tree)
jax.tree_util.tree_map(f,tree)
jax.tree_util.tree_multimap(f,tree,*rest)
jax.tree_util.tree_reduce(f,tree)
jax.tree_util.tree_structure(tree)
jax.tree_util.tree_transpose(outer_treedef,inner_treedef,pytree_to_transpose)
jax.tree_util.tree_unflatten(treedef,xs)
jax.tree_util.treedef_children(treedef)
jax.tree_util.treedef_is_leaf(treedef)
jax.tree_util.treedef_tuple(trees)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax_linalg.py----------------------------------------
A:jax.lax_linalg.x->jax.interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax_linalg.(w, vl, vr)->Primitive('eig').bind(x)
A:jax.lax_linalg.(v, w)->Primitive('eigh').bind(symmetrize(a), lower=lower)
A:jax.lax_linalg.(lu, pivots)->Primitive('lu').bind(a)
A:jax.lax_linalg.(q, r)->Primitive('qr').bind(x, full_matrices=False)
A:jax.lax_linalg.(s, u, v)->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.t->f(c, *args, **kwargs)
A:jax.lax_linalg.L->jax.numpy.lax_numpy.tril(cholesky_p.bind(x))
A:jax.lax_linalg.l->jax.lax.pad(np.tril(lu[..., :, :k], -1), zero, l_padding)
A:jax.lax_linalg.tmp->triangular_solve(a, g_a, left_side, lower, transpose_a, conjugate_a, unit_diagonal)
A:jax.lax_linalg.L_dot->jax.lax.batch_matmul(L, phi(triangular_solve(L, tmp, left_side=True, transpose_a=False, lower=True)))
A:jax.lax_linalg.cholesky_p->standard_unop(_float | _complex, 'cholesky')
A:jax.lax_linalg.shape->c.GetShape(operand)
A:jax.lax_linalg.dtype->jax.lax.dtype(a)
A:jax.lax_linalg.nan->c.Constant(onp.array(onp.nan, dtype=dtype))
A:jax.lax_linalg.(result, info)->jax.lib.lapack.potrf(c, operand, lower=True)
A:jax.lax_linalg.vlvr->ShapedArray(batch_dims + (n, n), dtype)
A:jax.lax_linalg.w->w.astype(a.dtype).astype(a.dtype)
A:jax.lax_linalg.(w, vl, vr, info)->_cpu_geev(c, operand)
A:jax.lax_linalg.ok->c.Eq(info, c.ConstantS32Scalar(0))
A:jax.lax_linalg.vl->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vl, _nan_like(c, vl))
A:jax.lax_linalg.vr->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vr, _nan_like(c, vr))
A:jax.lax_linalg.eig_p->Primitive('eig')
A:jax.lax_linalg.v->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), v, _nan_like(c, v))
A:jax.lax_linalg.(v, w, info)->syevd_impl(c, operand, lower=lower)
A:jax.lax_linalg.eye_n->jax.numpy.lax_numpy.eye(a.shape[-1], dtype=a.dtype)
A:jax.lax_linalg.vdag_adot_v->dot(dot(_H(v), a_dot), v)
A:jax.lax_linalg.dv->dot(v, np.multiply(Fmat, vdag_adot_v))
A:jax.lax_linalg.dw->jax.numpy.lax_numpy.diagonal(vdag_adot_v)
A:jax.lax_linalg.eigh_p->Primitive('eigh')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, _cpu_syevd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, cusolver.syevd)
A:jax.lax_linalg.triangular_solve_dtype_rule->partial(binop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex), 'triangular_solve')
A:jax.lax_linalg.g_a->jax.lax.neg(g_a)
A:jax.lax_linalg.cotangent_b->triangular_solve(a, cotangent, left_side, lower, not transpose_a, conjugate_a, unit_diagonal)
A:jax.lax_linalg.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax.lax_linalg.y->jax.interpreters.batching.bdim_at_front(y, by, size)
A:jax.lax_linalg.triangular_solve_p->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve')
A:jax.lax_linalg.a->jax.ops.index_add(a, ops.index[k + b:, k + b:], -lax.dot(a[k + b:, k:k + b], a[k:k + b, k + b:], precision=lax.Precision.HIGHEST))
A:jax.lax_linalg.dims->c.GetShape(operand).dimensions()
A:jax.lax_linalg.batch->prod(dims[:-2])
A:jax.lax_linalg.m_idx->jax.numpy.lax_numpy.arange(m)
A:jax.lax_linalg.n_idx->jax.numpy.lax_numpy.arange(n)
A:jax.lax_linalg.magnitude->jax.numpy.lax_numpy.abs(a[:, k])
A:jax.lax_linalg.i->jax.numpy.lax_numpy.argmax(np.where(m_idx >= k, magnitude, -np.inf))
A:jax.lax_linalg.pivot->c.Sub(pivot, c.ConstantS32Scalar(1))
A:jax.lax_linalg.perm->jax.numpy.lax_numpy.arange(m, dtype=np.int32)
A:jax.lax_linalg.r->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), r, _nan_like(c, r))
A:jax.lax_linalg.b->min(r - k, block_size)
A:jax.lax_linalg.(block_pivot, perm, lu_block)->_lu_unblocked(a[k:, k:k + b])
A:jax.lax_linalg.batch_size->numpy.prod(batch_dims, dtype=onp.int64)
A:jax.lax_linalg.(pivot, lu)->_lu_blocked(x)
A:jax.lax_linalg.lu->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), lu, _nan_like(c, lu))
A:jax.lax_linalg.(lu, pivot)->jax.interpreters.xla.apply_primitive(lu_p, operand)
A:jax.lax_linalg.a_shape->jax.numpy.lax_numpy.shape(a)
A:jax.lax_linalg.k->min(m, n)
A:jax.lax_linalg.permutation->jax.lax.broadcasted_iota(np.int32, batch_dims + (m,), len(batch_dims))
A:jax.lax_linalg.iotas->jax.numpy.lax_numpy.ix_(*(lax.iota(np.int32, b) for b in batch_dims))
A:jax.lax_linalg.ndims->len(a_shape)
A:jax.lax_linalg.zero->jax.numpy.lax_numpy._constant_like(lu, 0)
A:jax.lax_linalg.u_eye->jax.lax.pad(np.eye(n - k, n - k, dtype=dtype), zero, ((k, 0, 0), (k, 0, 0)))
A:jax.lax_linalg.la->triangular_solve(l, x, left_side=True, transpose_a=False, lower=True, unit_diagonal=True)
A:jax.lax_linalg.lau->triangular_solve(u, la, left_side=False, transpose_a=False, lower=False)
A:jax.lax_linalg.l_dot->jax.numpy.lax_numpy.matmul(l, np.tril(lau, -1))
A:jax.lax_linalg.u_dot->jax.numpy.lax_numpy.matmul(np.triu(lau), u)
A:jax.lax_linalg.(lu, pivot, info)->getrf_impl(c, operand)
A:jax.lax_linalg.lu_p->Primitive('lu')
A:jax.lax_linalg.xla.translations[lu_p]->jax.interpreters.xla.lower_fun(_lu_python, instantiate=True)
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, lapack.getrf)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, cusolver.getrf)
A:jax.lax_linalg.q->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), q, _nan_like(c, q))
A:jax.lax_linalg.dx_rinv->triangular_solve(r, dx)
A:jax.lax_linalg.qt_dx_rinv->jax.numpy.lax_numpy.matmul(_H(q), dx_rinv)
A:jax.lax_linalg.qt_dx_rinv_lower->jax.numpy.lax_numpy.tril(qt_dx_rinv, -1)
A:jax.lax_linalg.dr->jax.numpy.lax_numpy.matmul(qt_dx_rinv - domega, r)
A:jax.lax_linalg.(r, tau, info_geqrf)->geqrf_impl(c, operand)
A:jax.lax_linalg.(q, info_orgqr)->orgqr_impl(c, q, tau)
A:jax.lax_linalg.qr_p->Primitive('qr')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, lapack.geqrf, lapack.orgqr)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, cusolver.geqrf, cusolver.orgqr)
A:jax.lax_linalg.(s, u, vt)->jax.interpreters.xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.s->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1,)), s, _nan_like(c, s))
A:jax.lax_linalg.u->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), u, _nan_like(c, u))
A:jax.lax_linalg.vt->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vt, _nan_like(c, vt))
A:jax.lax_linalg.(s, U, Vt)->Primitive('svd').bind(A, full_matrices=False, compute_uv=True)
A:jax.lax_linalg.dS->jax.numpy.lax_numpy.matmul(np.matmul(Ut, dA), V)
A:jax.lax_linalg.ds->jax.numpy.lax_numpy.real(np.diagonal(dS, 0, -2, -1))
A:jax.lax_linalg.dU->jax.numpy.lax_numpy.matmul(U, F * (dSS + _T(dSS)))
A:jax.lax_linalg.dV->jax.numpy.lax_numpy.matmul(V, F * (SdS + _T(SdS)))
A:jax.lax_linalg.(s, u, vt, info)->gesvd_impl(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.outs->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.svd_p->Primitive('svd')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, lapack.gesdd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, cusolver.gesvd)
jax.lax_linalg._H(x)
jax.lax_linalg._T(x)
jax.lax_linalg._eigh_cpu_gpu_translation_rule(syevd_impl,c,operand,lower)
jax.lax_linalg._lu_abstract_eval(operand)
jax.lax_linalg._lu_batching_rule(batched_args,batch_dims)
jax.lax_linalg._lu_blocked(a,block_size=32)
jax.lax_linalg._lu_cpu_gpu_translation_rule(getrf_impl,c,operand)
jax.lax_linalg._lu_impl(operand)
jax.lax_linalg._lu_jvp_rule(primals,tangents)
jax.lax_linalg._lu_python(x)
jax.lax_linalg._lu_unblocked(a)
jax.lax_linalg._nan_like(c,operand)
jax.lax_linalg._qr_cpu_gpu_translation_rule(geqrf_impl,orgqr_impl,c,operand,full_matrices)
jax.lax_linalg._svd_cpu_gpu_translation_rule(gesvd_impl,c,operand,full_matrices,compute_uv)
jax.lax_linalg._triangular_solve_cpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._triangular_solve_gpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._unpack_tuple(f,n)
jax.lax_linalg.cholesky(x,symmetrize_input=True)
jax.lax_linalg.cholesky_batching_rule(batched_args,batch_dims)
jax.lax_linalg.cholesky_cpu_translation_rule(c,operand)
jax.lax_linalg.cholesky_jvp_rule(primals,tangents)
jax.lax_linalg.eig(x)
jax.lax_linalg.eig_abstract_eval(operand)
jax.lax_linalg.eig_batching_rule(batched_args,batch_dims)
jax.lax_linalg.eig_cpu_translation_rule(c,operand)
jax.lax_linalg.eig_impl(operand)
jax.lax_linalg.eig_translation_rule(c,operand)
jax.lax_linalg.eigh(x,lower=True,symmetrize_input=True)
jax.lax_linalg.eigh_abstract_eval(operand,lower)
jax.lax_linalg.eigh_batching_rule(batched_args,batch_dims,lower)
jax.lax_linalg.eigh_impl(operand,lower)
jax.lax_linalg.eigh_jvp_rule(primals,tangents,lower)
jax.lax_linalg.eigh_translation_rule(c,operand,lower)
jax.lax_linalg.lu(x)
jax.lax_linalg.lu_pivots_to_permutation(swaps,m)
jax.lax_linalg.qr(x,full_matrices=True)
jax.lax_linalg.qr_abstract_eval(operand,full_matrices)
jax.lax_linalg.qr_batching_rule(batched_args,batch_dims,full_matrices)
jax.lax_linalg.qr_impl(operand,full_matrices)
jax.lax_linalg.qr_jvp_rule(primals,tangents,full_matrices)
jax.lax_linalg.qr_translation_rule(c,operand,full_matrices)
jax.lax_linalg.svd(x,full_matrices=True,compute_uv=True)
jax.lax_linalg.svd_abstract_eval(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_batching_rule(batched_args,batch_dims,full_matrices,compute_uv)
jax.lax_linalg.svd_impl(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_jvp_rule(primals,tangents,full_matrices,compute_uv)
jax.lax_linalg.svd_translation_rule(c,operand,full_matrices,compute_uv)
jax.lax_linalg.symmetrize(x)
jax.lax_linalg.triangular_solve(a,b,left_side=False,lower=False,transpose_a=False,conjugate_a=False,unit_diagonal=False)
jax.lax_linalg.triangular_solve_batching_rule(batched_args,batch_dims,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_jvp_rule_a(g_a,ans,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_shape_rule(a,b,left_side=False,**unused_kwargs)
jax.lax_linalg.triangular_solve_transpose_rule(cotangent,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/api_util.py----------------------------------------
A:jax.api_util.fun.__name__->namestr.format(fun=get_name(wrapped))
A:jax.api_util.fun.__module__->get_module(wrapped)
A:jax.api_util.fun.__doc__->docstr.format(fun=get_name(wrapped), doc=get_doc(wrapped), **kwargs)
A:jax.api_util.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(args, in_tree)->tree_flatten(py_args)
A:jax.api_util.ans->fun(*args)
A:jax.api_util.py_args->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax.api_util.(aux_flat, aux_tree)->tree_flatten(aux)
jax.api_util.apply_flat_fun(fun,io_tree,*py_args)
jax.api_util.apply_flat_fun_nokwargs(fun,io_tree,py_args)
jax.api_util.flatten_fun(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs2(in_tree,*args_flat)
jax.api_util.get_doc(fun)
jax.api_util.get_module(fun)
jax.api_util.get_name(fun)
jax.api_util.wraps(wrapped,fun,namestr='{fun}',docstr='{doc}',**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/pprint_util.py----------------------------------------
A:jax.pprint_util.indented_block->rhs.indent(indent + len(s))
A:jax.pprint_util.kv_pairs->vcat([pp('{}='.format(k)) >> pp(v) for (k, v) in kv_pairs])
jax.pprint_util.PrettyPrint(self,lines)
jax.pprint_util.PrettyPrint.__add__(self,rhs)
jax.pprint_util.PrettyPrint.__init__(self,lines)
jax.pprint_util.PrettyPrint.__rshift__(self,rhs)
jax.pprint_util.PrettyPrint.__str__(self)
jax.pprint_util.PrettyPrint.indent(self,indent)
jax.pprint_util.hcat(ps)
jax.pprint_util.pp(s)
jax.pprint_util.pp_kv_pairs(kv_pairs)
jax.pprint_util.print_list(xs)
jax.pprint_util.vcat(ps)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/linalg.py----------------------------------------
A:jax.scipy.linalg.a->numpy.linalg._promote_arg_dtypes(np.asarray(a))
A:jax.scipy.linalg.l->lax_linalg.cholesky(a if lower else np.conj(_T(a)), symmetrize_input=False)
A:jax.scipy.linalg.(c, b)->numpy.linalg._promote_arg_dtypes(np.asarray(c), np.asarray(b))
A:jax.scipy.linalg.c_shape->numpy.lax_numpy.shape(c)
A:jax.scipy.linalg.b_shape->numpy.lax_numpy.shape(b)
A:jax.scipy.linalg.c_ndims->len(c_shape)
A:jax.scipy.linalg.b_ndims->len(b_shape)
A:jax.scipy.linalg.b->lax_linalg.triangular_solve(c, b, left_side=True, lower=lower, transpose_a=lower, conjugate_a=lower)
A:jax.scipy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower)
A:jax.scipy.linalg.lu_shape->numpy.lax_numpy.shape(lu)
A:jax.scipy.linalg.permutation->lax_linalg.lu_pivots_to_permutation(pivots, m)
A:jax.scipy.linalg.x->lax_linalg.triangular_solve(lu, x, left_side=True, lower=True, unit_diagonal=True, transpose_a=True, conjugate_a=conj)
A:jax.scipy.linalg.(lu, pivots)->lax_linalg.lu(a)
A:jax.scipy.linalg.dtype->lax.dtype(a)
A:jax.scipy.linalg.(m, n)->numpy.lax_numpy.shape(a)
A:jax.scipy.linalg.p->numpy.lax_numpy.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
A:jax.scipy.linalg.k->min(m, n)
A:jax.scipy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.scipy.linalg.(a, b)->numpy.linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
A:jax.scipy.linalg.out->lax_linalg.triangular_solve(a, b, left_side=True, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
jax.scipy.linalg._lu_solve(lu,pivots,b,trans)
jax.scipy.linalg.cho_factor(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.cho_solve(c_and_lower,b,overwrite_b=False,check_finite=True)
jax.scipy.linalg.cholesky(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.det(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.eigh(a,b=None,lower=True,eigvals_only=False,overwrite_a=False,overwrite_b=False,turbo=True,eigvals=None,type=1,check_finite=True)
jax.scipy.linalg.inv(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu(a,permute_l=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_factor(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_solve(lu_and_piv,b,trans=0,overwrite_b=False,check_finite=True)
jax.scipy.linalg.qr(a,overwrite_a=False,lwork=None,mode='full',pivoting=False,check_finite=True)
jax.scipy.linalg.solve(a,b,sym_pos=False,lower=False,overwrite_a=False,overwrite_b=False,debug=False,check_finite=True)
jax.scipy.linalg.solve_triangular(a,b,trans=0,lower=False,unit_diagonal=False,overwrite_b=False,debug=None,check_finite=True)
jax.scipy.linalg.svd(a,full_matrices=True,compute_uv=True,overwrite_a=False,check_finite=True,lapack_driver='gesdd')
jax.scipy.linalg.tril(m,k=0)
jax.scipy.linalg.triu(m,k=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/special.py----------------------------------------
A:jax.scipy.special.(x,)->_promote_args_like(osp_special.entr, x)
A:jax.scipy.special.x->numpy.lax_numpy.asarray(x)
A:jax.scipy.special.one->lax._const(x, 1)
A:jax.scipy.special.dims->_reduction_dims(a, axis)
A:jax.scipy.special.shape->numpy.lax_numpy.shape(p)
A:jax.scipy.special.amax->lax.reduce(a, _constant_like(a, -onp.inf), lax.max, dims)
A:jax.scipy.special.amax_singletons->dimadd(amax)
A:jax.scipy.special.out->lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)), _constant_like(a, 0), lax.add, dims)), amax)
A:jax.scipy.special.(x, y)->_promote_args_like(osp_special.xlog1py, x, y)
A:jax.scipy.special.(a,)->_promote_args_like(lambda a: osp_special.multigammaln(a, 1), a)
A:jax.scipy.special.d->lax.convert_element_type(d, lax.dtype(a))
A:jax.scipy.special.constant->lax.mul(lax.mul(lax.mul(_constant_like(a, 0.25), d), lax.sub(d, _constant_like(a, 1))), lax.log(_constant_like(a, onp.pi)))
A:jax.scipy.special.res->numpy.lax_numpy.sum(gammaln(np.expand_dims(a, axis=-1) - lax.div(np.arange(d), _constant_like(a, 2))), axis=-1)
A:jax.scipy.special._LOGNDTR_FLOAT64_LOWER->numpy.array(-20, onp.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_LOWER->numpy.array(-10, onp.float32)
A:jax.scipy.special._LOGNDTR_FLOAT64_UPPER->numpy.array(8, onp.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_UPPER->numpy.array(5, onp.float32)
A:jax.scipy.special.dtype->lax.dtype(x)
A:jax.scipy.special.z->lax.sqrt(dtype(-2.0) * lax.log(sanitized_mcp))
A:jax.scipy.special.y->lax.select(lax.lt(z, half_sqrt_2), dtype(1.0) + lax.erf(w), lax.select(lax.gt(w, dtype(0.0)), dtype(2.0) - lax.erfc(z), lax.erfc(z)))
A:jax.scipy.special.p0->list(reversed([-59.96335010141079, 98.00107541859997, -56.67628574690703, 13.931260938727968, -1.2391658386738125]))
A:jax.scipy.special.q0->list(reversed([1.0, 1.9544885833814176, 4.676279128988815, 86.36024213908905, -225.46268785411937, 200.26021238006066, -82.03722561683334, 15.90562251262117, -1.1833162112133]))
A:jax.scipy.special.p1->list(reversed([4.0554489230596245, 31.525109459989388, 57.16281922464213, 44.08050738932008, 14.684956192885803, 2.1866330685079025, -0.1402560791713545, -0.03504246268278482, -0.0008574567851546854]))
A:jax.scipy.special.q1->list(reversed([1.0, 15.779988325646675, 45.39076351288792, 41.3172038254672, 15.04253856929075, 2.504649462083094, -0.14218292285478779, -0.03808064076915783, -0.0009332594808954574]))
A:jax.scipy.special.p2->list(reversed([3.2377489177694603, 6.915228890689842, 3.9388102529247444, 1.3330346081580755, 0.20148538954917908, 0.012371663481782003, 0.00030158155350823543, 2.6580697468673755e-06, 6.239745391849833e-09]))
A:jax.scipy.special.q2->list(reversed([1.0, 6.02427039364742, 3.6798356385616087, 1.3770209948908132, 0.21623699359449663, 0.013420400608854318, 0.00032801446468212774, 2.8924786474538068e-06, 6.790194080099813e-09]))
A:jax.scipy.special.coeffs->numpy.array(coeffs, dtype)
A:jax.scipy.special.maybe_complement_p->numpy.lax_numpy.where(p > dtype(-onp.expm1(-2.0)), dtype(1.0) - p, p)
A:jax.scipy.special.sanitized_mcp->numpy.lax_numpy.where(maybe_complement_p <= dtype(0.0), np.full(shape, dtype(0.5)), maybe_complement_p)
A:jax.scipy.special.ww->lax.square(w)
A:jax.scipy.special.infinity->numpy.lax_numpy.full(shape, dtype(onp.inf))
A:jax.scipy.special.x_nan_replaced->numpy.lax_numpy.where(p <= dtype(0.0), -infinity, np.where(p >= dtype(1.0), infinity, x))
A:jax.scipy.special.x_2->lax.square(x)
A:jax.scipy.special.even_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special.odd_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special._norm_logpdf_constant->numpy.log(onp.sqrt(2 * onp.pi))
A:jax.scipy.special.neg_half->_constant_like(x, -0.5)
A:jax.scipy.special.log_normalizer->_constant_like(x, _norm_logpdf_constant)
jax.scipy.special._double_factorial(n)
jax.scipy.special._log_ndtr_asymptotic_series(x,series_order)
jax.scipy.special._log_ndtr_lower(x,series_order)
jax.scipy.special._ndtr(x)
jax.scipy.special._ndtri(p)
jax.scipy.special._norm_logpdf(x)
jax.scipy.special.digamma(x)
jax.scipy.special.entr(x)
jax.scipy.special.erf(x)
jax.scipy.special.erfc(x)
jax.scipy.special.erfinv(x)
jax.scipy.special.expit(x)
jax.scipy.special.gammaln(x)
jax.scipy.special.log_ndtr(x,series_order=3)
jax.scipy.special.logit(x)
jax.scipy.special.logsumexp(a,axis=None,b=None,keepdims=False,return_sign=False)
jax.scipy.special.multigammaln(a,d)
jax.scipy.special.ndtr(x)
jax.scipy.special.ndtri(p)
jax.scipy.special.xlog1py(x,y)
jax.scipy.special.xlogy(x,y)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/pareto.py----------------------------------------
A:jax.scipy.stats.pareto.(x, b, loc, scale)->_promote_args_like(osp_stats.pareto.logpdf, x, b, loc, scale)
A:jax.scipy.stats.pareto.one->_constant_like(x, 1)
A:jax.scipy.stats.pareto.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.pareto.normalize_term->lax.log(lax.div(scale, b))
A:jax.scipy.stats.pareto.log_probs->lax.neg(lax.add(normalize_term, lax.mul(lax.add(b, one), lax.log(scaled_x))))
jax.scipy.stats.pareto.logpdf(x,b,loc=0,scale=1)
jax.scipy.stats.pareto.pdf(x,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/cauchy.py----------------------------------------
A:jax.scipy.stats.cauchy.(x, loc, scale)->_promote_args_like(osp_stats.cauchy.logpdf, x, loc, scale)
A:jax.scipy.stats.cauchy.one->_constant_like(x, 1)
A:jax.scipy.stats.cauchy.pi->_constant_like(x, onp.pi)
A:jax.scipy.stats.cauchy.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.cauchy.normalize_term->lax.log(lax.mul(pi, scale))
jax.scipy.stats.cauchy.logpdf(x,loc=0,scale=1)
jax.scipy.stats.cauchy.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/expon.py----------------------------------------
A:jax.scipy.stats.expon.(x, loc, scale)->_promote_args_like(osp_stats.expon.logpdf, x, loc, scale)
A:jax.scipy.stats.expon.log_scale->lax.log(scale)
A:jax.scipy.stats.expon.linear_term->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.expon.log_probs->lax.neg(lax.add(linear_term, log_scale))
jax.scipy.stats.expon.logpdf(x,loc=0,scale=1)
jax.scipy.stats.expon.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/laplace.py----------------------------------------
A:jax.scipy.stats.laplace.(x, loc, scale)->_promote_args_like(osp_stats.laplace.cdf, x, loc, scale)
A:jax.scipy.stats.laplace.two->_constant_like(x, 2)
A:jax.scipy.stats.laplace.linear_term->lax.div(lax.abs(lax.sub(x, loc)), scale)
A:jax.scipy.stats.laplace.half->_constant_like(x, 0.5)
A:jax.scipy.stats.laplace.one->_constant_like(x, 1)
A:jax.scipy.stats.laplace.zero->_constant_like(x, 0)
A:jax.scipy.stats.laplace.diff->lax.div(lax.sub(x, loc), scale)
jax.scipy.stats.laplace.cdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.logpdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/multivariate_normal.py----------------------------------------
A:jax.scipy.stats.multivariate_normal.x->x.astype(cov.dtype).astype(cov.dtype)
A:jax.scipy.stats.multivariate_normal.mean->mean.astype(cov.dtype).astype(cov.dtype)
A:jax.scipy.stats.multivariate_normal.two->_constant_like(x, 2)
A:jax.scipy.stats.multivariate_normal.dim->_constant_like(x, mean.shape[0])
A:jax.scipy.stats.multivariate_normal.det_sig->det(cov).astype(cov.dtype)
A:jax.scipy.stats.multivariate_normal.log_normalizer->lax.log(lax.mul(lax.pow(_constant_like(x, 2 * onp.pi), dim), det_sig))
A:jax.scipy.stats.multivariate_normal.x_2d->x.astype(cov.dtype).astype(cov.dtype).reshape((-1, mean.shape[0]))
A:jax.scipy.stats.multivariate_normal.quadratic->dot(dot(subtract(x, mean), inv(cov)), subtract(x, mean).T).astype(cov.dtype)
jax.scipy.stats.multivariate_normal.logpdf(x,mean,cov)
jax.scipy.stats.multivariate_normal.pdf(x,mean,cov)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/gamma.py----------------------------------------
A:jax.scipy.stats.gamma.(x, a, loc, scale)->_promote_args_like(osp_stats.gamma.logpdf, x, a, loc, scale)
A:jax.scipy.stats.gamma.one->_constant_like(x, 1)
A:jax.scipy.stats.gamma.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.gamma.log_linear_term->lax.sub(lax.mul(lax.sub(a, one), lax.log(y)), y)
A:jax.scipy.stats.gamma.shape_terms->lax.add(gammaln(a), lax.log(scale))
A:jax.scipy.stats.gamma.log_probs->lax.sub(log_linear_term, shape_terms)
jax.scipy.stats.gamma.logpdf(x,a,loc=0,scale=1)
jax.scipy.stats.gamma.pdf(x,a,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/bernoulli.py----------------------------------------
A:jax.scipy.stats.bernoulli.(k, p, loc)->numpy.lax_numpy._promote_args_like(osp_stats.bernoulli.logpmf, k, p, loc)
A:jax.scipy.stats.bernoulli.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.bernoulli.one->numpy.lax_numpy._constant_like(k, 1)
A:jax.scipy.stats.bernoulli.x->lax.sub(k, loc)
jax.scipy.stats.bernoulli.logpmf(k,p,loc=0)
jax.scipy.stats.bernoulli.pmf(k,p,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/dirichlet.py----------------------------------------
A:jax.scipy.stats.dirichlet.x_sum->numpy.lax_numpy.sum(x, axis=-1)
A:jax.scipy.stats.dirichlet.to_dtype->lax.dtype(osp_stats.dirichlet.logpdf(*args))
A:jax.scipy.stats.dirichlet.one->numpy.lax_numpy._constant_like(x, 1)
A:jax.scipy.stats.dirichlet.log_probs->lax.sub(np.sum(xlogy(lax.sub(alpha, one), x), axis=-1), normalize_term)
jax.scipy.stats.dirichlet._is_simplex(x)
jax.scipy.stats.dirichlet.logpdf(x,alpha)
jax.scipy.stats.dirichlet.pdf(x,alpha)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/uniform.py----------------------------------------
A:jax.scipy.stats.uniform.(x, loc, scale)->_promote_args_like(osp_stats.uniform.logpdf, x, loc, scale)
A:jax.scipy.stats.uniform.log_probs->lax.neg(lax.log(scale))
jax.scipy.stats.uniform.logpdf(x,loc=0,scale=1)
jax.scipy.stats.uniform.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/t.py----------------------------------------
A:jax.scipy.stats.t.(x, df, loc, scale)->_promote_args_like(osp_stats.t.logpdf, x, df, loc, scale)
A:jax.scipy.stats.t.two->_constant_like(x, 2)
A:jax.scipy.stats.t.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.t.df_over_two->lax.div(df, two)
A:jax.scipy.stats.t.df_plus_one_over_two->lax.add(df_over_two, _constant_like(x, 0.5))
A:jax.scipy.stats.t.normalize_term_const->lax.mul(lax.mul(scale, scale), _constant_like(x, onp.pi))
A:jax.scipy.stats.t.normalize_term_tmp->lax.div(lax.log(lax.mul(normalize_term_const, df)), two)
A:jax.scipy.stats.t.normalize_term->lax.sub(lax.add(lax.lgamma(df_over_two), normalize_term_tmp), lax.lgamma(df_plus_one_over_two))
A:jax.scipy.stats.t.quadratic->lax.div(lax.mul(scaled_x, scaled_x), df)
jax.scipy.stats.t.logpdf(x,df,loc=0,scale=1)
jax.scipy.stats.t.pdf(x,df,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/beta.py----------------------------------------
A:jax.scipy.stats.beta.(x, a, b, loc, scale)->_promote_args_like(osp_stats.beta.logpdf, x, a, b, loc, scale)
A:jax.scipy.stats.beta.one->_constant_like(x, 1)
A:jax.scipy.stats.beta.shape_term_tmp->lax.add(gammaln(a), gammaln(b))
A:jax.scipy.stats.beta.shape_term->lax.sub(gammaln(lax.add(a, b)), shape_term_tmp)
A:jax.scipy.stats.beta.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.beta.log_linear_term->lax.add(lax.mul(lax.sub(a, one), lax.log(y)), lax.mul(lax.sub(b, one), lax.log1p(lax.neg(y))))
A:jax.scipy.stats.beta.log_probs->lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
jax.scipy.stats.beta.logpdf(x,a,b,loc=0,scale=1)
jax.scipy.stats.beta.pdf(x,a,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/scipy/stats/norm.py----------------------------------------
A:jax.scipy.stats.norm.(x, loc, scale)->_promote_args_like(osp_stats.norm.logcdf, x, loc, scale)
A:jax.scipy.stats.norm.two->_constant_like(x, 2)
A:jax.scipy.stats.norm.scale_sqrd->lax.pow(scale, two)
A:jax.scipy.stats.norm.log_normalizer->lax.log(lax.mul(_constant_like(x, 2 * onp.pi), scale_sqrd))
A:jax.scipy.stats.norm.quadratic->lax.div(lax.pow(lax.sub(x, loc), two), scale_sqrd)
jax.scipy.stats.norm.cdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logcdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logpdf(x,loc=0,scale=1)
jax.scipy.stats.norm.pdf(x,loc=0,scale=1)
jax.scipy.stats.norm.ppf(q,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lib/xla_bridge.py----------------------------------------
A:jax.lib.xla_bridge.device_assignment->xla_client.DeviceAssignment.create(device_assignment)
A:jax.lib.xla_bridge.backend->_backends.get(FLAGS.jax_xla_backend)
A:jax.lib.xla_bridge.tf_context->xrt.get_tf_context(FLAGS.jax_backend_target, worker)
A:jax.lib.xla_bridge._backend_lock->threading.Lock()
A:jax.lib.xla_bridge.dtype->numpy.dtype(dtype)
A:jax.lib.xla_bridge.value->normalize_to_xla_dtypes(value)
A:jax.lib.xla_bridge.example_value->numpy.asarray(example_value)
A:jax.lib.xla_bridge.py_type->type(py_val)
A:jax.lib.xla_bridge.(zero_stride_axes,)->numpy.where(onp.equal(0, val.strides))
A:jax.lib.xla_bridge.(other_axes,)->numpy.where(onp.not_equal(0, val.strides))
A:jax.lib.xla_bridge.xla_val->c.Broadcast(c.NumpyArrayConstant(collapsed_val, canonicalize_types), onp.take(val.shape, zero_stride_axes))
A:jax.lib.xla_bridge.permutation->numpy.argsort(tuple(zero_stride_axes) + tuple(other_axes))
jax.lib.xla_bridge._JaxComputationBuilder(xla_client.ComputationBuilder)
jax.lib.xla_bridge._JaxComputationBuilder.AllToAll(self,operand,split_axis,concat_axis,replica_groups)
jax.lib.xla_bridge._JaxComputationBuilder.Build(self,*args,**kwargs)
jax.lib.xla_bridge._JaxComputationBuilder.Constant(self,py_val,canonicalize_types=True)
jax.lib.xla_bridge._JaxComputationBuilder.ConstantLike(self,example_value,value,canonicalize_types=True)
jax.lib.xla_bridge._JaxComputationBuilder.CrossReplicaSum(self,operand,replica_groups)
jax.lib.xla_bridge._JaxComputationBuilder.NumpyArrayConstant(self,value,canonicalize_types=True)
jax.lib.xla_bridge._JaxComputationBuilder.Parameter(self,value,name=None,parameter_num=None)
jax.lib.xla_bridge._get_local_backend(platform=None)
jax.lib.xla_bridge._get_xrt_backend(platform=None)
jax.lib.xla_bridge._ndarray_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge._scalar_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge.canonicalize_dtype(dtype)
jax.lib.xla_bridge.device_count(backend=None)
jax.lib.xla_bridge.device_ordinal(buf)
jax.lib.xla_bridge.devices(backend=None)
jax.lib.xla_bridge.dtype_to_etype(dtype)
jax.lib.xla_bridge.get_backend(platform=None)
jax.lib.xla_bridge.get_compile_options(num_replicas=None,device_assignment=None)
jax.lib.xla_bridge.host_id(backend=None)
jax.lib.xla_bridge.local_device_count(backend=None)
jax.lib.xla_bridge.make_computation_builder(name)
jax.lib.xla_bridge.normalize_to_xla_dtypes(val)
jax.lib.xla_bridge.register_backend(name,factory)
jax.lib.xla_bridge.register_constant_handler(type_,handler_fun)
jax.lib.xla_bridge.shape_of(value)
jax.lib.xla_bridge.supported_numpy_dtypes()


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lib/__init__.py----------------------------------------
A:jax.lib.__init__.version->tuple((int(x) for x in jaxlib_version.__version__.split('.')))
jax.lib.__init__._check_jaxlib_jaxlib_version()
jax.lib.__init__._check_jaxlib_version()


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/tools/jax_to_hlo.py----------------------------------------
A:jax.tools.jax_to_hlo.shape_with_default_layout->jax.lib.xla_client.Shape.array_shape(shape.xla_element_type(), shape.dimensions()).with_major_to_minor_layout_if_absent()
A:jax.tools.jax_to_hlo.fn_curried->functools.partial(fn, **constants)
A:jax.tools.jax_to_hlo.comp->jax.api.xla_computation(ordered_wrapper)(*args)
A:jax.tools.jax_to_hlo.(module_name, fn_name)->FLAGS.fn.rsplit('.', 1)
A:jax.tools.jax_to_hlo.module->importlib.import_module(module_name)
A:jax.tools.jax_to_hlo.fn->getattr(module, fn_name)
A:jax.tools.jax_to_hlo.v->jax.numpy.asarray(v)
A:jax.tools.jax_to_hlo.(hlo_proto, hlo_text)->jax_to_hlo(fn, input_shapes, constants)
jax.tools.jax_to_hlo.jax_to_hlo(fn,input_shapes,constants=None)
jax.tools.jax_to_hlo.main(argv)
jax.tools.jax_to_hlo.set_up_flags()


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/tools/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/lapax.py----------------------------------------
A:jax.experimental.lapax.out->jax.lax.rev(out, *onp.where([row_rev, col_rev]))
A:jax.experimental.lapax.out[i, i]->_cholesky(a[i, i])
A:jax.experimental.lapax.out[i:, i]->solve(out[i, i], a[i:, i])
A:jax.experimental.lapax.out[i, :]->solve(a[i, i], b[i, :])
A:jax.experimental.lapax.out[:, i]->solve(a[i, i], b[:, i])
A:jax.experimental.lapax.out[:, i:]->solve(a[i, i], b[:, i])
A:jax.experimental.lapax.dims->tuple(range(ndarray.ndim))
A:jax.experimental.lapax.self.shape->tuple(onp.floor_divide(ndarray.shape, block_size) + (onp.mod(ndarray.shape, block_size) > 0))
A:jax.experimental.lapax.self.ndarray->_matrix_put(self.ndarray, idx, val.ndarray, self.bs)
A:jax.experimental.lapax.__add__->_make_infix_op(lax.add)
A:jax.experimental.lapax.__sub__->_make_infix_op(lax.sub)
A:jax.experimental.lapax.__mul__->_make_infix_op(lax.batch_matmul)
A:jax.experimental.lapax.__div__->_make_infix_op(lax.div)
A:jax.experimental.lapax.__truediv__->_make_infix_op(lax.div)
A:jax.experimental.lapax.T->property(_make_infix_op(_matrix_transpose))
A:jax.experimental.lapax.idx_elt->slice(idx_elt, idx_elt + 1, 1)
A:jax.experimental.lapax.indices->tuple(onp.arange(block_dim)[idx_elt])
A:jax.experimental.lapax.end->min(k * (start - step), shape[axis])
A:jax.experimental.lapax.(sli, row_rev)->_canonical_idx(ndarray.shape, idx_i, -2, block_size)
A:jax.experimental.lapax.(slj, col_rev)->_canonical_idx(ndarray.shape, idx_j, -1, block_size)
A:jax.experimental.lapax.val->jax.lax.rev(val, *onp.where([row_rev, col_rev]))
jax.experimental.lapax.LapaxMatrix(self,ndarray,block_size=1)
jax.experimental.lapax.LapaxMatrix.__getitem__(self,idx)
jax.experimental.lapax.LapaxMatrix.__init__(self,ndarray,block_size=1)
jax.experimental.lapax.LapaxMatrix.__setitem__(self,idx,val)
jax.experimental.lapax.LapaxMatrix.bview(self,block_size)
jax.experimental.lapax._canonical_idx(shape,idx_elt,axis,block_size=1)
jax.experimental.lapax._cholesky(a)
jax.experimental.lapax._make_infix_op(fun)
jax.experimental.lapax._matrix_put(ndarray,idx,val,block_size=1)
jax.experimental.lapax._matrix_take(ndarray,idx,block_size=1)
jax.experimental.lapax._matrix_transpose(ndarray)
jax.experimental.lapax._solve_triangular_left(a,b,left_side,lower,trans_a)
jax.experimental.lapax._solve_triangular_right(a,b,left_side,lower,trans_a)
jax.experimental.lapax.cholesky(a,block_size=1)
jax.experimental.lapax.full_like(x,val)
jax.experimental.lapax.solve_triangular(a,b,left_side,lower,trans_a,block_size=1)
jax.experimental.lapax.sqrt(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/ode.py----------------------------------------
A:jax.experimental.ode.alpha->jax.numpy.array([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1.0, 1.0, 0])
A:jax.experimental.ode.beta->jax.numpy.array([[1 / 5, 0, 0, 0, 0, 0, 0], [3 / 40, 9 / 40, 0, 0, 0, 0, 0], [44 / 45, -56 / 15, 32 / 9, 0, 0, 0, 0], [19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729, 0, 0, 0], [9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656, 0, 0], [35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0]])
A:jax.experimental.ode.c_sol->jax.numpy.array([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0])
A:jax.experimental.ode.c_error->jax.numpy.array([35 / 384 - 1951 / 21600, 0, 500 / 1113 - 22642 / 50085, 125 / 192 - 451 / 720, -2187 / 6784 - -12231 / 42400, 11 / 84 - 649 / 6300, -1.0 / 60.0])
A:jax.experimental.ode.dps_c_mid->jax.numpy.array([6025192743 / 30085553152 / 2, 0, 51252292925 / 65400821598 / 2, -2691868925 / 45128329728 / 2, 187940372067 / 1594534317056 / 2, -1776094331 / 19743644256 / 2, 11237099 / 235043384 / 2])
A:jax.experimental.ode.v->jax.numpy.stack([dy0, dy1, y0, y1, y_mid])
A:jax.experimental.ode.a->jax.numpy.dot(np.hstack([-2.0 * dt, 2.0 * dt, np.array([-8.0, -8.0, 16.0])]), v)
A:jax.experimental.ode.b->jax.numpy.dot(np.hstack([5.0 * dt, -3.0 * dt, np.array([18.0, 14.0, -32.0])]), v)
A:jax.experimental.ode.c->jax.numpy.dot(np.hstack([-4.0 * dt, dt, np.array([-11.0, -5.0, 16.0])]), v)
A:jax.experimental.ode.d0->jax.numpy.linalg.norm(y0 / scale)
A:jax.experimental.ode.d1->jax.numpy.linalg.norm(f0 / scale)
A:jax.experimental.ode.h0->jax.numpy.where(np.any(np.asarray([d0 < 1e-05, d1 < 1e-05])), 1e-06, 0.01 * d0 / d1)
A:jax.experimental.ode.f1->fun(y1, t0 + h0)
A:jax.experimental.ode.h1->jax.numpy.where(np.all(np.asarray([d0 <= 1e-15, d1 < 1e-15])), np.maximum(1e-06, h0 * 0.001), (0.01 / np.max(d1 + d2)) ** order_pow)
A:jax.experimental.ode.ft->func(yi, ti)
A:jax.experimental.ode.k->jax.lax.fori_loop(1, 7, _fori_body_fun, jax.ops.index_update(np.zeros((7, f0.shape[0])), jax.ops.index[0, :], f0))
A:jax.experimental.ode.mean_error_ratio->jax.numpy.max(mean_error_ratio)
A:jax.experimental.ode.dfactor->jax.numpy.where(mean_error_ratio < 1, 1.0, dfactor)
A:jax.experimental.ode.err_ratio->jax.numpy.sqrt(mean_error_ratio)
A:jax.experimental.ode.factor->jax.numpy.maximum(1.0 / ifactor, np.minimum(err_ratio ** (1.0 / order) / safety, 1.0 / dfactor))
A:jax.experimental.ode.(cur_y, cur_f, cur_t, dt, last_t, interp_coeff)->jax.lax.while_loop(lambda x: x[2] < t[i], functools.partial(_while_body_fun, func, rtol=rtol, atol=atol), (cur_y, cur_f, cur_t, dt, last_t, interp_coeff))
A:jax.experimental.ode.out_x->jax.numpy.polyval(interp_coeff, relative_output_time)
A:jax.experimental.ode.(next_y, next_f, next_y_error, k)->runge_kutta_step(func, cur_y, cur_f, cur_t, dt)
A:jax.experimental.ode.error_ratios->error_ratio(next_y_error, rtol, atol, cur_y, next_y)
A:jax.experimental.ode.new_interp_coeff->interp_fit_dopri(cur_y, next_y, k, dt)
A:jax.experimental.ode.dt->initial_step_size(func, t[0], y0, 4, rtol, atol, f0)
A:jax.experimental.ode.(new_rav, unravel)->ravel_pytree((next_y, next_f, next_t, dt, cur_t, new_interp_coeff))
A:jax.experimental.ode.(old_rav, _)->ravel_pytree((cur_y, cur_f, cur_t, dt, last_t, interp_coeff))
A:jax.experimental.ode.f0->func(y0, t[0])
A:jax.experimental.ode.interp_coeff->jax.numpy.array([y0] * 5)
A:jax.experimental.ode.(flat_args, unravel_args)->ravel_pytree(args)
A:jax.experimental.ode.state_len->int(np.floor_divide(augmented_state.shape[0] - flat_args.shape[0] - 1, 2))
A:jax.experimental.ode.(dy_dt, vjpfun)->jax.vjp(flat_func, y, t, flat_args)
A:jax.experimental.ode.vjp_cur_t->jax.numpy.dot(flat_func(this_yt, this_t, flat_args), this_gi)
A:jax.experimental.ode.aug_y0->jax.numpy.hstack((this_yt, vjp_y, vjp_t0, vjp_args))
A:jax.experimental.ode.aug_ans->odeint(rev_aug_dynamics, (), aug_y0, this_tarray)
A:jax.experimental.ode.time_vjp_list->jax.ops.index_update(result[-1], -1, result[-3])
A:jax.experimental.ode.vjp_args->jax.numpy.zeros_like(flat_args)
A:jax.experimental.ode.result->jax.lax.fori_loop(0, rev_t.shape[0], _fori_body_fun, (rev_yt, rev_t, rev_tarray, rev_gi, vjp_y, vjp_t0, vjp_args, time_vjp_list))
A:jax.experimental.ode.(flat_x, unravel)->ravel_pytree(x)
A:jax.experimental.ode.dim->len(flat_x)
A:jax.experimental.ode.g->jax.numpy.ones_like(ys)
A:jax.experimental.ode.d->numpy.zeros_like(flat_x)
A:jax.experimental.ode.y0->jax.numpy.array([1.0])
A:jax.experimental.ode.numerical_grad->nd(onearg_odeint, (y0, np.array([t0, t1]), fargs))
A:jax.experimental.ode.ys->odeint(f, fargs, y0, ts, atol=0.001, rtol=0.001)
A:jax.experimental.ode.ode_vjp->grad_odeint(f, fargs)
A:jax.experimental.ode.(exact_grad, _)->ravel_pytree(ode_vjp(g, ys, np.array([t0, t1])))
A:jax.experimental.ode.x->jax.numpy.linspace(*xlimits, num=numticks)
A:jax.experimental.ode.y->jax.numpy.linspace(*ylimits, num=numticks)
A:jax.experimental.ode.(x_mesh, y_mesh)->jax.numpy.meshgrid(x, y)
A:jax.experimental.ode.zs->jax.vmap(func)(y_mesh.ravel(), x_mesh.ravel())
A:jax.experimental.ode.z_mesh->jax.vmap(func)(y_mesh.ravel(), x_mesh.ravel()).reshape(x_mesh.shape)
A:jax.experimental.ode.ts->jax.numpy.linspace(t0, t1, 100)
A:jax.experimental.ode.fig->matplotlib.pyplot.figure(figsize=(8, 6), facecolor='white')
A:jax.experimental.ode.ax->matplotlib.pyplot.figure(figsize=(8, 6), facecolor='white').add_subplot(111, frameon=False)
A:jax.experimental.ode.dydt->jax.numpy.array([omega, -b * omega - c * np.sin(theta)])
A:jax.experimental.ode.start->time.time()
A:jax.experimental.ode.scipy_result->scipy.integrate.odeint(fun, y0, tspace, args)
A:jax.experimental.ode.end->time.time()
A:jax.experimental.ode.jax_result->odeint(fun, args, np.asarray(y0), np.asarray(tspace))
A:jax.experimental.ode.(_, _)->benchmark_odeint(pend, (0.25, 9.8), (onp.pi - 0.1, 0.0), onp.linspace(0.0, 10.0, 101))
jax.experimental.ode.benchmark_odeint(fun,args,y0,tspace)
jax.experimental.ode.error_ratio(error_estimate,rtol,atol,y0,y1)
jax.experimental.ode.fit_4th_order_polynomial(y0,y1,y_mid,dy0,dy1,dt)
jax.experimental.ode.grad_odeint(ofunc,args)
jax.experimental.ode.initial_step_size(fun,t0,y0,order,rtol,atol,f0)
jax.experimental.ode.interp_fit_dopri(y0,y1,k,dt)
jax.experimental.ode.odeint(ofunc,args,y0,t,rtol=1.4e-08,atol=1.4e-08)
jax.experimental.ode.optimal_step_size(last_step,mean_error_ratio,safety=0.9,ifactor=10.0,dfactor=0.2,order=5.0)
jax.experimental.ode.pend(y,t,b,c)
jax.experimental.ode.pend_benchmark_odeint()
jax.experimental.ode.plot_demo()
jax.experimental.ode.plot_gradient_field(ax,func,xlimits,ylimits,numticks=30)
jax.experimental.ode.runge_kutta_step(func,y0,f0,t0,dt)
jax.experimental.ode.test_grad_odeint()


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/stax.py----------------------------------------
A:jax.experimental.stax.(k1, k2)->jax.random.split(rng)
A:jax.experimental.stax.filter_shape_iter->iter(filter_shape)
A:jax.experimental.stax.output_shape->jax.lax.conv_transpose_shape_tuple(input_shape, kernel_shape, strides, padding, dimension_numbers)
A:jax.experimental.stax.bias_shape->tuple(itertools.dropwhile(lambda x: x == 1, bias_shape))
A:jax.experimental.stax.Conv->functools.partial(GeneralConv, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.Conv1DTranspose->functools.partial(GeneralConvTranspose, ('NHC', 'HIO', 'NHC'))
A:jax.experimental.stax.ConvTranspose->functools.partial(GeneralConvTranspose, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.shape->tuple((d for (i, d) in enumerate(input_shape) if i not in axis))
A:jax.experimental.stax.ed->tuple((None if i in axis else slice(None) for i in range(np.ndim(x))))
A:jax.experimental.stax.z->normalize(x, axis, epsilon=epsilon)
A:jax.experimental.stax.Tanh->elementwise(np.tanh)
A:jax.experimental.stax.Relu->elementwise(relu)
A:jax.experimental.stax.Exp->elementwise(np.exp)
A:jax.experimental.stax.LogSoftmax->elementwise(log_softmax, axis=-1)
A:jax.experimental.stax.Softmax->elementwise(softmax, axis=-1)
A:jax.experimental.stax.Softplus->elementwise(softplus)
A:jax.experimental.stax.Sigmoid->elementwise(sigmoid)
A:jax.experimental.stax.Elu->elementwise(elu)
A:jax.experimental.stax.LeakyRelu->elementwise(leaky_relu)
A:jax.experimental.stax.Selu->elementwise(selu)
A:jax.experimental.stax.Gelu->elementwise(gelu)
A:jax.experimental.stax.out_shape->jax.lax.reduce_window_shape_tuple(input_shape, dims, strides, padding)
A:jax.experimental.stax.out->jax.lax.reduce_window(inputs, init_val, reducer, dims, strides, padding)
A:jax.experimental.stax.MaxPool->_pooling_layer(lax.max, -np.inf)
A:jax.experimental.stax.SumPool->_pooling_layer(lax.add, 0.0)
A:jax.experimental.stax.one->jax.numpy.ones(inputs.shape[1:-1], dtype=inputs.dtype)
A:jax.experimental.stax.window_sizes->jax.lax.reduce_window(one, 0.0, lax.add, dims, strides, padding)
A:jax.experimental.stax.AvgPool->_pooling_layer(lax.add, 0.0, _normalize_by_window_size)
A:jax.experimental.stax.Flatten->Flatten()
A:jax.experimental.stax.Identity->Identity()
A:jax.experimental.stax.FanInSum->FanInSum()
A:jax.experimental.stax.concat_size->sum((shape[ax] for shape in input_shape))
A:jax.experimental.stax.rng->kwargs.pop('rng', None)
A:jax.experimental.stax.keep->jax.random.bernoulli(rng, rate, inputs.shape)
A:jax.experimental.stax.nlayers->len(layers)
A:jax.experimental.stax.(init_funs, apply_funs)->zip(*layers)
A:jax.experimental.stax.(rng, layer_rng)->jax.random.split(rng)
A:jax.experimental.stax.(input_shape, param)->init_fun(layer_rng, input_shape)
A:jax.experimental.stax.inputs->fun(param, inputs, rng=rng, **kwargs)
A:jax.experimental.stax.rngs->jax.random.split(rng, nlayers)
jax.experimental.stax.BatchNorm(axis=(0,1,2),epsilon=1e-05,center=True,scale=True,beta_init=zeros,gamma_init=ones)
jax.experimental.stax.Dense(out_dim,W_init=glorot_normal(),b_init=normal())
jax.experimental.stax.Dropout(rate,mode='train')
jax.experimental.stax.FanInConcat(axis=-1)
jax.experimental.stax.FanInSum()
jax.experimental.stax.FanOut(num)
jax.experimental.stax.Flatten()
jax.experimental.stax.GeneralConv(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.GeneralConvTranspose(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.Identity()
jax.experimental.stax._normalize_by_window_size(dims,strides,padding)
jax.experimental.stax._pooling_layer(reducer,init_val,rescaler=None)
jax.experimental.stax.elementwise(fun,**fun_kwargs)
jax.experimental.stax.parallel(*layers)
jax.experimental.stax.serial(*layers)
jax.experimental.stax.shape_dependent(make_layer)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/optimizers.py----------------------------------------
A:jax.experimental.optimizers.OptimizerState->namedtuple('OptimizerState', ['packed_state', 'tree_def', 'subtree_defs'])
A:jax.experimental.optimizers.(init, update, get_params)->opt_maker(*args, **kwargs)
A:jax.experimental.optimizers.(x0_flat, tree)->tree_flatten(x0_tree)
A:jax.experimental.optimizers.(states_flat, subtrees)->unzip2(map(tree_flatten, initial_states))
A:jax.experimental.optimizers.packed_state->pack(map(pack, states_flat))
A:jax.experimental.optimizers.(grad_flat, tree2)->tree_flatten(grad_tree)
A:jax.experimental.optimizers.states->map(tree_unflatten, subtrees, packed_state)
A:jax.experimental.optimizers.new_states->map(partial(update, i), grad_flat, states)
A:jax.experimental.optimizers.(new_states_flat, subtrees2)->unzip2(map(tree_flatten, new_states))
A:jax.experimental.optimizers.new_packed_state->pack(map(pack, new_states_flat))
A:jax.experimental.optimizers.params->map(get_params, states)
A:jax.experimental.optimizers.step_size->make_schedule(step_size)
A:jax.experimental.optimizers.v0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq_inv_sqrt->jax.numpy.where(g_sq > 0, 1.0 / np.sqrt(g_sq), 0.0)
A:jax.experimental.optimizers.avg_sq_grad->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.mom->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.lst->list(seq)
A:jax.experimental.optimizers.idx->splice([None] * ndim, axis, [slice(None)])
A:jax.experimental.optimizers.accum_inv_sqrt->jax.numpy.where(accum > 0, 1.0 / np.sqrt(accum), 0)
A:jax.experimental.optimizers.step_num->jax.numpy.minimum(step_num, decay_steps)
A:jax.experimental.optimizers.boundaries->jax.numpy.array(boundaries)
A:jax.experimental.optimizers.values->jax.numpy.array(values)
A:jax.experimental.optimizers.(leaves, _)->tree_flatten(tree)
A:jax.experimental.optimizers.norm->l2_norm(grad_tree)
A:jax.experimental.optimizers.subtrees->map(tree_unflatten, subtree_defs, packed_state)
A:jax.experimental.optimizers.(sentinels, tree_def)->tree_flatten(marked_pytree)
A:jax.experimental.optimizers.(states_flat, subtree_defs)->unzip2(map(tree_flatten, subtrees))
jax.experimental.optimizers.JoinPoint(self,subtree)
jax.experimental.optimizers.JoinPoint.__init__(self,subtree)
jax.experimental.optimizers.JoinPoint.__iter__(self)
jax.experimental.optimizers.adagrad(step_size,momentum=0.9)
jax.experimental.optimizers.adam(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optimizers.clip_grads(grad_tree,max_norm)
jax.experimental.optimizers.constant(step_size)
jax.experimental.optimizers.exponential_decay(step_size,decay_steps,decay_rate)
jax.experimental.optimizers.inverse_time_decay(step_size,decay_steps,decay_rate,staircase=False)
jax.experimental.optimizers.l2_norm(tree)
jax.experimental.optimizers.make_schedule(scalar_or_schedule)
jax.experimental.optimizers.momentum(step_size,mass)
jax.experimental.optimizers.optimizer(opt_maker)
jax.experimental.optimizers.pack_optimizer_state(marked_pytree)
jax.experimental.optimizers.piecewise_constant(boundaries,values)
jax.experimental.optimizers.polynomial_decay(step_size,decay_steps,final_step_size,power=1.0)
jax.experimental.optimizers.rmsprop(step_size,gamma=0.9,eps=1e-08)
jax.experimental.optimizers.rmsprop_momentum(step_size,gamma=0.9,eps=1e-08,momentum=0.9)
jax.experimental.optimizers.sgd(step_size)
jax.experimental.optimizers.sm3(step_size,momentum=0.9)
jax.experimental.optimizers.unpack_optimizer_state(opt_state)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/vectorize.py----------------------------------------
A:jax.experimental.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax.experimental.vectorize._ARGUMENT->'\\({}\\)'.format(_CORE_DIMENSION_LIST)
A:jax.experimental.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax.experimental.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax.experimental.vectorize.num_core_dims->len(core_dims)
A:jax.experimental.vectorize.dummy_array->numpy.lib.stride_tricks.as_strided(0, arg.shape[:ndim])
A:jax.experimental.vectorize.broadcast_shape->numpy.lib.stride_tricks._broadcast_shape(*broadcast_args)
A:jax.experimental.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims)
A:jax.experimental.vectorize.input_shapes->_calculate_shapes(broadcast_shape, dim_sizes, input_core_dims)
A:jax.experimental.vectorize.all_core_dims->set()
A:jax.experimental.vectorize.result->reorder_outputs(result, axis, output_core_dims)
A:jax.experimental.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax.experimental.vectorize.axis->kwargs.get('axis')
A:jax.experimental.vectorize.args->reorder_inputs(args, axis, input_core_dims)
A:jax.experimental.vectorize.broadcast_args->broadcast_with_core_dims(args, input_core_dims, output_core_dims)
A:jax.experimental.vectorize.vectorized_func->vmap(vectorized_func)
jax.experimental.vectorize._calculate_shapes(broadcast_shape,dim_sizes,list_of_core_dims)
jax.experimental.vectorize._parse_gufunc_signature(signature)
jax.experimental.vectorize._parse_input_dimensions(args,input_core_dims)
jax.experimental.vectorize._update_dim_sizes(dim_sizes,arg,core_dims)
jax.experimental.vectorize.broadcast_with_core_dims(args,input_core_dims,output_core_dims)
jax.experimental.vectorize.reorder_inputs(args,axis,input_core_dims)
jax.experimental.vectorize.reorder_outputs(result,axis,output_core_dims)
jax.experimental.vectorize.vectorize(signature)
jax.experimental.vectorize.verify_axis_is_supported(input_core_dims,output_core_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/experimental/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/numpy/linalg.py----------------------------------------
A:jax.numpy.linalg.dtype->lax.dtype(a)
A:jax.numpy.linalg.a->_promote_arg_dtypes(np.asarray(a))
A:jax.numpy.linalg.a_shape->np.shape(a)
A:jax.numpy.linalg.(lu, pivot)->lax_linalg.lu(a)
A:jax.numpy.linalg.diag->np.diagonal(lu, axis1=-2, axis2=-1)
A:jax.numpy.linalg.is_zero->np.any(diag == np.array(0, dtype=dtype), axis=-1)
A:jax.numpy.linalg.parity->np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
A:jax.numpy.linalg.sign->np.where(is_zero, np.array(0, dtype=dtype), sign * np.array(-2 * (parity % 2) + 1, dtype=dtype))
A:jax.numpy.linalg.logdet->np.where(is_zero, np.array(-np.inf, dtype=dtype), np.sum(np.log(np.abs(diag)), axis=-1))
A:jax.numpy.linalg.(sign, logdet)->slogdet(a)
A:jax.numpy.linalg.(w, vl, vr)->lax_linalg.eig(a)
A:jax.numpy.linalg.msg->"UPLO must be one of None, 'L', or 'U', got {}".format(UPLO)
A:jax.numpy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input)
A:jax.numpy.linalg.x->lax_linalg.triangular_solve(lu, x, left_side=True, lower=False)
A:jax.numpy.linalg.x_shape->np.shape(x)
A:jax.numpy.linalg.ndim->len(x_shape)
A:jax.numpy.linalg.axis->tuple((np._canonicalize_axis(x, ndim) for x in axis))
A:jax.numpy.linalg.num_axes->len(axis)
A:jax.numpy.linalg.y->np.reshape(y, result_shape)
A:jax.numpy.linalg.result_shape->list(x_shape)
A:jax.numpy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.numpy.linalg.(a, b)->_promote_arg_dtypes(np.asarray(a), np.asarray(b))
A:jax.numpy.linalg.b_shape->np.shape(b)
A:jax.numpy.linalg.a_ndims->len(a_shape)
A:jax.numpy.linalg.b_ndims->len(b_shape)
A:jax.numpy.linalg.(lu, pivots)->lax_linalg.lu(a)
A:jax.numpy.linalg.batch_dims->lax.broadcast_shapes(lu.shape[:-2], x.shape[:-2])
A:jax.numpy.linalg.lu->np.broadcast_to(lu, batch_dims + lu.shape[-2:])
A:jax.numpy.linalg.permutation->np.broadcast_to(permutation, batch_dims + (m,))
A:jax.numpy.linalg.iotas->np.ix_(*(lax.iota(np.int32, b) for b in batch_dims + (1,)))
A:jax.numpy.linalg.globals()[func.__name__]->_not_implemented(func)
jax.numpy.linalg._norm(x,ord,axis,keepdims)
jax.numpy.linalg._promote_arg_dtypes(*args)
jax.numpy.linalg.cholesky(a)
jax.numpy.linalg.det(a)
jax.numpy.linalg.eig(a)
jax.numpy.linalg.eigh(a,UPLO=None,symmetrize_input=True)
jax.numpy.linalg.inv(a)
jax.numpy.linalg.norm(x,ord=None,axis=None,keepdims=False)
jax.numpy.linalg.qr(a,mode='reduced')
jax.numpy.linalg.slogdet(a)
jax.numpy.linalg.solve(a,b)
jax.numpy.linalg.svd(a,full_matrices=True,compute_uv=True)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/numpy/lax_numpy.py----------------------------------------
A:jax.numpy.lax_numpy.result_rank->len(lax.broadcast_shapes(*shapes))
A:jax.numpy.lax_numpy.from_dtypes->map(_dtype, args)
A:jax.numpy.lax_numpy.to_dtype->_result_dtype(op, *args)
A:jax.numpy.lax_numpy.(pos, arg)->next(((i, arg) for (i, arg) in enumerate(args) if not_array(arg)))
A:jax.numpy.lax_numpy._numpy_signature_re->lax.real(x).compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\(.*\\)$')
A:jax.numpy.lax_numpy.sections->fun.__doc__.split('\n\n')
A:jax.numpy.lax_numpy.summary->sections[i].strip()
A:jax.numpy.lax_numpy.body->'\n\n'.join(signatures + sections[i + 1:])
A:jax.numpy.lax_numpy.docstr->'{summary}\n\nLAX-backend implementation of :func:`{fun}`. Original docstring below.\n\n{body}'.format(summary=summary, fun=fun.__name__, body=body)
A:jax.numpy.lax_numpy.axis->_canonicalize_axis(axis, ndim(a))
A:jax.numpy.lax_numpy.absoluteabs->_one_to_one_unop(onp.absolute, lax.abs)
A:jax.numpy.lax_numpy.fabs->_one_to_one_unop(onp.fabs, lax.abs, True)
A:jax.numpy.lax_numpy.bitwise_not->_one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.negative->_one_to_one_unop(onp.negative, lax.neg)
A:jax.numpy.lax_numpy.positive->_one_to_one_unop(onp.positive, lambda x: x)
A:jax.numpy.lax_numpy.sign->_one_to_one_unop(onp.sign, lax.sign)
A:jax.numpy.lax_numpy.floor->_one_to_one_unop(onp.floor, lax.floor, True)
A:jax.numpy.lax_numpy.ceil->_one_to_one_unop(onp.ceil, lax.ceil, True)
A:jax.numpy.lax_numpy.exp->_one_to_one_unop(onp.exp, lax.exp, True)
A:jax.numpy.lax_numpy.log->_one_to_one_unop(onp.log, lax.log, True)
A:jax.numpy.lax_numpy.expm1->_one_to_one_unop(onp.expm1, lax.expm1, True)
A:jax.numpy.lax_numpy.log1p->_one_to_one_unop(onp.log1p, lax.log1p, True)
A:jax.numpy.lax_numpy.sin->_one_to_one_unop(onp.sin, lax.sin, True)
A:jax.numpy.lax_numpy.cos->_one_to_one_unop(onp.cos, lax.cos, True)
A:jax.numpy.lax_numpy.tan->_one_to_one_unop(onp.tan, lax.tan, True)
A:jax.numpy.lax_numpy.arcsin->_one_to_one_unop(onp.arcsin, lax.asin, True)
A:jax.numpy.lax_numpy.arccos->_one_to_one_unop(onp.arccos, lax.acos, True)
A:jax.numpy.lax_numpy.arctan->_one_to_one_unop(onp.arctan, lax.atan, True)
A:jax.numpy.lax_numpy.sinh->_one_to_one_unop(onp.sinh, lax.sinh, True)
A:jax.numpy.lax_numpy.cosh->_one_to_one_unop(onp.cosh, lax.cosh, True)
A:jax.numpy.lax_numpy.tanh->_one_to_one_unop(onp.tanh, lax.tanh, True)
A:jax.numpy.lax_numpy.sqrt->_one_to_one_unop(onp.sqrt, lax.sqrt, True)
A:jax.numpy.lax_numpy.add->_one_to_one_binop(onp.add, lax.add)
A:jax.numpy.lax_numpy.bitwise_and->_one_to_one_binop(onp.bitwise_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.bitwise_or->_one_to_one_binop(onp.bitwise_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.bitwise_xor->_one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.right_shift->_one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
A:jax.numpy.lax_numpy.left_shift->_one_to_one_binop(onp.left_shift, lax.shift_left)
A:jax.numpy.lax_numpy.equal->_one_to_one_binop(onp.equal, lax.eq)
A:jax.numpy.lax_numpy.multiply->_one_to_one_binop(onp.multiply, lax.mul)
A:jax.numpy.lax_numpy.not_equal->_one_to_one_binop(onp.not_equal, lax.ne)
A:jax.numpy.lax_numpy.subtract->_one_to_one_binop(onp.subtract, lax.sub)
A:jax.numpy.lax_numpy.arctan2->_one_to_one_binop(onp.arctan2, lax.atan2, True)
A:jax.numpy.lax_numpy.minimum->_one_to_one_binop(onp.minimum, lax.min)
A:jax.numpy.lax_numpy.maximum->_one_to_one_binop(onp.maximum, lax.max)
A:jax.numpy.lax_numpy.float_power->_one_to_one_binop(onp.float_power, lax.pow, True)
A:jax.numpy.lax_numpy.(x, y)->_promote_to_result_dtype(onp.hypot, x, y)
A:jax.numpy.lax_numpy.rx->lax.real(x)
A:jax.numpy.lax_numpy.ry->lax.real(y)
A:jax.numpy.lax_numpy.greater_equal->_comparison_op(onp.greater_equal, lax.ge)
A:jax.numpy.lax_numpy.greater->_comparison_op(onp.greater, lax.gt)
A:jax.numpy.lax_numpy.less_equal->_comparison_op(onp.less_equal, lax.le)
A:jax.numpy.lax_numpy.less->_comparison_op(onp.less, lax.lt)
A:jax.numpy.lax_numpy.logical_and->_logical_op(onp.logical_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.logical_not->_logical_op(onp.logical_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.logical_or->_logical_op(onp.logical_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.logical_xor->_logical_op(onp.logical_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.result_dtype->_dtype(np_fun(onp.ones((), dtype=dtype or _dtype(a))))
A:jax.numpy.lax_numpy.(x1, x2)->broadcast_arrays(x1, x2)
A:jax.numpy.lax_numpy.dtype->_dtype(x)
A:jax.numpy.lax_numpy.quotient->lax.div(x1, x2)
A:jax.numpy.lax_numpy.select->logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
A:jax.numpy.lax_numpy.x1r->lax.real(x1)
A:jax.numpy.lax_numpy.x1i->lax.imag(x1)
A:jax.numpy.lax_numpy.x2r->lax.real(x2)
A:jax.numpy.lax_numpy.x2i->lax.imag(x2)
A:jax.numpy.lax_numpy.which->lax.ge(lax.abs(x2r), lax.abs(x2i))
A:jax.numpy.lax_numpy.rat1->where(which, lax._const(x2i, 1), lax.div(x2r, x2i))
A:jax.numpy.lax_numpy.rat2->where(which, lax.div(x2i, x2r), lax._const(x2i, 1))
A:jax.numpy.lax_numpy.out->numpy.linspace(start, stop, num, endpoint, retstep, dtype)
A:jax.numpy.lax_numpy.mod->lax.select(ind, mod + x1, mod)
A:jax.numpy.lax_numpy.div->lax.select(ind, div - _constant_like(div, 1), div)
A:jax.numpy.lax_numpy.ind->lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
A:jax.numpy.lax_numpy.x1->lax.mul(x1, x1)
A:jax.numpy.lax_numpy.x2->lax.shift_right_logical(x2, _constant_like(x2, 1))
A:jax.numpy.lax_numpy.acc->where(lax.bitwise_and(x2, _constant_like(x2, 1)), lax.mul(acc, x1), acc)
A:jax.numpy.lax_numpy.amax->lax.max(x1, x2)
A:jax.numpy.lax_numpy.(x,)->_promote_to_result_dtype(onp.arctanh, x)
A:jax.numpy.lax_numpy.zero->lax._const(x, 0)
A:jax.numpy.lax_numpy.trunc_mod->lax.rem(x1, x2)
A:jax.numpy.lax_numpy.trunc_mod_not_zero->lax.ne(trunc_mod, zero)
A:jax.numpy.lax_numpy.do_plus->lax.bitwise_and(lax.ne(lax.lt(trunc_mod, zero), lax.lt(x2, zero)), trunc_mod_not_zero)
A:jax.numpy.lax_numpy.fmod->_wraps(onp.fmod)(lambda x, y: lax.rem(x, y))
A:jax.numpy.lax_numpy.pi_x->lax.mul(lax._const(x, pi), x)
A:jax.numpy.lax_numpy.one->lax._const(x, 1)
A:jax.numpy.lax_numpy.result->lax.dot_general(lhs, rhs, dimension_numbers)
A:jax.numpy.lax_numpy.a->lax.sort(a, dimension=axis)
A:jax.numpy.lax_numpy.sqrt_max_value->numpy.sqrt(onp.finfo(_dtype(x)).max)
A:jax.numpy.lax_numpy.log2->lax._const(x, onp.log(2))
A:jax.numpy.lax_numpy.ax1->_canonicalize_axis(ax1, m.ndim)
A:jax.numpy.lax_numpy.ax2->_canonicalize_axis(ax2, m.ndim)
A:jax.numpy.lax_numpy.perm->tuple([names.index(name) for name in result_names])
A:jax.numpy.lax_numpy.i->array(i)
A:jax.numpy.lax_numpy.re->lax.real(x)
A:jax.numpy.lax_numpy.im->lax.imag(x)
A:jax.numpy.lax_numpy.slice1[axis]->slice(1, None)
A:jax.numpy.lax_numpy.slice2[axis]->slice(None, -1)
A:jax.numpy.lax_numpy.slice1->tuple(slice1)
A:jax.numpy.lax_numpy.slice2->tuple(slice2)
A:jax.numpy.lax_numpy.dummy_val->numpy.broadcast_to(0, ary.shape)
A:jax.numpy.lax_numpy.order->kwargs.pop('order', 'C')
A:jax.numpy.lax_numpy.invalid_kwargs->"'{}'".format("'".join(kwargs))
A:jax.numpy.lax_numpy.shape->tuple(map(int, shape))
A:jax.numpy.lax_numpy.source->tuple((_canonicalize_axis(i, ndim(a)) for i in source))
A:jax.numpy.lax_numpy.destination->tuple((_canonicalize_axis(i, ndim(a)) for i in destination))
A:jax.numpy.lax_numpy.(a, b)->_promote_dtypes(a, b)
A:jax.numpy.lax_numpy.rtol->lax.convert_element_type(rtol, dtype)
A:jax.numpy.lax_numpy.atol->lax.convert_element_type(atol, dtype)
A:jax.numpy.lax_numpy.numpy_version->tuple(map(int, onp.version.version.split('.')))
A:jax.numpy.lax_numpy.condition->lax.ne(condition, zeros_like(condition))
A:jax.numpy.lax_numpy.(condition, x, y)->broadcast_arrays(condition, x, y)
A:jax.numpy.lax_numpy.(empty, _)->_promote_dtypes(x, y)
A:jax.numpy.lax_numpy.output->where(cond, choice, output)
A:jax.numpy.lax_numpy.result_shape->lax.broadcast_shapes(*shapes)
A:jax.numpy.lax_numpy.(diff,)->numpy.where(onp.not_equal(shape[nlead:], _shape(arr)))
A:jax.numpy.lax_numpy.kept_dims->tuple(onp.delete(onp.arange(len(shape)), new_dims))
A:jax.numpy.lax_numpy.broadcast_dims->numpy.concatenate((onp.arange(0, axis + 1), onp.arange(axis + 2, num_dims + 1)))
A:jax.numpy.lax_numpy.squeezed_array->squeeze(arr, diff)
A:jax.numpy.lax_numpy.subarrays->numpy.split(dummy_val, indices_or_sections, axis)
A:jax.numpy.lax_numpy.split_indices->numpy.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
A:jax.numpy.lax_numpy.vsplit->_split_on_axis(onp.vsplit, axis=0)
A:jax.numpy.lax_numpy.hsplit->_split_on_axis(onp.hsplit, axis=1)
A:jax.numpy.lax_numpy.dsplit->_split_on_axis(onp.dsplit, axis=2)
A:jax.numpy.lax_numpy.a_min->lax.convert_element_type(a_min, _dtype(a))
A:jax.numpy.lax_numpy.a_max->lax.convert_element_type(a_max, _dtype(a))
A:jax.numpy.lax_numpy.factor->_constant_like(x, 10 ** decimals)
A:jax.numpy.lax_numpy.isposinf->_wraps(onp.isposinf)(partial(_isposneginf, inf))
A:jax.numpy.lax_numpy.isneginf->_wraps(onp.isneginf)(partial(_isposneginf, -inf))
A:jax.numpy.lax_numpy.info->finfo(xla_bridge.canonicalize_dtype(dtype))
A:jax.numpy.lax_numpy.x->remainder(x, a_shape[i] or 1)
A:jax.numpy.lax_numpy.dims->_reduction_dims(a, axis)
A:jax.numpy.lax_numpy.shape_with_singletons->lax.subvals(shape(a), zip(dims, (1,) * len(dims)))
A:jax.numpy.lax_numpy.a_dtype->lib.xla_bridge.canonicalize_dtype(_dtype(a))
A:jax.numpy.lax_numpy._cast_to_bool->partial(lax.convert_element_type, new_dtype=onp.bool_)
A:jax.numpy.lax_numpy.sum->_make_reduction(onp.sum, lax.add, 0)
A:jax.numpy.lax_numpy.productprod->_make_reduction(onp.prod, lax.mul, 1)
A:jax.numpy.lax_numpy.amaxmax->_make_reduction(onp.max, lax.max, -onp.inf)
A:jax.numpy.lax_numpy.aminmin->_make_reduction(onp.min, lax.min, onp.inf)
A:jax.numpy.lax_numpy.allalltrue->_make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
A:jax.numpy.lax_numpy.anysometrue->_make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
A:jax.numpy.lax_numpy.normalizer->numpy.prod(onp.take(shape(a), axis))
A:jax.numpy.lax_numpy.avg->mean(a, axis=axis)
A:jax.numpy.lax_numpy.weights_sum->broadcast_to(weights_sum, avg.shape)
A:jax.numpy.lax_numpy.weights->moveaxis(weights, -1, axis)
A:jax.numpy.lax_numpy.out_dtype->lib.xla_bridge.canonicalize_dtype(result_type(a.dtype, weights.dtype))
A:jax.numpy.lax_numpy.a_shape->shape(a)
A:jax.numpy.lax_numpy.a_ndim->len(a_shape)
A:jax.numpy.lax_numpy.weights_shape->shape(weights)
A:jax.numpy.lax_numpy.centered->lax.abs(centered)
A:jax.numpy.lax_numpy.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.numpy.lax_numpy.nanmin->_make_nan_reduction(onp.nanmin, min, inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nanmax->_make_nan_reduction(onp.nanmax, max, -inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nansum->_make_nan_reduction(onp.nansum, sum, 0, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.nanprod->_make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.num_dims->len(a_shape)
A:jax.numpy.lax_numpy.cumsum->_make_cumulative_reduction(onp.cumsum, lax._reduce_window_sum, 0, squash_nan=False)
A:jax.numpy.lax_numpy.cumprod->_make_cumulative_reduction(onp.cumprod, lax._reduce_window_prod, 1, squash_nan=False)
A:jax.numpy.lax_numpy.nancumsum->_make_cumulative_reduction(onp.nancumsum, lax._reduce_window_sum, 0, squash_nan=True)
A:jax.numpy.lax_numpy.nancumprod->_make_cumulative_reduction(onp.nancumprod, lax._reduce_window_prod, 1, squash_nan=True)
A:jax.numpy.lax_numpy.array->lax.concatenate(parts, dimension=i)
A:jax.numpy.lax_numpy.nd->ndim(array)
A:jax.numpy.lax_numpy.pad_width->numpy.broadcast_to(onp.asarray(pad_width), (nd, 2))
A:jax.numpy.lax_numpy.constant_values->lax.convert_element_type(constant_values, array.dtype)
A:jax.numpy.lax_numpy.rarray->lax.rev(array, dimensions=(i,))
A:jax.numpy.lax_numpy.parts->reversed(build_padding(pad_width[i, 0], forward=not wrap_mode))
A:jax.numpy.lax_numpy.shape0->shape(arrays[0])
A:jax.numpy.lax_numpy.new_shape->list(shape0)
A:jax.numpy.lax_numpy.arrays->_promote_dtypes(*arrays)
A:jax.numpy.lax_numpy.arr->asarray(arr)
A:jax.numpy.lax_numpy.view->memoryview(object)
A:jax.numpy.lax_numpy.k_dtype->_dtype(k)
A:jax.numpy.lax_numpy.cols->lax.broadcasted_iota(k_dtype, (N, M), 1)
A:jax.numpy.lax_numpy.logspace->_wrap_numpy_nullary_function(onp.logspace)
A:jax.numpy.lax_numpy.geomspace->_wrap_numpy_nullary_function(onp.geomspace)
A:jax.numpy.lax_numpy.indexing->kwargs.get('indexing', 'xy')
A:jax.numpy.lax_numpy.sparse->kwargs.get('sparse', False)
A:jax.numpy.lax_numpy.copy->kwargs.get('copy', True)
A:jax.numpy.lax_numpy.args->list(args)
A:jax.numpy.lax_numpy.args[i]a->asarray(a)
A:jax.numpy.lax_numpy.s->list(s)
A:jax.numpy.lax_numpy.n->len(args)
A:jax.numpy.lax_numpy.broadcast_shape->list(a_shape)
A:jax.numpy.lax_numpy.mask->tri(*m_shape[-2:], k=k - 1, dtype=bool)
A:jax.numpy.lax_numpy.m_shape->shape(m)
A:jax.numpy.lax_numpy.axis1->_canonicalize_axis(axis1, a_ndims)
A:jax.numpy.lax_numpy.axis2->_canonicalize_axis(axis2, a_ndims)
A:jax.numpy.lax_numpy.default_int->lib.xla_bridge.canonicalize_dtype(onp.int_)
A:jax.numpy.lax_numpy.diag_indices->_wrap_indices_function(onp.diag_indices)
A:jax.numpy.lax_numpy.tril_indices->_wrap_indices_function(onp.tril_indices)
A:jax.numpy.lax_numpy.triu_indices->_wrap_indices_function(onp.triu_indices)
A:jax.numpy.lax_numpy.mask_indices->_wrap_indices_function(onp.mask_indices)
A:jax.numpy.lax_numpy.a_ndims->len(shape(a))
A:jax.numpy.lax_numpy.d->diag(c)
A:jax.numpy.lax_numpy.diag_size->_max(0, _min(a_shape[axis1] + _min(offset, 0), a_shape[axis2] - _max(offset, 0)))
A:jax.numpy.lax_numpy.v_shape->shape(v)
A:jax.numpy.lax_numpy.v->lax.pad(v, zero(v), ((_max(0, k), _max(0, -k), 0),))
A:jax.numpy.lax_numpy.p->numpy.asarray(p)
A:jax.numpy.lax_numpy.batch_shape->lax.broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
A:jax.numpy.lax_numpy.b->reshape(b, (1,) * (ndim(a) - ndim(b)) + shape(b))
A:jax.numpy.lax_numpy.batch_dims->tuple(range(nbatch))
A:jax.numpy.lax_numpy.a_reshape->lax.reshape(a, (_prod(a.shape[:-axes]), _prod(a.shape[-axes:])))
A:jax.numpy.lax_numpy.b_reshape->lax.reshape(b, (_prod(b.shape[:axes]), _prod(b.shape[axes:])))
A:jax.numpy.lax_numpy.out_reshape->lax.dot(a_reshape, b_reshape)
A:jax.numpy.lax_numpy.num_axes->len(ax1)
A:jax.numpy.lax_numpy.a_transposed->moveaxis(a, ax1, tuple(range(a.ndim - num_axes, a.ndim)))
A:jax.numpy.lax_numpy.b_transposed->moveaxis(b, ax2, tuple(range(num_axes)))
A:jax.numpy.lax_numpy.optimize->kwargs.pop('optimize', 'greedy')
A:jax.numpy.lax_numpy.(operands, contractions)->opt_einsum.contract_path(*operands, einsum_call=True, use_blas=True, optimize=optimize)
A:jax.numpy.lax_numpy.contractions->tuple((data[:3] for data in contractions))
A:jax.numpy.lax_numpy.operands->list(_promote_dtypes(*operands))
A:jax.numpy.lax_numpy.operand->lax.transpose(operand, perm)
A:jax.numpy.lax_numpy.names->names.replace(name, '', count - 1).replace(name, '', count - 1)
A:jax.numpy.lax_numpy.eye->lax.broadcasted_eye(operand.dtype, operand.shape, axes)
A:jax.numpy.lax_numpy.(input_str, result_names)->einstr.split('->')
A:jax.numpy.lax_numpy.input_names->input_str.split(',')
A:jax.numpy.lax_numpy.counts->collections.Counter(names)
A:jax.numpy.lax_numpy.(operand, names)->sum_repeats(operand, names, counts, result_names)
A:jax.numpy.lax_numpy.(lhs, rhs)->map(operands.pop, operand_indices)
A:jax.numpy.lax_numpy.(lhs_counts, rhs_counts)->map(collections.Counter, input_names)
A:jax.numpy.lax_numpy.(lhs, lhs_names)->sum_repeats(lhs, lhs_names, lhs_counts, result_names + rhs_names)
A:jax.numpy.lax_numpy.(rhs, rhs_names)->sum_repeats(rhs, rhs_names, rhs_counts, result_names + lhs_names)
A:jax.numpy.lax_numpy.(lhs_batch, rhs_batch)->unzip2(((lhs_names.find(n), rhs_names.find(n)) for n in batch_names))
A:jax.numpy.lax_numpy.lhs->lhs.reshape(lhs.shape[:nbatch] + (-1,) + lhs.shape[-1:]).reshape(lhs.shape[:nbatch] + (-1,) + lhs.shape[-1:])
A:jax.numpy.lax_numpy.lhs_names->_movechars(lhs_names, lhs_batch, batch_dims)
A:jax.numpy.lax_numpy.rhs->rhs.reshape(rhs.shape[:nbatch] + (-1,) + rhs.shape[-1:]).reshape(rhs.shape[:nbatch] + (-1,) + rhs.shape[-1:])
A:jax.numpy.lax_numpy.rhs_names->_movechars(rhs_names, rhs_batch, batch_dims)
A:jax.numpy.lax_numpy.batch_names->''.join((lhs_names[i] for i in range(len(lhs_names)) if i in batch_dims))
A:jax.numpy.lax_numpy.(lhs_cont, rhs_cont)->unzip2(((lhs_names.index(n), rhs_names.index(n)) for n in contracted_names))
A:jax.numpy.lax_numpy.nbatch->len(batch_names)
A:jax.numpy.lax_numpy.ncont->len(lhs_cont)
A:jax.numpy.lax_numpy.lhs_cdims->tuple(range(lhs.ndim - ncont, lhs.ndim))
A:jax.numpy.lax_numpy.rhs_cdims->tuple(range(rhs.ndim - ncont, rhs.ndim))
A:jax.numpy.lax_numpy.b_ndims->len(shape(b))
A:jax.numpy.lax_numpy.axisa->_canonicalize_axis(axisa, a_ndims)
A:jax.numpy.lax_numpy.axisb->_canonicalize_axis(axisb, b_ndims)
A:jax.numpy.lax_numpy.b_shape->lax.broadcast_shapes(shift.shape, axis.shape, (1,))
A:jax.numpy.lax_numpy.c->lax.complex(real_part, complex_part)
A:jax.numpy.lax_numpy.c_ndims->len(shape(c))
A:jax.numpy.lax_numpy.axisc->_canonicalize_axis(axisc, c_ndims)
A:jax.numpy.lax_numpy.a_reshaped->reshape(a, [i for d in shape(a) for i in (d, 1)])
A:jax.numpy.lax_numpy.b_reshaped->reshape(b, [i for d in shape(b) for i in (1, d)])
A:jax.numpy.lax_numpy.out_shape->lax.broadcast_shapes(idx_shape, tuple(arr_shape))
A:jax.numpy.lax_numpy.x_shape->shape(x)
A:jax.numpy.lax_numpy.iota->lax.broadcast_in_dim(iota, gather_index_shape, (j,))
A:jax.numpy.lax_numpy.idxs->numpy.arange(a.shape[axis]).reshape(shape)
A:jax.numpy.lax_numpy.mask_idxs->where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
A:jax.numpy.lax_numpy.(_, perm)->lax.sort_key_val(a, iota, dimension=axis)
A:jax.numpy.lax_numpy.shift->asarray(shift)
A:jax.numpy.lax_numpy.indices->_normalize_index(indices, axis_size)
A:jax.numpy.lax_numpy.index_dims->len(shape(indices))
A:jax.numpy.lax_numpy.slice_sizes->list(a_shape)
A:jax.numpy.lax_numpy.dnums->lax.GatherDimensionNumbers(offset_dims=tuple(range(q_ndim, len(a_shape) + q_ndim if keepdims else len(a_shape) + q_ndim - 1)), collapsed_slice_dims=() if keepdims else (axis,), start_index_map=(axis,))
A:jax.numpy.lax_numpy.rank->ndim(arr)
A:jax.numpy.lax_numpy.arr_shape->list(shape(arr))
A:jax.numpy.lax_numpy.idx_shape->shape(indices)
A:jax.numpy.lax_numpy.gather_indices->concatenate((gather_indices, i), len(gather_indices_shape))
A:jax.numpy.lax_numpy.(treedef, static_idx, dynamic_idx)->_split_index_for_jit(idx)
A:jax.numpy.lax_numpy.idx->tuple(idx)
A:jax.numpy.lax_numpy.indexer->_index_to_gather(shape(arr), idx)
A:jax.numpy.lax_numpy._Indexer->collections.namedtuple('_Indexer', ['slice_shape', 'gather_slice_shape', 'gather_indices', 'dnums', 'reversed_y_dims', 'newaxis_dims'])
A:jax.numpy.lax_numpy.(leaves, treedef)->lib.pytree.flatten(idx)
A:jax.numpy.lax_numpy.(advanced_indexes, idx_advanced_axes, x_advanced_axes)->zip(*advanced_pairs)
A:jax.numpy.lax_numpy.advanced_axes_are_contiguous->numpy.all(onp.diff(idx_advanced_axes) == 1)
A:jax.numpy.lax_numpy.advanced_indexes->broadcast_arrays(*advanced_indexes)
A:jax.numpy.lax_numpy.ndim->len(shape)
A:jax.numpy.lax_numpy.abstract_i->core.get_aval(i)
A:jax.numpy.lax_numpy.(start, limit, stride, needs_rev)->_static_idx(i, x_shape[x_axis])
A:jax.numpy.lax_numpy.len_without_none->_sum((1 for e in idx if e is not None and e is not Ellipsis))
A:jax.numpy.lax_numpy.ellipsis_index->next(ellipses, None)
A:jax.numpy.lax_numpy.(start, stop, step)->tuple(idx).indices(size)
A:jax.numpy.lax_numpy.blackman->_wrap_numpy_nullary_function(onp.blackman)
A:jax.numpy.lax_numpy.bartlett->_wrap_numpy_nullary_function(onp.bartlett)
A:jax.numpy.lax_numpy.hamming->_wrap_numpy_nullary_function(onp.hamming)
A:jax.numpy.lax_numpy.hanning->_wrap_numpy_nullary_function(onp.hanning)
A:jax.numpy.lax_numpy.kaiser->_wrap_numpy_nullary_function(onp.kaiser)
A:jax.numpy.lax_numpy.(gcd, _)->lax.while_loop(cond_fn, body_fn, (x1, x2))
A:jax.numpy.lax_numpy.X->array(m, ndmin=2, dtype=xla_bridge.canonicalize_dtype(result_type(m, onp.float64)), copy=False)
A:jax.numpy.lax_numpy.w->asarray(fweights)
A:jax.numpy.lax_numpy.(avg, w_sum)->average(X, axis=1, weights=w, returned=True)
A:jax.numpy.lax_numpy.stddev->sqrt(real(d))
A:jax.numpy.lax_numpy.real_part->clip(real(c), -1, 1)
A:jax.numpy.lax_numpy.complex_part->clip(imag(c), -1, 1)
A:jax.numpy.lax_numpy.q->true_divide(asarray(q), float32(100.0))
A:jax.numpy.lax_numpy.q_ndim->ndim(q)
A:jax.numpy.lax_numpy.(a, q)->_promote_dtypes(a, q)
A:jax.numpy.lax_numpy.low->lax.convert_element_type(low, int64)
A:jax.numpy.lax_numpy.high->lax.convert_element_type(high, int64)
A:jax.numpy.lax_numpy.high_weight->lax.broadcast_in_dim(high_weight, high_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_weight->lax.broadcast_in_dim(low_weight, low_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_value->lax.gather(a, low, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.high_value->lax.gather(a, high, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.globals()[func.__name__]->_not_implemented(func)
jax.numpy._ArrayMeta(type(onp.ndarray))
jax.numpy._ArrayMeta.__instancecheck__(self,instance)
jax.numpy._argminmax(op,a,axis)
jax.numpy._astype(arr,dtype)
jax.numpy._canonicalize_axis(axis,num_dims)
jax.numpy._canonicalize_tuple_index(arr_ndim,idx)
jax.numpy._check_arraylike(fun_name,*args)
jax.numpy._comparison_op(numpy_fn,lax_fn)
jax.numpy._constant_like(x,const)
jax.numpy._dot_general(lhs,rhs,lhs_cont,rhs_cont,nbatch)
jax.numpy._dtype_info(dtype)
jax.numpy._einsum(operands,contractions)
jax.numpy._eliminate_deprecated_list_indexing(idx)
jax.numpy._expand_bool_indices(idx)
jax.numpy._float_divmod(x1,x2)
jax.numpy._gather(arr,treedef,static_idx,dynamic_idx)
jax.numpy._index_to_gather(x_shape,idx)
jax.numpy._int(aval)
jax.numpy._is_advanced_int_indexer(idx)
jax.numpy._is_int_arraylike(x)
jax.numpy._is_slice_none(idx)
jax.numpy._isposneginf(infinity,x)
jax.numpy._logical_op(np_op,bitwise_op)
jax.numpy._make_cumulative_reduction(onp_reduction,window_reduce,init_val,squash_nan=False)
jax.numpy._make_nan_reduction(onp_reduction,np_reduction,init_val,nan_if_all_nan)
jax.numpy._make_reduction(np_fun,op,init_val,preproc=None)
jax.numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax.numpy._movechars(s,src,dst)
jax.numpy._normalize_index(index,axis_size)
jax.numpy._not_implemented(fun)
jax.numpy._one_to_one_binop(numpy_fn,lax_fn,promote_like=False)
jax.numpy._one_to_one_unop(numpy_fn,lax_fn,promote_like=False)
jax.numpy._pad(array,pad_width,mode,constant_values)
jax.numpy._promote_args(fun_name,*args)
jax.numpy._promote_args_like(op,*args)
jax.numpy._promote_dtypes(*args)
jax.numpy._promote_shapes(fun_name,*args)
jax.numpy._promote_to_result_dtype(op,*args)
jax.numpy._rank_promotion_warning_or_error(fun_name,shapes)
jax.numpy._reduction_dims(a,axis)
jax.numpy._reduction_init_val(a,init_val)
jax.numpy._reshape(a,newshape,order='C')
jax.numpy._reshape_method(a,*newshape,**kwargs)
jax.numpy._result_dtype(op,*args)
jax.numpy._rewriting_take(arr,idx)
jax.numpy._should_unpack_list_index(x)
jax.numpy._split_index_for_jit(idx)
jax.numpy._split_on_axis(onp_fun,axis)
jax.numpy._static_idx(idx,size)
jax.numpy._swap_args(f)
jax.numpy._take_along_axis(arr,indices,axis)
jax.numpy._unimplemented_setitem(self,i,x)
jax.numpy._unstack(x)
jax.numpy._wrap_indices_function(f)
jax.numpy._wrap_numpy_nullary_function(f)
jax.numpy._wraps(fun)
jax.numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.angle(x)
jax.numpy.append(arr,values,axis=None)
jax.numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.arccosh(x)
jax.numpy.arcsinh(x)
jax.numpy.arctanh(x)
jax.numpy.argmax(a,axis=None)
jax.numpy.argmin(a,axis=None)
jax.numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.array_equal(a1,a2)
jax.numpy.asarray(a,dtype=None,order=None)
jax.numpy.atleast_1d(*arys)
jax.numpy.atleast_2d(*arys)
jax.numpy.atleast_3d(*arys)
jax.numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.broadcast_arrays(*args)
jax.numpy.broadcast_to(arr,shape)
jax.numpy.cbrt(x)
jax.numpy.clip(a,a_min=None,a_max=None)
jax.numpy.column_stack(tup)
jax.numpy.concatenate(arrays,axis=0)
jax.numpy.conjugate(x)
jax.numpy.corrcoef(x,y=None,rowvar=True,bias=None,ddof=None)
jax.numpy.count_nonzero(a,axis=None)
jax.numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.deg2rad(x)
jax.numpy.diag(v,k=0)
jax.numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.diff(a,n=1,axis=-1)
jax.numpy.divide(x1,x2)
jax.numpy.divmod(x1,x2)
jax.numpy.dot(a,b)
jax.numpy.dstack(tup)
jax.numpy.einsum(*operands,**kwargs)
jax.numpy.einsum_path(subscripts,*operands,**kwargs)
jax.numpy.exp2(x)
jax.numpy.expand_dims(a,axis)
jax.numpy.eye(N,M=None,k=None,dtype=None)
jax.numpy.fix(x,out=None)
jax.numpy.flip(m,axis)
jax.numpy.fliplr(m)
jax.numpy.flipud(m)
jax.numpy.floor_divide(x1,x2)
jax.numpy.full(shape,fill_value,dtype=None)
jax.numpy.full_like(a,fill_value,dtype=None)
jax.numpy.gcd(x1,x2)
jax.numpy.heaviside(x,y)
jax.numpy.hstack(tup)
jax.numpy.hypot(x,y)
jax.numpy.identity(n,dtype=None)
jax.numpy.imag(x)
jax.numpy.inner(a,b)
jax.numpy.isclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.iscomplex(x)
jax.numpy.isfinite(x)
jax.numpy.isinf(x)
jax.numpy.isnan(x)
jax.numpy.isreal(x)
jax.numpy.isrealobj(a)
jax.numpy.ix_(*args)
jax.numpy.kron(a,b)
jax.numpy.lax_numpy._ArrayMeta(type(onp.ndarray))
jax.numpy.lax_numpy._ArrayMeta.__instancecheck__(self,instance)
jax.numpy.lax_numpy._argminmax(op,a,axis)
jax.numpy.lax_numpy._astype(arr,dtype)
jax.numpy.lax_numpy._canonicalize_axis(axis,num_dims)
jax.numpy.lax_numpy._canonicalize_tuple_index(arr_ndim,idx)
jax.numpy.lax_numpy._check_arraylike(fun_name,*args)
jax.numpy.lax_numpy._comparison_op(numpy_fn,lax_fn)
jax.numpy.lax_numpy._constant_like(x,const)
jax.numpy.lax_numpy._dot_general(lhs,rhs,lhs_cont,rhs_cont,nbatch)
jax.numpy.lax_numpy._dtype_info(dtype)
jax.numpy.lax_numpy._einsum(operands,contractions)
jax.numpy.lax_numpy._eliminate_deprecated_list_indexing(idx)
jax.numpy.lax_numpy._expand_bool_indices(idx)
jax.numpy.lax_numpy._float_divmod(x1,x2)
jax.numpy.lax_numpy._gather(arr,treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._index_to_gather(x_shape,idx)
jax.numpy.lax_numpy._int(aval)
jax.numpy.lax_numpy._is_advanced_int_indexer(idx)
jax.numpy.lax_numpy._is_int_arraylike(x)
jax.numpy.lax_numpy._is_slice_none(idx)
jax.numpy.lax_numpy._isposneginf(infinity,x)
jax.numpy.lax_numpy._logical_op(np_op,bitwise_op)
jax.numpy.lax_numpy._make_cumulative_reduction(onp_reduction,window_reduce,init_val,squash_nan=False)
jax.numpy.lax_numpy._make_nan_reduction(onp_reduction,np_reduction,init_val,nan_if_all_nan)
jax.numpy.lax_numpy._make_reduction(np_fun,op,init_val,preproc=None)
jax.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._movechars(s,src,dst)
jax.numpy.lax_numpy._normalize_index(index,axis_size)
jax.numpy.lax_numpy._not_implemented(fun)
jax.numpy.lax_numpy._one_to_one_binop(numpy_fn,lax_fn,promote_like=False)
jax.numpy.lax_numpy._one_to_one_unop(numpy_fn,lax_fn,promote_like=False)
jax.numpy.lax_numpy._pad(array,pad_width,mode,constant_values)
jax.numpy.lax_numpy._promote_args(fun_name,*args)
jax.numpy.lax_numpy._promote_args_like(op,*args)
jax.numpy.lax_numpy._promote_dtypes(*args)
jax.numpy.lax_numpy._promote_shapes(fun_name,*args)
jax.numpy.lax_numpy._promote_to_result_dtype(op,*args)
jax.numpy.lax_numpy._rank_promotion_warning_or_error(fun_name,shapes)
jax.numpy.lax_numpy._reduction_dims(a,axis)
jax.numpy.lax_numpy._reduction_init_val(a,init_val)
jax.numpy.lax_numpy._reshape(a,newshape,order='C')
jax.numpy.lax_numpy._reshape_method(a,*newshape,**kwargs)
jax.numpy.lax_numpy._result_dtype(op,*args)
jax.numpy.lax_numpy._rewriting_take(arr,idx)
jax.numpy.lax_numpy._should_unpack_list_index(x)
jax.numpy.lax_numpy._split_index_for_jit(idx)
jax.numpy.lax_numpy._split_on_axis(onp_fun,axis)
jax.numpy.lax_numpy._static_idx(idx,size)
jax.numpy.lax_numpy._swap_args(f)
jax.numpy.lax_numpy._take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy._unimplemented_setitem(self,i,x)
jax.numpy.lax_numpy._unstack(x)
jax.numpy.lax_numpy._wrap_indices_function(f)
jax.numpy.lax_numpy._wrap_numpy_nullary_function(f)
jax.numpy.lax_numpy._wraps(fun)
jax.numpy.lax_numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.lax_numpy.angle(x)
jax.numpy.lax_numpy.append(arr,values,axis=None)
jax.numpy.lax_numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.lax_numpy.arccosh(x)
jax.numpy.lax_numpy.arcsinh(x)
jax.numpy.lax_numpy.arctanh(x)
jax.numpy.lax_numpy.argmax(a,axis=None)
jax.numpy.lax_numpy.argmin(a,axis=None)
jax.numpy.lax_numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.lax_numpy.array_equal(a1,a2)
jax.numpy.lax_numpy.asarray(a,dtype=None,order=None)
jax.numpy.lax_numpy.atleast_1d(*arys)
jax.numpy.lax_numpy.atleast_2d(*arys)
jax.numpy.lax_numpy.atleast_3d(*arys)
jax.numpy.lax_numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.lax_numpy.broadcast_arrays(*args)
jax.numpy.lax_numpy.broadcast_to(arr,shape)
jax.numpy.lax_numpy.cbrt(x)
jax.numpy.lax_numpy.clip(a,a_min=None,a_max=None)
jax.numpy.lax_numpy.column_stack(tup)
jax.numpy.lax_numpy.concatenate(arrays,axis=0)
jax.numpy.lax_numpy.conjugate(x)
jax.numpy.lax_numpy.corrcoef(x,y=None,rowvar=True,bias=None,ddof=None)
jax.numpy.lax_numpy.count_nonzero(a,axis=None)
jax.numpy.lax_numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.lax_numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.lax_numpy.deg2rad(x)
jax.numpy.lax_numpy.diag(v,k=0)
jax.numpy.lax_numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.lax_numpy.diff(a,n=1,axis=-1)
jax.numpy.lax_numpy.divide(x1,x2)
jax.numpy.lax_numpy.divmod(x1,x2)
jax.numpy.lax_numpy.dot(a,b)
jax.numpy.lax_numpy.dstack(tup)
jax.numpy.lax_numpy.einsum(*operands,**kwargs)
jax.numpy.lax_numpy.einsum_path(subscripts,*operands,**kwargs)
jax.numpy.lax_numpy.exp2(x)
jax.numpy.lax_numpy.expand_dims(a,axis)
jax.numpy.lax_numpy.eye(N,M=None,k=None,dtype=None)
jax.numpy.lax_numpy.fix(x,out=None)
jax.numpy.lax_numpy.flip(m,axis)
jax.numpy.lax_numpy.fliplr(m)
jax.numpy.lax_numpy.flipud(m)
jax.numpy.lax_numpy.floor_divide(x1,x2)
jax.numpy.lax_numpy.full(shape,fill_value,dtype=None)
jax.numpy.lax_numpy.full_like(a,fill_value,dtype=None)
jax.numpy.lax_numpy.gcd(x1,x2)
jax.numpy.lax_numpy.heaviside(x,y)
jax.numpy.lax_numpy.hstack(tup)
jax.numpy.lax_numpy.hypot(x,y)
jax.numpy.lax_numpy.identity(n,dtype=None)
jax.numpy.lax_numpy.imag(x)
jax.numpy.lax_numpy.inner(a,b)
jax.numpy.lax_numpy.isclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.lax_numpy.iscomplex(x)
jax.numpy.lax_numpy.isfinite(x)
jax.numpy.lax_numpy.isinf(x)
jax.numpy.lax_numpy.isnan(x)
jax.numpy.lax_numpy.isreal(x)
jax.numpy.lax_numpy.isrealobj(a)
jax.numpy.lax_numpy.ix_(*args)
jax.numpy.lax_numpy.kron(a,b)
jax.numpy.lax_numpy.lcm(x1,x2)
jax.numpy.lax_numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.lax_numpy.log10(x)
jax.numpy.lax_numpy.log2(x)
jax.numpy.lax_numpy.logaddexp(x1,x2)
jax.numpy.lax_numpy.logaddexp2(x1,x2)
jax.numpy.lax_numpy.matmul(a,b)
jax.numpy.lax_numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.lax_numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.lax_numpy.meshgrid(*args,**kwargs)
jax.numpy.lax_numpy.moveaxis(a,source,destination)
jax.numpy.lax_numpy.nan_to_num(x,copy=True)
jax.numpy.lax_numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.lax_numpy.ndarray.__init__(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.lax_numpy.ones(shape,dtype=None)
jax.numpy.lax_numpy.ones_like(x,dtype=None)
jax.numpy.lax_numpy.outer(a,b,out=None)
jax.numpy.lax_numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.lax_numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.polyval(p,x)
jax.numpy.lax_numpy.power(x1,x2)
jax.numpy.lax_numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.lax_numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.rad2deg(x)
jax.numpy.lax_numpy.ravel(a,order='C')
jax.numpy.lax_numpy.real(x)
jax.numpy.lax_numpy.reciprocal(x)
jax.numpy.lax_numpy.remainder(x1,x2)
jax.numpy.lax_numpy.repeat(a,repeats,axis=None)
jax.numpy.lax_numpy.reshape(a,newshape,order='C')
jax.numpy.lax_numpy.roll(a,shift,axis=None)
jax.numpy.lax_numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.lax_numpy.round(a,decimals=0)
jax.numpy.lax_numpy.select(condlist,choicelist,default=0)
jax.numpy.lax_numpy.sinc(x)
jax.numpy.lax_numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.lax_numpy.square(x)
jax.numpy.lax_numpy.squeeze(a,axis=None)
jax.numpy.lax_numpy.stack(arrays,axis=0)
jax.numpy.lax_numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.swapaxes(a,axis1,axis2)
jax.numpy.lax_numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.lax_numpy.take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy.tensordot(a,b,axes=2)
jax.numpy.lax_numpy.tile(a,reps)
jax.numpy.lax_numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.lax_numpy.transpose(x,axes=None)
jax.numpy.lax_numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.lax_numpy.tril(m,k=0)
jax.numpy.lax_numpy.triu(m,k=0)
jax.numpy.lax_numpy.true_divide(x1,x2)
jax.numpy.lax_numpy.vander(x,N=None,increasing=False)
jax.numpy.lax_numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.vdot(a,b)
jax.numpy.lax_numpy.vstack(tup)
jax.numpy.lax_numpy.where(condition,x=None,y=None)
jax.numpy.lax_numpy.zeros(shape,dtype=None)
jax.numpy.lax_numpy.zeros_like(x,dtype=None)
jax.numpy.lcm(x1,x2)
jax.numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.log10(x)
jax.numpy.log2(x)
jax.numpy.logaddexp(x1,x2)
jax.numpy.logaddexp2(x1,x2)
jax.numpy.matmul(a,b)
jax.numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.meshgrid(*args,**kwargs)
jax.numpy.moveaxis(a,source,destination)
jax.numpy.nan_to_num(x,copy=True)
jax.numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.ones(shape,dtype=None)
jax.numpy.ones_like(x,dtype=None)
jax.numpy.outer(a,b,out=None)
jax.numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.polyval(p,x)
jax.numpy.power(x1,x2)
jax.numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.rad2deg(x)
jax.numpy.ravel(a,order='C')
jax.numpy.real(x)
jax.numpy.reciprocal(x)
jax.numpy.remainder(x1,x2)
jax.numpy.repeat(a,repeats,axis=None)
jax.numpy.reshape(a,newshape,order='C')
jax.numpy.roll(a,shift,axis=None)
jax.numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.round(a,decimals=0)
jax.numpy.select(condlist,choicelist,default=0)
jax.numpy.sinc(x)
jax.numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.square(x)
jax.numpy.squeeze(a,axis=None)
jax.numpy.stack(arrays,axis=0)
jax.numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.swapaxes(a,axis1,axis2)
jax.numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.take_along_axis(arr,indices,axis)
jax.numpy.tensordot(a,b,axes=2)
jax.numpy.tile(a,reps)
jax.numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.transpose(x,axes=None)
jax.numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.tril(m,k=0)
jax.numpy.triu(m,k=0)
jax.numpy.true_divide(x1,x2)
jax.numpy.vander(x,N=None,increasing=False)
jax.numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.vdot(a,b)
jax.numpy.vstack(tup)
jax.numpy.where(condition,x=None,y=None)
jax.numpy.zeros(shape,dtype=None)
jax.numpy.zeros_like(x,dtype=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/numpy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/numpy/fft.py----------------------------------------
A:jax.numpy.fft.dtype->np.result_type(arg, onp.complex64)
A:jax.numpy.fft.axes->range(a.ndim - len(s), a.ndim)
A:jax.numpy.fft.a->_promote_to_complex(a)
A:jax.numpy.fft.globals()[func.__name__]->_not_implemented(func)
jax.numpy.fft._promote_to_complex(arg)
jax.numpy.fft.fftn(a,s=None,axes=None,norm=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/ops/scatter.py----------------------------------------
A:jax.ops.scatter.x->numpy.lax_numpy.asarray(x)
A:jax.ops.scatter.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.ops.scatter.(treedef, static_idx, dynamic_idx)->numpy.lax_numpy._split_index_for_jit(idx)
A:jax.ops.scatter.idx->numpy.lax_numpy._merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)
A:jax.ops.scatter.indexer->numpy.lax_numpy._index_to_gather(np.shape(x), idx)
A:jax.ops.scatter.dnums->lax.ScatterDimensionNumbers(update_window_dims=indexer.dnums.offset_dims, inserted_window_dims=indexer.dnums.collapsed_slice_dims, scatter_dims_to_operand_dims=indexer.dnums.start_index_map)
A:jax.ops.scatter.index->_Indexable()
A:jax.ops.scatter.num_segments->int(num_segments)
A:jax.ops.scatter.out->numpy.lax_numpy.zeros((num_segments,) + data.shape[1:], dtype=data.dtype)
A:jax.ops.scatter.segment_ids->numpy.lax_numpy.mod(segment_ids, num_segments)
jax.ops.index_add(x,idx,y)
jax.ops.index_max(x,idx,y)
jax.ops.index_min(x,idx,y)
jax.ops.index_update(x,idx,y)
jax.ops.scatter._Indexable(object)
jax.ops.scatter._Indexable.__getitem__(self,index)
jax.ops.scatter._scatter_impl(x,y,scatter_op,treedef,static_idx,dynamic_idx)
jax.ops.scatter._scatter_update(x,idx,y,scatter_op)
jax.ops.scatter.index_add(x,idx,y)
jax.ops.scatter.index_max(x,idx,y)
jax.ops.scatter.index_min(x,idx,y)
jax.ops.scatter.index_update(x,idx,y)
jax.ops.scatter.segment_sum(data,segment_ids,num_segments=None)
jax.ops.segment_sum(data,segment_ids,num_segments=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/ops/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/pxla.py----------------------------------------
A:jax.interpreters.pxla.buffers[r][a]->xla.device_put(x[assignments[r]], ordinals[r], backend=backend)
A:jax.interpreters.pxla.bufs->shard_arg_handlers[type(arg)](arg, device_ordinals, assignments, backend=backend)
A:jax.interpreters.pxla.nrep->len(ordinals)
A:jax.interpreters.pxla.xs->x._unstack()
A:jax.interpreters.pxla.full_aval->ShapedArray((size,) + aval.shape, aval.dtype)
A:jax.interpreters.pxla.(groupsize, ragged)->divmod(nrep, size)
A:jax.interpreters.pxla.indices->numpy.tile(onp.arange(size)[:, None], (1, groupsize))
A:jax.interpreters.pxla.(trailing_size, ragged)->divmod(nrep, prod(mesh_spec))
A:jax.interpreters.pxla.iota->numpy.arange(prod(full_spec)).reshape(full_spec)
A:jax.interpreters.pxla.groups->numpy.reshape(onp.moveaxis(iota, mesh_axes, onp.arange(len(mesh_axes))), (prod(onp.take(full_spec, mesh_axes)), -1))
A:jax.interpreters.pxla.num_replicas->len(devices)
A:jax.interpreters.pxla.device_assignment->tuple((d.id for d in devices))
A:jax.interpreters.pxla.axis_env->xla.AxisEnv(num_replicas, [axis_name], [axis_size], devices)
A:jax.interpreters.pxla.arg_shapes->list(map(aval_to_xla_shape, abstract_args))
A:jax.interpreters.pxla.built_c->xla.jaxpr_computation(jaxpr, backend, axis_env, consts, (), *arg_shapes)
A:jax.interpreters.pxla.compiled->xla.jaxpr_computation(jaxpr, backend, axis_env, consts, (), *arg_shapes).Compile(arg_shapes, xb.get_compile_options(num_replicas, device_assignment), backend=xb.get_backend(backend))
A:jax.interpreters.pxla.self.dynamic_axis_env->DynamicAxisEnv()
A:jax.interpreters.pxla._thread_local_state->_ThreadLocalState()
A:jax.interpreters.pxla.mapped->prod((frame.hard_size for frame in dynamic_axis_env))
A:jax.interpreters.pxla.(unmapped, ragged)->divmod(xb.device_count(backend), mapped)
A:jax.interpreters.pxla.axis_name->params.pop('axis_name')
A:jax.interpreters.pxla.shape->tuple((logical_size(dynamic_axis_env[name]) for name in axis_name))
A:jax.interpreters.pxla.dummy_arg->frame.soft_trace.pure(dummy_arg)
A:jax.interpreters.pxla.out_aval->ShapedArray((), onp.int32)
A:jax.interpreters.pxla.out_tracer->pe.JaxprTracer(trace, pe.PartialVal((out_aval, core.unit)), None)
A:jax.interpreters.pxla.eqn->core.new_jaxpr_eqn([], [out_tracer], axis_index_p, (), params)
A:jax.interpreters.pxla.unsigned_index->c.Rem(c.ReplicaId(), c.Constant(onp.array(hard_size, onp.uint32)))
A:jax.interpreters.pxla.axis_index_p->core.Primitive('axis_index')
A:jax.interpreters.pxla._collect->staticmethod(onp.concatenate)
A:jax.interpreters.pxla.num_bufs->len(self.device_buffers)
A:jax.interpreters.pxla.assignments->assign_shards_to_replicas(nrep, axis_size)
A:jax.interpreters.pxla.(_, ids)->numpy.unique(assignments, return_index=True)
A:jax.interpreters.pxla.ids->self._ids()
A:jax.interpreters.pxla.self._npy_value->self._collect([self.device_buffers[i].to_py() for i in ids])
A:jax.interpreters.pxla.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.pxla.handler->xla.aval_to_result_handler(aval)
A:jax.interpreters.pxla.n->len(ordinals)
A:jax.interpreters.pxla.axis_size->len(replica_groups[0])
A:jax.interpreters.pxla.devices->params.pop('devices')
A:jax.interpreters.pxla.backend->params.pop('backend', None)
A:jax.interpreters.pxla.abstract_args->map(xla.abstractify, args)
A:jax.interpreters.pxla.compiled_fun->parallel_callable(fun, backend, axis_name, axis_size, devices, *abstract_args)
A:jax.interpreters.pxla.avals->tuple(map(partial(shard_aval, axis_size), avals))
A:jax.interpreters.pxla.pval->PartialVal([core.abstract_unit, core.unit])
A:jax.interpreters.pxla.(jaxpr, (out_pvals, consts, env))->trace_to_subjaxpr(dynamic_fun, master, False).call_wrapped([pval] + pvals)
A:jax.interpreters.pxla.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.pxla.(compiled, nrep)->compile_replicated(jaxpr, backend, axis_name, axis_size, devices, consts, *avals)
A:jax.interpreters.pxla.device_ordinals->xla.jaxpr_computation(jaxpr, backend, axis_env, consts, (), *arg_shapes).Compile(arg_shapes, xb.get_compile_options(num_replicas, device_assignment), backend=xb.get_backend(backend)).DeviceOrdinals()
A:jax.interpreters.pxla.handle_args->partial(shard_args, backend, device_ordinals, assignments, axis_size)
A:jax.interpreters.pxla.handle_outs->_pvals_to_results_handler(axis_size, nrep, out_pvals)
A:jax.interpreters.pxla.nouts->len(out_pvals)
A:jax.interpreters.pxla.input_bufs->in_handler(args)
A:jax.interpreters.pxla.out_bufs->xla.jaxpr_computation(jaxpr, backend, axis_env, consts, (), *arg_shapes).Compile(arg_shapes, xb.get_compile_options(num_replicas, device_assignment), backend=xb.get_backend(backend)).ExecutePerReplica(list(input_bufs))
A:jax.interpreters.pxla.xla_pmap_p->core.Primitive('xla_pmap')
A:jax.interpreters.pxla.xla_pmap->partial(core.call_bind, xla_pmap_p)
A:jax.interpreters.pxla.new_env->xla.extend_axis_env(axis_env, axis_name, axis_size)
A:jax.interpreters.pxla.in_nodes_sharded->list(map(partial(_xla_shard, c, new_env.sizes), in_nodes))
A:jax.interpreters.pxla.sharded_outs->xla.jaxpr_subcomp(c, jaxpr, backend, new_env, const_nodes, freevar_nodes, *in_nodes_sharded)
A:jax.interpreters.pxla.ad.primitive_transposes[xla_pmap_p]->partial(ad.map_transpose, xla_pmap_p)
A:jax.interpreters.pxla.xla_shape->c.GetShape(x)
A:jax.interpreters.pxla.dims->list(xla_shape.dimensions())
A:jax.interpreters.pxla.start_indices->_xla_shard_start_indices(c, axis_size, len(dims) + 1)
A:jax.interpreters.pxla.padded->c.DynamicUpdateSlice(padded, c.Reshape(x, None, [1] + dims), start_indices)
A:jax.interpreters.pxla.idx->c.Rem(c.ReplicaId(), c.Constant(onp.array(axis_size, onp.uint32)))
A:jax.interpreters.pxla.zero->numpy.zeros(ndim - 1, onp.uint32)
A:jax.interpreters.pxla.trace->SplitAxisTrace(master, core.cur_sublevel())
A:jax.interpreters.pxla.in_tracers->list(map(partial(SplitAxisTracer, trace, axis_name), args))
A:jax.interpreters.pxla.out_tracers->list(map(trace.full_raise, outs))
A:jax.interpreters.pxla.(out_vals, out_names)->unzip2(((t.val, t.axis_name) for t in out_tracers))
A:jax.interpreters.pxla.(vals_in, names_in)->unzip2(((t.val, t.axis_name) for t in tracers))
A:jax.interpreters.pxla.hard_idx->primitive.bind(dummy, **params)
A:jax.interpreters.pxla.(name,)->set((n for n in names_in if n is not not_mapped))
A:jax.interpreters.pxla.(val_out, is_mapped)->rule(vals_in, which_mapped, **params)
A:jax.interpreters.pxla.val_out->primitive.bind(*vals_in, **params)
A:jax.interpreters.pxla.rule->batching.get_primitive_batcher(primitive)
A:jax.interpreters.pxla.(val_out, axis_out)->rule(vals_in, axes_in, **params)
A:jax.interpreters.pxla.(vals, names)->unzip2(((t.val, t.axis_name) for t in tracers))
A:jax.interpreters.pxla.(f, names_out)->split_axis_subtrace(f, self.master, names)
A:jax.interpreters.pxla.vals_out->call_primitive.bind(f, *vals, **params)
A:jax.interpreters.pxla.vals_out_trans->map_primitive.bind(f, *vals_trans, **params)
jax.interpreters.pxla.ChunkedDeviceArray(self,axis_size,aval,device_buffers)
jax.interpreters.pxla.ChunkedDeviceArray.__getitem__(self,idx)
jax.interpreters.pxla.ChunkedDeviceArray.__init__(self,axis_size,aval,device_buffers)
jax.interpreters.pxla.DynamicAxisEnv(list)
jax.interpreters.pxla.DynamicAxisEnv.__contains__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.__getitem__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnvFrame(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.DynamicAxisEnvFrame.__init__(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.ShardedDeviceArray(self,aval,device_buffers)
jax.interpreters.pxla.ShardedDeviceArray.__getitem__(self,idx)
jax.interpreters.pxla.ShardedDeviceArray.__init__(self,aval,device_buffers)
jax.interpreters.pxla.ShardedDeviceArray._ids(self)
jax.interpreters.pxla.ShardedDeviceArray._value(self)
jax.interpreters.pxla.ShardedDeviceArray.copy_to_host_async(self)
jax.interpreters.pxla.ShardedDeviceArray.delete(self)
jax.interpreters.pxla.ShardedDeviceValue(xla.DeviceValue)
jax.interpreters.pxla.ShardedDeviceValue._check_if_deleted(self)
jax.interpreters.pxla.ShardedDeviceValue.block_until_ready(self)
jax.interpreters.pxla.SplitAxisTrace(core.Trace)
jax.interpreters.pxla.SplitAxisTrace.lift(self,val)
jax.interpreters.pxla.SplitAxisTrace.post_process_call(self,call_primitive,out_tracer,params)
jax.interpreters.pxla.SplitAxisTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.pure(self,val)
jax.interpreters.pxla.SplitAxisTrace.sublift(self,val)
jax.interpreters.pxla.SplitAxisTracer(self,trace,axis_name,val)
jax.interpreters.pxla.SplitAxisTracer.__init__(self,trace,axis_name,val)
jax.interpreters.pxla.SplitAxisTracer.aval(self)
jax.interpreters.pxla.SplitAxisTracer.full_lower(self)
jax.interpreters.pxla._ThreadLocalState(self)
jax.interpreters.pxla._ThreadLocalState.__init__(self)
jax.interpreters.pxla._axis_index_partial_eval(trace,_,**params)
jax.interpreters.pxla._axis_index_translation_rule(c,hard_size,soft_size,axis_name)
jax.interpreters.pxla._pmap_translation_rule(c,jaxpr,axis_env,const_nodes,freevar_nodes,in_nodes,axis_name,axis_size,devices,backend=None)
jax.interpreters.pxla._pval_to_result_handler(size,nrep,pval)
jax.interpreters.pxla._pvals_to_results_handler(size,nrep,out_pvals)
jax.interpreters.pxla._shard_abstract_array(size,x)
jax.interpreters.pxla._shard_array(x,ordinals,assignments,backend=None)
jax.interpreters.pxla._shard_device_array(x,ordinals,assignments,backend=None)
jax.interpreters.pxla._shard_sharded_device_array(x,ordinals,assignments)
jax.interpreters.pxla._xla_shard(c,sizes,x)
jax.interpreters.pxla._xla_shard_start_indices(c,axis_size,ndim)
jax.interpreters.pxla._xla_unshard(c,replica_groups,x)
jax.interpreters.pxla.add_chunk_to_axis_env(axis_name,soft_trace,soft_size)
jax.interpreters.pxla.apply_parallel_primitive(prim,*args,**params)
jax.interpreters.pxla.array_result_handler(size,nrep,aval)
jax.interpreters.pxla.assign_shards_to_replicas(nrep,size)
jax.interpreters.pxla.aval_to_result_handler(size,nrep,aval)
jax.interpreters.pxla.axis_index(axis_name)
jax.interpreters.pxla.compile_replicated(jaxpr,backend,axis_name,axis_size,devices,consts,*abstract_args)
jax.interpreters.pxla.execute_replicated(compiled,backend,nrep,in_handler,out_handler,*args)
jax.interpreters.pxla.extend_dynamic_axis_env(axis_name,pmap_trace,hard_size)
jax.interpreters.pxla.identity(x)
jax.interpreters.pxla.parallel_callable(fun,backend,axis_name,axis_size,devices,*avals)
jax.interpreters.pxla.replica_groups(nrep,mesh_spec,mesh_axes)
jax.interpreters.pxla.shard_args(backend,device_ordinals,assignments,axis_size,args)
jax.interpreters.pxla.shard_aval(size,aval)
jax.interpreters.pxla.split_axis(axis_name,chunk_size,*args)
jax.interpreters.pxla.split_axis_subtrace(master,names,*vals)
jax.interpreters.pxla.unmapped_device_count(backend=None)
jax.interpreters.pxla.xla_pmap_impl(fun,*args,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/batching.py----------------------------------------
A:jax.interpreters.batching.(out_vals, out_dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(fun, out_dims)->batch_subtrace(fun, master, in_dims)
A:jax.interpreters.batching.out_vals->fun.call_wrapped(*in_vals)
A:jax.interpreters.batching.trace->BatchTrace(master, core.cur_sublevel())
A:jax.interpreters.batching.in_tracers->map(partial(BatchTracer, trace), in_vals, in_dims)
A:jax.interpreters.batching.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.batching.NotMapped->type(None)
A:jax.interpreters.batching.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.batching.new_shape->tuple(onp.delete(aval.shape, self.batch_dim))
A:jax.interpreters.batching.(vals_in, dims_in)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.batched_primitive->get_primitive_batcher(primitive)
A:jax.interpreters.batching.(val_out, dim_out)->batched_primitive(vals_in, dims_in, **params)
A:jax.interpreters.batching.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(f, dims_out)->batch_subtrace(f, self.master, dims)
A:jax.interpreters.batching.vals_out->map_primitive.bind(f, *vals, **params)
A:jax.interpreters.batching.is_batched->tuple((d is not not_mapped for d in dims))
A:jax.interpreters.batching.dims->tuple((not_mapped if d is not_mapped else 0 for d in dims))
A:jax.interpreters.batching.dims_out->tuple((d + 1 if d is not not_mapped else d for d in dims_out()))
A:jax.interpreters.batching.primitive_batchers[prim]->partial(reducer_batcher, prim)
A:jax.interpreters.batching.d->next((d for d in dims if d is not not_mapped))
A:jax.interpreters.batching.ndim->max((onp.ndim(x) for x in args))
A:jax.interpreters.batching.axes->tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
A:jax.interpreters.batching.bdim_out->int(list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim))
A:jax.interpreters.batching.params->dict(params, input_shape=operand.shape)
A:jax.interpreters.batching.x->moveaxis(x, bdx, bdy)
A:jax.interpreters.batching.y->broadcast(y, x.shape[bdx], bdx)
A:jax.interpreters.batching.shape->list(onp.shape(x))
A:jax.interpreters.batching.broadcast_dims->tuple(onp.delete(onp.arange(len(shape)), axis))
A:jax.interpreters.batching.f->wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.batching.(f, batched_out)->batched_traceable(f, size, batched, instantiate)
A:jax.interpreters.batching.(jaxpr_out, pvals_out, consts_out)->pe.trace_to_jaxpr(f, in_pvals, instantiate=True)
A:jax.interpreters.batching.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.batching.jaxpr_out->core.TypedJaxpr(jaxpr_out, consts_out, avals_in, avals_out)
jax.interpreters.batching.BatchTrace(Trace)
jax.interpreters.batching.BatchTrace.lift(self,val)
jax.interpreters.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.batching.BatchTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.batching.BatchTrace.pure(self,val)
jax.interpreters.batching.BatchTrace.sublift(self,val)
jax.interpreters.batching.BatchTracer(self,trace,val,batch_dim)
jax.interpreters.batching.BatchTracer.__init__(self,trace,val,batch_dim)
jax.interpreters.batching.BatchTracer.aval(self)
jax.interpreters.batching.BatchTracer.full_lower(self)
jax.interpreters.batching._handle_scalar_broadcasting(nd,x,d)
jax.interpreters.batching._promote_aval_rank(sz,aval)
jax.interpreters.batching.add_batched(batched_args,batch_dims)
jax.interpreters.batching.batch(fun,in_vals,in_dims,out_dim_dests)
jax.interpreters.batching.batch_fun(fun,in_vals,in_dims)
jax.interpreters.batching.batch_jaxpr(jaxpr,size,batched,instantiate)
jax.interpreters.batching.batch_subtrace(master,in_dims,*in_vals)
jax.interpreters.batching.batched_traceable(size,batched,instantiate,*vals)
jax.interpreters.batching.bdim_at_front(x,bdim,size)
jax.interpreters.batching.broadcast(x,sz,axis)
jax.interpreters.batching.broadcast_batcher(prim,args,dims,**params)
jax.interpreters.batching.defbroadcasting(prim)
jax.interpreters.batching.defreducer(prim)
jax.interpreters.batching.defvectorized(prim)
jax.interpreters.batching.get_primitive_batcher(p)
jax.interpreters.batching.matchaxis(sz,src,dst,x)
jax.interpreters.batching.moveaxis(x,src,dst)
jax.interpreters.batching.reducer_batcher(prim,batched_args,batch_dims,axes,**params)
jax.interpreters.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax.interpreters.batching.zeros_like_batched(batched_args,batch_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/partial_eval.py----------------------------------------
A:jax.interpreters.partial_eval.(pvs, consts)->unzip2((t.pval for t in tracers))
A:jax.interpreters.partial_eval.tracers->map(self.instantiate_const, tracers)
A:jax.interpreters.partial_eval.out_aval->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.eqn->new_jaxpr_eqn([], out_tracers, call_primitive, (bound_subjaxpr,), params)
A:jax.interpreters.partial_eval.out_tracer->JaxprTracer(self, PartialVal((out_aval, unit)), None)
A:jax.interpreters.partial_eval.out_tracer.recipe->new_jaxpr_eqn(tracers, [out_tracer], primitive, (), params)
A:jax.interpreters.partial_eval.(in_pvs, in_consts)->unzip2([t.pval for t in tracers])
A:jax.interpreters.partial_eval.(fun, aux)->partial_eval(f, self, reduced_pvs)
A:jax.interpreters.partial_eval.out_flat->map_primitive.bind(fun, *in_consts, **params)
A:jax.interpreters.partial_eval.(out_pvs, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.(out_pv_consts, consts)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.const_tracers->map(trace.new_instantiated_const, consts)
A:jax.interpreters.partial_eval.(out_pvs_reduced, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.lifted_jaxpr->Jaxpr(constvars=(), freevars=jaxpr.freevars, invars=jaxpr.constvars + jaxpr.invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns)
A:jax.interpreters.partial_eval.(jaxpr, consts, env)->tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.interpreters.partial_eval.(out_pvs, out_pv_consts)->unzip2((t.pval for t in out_tracers))
A:jax.interpreters.partial_eval.n->len(jaxpr.outvars)
A:jax.interpreters.partial_eval.trace->JaxprTrace(master, core.cur_sublevel())
A:jax.interpreters.partial_eval.env_tracers->map(trace.full_raise, env)
A:jax.interpreters.partial_eval.map_primitives->set()
A:jax.interpreters.partial_eval.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.partial_eval.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.(_, pvals_out, _)->trace_to_jaxpr(lu.wrap_init(fun, params), pvals_in, instantiate=True)
A:jax.interpreters.partial_eval.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.partial_eval.Destructuring->namedtuple('Destructuring', ['i', 'eqn', 'key'])
A:jax.interpreters.partial_eval.aval->core.lattice_join(pv1, pv2)
A:jax.interpreters.partial_eval.instantiate->kwargs.pop('instantiate', False)
A:jax.interpreters.partial_eval.fun->trace_to_subjaxpr(fun, master, instantiate)
A:jax.interpreters.partial_eval.(jaxpr, (out_pvals, consts, env))->trace_to_subjaxpr(fun, master, instantiate).call_wrapped(pvals)
A:jax.interpreters.partial_eval.in_tracers->map(trace.new_arg, pvals)
A:jax.interpreters.partial_eval.out_tracers->map(partial(instantiate_const_at, trace), instantiate, out_tracers)
A:jax.interpreters.partial_eval.FreeVar->namedtuple('FreeVar', ['val'])
A:jax.interpreters.partial_eval.ConstVar->namedtuple('ConstVar', ['val'])
A:jax.interpreters.partial_eval.LambdaBinding->namedtuple('LambdaBinding', [])
A:jax.interpreters.partial_eval.invars->map(var, in_tracers)
A:jax.interpreters.partial_eval.outvars->map(var, out_tracers)
A:jax.interpreters.partial_eval.newvar->gensym('')
A:jax.interpreters.partial_eval.t_to_var->defaultdict(newvar)
A:jax.interpreters.partial_eval.sorted_tracers->toposort(out_tracers)
A:jax.interpreters.partial_eval.const_to_var->defaultdict(newvar)
A:jax.interpreters.partial_eval.processed_eqns->set()
A:jax.interpreters.partial_eval.(env_vars, env_vals)->unzip2(env.items())
A:jax.interpreters.partial_eval.(const_vars, const_vals)->unzip2(consts.items())
A:jax.interpreters.partial_eval.jaxpr->Jaxpr(const_vars, env_vars, invars, list(map(var, out_tracers)), eqns)
A:jax.interpreters.partial_eval.counter->itertools.count()
A:jax.interpreters.partial_eval.(jaxpr_2, out_pvals_2, consts_2)->trace_to_jaxpr(f, pvals, instantiate=instantiate)
A:jax.interpreters.partial_eval.(out_pvs_2, out_consts_2)->unzip2(out_pvals_2)
A:jax.interpreters.partial_eval.(jaxpr_1, out_pvals, consts_1)->trace_to_jaxpr(lu.wrap_init(fun), pvals, instantiate=True)
A:jax.interpreters.partial_eval.jaxpr_2->closure_convert_jaxpr(jaxpr_2)
A:jax.interpreters.partial_eval.(in_avals_1, in_avals_2)->unzip2(map(_split_aval, unknowns, jaxpr.in_avals))
A:jax.interpreters.partial_eval.(out_avals_1, out_avals_2)->unzip2(map(_split_aval, uk_out, jaxpr.out_avals))
A:jax.interpreters.partial_eval.(out_pvs, _)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.typed_jaxpr_1->TypedJaxpr(jaxpr_1, consts_1, in_avals_1, out_avals_1)
A:jax.interpreters.partial_eval.typed_jaxpr_2->TypedJaxpr(jaxpr_2, (), in_avals_2, out_avals_2)
jax.interpreters.partial_eval.JaxprTrace(Trace)
jax.interpreters.partial_eval.JaxprTrace.instantiate_const(self,tracer)
jax.interpreters.partial_eval.JaxprTrace.lift(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_arg(self,pval)
jax.interpreters.partial_eval.JaxprTrace.new_const(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_const(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_literal(self,val)
jax.interpreters.partial_eval.JaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.pure(self,val)
jax.interpreters.partial_eval.JaxprTrace.sublift(self,val)
jax.interpreters.partial_eval.JaxprTracer(self,trace,pval,recipe)
jax.interpreters.partial_eval.JaxprTracer.__init__(self,trace,pval,recipe)
jax.interpreters.partial_eval.JaxprTracer.__repr__(self)
jax.interpreters.partial_eval.JaxprTracer.aval(self)
jax.interpreters.partial_eval.JaxprTracer.full_lower(self)
jax.interpreters.partial_eval.JaxprTracer.ispure(self)
jax.interpreters.partial_eval.JaxprTracer.parents(self)
jax.interpreters.partial_eval.PartialVal(cls,xs)
jax.interpreters.partial_eval.PartialVal.__new__(cls,xs)
jax.interpreters.partial_eval.Var(self,count,suffix)
jax.interpreters.partial_eval.Var.__init__(self,count,suffix)
jax.interpreters.partial_eval.Var.__repr__(self)
jax.interpreters.partial_eval._mapped_aval(aval)
jax.interpreters.partial_eval._split_aval(unknown,aval)
jax.interpreters.partial_eval._unmapped_aval(size,aval)
jax.interpreters.partial_eval.abstract_eval_fun(fun,*avals,**params)
jax.interpreters.partial_eval.as_abstract_val(pv)
jax.interpreters.partial_eval.closure_convert_jaxpr(jaxpr)
jax.interpreters.partial_eval.eqn_parents(eqn)
jax.interpreters.partial_eval.eqn_tracer_to_var(var,eqn)
jax.interpreters.partial_eval.gensym(suffix)
jax.interpreters.partial_eval.identity(x)
jax.interpreters.partial_eval.instantiate_const_at(trace,instantiate,tracer)
jax.interpreters.partial_eval.join_pvals(pval1,pval2)
jax.interpreters.partial_eval.merge_pvals(val,pval)
jax.interpreters.partial_eval.partial_eval(f,trace,pvs)
jax.interpreters.partial_eval.partial_eval_jaxpr(jaxpr,unknowns,instantiate)
jax.interpreters.partial_eval.partial_eval_wrapper(avals,*consts)
jax.interpreters.partial_eval.partial_val_aval(pv,const)
jax.interpreters.partial_eval.trace_to_jaxpr(fun,pvals,**kwargs)
jax.interpreters.partial_eval.trace_to_subjaxpr(master,instantiate,pvals)
jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers,out_tracers)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/parallel.py----------------------------------------
A:jax.interpreters.parallel.(fun, _)->papply_transform(fun, name, axis_size)
A:jax.interpreters.parallel.trace->PapplyTrace(master, core.cur_sublevel())
A:jax.interpreters.parallel.in_tracers->map(partial(PapplyTracer, trace, name, axis_size, axis=0), args)
A:jax.interpreters.parallel.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.parallel.(out_vals, out_axes)->unzip2(((t.val, t.axis) for t in out_tracers))
A:jax.interpreters.parallel.NotSharded->type(None)
A:jax.interpreters.parallel.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.parallel.new_shape->list(aval.shape)
A:jax.interpreters.parallel.(names, vals, axes)->unzip3(((t.name, t.val, t.axis) for t in tracers))
A:jax.interpreters.parallel.(val_out, axis_out)->rule(name, size, vals, axes, **params)
A:jax.interpreters.parallel.(f_papply, axes_out)->papply_subtrace(f, self.master, name, size, axes)
A:jax.interpreters.parallel.vals_out->call_primitive.bind(f_papply, *vals, **params)
jax.interpreters.parallel.PapplyTrace(Trace)
jax.interpreters.parallel.PapplyTrace.lift(self,val)
jax.interpreters.parallel.PapplyTrace.post_process_call(self,call_primitive,out_tracer)
jax.interpreters.parallel.PapplyTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.parallel.PapplyTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.parallel.PapplyTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.parallel.PapplyTrace.pure(self,val)
jax.interpreters.parallel.PapplyTrace.sublift(self,val)
jax.interpreters.parallel.PapplyTracer(self,trace,name,axis_size,val,axis)
jax.interpreters.parallel.PapplyTracer.__init__(self,trace,name,axis_size,val,axis)
jax.interpreters.parallel.PapplyTracer.aval(self)
jax.interpreters.parallel.PapplyTracer.full_lower(self)
jax.interpreters.parallel.identity(x)
jax.interpreters.parallel.papply(fun,name,in_vals,axis_size)
jax.interpreters.parallel.papply_subtrace(master,name,axis_size,axes,*vals)
jax.interpreters.parallel.papply_transform(name,axis_size,*args)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/masking.py----------------------------------------
A:jax.interpreters.masking.xs->list(xs)
A:jax.interpreters.masking.ShapeEnvs->namedtuple('ShapeEnvs', ['logical', 'padded'])
A:jax.interpreters.masking.shape_envs->ShapeEnvs({}, {})
A:jax.interpreters.masking.new_logical->dict(it.chain(shape_envs.logical.items(), logical_env.items()))
A:jax.interpreters.masking.new_padded->dict(it.chain(shape_envs.padded.items(), padded_env.items()))
A:jax.interpreters.masking.(fun, out_shapes)->mask_subtrace(fun, master)
A:jax.interpreters.masking.out_vals->fun.call_wrapped(in_vals, shape_exprs)
A:jax.interpreters.masking.trace->ShapeCheckTrace(master, core.cur_sublevel())
A:jax.interpreters.masking.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.masking.(out_vals, out_shapes)->unzip2(((t.val, t.shape_expr) for t in out_tracers))
A:jax.interpreters.masking.new_poly->Poly()
A:jax.interpreters.masking.mon->Mon(mon1 + mon2)
A:jax.interpreters.masking.deg->int(deg)
A:jax.interpreters.masking.coeff->int(coeff)
A:jax.interpreters.masking.(([], _),)->poly.items()
A:jax.interpreters.masking.dims->map(parse_dim, spec.replace(' ', '').strip(',').split(','))
A:jax.interpreters.masking.terms->map(parse_dim, spec.split('*'))
A:jax.interpreters.masking.digits->frozenset(string.digits)
A:jax.interpreters.masking.identifiers->frozenset(string.ascii_lowercase)
A:jax.interpreters.masking.monomorphic_dim->MonomorphicDim()
A:jax.interpreters.masking.s_->S_()
A:jax.interpreters.masking.(vals, shape_exprs)->unzip2(((t.val, t.shape_expr) for t in tracers))
A:jax.interpreters.masking.(out, out_shape)->rule(shape_envs, vals, shape_exprs, **params)
A:jax.interpreters.masking.out_shape->shape_rules[primitive](shape_exprs, **params)
A:jax.interpreters.masking.logical_shapes->map(partial(eval_shape_expr, shape_envs.logical), shape_exprs)
A:jax.interpreters.masking.out->masking_rules[primitive](vals, logical_shapes, **params)
A:jax.interpreters.masking.masking_rules[prim]->partial(binop_masking_rule, prim)
A:jax.interpreters.masking.out_shapes->check_subtrace(fun, master).call_wrapped(in_shapes)
A:jax.interpreters.masking.in_tracers->map(partial(ShapeCheckTracer, trace), in_shapes)
A:jax.interpreters.masking.out_shape_expr->shape_rules[primitive](shape_exprs, **params)
jax.interpreters.masking.MaskTrace(Trace)
jax.interpreters.masking.MaskTrace.lift(self,val)
jax.interpreters.masking.MaskTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.masking.MaskTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.masking.MaskTrace.pure(self,val)
jax.interpreters.masking.MaskTrace.sublift(self,val)
jax.interpreters.masking.MaskTracer(self,trace,val,shape_expr)
jax.interpreters.masking.MaskTracer.__init__(self,trace,val,shape_expr)
jax.interpreters.masking.MaskTracer.aval(self)
jax.interpreters.masking.MaskTracer.full_lower(self)
jax.interpreters.masking.MaskTracer.is_pure(self)
jax.interpreters.masking.Mon(Counter)
jax.interpreters.masking.Mon.__hash__(self)
jax.interpreters.masking.Mon.__lt__(self,other)
jax.interpreters.masking.Mon.__str__(self)
jax.interpreters.masking.Mon.degree(self)
jax.interpreters.masking.MonomorphicDim(object)
jax.interpreters.masking.MonomorphicDim.__str__(self)
jax.interpreters.masking.Poly(Counter)
jax.interpreters.masking.Poly.__add__(p1,p2)
jax.interpreters.masking.Poly.__hash__(self)
jax.interpreters.masking.Poly.__mul__(p1,p2)
jax.interpreters.masking.Poly.__str__(self)
jax.interpreters.masking.S_(object)
jax.interpreters.masking.S_.__getitem__(self,idx)
jax.interpreters.masking.ShapeCheckTrace(Trace)
jax.interpreters.masking.ShapeCheckTrace.lift(self,val)
jax.interpreters.masking.ShapeCheckTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.masking.ShapeCheckTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.masking.ShapeCheckTrace.pure(self,val)
jax.interpreters.masking.ShapeCheckTrace.sublift(self,val)
jax.interpreters.masking.ShapeCheckTracer(self,trace,shape_expr)
jax.interpreters.masking.ShapeCheckTracer.__init__(self,trace,shape_expr)
jax.interpreters.masking.ShapeCheckTracer.aval(self)
jax.interpreters.masking.ShapeCheckTracer.full_lower(self)
jax.interpreters.masking.ShapeError(Exception)
jax.interpreters.masking.ShapeExpr(tuple)
jax.interpreters.masking.ShapeExpr.__getitem__(self,idx)
jax.interpreters.masking.ShapeExpr.__str__(self)
jax.interpreters.masking.ShapeSpec(list)
jax.interpreters.masking.ShapeSpec.__str__(self)
jax.interpreters.masking.ShapeSyntaxError(Exception)
jax.interpreters.masking.binop_masking_rule(prim,padded_vals,logical_shapes)
jax.interpreters.masking.binop_shape_rule(shape_exprs)
jax.interpreters.masking.check_subtrace(master,in_shapes)
jax.interpreters.masking.concrete_shape(shape)
jax.interpreters.masking.defbinop(prim)
jax.interpreters.masking.defvectorized(prim)
jax.interpreters.masking.eval_dim_expr(env,poly)
jax.interpreters.masking.eval_shape_expr(env,expr)
jax.interpreters.masking.extend_shape_envs(logical_env,padded_env)
jax.interpreters.masking.finalize_spec(spec,shape)
jax.interpreters.masking.is_constant(poly)
jax.interpreters.masking.mask_fun(fun,logical_env,padded_env,in_vals,shape_exprs)
jax.interpreters.masking.mask_subtrace(master,in_vals,shape_exprs)
jax.interpreters.masking.mul(coeff,mon)
jax.interpreters.masking.padded_shape_as_value(expr)
jax.interpreters.masking.parse_dim(spec)
jax.interpreters.masking.parse_id(name)
jax.interpreters.masking.parse_lit(val_str)
jax.interpreters.masking.parse_spec(spec='')
jax.interpreters.masking.pow(x,deg)
jax.interpreters.masking.prod(xs)
jax.interpreters.masking.shape_as_value(expr)
jax.interpreters.masking.shapecheck(fun,in_shapes)
jax.interpreters.masking.vectorized_masking_rule(prim,padded_vals,logical_shapes,**params)
jax.interpreters.masking.vectorized_shape_rule(shape_exprs,**unused_params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/ad.py----------------------------------------
A:jax.interpreters.ad.(fun, aux)->jvp_subtrace_aux(fun)
A:jax.interpreters.ad.trace->JVPTrace(master, core.cur_sublevel())
A:jax.interpreters.ad.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.ans_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.aux_tracers->map(trace.full_raise, aux)
A:jax.interpreters.ad.(out_primals, out_tangents)->unzip2(((t.primal, t.tangent) for t in ans_tracers))
A:jax.interpreters.ad.(aux_primals, _)->unzip2(((t.primal, t.tangent) for t in aux_tracers))
A:jax.interpreters.ad.has_aux->kwargs.pop('has_aux', False)
A:jax.interpreters.ad.jvpfun->jvp(traceable)
A:jax.interpreters.ad.(jvpfun, aux)->jvp(traceable, has_aux=True)
A:jax.interpreters.ad.(_, in_tree)->tree_flatten(((primals, primals), {}))
A:jax.interpreters.ad.(jvpfun_flat, out_tree)->flatten_fun(jvpfun, in_tree)
A:jax.interpreters.ad.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(jvpfun_flat, in_pvals)
A:jax.interpreters.ad.(pval_primals, pval_tangents)->tree_unflatten(out_tree(), out_pvals)
A:jax.interpreters.ad.(aval_primals, const_primals)->unzip2(pval_primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts)->linearize(traceable, *primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts, aux)->linearize(traceable, *primals, has_aux=True)
A:jax.interpreters.ad.cts->map(instantiate_zeros_aval, kwargs['out_avals'], cts)
A:jax.interpreters.ad.(_, arg_cts)->backward_pass(jaxpr, consts, (), dummy_args, dummy_primals_and_cts)
A:jax.interpreters.ad.invals->map(read_primal, eqn.invars)
A:jax.interpreters.ad.cts_in->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.(cts_in,)->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.sub_consts->map(read_primal, const_vars)
A:jax.interpreters.ad.sub_freevar_vals->map(read_primal, bound_vars)
A:jax.interpreters.ad.(ct_free_vars_out, cts_out)->get_primitive_transpose(eqn.primitive)(eqn.params, subjaxpr, sub_consts, sub_freevar_vals, invals, cts_in)
A:jax.interpreters.ad.cts_out->get_primitive_transpose(eqn.primitive)(cts_in, *invals, **eqn.params)
A:jax.interpreters.ad.freevar_cts->map(read_cotangent, jaxpr.freevars)
A:jax.interpreters.ad.cotangents_out->map(read_cotangent, jaxpr.invars)
A:jax.interpreters.ad.undefined_primal->UndefinedPrimal()
A:jax.interpreters.ad.(primals_in, tangents_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax.interpreters.ad.(primal_out, tangent_out)->tree_unflatten(out_tree_def(), result)
A:jax.interpreters.ad.(nonzero_tangents, in_tree_def)->tree_flatten(tangents)
A:jax.interpreters.ad.(f_jvp, out_tree_def)->traceable(jvp_subtrace(f, self.master), len(primals), in_tree_def)
A:jax.interpreters.ad.result->call_primitive.bind(f_jvp, *primals + nonzero_tangents, **params)
A:jax.interpreters.ad.(primals, tangents)->split_list(tracers, [len(tracers) // 2])
A:jax.interpreters.ad.primal_aval->raise_to_shaped(get_aval(primal))
A:jax.interpreters.ad.tangent_aval->raise_to_shaped(get_aval(tangent))
A:jax.interpreters.ad.primitive_jvps[primitive]->partial(zero_jvp, primitive)
A:jax.interpreters.ad.primitive_transposes[primitive]->partial(linear_transpose, transpose_rule)
A:jax.interpreters.ad.val_out->primitive.bind(*primals, **params)
A:jax.interpreters.ad.tangents->map(instantiate_zeros, primals, tangents)
A:jax.interpreters.ad.ts->map(instantiate_zeros, xs, ts)
A:jax.interpreters.ad.primals_and_tangents->core.Primitive('{name}_jvp'.format(name=name)).bind(*it.chain(xs, ts), **params)
A:jax.interpreters.ad.fun_jvp_p->core.Primitive('{name}_jvp'.format(name=name))
A:jax.interpreters.ad.(primals_out, vjp_py)->custom_vjp(*primals, **params)
A:jax.interpreters.ad.(jaxpr, _, res)->pe.trace_to_jaxpr(wrap_init(vjp_py), ct_pvals, instantiate=True)
A:jax.interpreters.ad.tangents_out->core.Primitive('{name}_lin'.format(name=name)).bind(*it.chain(res, tangents), trans_jaxpr=jaxpr, num_res=len(res), out_avals=out_avals)
A:jax.interpreters.ad.fun_lin_p->core.Primitive('{name}_lin'.format(name=name))
A:jax.interpreters.ad.(res, _)->split_list(args, [num_res])
A:jax.interpreters.ad.outs->core.eval_jaxpr(trans_jaxpr, res, (), *cts)
A:jax.interpreters.ad.ans->prim.bind(*primals)
A:jax.interpreters.ad.primitive_transposes[prim]->partial(bilinear_transpose, lhs_rule, rhs_rule)
A:jax.interpreters.ad.defbilinear->partial(defbilinear_broadcasting, lambda g, x: g)
A:jax.interpreters.ad.new_tangents->tree_unflatten(in_tree_def, new_tangents)
A:jax.interpreters.ad.(out_flat, tree_def)->tree_flatten((primal_out, tangent_out))
A:jax.interpreters.ad.(all_args, in_tree_def)->tree_flatten((consts, freevar_vals, args, ct))
A:jax.interpreters.ad.fun->wrap_init(partial(backward_pass, jaxpr))
A:jax.interpreters.ad.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax.interpreters.ad.out_flat->primitive.bind(fun, *all_args, **params)
A:jax.interpreters.ad.primitive_transposes[core.call_p]->partial(call_transpose, call_p)
A:jax.interpreters.ad.(freevar_cts, arg_cts)->tree_unflatten(out_tree(), out_flat)
A:jax.interpreters.ad.f->wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.ad.(f_jvp, out_nonzeros)->f_jvp_traceable(jvp(f, instantiate=instantiate), nonzeros)
A:jax.interpreters.ad.avals_in->list(it.chain(jaxpr.in_avals, tangent_avals))
A:jax.interpreters.ad.(jaxpr_out, pvals_out, literals_out)->pe.trace_to_jaxpr(f_jvp, pvals, instantiate=True)
A:jax.interpreters.ad.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.ad.jaxpr_out->core.TypedJaxpr(jaxpr_out, literals_out, avals_in, avals_out)
A:jax.interpreters.ad.num_primals->len(nonzeros)
A:jax.interpreters.ad.primals->list(primals_and_nztangents[:num_primals])
A:jax.interpreters.ad.nonzero_tangents->iter(primals_and_nztangents[num_primals:])
A:jax.interpreters.ad.new_invars->_perm(primals_in, tangents_in, jaxpr.jaxpr.invars)
A:jax.interpreters.ad.new_outvars->_perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)
A:jax.interpreters.ad.new_jaxpr->core.Jaxpr(jaxpr.jaxpr.constvars, jaxpr.jaxpr.freevars, new_invars, new_outvars, jaxpr.jaxpr.eqns)
A:jax.interpreters.ad.new_in_avals->_perm(primals_in, tangents_in, jaxpr.in_avals)
A:jax.interpreters.ad.new_out_avals->_perm(primals_out, tangents_out, jaxpr.out_avals)
A:jax.interpreters.ad.new_typed_jaxpr->core.TypedJaxpr(new_jaxpr, jaxpr.literals, new_in_avals, new_out_avals)
A:jax.interpreters.ad.n->sum(primal_counts)
A:jax.interpreters.ad.primal_groups->split_list(primals, primal_counts[:-1])
A:jax.interpreters.ad.tangent_groups->split_list(tangents, tangent_counts[:-1])
jax.interpreters.ad.JVPTrace(Trace)
jax.interpreters.ad.JVPTrace.join(self,xt,yt)
jax.interpreters.ad.JVPTrace.lift(self,val)
jax.interpreters.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.ad.JVPTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.ad.JVPTrace.pure(self,val)
jax.interpreters.ad.JVPTrace.sublift(self,val)
jax.interpreters.ad.JVPTracer(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.__init__(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.aval(self)
jax.interpreters.ad.JVPTracer.full_lower(self)
jax.interpreters.ad.UndefinedPrimal(object)
jax.interpreters.ad.UndefinedPrimal.__repr__(self)
jax.interpreters.ad._interleave(xs,ys)
jax.interpreters.ad._perm(primal_counts,tangent_counts,lst)
jax.interpreters.ad._primal_tangent_shapes_match(primal,tangent)
jax.interpreters.ad.add_tangents(x,y)
jax.interpreters.ad.backward_pass(jaxpr,consts,freevar_vals,args,cotangents_in)
jax.interpreters.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax.interpreters.ad.call_transpose(primitive,params,jaxpr,consts,freevar_vals,args,ct)
jax.interpreters.ad.defbilinear_broadcasting(bcast,prim,lhs_rule,rhs_rule)
jax.interpreters.ad.defjvp(primitive,*jvprules)
jax.interpreters.ad.defjvp2(primitive,*jvprules)
jax.interpreters.ad.defjvp_zero(primitive)
jax.interpreters.ad.deflinear(primitive,transpose_rule)
jax.interpreters.ad.defvjp(prim,*vjps)
jax.interpreters.ad.defvjp2(prim,*vjps)
jax.interpreters.ad.defvjp_all(prim,custom_vjp)
jax.interpreters.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax.interpreters.ad.get_primitive_transpose(p)
jax.interpreters.ad.identity(x)
jax.interpreters.ad.ignore_consts(ct,pval)
jax.interpreters.ad.instantiate_zeros(example,tangent)
jax.interpreters.ad.instantiate_zeros_aval(aval,tangent)
jax.interpreters.ad.jvp(fun,has_aux=False,instantiate=True)
jax.interpreters.ad.jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax.interpreters.ad.jvp_subtrace(master,primals,tangents)
jax.interpreters.ad.jvp_subtrace_aux(master,primals,tangents)
jax.interpreters.ad.jvpfun(instantiate,primals,tangents)
jax.interpreters.ad.linear_jvp(primitive,primals,tangents,**params)
jax.interpreters.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linearize(traceable,*primals,**kwargs)
jax.interpreters.ad.map_transpose(primitive,params,jaxpr,consts,freevar_vals,args,ct)
jax.interpreters.ad.rearrange_binders(jaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax.interpreters.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.traceable(num_primals,in_tree_def,*primals_and_tangents)
jax.interpreters.ad.unpair_pval(pval)
jax.interpreters.ad.vjp(traceable,primals,has_aux=False)
jax.interpreters.ad.zero_jvp(primitive,primals,tangents,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/interpreters/xla.py----------------------------------------
A:jax.interpreters.xla.x->canonicalize_dtype(x)
A:jax.interpreters.xla.abstract_args->map(abstractify, args)
A:jax.interpreters.xla.compiled_fun->_xla_callable(fun, device_assignment, backend, *map(abstractify, args))
A:jax.interpreters.xla.backend->params.get('backend', None)
A:jax.interpreters.xla.aval_out->prim.abstract_eval(*abstract_args, **params)
A:jax.interpreters.xla.handlers->tuple(map(aval_to_result_handler, aval_out))
A:jax.interpreters.xla.handle_result->aval_to_result_handler(aval_out)
A:jax.interpreters.xla.xla_shapes->tuple(map(c.GetShape, xla_args))
A:jax.interpreters.xla.built_c->jaxpr_computation(jaxpr, backend, axis_env, consts, (), *xla_shapes)
A:jax.interpreters.xla.compiled->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').Build(xla_const).Compile((), opts, backend=xb.get_backend(backend))
A:jax.interpreters.xla.c->lib.xla_bridge.make_computation_builder('constant_instantiating_computation')
A:jax.interpreters.xla.xla_args->map(c.ParameterWithShape, xla_shapes)
A:jax.interpreters.xla.(device_num,)->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').Build(xla_const).Compile((), opts, backend=xb.get_backend(backend)).DeviceOrdinals()
A:jax.interpreters.xla.out_buf->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').Build(xla_const).Compile((), opts, backend=xb.get_backend(backend)).Execute(input_bufs)
A:jax.interpreters.xla.arg_shapes->map(aval_to_xla_shape, abstract_args)
A:jax.interpreters.xla.compile_opts->lib.xla_bridge.get_compile_options(num_replicas=axis_env.nreps, device_assignment=device_assignment)
A:jax.interpreters.xla.consts->_map(c.Constant, const_vals)
A:jax.interpreters.xla.freevars->_map(c.ParameterWithShape, freevar_shapes)
A:jax.interpreters.xla.args->_map(c.ParameterWithShape, arg_shapes)
A:jax.interpreters.xla.out_nodes->jaxpr_subcomp(subc, jaxpr, backend, axis_env, consts, freevars, *args)
A:jax.interpreters.xla.in_nodes->list(map(read, eqn.invars))
A:jax.interpreters.xla.ans->rule(c, subjaxpr, axis_env, const_nodes, freevar_nodes, in_nodes, backend=backend, **new_params)
A:jax.interpreters.xla.new_params->check_backend_params(eqn.params, backend)
A:jax.interpreters.xla.replica_groups->axis_groups(axis_env, new_params['axis_name'])
A:jax.interpreters.xla.const_nodes->_map(read, const_bindings)
A:jax.interpreters.xla.freevar_nodes->_map(read, freevar_bindings)
A:jax.interpreters.xla.num_elements->len(c.GetShape(ans).tuple_shapes())
A:jax.interpreters.xla.inner_backend->params.get('backend', None)
A:jax.interpreters.xla.mesh_axes->tuple(map(partial(axis_read, axis_env), name))
A:jax.interpreters.xla.(trailing_size, ragged)->divmod(nrep, prod(mesh_spec))
A:jax.interpreters.xla.iota->numpy.arange(prod(full_spec)).reshape(full_spec)
A:jax.interpreters.xla.groups->numpy.reshape(onp.moveaxis(iota, mesh_axes, onp.arange(len(mesh_axes))), (prod(onp.take(full_spec, mesh_axes)), -1))
A:jax.interpreters.xla.(jaxpr, (pvals, consts, env))->pe.trace_to_subjaxpr(fun, master, False).call_wrapped(pvals)
A:jax.interpreters.xla.axis_env->AxisEnv(jaxpr_replicas(jaxpr), [], [])
A:jax.interpreters.xla.result_handlers->tuple(map(_pval_to_result_handler, pvals))
A:jax.interpreters.xla.out_bufs->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').Build(xla_const).Compile((), opts, backend=xb.get_backend(backend)).ExecutePerReplica(input_bufs)[0].destructure()
A:jax.interpreters.xla.xla_call_p->core.Primitive('xla_call')
A:jax.interpreters.xla.xla_call->partial(core.call_bind, xla_call_p)
A:jax.interpreters.xla.subc->subc.Build(subc.Tuple(*out_nodes)).Build(subc.Tuple(*out_nodes))
A:jax.interpreters.xla.ad.primitive_transposes[xla_call_p]->partial(ad.call_transpose, xla_call_p)
A:jax.interpreters.xla.backend_specific_translations->defaultdict(dict)
A:jax.interpreters.xla.shape->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').GetShape(x)
A:jax.interpreters.xla.zero->lib.xla_bridge.make_computation_builder('constant_instantiating_computation').Constant(onp.array(0, shape.element_type()))
A:jax.interpreters.xla.avals->map(_aval_from_xla_shape, xla_shapes)
A:jax.interpreters.xla.(jaxpr, _, consts)->pe.trace_to_jaxpr(lu.wrap_init(fun, new_params), pvals, instantiate=True)
A:jax.interpreters.xla._forward_to_value->partial(_forward_method, '_value')
A:jax.interpreters.xla.self._npy_value->self.device_buffer.to_py()
A:jax.interpreters.xla.__str__->partialmethod(_forward_to_value, str)
A:jax.interpreters.xla.__bool____nonzero__->partialmethod(_forward_to_value, bool)
A:jax.interpreters.xla.__float__->partialmethod(_forward_to_value, float)
A:jax.interpreters.xla.__int__->partialmethod(_forward_to_value, int)
A:jax.interpreters.xla.__long__->partialmethod(_forward_to_value, long)
A:jax.interpreters.xla.__complex__->partialmethod(_forward_to_value, complex)
A:jax.interpreters.xla.__hex__->partialmethod(_forward_to_value, hex)
A:jax.interpreters.xla.__oct__->partialmethod(_forward_to_value, oct)
A:jax.interpreters.xla.__index__->partialmethod(_forward_to_value, op.index)
A:jax.interpreters.xla.__reduce__->partialmethod(_forward_to_value, op.methodcaller('__reduce__'))
A:jax.interpreters.xla.a->abstractify(x)
A:jax.interpreters.xla.handler->aval_to_result_handler(a)
A:jax.interpreters.xla.device_put_p->core.Primitive('device_put')
A:jax.interpreters.xla.xla_const->const.constant_handler(c, const)
A:jax.interpreters.xla.opts->lib.xla_bridge.get_compile_options(device_assignment=(device_num,))
jax.interpreters.xla.AxisEnv(self,nreps=1,names=None,sizes=None,devices=None)
jax.interpreters.xla.AxisEnv.__init__(self,nreps=1,names=None,sizes=None,devices=None)
jax.interpreters.xla.DeviceArray(self,aval,device_buffer)
jax.interpreters.xla.DeviceArray.__array__(self,dtype=None,context=None)
jax.interpreters.xla.DeviceArray.__eq__(self,other)
jax.interpreters.xla.DeviceArray.__format__(self,format_spec)
jax.interpreters.xla.DeviceArray.__hash__(self)
jax.interpreters.xla.DeviceArray.__init__(self,aval,device_buffer)
jax.interpreters.xla.DeviceArray.__iter__(self)
jax.interpreters.xla.DeviceArray.__len__(self)
jax.interpreters.xla.DeviceArray.__repr__(self)
jax.interpreters.xla.DeviceArray.__reversed__(self)
jax.interpreters.xla.DeviceArray._value(self)
jax.interpreters.xla.DeviceArray.copy(self)
jax.interpreters.xla.DeviceArray.copy_to_host_async(self)
jax.interpreters.xla.DeviceArray.delete(self)
jax.interpreters.xla.DeviceArray.dtype(self)
jax.interpreters.xla.DeviceArray.item(self)
jax.interpreters.xla.DeviceArray.ndim(self)
jax.interpreters.xla.DeviceArray.shape(self)
jax.interpreters.xla.DeviceArray.size(self)
jax.interpreters.xla.DeviceConstant(DeviceArray)
jax.interpreters.xla.DeviceConstant.constant_handler(c,constant_instance,canonicalize_types=True)
jax.interpreters.xla.DeviceConstant.copy_to_host_async(self)
jax.interpreters.xla.DeviceValue(self,aval,device_buffer)
jax.interpreters.xla.DeviceValue.__init__(self,aval,device_buffer)
jax.interpreters.xla.DeviceValue._check_if_deleted(self)
jax.interpreters.xla.DeviceValue.block_until_ready(self)
jax.interpreters.xla._aval_from_xla_shape(xla_shape)
jax.interpreters.xla._axis_groups(nrep,mesh_spec,mesh_axes)
jax.interpreters.xla._canonicalize_ndarray_dtype(x)
jax.interpreters.xla._check_nans(name,xla_shape,buf)
jax.interpreters.xla._device_array_constant_handler(c,val,canonicalize_types=True)
jax.interpreters.xla._device_put_array(x,n,backend=None)
jax.interpreters.xla._device_put_device_array(x,device_num,backend)
jax.interpreters.xla._device_put_impl(x,device_num=0,backend=None)
jax.interpreters.xla._execute_compiled(compiled,backend,handlers,*args)
jax.interpreters.xla._execute_compiled_primitive(prim,compiled,backend,result_handler,*args)
jax.interpreters.xla._execute_replicated(compiled,backend,handlers,*args)
jax.interpreters.xla._forward_method(attrname,self,fun,*args)
jax.interpreters.xla._instantiate_device_constant(const,device_num=0,backend=None,cutoff=1000000.0)
jax.interpreters.xla._map(f,*xs)
jax.interpreters.xla._pval_to_result_handler(pval)
jax.interpreters.xla._xla_call_impl(fun,*args,**params)
jax.interpreters.xla._xla_call_translation_rule(c,jaxpr,axis_env,const_nodes,freevar_nodes,in_nodes,device_assignment=None,backend=None)
jax.interpreters.xla._xla_callable(fun,device_assignment,backend,*abstract_args)
jax.interpreters.xla.abstractify(x)
jax.interpreters.xla.add_jaxvals_translation_rule(c,x,y)
jax.interpreters.xla.apply_primitive(prim,*args,**params)
jax.interpreters.xla.array_result_handler(aval)
jax.interpreters.xla.aval_to_result_handler(aval)
jax.interpreters.xla.aval_to_xla_shape(aval)
jax.interpreters.xla.axis_groups(axis_env,name)
jax.interpreters.xla.axis_read(axis_env,axis_name)
jax.interpreters.xla.build_jaxpr(jaxpr,backend,axis_env,const_vals,*abstract_args)
jax.interpreters.xla.canonicalize_dtype(x)
jax.interpreters.xla.check_backend_params(params,outer_backend)
jax.interpreters.xla.check_nans(prim,bufs)
jax.interpreters.xla.compile_jaxpr(jaxpr,device_assignment,backend,axis_env,const_vals,*abstract_args)
jax.interpreters.xla.device_put(x,device_num=0,backend=None)
jax.interpreters.xla.eqn_literals(eqn)
jax.interpreters.xla.eqn_replicas(eqn)
jax.interpreters.xla.extend_axis_env(env,name,size)
jax.interpreters.xla.identity(x)
jax.interpreters.xla.jaxpr_computation(jaxpr,backend,axis_env,const_vals,freevar_shapes,*arg_shapes)
jax.interpreters.xla.jaxpr_literals(jaxpr)
jax.interpreters.xla.jaxpr_replicas(jaxpr)
jax.interpreters.xla.jaxpr_subcomp(c,jaxpr,backend,axis_env,consts,freevars,*args)
jax.interpreters.xla.lower_fun(fun,instantiate=False,initial_style=False)
jax.interpreters.xla.prefetch(x)
jax.interpreters.xla.primitive_computation(prim,*xla_shapes,**params)
jax.interpreters.xla.xla_destructure(c,ans)
jax.interpreters.xla.xla_primitive_callable(prim,*abstract_args,**params)
jax.interpreters.xla.zeros_like_translation_rule(c,x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/nn/functions.py----------------------------------------
A:jax.nn.functions.unnormalized->jax.numpy.exp(x - x.max(axis, keepdims=True))
A:jax.nn.functions.mean->jax.numpy.mean(x, axis, keepdims=True)
jax.nn.celu(x,alpha=1.0)
jax.nn.elu(x,alpha=1.0)
jax.nn.functions.celu(x,alpha=1.0)
jax.nn.functions.elu(x,alpha=1.0)
jax.nn.functions.gelu(x)
jax.nn.functions.glu(x,axis=-1)
jax.nn.functions.hard_tanh(x)
jax.nn.functions.leaky_relu(x,negative_slope=0.01)
jax.nn.functions.log_sigmoid(x)
jax.nn.functions.log_softmax(x,axis=-1)
jax.nn.functions.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.functions.relu(x)
jax.nn.functions.selu(x)
jax.nn.functions.sigmoid(x)
jax.nn.functions.soft_sign(x)
jax.nn.functions.softmax(x,axis=-1)
jax.nn.functions.softplus(x)
jax.nn.functions.swish(x)
jax.nn.gelu(x)
jax.nn.glu(x,axis=-1)
jax.nn.hard_tanh(x)
jax.nn.leaky_relu(x,negative_slope=0.01)
jax.nn.log_sigmoid(x)
jax.nn.log_softmax(x,axis=-1)
jax.nn.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.relu(x)
jax.nn.selu(x)
jax.nn.sigmoid(x)
jax.nn.soft_sign(x)
jax.nn.softmax(x,axis=-1)
jax.nn.softplus(x)
jax.nn.swish(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/nn/initializers.py----------------------------------------
A:jax.nn.initializers.(fan_in, fan_out)->_compute_fans(shape, in_axis, out_axis)
A:jax.nn.initializers.variance->jax.numpy.array(scale / denominator, dtype=dtype)
A:jax.nn.initializers.glorot_uniform->partial(variance_scaling, 1.0, 'fan_avg', 'uniform')
A:jax.nn.initializers.glorot_normal->partial(variance_scaling, 1.0, 'fan_avg', 'truncated_normal')
A:jax.nn.initializers.lecun_uniform->partial(variance_scaling, 1.0, 'fan_in', 'uniform')
A:jax.nn.initializers.lecun_normal->partial(variance_scaling, 1.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.kaiming_uniformhe_uniform->partial(variance_scaling, 2.0, 'fan_in', 'uniform')
A:jax.nn.initializers.kaiming_normalhe_normal->partial(variance_scaling, 2.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.A->jax.random.normal(key, matrix_shape, dtype)
A:jax.nn.initializers.(Q, R)->jax.numpy.linalg.qr(A)
A:jax.nn.initializers.Q->jax.numpy.moveaxis(Q, -1, column_axis)
jax.nn.initializers._compute_fans(shape,in_axis=-2,out_axis=-1)
jax.nn.initializers.normal(stddev=0.01)
jax.nn.initializers.ones(key,shape,dtype=np.float32)
jax.nn.initializers.orthogonal(scale=1.0,column_axis=-1)
jax.nn.initializers.uniform(scale=0.01)
jax.nn.initializers.variance_scaling(scale,mode,distribution,in_axis=-2,out_axis=-1)
jax.nn.initializers.zeros(key,shape,dtype=np.float32)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax/lax.py----------------------------------------
A:jax.lax.lax.ndim->_max((len(shape) for shape in shapes))
A:jax.lax.lax.shapes->numpy.array([operand.shape for operand in operands])
A:jax.lax.lax.min_shape->numpy.min(shapes, axis=0)
A:jax.lax.lax.max_shape->numpy.max(shapes, axis=0)
A:jax.lax.lax.result_shape->numpy.floor_divide(onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
A:jax.lax.lax.new_dtype->lib.xla_bridge.canonicalize_dtype(new_dtype)
A:jax.lax.lax.old_dtype->_dtype(operand)
A:jax.lax.lax.operand->interpreters.batching.moveaxis(operand, o_bdims, 0)
A:jax.lax.lax.dimension_numbers->_conv_general_proto(dimension_numbers)
A:jax.lax.lax.padding->list(map(onp.sum, padding))
A:jax.lax.lax.lhs_shape->numpy.shape(lhs)
A:jax.lax.lax.lhs_ndim->len(lhs_shape)
A:jax.lax.lax.rhs_ndim->numpy.ndim(rhs)
A:jax.lax.lax.lhs->interpreters.batching.moveaxis(lhs, lbd, lhs.ndim - 1)
A:jax.lax.lax.rhs->interpreters.batching.moveaxis(rhs, rbd, rhs.ndim - 1)
A:jax.lax.lax.contract_dims->tuple(map(tuple, contract_dims))
A:jax.lax.lax.batch_dims->tuple(map(tuple, batch_dims))
A:jax.lax.lax.lhs_noncontract_dims->tuple(sorted(set(range(onp.ndim(lhs))) - set(lhs_batch_dims) - set(lhs_contract_dims)))
A:jax.lax.lax.rhs_noncontract_dims->tuple(sorted(set(range(onp.ndim(rhs))) - set(rhs_batch_dims) - set(rhs_contract_dims)))
A:jax.lax.lax.new_lhs_shape->numpy.insert(onp.shape(lhs), len(lhs_batch_dims) + len(lhs_noncontract_dims), (1,) * len(rhs_noncontract_dims))
A:jax.lax.lax.new_rhs_shape->numpy.insert(onp.shape(rhs), len(lhs_batch_dims), (1,) * len(lhs_noncontract_dims))
A:jax.lax.lax.new_sizes->tuple(new_sizes)
A:jax.lax.lax.start_indices->concatenate([counts, start_indices], len(count_shape) - 1)
A:jax.lax.lax.(jaxpr, consts)->_reduction_jaxpr(mul, _abstractify(init_value))
A:jax.lax.lax.indices->concatenate([broadcast_in_dim(x, (size, 1), broadcast_dimensions=(0,) if i is not None else ()) for (x, i) in zip(indices, bdims)], dimension=1)
A:jax.lax.lax.slice_sizes->iter(onp.delete(slice_sizes, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.offset_dims->tuple(onp.add(1, dimension_numbers.offset_dims))
A:jax.lax.lax.dnums->ScatterDimensionNumbers(update_window_dims=update_window_dims, inserted_window_dims=inserted_window_dims, scatter_dims_to_operand_dims=scatter_dims_to_operand_dims)
A:jax.lax.lax.permutation->tuple(permutation)
A:jax.lax.lax.monoid_reducer->_get_monoid_window_reducer(computation, init_value)
A:jax.lax.lax.pval->interpreters.partial_eval.PartialVal((aval, core.unit))
A:jax.lax.lax.comp->lu.wrap_init(lambda x, y: (computation(x, y),))
A:jax.lax.lax.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(comp, (pval, pval), instantiate=False)
A:jax.lax.lax.aval->ShapedArray(new_sizes, operand.dtype)
A:jax.lax.lax.dtype->lib.xla_bridge.canonicalize_dtype(dtype)
A:jax.lax.lax.init_value->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Constant(onp.array(0, dtype))
A:jax.lax.lax.(select_jaxpr, select_consts)->_reduction_jaxpr(select, _abstractify(init_value))
A:jax.lax.lax.(scatter_jaxpr, scatter_consts)->_reduction_jaxpr(scatter, _abstractify(init_value))
A:jax.lax.lax.result->numpy.asarray(_reduce(operator.and_, eyes), self.dtype)
A:jax.lax.lax.shape->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').GetShape(operand)
A:jax.lax.lax.val->numpy.asarray(fill_value, dtype)
A:jax.lax.lax.dimension->kwargs.pop('dimension')
A:jax.lax.lax.axes->tuple(onp.delete(range(len(shape)), broadcast_dimensions))
A:jax.lax.lax.pads->padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
A:jax.lax.lax.pad_a->int(onp.ceil(pad_len / 2))
A:jax.lax.lax.x->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').ParameterWithShape(xla_client.Shape.array_shape(onp.dtype(double_word_dtype), ()))
A:jax.lax.lax.ndims->len(lhs.shape)
A:jax.lax.lax.dn->conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)
A:jax.lax.lax.k_shape->numpy.take(rhs.shape, dn.rhs_spec)
A:jax.lax.lax.out->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').ReduceWindow(pack(operand, tangents), pack(const(c, dtype, init), const(c, dtype, 0)), reducer(), window_dimensions, window_strides, padding)
A:jax.lax.lax.size->next((x.shape[ax] for (x, ax) in zip(batched_args, batch_dims) if ax is not None))
A:jax.lax.lax.limit_indices->list(operand.shape)
A:jax.lax.lax.axis->int(axis)
A:jax.lax.lax.start_indices[axis]->int(start_index)
A:jax.lax.lax.limit_indices[axis]->int(limit_index)
A:jax.lax.lax.strides[axis]->int(stride)
A:jax.lax.lax.slice_sizes[axis]->int(slice_size)
A:jax.lax.lax.update->reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax + 1:])
A:jax.lax.lax.batch->tuple(range(lhs.ndim - 2))
A:jax.lax.lax.log_half->_const(x, onp.log(0.5))
A:jax.lax.lax.ShapedArray.broadcast->core.aval_method(broadcast)
A:jax.lax.lax.ShapedArray.transpose->core.aval_method(transpose)
A:jax.lax.lax.ShapedArray.reshape->core.aval_method(reshape)
A:jax.lax.lax.ShapedArray._iter->staticmethod(_iter)
A:jax.lax.lax.prim->standard_primitive(shape_rule, dtype_rule, name, translation_rule=translation_rule)
A:jax.lax.lax.least_specialized->_max(map(type, args), key=operator.attrgetter('array_abstraction_level'))
A:jax.lax.lax.xla_opname->''.join((term.capitalize() for term in name.split('_')))
A:jax.lax.lax.typename->str(onp.dtype(aval_dtype).name)
A:jax.lax.lax.dtype_rule->partial(binop_dtype_rule, result_dtype, accepted_dtypes, name)
A:jax.lax.lax.standard_unop->partial(unop, _identity)
A:jax.lax.lax.typenames->', '.join((str(onp.dtype(t).name) for t in types))
A:jax.lax.lax.shape_rule->partial(_broadcasting_shape_rule, name)
A:jax.lax.lax.standard_binop->partial(binop, _input_dtype)
A:jax.lax.lax.x_shape->numpy.shape(x)
A:jax.lax.lax.(broadcast_dimensions,)->numpy.where(onp.equal(x_shape, shape))
A:jax.lax.lax.(squeezed_dimensions,)->numpy.where(onp.not_equal(x_shape, shape))
A:jax.lax.lax.inshape->numpy.delete(x_shape, squeezed_dimensions)
A:jax.lax.lax.neg_p->standard_unop(_num, 'neg')
A:jax.lax.lax.sign_p->standard_unop(_num, 'sign')
A:jax.lax.lax.floor_p->standard_unop(_float, 'floor')
A:jax.lax.lax.ceil_p->standard_unop(_float, 'ceil')
A:jax.lax.lax.round_p->standard_unop(_float, 'round')
A:jax.lax.lax.is_finite_p->unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
A:jax.lax.lax.exp_p->standard_unop(_float | _complex, 'exp')
A:jax.lax.lax.log_p->standard_unop(_float | _complex, 'log')
A:jax.lax.lax.expm1_p->standard_unop(_float | _complex, 'expm1')
A:jax.lax.lax.log1p_p->standard_unop(_float | _complex, 'log1p')
A:jax.lax.lax.tanh_p->standard_unop(_float | _complex, 'tanh')
A:jax.lax.lax.sin_p->standard_unop(_float | _complex, 'sin')
A:jax.lax.lax.cos_p->standard_unop(_float | _complex, 'cos')
A:jax.lax.lax.atan2_p->standard_binop([_float, _float], 'atan2')
A:jax.lax.lax.lgamma_p->standard_unop(_float, 'lgamma')
A:jax.lax.lax.digamma_p->standard_unop(_float, 'digamma')
A:jax.lax.lax.erf_p->standard_unop(_float, 'erf')
A:jax.lax.lax.erfc_p->standard_unop(_float, 'erfc')
A:jax.lax.lax.erf_inv_p->standard_unop(_float, 'erf_inv')
A:jax.lax.lax.real_p->unop(_complex_basetype, _complex, 'real')
A:jax.lax.lax.imag_p->unop(_complex_basetype, _complex, 'imag')
A:jax.lax.lax.complex_p->binop(_complex_dtype, [_complex_elem_types, _complex_elem_types], 'complex')
A:jax.lax.lax.conj_p->unop(_complex_dtype, _float | _complex, 'conj')
A:jax.lax.lax.ad.primitive_jvps[conj_p]->partial(ad.linear_jvp, conj_p)
A:jax.lax.lax.abs_p->unop(_complex_basetype, _num, 'abs')
A:jax.lax.lax.sqrt_p->standard_unop(_float | _complex, 'sqrt')
A:jax.lax.lax.rsqrt_p->standard_unop(_float | _complex, 'rsqrt')
A:jax.lax.lax.pow_p->standard_binop([_float | _complex, _float | _complex], 'pow')
A:jax.lax.lax.jac->mul(y, pow(x, select(eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))
A:jax.lax.lax.not_p->standard_unop(_int | _bool, 'not')
A:jax.lax.lax.and_p->standard_binop([_any, _any], 'and')
A:jax.lax.lax.or_p->standard_binop([_any, _any], 'or')
A:jax.lax.lax.xor_p->standard_binop([_any, _any], 'xor')
A:jax.lax.lax.add_p->standard_binop([_num, _num], 'add')
A:jax.lax.lax.sub_p->standard_binop([_num, _num], 'sub')
A:jax.lax.lax.mul_p->standard_binop([_num, _num], 'mul')
A:jax.lax.lax.zero->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Constant(onp.array(0, dtype))
A:jax.lax.lax.out_shape->numpy.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
A:jax.lax.lax.safe_mul_p->standard_binop([_num, _num], 'safe_mul', translation_rule=_safe_mul_translation_rule)
A:jax.lax.lax.div_p->standard_binop([_num, _num], 'div')
A:jax.lax.lax.rem_p->standard_binop([_num, _num], 'rem')
A:jax.lax.lax.which->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').BroadcastInDim(which, out_shape, bcast_dims(which_shape))
A:jax.lax.lax.y->tie_in(*batched_args)
A:jax.lax.lax.comparator->cmp(c)
A:jax.lax.lax.rx->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Real(x)
A:jax.lax.lax.ry->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Real(y)
A:jax.lax.lax.max_p->standard_binop([_any, _any], 'max', translation_rule=partial(_minmax_translation_rule, minmax=lambda c: c.Max, cmp=lambda c: c.Gt))
A:jax.lax.lax.min_p->standard_binop([_any, _any], 'min', translation_rule=partial(_minmax_translation_rule, minmax=lambda c: c.Min, cmp=lambda c: c.Lt))
A:jax.lax.lax.shift_left_p->standard_binop([_int, _int], 'shift_left')
A:jax.lax.lax.shift_right_arithmetic_p->standard_binop([_int, _int], 'shift_right_arithmetic')
A:jax.lax.lax.shift_right_logical_p->standard_binop([_int, _int], 'shift_right_logical')
A:jax.lax.lax.eq_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
A:jax.lax.lax.ne_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
A:jax.lax.lax.ge_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
A:jax.lax.lax.gt_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
A:jax.lax.lax.le_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
A:jax.lax.lax.lt_p->binop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
A:jax.lax.lax.new_etype->lib.xla_bridge.dtype_to_etype(new_dtype)
A:jax.lax.lax.convert_element_type_p->standard_primitive(_convert_element_type_shape_rule, _convert_element_type_dtype_rule, 'convert_element_type', _convert_element_type_translation_rule)
A:jax.lax.lax.bitcast_convert_type_p->standard_primitive(_bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule, 'bitcast_convert_type', _bitcast_convert_type_translation_rule)
A:jax.lax.lax.(quot, rem)->divmod(lhs_feature_count, feature_group_count)
A:jax.lax.lax.lhs_trans->numpy.take(lhs_shape, lhs_perm)
A:jax.lax.lax.rhs_trans->numpy.take(rhs_shape, rhs_perm)
A:jax.lax.lax.out_trans->tuple((lhs_trans[0], rhs_trans[0]) + tuple(out_space))
A:jax.lax.lax.(lhs_sdims, rhs_sdims, out_sdims)->map(_conv_sdims, dimension_numbers)
A:jax.lax.lax.t_rhs_spec->_conv_spec_transpose(rhs_spec)
A:jax.lax.lax.trans_dimension_numbers->ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
A:jax.lax.lax.revd_weights->rev(rhs, rhs_sdims)
A:jax.lax.lax.(lhs_trans, rhs_trans, out_trans)->map(_conv_spec_transpose, dimension_numbers)
A:jax.lax.lax.new_lhs->_reshape_axis_into(lhs_bdim, lhs_spec[0], lhs)
A:jax.lax.lax.new_rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[0], new_rhs)
A:jax.lax.lax.conv_general_dilated_p->standard_primitive(_conv_general_dilated_shape_rule, _conv_general_dilated_dtype_rule, 'conv_general_dilated', _conv_general_dilated_translation_rule)
A:jax.lax.lax.new_shape->list(onp.delete(x.shape, src))
A:jax.lax.lax.(size2, ragged)->divmod(shape[src], size1)
A:jax.lax.lax.config->lib.xla_client.PrecisionConfig()
A:jax.lax.lax.masked_lhs->select(broadcasted_iota(onp.int32, lhs.shape, 1) < lhs_shape[1], lhs, zeros_like_array(lhs))
A:jax.lax.lax._dot_dtype_rule->partial(binop_dtype_rule, _input_dtype, [_num, _num], 'dot')
A:jax.lax.lax.dot_p->standard_primitive(_dot_shape_rule, _dot_dtype_rule, 'dot', _dot_translation_rule)
A:jax.lax.lax.lhs_batch_shape->numpy.take(lhs.shape, lhs_batch)
A:jax.lax.lax.rhs_batch_shape->numpy.take(rhs.shape, rhs_batch)
A:jax.lax.lax.lhs_contracting_shape->numpy.take(lhs.shape, lhs_contracting)
A:jax.lax.lax.rhs_contracting_shape->numpy.take(rhs.shape, rhs_contracting)
A:jax.lax.lax.batch_shape->tuple(onp.take(lhs.shape, lhs_batch))
A:jax.lax.lax.lhs_tensored_shape->tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
A:jax.lax.lax.rhs_tensored_shape->tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
A:jax.lax.lax.x_kept->remaining(range(x_ndim), x_contract, x_batch)
A:jax.lax.lax.y_kept->remaining(range(y.ndim), y_contract, y_batch)
A:jax.lax.lax.(ans_batch, ans_y, _)->ranges_like(x_batch, y_kept, x_kept)
A:jax.lax.lax.(ans_batch, _, ans_y)->ranges_like(x_batch, x_kept, y_kept)
A:jax.lax.lax.x_contract_sorted_by_y->list(onp.take(x_contract, onp.argsort(y_contract)))
A:jax.lax.lax.out_axes->numpy.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
A:jax.lax.lax.lhs_contract->tuple(onp.add(1, lhs_contract))
A:jax.lax.lax.rhs_contract->tuple(onp.add(1, rhs_contract))
A:jax.lax.lax.batched_out->dot_general(lhs, rhs, new_dimension_numbers, precision=precision)
A:jax.lax.lax.dot_general_p->standard_primitive(_dot_general_shape_rule, _dot_general_dtype_rule, 'dot_general', _dot_general_translation_rule)
A:jax.lax.lax.broadcast_p->standard_primitive(_broadcast_shape_rule, _input_dtype, 'broadcast')
A:jax.lax.lax.new_operand->pad(new_operand, _zero(operand), ((0, 1, 0),) + tuple(((0, 0, 0) for _ in operand_shape)))
A:jax.lax.lax.broadcast_in_dim_p->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
A:jax.lax.lax._clamp_dtype_rule->partial(binop_dtype_rule, _input_dtype, [_any, _any, _any], 'clamp')
A:jax.lax.lax.clamp_p->standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')
A:jax.lax.lax.op->next((op for op in operands if not isinstance(op, UnshapedArray)))
A:jax.lax.lax.concat_size->sum((o.shape[dimension] for o in operands))
A:jax.lax.lax.operand_shapes->kwargs.pop('operand_shapes')
A:jax.lax.lax.limit_points->numpy.cumsum([shape[dimension] for shape in operand_shapes])
A:jax.lax.lax.starts->numpy.zeros((len(operands), t.ndim), dtype=int)
A:jax.lax.lax.limits->numpy.tile(t.shape, (len(operands), 1))
A:jax.lax.lax.out_shape[dimension]->_reduce(operator.add, [e[dimension] for e in shape_exprs])
A:jax.lax.lax.concatenate_p->standard_primitive(_concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate', _concatenate_translation_rule)
A:jax.lax.lax.(lo, hi, interior)->zip(*padding_config)
A:jax.lax.lax.unpad_config->zip(onp.negative(lo), onp.negative(hi), onp.zeros_like(interior))
A:jax.lax.lax.unpadded->pad(t, onp.array(0.0, t.dtype), unpad_config)
A:jax.lax.lax.padding_config->list(padding_config)
A:jax.lax.lax.pad_p->standard_primitive(_pad_shape_rule, _input_dtype, 'pad')
A:jax.lax.lax.reshape_p->standard_primitive(_reshape_shape_rule, _reshape_dtype_rule, 'reshape', _reshape_translation_rule)
A:jax.lax.lax.rev_p->standard_primitive(_rev_shape_rule, _input_dtype, 'rev')
A:jax.lax.lax.transpose_p->standard_primitive(_transpose_shape_rule, _input_dtype, 'transpose')
A:jax.lax.lax.zeros->full(operand_shape, tie_in(t, _zero(t)))
A:jax.lax.lax.pred->broadcast_in_dim(pred, on_true.shape, [0])
A:jax.lax.lax.on_false->broadcast(on_false, pred.shape)
A:jax.lax.lax.on_true->broadcast(on_true, pred.shape)
A:jax.lax.lax.select_p->standard_primitive(_select_shape_rule, _select_dtype_rule, 'select')
A:jax.lax.lax.strides->numpy.ones(operand.ndim, onp.int32)
A:jax.lax.lax.real_limits->numpy.add(onp.add(start_indices, 1), onp.multiply(onp.subtract(t.shape, 1), strides))
A:jax.lax.lax.new_start_indices->list(start_indices)
A:jax.lax.lax.new_limit_indices->list(limit_indices)
A:jax.lax.lax.new_strides->list(strides)
A:jax.lax.lax.slice_p->standard_primitive(_slice_shape_rule, _input_dtype, 'slice', _slice_translation_rule)
A:jax.lax.lax.tangent_out->_select_and_gather_add(g_source, operand, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.dims->tuple(range(len(update_shape)))
A:jax.lax.lax.(index, index_bdim)->_batch_dynamic_slice_indices(batched_args[2:], batch_dims[2:])
A:jax.lax.lax.dynamic_slice_p->standard_primitive(_dynamic_slice_shape_rule, _input_dtype, 'dynamic_slice', _dynamic_slice_translation_rule)
A:jax.lax.lax.val_out->sort_key_val(keys, values, dimension)
A:jax.lax.lax.g_operand->interpreters.ad.instantiate_zeros(operand, g_operand)
A:jax.lax.lax.g_update->interpreters.ad.instantiate_zeros(update, g_update)
A:jax.lax.lax.dynamic_update_slice_p->standard_primitive(_dynamic_update_slice_shape_rule, _dynamic_update_slice_dtype_rule, 'dynamic_update_slice', _dynamic_update_slice_translation_rule)
A:jax.lax.lax.proto->lib.xla_client.ConvolutionDimensionNumbers()
A:jax.lax.lax.msg->'slice_sizes must have rank equal to the gather operand; operand.shape={}, slice_sizes={}'.format(operand_shape, slice_sizes)
A:jax.lax.lax.start_indices_shape->iter(start_indices.shape[:-1])
A:jax.lax.lax.indices_shape->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').GetShape(scatter_indices)
A:jax.lax.lax.scatter_dnums->ScatterDimensionNumbers(update_window_dims=dimension_numbers.offset_dims, inserted_window_dims=dimension_numbers.collapsed_slice_dims, scatter_dims_to_operand_dims=dimension_numbers.start_index_map)
A:jax.lax.lax.collapsed_slice_dims->tuple(onp.add(1, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.start_index_map->tuple(onp.add(1, dimension_numbers.start_index_map))
A:jax.lax.lax.count_shape->list(scatter_indices.shape)
A:jax.lax.lax.counts->_reduce_sum(location_indicators, axes)
A:jax.lax.lax.gather_p->standard_primitive(_gather_shape_rule, _gather_dtype_rule, 'gather', _gather_translation_rule)
A:jax.lax.lax.update_computation->_reduction_computation(c, update_jaxpr, backend, update_consts, init_value)
A:jax.lax.lax.g_updates->interpreters.ad.instantiate_zeros(updates, g_updates)
A:jax.lax.lax.gather_dnums->GatherDimensionNumbers(offset_dims=dnums.update_window_dims, collapsed_slice_dims=dnums.inserted_window_dims, start_index_map=dnums.scatter_dims_to_operand_dims)
A:jax.lax.lax.update_t->gather(t, scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.updates->reshape(updates, (1,) + updates_shape)
A:jax.lax.lax.inserted_window_dims->tuple(onp.add(1, dimension_numbers.inserted_window_dims))
A:jax.lax.lax.scatter_dims_to_operand_dims->tuple(onp.add(1, dimension_numbers.scatter_dims_to_operand_dims))
A:jax.lax.lax.scatter_indices->concatenate([counts, scatter_indices], len(count_shape) - 1)
A:jax.lax.lax.update_window_dims->tuple(onp.add(1, dimension_numbers.update_window_dims))
A:jax.lax.lax.scatter_add_p->standard_reduction_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-add', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_add_p]->partial(_scatter_batching_rule, scatter_add)
A:jax.lax.lax.scatter_min_p->standard_reduction_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-min', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_min_p]->partial(_scatter_batching_rule, scatter_min)
A:jax.lax.lax.scatter_max_p->standard_reduction_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-max', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_max_p]->partial(_scatter_batching_rule, scatter_max)
A:jax.lax.lax.updates_dtype->_dtype(updates)
A:jax.lax.lax.ids_shape->numpy.array(updates_shape)
A:jax.lax.lax.num_ids->numpy.prod(ids_shape)
A:jax.lax.lax.update_ids->add(reshape(iota(updates_dtype, num_ids), ids_shape), _ones(updates))
A:jax.lax.lax.reshaped_update_ids->reshape(update_ids, (1,) + updates_shape)
A:jax.lax.lax.updates_and_ids->concatenate((updates, reshaped_update_ids), 0)
A:jax.lax.lax.new_dnums->ScatterDimensionNumbers(update_window_dims=(0,) + tuple((d + 1 for d in dnums.update_window_dims)), inserted_window_dims=tuple((d + 1 for d in dnums.inserted_window_dims)), scatter_dims_to_operand_dims=tuple((d + 1 for d in dnums.scatter_dims_to_operand_dims)))
A:jax.lax.lax.outputs->concatenate(outputs, 0)
A:jax.lax.lax.scattered_ids->index_in_dim(outputs, 1, keepdims=False)
A:jax.lax.lax.gathered_update_ids->gather(scattered_ids, scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.masked_g_operand->select(eq(scattered_ids, _zeros(scattered_ids)), g_operand, _zeros(g_operand))
A:jax.lax.lax.masked_g_updates->select(eq(update_ids, gathered_update_ids), g_updates, _zeros(g_updates))
A:jax.lax.lax.scatter_p->standard_reduction_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_p]->partial(_scatter_batching_rule, scatter)
A:jax.lax.lax.xla_computation->_reduction_computation(c, jaxpr, backend, consts, init_value)
A:jax.lax.lax.axis_env->interpreters.xla.AxisEnv()
A:jax.lax.lax.subc->lib.xla_bridge.make_computation_builder('reduction_computation')
A:jax.lax.lax.(out,)->interpreters.xla.jaxpr_subcomp(subc, jaxpr, backend, axis_env, consts, (), *args)
A:jax.lax.lax.masking.masking_rules[prim]->partial(_reducer_masking_rule, prim, identity)
A:jax.lax.lax.padded_shape->interpreters.masking.padded_shape_as_value(padded_val.shape)
A:jax.lax.lax.mask->_reduce(operator.and_, masks)
A:jax.lax.lax.masked_val->select(mask, padded_val, identity(padded_shape, padded_val.dtype))
A:jax.lax.lax.reduce_p->standard_reduction_primitive(_reduce_shape_rule, _input_dtype, 'reduce', _reduce_translation_rule)
A:jax.lax.lax.scalar->lib.xla_client.Shape.array_shape(dtype, ())
A:jax.lax.lax.broadcast_dimensions->numpy.delete(onp.arange(keys.ndim), keys_bdim)
A:jax.lax.lax.reduce_sum_p->standard_primitive(_reduce_sum_shape_rule, _input_dtype, 'reduce_sum', _reduce_sum_translation_rule)
A:jax.lax.lax.input_shape->numpy.array(operand.shape)
A:jax.lax.lax.n->numpy.prod(input_shape[list(axes)])
A:jax.lax.lax.non_axes->numpy.delete(onp.arange(len(input_shape)), axes)
A:jax.lax.lax.tangent->reshape(tangent, new_shape, permutation)
A:jax.lax.lax.one->_const(operand, 1)
A:jax.lax.lax.left_products->_reduce_window_prod(pad(operand, one, left_padding), window_dims, window_strides, xla_client.PaddingType.VALID)
A:jax.lax.lax.right_products->_reduce_window_prod(pad(operand, one, right_padding), window_dims, window_strides, xla_client.PaddingType.VALID)
A:jax.lax.lax.reduce_prod_p->standard_primitive(_reduce_prod_shape_rule, _input_dtype, 'reduce_prod', _reduce_prod_translation_rule)
A:jax.lax.lax.location_indicators->convert_element_type(_eq_meet(operand, reshape(ans, shape)), g.dtype)
A:jax.lax.lax._reduce_max_translation_rule->partial(_reduce_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_max_p->standard_primitive(_reduce_chooser_shape_rule, _input_dtype, 'reduce_max', _reduce_max_translation_rule)
A:jax.lax.lax._reduce_min_translation_rule->partial(_reduce_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_min_p->standard_primitive(_reduce_chooser_shape_rule, _input_dtype, 'reduce_min', _reduce_min_translation_rule)
A:jax.lax.lax._reduce_or_translation_rule->partial(_reduce_logical_translation_rule, or_p, _get_max_identity)
A:jax.lax.lax.reduce_or_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(onp.bool_), 'reduce_or', _reduce_or_translation_rule)
A:jax.lax.lax._reduce_and_translation_rule->partial(_reduce_logical_translation_rule, and_p, _get_min_identity)
A:jax.lax.lax.reduce_and_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(onp.bool_), 'reduce_and', _reduce_and_translation_rule)
A:jax.lax.lax.reduce_window_p->standard_reduction_primitive(_reduce_window_shape_rule, _input_dtype, 'reduce_window', _reduce_window_translation_rule)
A:jax.lax.lax.in_pads->padtype_to_pads(input_shape, window_dimensions, window_strides, padding)
A:jax.lax.lax.pad_cotangent->pad(cotangent, _zero(cotangent), padding_config)
A:jax.lax.lax.reduce_window_sum_p->standard_primitive(_reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum', _reduce_window_sum_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_sum_p]->partial(_reduce_window_batch_rule, _reduce_window_sum)
A:jax.lax.lax.operand_padded->numpy.add(operand_shape, onp.add(*zip(*pads)))
A:jax.lax.lax._reduce_window_max_translation_rule->partial(_reduce_window_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_window_max_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max', _reduce_window_max_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_max_p]->partial(_reduce_window_batch_rule, _reduce_window_max)
A:jax.lax.lax._reduce_window_min_translation_rule->partial(_reduce_window_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_window_min_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min', _reduce_window_min_translation_rule)
A:jax.lax.lax._reduce_window_min_batch_rule->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_min_p]->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.select->interpreters.xla.primitive_computation(select_prim, scalar, scalar)
A:jax.lax.lax.scatter->interpreters.xla.primitive_computation(add_p, scalar, scalar)
A:jax.lax.lax.select_and_scatter_p->standard_reduction_primitive(_select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter', _select_and_scatter_translation)
A:jax.lax.lax.source_t->_select_and_gather_add(t, operand, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.source->interpreters.batching.moveaxis(source, s_bdims, 0)
A:jax.lax.lax.select_and_scatter_add_p->standard_primitive(_select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add', _select_and_scatter_add_translation)
A:jax.lax.lax.etype->lib.xla_bridge.dtype_to_etype(diag_const.dtype)
A:jax.lax.lax.word_type->lib.xla_client.dtype_to_etype(word_dtype)
A:jax.lax.lax.double_word_type->lib.xla_client.dtype_to_etype(double_word_dtype)
A:jax.lax.lax.a->convert_element_type(a, b_dtype)
A:jax.lax.lax.b->convert_element_type(b, a_dtype)
A:jax.lax.lax.st->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').And(t, const(c, word_dtype, (1 << r_nbits) - 1 << r_nbits))
A:jax.lax.lax.c->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer')
A:jax.lax.lax.select_and_gather_add_p->standard_primitive(_select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add', _select_and_gather_add_translation)
A:jax.lax.lax.xla.backend_specific_translations['tpu'][select_and_gather_add_p]->partial(_select_and_gather_add_translation, max_bits=32)
A:jax.lax.lax.(_, g_out)->sort_key_val(operand, g, dimension)
A:jax.lax.lax.sort_p->standard_primitive(sort_shape, _input_dtype, 'sort')
A:jax.lax.lax.keys_tangents_out->_sort_jvp_rule(keys_tangents, keys, dimension)
A:jax.lax.lax.values_tangents_out->_sort_jvp_rule(values_tangents, keys, dimension)
A:jax.lax.lax.iota->tuple(range(len(lhs_shape)))
A:jax.lax.lax.(_, perm)->sort_key_val(keys, iota)
A:jax.lax.lax.keys_trans->interpreters.batching.moveaxis(keys, keys_bdim, values_bdim)
A:jax.lax.lax.new_keys->broadcast_in_dim(keys, values.shape, broadcast_dimensions)
A:jax.lax.lax.new_values->broadcast_in_dim(values, keys.shape, broadcast_dimensions)
A:jax.lax.lax.sort_key_val_p->Primitive('sort_key_val')
A:jax.lax.lax.xla.translations[sort_key_val_p]->partial(standard_translate, 'sort_key_val')
A:jax.lax.lax.tie_in_p->Primitive('tie_in')
A:jax.lax.lax.shaped_identity_p->Primitive('shape_id')
A:jax.lax.lax.self.aval->ShapedArray(shape, onp.dtype(dtype))
A:jax.lax.lax.self._npy_value->numpy.broadcast_to(result, self.shape)
A:jax.lax.lax.stop_gradient_p->Primitive('stop_gradient')
A:jax.lax.lax.dtypes->list(map(onp.dtype, dtypes))
A:jax.lax.lax.lhs_padded->numpy.add(lhs_shape[2:], onp.add(*zip(*pads)))
A:jax.lax.lax.out_space->numpy.sum([unpad_out_space, padding], axis=0).tolist()
A:jax.lax.lax.(lhs_perm, rhs_perm, out_perm)->map(getperm, dimension_numbers, charpairs)
A:jax.lax.lax.obj_arr->numpy.array(obj)
A:jax.lax.lax._zeros->partial(full_like, fill_value=0)
A:jax.lax.lax._zero->partial(full_like, shape=(), fill_value=0)
A:jax.lax.lax._ones->partial(full_like, fill_value=1)
A:jax.lax.lax._one->partial(full_like, shape=(), fill_value=1)
A:jax.lax.lax._twos->partial(full_like, fill_value=2)
A:jax.lax.lax._two->partial(full_like, shape=(), fill_value=2)
A:jax.lax.lax.x_len->len(x)
A:jax.lax.lax.blacklist->set(itertools.chain(*removed_lists))
A:jax.lax.lax.(lhs_spec, rhs_spec, out_spec)->conv_general_permutations(dimension_numbers)
A:jax.lax.lax.spatial->sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
A:jax.lax.lax.lhs_dilated_shape->_dilate_shape(in_shape, lhs_dilation)
A:jax.lax.lax.rhs_dilated_shape->_dilate_shape(window_dimensions, rhs_dilation)
A:jax.lax.lax.out_dilated_shape->_dilate_shape(out_shape, window_strides)
A:jax.lax.lax.higher_dtype->numpy.promote_types(a_dtype, b_dtype)
A:jax.lax.lax.lst->list(lst)
jax.ConvDimensionNumbers(collections.namedtuple('ConvDimensionNumbers',['lhs_spec','rhs_spec','out_spec']))
jax.GatherDimensionNumbers(collections.namedtuple('GatherDimensionNumbers',['offset_dims','collapsed_slice_dims','start_index_map']))
jax.ScatterDimensionNumbers(collections.namedtuple('ScatterDimensionNumbers',['update_window_dims','inserted_window_dims','scatter_dims_to_operand_dims']))
jax._EyeConstant(self,shape,axes,dtype)
jax._EyeConstant._value(self)
jax._EyeConstant.constant_handler(c,diag_const,canonicalize_types=True)
jax._FilledConstant(self,fill_value,shape)
jax._FilledConstant._value(self)
jax._FilledConstant.constant_handler(c,filled_const,canonicalize_types=True)
jax._IotaConstant(self,dtype,shape,axis)
jax._IotaConstant._value(self)
jax._IotaConstant.constant_handler(c,iota_constant,canonicalize_types=True)
jax._abstractify(x)
jax._add_transpose(t,x,y)
jax._balanced_eq(x,z,y)
jax._batch_dynamic_slice_indices(indices,bdims)
jax._bitcast_convert_type_dtype_rule(operand,new_dtype)
jax._bitcast_convert_type_shape_rule(operand,new_dtype)
jax._bitcast_convert_type_translation_rule(c,operand,new_dtype)
jax._brcast(x,*others)
jax._brcast_to(x,shape)
jax._broadcast_batch_rule(batched_args,batch_dims,sizes)
jax._broadcast_in_dim_batch_rule(batched_args,batch_dims,shape,broadcast_dimensions)
jax._broadcast_in_dim_shape_rule(operand,shape,broadcast_dimensions)
jax._broadcast_in_dim_transpose_rule(t,shape,broadcast_dimensions)
jax._broadcast_shape_rule(operand,sizes)
jax._broadcasting_shape_rule(name,*avals)
jax._canonicalize_precision(precision)
jax._canonicalize_shape(shape)
jax._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax._check_same_dtypes(name,ignore_fp_precision,*dtypes)
jax._check_shapelike(fun_name,arg_name,obj)
jax._clamp_shape_rule(min,operand,max)
jax._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding)
jax._concat_polymorphic_shape_rule(shape_exprs,dimension,operand_shapes)
jax._concatenate_batch_rule(batched_args,batch_dims,dimension,operand_shapes)
jax._concatenate_dtype_rule(*operands,**kwargs)
jax._concatenate_shape_rule(*operands,**kwargs)
jax._concatenate_translation_rule(c,*operands,**kwargs)
jax._concatenate_transpose_rule(t,*operands,**kwargs)
jax._conj_transpose_rule(t,x,input_dtype)
jax._conv_general_dilated_batch_rule(batched_args,batch_dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax._conv_general_dilated_dtype_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,**unused_kwargs)
jax._conv_general_dilated_shape_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,**unused_kwargs)
jax._conv_general_dilated_translation_rule(c,lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax._conv_general_dilated_transpose_lhs(g,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax._conv_general_dilated_transpose_rhs(g,lhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax._conv_general_proto(dimension_numbers)
jax._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._conv_transpose_padding(k,s,padding)
jax._convert_element_type_dtype_rule(operand,new_dtype,old_dtype)
jax._convert_element_type_shape_rule(operand,new_dtype,old_dtype)
jax._convert_element_type_translation_rule(c,operand,new_dtype,old_dtype)
jax._dilate_shape(shape,dilation)
jax._div_transpose_rule(cotangent,x,y)
jax._dot_batch_rule(batched_args,batch_dims,precision=None)
jax._dot_general_batch_rule(batched_args,batch_dims,dimension_numbers,precision)
jax._dot_general_dtype_rule(lhs,rhs,dimension_numbers,precision)
jax._dot_general_shape_rule(lhs,rhs,dimension_numbers,precision)
jax._dot_general_translation_rule(c,lhs,rhs,dimension_numbers,precision)
jax._dot_general_transpose_lhs(g,y,dimension_numbers,precision,swap_ans=False)
jax._dot_general_transpose_rhs(g,x,dimension_numbers,precision)
jax._dot_masking_rule(padded_vals,logical_shapes,precision)
jax._dot_polymorphic_shape_rule(shape_exprs,precision)
jax._dot_shape_rule(lhs,rhs,precision)
jax._dot_translation_rule(c,lhs,rhs,precision)
jax._dot_transpose_lhs(t,rhs,precision)
jax._dot_transpose_rhs(t,lhs,precision)
jax._dynamic_slice_batching_rule(batched_args,batch_dims,slice_sizes,operand_shape)
jax._dynamic_slice_indices(operand,start_indices)
jax._dynamic_slice_jvp(primals,tangents,slice_sizes,operand_shape)
jax._dynamic_slice_shape_rule(operand,*start_indices,**kwargs)
jax._dynamic_slice_translation_rule(c,operand,*start_indices,**kwargs)
jax._dynamic_slice_transpose_rule(t,operand,*start_indices,**kwargs)
jax._dynamic_update_slice_batching_rule(batched_args,batch_dims,update_shape)
jax._dynamic_update_slice_dtype_rule(operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_jvp(primals,tangents,update_shape)
jax._dynamic_update_slice_shape_rule(operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_translation_rule(c,operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices,**kwargs)
jax._flip_axes(x,axes)
jax._gather_batching_rule(batched_args,batch_dims,dimension_numbers,slice_sizes,operand_shape)
jax._gather_dimensions_proto(indices_shape,dimension_numbers)
jax._gather_dtype_rule(operand,start_indices,**kwargs)
jax._gather_jvp_rule(g,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax._gather_shape_rule(operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax._gather_translation_rule(c,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax._gather_transpose_rule(t,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax._generic_reduce_window_batch_rule(batched_args,batch_dims,jaxpr,consts,window_dimensions,window_strides,padding)
jax._get_max_identity(dtype)
jax._get_min_identity(dtype)
jax._get_monoid_reducer(monoid_op,x)
jax._get_monoid_window_reducer(monoid_op,x)
jax._identity(x)
jax._is_axis_merge(s1,s2)
jax._is_axis_split(s1,s2)
jax._iter(tracer)
jax._masking_defreducer(prim,identity)
jax._minmax_translation_rule(c,x,y,minmax=None,cmp=None)
jax._outer(x,y)
jax._pad_batch_rule(batched_args,batch_dims,padding_config)
jax._pad_shape_rule(operand,padding_value,padding_config)
jax._pad_transpose(t,operand,padding_value,padding_config)
jax._pow_jvp_lhs(g,x,y)
jax._pow_jvp_rhs(g,x,y)
jax._precision_config(precision)
jax._reduce_batch_rule(batched_args,batch_dims,computation,jaxpr,consts,dimensions)
jax._reduce_chooser_jvp_rule(g,ans,operand,axes)
jax._reduce_chooser_shape_rule(operand,axes)
jax._reduce_chooser_translation_rule(prim,identity,c,operand,axes)
jax._reduce_logical_shape_rule(operand,axes)
jax._reduce_logical_translation_rule(prim,identity,c,operand,axes)
jax._reduce_prod(operand,axes)
jax._reduce_prod_jvp_rule(tangent,operand,axes)
jax._reduce_prod_shape_rule(operand,axes)
jax._reduce_prod_translation_rule(c,operand,axes)
jax._reduce_shape_rule(operand,init_value,computation,jaxpr,consts,dimensions)
jax._reduce_translation_rule(c,operand,init_value,computation,jaxpr,consts,dimensions,backend=None)
jax._reduce_window_batch_rule(reduce_window,batched_args,bdims,window_dimensions,window_strides,padding,input_shape=None)
jax._reduce_window_chooser_jvp_rule(prim,g,operand,window_dimensions,window_strides,padding)
jax._reduce_window_chooser_translation_rule(prim,identity,c,operand,window_dimensions,window_strides,padding)
jax._reduce_window_shape_rule(operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax._reduce_window_translation_rule(c,operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding,backend=None)
jax._reducer_masking_rule(prim,identity,padded_vals,logical_shapes,axes,input_shape)
jax._reducer_polymorphic_shape_rule(shape_exprs,axes,**unused_params)
jax._reduction_computation(c,jaxpr,backend,consts,init_value)
jax._reduction_jaxpr(computation,aval)
jax._reshape_axis_into(src,dst,x)
jax._reshape_axis_out_of(src,size1,x)
jax._reshape_batch_rule(batched_args,batch_dims,new_sizes,dimensions,**unused)
jax._reshape_dtype_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax._reshape_impl(operand,new_sizes,dimensions,old_sizes)
jax._reshape_polymorphic_shape_rule(shape_exprs,new_sizes,dimensions,old_sizes)
jax._reshape_shape_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax._reshape_translation_rule(c,operand,new_sizes,dimensions,old_sizes)
jax._reshape_transpose_rule(t,new_sizes,dimensions,old_sizes)
jax._rev_batch_rule(batched_args,batch_dims,dimensions)
jax._rev_shape_rule(operand,dimensions)
jax._scatter_add_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax._scatter_add_transpose_rule(t,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax._scatter_batching_rule(scatter_op,batched_args,batch_dims,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax._scatter_dtype_rule(operand,scatter_indices,updates,**kwargs)
jax._scatter_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax._scatter_shape_rule(operand,scatter_indices,updates,**kwargs)
jax._scatter_translation_rule(c,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers,updates_shape,backend=None)
jax._select_and_gather_add(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_shape_rule(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_translation(c,tangents,operand,select_prim,window_dimensions,window_strides,padding,max_bits=64)
jax._select_and_gather_add_transpose(t,tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter(operand,select,window_dimensions,window_strides,padding,source,init_value,scatter)
jax._select_and_scatter_add(source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_batch_rule(batched_args,batch_dims,**kwargs)
jax._select_and_scatter_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_shape_rule(source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_translation(c,source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_transpose(t,source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_shape_rule(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._select_and_scatter_translation(c,operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding,backend=None)
jax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax._select_dtype_rule(pred,on_true,on_false)
jax._select_shape_rule(pred,on_true,on_false)
jax._select_transpose_rule(t,pred,on_true,on_false)
jax._slice_batching_rule(batched_args,batch_dims,start_indices,limit_indices,strides,**unused_kwargs)
jax._slice_shape_rule(operand,start_indices,limit_indices,strides,operand_shape)
jax._slice_translation_rule(c,operand,start_indices,limit_indices,strides,operand_shape)
jax._slice_transpose_rule(t,start_indices,limit_indices,strides,operand_shape)
jax._sort_batch_rule(batched_args,batch_dims,dimension)
jax._sort_jvp_rule(g,operand,dimension)
jax._sort_key_val_abstract_eval(keys,values,dimension)
jax._sort_key_val_batch_rule(batched_args,batch_dims,dimension)
jax._sort_key_val_jvp(primals,tangents,dimension)
jax._sort_key_val_transpose_rule(t,keys,values,dimension)
jax._stop_gradient_batch_rule(batched_args,batch_dims)
jax._stop_gradient_jvp_rule(primals,tangents)
jax._sub_transpose(t,x,y)
jax._tie_in_batch_rule(batched_args,batch_dims)
jax._tie_in_transpose_rule(t)
jax._transpose_batch_rule(batched_args,batch_dims,permutation)
jax._transpose_shape_rule(operand,permutation)
jax.abs(x)
jax.acos(x)
jax.add(x,y)
jax.asin(x)
jax.atan(x)
jax.atan2(x,y)
jax.batch_matmul(lhs,rhs)
jax.binop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.binop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.bitcast_convert_type(operand,new_dtype)
jax.bitwise_and(x,y)
jax.bitwise_not(x)
jax.bitwise_or(x,y)
jax.bitwise_xor(x,y)
jax.broadcast(operand,sizes)
jax.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.broadcast_shapes(*shapes)
jax.broadcasted_eye(dtype,shape,axes)
jax.broadcasted_iota(dtype,shape,dimension)
jax.ceil(x)
jax.clamp(min,x,max)
jax.collapse(operand,start_dimension,stop_dimension)
jax.complex(x,y)
jax.concatenate(operands,dimension)
jax.conj(x)
jax.conv(lhs,rhs,window_strides,padding,precision=None)
jax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,precision=None)
jax.conv_general_permutations(dimension_numbers)
jax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads)
jax.conv_transpose(lhs,rhs,strides,padding,dimension_numbers=None,transpose_kernel=False,precision=None)
jax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,precision=None)
jax.convert_element_type(operand,new_dtype)
jax.cos(x)
jax.cosh(x)
jax.digamma(x)
jax.div(x,y)
jax.dot(lhs,rhs,precision=None)
jax.dot_general(lhs,rhs,dimension_numbers,precision=None)
jax.dynamic_index_in_dim(operand,index,axis=0,keepdims=True)
jax.dynamic_slice(operand,start_indices,slice_sizes)
jax.dynamic_slice_in_dim(operand,start_index,slice_size,axis=0)
jax.dynamic_update_index_in_dim(operand,update,index,axis)
jax.dynamic_update_slice(operand,update,start_indices)
jax.dynamic_update_slice_in_dim(operand,update,start_index,axis)
jax.eq(x,y)
jax.erf(x)
jax.erf_inv(x)
jax.erfc(x)
jax.exp(x)
jax.expm1(x)
jax.eye(dtype,size)
jax.floor(x)
jax.full(shape,fill_value,dtype=None)
jax.full_like(x,fill_value,dtype=None,shape=None)
jax.gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.ge(x,y)
jax.gt(x,y)
jax.imag(x)
jax.index_in_dim(operand,index,axis=0,keepdims=True)
jax.index_take(src,idxs,axes)
jax.iota(dtype,size)
jax.is_finite(x)
jax.lax._broadcasting_select(c,which,x,y)
jax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax._const(example,val)
jax.lax._eq_meet(a,b)
jax.lax._reduce_and(operand,axes)
jax.lax._reduce_max(operand,axes)
jax.lax._reduce_min(operand,axes)
jax.lax._reduce_or(operand,axes)
jax.lax._reduce_sum(operand,axes)
jax.lax._reduce_sum_shape_rule(operand,axes,input_shape)
jax.lax._reduce_sum_translation_rule(c,operand,axes,input_shape)
jax.lax._reduce_sum_transpose_rule(cotangent,input_shape,axes)
jax.lax._reduce_window_max(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_min(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_prod(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_sum(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_sum_shape_rule(operand,window_dimensions,window_strides,padding,input_shape)
jax.lax._reduce_window_sum_translation_rule(c,operand,window_dimensions,window_strides,padding,input_shape)
jax.lax._reduce_window_sum_transpose_rule(cotangent,window_dimensions,window_strides,padding,input_shape)
jax.lax._safe_mul(x,y)
jax.lax._safe_mul_translation_rule(c,x,y)
jax.lax.lax.ConvDimensionNumbers(collections.namedtuple('ConvDimensionNumbers',['lhs_spec','rhs_spec','out_spec']))
jax.lax.lax.GatherDimensionNumbers(collections.namedtuple('GatherDimensionNumbers',['offset_dims','collapsed_slice_dims','start_index_map']))
jax.lax.lax.ScatterDimensionNumbers(collections.namedtuple('ScatterDimensionNumbers',['update_window_dims','inserted_window_dims','scatter_dims_to_operand_dims']))
jax.lax.lax._EyeConstant(self,shape,axes,dtype)
jax.lax.lax._EyeConstant.__init__(self,shape,axes,dtype)
jax.lax.lax._EyeConstant._value(self)
jax.lax.lax._EyeConstant.constant_handler(c,diag_const,canonicalize_types=True)
jax.lax.lax._FilledConstant(self,fill_value,shape)
jax.lax.lax._FilledConstant.__init__(self,fill_value,shape)
jax.lax.lax._FilledConstant._value(self)
jax.lax.lax._FilledConstant.constant_handler(c,filled_const,canonicalize_types=True)
jax.lax.lax._IotaConstant(self,dtype,shape,axis)
jax.lax.lax._IotaConstant.__init__(self,dtype,shape,axis)
jax.lax.lax._IotaConstant._value(self)
jax.lax.lax._IotaConstant.constant_handler(c,iota_constant,canonicalize_types=True)
jax.lax.lax._abstractify(x)
jax.lax.lax._add_transpose(t,x,y)
jax.lax.lax._balanced_eq(x,z,y)
jax.lax.lax._batch_dynamic_slice_indices(indices,bdims)
jax.lax.lax._bitcast_convert_type_dtype_rule(operand,new_dtype)
jax.lax.lax._bitcast_convert_type_shape_rule(operand,new_dtype)
jax.lax.lax._bitcast_convert_type_translation_rule(c,operand,new_dtype)
jax.lax.lax._brcast(x,*others)
jax.lax.lax._brcast_to(x,shape)
jax.lax.lax._broadcast_batch_rule(batched_args,batch_dims,sizes)
jax.lax.lax._broadcast_in_dim_batch_rule(batched_args,batch_dims,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_shape_rule(operand,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_transpose_rule(t,shape,broadcast_dimensions)
jax.lax.lax._broadcast_shape_rule(operand,sizes)
jax.lax.lax._broadcasting_select(c,which,x,y)
jax.lax.lax._broadcasting_shape_rule(name,*avals)
jax.lax.lax._canonicalize_precision(precision)
jax.lax.lax._canonicalize_shape(shape)
jax.lax.lax._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax.lax.lax._check_same_dtypes(name,ignore_fp_precision,*dtypes)
jax.lax.lax._check_shapelike(fun_name,arg_name,obj)
jax.lax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax.lax._clamp_shape_rule(min,operand,max)
jax.lax.lax._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding)
jax.lax.lax._concat_polymorphic_shape_rule(shape_exprs,dimension,operand_shapes)
jax.lax.lax._concatenate_batch_rule(batched_args,batch_dims,dimension,operand_shapes)
jax.lax.lax._concatenate_dtype_rule(*operands,**kwargs)
jax.lax.lax._concatenate_shape_rule(*operands,**kwargs)
jax.lax.lax._concatenate_translation_rule(c,*operands,**kwargs)
jax.lax.lax._concatenate_transpose_rule(t,*operands,**kwargs)
jax.lax.lax._conj_transpose_rule(t,x,input_dtype)
jax.lax.lax._const(example,val)
jax.lax.lax._conv_general_dilated_batch_rule(batched_args,batch_dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax._conv_general_dilated_dtype_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,**unused_kwargs)
jax.lax.lax._conv_general_dilated_shape_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,**unused_kwargs)
jax.lax.lax._conv_general_dilated_translation_rule(c,lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax._conv_general_dilated_transpose_lhs(g,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_dilated_transpose_rhs(g,lhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_proto(dimension_numbers)
jax.lax.lax._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax.lax.lax._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax.lax.lax._conv_transpose_padding(k,s,padding)
jax.lax.lax._convert_element_type_dtype_rule(operand,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_shape_rule(operand,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_translation_rule(c,operand,new_dtype,old_dtype)
jax.lax.lax._dilate_shape(shape,dilation)
jax.lax.lax._div_transpose_rule(cotangent,x,y)
jax.lax.lax._dot_batch_rule(batched_args,batch_dims,precision=None)
jax.lax.lax._dot_general_batch_rule(batched_args,batch_dims,dimension_numbers,precision)
jax.lax.lax._dot_general_dtype_rule(lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_shape_rule(lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_translation_rule(c,lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_transpose_lhs(g,y,dimension_numbers,precision,swap_ans=False)
jax.lax.lax._dot_general_transpose_rhs(g,x,dimension_numbers,precision)
jax.lax.lax._dot_masking_rule(padded_vals,logical_shapes,precision)
jax.lax.lax._dot_polymorphic_shape_rule(shape_exprs,precision)
jax.lax.lax._dot_shape_rule(lhs,rhs,precision)
jax.lax.lax._dot_translation_rule(c,lhs,rhs,precision)
jax.lax.lax._dot_transpose_lhs(t,rhs,precision)
jax.lax.lax._dot_transpose_rhs(t,lhs,precision)
jax.lax.lax._dynamic_slice_batching_rule(batched_args,batch_dims,slice_sizes,operand_shape)
jax.lax.lax._dynamic_slice_indices(operand,start_indices)
jax.lax.lax._dynamic_slice_jvp(primals,tangents,slice_sizes,operand_shape)
jax.lax.lax._dynamic_slice_shape_rule(operand,*start_indices,**kwargs)
jax.lax.lax._dynamic_slice_translation_rule(c,operand,*start_indices,**kwargs)
jax.lax.lax._dynamic_slice_transpose_rule(t,operand,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_batching_rule(batched_args,batch_dims,update_shape)
jax.lax.lax._dynamic_update_slice_dtype_rule(operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_jvp(primals,tangents,update_shape)
jax.lax.lax._dynamic_update_slice_shape_rule(operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_translation_rule(c,operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices,**kwargs)
jax.lax.lax._eq_meet(a,b)
jax.lax.lax._flip_axes(x,axes)
jax.lax.lax._gather_batching_rule(batched_args,batch_dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._gather_dtype_rule(operand,start_indices,**kwargs)
jax.lax.lax._gather_jvp_rule(g,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax._gather_shape_rule(operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax._gather_translation_rule(c,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax._gather_transpose_rule(t,operand,start_indices,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax._generic_reduce_window_batch_rule(batched_args,batch_dims,jaxpr,consts,window_dimensions,window_strides,padding)
jax.lax.lax._get_max_identity(dtype)
jax.lax.lax._get_min_identity(dtype)
jax.lax.lax._get_monoid_reducer(monoid_op,x)
jax.lax.lax._get_monoid_window_reducer(monoid_op,x)
jax.lax.lax._identity(x)
jax.lax.lax._is_axis_merge(s1,s2)
jax.lax.lax._is_axis_split(s1,s2)
jax.lax.lax._iter(tracer)
jax.lax.lax._masking_defreducer(prim,identity)
jax.lax.lax._minmax_translation_rule(c,x,y,minmax=None,cmp=None)
jax.lax.lax._outer(x,y)
jax.lax.lax._pad_batch_rule(batched_args,batch_dims,padding_config)
jax.lax.lax._pad_shape_rule(operand,padding_value,padding_config)
jax.lax.lax._pad_transpose(t,operand,padding_value,padding_config)
jax.lax.lax._pow_jvp_lhs(g,x,y)
jax.lax.lax._pow_jvp_rhs(g,x,y)
jax.lax.lax._precision_config(precision)
jax.lax.lax._reduce_and(operand,axes)
jax.lax.lax._reduce_batch_rule(batched_args,batch_dims,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_chooser_jvp_rule(g,ans,operand,axes)
jax.lax.lax._reduce_chooser_shape_rule(operand,axes)
jax.lax.lax._reduce_chooser_translation_rule(prim,identity,c,operand,axes)
jax.lax.lax._reduce_logical_shape_rule(operand,axes)
jax.lax.lax._reduce_logical_translation_rule(prim,identity,c,operand,axes)
jax.lax.lax._reduce_max(operand,axes)
jax.lax.lax._reduce_min(operand,axes)
jax.lax.lax._reduce_or(operand,axes)
jax.lax.lax._reduce_prod(operand,axes)
jax.lax.lax._reduce_prod_jvp_rule(tangent,operand,axes)
jax.lax.lax._reduce_prod_shape_rule(operand,axes)
jax.lax.lax._reduce_prod_translation_rule(c,operand,axes)
jax.lax.lax._reduce_shape_rule(operand,init_value,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_sum(operand,axes)
jax.lax.lax._reduce_sum_shape_rule(operand,axes,input_shape)
jax.lax.lax._reduce_sum_translation_rule(c,operand,axes,input_shape)
jax.lax.lax._reduce_sum_transpose_rule(cotangent,input_shape,axes)
jax.lax.lax._reduce_translation_rule(c,operand,init_value,computation,jaxpr,consts,dimensions,backend=None)
jax.lax.lax._reduce_window_batch_rule(reduce_window,batched_args,bdims,window_dimensions,window_strides,padding,input_shape=None)
jax.lax.lax._reduce_window_chooser_jvp_rule(prim,g,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_chooser_translation_rule(prim,identity,c,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_max(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_min(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_prod(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_shape_rule(operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum_shape_rule(operand,window_dimensions,window_strides,padding,input_shape)
jax.lax.lax._reduce_window_sum_translation_rule(c,operand,window_dimensions,window_strides,padding,input_shape)
jax.lax.lax._reduce_window_sum_transpose_rule(cotangent,window_dimensions,window_strides,padding,input_shape)
jax.lax.lax._reduce_window_translation_rule(c,operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding,backend=None)
jax.lax.lax._reducer_masking_rule(prim,identity,padded_vals,logical_shapes,axes,input_shape)
jax.lax.lax._reducer_polymorphic_shape_rule(shape_exprs,axes,**unused_params)
jax.lax.lax._reduction_computation(c,jaxpr,backend,consts,init_value)
jax.lax.lax._reduction_jaxpr(computation,aval)
jax.lax.lax._reshape_axis_into(src,dst,x)
jax.lax.lax._reshape_axis_out_of(src,size1,x)
jax.lax.lax._reshape_batch_rule(batched_args,batch_dims,new_sizes,dimensions,**unused)
jax.lax.lax._reshape_dtype_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax.lax.lax._reshape_impl(operand,new_sizes,dimensions,old_sizes)
jax.lax.lax._reshape_polymorphic_shape_rule(shape_exprs,new_sizes,dimensions,old_sizes)
jax.lax.lax._reshape_shape_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax.lax.lax._reshape_translation_rule(c,operand,new_sizes,dimensions,old_sizes)
jax.lax.lax._reshape_transpose_rule(t,new_sizes,dimensions,old_sizes)
jax.lax.lax._rev_batch_rule(batched_args,batch_dims,dimensions)
jax.lax.lax._rev_shape_rule(operand,dimensions)
jax.lax.lax._safe_mul(x,y)
jax.lax.lax._safe_mul_translation_rule(c,x,y)
jax.lax.lax._scatter_add_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax.lax.lax._scatter_add_transpose_rule(t,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax.lax.lax._scatter_batching_rule(scatter_op,batched_args,batch_dims,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax.lax.lax._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._scatter_dtype_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers,updates_shape)
jax.lax.lax._scatter_shape_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_translation_rule(c,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers,updates_shape,backend=None)
jax.lax.lax._select_and_gather_add(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_shape_rule(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_translation(c,tangents,operand,select_prim,window_dimensions,window_strides,padding,max_bits=64)
jax.lax.lax._select_and_gather_add_transpose(t,tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter(operand,select,window_dimensions,window_strides,padding,source,init_value,scatter)
jax.lax.lax._select_and_scatter_add(source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_batch_rule(batched_args,batch_dims,**kwargs)
jax.lax.lax._select_and_scatter_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_shape_rule(source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_translation(c,source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_transpose(t,source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_shape_rule(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_translation(c,operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding,backend=None)
jax.lax.lax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax.lax.lax._select_dtype_rule(pred,on_true,on_false)
jax.lax.lax._select_shape_rule(pred,on_true,on_false)
jax.lax.lax._select_transpose_rule(t,pred,on_true,on_false)
jax.lax.lax._slice_batching_rule(batched_args,batch_dims,start_indices,limit_indices,strides,**unused_kwargs)
jax.lax.lax._slice_shape_rule(operand,start_indices,limit_indices,strides,operand_shape)
jax.lax.lax._slice_translation_rule(c,operand,start_indices,limit_indices,strides,operand_shape)
jax.lax.lax._slice_transpose_rule(t,start_indices,limit_indices,strides,operand_shape)
jax.lax.lax._sort_batch_rule(batched_args,batch_dims,dimension)
jax.lax.lax._sort_jvp_rule(g,operand,dimension)
jax.lax.lax._sort_key_val_abstract_eval(keys,values,dimension)
jax.lax.lax._sort_key_val_batch_rule(batched_args,batch_dims,dimension)
jax.lax.lax._sort_key_val_jvp(primals,tangents,dimension)
jax.lax.lax._sort_key_val_transpose_rule(t,keys,values,dimension)
jax.lax.lax._stop_gradient_batch_rule(batched_args,batch_dims)
jax.lax.lax._stop_gradient_jvp_rule(primals,tangents)
jax.lax.lax._sub_transpose(t,x,y)
jax.lax.lax._tie_in_batch_rule(batched_args,batch_dims)
jax.lax.lax._tie_in_transpose_rule(t)
jax.lax.lax._transpose_batch_rule(batched_args,batch_dims,permutation)
jax.lax.lax._transpose_shape_rule(operand,permutation)
jax.lax.lax.abs(x)
jax.lax.lax.acos(x)
jax.lax.lax.add(x,y)
jax.lax.lax.asin(x)
jax.lax.lax.atan(x)
jax.lax.lax.atan2(x,y)
jax.lax.lax.batch_matmul(lhs,rhs)
jax.lax.lax.binop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.lax.binop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.lax.lax.bitcast_convert_type(operand,new_dtype)
jax.lax.lax.bitwise_and(x,y)
jax.lax.lax.bitwise_not(x)
jax.lax.lax.bitwise_or(x,y)
jax.lax.lax.bitwise_xor(x,y)
jax.lax.lax.broadcast(operand,sizes)
jax.lax.lax.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.lax.lax.broadcast_shapes(*shapes)
jax.lax.lax.broadcasted_eye(dtype,shape,axes)
jax.lax.lax.broadcasted_iota(dtype,shape,dimension)
jax.lax.lax.ceil(x)
jax.lax.lax.clamp(min,x,max)
jax.lax.lax.collapse(operand,start_dimension,stop_dimension)
jax.lax.lax.complex(x,y)
jax.lax.lax.concatenate(operands,dimension)
jax.lax.lax.conj(x)
jax.lax.lax.conv(lhs,rhs,window_strides,padding,precision=None)
jax.lax.lax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.lax.lax.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,precision=None)
jax.lax.lax.conv_general_permutations(dimension_numbers)
jax.lax.lax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads)
jax.lax.lax.conv_transpose(lhs,rhs,strides,padding,dimension_numbers=None,transpose_kernel=False,precision=None)
jax.lax.lax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,precision=None)
jax.lax.lax.convert_element_type(operand,new_dtype)
jax.lax.lax.cos(x)
jax.lax.lax.cosh(x)
jax.lax.lax.digamma(x)
jax.lax.lax.div(x,y)
jax.lax.lax.dot(lhs,rhs,precision=None)
jax.lax.lax.dot_general(lhs,rhs,dimension_numbers,precision=None)
jax.lax.lax.dynamic_index_in_dim(operand,index,axis=0,keepdims=True)
jax.lax.lax.dynamic_slice(operand,start_indices,slice_sizes)
jax.lax.lax.dynamic_slice_in_dim(operand,start_index,slice_size,axis=0)
jax.lax.lax.dynamic_update_index_in_dim(operand,update,index,axis)
jax.lax.lax.dynamic_update_slice(operand,update,start_indices)
jax.lax.lax.dynamic_update_slice_in_dim(operand,update,start_index,axis)
jax.lax.lax.eq(x,y)
jax.lax.lax.erf(x)
jax.lax.lax.erf_inv(x)
jax.lax.lax.erfc(x)
jax.lax.lax.exp(x)
jax.lax.lax.expm1(x)
jax.lax.lax.eye(dtype,size)
jax.lax.lax.floor(x)
jax.lax.lax.full(shape,fill_value,dtype=None)
jax.lax.lax.full_like(x,fill_value,dtype=None,shape=None)
jax.lax.lax.gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax.ge(x,y)
jax.lax.lax.gt(x,y)
jax.lax.lax.imag(x)
jax.lax.lax.index_in_dim(operand,index,axis=0,keepdims=True)
jax.lax.lax.index_take(src,idxs,axes)
jax.lax.lax.iota(dtype,size)
jax.lax.lax.is_finite(x)
jax.lax.lax.le(x,y)
jax.lax.lax.lgamma(x)
jax.lax.lax.log(x)
jax.lax.lax.log1p(x)
jax.lax.lax.lt(x,y)
jax.lax.lax.max(x,y)
jax.lax.lax.min(x,y)
jax.lax.lax.mul(x,y)
jax.lax.lax.ne(x,y)
jax.lax.lax.neg(x)
jax.lax.lax.pad(operand,padding_value,padding_config)
jax.lax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.lax.lax.pow(x,y)
jax.lax.lax.ranges_like(*xs)
jax.lax.lax.real(x)
jax.lax.lax.reciprocal(x)
jax.lax.lax.reduce(operand,init_value,computation,dimensions)
jax.lax.lax.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.lax.lax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding)
jax.lax.lax.rem(x,y)
jax.lax.lax.remaining(original,*removed_lists)
jax.lax.lax.reshape(operand,new_sizes,dimensions=None)
jax.lax.lax.rev(operand,dimensions)
jax.lax.lax.round(x)
jax.lax.lax.rsqrt(x)
jax.lax.lax.scatter(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_add(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_max(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_min(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.select(pred,on_true,on_false)
jax.lax.lax.shaped_identity(x)
jax.lax.lax.shift_left(x,y)
jax.lax.lax.shift_right_arithmetic(x,y)
jax.lax.lax.shift_right_logical(x,y)
jax.lax.lax.sign(x)
jax.lax.lax.sin(x)
jax.lax.lax.sinh(x)
jax.lax.lax.slice(operand,start_indices,limit_indices,strides=None)
jax.lax.lax.slice_in_dim(operand,start_index,limit_index,stride=1,axis=0)
jax.lax.lax.sort(operand,dimension=-1)
jax.lax.lax.sort_key_val(keys,values,dimension=-1)
jax.lax.lax.sqrt(x)
jax.lax.lax.square(x)
jax.lax.lax.standard_abstract_eval(shape_rule,dtype_rule,*args,**kwargs)
jax.lax.lax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.lax.lax.standard_reduction_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.lax.lax.standard_translate(name,c,*args,**kwargs)
jax.lax.lax.stop_gradient(x)
jax.lax.lax.sub(x,y)
jax.lax.lax.subvals(lst,replace)
jax.lax.lax.tan(x)
jax.lax.lax.tanh(x)
jax.lax.lax.tie_in(x,y)
jax.lax.lax.transpose(operand,permutation)
jax.lax.lax.unop(result_dtype,accepted_dtypes,name)
jax.lax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.lax.lax.zeros_like_array(x)
jax.le(x,y)
jax.lgamma(x)
jax.log(x)
jax.log1p(x)
jax.lt(x,y)
jax.max(x,y)
jax.min(x,y)
jax.mul(x,y)
jax.ne(x,y)
jax.neg(x)
jax.pad(operand,padding_value,padding_config)
jax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.pow(x,y)
jax.ranges_like(*xs)
jax.real(x)
jax.reciprocal(x)
jax.reduce(operand,init_value,computation,dimensions)
jax.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding)
jax.rem(x,y)
jax.remaining(original,*removed_lists)
jax.reshape(operand,new_sizes,dimensions=None)
jax.rev(operand,dimensions)
jax.round(x)
jax.rsqrt(x)
jax.scatter(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_add(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_max(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_min(operand,scatter_indices,updates,dimension_numbers)
jax.select(pred,on_true,on_false)
jax.shaped_identity(x)
jax.shift_left(x,y)
jax.shift_right_arithmetic(x,y)
jax.shift_right_logical(x,y)
jax.sign(x)
jax.sin(x)
jax.sinh(x)
jax.slice(operand,start_indices,limit_indices,strides=None)
jax.slice_in_dim(operand,start_index,limit_index,stride=1,axis=0)
jax.sort(operand,dimension=-1)
jax.sort_key_val(keys,values,dimension=-1)
jax.sqrt(x)
jax.square(x)
jax.standard_abstract_eval(shape_rule,dtype_rule,*args,**kwargs)
jax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.standard_reduction_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.standard_translate(name,c,*args,**kwargs)
jax.stop_gradient(x)
jax.sub(x,y)
jax.subvals(lst,replace)
jax.tan(x)
jax.tanh(x)
jax.tie_in(x,y)
jax.transpose(operand,permutation)
jax.unop(result_dtype,accepted_dtypes,name)
jax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.zeros_like_array(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax/lax_parallel.py----------------------------------------
A:jax.lax.lax_parallel.prim->jax.core.Primitive(name)
A:jax.lax.lax_parallel.dtype->c.GetShape(val).numpy_dtype()
A:jax.lax.lax_parallel.scalar->jax.lib.xla_client.Shape.array_shape(dtype, ())
A:jax.lax.lax_parallel.computation->jax.interpreters.xla.primitive_computation(prim, scalar, scalar, backend=backend)
A:jax.lax.lax_parallel.psum_p->standard_pmap_primitive('psum')
A:jax.lax.lax_parallel.pxla.split_axis_rules[psum_p]->partial(_allreduce_split_axis_rule, psum_p, lax._reduce_sum)
A:jax.lax.lax_parallel.xla.parallel_translations[psum_p]->partial(_allreduce_translation_rule, lax.add_p)
A:jax.lax.lax_parallel.pmax_p->standard_pmap_primitive('pmax')
A:jax.lax.lax_parallel.xla.parallel_translations[pmax_p]->partial(_allreduce_translation_rule, lax.max_p)
A:jax.lax.lax_parallel.pxla.split_axis_rules[pmax_p]->partial(_allreduce_split_axis_rule, pmax_p, lax._reduce_max)
A:jax.lax.lax_parallel.pmin_p->standard_pmap_primitive('pmin')
A:jax.lax.lax_parallel.xla.parallel_translations[pmin_p]->partial(_allreduce_translation_rule, lax.min_p)
A:jax.lax.lax_parallel.pxla.split_axis_rules[pmin_p]->partial(_allreduce_split_axis_rule, pmin_p, lax._reduce_min)
A:jax.lax.lax_parallel.group_size->len(replica_groups[0])
A:jax.lax.lax_parallel.(srcs, dsts)->unzip2(perm)
A:jax.lax.lax_parallel.grp->list(sorted(grp))
A:jax.lax.lax_parallel.inverse_perm->list(zip(dsts, srcs))
A:jax.lax.lax_parallel.ppermute_p->standard_pmap_primitive('ppermute')
A:jax.lax.lax_parallel.stacked->standard_pmap_primitive('all_to_all').bind(x, split_axis=split_axis + 1, concat_axis=0, axis_name=axis_name)
A:jax.lax.lax_parallel.out->jax.lax.lax.gather(operand, start_indices, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.lax.lax_parallel.all_to_all_p->standard_pmap_primitive('all_to_all')
A:jax.lax.lax_parallel.shape->list(x.shape)
A:jax.lax.lax_parallel.x->jax.lax.lax.psplit_like(x, y, name)
A:jax.lax.lax_parallel.y->jax.lax.lax.psplit_like(y, x, name)
A:jax.lax.lax_parallel.parallel.papply_primitive_rules[prim]->partial(_identity_papply, prim, argnum)
A:jax.lax.lax_parallel.result->jax.core.Primitive(name).bind(operand, axes=tuple(other_axes), **kwargs)
A:jax.lax.lax_parallel.xbatch->adjust_dims(xbatch, xdim)
A:jax.lax.lax_parallel.xcontract->adjust_dims(xcontract, xdim)
A:jax.lax.lax_parallel.ybatch->adjust_dims(ybatch, ydim)
A:jax.lax.lax_parallel.ycontract->adjust_dims(ycontract, ydim)
A:jax.lax.lax_parallel.z->jax.lax.lax.dot_general(x, y, sub_dims(xdim, None, xc, yc, xb, yb), precision)
A:jax.lax.lax_parallel.(ok, out)->cases(x, y, xdim, ydim, lhs_contract, rhs_contract, lhs_batch, rhs_batch)
A:jax.lax.lax_parallel.left->numpy.prod(old_sizes[:old_axis])
A:jax.lax.lax_parallel.new_axis->find_new_axis(axis, old_sizes, new_sizes)
A:jax.lax.lax_parallel.lhs->jax.lax.lax.reshape(lhs, tuple(onp.insert(lhs.shape, lhs_dim, 1)))
A:jax.lax.lax_parallel.sub_bdims->tuple(onp.delete(broadcast_dimensions, dim))
A:jax.lax.lax_parallel.sub_shape->tuple(onp.delete(shape, out_dim))
A:jax.lax.lax_parallel.padding_config->list(padding_config)
A:jax.lax.lax_parallel.padded->jax.lax.lax.pad(operand, padding_value, padding_config[:operand_dim] + padding_config[operand_dim + 1:])
A:jax.lax.lax_parallel.start_indices->list(start_indices)
A:jax.lax.lax_parallel.limit_indices->list(limit_indices)
A:jax.lax.lax_parallel.offset_dims->tuple((i - 1 if i > start_indices_dim else i for i in dimension_numbers.offset_dims))
A:jax.lax.lax_parallel.dnums->jax.lax.lax.GatherDimensionNumbers(offset_dims=offset_dims, collapsed_slice_dims=dimension_numbers.collapsed_slice_dims, start_index_map=dimension_numbers.start_index_map)
jax.lax._add_jaxvals_papply_rule(name,size,vals,dims)
jax.lax._all_to_all_split_axis_rule(vals,which_mapped,split_axis,concat_axis,axis_name)
jax.lax._all_to_all_translation_rule(c,x,split_axis,concat_axis,replica_groups,backend=None)
jax.lax._allgather(x,dim,size,axis_name)
jax.lax._allreduce_split_axis_rule(prim,reducer,vals,which_mapped,axis_name)
jax.lax._allreduce_translation_rule(prim,c,val,replica_groups,backend=None)
jax.lax._broadcast_in_dim_papply_rule(name,size,vals,dims,shape,broadcast_dimensions)
jax.lax._broadcasting_papply(prim,name,size,vals,axes,**params)
jax.lax._conv_general_dilated_papply_rule(name,size,vals,dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax._convert_element_type_papply_rule(name,size,vals,dims,new_dtype,**params)
jax.lax._defbroadcasting(prim)
jax.lax._defidentity(prim,argnum=0)
jax.lax._defreducer(prim,collective_prim)
jax.lax._defvectorized(prim)
jax.lax._dot_general_papply_rule(name,size,vals,dims,dimension_numbers,precision)
jax.lax._dot_papply_rule(name,size,vals,dims,precision)
jax.lax._drop(x,dim,axis_name)
jax.lax._gather_papply_rule(name,size,vals,dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax._identity_papply(prim,argnum,name,size,vals,axes,**params)
jax.lax._moveaxis(src,dst,x)
jax.lax._pad_papply_rule(name,size,vals,dims,padding_config)
jax.lax._ppermute_translation_rule(c,x,replica_groups,perm,backend=None)
jax.lax._ppermute_transpose_rule(t,perm,axis_name)
jax.lax._reducer_papply(prim,cprim,name,size,vals,papply_axes,axes,**kwargs)
jax.lax._reshape_papply_rule(name,size,vals,axes,new_sizes,dimensions,old_sizes)
jax.lax._select_papply_rule(name,size,vals,dims)
jax.lax._slice_papply_rule(name,size,vals,dims,start_indices,limit_indices,strides,**kwargs)
jax.lax._transpose_papply_rule(name,size,vals,dims,permutation)
jax.lax._vectorized_papply(prim,name,size,vals,axes,**params)
jax.lax.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel._add_jaxvals_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._all_to_all_split_axis_rule(vals,which_mapped,split_axis,concat_axis,axis_name)
jax.lax.lax_parallel._all_to_all_translation_rule(c,x,split_axis,concat_axis,replica_groups,backend=None)
jax.lax.lax_parallel._allgather(x,dim,size,axis_name)
jax.lax.lax_parallel._allreduce_split_axis_rule(prim,reducer,vals,which_mapped,axis_name)
jax.lax.lax_parallel._allreduce_translation_rule(prim,c,val,replica_groups,backend=None)
jax.lax.lax_parallel._broadcast_in_dim_papply_rule(name,size,vals,dims,shape,broadcast_dimensions)
jax.lax.lax_parallel._broadcasting_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel._conv_general_dilated_papply_rule(name,size,vals,dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax_parallel._convert_element_type_papply_rule(name,size,vals,dims,new_dtype,**params)
jax.lax.lax_parallel._defbroadcasting(prim)
jax.lax.lax_parallel._defidentity(prim,argnum=0)
jax.lax.lax_parallel._defreducer(prim,collective_prim)
jax.lax.lax_parallel._defvectorized(prim)
jax.lax.lax_parallel._dot_general_papply_rule(name,size,vals,dims,dimension_numbers,precision)
jax.lax.lax_parallel._dot_papply_rule(name,size,vals,dims,precision)
jax.lax.lax_parallel._drop(x,dim,axis_name)
jax.lax.lax_parallel._gather_papply_rule(name,size,vals,dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax_parallel._identity_papply(prim,argnum,name,size,vals,axes,**params)
jax.lax.lax_parallel._moveaxis(src,dst,x)
jax.lax.lax_parallel._pad_papply_rule(name,size,vals,dims,padding_config)
jax.lax.lax_parallel._ppermute_translation_rule(c,x,replica_groups,perm,backend=None)
jax.lax.lax_parallel._ppermute_transpose_rule(t,perm,axis_name)
jax.lax.lax_parallel._reducer_papply(prim,cprim,name,size,vals,papply_axes,axes,**kwargs)
jax.lax.lax_parallel._reshape_papply_rule(name,size,vals,axes,new_sizes,dimensions,old_sizes)
jax.lax.lax_parallel._select_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._slice_papply_rule(name,size,vals,dims,start_indices,limit_indices,strides,**kwargs)
jax.lax.lax_parallel._transpose_papply_rule(name,size,vals,dims,permutation)
jax.lax.lax_parallel._vectorized_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel.pmax(x,axis_name)
jax.lax.lax_parallel.pmin(x,axis_name)
jax.lax.lax_parallel.ppermute(x,axis_name,perm)
jax.lax.lax_parallel.psum(x,axis_name)
jax.lax.lax_parallel.pswapaxes(x,axis_name,axis)
jax.lax.lax_parallel.standard_pmap_primitive(name)
jax.lax.pmax(x,axis_name)
jax.lax.pmin(x,axis_name)
jax.lax.ppermute(x,axis_name,perm)
jax.lax.psum(x,axis_name)
jax.lax.pswapaxes(x,axis_name,axis)
jax.lax.standard_pmap_primitive(name)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax/lax_control_flow.py----------------------------------------
A:jax.lax.lax_control_flow.(fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.lax.lax_control_flow.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(fun, in_pvals, instantiate=True)
A:jax.lax.lax_control_flow.out_avals->_map(raise_to_shaped, unzip2(out_pvals)[0])
A:jax.lax.lax_control_flow.const_avals->tuple((raise_to_shaped(core.get_aval(c)) for c in consts))
A:jax.lax.lax_control_flow.typed_jaxpr->jax.core.TypedJaxpr(pe.closure_convert_jaxpr(jaxpr), (), const_avals + in_avals, out_avals)
A:jax.lax.lax_control_flow.aval->ShapedArray((), onp.int64)
A:jax.lax.lax_control_flow.(_, result)->while_loop(while_cond_fun, while_body_fun, (lower, init_val))
A:jax.lax.lax_control_flow.(init_vals, in_tree)->tree_flatten((init_val,))
A:jax.lax.lax_control_flow.init_avals->tuple(_map(_abstractify, init_vals))
A:jax.lax.lax_control_flow.(cond_jaxpr, cond_consts, cond_tree)->_initial_style_jaxpr(cond_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.(body_jaxpr, body_consts, body_tree)->_initial_style_jaxpr(body_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.outs->jax.core.Primitive('scan').bind(*new_args, forward=forward, length=length, jaxpr=jaxpr_batched, num_consts=num_consts, num_carry=num_carry, linear=linear)
A:jax.lax.lax_control_flow.backend->kwargs.pop('backend', None)
A:jax.lax.lax_control_flow.(cond_jaxpr, body_jaxpr, cond_nconsts, body_nconsts)->split_dict(kwargs, ['cond_jaxpr', 'body_jaxpr', 'cond_nconsts', 'body_nconsts'])
A:jax.lax.lax_control_flow.(cond_consts, body_consts, init_vals)->split_list(args, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.batched->bool(cond_jaxpr.out_avals[0].shape)
A:jax.lax.lax_control_flow.init_carry->jax.lib.xla_bridge.make_computation_builder(name).Tuple(*cond_consts + body_consts + init_vals)
A:jax.lax.lax_control_flow.cond_c->jax.lib.xla_bridge.make_computation_builder('cond_computation')
A:jax.lax.lax_control_flow.cond_carry->jax.lib.xla_bridge.make_computation_builder('cond_computation').ParameterWithShape(c.GetShape(init_carry))
A:jax.lax.lax_control_flow.(x, _, z)->split_list(cond_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(pred,)->jax.interpreters.xla.jaxpr_subcomp(cond_c, cond_jaxpr.jaxpr, backend, axis_env, _map(cond_c.Constant, cond_jaxpr.literals), (), *x + z)
A:jax.lax.lax_control_flow.scalar->jax.lib.xla_client.Shape.array_shape(onp.dtype(onp.bool_), ())
A:jax.lax.lax_control_flow.or_->jax.interpreters.xla.primitive_computation(lax.or_p, scalar, scalar)
A:jax.lax.lax_control_flow.pred->jax.lib.xla_bridge.make_computation_builder('cond_computation').Reduce(pred, cond_c.Constant(onp.array(False)), or_, list(range(cond_jaxpr.out_avals[0].ndim)))
A:jax.lax.lax_control_flow.body_c->jax.lib.xla_bridge.make_computation_builder('body_computation')
A:jax.lax.lax_control_flow.body_carry->jax.lib.xla_bridge.make_computation_builder('body_computation').ParameterWithShape(c.GetShape(init_carry))
A:jax.lax.lax_control_flow.(x, y, z)->split_list(body_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.new_z->_map(partial(_pred_bcast_select, body_c, body_pred), new_z, z)
A:jax.lax.lax_control_flow.(body_pred,)->jax.interpreters.xla.jaxpr_subcomp(body_c, cond_jaxpr.jaxpr, backend, axis_env, _map(body_c.Constant, cond_jaxpr.literals), (), *x + z)
A:jax.lax.lax_control_flow.new_carry->jax.lib.xla_bridge.make_computation_builder('body_computation').Tuple(*itertools.chain(x, y, new_z))
A:jax.lax.lax_control_flow.ans->jax.lib.xla_bridge.make_computation_builder(name).While(cond_c.Build(pred), body_c.Build(new_carry), init_carry)
A:jax.lax.lax_control_flow.(_, _, z)->split_list(ans_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.pred_shape->jax.lib.xla_bridge.make_computation_builder(name).GetShape(pred).dimensions()
A:jax.lax.lax_control_flow.x_shape->jax.lib.xla_bridge.make_computation_builder(name).GetShape(x).dimensions()
A:jax.lax.lax_control_flow.y_shape->jax.lib.xla_bridge.make_computation_builder(name).GetShape(y).dimensions()
A:jax.lax.lax_control_flow.bcast_pred->jax.lax.lax.broadcast_in_dim(pred, onp.shape(x), list(range(onp.ndim(pred))))
A:jax.lax.lax_control_flow.(cconst_bat, bconst_bat, init_bat)->split_list(orig_batched, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jaxpr_batched, carry_bat_out)->jax.interpreters.batching.batch_jaxpr(body_jaxpr, size, batched, instantiate=carry_bat)
A:jax.lax.lax_control_flow.(cond_jaxpr_batched, (pred_bat,))->jax.interpreters.batching.batch_jaxpr(cond_jaxpr, size, cconst_bat + carry_bat, instantiate=False)
A:jax.lax.lax_control_flow.carry_bat_out->_map(partial(operator.or_, pred_bat), carry_bat_out)
A:jax.lax.lax_control_flow.(consts, init)->split_list(args, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.(const_dims, init_dims)->split_list(dims, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.while_p->jax.lax.lax.Primitive('while')
A:jax.lax.lax_control_flow.(true_ops, true_tree)->tree_flatten((true_operand,))
A:jax.lax.lax_control_flow.true_avals->tuple(_map(_abstractify, true_ops))
A:jax.lax.lax_control_flow.(true_jaxpr, true_consts, out_tree)->_initial_style_jaxpr(true_fun, true_tree, true_avals)
A:jax.lax.lax_control_flow.(false_ops, false_tree)->tree_flatten((false_operand,))
A:jax.lax.lax_control_flow.false_avals->tuple(_map(_abstractify, false_ops))
A:jax.lax.lax_control_flow.(false_jaxpr, false_consts, out_tree2)->_initial_style_jaxpr(false_fun, false_tree, false_avals)
A:jax.lax.lax_control_flow.out->fun(*consts + carry + xs)
A:jax.lax.lax_control_flow.(true_jaxpr, false_jaxpr, true_nconsts, false_nconsts)->split_dict(kwargs, ['true_jaxpr', 'false_jaxpr', 'true_nconsts', 'false_nconsts'])
A:jax.lax.lax_control_flow.(true_consts, true_ops, false_consts, false_ops)->split_list(args, [true_nconsts, true_nops, false_nconsts])
A:jax.lax.lax_control_flow.c->jax.lib.xla_bridge.make_computation_builder(name)
A:jax.lax.lax_control_flow.op->jax.lib.xla_bridge.make_computation_builder(name).ParameterWithShape(op_shape)
A:jax.lax.lax_control_flow.true_op->jax.lib.xla_bridge.make_computation_builder(name).Tuple(*true_consts + true_ops)
A:jax.lax.lax_control_flow.true_c->make_computation('true_comp', true_jaxpr, c.GetShape(true_op))
A:jax.lax.lax_control_flow.false_op->jax.lib.xla_bridge.make_computation_builder(name).Tuple(*false_consts + false_ops)
A:jax.lax.lax_control_flow.false_c->make_computation('false_comp', false_jaxpr, c.GetShape(false_op))
A:jax.lax.lax_control_flow.((pred,), true_consts, true_ops, false_consts, false_ops)->split_list(args, [1, true_nconsts, true_nops, false_nconsts])
A:jax.lax.lax_control_flow.((pred_bat,), tconst_bat, t_bat, fconst_bat, f_bat)->split_list(orig_bat, [1, true_nconsts, true_nops, false_nconsts])
A:jax.lax.lax_control_flow.(_, true_out_bat)->jax.interpreters.batching.batch_jaxpr(true_jaxpr, size, tconst_bat + t_bat, False)
A:jax.lax.lax_control_flow.(_, false_out_bat)->jax.interpreters.batching.batch_jaxpr(false_jaxpr, size, fconst_bat + f_bat, False)
A:jax.lax.lax_control_flow.(true_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr(true_jaxpr, size, tconst_bat + t_bat, out_bat)
A:jax.lax.lax_control_flow.(false_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr(false_jaxpr, size, fconst_bat + f_bat, out_bat)
A:jax.lax.lax_control_flow.true_out->jax.core.jaxpr_as_fun(true_jaxpr_batched)(*true_consts + true_ops)
A:jax.lax.lax_control_flow.false_out->jax.core.jaxpr_as_fun(false_jaxpr_batched)(*false_consts + false_ops)
A:jax.lax.lax_control_flow.cond_p->jax.lax.lax.Primitive('cond')
A:jax.lax.lax_control_flow.num_carry->len(tree_flatten(init)[0])
A:jax.lax.lax_control_flow.(in_flat, in_tree)->tree_flatten((init, xs))
A:jax.lax.lax_control_flow.carry_avals->tuple(_map(_abstractify, init_flat))
A:jax.lax.lax_control_flow.x_avals->tuple(_map(ShapedArray, x_shapes, x_dtypes))
A:jax.lax.lax_control_flow.(jaxpr, consts, out_tree)->_initial_style_jaxpr(f, in_tree, carry_avals + x_avals)
A:jax.lax.lax_control_flow.(carry_avals_out, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.(forward, length, num_consts, num_carry, jaxpr, linear)->split_dict(kwargs, ['forward', 'length', 'num_consts', 'num_carry', 'jaxpr', 'linear'])
A:jax.lax.lax_control_flow.(consts, init, xs)->split_list(args, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(_, _, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(_, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.(carry, ys)->split_list(vals, [num_carry])
A:jax.lax.lax_control_flow.x->_map(partial(_index_array, i), x_avals, xs)
A:jax.lax.lax_control_flow.out_flat->jax.core.Primitive('scan').bind(*in_consts, forward=forward, length=length, jaxpr=jaxpr_1, num_consts=num_consts, num_carry=num_carry, linear=linear_1)
A:jax.lax.lax_control_flow.(carry_out, y_updates)->split_list(out_flat, [num_carry])
A:jax.lax.lax_control_flow.ys_out->_map(partial(_update_array, i), y_avals, ys, y_updates)
A:jax.lax.lax_control_flow.ys_init->_map(partial(_empty_array, length), y_avals)
A:jax.lax.lax_control_flow.(const_nz, init_nz, xs_nz)->split_list(nonzeros, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(jaxpr, nonzeros, instantiate=carry_nz + [False] * num_ys)
A:jax.lax.lax_control_flow.all_tangents->split_list(tangents, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts_dot, init_dot, xs_dot)->_map(_prune_zeros, all_tangents)
A:jax.lax.lax_control_flow.jaxpr_jvp_rearranged->jax.interpreters.ad.rearrange_binders(jaxpr_jvp, [num_consts, num_carry, num_xs], [len(consts_dot), len(init_dot), len(xs_dot)], [num_carry, num_ys], [len(init_dot), sum(nonzeros_out) - len(init_dot)])
A:jax.lax.lax_control_flow.(consts_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(carry, carry_dot, ys, ys_dot)->split_list(out_flat, [num_carry, len(init_dot), num_ys])
A:jax.lax.lax_control_flow.tangents_out->iter(carry_dot + ys_dot)
A:jax.lax.lax_control_flow.(const_uk, init_uk, xs_uk)->split_list(unknowns, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_1, jaxpr_2, out_uk)->jax.interpreters.partial_eval.partial_eval_jaxpr(jaxpr, unknowns, instantiate=carry_uk + [False] * num_ys)
A:jax.lax.lax_control_flow.(carry_avals, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.ys_avals->_map(partial(_promote_aval_rank, length), y_avals)
A:jax.lax.lax_control_flow.(out_carry, ys, residuals)->split_list(out_flat, [num_carry, num_ys])
A:jax.lax.lax_control_flow.residual_tracers->_map(trace.new_instantiated_const, residuals)
A:jax.lax.lax_control_flow.eqn->jax.interpreters.partial_eval.new_jaxpr_eqn(new_tracers + residual_tracers, out_tracers, scan_p, (), dict(forward=forward, length=length, jaxpr=jaxpr_2, num_consts=num_consts, num_carry=num_carry, linear=linear_2))
A:jax.lax.lax_control_flow.(consts_lin, init_lin, xs_lin)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.num_lin->sum(xs_lin)
A:jax.lax.lax_control_flow.(consts, init, xs, res)->split_list(args, [num_consts, num_carry, num_lin])
A:jax.lax.lax_control_flow.(ct_carry, ct_ys)->split_list(cts, [num_carry])
A:jax.lax.lax_control_flow.ct_carry->_map(ad.instantiate_zeros_aval, carry_avals, ct_carry)
A:jax.lax.lax_control_flow.ct_ys->_map(ad.instantiate_zeros_aval, ys_avals, ct_ys)
A:jax.lax.lax_control_flow.ct_consts->_map(ad_util.zeros_like_aval, jaxpr.in_avals[:num_consts])
A:jax.lax.lax_control_flow.jaxpr_trans->_transpose_jaxpr(num_consts, len(res), jaxpr)
A:jax.lax.lax_control_flow.(ct_consts, ct_init, ct_xs)->split_list(outs, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(c_avals, a_avals, res_avals)->split_list(jaxpr.in_avals, [num_c, num_a])
A:jax.lax.lax_control_flow.num_b->len(jaxpr.out_avals)
A:jax.lax.lax_control_flow.b_avals->list(jaxpr.out_avals)
A:jax.lax.lax_control_flow.(c_bar, b_bar, res)->split_list(cbar_bbar_res, [num_c, num_b])
A:jax.lax.lax_control_flow.(_, cbar_abar)->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, jaxpr.literals, (), primals, b_bar)
A:jax.lax.lax_control_flow.(new_c_bar, a_bar, _)->split_list(cbar_abar, [num_c, num_a])
A:jax.lax.lax_control_flow.a_bar->_map(ad.instantiate_zeros_aval, a_avals, a_bar)
A:jax.lax.lax_control_flow.c_bar->_map(ad.instantiate_zeros_aval, c_avals, _map(ad.add_tangents, c_bar, new_c_bar))
A:jax.lax.lax_control_flow.(jaxpr, pvals_out, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(traceable, pvals, instantiate=True)
A:jax.lax.lax_control_flow.(out_avals, _)->unzip2(pvals_out)
A:jax.lax.lax_control_flow.(const_batched, init_batched, xs_batched)->split_list(orig_batched, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_batched, batched_out)->jax.interpreters.batching.batch_jaxpr(jaxpr, size, batched, instantiate=carry_batched + [False] * num_ys)
A:jax.lax.lax_control_flow.(consts_bdims, init_bdims, xs_bdims)->split_list(dims, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(const_shexprs, init_shexprs, xs_shexprs)->split_list(shape_exprs, [num_consts, num_carry])
A:jax.lax.lax_control_flow.out_shape->_scan_polymorphic_shape_rule(shape_exprs, forward, length, jaxpr, num_consts, num_carry, linear)
A:jax.lax.lax_control_flow.dynamic_length->jax.interpreters.masking.eval_dim_expr(shape_envs.logical, length)
A:jax.lax.lax_control_flow.masked_jaxpr->_masked_scan_jaxpr(jaxpr, num_consts, num_carry)
A:jax.lax.lax_control_flow.(const_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.out_vals->jax.core.Primitive('scan').bind(*itertools.chain([dynamic_length] + consts, [0], init, xs), forward=forward, length=max_length, jaxpr=masked_jaxpr, num_consts=1 + num_consts, num_carry=1 + num_carry, linear=[False] + const_linear + [False] + init_linear + xs_linear)
A:jax.lax.lax_control_flow.fun->jax.core.jaxpr_as_fun(jaxpr)
A:jax.lax.lax_control_flow.([dynamic_length], consts, [i], carry, xs)->split_list(args, [1, num_consts, 1, num_carry])
A:jax.lax.lax_control_flow.(new_carry, ys)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.(const_avals, carry_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts_avals, init_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.xs_avals->_map(partial(_promote_aval_rank, length), x_avals)
A:jax.lax.lax_control_flow.(carry_avals, _)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.scan_p->jax.core.Primitive('scan')
A:jax.lax.lax_control_flow.xla.initial_style_translations[scan_p]->jax.interpreters.xla.lower_fun(_scan_impl, initial_style=True)
A:jax.lax.lax_control_flow.(_, ys)->scan(g, (), xs)
A:jax.lax.lax_control_flow.result->_memcpy(dimension, logical_shape[dimension], padded_val, result, offset)
A:jax.lax.lax_control_flow.update->jax.lax.lax.dynamic_index_in_dim(src, i, axis)
jax.lax.FixedPointError(Exception)
jax.lax._abstractify(x)
jax.lax._concat_masking_rule(padded_vals,logical_shapes,dimension,operand_shapes)
jax.lax._cond_abstract_eval(*args,**kwargs)
jax.lax._cond_batching_rule(args,dims,true_jaxpr,false_jaxpr,true_nconsts,false_nconsts)
jax.lax._cond_pred_bcast_select(pred,x,y)
jax.lax._cond_translation_rule(c,axis_env,pred,*args,**kwargs)
jax.lax._empty_array(sz,aval)
jax.lax._index_array(i,aval,x)
jax.lax._initial_style_jaxpr(fun,in_tree,in_avals)
jax.lax._make_typed_jaxpr(traceable,in_avals)
jax.lax._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax.lax._memcpy(axis,num,src,dst,offset)
jax.lax._pred_bcast_select(c,pred,x,y)
jax.lax._promote_aval_rank(sz,aval)
jax.lax._prune_zeros(ts)
jax.lax._scan_batching_rule(args,dims,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_impl(*args,**kwargs)
jax.lax._scan_jvp(primals,tangents,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_masking_rule(shape_envs,padded_vals,shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_partial_eval(trace,*tracers,**kwargs)
jax.lax._scan_polymorphic_shape_rule(shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_transpose(cts,*args,**kwargs)
jax.lax._transpose_jaxpr(num_c,num_res,jaxpr)
jax.lax._update_array(i,aval,xs,x)
jax.lax._while_loop_abstract_eval(*args,**kwargs)
jax.lax._while_loop_batching_rule(args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax._while_loop_translation_rule(c,axis_env,*args,**kwargs)
jax.lax.cond(pred,true_operand,true_fun,false_operand,false_fun)
jax.lax.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow.FixedPointError(Exception)
jax.lax.lax_control_flow._abstractify(x)
jax.lax.lax_control_flow._concat_masking_rule(padded_vals,logical_shapes,dimension,operand_shapes)
jax.lax.lax_control_flow._cond_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._cond_batching_rule(args,dims,true_jaxpr,false_jaxpr,true_nconsts,false_nconsts)
jax.lax.lax_control_flow._cond_pred_bcast_select(pred,x,y)
jax.lax.lax_control_flow._cond_translation_rule(c,axis_env,pred,*args,**kwargs)
jax.lax.lax_control_flow._empty_array(sz,aval)
jax.lax.lax_control_flow._index_array(i,aval,x)
jax.lax.lax_control_flow._initial_style_jaxpr(fun,in_tree,in_avals)
jax.lax.lax_control_flow._make_typed_jaxpr(traceable,in_avals)
jax.lax.lax_control_flow._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax.lax.lax_control_flow._memcpy(axis,num,src,dst,offset)
jax.lax.lax_control_flow._pred_bcast_select(c,pred,x,y)
jax.lax.lax_control_flow._promote_aval_rank(sz,aval)
jax.lax.lax_control_flow._prune_zeros(ts)
jax.lax.lax_control_flow._scan_batching_rule(args,dims,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_impl(*args,**kwargs)
jax.lax.lax_control_flow._scan_jvp(primals,tangents,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_masking_rule(shape_envs,padded_vals,shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_partial_eval(trace,*tracers,**kwargs)
jax.lax.lax_control_flow._scan_polymorphic_shape_rule(shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_transpose(cts,*args,**kwargs)
jax.lax.lax_control_flow._transpose_jaxpr(num_c,num_res,jaxpr)
jax.lax.lax_control_flow._update_array(i,aval,xs,x)
jax.lax.lax_control_flow._while_loop_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._while_loop_batching_rule(args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax.lax_control_flow._while_loop_translation_rule(c,axis_env,*args,**kwargs)
jax.lax.lax_control_flow.cond(pred,true_operand,true_fun,false_operand,false_fun)
jax.lax.lax_control_flow.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow.map(f,xs)
jax.lax.lax_control_flow.scan(f,init,xs)
jax.lax.lax_control_flow.scan_bind(*args,**kwargs)
jax.lax.lax_control_flow.typecheck(aval,x)
jax.lax.lax_control_flow.typematch(aval1,aval2)
jax.lax.lax_control_flow.while_loop(cond_fun,body_fun,init_val)
jax.lax.map(f,xs)
jax.lax.scan(f,init,xs)
jax.lax.scan_bind(*args,**kwargs)
jax.lax.typecheck(aval,x)
jax.lax.typematch(aval1,aval2)
jax.lax.while_loop(cond_fun,body_fun,init_val)


----------------------------------------/home/zhang/Packages/jax/jax0.1.46/lax/lax_fft.py----------------------------------------
A:jax.lax.lax_fft.fft_lengths->tuple(fft_lengths)
A:jax.lax.lax_fft.x->interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax.lax_fft.fft_p->Primitive('fft')
jax.lax.fft(x,fft_type,fft_lengths=None)
jax.lax.fft_abstract_eval(x,fft_type,fft_lengths)
jax.lax.fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax.lax.fft_impl(x,fft_type,fft_lengths)
jax.lax.fft_translation_rule(c,x,fft_type,fft_lengths)
jax.lax.fft_transpose_rule(t,fft_type,fft_lengths)
jax.lax.lax_fft.fft(x,fft_type,fft_lengths=None)
jax.lax.lax_fft.fft_abstract_eval(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax.lax.lax_fft.fft_impl(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_translation_rule(c,x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_transpose_rule(t,fft_type,fft_lengths)

