
----------------------------------------/home/zhang/Packages/jax/jax0.1.62/flatten_util.py----------------------------------------
A:jax.flatten_util.(leaves, treedef)->tree_flatten(pytree)
A:jax.flatten_util.(flat, unravel_list)->vjp(ravel_list, *leaves)
A:jax.flatten_util.pytree_args->unravel_inputs(flat_in)
jax.flatten_util.ravel_fun(unravel_inputs,flat_in,**kwargs)
jax.flatten_util.ravel_list(*lst)
jax.flatten_util.ravel_pytree(pytree)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/util.py----------------------------------------
A:jax.util.n->len(args[0])
A:jax.util.args->list(args)
A:jax.util.lst->list(lst)
A:jax.util.dct->dict(dct)
A:jax.util.wrapped->functools.partial(fun, *args, **kwargs)
A:jax.util.end_nodes->_remove_duplicates(end_nodes)
A:jax.util.stack->list(end_nodes)
A:jax.util.node->childless_nodes.pop()
A:jax.util.visited->set()
A:jax.util.seen->set()
A:jax.util.sides->list(map(predicate, xs))
A:jax.util.memoize->functools.lru_cache(maxsize=None)
A:jax.util.module_fns->set()
A:jax.util.attr->getattr(module, key)
jax.util.Hashable(self,val)
jax.util.Hashable.__eq__(self,other)
jax.util.Hashable.__hash__(self)
jax.util.WrapHashably(self,val)
jax.util.WrapHashably.__eq__(self,other)
jax.util.WrapHashably.__hash__(self)
jax.util._remove_duplicates(node_list)
jax.util.cache(max_size=4096)
jax.util.check_toposort(nodes)
jax.util.concatenate(xs)
jax.util.curry(f)
jax.util.extend_name_stack(stack,name='')
jax.util.get_module_functions(module)
jax.util.partial(fun,*args,**kwargs)
jax.util.partialmethod(functools.partial)
jax.util.partialmethod.__get__(self,instance,owner)
jax.util.prod(xs)
jax.util.safe_map(f,*args)
jax.util.safe_zip(*args)
jax.util.split_dict(dct,names)
jax.util.split_list(args,ns)
jax.util.split_merge(predicate,xs)
jax.util.subvals(lst,replace)
jax.util.toposort(end_nodes)
jax.util.unzip2(xys)
jax.util.unzip3(xyzs)
jax.util.wrap_name(name,transform_name)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/abstract_arrays.py----------------------------------------
A:jax.abstract_arrays.dtype->dtypes.canonicalize_dtype(dtypes.result_type(x))
jax.abstract_arrays._make_concrete_python_scalar(x)
jax.abstract_arrays._zeros_like_python_scalar(x)
jax.abstract_arrays.make_shaped_array(x)
jax.abstract_arrays.zeros_like_array(x)
jax.abstract_arrays.zeros_like_shaped_array(aval)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax_reference.py----------------------------------------
A:jax.lax_reference.quotient->numpy.floor_divide(lhs, rhs)
A:jax.lax_reference.select->numpy.logical_and(onp.sign(lhs) != onp.sign(rhs), onp.remainder(lhs, rhs) != 0)
A:jax.lax_reference.pads->padtype_to_pads(op.shape, dims, strides, padding)
A:jax.lax_reference.(lhs_perm, rhs_perm, out_perm)->_conv_general_permutations(dimension_numbers)
A:jax.lax_reference.padding->padtype_to_pads(onp.take(lhs.shape, lhs_perm)[2:], onp.take(rhs.shape, rhs_perm)[2:], window_strides, padding)
A:jax.lax_reference.trans_lhs->transpose(lhs, lhs_perm)
A:jax.lax_reference.trans_rhs->transpose(rhs, rhs_perm)
A:jax.lax_reference.out->numpy.zeros(operand.shape[:2] + tuple(outspace), operand.dtype)
A:jax.lax_reference.new_id->itertools.count()
A:jax.lax_reference.shared_id->next(new_id)
A:jax.lax_reference.out_axis_ids->filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)
A:jax.lax_reference.in_reshape->numpy.ones(len(shape), dtype=onp.int32)
A:jax.lax_reference.dimensions->frozenset(dimensions)
A:jax.lax_reference.(lo, hi, interior)->zip(*padding_config)
A:jax.lax_reference.outshape->numpy.add(onp.add(onp.add(lo, hi), operand.shape), onp.multiply(interior, onp.subtract(operand.shape, 1)))
A:jax.lax_reference.lhs_slices->tuple((_slice(None, None, step) for step in factors))
A:jax.lax_reference.rhs_slices->tuple((_slice(l if l < 0 else 0, -h if h < 0 else None) for (l, h) in zip(lo, hi)))
A:jax.lax_reference.strides->numpy.ones(len(start_indices)).astype(int)
A:jax.lax_reference.slices->tuple((_slice(abs(lo) if lo < 0 else 0, hi % dim if hi < 0 else None) for ((lo, hi), dim) in zip(pads, onp.shape(arr))))
A:jax.lax_reference.idx->tuple((_slice(start, start + size) for (start, size) in zip(start_indices, slice_sizes)))
A:jax.lax_reference.updated_operand->numpy.copy(operand)
A:jax.lax_reference.reducer->_make_reducer(computation, init_value)
A:jax.lax_reference.view->numpy.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)
A:jax.lax_reference.idxs->list(onp.ix_(*[onp.arange(d) for d in keys.shape]))
A:jax.lax_reference.idxs[dimension]->numpy.argsort(keys, axis=dimension)
A:jax.lax_reference.(view, view_axes, rhs_axes, out_axes)->_conv_view(lhs, rhs.shape, window_strides, pads, 0.0)
A:jax.lax_reference.out_shape->numpy.ceil(onp.true_divide(in_shape, window_strides)).astype(int)
A:jax.lax_reference.lhs->_pad(lhs, [(0, 0)] * 2 + list(pads), pad_value)
A:jax.lax_reference.dim->len(filter_shape)
A:jax.lax_reference.out_strides->numpy.multiply(window_strides, lhs.strides[2:])
A:jax.lax_reference.view_axes->list(range(view.ndim))
A:jax.lax_reference.outspace->numpy.add(operand.shape[2:], onp.multiply(onp.subtract(factors, 1), onp.subtract(operand.shape[2:], 1)))
A:jax.lax_reference.monoid_record->_monoids.get(getattr(py_binop, '__name__'))
A:jax.lax_reference.MonoidRecord->collections.namedtuple('MonoidRecord', ['reducer', 'identity'])
A:jax.lax_reference.result->numpy.full(onp.delete(onp.shape(operand), axis), init_val, dtype=onp.asarray(operand).dtype)
A:jax.lax_reference.out_idx->tuple(onp.delete(idx, axis))
A:jax.lax_reference.result[out_idx]->py_binop(result[out_idx], operand[idx])
jax.lax_reference._conv(lhs,rhs,window_strides,pads)
jax.lax_reference._conv_general_permutations(dimension_numbers)
jax.lax_reference._conv_view(lhs,rhs_shape,window_strides,pads,pad_value)
jax.lax_reference._dilate(operand,factors)
jax.lax_reference._get_max_identity(dt)
jax.lax_reference._get_min_identity(dt)
jax.lax_reference._identity_getter(op)
jax.lax_reference._make_reducer(py_binop,init_val)
jax.lax_reference._pad(arr,pads,pad_value)
jax.lax_reference._reducer_from_pyfunc(py_binop,init_val)
jax.lax_reference.bitcast_convert_type(operand,dtype)
jax.lax_reference.broadcast(operand,sizes)
jax.lax_reference.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.lax_reference.clamp(min,operand,max)
jax.lax_reference.complex(x,y)
jax.lax_reference.concatenate(operands,dimension)
jax.lax_reference.conj(x)
jax.lax_reference.conv(lhs,rhs,window_strides,padding)
jax.lax_reference.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers)
jax.lax_reference.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation)
jax.lax_reference.convert_element_type(operand,dtype)
jax.lax_reference.div(lhs,rhs)
jax.lax_reference.dot_general(lhs,rhs,dimension_numbers)
jax.lax_reference.dynamic_slice(operand,start_indices,slice_sizes)
jax.lax_reference.dynamic_update_slice(operand,update,start_indices)
jax.lax_reference.pad(operand,padding_value,padding_config)
jax.lax_reference.padtype_to_pads(in_shape,filter_shape,window_strides,padding)
jax.lax_reference.reduce(operand,init_value,computation,dimensions)
jax.lax_reference.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.lax_reference.rem(lhs,rhs)
jax.lax_reference.reshape(operand,new_sizes,dimensions=None)
jax.lax_reference.rev(operand,dimensions)
jax.lax_reference.slice(operand,start_indices,limit_indices,strides=None)
jax.lax_reference.sort_key_val(keys,values,dimension=-1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/profiler.py----------------------------------------
jax.profiler.TraceContext(xla_client.profiler.TraceMe)
jax.profiler.start_server(port:int)
jax.profiler.trace_function(func:Callable,name:str=None,**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lazy.py----------------------------------------
A:jax.lazy.class_namespace[f]->property(op.itemgetter(i + 1))
A:jax.lazy.LazyExpr->namedtuple('LazyExpr', ['input', 'shape', 'dims'])
A:jax.lazy.ArrayVar->taggedtuple('ArrayVar', [])
A:jax.lazy.Iota->taggedtuple('Iota', ['dtype', 'size'])
A:jax.lazy.Eye->taggedtuple('Eye', ['dtype', 'shape', 'offset'])
A:jax.lazy.Tri->taggedtuple('Tri', ['dtype', 'shape', 'offset'])
A:jax.lazy.Delta->taggedtuple('Delta', ['dtype', 'shape'])
A:jax.lazy.new_shape->tuple((lexpr.shape[i] for i in perm))
A:jax.lazy.new_dims->tuple((lexpr.dims[i] for i in perm))
A:jax.lazy.t->type(input_)
A:jax.lazy.x->c.BroadcastInDim(x, shape, bcast_dims)
A:jax.lazy.bool_eye->c.Eq(c.Add(c.BroadcastedIota(onp.int32, (N, M), 0), c.Constant(onp.array(input_.offset, onp.int32))), c.BroadcastedIota(onp.int32, (N, M), 1))
A:jax.lazy.bool_tri->c.Ge(c.Add(c.BroadcastedIota(onp.int32, (N, M), 0), c.Constant(onp.array(input_.offset, onp.int32))), c.BroadcastedIota(onp.int32, (N, M), 1))
A:jax.lazy.etype->lib.xla_bridge.dtype_to_etype(input_.dtype)
A:jax.lazy.(bcast_dims, perm)->unzip2(((i, d) for (i, d) in enumerate(dims) if d is not None))
jax.lazy.array(shape)
jax.lazy.broadcast(lexpr,shape,broadcast_dimensions)
jax.lazy.delta(dtype,shape)
jax.lazy.eval_lexpr(lexpr,x)
jax.lazy.eye(dtype,shape,offset)
jax.lazy.iota(dtype,size)
jax.lazy.is_constant(lexpr)
jax.lazy.is_trivial(lexpr)
jax.lazy.stage_lexpr(c,lexpr,x)
jax.lazy.taggedtuple(name,fields)->Callable[..., Any]
jax.lazy.transpose(lexpr,perm)
jax.lazy.tri(dtype,shape,offset)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/test_util.py----------------------------------------
A:jax.test_util.tol->_default_tolerance.copy()
A:jax.test_util.dtype->_dtype(x)
A:jax.test_util.tol1->_normalize_tolerance(tol1)
A:jax.test_util.tol2->_normalize_tolerance(tol2)
A:jax.test_util.out[k]->max(v, tol1.get(k, 0))
A:jax.test_util.atol->max(tolerance(_dtype(x), atol), tolerance(_dtype(y), atol))
A:jax.test_util.rtol->max(tolerance(_dtype(x), rtol), tolerance(_dtype(y), rtol))
A:jax.test_util.assert_close->partial(_assert_numpy_close, atol=atol, rtol=rtol)
A:jax.test_util.add->partial(tree_multimap, lambda x, y: onp.add(x, y, dtype=_dtype(x)))
A:jax.test_util.sub->partial(tree_multimap, lambda x, y: onp.subtract(x, y, dtype=_dtype(x)))
A:jax.test_util.conj->partial(tree_map, lambda x: onp.conj(x, dtype=_dtype(x)))
A:jax.test_util.shape->numpy.shape(x)
A:jax.test_util.delta->scalar_mul(tangents, eps)
A:jax.test_util.f_pos->f(*add(primals, delta))
A:jax.test_util.f_neg->f(*sub(primals, delta))
A:jax.test_util.out->default.copy()
A:jax.test_util.rng->numpy.random.RandomState(42)
A:jax.test_util.tangent->tree_map(_rand_like, args)
A:jax.test_util.(v_out, t_out)->f_jvp(args, tangent)
A:jax.test_util.v_out_expected->f(*args)
A:jax.test_util.t_out_expected->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util._rand_like->partial(rand_like, onp.random.RandomState(0))
A:jax.test_util.(v_out, vjpfun)->f_vjp(*args)
A:jax.test_util.tangent_out->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util.cotangent->tree_map(_rand_like, v_out)
A:jax.test_util.cotangent_out->conj(vjpfun(conj(cotangent)))
A:jax.test_util.ip->inner_prod(tangent, cotangent_out)
A:jax.test_util.ip_expected->inner_prod(tangent_out, cotangent)
A:jax.test_util.args->args_maker()
A:jax.test_util._check_jvp->partial(check_jvp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util._check_vjp->partial(check_vjp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util.(out_primal_py, vjp_py)->api.vjp(f, *args)
A:jax.test_util.device->device_under_test()
A:jax.test_util.test_name->getattr(test_method, '__name__', '[unknown test]')
A:jax.test_util.flag_value->getattr(FLAGS, flag_name)
A:jax.test_util.NUMPY_SCALAR_SHAPE->_NumpyScalar()
A:jax.test_util.PYTHON_SCALAR_SHAPE->_PythonScalar()
A:jax.test_util.shapestr->','.join((str(dim) for dim in shape))
A:jax.test_util.vals->numpy.where(zeros, onp.array(0, dtype=dtype), vals)
A:jax.test_util.x_ravel->numpy.asarray(x).ravel()
A:jax.test_util.base_rand->rand_default()
A:jax.test_util.dims->_dims_of_shape(shape)
A:jax.test_util.jaxpr->api.make_jaxpr(fun)(*args)
A:jax.test_util.msg->'Unexpected precision: {} != {}'.format(expected_precision, precision)
A:jax.test_util.xs->list(xs)
A:jax.test_util.n->len(xs)
A:jax.test_util.k->min(n, FLAGS.num_generated_cases)
A:jax.test_util.indices->_CACHED_INDICES.get(n)
A:jax.test_util._CACHED_INDICES[n]indices->numpy.random.RandomState(42).permutation(n)
A:jax.test_util.x->numpy.asarray(x)
A:jax.test_util.y->numpy.asarray(y)
A:jax.test_util.ignore_space_re->re.compile('\\s*\\n\\s*')
A:jax.test_util.expected_clean->re.sub(ignore_space_re, '\n', expected.strip())
A:jax.test_util.what_clean->re.sub(ignore_space_re, '\n', what.strip())
A:jax.test_util.python_ans->fun(*args)
A:jax.test_util.python_shapes->tree_map(lambda x: onp.shape(x), python_ans)
A:jax.test_util.onp_shapes->tree_map(lambda x: onp.shape(onp.asarray(x)), python_ans)
A:jax.test_util.cfun->api.jit(wrapped_fun)
A:jax.test_util.monitored_ans->cfun(*args)
A:jax.test_util.compiled_ans->cfun(*args)
A:jax.test_util.lax_ans->lax_op(*args)
A:jax.test_util.numpy_ans->numpy_reference_op(*args)
jax.test_util.JaxTestCase(parameterized.TestCase)
jax.test_util.JaxTestCase._CheckAgainstNumpy(self,numpy_reference_op,lax_op,args_maker,check_dtypes=False,tol=None)
jax.test_util.JaxTestCase._CompileAndCheck(self,fun,args_maker,check_dtypes,rtol=None,atol=None)
jax.test_util.JaxTestCase.assertAllClose(self,x,y,check_dtypes,atol=None,rtol=None)
jax.test_util.JaxTestCase.assertArraysAllClose(self,x,y,check_dtypes,atol=None,rtol=None)
jax.test_util.JaxTestCase.assertDtypesMatch(self,x,y)
jax.test_util.JaxTestCase.assertMultiLineStrippedEqual(self,expected,what)
jax.test_util.ScalarShape(object)
jax.test_util.ScalarShape.__len__(self)
jax.test_util._NumpyScalar(ScalarShape)
jax.test_util._PythonScalar(ScalarShape)
jax.test_util._assert_numpy_allclose(a,b,atol=None,rtol=None)
jax.test_util._assert_numpy_close(a,b,atol=None,rtol=None)
jax.test_util._cast_to_shape(value,shape,dtype)
jax.test_util._dims_of_shape(shape)
jax.test_util._dtype(x)
jax.test_util._iter_eqns(jaxpr)
jax.test_util._merge_tolerance(tol,default)
jax.test_util._normalize_tolerance(tol)
jax.test_util._rand_dtype(rand,shape,dtype,scale=1.0,post=lambdax:x)
jax.test_util.assert_dot_precision(expected_precision,fun,*args)
jax.test_util.cases_from_gens(*gens)
jax.test_util.cases_from_list(xs)
jax.test_util.check_close(xs,ys,atol=None,rtol=None)
jax.test_util.check_eq(xs,ys)
jax.test_util.check_grads(f,args,order,modes=['fwd','rev'],atol=None,rtol=None,eps=None)
jax.test_util.check_jvp(f,f_jvp,args,atol=None,rtol=None,eps=EPS)
jax.test_util.check_raises(thunk,err_type,msg)
jax.test_util.check_raises_regexp(thunk,err_type,pattern)
jax.test_util.check_vjp(f,f_vjp,args,atol=None,rtol=None,eps=EPS)
jax.test_util.count_jit_and_pmap_compiles()
jax.test_util.count_primitive_compiles()
jax.test_util.default_tolerance()
jax.test_util.device_under_test()
jax.test_util.dtype_str(dtype)
jax.test_util.format_shape_dtype_string(shape,dtype)
jax.test_util.format_test_name_suffix(opname,shapes,dtypes)
jax.test_util.if_device_under_test(device_type:Union[str,Sequence[str]],if_true,if_false)
jax.test_util.inner_prod(xs,ys)
jax.test_util.is_sequence(x)
jax.test_util.join_tolerance(tol1,tol2)
jax.test_util.numerical_jvp(f,primals,tangents,eps=EPS)
jax.test_util.rand_bool()
jax.test_util.rand_default(scale=3)
jax.test_util.rand_int(low,high=None)
jax.test_util.rand_like(rng,x)
jax.test_util.rand_nonzero()
jax.test_util.rand_not_small()
jax.test_util.rand_positive()
jax.test_util.rand_small()
jax.test_util.rand_small_positive()
jax.test_util.rand_some_equal()
jax.test_util.rand_some_inf()
jax.test_util.rand_some_inf_and_nan()
jax.test_util.rand_some_nan()
jax.test_util.rand_some_zero()
jax.test_util.rand_uniform(low=0.0,high=1.0)
jax.test_util.scalar_mul(xs,a)
jax.test_util.skip_on_devices(*disabled_devices)
jax.test_util.skip_on_flag(flag_name,skip_value)
jax.test_util.supported_dtypes()
jax.test_util.tolerance(dtype,tol=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/dtypes.py----------------------------------------
A:jax.dtypes._bfloat16_dtype->numpy.dtype(bfloat16)
A:jax.dtypes.eps->bfloat16(float.fromhex('0x1p-7'))
A:jax.dtypes.epsneg->bfloat16(float.fromhex('0x1p-8'))
A:jax.dtypes.max->bfloat16(float.fromhex('0x1.FEp127'))
A:jax.dtypes.tiny->bfloat16(float.fromhex('0x1p-126'))
A:jax.dtypes.dtype->python_scalar_dtypes.get(type(x), None)
A:jax.dtypes.typ->dtype(x)
A:jax.dtypes._type_promotion_table->_make_type_promotion_table()
A:jax.dtypes.a->numpy.dtype(a)
A:jax.dtypes.b->numpy.dtype(b)
jax.dtypes._bfloat16_finfo(object)
jax.dtypes._dtype_priority(dtype)
jax.dtypes._make_type_promotion_table()
jax.dtypes.canonicalize_dtype(dtype)
jax.dtypes.coerce_to_array(x)
jax.dtypes.dtype(x)
jax.dtypes.finfo(dtype)
jax.dtypes.is_python_scalar(x)
jax.dtypes.issubdtype(a,b)
jax.dtypes.promote_types(a,b)
jax.dtypes.result_type(*args)
jax.dtypes.scalar_type_of(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/core.py----------------------------------------
A:jax.core.self.constvars->list(constvars)
A:jax.core.self.invars->list(invars)
A:jax.core.self.outvars->list(outvars)
A:jax.core.self.eqns->list(eqns)
A:jax.core.self.literals->list(literals)
A:jax.core.self.in_avals->list(in_avals)
A:jax.core.self.out_avals->list(out_avals)
A:jax.core.self.aval->raise_to_shaped(aval)
A:jax.core.counter->itertools.count()
A:jax.core.self.hash->hash((val.item(), val.dtype))
A:jax.core.top_trace->find_top_trace(args)
A:jax.core.tracers->map(top_trace.full_raise, args)
A:jax.core.out_tracer->find_top_trace(args).process_primitive(self, tracers, kwargs)
A:jax.core.new_params->dict(params)
A:jax.core.in_vals->map(read, eqn.invars)
A:jax.core.(call_jaxpr, params)->extract_call_jaxpr(eqn.primitive, eqn.params)
A:jax.core.ans->max(tracers, key=lambda x: x._trace.level)
A:jax.core.attr->getattr(self.aval, name)
A:jax.core.t->ref(sublevel)
A:jax.core.aval_property->namedtuple('aval_property', ['fget'])
A:jax.core.aval_method->namedtuple('aval_method', ['fun'])
A:jax.core.self.trace_stack->TraceStack()
A:jax.core.trace_state->TraceState()
A:jax.core.level->TraceState().trace_stack.next_level(bottom)
A:jax.core.master->MasterTrace(level, trace_type)
A:jax.core.sublevel->Sublevel(len(trace_state.substack))
A:jax.core.bot->Bot()
A:jax.core.abstract_unit->AbstractUnit()
A:jax.core.unit->Unit()
A:jax.core.unitvar->UnitVar()
A:jax.core.identity_p->Primitive('id')
A:jax.core.fname->getattr(fun, '__name__', fun)
A:jax.core.self.dtype->numpy.dtype(dtypes.canonicalize_dtype(dtype))
A:jax.core._bool_nonzero->partialmethod(_forward_to_value, bool)
A:jax.core._float->partialmethod(_forward_to_value, float)
A:jax.core._int->partialmethod(_forward_to_value, int)
A:jax.core._complex->partialmethod(_forward_to_value, complex)
A:jax.core._hex->partialmethod(_forward_to_value, hex)
A:jax.core._oct->partialmethod(_forward_to_value, oct)
A:jax.core.self.shape->canonicalize_shape(shape)
A:jax.core.ndim->property(lambda self: len(self.shape))
A:jax.core.size->property(lambda self: prod(self.shape))
A:jax.core.shapestr->','.join(map(str, self.shape))
A:jax.core.abstract_token->AbstractToken()
A:jax.core.todos_list->list(todos)
A:jax.core.outs->map(full_lower, top_trace.process_call(primitive, f, tracers, params))
A:jax.core.params->dict(params_tuple)
A:jax.core.trace->type(ans._trace)(ans._trace.master, cur_sublevel())
A:jax.core.(outs, cur_todo)->type(ans._trace)(ans._trace.master, cur_sublevel()).post_process_call(primitive, outs, params)
A:jax.core.params_tuple->tuple(params.items())
A:jax.core.(f, env_trace_todo)->process_env_traces(f, primitive, level, params_tuple)
A:jax.core.call_p->Primitive('call')
A:jax.core.call->partial(call_bind, call_p)
A:jax.core.read->partial(read_env, env)
A:jax.core.write->partial(write_env, env)
A:jax.core.lhs->pp_vars(eqn.outvars)
A:jax.core.pp_subexpr->pp('')
A:jax.core.pp_outvars->str(tuple(jaxpr.outvars))
jax.core.AbstractToken(AbstractValue)
jax.core.AbstractUnit(AbstractValue)
jax.core.AbstractUnit._eq(self,self_traced,other)
jax.core.AbstractUnit.join(self,other)
jax.core.AbstractValue(object)
jax.core.AbstractValue.__repr__(self)
jax.core.AbstractValue.at_least_vspace(self)
jax.core.AbstractValue.strip_weak_type(self)
jax.core.Bot(AbstractValue)
jax.core.ConcreteArray(self,val,weak_type=False)
jax.core.ConcreteArray.__eq__(self,other)
jax.core.ConcreteArray.__hash__(self)
jax.core.ConcreteArray.at_least_vspace(self)
jax.core.ConcreteArray.join(self,other)
jax.core.ConcreteArray.str_short(self)
jax.core.ConcreteArray.strip_weak_type(self)
jax.core.Jaxpr(self,constvars,invars,outvars,eqns)
jax.core.Jaxpr.__str__(self)
jax.core.JaxprEqn(namedtuple('JaxprEqn',['invars','outvars','primitive','params']))
jax.core.JaxprEqn.__repr__(self)
jax.core.Literal(self,val)
jax.core.Literal.__eq__(self,other)
jax.core.Literal.__hash__(self)
jax.core.Literal.__repr__(self)
jax.core.Literal.aval(self)
jax.core.MasterTrace(self,level,trace_type)
jax.core.MasterTrace.__eq__(self,other)
jax.core.MasterTrace.__hash__(self)
jax.core.MasterTrace.__repr__(self)
jax.core.Primitive(self,name)
jax.core.Primitive.__repr__(self)
jax.core.Primitive.abstract_eval(self,*args,**kwargs)
jax.core.Primitive.bind(self,*args,**kwargs)
jax.core.Primitive.def_abstract_eval(self,abstract_eval)
jax.core.Primitive.def_custom_bind(self,bind)
jax.core.Primitive.def_impl(self,impl)
jax.core.Primitive.impl(self,*args,**kwargs)
jax.core.ShapedArray(self,shape,dtype,weak_type=False)
jax.core.ShapedArray.__eq__(self,other)
jax.core.ShapedArray.__hash__(self)
jax.core.ShapedArray.__len__(self)
jax.core.ShapedArray._len(self,ignored_tracer)
jax.core.ShapedArray.at_least_vspace(self)
jax.core.ShapedArray.join(self,other)
jax.core.ShapedArray.str_short(self)
jax.core.ShapedArray.strip_weak_type(self)
jax.core.Sublevel(int)
jax.core.Trace(self,master,sublevel)
jax.core.Trace.__repr__(self)
jax.core.Trace.escaped_tracer_error(self,detail)
jax.core.Trace.full_raise(self,val)
jax.core.Trace.lift(self,tracer)
jax.core.Trace.process_primitive(self,primitive,tracers,params)
jax.core.Trace.pure(self,val)
jax.core.Trace.sublift(self,tracer)
jax.core.TraceStack(self)
jax.core.TraceStack.__repr__(self)
jax.core.TraceStack.next_level(self,bottom)
jax.core.TraceStack.pop(self,bottom)
jax.core.TraceStack.push(self,val,bottom)
jax.core.TraceState(self)
jax.core.Tracer(self,trace)
jax.core.Tracer.__abs__(self)
jax.core.Tracer.__add__(self,other)
jax.core.Tracer.__and__(self,other)
jax.core.Tracer.__array__(self,*args,**kw)
jax.core.Tracer.__bool__(self)
jax.core.Tracer.__complex__(self)
jax.core.Tracer.__copy__(self)
jax.core.Tracer.__deepcopy__(self,unused_memo)
jax.core.Tracer.__div__(self,other)
jax.core.Tracer.__divmod__(self,other)
jax.core.Tracer.__eq__(self,other)
jax.core.Tracer.__float__(self)
jax.core.Tracer.__floordiv__(self,other)
jax.core.Tracer.__ge__(self,other)
jax.core.Tracer.__getattr__(self,name)
jax.core.Tracer.__getitem__(self,idx)
jax.core.Tracer.__gt__(self,other)
jax.core.Tracer.__hex__(self)
jax.core.Tracer.__int__(self)
jax.core.Tracer.__invert__(self)
jax.core.Tracer.__iter__(self)
jax.core.Tracer.__le__(self,other)
jax.core.Tracer.__len__(self)
jax.core.Tracer.__long__(self)
jax.core.Tracer.__lshift__(self,other)
jax.core.Tracer.__lt__(self,other)
jax.core.Tracer.__matmul__(self,other)
jax.core.Tracer.__mod__(self,other)
jax.core.Tracer.__mul__(self,other)
jax.core.Tracer.__ne__(self,other)
jax.core.Tracer.__neg__(self)
jax.core.Tracer.__nonzero__(self)
jax.core.Tracer.__oct__(self)
jax.core.Tracer.__or__(self,other)
jax.core.Tracer.__pos__(self)
jax.core.Tracer.__pow__(self,other)
jax.core.Tracer.__radd__(self,other)
jax.core.Tracer.__rand__(self,other)
jax.core.Tracer.__rdiv__(self,other)
jax.core.Tracer.__rdivmod__(self,other)
jax.core.Tracer.__repr__(self)
jax.core.Tracer.__rfloordiv__(self,other)
jax.core.Tracer.__rmatmul__(self,other)
jax.core.Tracer.__rmod__(self,other)
jax.core.Tracer.__rmul__(self,other)
jax.core.Tracer.__ror__(self,other)
jax.core.Tracer.__rpow__(self,other)
jax.core.Tracer.__rshift__(self,other)
jax.core.Tracer.__rsub__(self,other)
jax.core.Tracer.__rtruediv__(self,other)
jax.core.Tracer.__rxor__(self,other)
jax.core.Tracer.__setitem__(self,idx,val)
jax.core.Tracer.__sub__(self,other)
jax.core.Tracer.__truediv__(self,other)
jax.core.Tracer.__xor__(self,other)
jax.core.Tracer.aval(self)
jax.core.TypedJaxpr(self,jaxpr:Jaxpr,literals:Sequence,in_avals:Sequence['AbstractValue'],out_avals:Sequence['AbstractValue'])
jax.core.TypedJaxpr.__iter__(self)
jax.core.TypedJaxpr.__str__(self)
jax.core.Unit(object)
jax.core.Unit.__repr__(self)
jax.core.UnitVar(object)
jax.core.UnitVar.__repr__(self)
jax.core.UnitVar.aval(self)
jax.core.UnshapedArray(self,dtype,weak_type=False)
jax.core.UnshapedArray.__eq__(self,other)
jax.core.UnshapedArray.__hash__(self)
jax.core.UnshapedArray.__ne__(self,other)
jax.core.UnshapedArray.__repr__(self)
jax.core.UnshapedArray.at_least_vspace(self)
jax.core.UnshapedArray.join(self,other)
jax.core.UnshapedArray.str_short(self)
jax.core.UnshapedArray.strip_weak_type(self)
jax.core.Var(self,count,suffix,aval)
jax.core.Var.__lt__(self,other)
jax.core.Var.__repr__(self)
jax.core._canonicalize_dimension(dim)
jax.core._forward_to_value(self,fun,ignored_tracer,*args)
jax.core.apply_todos(todos,outs)
jax.core.call_bind(primitive,f:lu.WrappedFun,*args,**params)
jax.core.call_impl(f:lu.WrappedFun,*args,**params)
jax.core.canonicalize_shape(shape)
jax.core.check_jaxpr(jaxpr:Jaxpr)
jax.core.concrete_aval(x)
jax.core.concretization_err_msg(fun,context=None)
jax.core.concretization_function_error(fun,context=None)
jax.core.cur_sublevel()
jax.core.eval_jaxpr(jaxpr,consts,*args)
jax.core.extract_call_jaxpr(primitive,params)
jax.core.find_top_trace(xs)
jax.core.full_lower(val)
jax.core.gensym(suffix)
jax.core.get_aval(x)
jax.core.jaxpr_as_fun(typed_jaxpr:TypedJaxpr,*args)
jax.core.lattice_join(x,y)
jax.core.new_master(trace_type:Type[Trace],bottom=False)->Generator[MasterTrace, None, None]
jax.core.new_sublevel()
jax.core.pp_eqn(eqn:JaxprEqn)->PrettyPrint
jax.core.pp_eqn_compact(primitive_name:str,params:Dict)->PrettyPrint
jax.core.pp_jaxpr(jaxpr)->PrettyPrint
jax.core.pp_vars(vs)->str
jax.core.process_env_traces(primitive,level,params_tuple,*args)
jax.core.raise_to_shaped(aval,weak_type=False)
jax.core.subjaxprs(jaxpr:Jaxpr)->Iterator[Jaxpr]
jax.core.valid_jaxtype(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/ad_util.py----------------------------------------
A:jax.ad_util.add_jaxvals_p->Primitive('add_any')
A:jax.ad_util.zeros_like_p->Primitive('zeros_like')
A:jax.ad_util.zero->Zero()
jax.ad_util.Zero(object)
jax.ad_util.Zero.__repr__(self)
jax.ad_util.add_abstract(xs,ys)
jax.ad_util.add_impl(xs,ys)
jax.ad_util.add_jaxvals(x,y)
jax.ad_util.zeros_like_aval(aval)
jax.ad_util.zeros_like_impl(example)
jax.ad_util.zeros_like_jaxval(val)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/config.py----------------------------------------
A:jax.config.val->val.lower().lower()
A:jax.config.self.FLAGS->NameSpace(self.read)
A:jax.config.config->Config()
jax.config.Config(self)
jax.config.Config.DEFINE_bool(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_enum(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_integer(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_string(self,name,default,*args,**kwargs)
jax.config.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs)
jax.config.Config.check_exists(self,name)
jax.config.Config.complete_absl_config(self,absl_flags)
jax.config.Config.config_with_absl(self)
jax.config.Config.parse_flags_with_absl(self)
jax.config.Config.read(self,name)
jax.config.Config.update(self,name,val)
jax.config.NameSpace(self,getter)
jax.config.NameSpace.__getattr__(self,name)
jax.config.bool_env(varname:str,default:bool)->bool


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/dlpack.py----------------------------------------
A:jax.dlpack.buf->lib.xla_client._xla.DLPackManagedTensorToBuffer(dlpack, backend.client)
A:jax.dlpack.xla_shape->lib.xla_client._xla.DLPackManagedTensorToBuffer(dlpack, backend.client).shape()
A:jax.dlpack.aval->core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
jax.dlpack.from_dlpack(dlpack,backend=None)
jax.dlpack.to_dlpack(x:xla.DeviceArray)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/api.py----------------------------------------
A:jax.api._thread_local_state->_ThreadLocalState()
A:jax.api.f->lu.wrap_init(fun)
A:jax.api.(f, dyn_args)->_argnums_partial(f, dyn_argnums, args)
A:jax.api.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax.api.(flat_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax.api.out->interpreters.partial_eval.abstract_eval_fun(wrapped_fun.call_wrapped, *map(abstractify, args_flat))
A:jax.api.f_jitted.__name__->jitted_name.format(f_jitted.__name__, static_argnums)
A:jax.api.fun_name->getattr(fun, '__name__', 'unknown')
A:jax.api.(names, sizes)->zip(*axis_env)
A:jax.api.wrapped->lu.wrap_init(fun, kwargs)
A:jax.api.(jax_args, in_tree)->tree_flatten((args, kwargs))
A:jax.api.(jaxtree_fun, out_tree)->flatten_fun(wrapped, in_tree)
A:jax.api.avals->map(xla.abstractify, jax_args)
A:jax.api.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(jaxtree_fun, pvals)
A:jax.api.axis_env_->make_axis_env(xla.jaxpr_replicas(jaxpr))
A:jax.api.c->lib.xla_bridge.make_computation_builder('xla_computation_{}'.format(fun_name))
A:jax.api.xla_consts->map(c.Constant, consts)
A:jax.api.xla_args->interpreters.xla._xla_callable_args(c, avals, tuple_args)
A:jax.api.outs->self.prim.bind(*it.chain(consts, args_flat), jaxpr=jaxpr, in_tree=in_tree, out_tree=out_tree(), num_consts=len(consts))
A:jax.api.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, holomorphic=holomorphic)
A:jax.api.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax.api.((_, aux), g)->value_and_grad_f(*args, **kwargs)
A:jax.api.(f_partial, dyn_args)->_argnums_partial(f, argnums, args)
A:jax.api.(ans, vjp_py)->_vjp(f_partial, *dyn_args)
A:jax.api.(ans, vjp_py, aux)->_vjp(f_partial, *dyn_args, has_aux=True)
A:jax.api.dtype->dtypes.result_type(*leaves)
A:jax.api.g->vjp_py(onp.ones((), dtype=dtype))
A:jax.api.aval->interpreters.xla.abstractify(x)
A:jax.api.pushfwd->partial(jvp, fun, primals)
A:jax.api.(y, jac)->vmap(pushfwd, out_axes=(None, batching.last))(_std_basis(dyn_args))
A:jax.api.(y, pullback)->_vjp(f_partial, *dyn_args)
A:jax.api.jac->tree_map(partial(_unravel_array_into_pytree, y, 0), jac)
A:jax.api.(leaves, _)->tree_flatten(pytree)
A:jax.api.ndim->property(lambda self: len(self.shape))
A:jax.api.flat_basis->numpy.eye(ndim, dtype=dtype)
A:jax.api.(leaves, treedef)->tree_flatten(pytree)
A:jax.api.parts->_split(arr, onp.cumsum(map(onp.size, leaves[:-1])), axis)
A:jax.api.in_axes->tuple(in_axes)
A:jax.api.sizes->tree_unflatten(tree, sizes)
A:jax.api.in_axes_flat->_flatten_axes(in_tree, in_axes)
A:jax.api.out_flat->interpreters.partial_eval.remat_call(flat_fun, *args_flat, name=flat_fun.__name__, concrete=concrete)
A:jax.api.proxy->object()
A:jax.api.dummy->tree_unflatten(treedef, [object()] * treedef.num_leaves)
A:jax.api.(args, in_tree)->tree_flatten(py_args)
A:jax.api.local_axis_size->_pmap_axis_size(args)
A:jax.api.f_pmapped.__name__->namestr(f_pmapped.__name__, axis_name)
A:jax.api.axis_size->_pmap_axis_size(args_flat)
A:jax.api.(chunk_size, leftover)->divmod(axis_size, pxla.unmapped_device_count())
A:jax.api.soft_mapped_fun->interpreters.pxla.split_axis(flat_fun, axis_name, chunk_size)
A:jax.api.reshaped_outs->interpreters.pxla.xla_pmap(soft_mapped_fun, *reshaped_args, backend=backend, axis_name=axis_name, axis_size=num_chunks, global_axis_size=None, devices=None, name=soft_mapped_fun.__name__)
A:jax.api.axis_name->_TempAxisName(fun)
A:jax.api.(f, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax.api.(f, out_axes)->interpreters.parallel.papply_transform(f, axis_name, axis_size)
A:jax.api.(in_specs, in_shapes_tree)->tree_flatten(in_shapes)
A:jax.api.(out_specs, out_shapes_tree)->tree_flatten(out_shape)
A:jax.api.in_specs->map(partial(_remap_ids, unique_ids), in_specs)
A:jax.api.out_specs->map(partial(_remap_ids, unique_ids), out_specs)
A:jax.api.in_shapes->map(masking.parse_spec, in_shapes)
A:jax.api.padded_env->_bind_shapes(in_shapes, [x.shape for x in args_flat])
A:jax.api.(outs, out_shapes_)->interpreters.masking.mask_fun(flat_fun, logical_env, padded_env, args_flat, in_shapes)
A:jax.api.out_shapes->map(masking.parse_spec, out_shapes)
A:jax.api.(in_shapes, in_tree)->tree_flatten(in_shapes)
A:jax.api.(out_shapes, out_tree)->tree_flatten(out_shape)
A:jax.api.(flat_fun, out_tree_)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.api.out_shapes_->interpreters.masking.shapecheck(flat_fun, in_shapes)
A:jax.api.(ps_flat, tree_def)->tree_flatten(primals)
A:jax.api.(ts_flat, tree_def_2)->tree_flatten(tangents)
A:jax.api.(out_primals, out_tangents)->interpreters.ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
A:jax.api.(primals_flat, in_tree)->tree_flatten(primals)
A:jax.api.(out_primals, out_pvals, jaxpr, consts)->interpreters.ad.linearize(jaxtree_fun, *primals_flat)
A:jax.api.out_tree->out_tree()
A:jax.api.out_primal_py->tree_unflatten(out_tree, out_primal)
A:jax.api.primal_avals->list(map(core.get_aval, primals_flat))
A:jax.api.lifted_jvp->partial(lift_linearized, jaxpr, primal_avals, consts, (in_tree, out_tree), out_pvals)
A:jax.api.tangent_avals->list(map(core.get_aval, tangents))
A:jax.api.ans->fun(*primals)
A:jax.api.has_aux->kwargs.pop('has_aux', False)
A:jax.api.(out_primal, out_vjp)->interpreters.ad.vjp(flat_fun, primals_flat)
A:jax.api.(flat_fun, out_aux_trees)->flatten_fun_nokwargs2(fun, in_tree)
A:jax.api.(out_primal, out_vjp, aux)->interpreters.ad.vjp(flat_fun, primals_flat, has_aux=True)
A:jax.api.(out_tree, aux_tree)->out_aux_trees()
A:jax.api.vjp_py->partial(_vjp_pullback_wrapper, out_vjp, [_dtype(x) for x in out_primal], (out_tree, in_tree))
A:jax.api.in_pvals->map(pv_like, jax_args)
A:jax.api.(jaxpr, out_pvals, consts)->interpreters.partial_eval.trace_to_jaxpr(jaxtree_fun, in_pvals, instantiate=True, stage_out_calls=True)
A:jax.api.out_avals->map(raise_to_shaped, unzip2(out_pvals)[0])
A:jax.api.in_avals->tuple((raise_to_shaped(in_aval) for (in_aval, _) in in_pvals))
A:jax.api.typed_jaxpr->core.TypedJaxpr(jaxpr, consts, in_avals, out_avals)
A:jax.api.jaxpr_maker.__name__->'make_jaxpr({})'.format(jaxpr_maker.__name__)
A:jax.api.dyn_argnums->tuple(dyn_argnums)
A:jax.api.fixed_args->tuple([core.unit if i in dyn_argnums else _wrap_hashably(arg) for (i, arg) in enumerate(args)])
A:jax.api.dyn_args->tuple((args[i] for i in dyn_argnums))
A:jax.api.name->getattr(fun, '__name__', '<unnamed custom_transforms primitive>')
A:jax.api.fun_p->core.Primitive(name)
A:jax.api.(consts, args)->split_list(args, [params['num_consts']])
A:jax.api.(_, args_flat)->split_list(primals, [num_consts])
A:jax.api.(consts_dot, args_dot_flat)->split_list(tangents, [num_consts])
A:jax.api.args->tree_unflatten(params['in_tree'], args_flat)
A:jax.api.args_dot->tree_unflatten(in_tree, args_dot_flat)
A:jax.api.(out, out_dot)->custom_jvp(args, args_dot)
A:jax.api.(out_flat, out_tree)->tree_flatten(out)
A:jax.api.(out_dot_flat, out_tree2)->tree_flatten(out_dot)
A:jax.api.(consts, args_flat)->split_list(consts_and_args, [num_consts])
A:jax.api.(out, vjp)->custom_vjp(*args)
A:jax.api.msg->"Output of the `vjp`: {} doesn't match the structure of args of `fun`: {}\n{}\nvs\n{}\n".format(vjp, fun, in_tree2, in_tree)
A:jax.api.cts->tree_unflatten(out_tree, cts_flat)
A:jax.api.(args_cts_flat, in_tree2)->tree_flatten(vjp(cts))
A:jax.api.(ans, _)->fun(*args, **kwargs)
A:jax.api.primal_fun->custom_transforms(primal_fun)
A:jax.api.new_fun->custom_transforms(fun)
A:jax.api.(y, jacs)->vmap(pushfwd, out_axes=(None, 0))(_elementwise_std_basis(tangents))
A:jax.api.(flat_tangents, _)->tree_flatten(tangents)
A:jax.api.out_tangent->sum([t * jac for (t, jac) in zip(flat_tangents, jacs)])
A:jax.api.arity->len(leaves)
A:jax.api.dims->map(onp.size, leaves)
A:jax.api.basis_array->numpy.stack([onp.concatenate([onp.ones(dims[j], dtype) if i == j else onp.zeros(dims[j], dtype) for j in range(arity)]) for i in range(arity)])
A:jax.api.id_name->next(id_names)
A:jax.api.pvals->map(pv_like, jax_args)
A:jax.api.graphviz_maker.__name__->'make_graphviz({})'.format(graphviz_maker.__name__)
A:jax.api.self.dtype->numpy.dtype(dtype)
A:jax.api.size->property(lambda self: onp.prod(self.shape))
A:jax.api.(wrapped_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
jax.CustomTransformsFunction(self,fun,prim)
jax.CustomTransformsFunction.__repr__(self)
jax.ShapeDtypeStruct(self,shape,dtype)
jax.ShapeDtypeStruct.__eq__(self,other)
jax.ShapeDtypeStruct.__hash__(self)
jax.ShapeDtypeStruct.__len__(self)
jax.ShapeDtypeStruct.__repr__(self)
jax._TempAxisName(self,obj)
jax._TempAxisName.__eq__(self,other)
jax._TempAxisName.__hash__(self)
jax._TempAxisName.__repr__(self)
jax._ThreadLocalState(self)
jax._argnums_partial(f:lu.WrappedFun,dyn_argnums,args)
jax._argnums_partial_(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax._bind_shapes(shape_exprs,shapes)
jax._check_args(args)
jax._check_callable(fun)
jax._check_custom_transforms_type(name,fun)
jax._check_inexact_input_vjp(x)
jax._check_real_input_jacfwd(x)
jax._check_real_output_jacrev(x)
jax._check_scalar(x)
jax._device_get(x)
jax._dtype(x)
jax._elementwise_std_basis(pytree)
jax._flatten_axes(treedef,axis_tree)
jax._jvp(fun:lu.WrappedFun,primals,tangents)
jax._make_graphviz(fun)
jax._papply(fun)
jax._parallelize(fun)
jax._pmap_axis_size(xs)
jax._remap_ids(names,shape_spec)
jax._reshape_merge(x)
jax._reshape_split(num_chunks,x)
jax._shape_spec_consistent(spec,expr)
jax._split(x,indices,axis)
jax._std_basis(pytree)
jax._unravel_array_into_pytree(pytree,axis,arr)
jax._valid_jaxtype(arg)
jax._vjp(fun:lu.WrappedFun,*primals,**kwargs)
jax._vjp_pullback_wrapper(fun,cotangent_dtypes,io_tree,py_args)
jax._wrap_hashably(arg)
jax.api.CustomTransformsFunction(self,fun,prim)
jax.api.CustomTransformsFunction.__repr__(self)
jax.api.ShapeDtypeStruct(self,shape,dtype)
jax.api.ShapeDtypeStruct.__eq__(self,other)
jax.api.ShapeDtypeStruct.__hash__(self)
jax.api.ShapeDtypeStruct.__len__(self)
jax.api.ShapeDtypeStruct.__repr__(self)
jax.api._TempAxisName(self,obj)
jax.api._TempAxisName.__eq__(self,other)
jax.api._TempAxisName.__hash__(self)
jax.api._TempAxisName.__repr__(self)
jax.api._ThreadLocalState(self)
jax.api._argnums_partial(f:lu.WrappedFun,dyn_argnums,args)
jax.api._argnums_partial_(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax.api._bind_shapes(shape_exprs,shapes)
jax.api._check_args(args)
jax.api._check_callable(fun)
jax.api._check_custom_transforms_type(name,fun)
jax.api._check_inexact_input_vjp(x)
jax.api._check_real_input_jacfwd(x)
jax.api._check_real_output_jacrev(x)
jax.api._check_scalar(x)
jax.api._device_get(x)
jax.api._dtype(x)
jax.api._elementwise_std_basis(pytree)
jax.api._flatten_axes(treedef,axis_tree)
jax.api._jvp(fun:lu.WrappedFun,primals,tangents)
jax.api._make_graphviz(fun)
jax.api._papply(fun)
jax.api._parallelize(fun)
jax.api._pmap_axis_size(xs)
jax.api._remap_ids(names,shape_spec)
jax.api._reshape_merge(x)
jax.api._reshape_split(num_chunks,x)
jax.api._shape_spec_consistent(spec,expr)
jax.api._split(x,indices,axis)
jax.api._std_basis(pytree)
jax.api._unravel_array_into_pytree(pytree,axis,arr)
jax.api._valid_jaxtype(arg)
jax.api._vjp(fun:lu.WrappedFun,*primals,**kwargs)
jax.api._vjp_pullback_wrapper(fun,cotangent_dtypes,io_tree,py_args)
jax.api._wrap_hashably(arg)
jax.api.checkpoint(fun:Callable,concrete:bool=False)->Callable
jax.api.custom_gradient(fun)
jax.api.custom_transforms(fun)
jax.api.defjvp(fun,*jvprules)
jax.api.defjvp_all(fun,custom_jvp)
jax.api.defvjp(fun,*vjprules)
jax.api.defvjp_all(fun,custom_vjp)
jax.api.device_get(x)
jax.api.device_put(x,device=None)
jax.api.disable_jit()
jax.api.eval_shape(fun:Callable,*args,**kwargs)
jax.api.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.api.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.jarrett(fun)
jax.api.jit(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),device=None,backend:Optional[str]=None)->Callable
jax.api.jvp(fun:Callable,primals,tangents)->Tuple[Any, Any]
jax.api.lift_linearized(jaxpr,primal_avals,consts,io_tree,out_pvals,*py_args)
jax.api.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax.api.make_jaxpr(fun:Callable)->Callable[..., core.TypedJaxpr]
jax.api.mask(fun:Callable,in_shapes,out_shape)->Callable
jax.api.pmap(fun:Callable,axis_name:Optional[AxisName]=None,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices=None,backend:Optional[str]=None,axis_size:Optional[int]=None)->Callable
jax.api.shapecheck(in_shapes,out_shape,fun)
jax.api.soft_pmap(fun:Callable,axis_name:Optional[AxisName]=None,backend:Optional[str]=None)->Callable
jax.api.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.api.vjp(fun:Callable,*primals,**kwargs)->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax.api.vmap(fun:Callable,in_axes=0,out_axes=0)->Callable
jax.api.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:bool=True)->Callable
jax.checkpoint(fun:Callable,concrete:bool=False)->Callable
jax.custom_gradient(fun)
jax.custom_transforms(fun)
jax.defjvp(fun,*jvprules)
jax.defjvp_all(fun,custom_jvp)
jax.defvjp(fun,*vjprules)
jax.defvjp_all(fun,custom_vjp)
jax.device_get(x)
jax.device_put(x,device=None)
jax.disable_jit()
jax.eval_shape(fun:Callable,*args,**kwargs)
jax.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.jarrett(fun)
jax.jit(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),device=None,backend:Optional[str]=None)->Callable
jax.jvp(fun:Callable,primals,tangents)->Tuple[Any, Any]
jax.lift_linearized(jaxpr,primal_avals,consts,io_tree,out_pvals,*py_args)
jax.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax.make_jaxpr(fun:Callable)->Callable[..., core.TypedJaxpr]
jax.mask(fun:Callable,in_shapes,out_shape)->Callable
jax.pmap(fun:Callable,axis_name:Optional[AxisName]=None,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices=None,backend:Optional[str]=None,axis_size:Optional[int]=None)->Callable
jax.shapecheck(in_shapes,out_shape,fun)
jax.soft_pmap(fun:Callable,axis_name:Optional[AxisName]=None,backend:Optional[str]=None)->Callable
jax.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.vjp(fun:Callable,*primals,**kwargs)->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax.vmap(fun:Callable,in_axes=0,out_axes=0)->Callable
jax.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:bool=True)->Callable


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/linear_util.py----------------------------------------
A:jax.linear_util._EMPTY_STORE_VALUE->EmptyStoreValue()
A:jax.linear_util.gen->gen(*gen_static_args + tuple(args), **kwargs)
A:jax.linear_util.(args, kwargs)->next(gen)
A:jax.linear_util.ans->call(fun, *args)
A:jax.linear_util.(gen, out_store)->stack.pop()
A:jax.linear_util.transformation_stack->map(transform_to_str, enumerate(self.transforms))
A:jax.linear_util.out_store->Store()
A:jax.linear_util.fun_caches->weakref.WeakKeyDictionary()
A:jax.linear_util.cache->weakref.WeakKeyDictionary().setdefault(fun.f, {})
A:jax.linear_util.result->weakref.WeakKeyDictionary().setdefault(fun.f, {}).get(key, None)
jax.linear_util.EmptyStoreValue(object)
jax.linear_util.Store(self)
jax.linear_util.Store.__nonzero__(self)
jax.linear_util.Store.store(self,val)
jax.linear_util.Store.val(self)
jax.linear_util.StoreException(Exception)
jax.linear_util.WrappedFun(self,f,transforms,stores,params)
jax.linear_util.WrappedFun.__eq__(self,other)
jax.linear_util.WrappedFun.__hash__(self)
jax.linear_util.WrappedFun.__name__(self)
jax.linear_util.WrappedFun.__repr__(self)
jax.linear_util.WrappedFun.call_wrapped(self,*args,**kwargs)
jax.linear_util.WrappedFun.populate_stores(self,stores)
jax.linear_util.WrappedFun.wrap(self,gen,gen_static_args,out_store)->'WrappedFun'
jax.linear_util.cache(call)
jax.linear_util.fun_name(f)
jax.linear_util.hashable_partial(x,*args)
jax.linear_util.transformation(gen,fun:WrappedFun,*gen_static_args)->WrappedFun
jax.linear_util.transformation_with_aux(gen,fun:WrappedFun,*gen_static_args)->Tuple[WrappedFun, Any]
jax.linear_util.wrap_init(f,params={})->WrappedFun


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/random.py----------------------------------------
A:jax.random.k1->convert(lax.shift_right_logical(seed, lax._const(seed, 32)))
A:jax.random.k2->convert(np.bitwise_and(seed, 4294967295))
A:jax.random.nbits->numpy.array(np.iinfo(dtype).bits, dtype)
A:jax.random.d->lax.sub(alpha, one_over_three)
A:jax.random.shape->numpy.shape(df)
A:jax.random.aval->jax.abstract_arrays.UnshapedArray(np.dtype(np.uint32))
A:jax.random.rotate_left->_make_rotate_left(onp.uint32)
A:jax.random.v[1]->rotate_left(v[1], rot)
A:jax.random.x->normal(subkey, (), dtype=dtype)
A:jax.random.(x, _, _)->lax.fori_loop(0, 5, step, (x, rotate_list(ks), rotations))
A:jax.random.rank->len(shape)
A:jax.random.ndims->lax.div(one_over_three, lax.pow(d, one_over_two)).GetShape(x).rank()
A:jax.random.threefry2x32_p->jax.core.Primitive('threefry2x32')
A:jax.random.xla.translations[threefry2x32_p]->jax.interpreters.xla.lower_fun(partial(_threefry2x32_lowering, use_rolled_loops=False))
A:jax.random.xla.backend_specific_translations['cpu'][threefry2x32_p]->jax.interpreters.xla.lower_fun(partial(_threefry2x32_lowering, use_rolled_loops=True))
A:jax.random.out->np.concatenate(x)
A:jax.random.counts->lax.tie_in(key, lax.iota(onp.uint32, max_count))
A:jax.random.key2->lax.tie_in(key, PRNGKey(data))
A:jax.random.bits->_random_bits(key, nbits, shape)
A:jax.random.shape_->lax.broadcast_shapes(shape, *param_shapes)
A:jax.random.dtype->dtypes.canonicalize_dtype(dtype)
A:jax.random.minval->lax.convert_element_type(minval, dtype)
A:jax.random.maxval->lax.max(lax.add(minval, onp.array(1, dtype)), maxval)
A:jax.random.finfo->np.finfo(dtype)
A:jax.random.float_bits->lax.bitwise_or(lax.shift_right_logical(bits, onp.array(nbits - nmant, lax.dtype(bits))), onp.array(1.0, dtype).view(onp.uint32 if nbits == 32 else onp.uint64))
A:jax.random.(k1, k2)->split(key)
A:jax.random.span->lax.convert_element_type(maxval - minval, unsigned_dtype)
A:jax.random.multiplier->lax.rem(lax.mul(multiplier, multiplier), span)
A:jax.random.random_offset->lax.rem(random_offset, span)
A:jax.random.num_rounds->int(onp.ceil(exponent * onp.log(x.size) / onp.log(uint32max)))
A:jax.random.(key, subkey)->split(key)
A:jax.random.sort_keys->_random_bits(subkey, 32, x.shape)
A:jax.random.(_, x)->lax.sort_key_val(sort_keys, x, axis)
A:jax.random.lo->numpy.nextafter(onp.array(-1.0, dtype), 0.0, dtype=dtype)
A:jax.random.hi->numpy.array(1.0, dtype)
A:jax.random.u->uniform(key, shape, dtype, minval=-1.0 + np.finfo(dtype).epsneg, maxval=1.0)
A:jax.random.chol_factor->cholesky(cov)
A:jax.random.normal_samples->normal(key, shape + mean.shape[-1:], dtype)
A:jax.random.sqrt2->numpy.array(onp.sqrt(2), dtype)
A:jax.random.a->np.broadcast_to(a, shape)
A:jax.random.b->lax.convert_element_type(b, dtype)
A:jax.random.p->lax.convert_element_type(p, dtype)
A:jax.random.(key_a, key_b)->split(key)
A:jax.random.gamma_a->gamma(key_a, a, shape, dtype)
A:jax.random.gamma_b->gamma(key_b, b, shape, dtype)
A:jax.random.pi->_constant_like(u, onp.pi)
A:jax.random.alpha->lax.select(lax.ge(alpha, one), alpha, lax.add(alpha, one))
A:jax.random.gamma_samples->gamma(key, alpha, shape + onp.shape(alpha)[-1:], dtype)
A:jax.random.zero->_constant_like(alpha, 0)
A:jax.random.one->_constant_like(alpha, 1)
A:jax.random.minus_one->_constant_like(alpha, -1)
A:jax.random.one_over_two->_constant_like(alpha, 0.5)
A:jax.random.one_over_three->_constant_like(alpha, 1.0 / 3.0)
A:jax.random.squeeze_const->_constant_like(alpha, 0.0331)
A:jax.random.boost->lax.select(lax.ge(alpha, one), one, lax.pow(uniform(subkey, (), dtype=dtype), lax.div(one, alpha)))
A:jax.random.c->lax.div(one_over_three, lax.pow(d, one_over_two))
A:jax.random.cond->lax.bitwise_and(lax.ge(U, lax.sub(one, lax.mul(squeeze_const, lax.mul(X, X)))), lax.ge(lax.log(U), lax.add(lax.mul(X, one_over_two), lax.mul(d, lax.add(lax.sub(one, V), lax.log(V))))))
A:jax.random.v->np.log(alpha)
A:jax.random.(key, x_key, U_key)->split(key, 3)
A:jax.random.(_, x, v)->lax.while_loop(lambda kxv: lax.le(kxv[2], zero), _next_kxv, (x_key, zero, minus_one))
A:jax.random.X->lax.mul(x, x)
A:jax.random.V->lax.mul(lax.mul(v, v), v)
A:jax.random.U->uniform(U_key, (), dtype=dtype)
A:jax.random.(_, _, V, _)->lax.while_loop(_cond_fn, _body_fn, (key, zero, one, _constant_like(alpha, 2)))
A:jax.random.z->lax.mul(lax.mul(d, V), boost)
A:jax.random.sqrt_8a->np.sqrt(8 * alpha)
A:jax.random.log_z_div_a->np.log(z / alpha)
A:jax.random.sign->np.where(z < alpha, lax._const(z, 1.0), lax._const(z, -1.0))
A:jax.random.z_div_a->np.divide(z, alpha)
A:jax.random.grad->np.exp(p / np.maximum(q, 0.01))
A:jax.random.(_, _, grad, flag)->lax.while_loop(lambda zagf: ~zagf[3], _case4, (z, alpha, grad, flag))
A:jax.random.samples->vmap(_gamma_one)(keys, alphas)
A:jax.random.alphas->np.reshape(a, -1)
A:jax.random.grads->vmap(_gamma_grad_one)(samples, alphas)
A:jax.random.a_shape->np.shape(a)
A:jax.random.key->vmap(split, in_axes=(0, None))(key, prod(a_shape[key_ndim:]))
A:jax.random.keys->np.reshape(key, (-1, 2))
A:jax.random.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax.random.k->jax.interpreters.batching.bdim_at_front(k, bk, size)
A:jax.random.random_gamma_p->jax.core.Primitive('random_gamma')
A:jax.random.xla.translations[random_gamma_p]->jax.interpreters.xla.lower_fun(_gamma_impl)
A:jax.random.batch_shape->tuple(onp.delete(logits.shape, axis))
A:jax.random.e->exponential(key, shape, dtype)
A:jax.random.df->lax.convert_element_type(df, dtype)
A:jax.random.(key_n, key_g)->split(key)
A:jax.random.n->normal(key_n, shape, dtype)
A:jax.random.two->_constant_like(n, 2)
A:jax.random.half_df->lax.div(df, two)
A:jax.random.g->gamma(key_n, half_df, shape, dtype)
jax.random.PRNGKey(seed)
jax.random._bernoulli(key,p,shape)
jax.random._beta(key,a,b,shape,dtype)
jax.random._bit_stats(bits)
jax.random._cauchy(key,shape,dtype)
jax.random._check_shape(name,shape,*param_shapes)
jax.random._dirichlet(key,alpha,shape,dtype)
jax.random._exponential(key,shape,dtype)
jax.random._fold_in(key,data)
jax.random._gamma(key,a,shape,dtype)
jax.random._gamma_batching_rule(batched_args,batch_dims)
jax.random._gamma_grad(sample,a)
jax.random._gamma_grad_one(z,alpha)
jax.random._gamma_impl(key,a)
jax.random._gamma_one(key,alpha)
jax.random._gumbel(key,shape,dtype)
jax.random._is_prng_key(key)
jax.random._laplace(key,shape,dtype)
jax.random._logistic(key,shape,dtype)
jax.random._make_rotate_left(dtype)
jax.random._multivariate_normal(key,mean,cov,shape,dtype)
jax.random._normal(key,shape,dtype)
jax.random._pareto(key,b,shape,dtype)
jax.random._randint(key,shape,minval,maxval,dtype)
jax.random._random_bits(key,bit_width,shape)
jax.random._shuffle(key,x,axis)
jax.random._split(key,num)
jax.random._t(key,df,shape,dtype)
jax.random._threefry2x32_abstract_eval(*args)
jax.random._threefry2x32_gpu_translation_rule(c,k1,k2,x1,x2)
jax.random._threefry2x32_lowering(key1,key2,x1,x2,use_rolled_loops=True)
jax.random._truncated_normal(key,lower,upper,shape,dtype)
jax.random._uniform(key,shape,dtype,minval,maxval)
jax.random.bernoulli(key,p=onp.float32(0.5),shape=None)
jax.random.beta(key,a,b,shape=None,dtype=onp.float64)
jax.random.categorical(key,logits,axis=-1,shape=None)
jax.random.cauchy(key,shape=(),dtype=onp.float64)
jax.random.dirichlet(key,alpha,shape=None,dtype=onp.float64)
jax.random.exponential(key,shape=(),dtype=onp.float64)
jax.random.fold_in(key,data)
jax.random.gamma(key,a,shape=None,dtype=onp.float64)
jax.random.gumbel(key,shape=(),dtype=onp.float64)
jax.random.laplace(key,shape=(),dtype=onp.float64)
jax.random.logistic(key,shape=(),dtype=onp.float64)
jax.random.multivariate_normal(key,mean,cov,shape=None,dtype=onp.float64)
jax.random.normal(key,shape=(),dtype=onp.float64)
jax.random.pareto(key,b,shape=None,dtype=onp.float64)
jax.random.randint(key,shape,minval,maxval,dtype=onp.int64)
jax.random.shuffle(key,x,axis=0)
jax.random.split(key,num=2)
jax.random.t(key,df,shape=(),dtype=onp.float64)
jax.random.threefry_2x32(keypair,count)
jax.random.truncated_normal(key,lower,upper,shape=None,dtype=onp.float64)
jax.random.uniform(key,shape=(),dtype=onp.float64,minval=0.0,maxval=1.0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/tree_util.py----------------------------------------
A:jax.tree_util._registry[nodetype]->_RegistryEntry(flatten_func, unflatten_func)
A:jax.tree_util.(leaves, treedef)->lib.pytree.flatten(tree)
A:jax.tree_util.(flat, treedef)->tree_flatten(pytree_to_transpose)
A:jax.tree_util.expected_treedef->outer_treedef.compose(inner_treedef)
A:jax.tree_util.flat->iter(flat)
A:jax.tree_util.transposed_lol->zip(*lol)
A:jax.tree_util.subtrees->map(partial(tree_unflatten, outer_treedef), transposed_lol)
A:jax.tree_util._RegistryEntry->collections.namedtuple('RegistryEntry', ['to_iter', 'from_iter'])
A:jax.tree_util.handler->_registry.get(type(tree))
A:jax.tree_util.(children, metadata)->_registry.get(type(tree)).to_iter(tree)
A:jax.tree_util.children->iter(tree)
jax.tree_util.Partial(functools.partial)
jax.tree_util._process_pytree(process_node,tree)
jax.tree_util._replace_nones(sentinel,tree)
jax.tree_util.build_tree(treedef,xs)
jax.tree_util.register_pytree_node(nodetype,flatten_func,unflatten_func)
jax.tree_util.register_pytree_node_class(cls)
jax.tree_util.tree_all(tree)
jax.tree_util.tree_flatten(tree)
jax.tree_util.tree_leaves(tree)
jax.tree_util.tree_map(f,tree)
jax.tree_util.tree_multimap(f,tree,*rest)
jax.tree_util.tree_reduce(f,tree)
jax.tree_util.tree_structure(tree)
jax.tree_util.tree_transpose(outer_treedef,inner_treedef,pytree_to_transpose)
jax.tree_util.tree_unflatten(treedef,leaves)
jax.tree_util.treedef_children(treedef)
jax.tree_util.treedef_is_leaf(treedef)
jax.tree_util.treedef_tuple(treedefs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax_linalg.py----------------------------------------
A:jax.lax_linalg.x->jax.interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax_linalg.(w, vl, vr)->Primitive('eig').bind(x)
A:jax.lax_linalg.(v, w)->Primitive('eigh').bind(symmetrize(a), lower=lower)
A:jax.lax_linalg.(lu, pivots)->Primitive('lu').bind(a)
A:jax.lax_linalg.(q, r)->Primitive('qr').bind(x, full_matrices=False)
A:jax.lax_linalg.(s, u, v)->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.b->min(r - k, block_size)
A:jax.lax_linalg.out->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve').bind(a, b, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.lax_linalg.t->f(c, *args, **kwargs)
A:jax.lax_linalg.L->jax.numpy.lax_numpy.tril(cholesky_p.bind(x))
A:jax.lax_linalg.l->jax.lax.pad(np.tril(lu[..., :, :k], -1), zero, l_padding)
A:jax.lax_linalg.tmp->triangular_solve(L, sigma_dot, left_side=False, transpose_a=True, conjugate_a=True, lower=True)
A:jax.lax_linalg.L_dot->jax.lax.batch_matmul(L, phi(triangular_solve(L, tmp, left_side=True, transpose_a=False, lower=True)), precision=lax.Precision.HIGHEST)
A:jax.lax_linalg.cholesky_p->standard_unop(_float | _complex, 'cholesky')
A:jax.lax_linalg.shape->c.GetShape(operand)
A:jax.lax_linalg.dtype->jax.lax.dtype(a)
A:jax.lax_linalg.nan->c.Constant(onp.array(onp.nan, dtype=dtype))
A:jax.lax_linalg.(result, info)->potrf_impl(c, operand, lower=True)
A:jax.lax_linalg.ok->c.Eq(info, c.ConstantS32Scalar(0))
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][cholesky_p]->partial(_cholesky_cpu_gpu_translation_rule, lapack.potrf)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][cholesky_p]->partial(_cholesky_cpu_gpu_translation_rule, cusolver.potrf)
A:jax.lax_linalg.vlvr->ShapedArray(batch_dims + (n, n), dtype)
A:jax.lax_linalg.w->w.astype(a.dtype).astype(a.dtype)
A:jax.lax_linalg.(w, vl, vr, info)->_cpu_geev(c, operand)
A:jax.lax_linalg.vl->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vl, _nan_like(c, vl))
A:jax.lax_linalg.vr->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vr, _nan_like(c, vr))
A:jax.lax_linalg.eig_p->Primitive('eig')
A:jax.lax_linalg.dims->c.GetShape(operand).dimensions()
A:jax.lax_linalg.n->len(dims)
A:jax.lax_linalg.operand->c.Transpose(operand, list(range(n - 2)) + [n - 1, n - 2])
A:jax.lax_linalg.v->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), v, _nan_like(c, v))
A:jax.lax_linalg.(v, w, info)->syevd_impl(c, operand, lower=lower)
A:jax.lax_linalg.eye_n->jax.numpy.lax_numpy.eye(a.shape[-1], dtype=a.dtype)
A:jax.lax_linalg.dot->partial(lax.dot if g_a.ndim == 2 else lax.batch_matmul, precision=lax.Precision.HIGHEST)
A:jax.lax_linalg.vdag_adot_v->dot(dot(_H(v), a_dot), v)
A:jax.lax_linalg.dv->dot(v, np.multiply(Fmat, vdag_adot_v))
A:jax.lax_linalg.dw->jax.numpy.lax_numpy.diagonal(vdag_adot_v, axis1=-2, axis2=-1)
A:jax.lax_linalg.eigh_p->Primitive('eigh')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, _cpu_syevd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, cusolver.syevd)
A:jax.lax_linalg.triangular_solve_dtype_rule->partial(naryop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex), 'triangular_solve')
A:jax.lax_linalg.g_a->jax.lax.neg(g_a)
A:jax.lax_linalg.cotangent_b->triangular_solve(a, cotangent, left_side, lower, not transpose_a, conjugate_a, unit_diagonal)
A:jax.lax_linalg.y->jax.interpreters.batching.bdim_at_front(y, by, size)
A:jax.lax_linalg.y_flat->jax.interpreters.batching.bdim_at_front(y, by, size).reshape(y.shape[:-3] + (y.shape[-3] * y.shape[-2], y.shape[-1]))
A:jax.lax_linalg.out_flat->triangular_solve(x, y_flat, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.lax_linalg.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax.lax_linalg.triangular_solve_p->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve')
A:jax.lax_linalg.a->jax.ops.index_add(a, ops.index[k + b:, k + b:], -lax.dot(a[k + b:, k:k + b], a[k:k + b, k + b:], precision=lax.Precision.HIGHEST))
A:jax.lax_linalg.batch->prod(dims[:-2])
A:jax.lax_linalg.m_idx->jax.numpy.lax_numpy.arange(m)
A:jax.lax_linalg.n_idx->jax.numpy.lax_numpy.arange(n)
A:jax.lax_linalg.magnitude->jax.numpy.lax_numpy.abs(a[:, k])
A:jax.lax_linalg.i->jax.numpy.lax_numpy.argmax(np.where(m_idx >= k, magnitude, -np.inf))
A:jax.lax_linalg.pivot->c.Sub(pivot, c.ConstantS32Scalar(1))
A:jax.lax_linalg.perm->jax.numpy.lax_numpy.arange(m, dtype=np.int32)
A:jax.lax_linalg.r->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), r, _nan_like(c, r))
A:jax.lax_linalg.(block_pivot, perm, lu_block)->_lu_unblocked(a[k:, k:k + b])
A:jax.lax_linalg.batch_size->numpy.prod(batch_dims, dtype=onp.int64)
A:jax.lax_linalg.(pivot, lu)->_lu_blocked(x)
A:jax.lax_linalg.lu->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), lu, _nan_like(c, lu))
A:jax.lax_linalg.(lu, pivot)->jax.interpreters.xla.apply_primitive(lu_p, operand)
A:jax.lax_linalg.a_shape->jax.numpy.lax_numpy.shape(a)
A:jax.lax_linalg.k->min(m, n)
A:jax.lax_linalg.permutation->lu_pivots_to_permutation(pivots, m)
A:jax.lax_linalg.iotas->jax.numpy.lax_numpy.ix_(*(lax.iota(np.int32, b) for b in batch_dims))
A:jax.lax_linalg.ndims->len(a_shape)
A:jax.lax_linalg.zero->jax.numpy.lax_numpy._constant_like(lu, 0)
A:jax.lax_linalg.u_eye->jax.lax.pad(np.eye(n - k, n - k, dtype=dtype), zero, ((k, 0, 0), (k, 0, 0)))
A:jax.lax_linalg.la->triangular_solve(l, x, left_side=True, transpose_a=False, lower=True, unit_diagonal=True)
A:jax.lax_linalg.lau->triangular_solve(u, la, left_side=False, transpose_a=False, lower=False)
A:jax.lax_linalg.l_dot->jax.numpy.lax_numpy.matmul(l, np.tril(lau, -1))
A:jax.lax_linalg.u_dot->jax.numpy.lax_numpy.matmul(np.triu(lau), u)
A:jax.lax_linalg.(lu, pivot, info)->getrf_impl(c, operand)
A:jax.lax_linalg.lu_p->Primitive('lu')
A:jax.lax_linalg.xla.translations[lu_p]->jax.interpreters.xla.lower_fun(_lu_python)
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, lapack.getrf)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, cusolver.getrf)
A:jax.lax_linalg.(result, _)->jax.lax.fori_loop(onp.array(0, onp.int32), onp.array(k, onp.int32), _lu_pivots_body_fn, (permutation, swaps))
A:jax.lax_linalg.q->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), q, _nan_like(c, q))
A:jax.lax_linalg.dx_rinv->triangular_solve(r, dx)
A:jax.lax_linalg.qt_dx_rinv->jax.numpy.lax_numpy.matmul(_H(q), dx_rinv)
A:jax.lax_linalg.qt_dx_rinv_lower->jax.numpy.lax_numpy.tril(qt_dx_rinv, -1)
A:jax.lax_linalg.dr->jax.numpy.lax_numpy.matmul(qt_dx_rinv - domega, r)
A:jax.lax_linalg.(r, tau, info_geqrf)->geqrf_impl(c, operand)
A:jax.lax_linalg.(q, info_orgqr)->orgqr_impl(c, q, tau)
A:jax.lax_linalg.qr_p->Primitive('qr')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, lapack.geqrf, lapack.orgqr)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, cusolver.geqrf, cusolver.orgqr)
A:jax.lax_linalg.(s, u, vt)->jax.interpreters.xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.s->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1,)), s, _nan_like(c, s))
A:jax.lax_linalg.u->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), u, _nan_like(c, u))
A:jax.lax_linalg.vt->_broadcasting_select(c, c.Reshape(ok, None, batch_dims + (1, 1)), vt, _nan_like(c, vt))
A:jax.lax_linalg.(s, U, Vt)->Primitive('svd').bind(A, full_matrices=False, compute_uv=True)
A:jax.lax_linalg.dS->jax.numpy.lax_numpy.matmul(np.matmul(Ut, dA), V)
A:jax.lax_linalg.ds->jax.numpy.lax_numpy.real(np.diagonal(dS, 0, -2, -1))
A:jax.lax_linalg.dU->jax.numpy.lax_numpy.matmul(U, F * (dSS + _T(dSS)))
A:jax.lax_linalg.dV->jax.numpy.lax_numpy.matmul(V, F * (SdS + _T(SdS)))
A:jax.lax_linalg.(s, u, vt, info)->gesvd_impl(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.outs->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.svd_p->Primitive('svd')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, lapack.gesdd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, cusolver.gesvd)
jax.lax_linalg._H(x)
jax.lax_linalg._T(x)
jax.lax_linalg._cholesky_cpu_gpu_translation_rule(potrf_impl,c,operand)
jax.lax_linalg._eigh_cpu_gpu_translation_rule(syevd_impl,c,operand,lower)
jax.lax_linalg._lu_abstract_eval(operand)
jax.lax_linalg._lu_batching_rule(batched_args,batch_dims)
jax.lax_linalg._lu_blocked(a,block_size=32)
jax.lax_linalg._lu_cpu_gpu_translation_rule(getrf_impl,c,operand)
jax.lax_linalg._lu_impl(operand)
jax.lax_linalg._lu_jvp_rule(primals,tangents)
jax.lax_linalg._lu_pivots_body_fn(i,permutation_and_swaps)
jax.lax_linalg._lu_python(x)
jax.lax_linalg._lu_solve(lu,pivots,b,trans)
jax.lax_linalg._lu_solve_core(lu,pivots,b,trans)
jax.lax_linalg._lu_unblocked(a)
jax.lax_linalg._nan_like(c,operand)
jax.lax_linalg._qr_cpu_gpu_translation_rule(geqrf_impl,orgqr_impl,c,operand,full_matrices)
jax.lax_linalg._svd_cpu_gpu_translation_rule(gesvd_impl,c,operand,full_matrices,compute_uv)
jax.lax_linalg._triangular_solve_cpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._triangular_solve_gpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._unpack_tuple(f,n)
jax.lax_linalg.cholesky(x,symmetrize_input=True)
jax.lax_linalg.cholesky_batching_rule(batched_args,batch_dims)
jax.lax_linalg.cholesky_jvp_rule(primals,tangents)
jax.lax_linalg.eig(x)
jax.lax_linalg.eig_abstract_eval(operand)
jax.lax_linalg.eig_batching_rule(batched_args,batch_dims)
jax.lax_linalg.eig_cpu_translation_rule(c,operand)
jax.lax_linalg.eig_impl(operand)
jax.lax_linalg.eig_translation_rule(c,operand)
jax.lax_linalg.eigh(x,lower=True,symmetrize_input=True)
jax.lax_linalg.eigh_abstract_eval(operand,lower)
jax.lax_linalg.eigh_batching_rule(batched_args,batch_dims,lower)
jax.lax_linalg.eigh_impl(operand,lower)
jax.lax_linalg.eigh_jvp_rule(primals,tangents,lower)
jax.lax_linalg.eigh_translation_rule(c,operand,lower)
jax.lax_linalg.lu(x)
jax.lax_linalg.lu_pivots_to_permutation(swaps,m)
jax.lax_linalg.lu_solve(lu,pivots,b,trans=0)
jax.lax_linalg.qr(x,full_matrices=True)
jax.lax_linalg.qr_abstract_eval(operand,full_matrices)
jax.lax_linalg.qr_batching_rule(batched_args,batch_dims,full_matrices)
jax.lax_linalg.qr_impl(operand,full_matrices)
jax.lax_linalg.qr_jvp_rule(primals,tangents,full_matrices)
jax.lax_linalg.qr_translation_rule(c,operand,full_matrices)
jax.lax_linalg.svd(x,full_matrices=True,compute_uv=True)
jax.lax_linalg.svd_abstract_eval(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_batching_rule(batched_args,batch_dims,full_matrices,compute_uv)
jax.lax_linalg.svd_impl(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_jvp_rule(primals,tangents,full_matrices,compute_uv)
jax.lax_linalg.svd_translation_rule(c,operand,full_matrices,compute_uv)
jax.lax_linalg.symmetrize(x)
jax.lax_linalg.triangular_solve(a,b,left_side=False,lower=False,transpose_a=False,conjugate_a=False,unit_diagonal=False)
jax.lax_linalg.triangular_solve_batching_rule(batched_args,batch_dims,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_jvp_rule_a(g_a,ans,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_shape_rule(a,b,left_side=False,**unused_kwargs)
jax.lax_linalg.triangular_solve_transpose_rule(cotangent,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/api_util.py----------------------------------------
A:jax.api_util.fun.__name__->namestr.format(fun=get_name(wrapped))
A:jax.api_util.fun.__module__->get_module(wrapped)
A:jax.api_util.fun.__doc__->docstr.format(fun=get_name(wrapped), doc=get_doc(wrapped), **kwargs)
A:jax.api_util.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(args, in_tree)->tree_flatten(py_args)
A:jax.api_util.ans->fun(*args)
A:jax.api_util.py_args->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax.api_util.(aux_flat, aux_tree)->tree_flatten(aux)
jax.api_util.apply_flat_fun(fun,io_tree,*py_args)
jax.api_util.apply_flat_fun_nokwargs(fun,io_tree,py_args)
jax.api_util.flatten_fun(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs2(in_tree,*args_flat)
jax.api_util.get_doc(fun)
jax.api_util.get_module(fun)
jax.api_util.get_name(fun)
jax.api_util.wraps(wrapped,fun,namestr='{fun}',docstr='{doc}',**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/pprint_util.py----------------------------------------
A:jax.pprint_util.indented_block->rhs.indent(indent + len(s))
A:jax.pprint_util.kv_pairs->vcat([pp('{}='.format(k)) >> pp(v) for (k, v) in kv_pairs])
jax.pprint_util.PrettyPrint(self,lines)
jax.pprint_util.PrettyPrint.__add__(self,rhs)
jax.pprint_util.PrettyPrint.__rshift__(self,rhs)
jax.pprint_util.PrettyPrint.__str__(self)
jax.pprint_util.PrettyPrint.indent(self,indent)
jax.pprint_util.hcat(ps)
jax.pprint_util.pp(s)
jax.pprint_util.pp_kv_pairs(kv_pairs)
jax.pprint_util.print_list(xs)
jax.pprint_util.vcat(ps)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/ndimage.py----------------------------------------
A:jax.scipy.ndimage._nonempty_prod->functools.partial(functools.reduce, operator.mul)
A:jax.scipy.ndimage._nonempty_sum->functools.partial(functools.reduce, operator.add)
A:jax.scipy.ndimage.index->numpy.lax_numpy.around(coordinate).astype(np.int32)
A:jax.scipy.ndimage.weight->coordinate.dtype.type(1)
A:jax.scipy.ndimage.lower->numpy.lax_numpy.floor(coordinate)
A:jax.scipy.ndimage.upper->numpy.lax_numpy.ceil(coordinate)
A:jax.scipy.ndimage.l_index->numpy.lax_numpy.floor(coordinate).astype(np.int32)
A:jax.scipy.ndimage.u_index->numpy.lax_numpy.ceil(coordinate).astype(np.int32)
A:jax.scipy.ndimage.one->coordinate.dtype.type(1)
A:jax.scipy.ndimage.input->numpy.lax_numpy.asarray(input)
A:jax.scipy.ndimage.cval->numpy.lax_numpy.asarray(cval, input.dtype)
A:jax.scipy.ndimage.index_fixer->_INDEX_FIXERS.get(mode)
A:jax.scipy.ndimage.interp_nodes->interp_fun(coordinate)
A:jax.scipy.ndimage.fixed_index->index_fixer(index, size)
A:jax.scipy.ndimage.valid->is_valid(index, size)
A:jax.scipy.ndimage.(indices, validities, weights)->zip(*items)
A:jax.scipy.ndimage.all_valid->functools.reduce(operator.and_, validities)
A:jax.scipy.ndimage.contribution->numpy.lax_numpy.where(all_valid, input[indices], cval)
A:jax.scipy.ndimage.result->_nonempty_sum(outputs)
jax.scipy.ndimage._linear_indices_and_weights(coordinate)
jax.scipy.ndimage._map_coordinates(input,coordinates,order,mode,cval)
jax.scipy.ndimage._nearest_indices_and_weights(coordinate)
jax.scipy.ndimage.map_coordinates(input,coordinates,order,mode='constant',cval=0.0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/linalg.py----------------------------------------
A:jax.scipy.linalg.a->lax.pad(a, dtype.type(0), ((0, 0, 0), (acc.shape[-1], 0, 0)))
A:jax.scipy.linalg.l->lax_linalg.cholesky(a if lower else np.conj(_T(a)), symmetrize_input=False)
A:jax.scipy.linalg.(c, b)->numpy.linalg._promote_arg_dtypes(np.asarray(c), np.asarray(b))
A:jax.scipy.linalg.b->lax_linalg.triangular_solve(c, b, left_side=True, lower=lower, transpose_a=lower, conjugate_a=lower)
A:jax.scipy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower)
A:jax.scipy.linalg.(lu, pivots)->lax_linalg.lu(a)
A:jax.scipy.linalg.dtype->lax.dtype(acc)
A:jax.scipy.linalg.(m, n)->numpy.lax_numpy.shape(a)
A:jax.scipy.linalg.permutation->lax_linalg.lu_pivots_to_permutation(pivots, m)
A:jax.scipy.linalg.p->numpy.lax_numpy.real(np.array(permutation == np.arange(m)[:, None], dtype=dtype))
A:jax.scipy.linalg.k->min(m, n)
A:jax.scipy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.scipy.linalg.(a, b)->numpy.linalg._promote_arg_dtypes(np.asarray(a), np.asarray(b))
A:jax.scipy.linalg.factors->lax.stop_gradient(cho_factor)(a, lower=lower)
A:jax.scipy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: np_linalg._matvec_multiply(a, x), solve=lambda _, x: cho_solve(factors, x), symmetric=True)
A:jax.scipy.linalg.out->lax_linalg.triangular_solve(a, b, left_side=True, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.scipy.linalg.(P, Q, n_squarings)->_calc_P_Q(A)
A:jax.scipy.linalg.R->lax.fori_loop(lower[0], n_squarings, my_body_fun, R)
A:jax.scipy.linalg.A->numpy.lax_numpy.asarray(A)
A:jax.scipy.linalg.A_L1->numpy.linalg.norm(A, 1)
A:jax.scipy.linalg.(U3, V3)->_pade3(A)
A:jax.scipy.linalg.(U5, V5)->_pade5(A)
A:jax.scipy.linalg.(U7, V7)->_pade7(A)
A:jax.scipy.linalg.(U9, V9)->_pade9(A)
A:jax.scipy.linalg.n_squarings->numpy.lax_numpy.maximum(0, np.floor(np.log2(A_L1 / maxnorm)))
A:jax.scipy.linalg.(U13, V13)->_pade13(A)
A:jax.scipy.linalg.conds->numpy.lax_numpy.array([0.4258730016922831, 1.880152677804762])
A:jax.scipy.linalg.U->numpy.lax_numpy.dot(A, np.dot(A6, b[13] * A6 + b[11] * A4 + b[9] * A2) + b[7] * A6 + b[5] * A4 + b[3] * A2 + b[1] * ident)
A:jax.scipy.linalg.V->numpy.lax_numpy.select(maxnorm < conds, (V3, V5), V7)
A:jax.scipy.linalg.lower->numpy.lax_numpy.zeros(1, dtype=n_squarings.dtype)
A:jax.scipy.linalg.ident->numpy.lax_numpy.eye(*A.shape, dtype=A.dtype)
A:jax.scipy.linalg.A2->numpy.lax_numpy.dot(A, A)
A:jax.scipy.linalg.A4->numpy.lax_numpy.dot(A2, A2)
A:jax.scipy.linalg.A6->numpy.lax_numpy.dot(A4, A2)
A:jax.scipy.linalg.A8->numpy.lax_numpy.dot(A6, A2)
A:jax.scipy.linalg.arrs->numpy.lax_numpy._promote_dtypes(*arrs)
A:jax.scipy.linalg.acc->lax.concatenate([acc, a], dimension=0)
jax.scipy.linalg._calc_P_Q(A)
jax.scipy.linalg._cho_solve(c,b,lower)
jax.scipy.linalg._cholesky(a,lower)
jax.scipy.linalg._expm(A,upper_triangular=False)
jax.scipy.linalg._lu(a,permute_l)
jax.scipy.linalg._pade13(A)
jax.scipy.linalg._pade3(A)
jax.scipy.linalg._pade5(A)
jax.scipy.linalg._pade7(A)
jax.scipy.linalg._pade9(A)
jax.scipy.linalg._qr(a,mode,pivoting)
jax.scipy.linalg._solve(a,b,sym_pos,lower)
jax.scipy.linalg._solve_P_Q(P,Q,upper_triangular=False)
jax.scipy.linalg._solve_triangular(a,b,trans,lower,unit_diagonal)
jax.scipy.linalg._squaring(R,n_squarings)
jax.scipy.linalg.block_diag(*arrs)
jax.scipy.linalg.cho_factor(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.cho_solve(c_and_lower,b,overwrite_b=False,check_finite=True)
jax.scipy.linalg.cholesky(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.det(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.eigh(a,b=None,lower=True,eigvals_only=False,overwrite_a=False,overwrite_b=False,turbo=True,eigvals=None,type=1,check_finite=True)
jax.scipy.linalg.expm(A,*,upper_triangular=False)
jax.scipy.linalg.inv(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu(a,permute_l=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_factor(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_solve(lu_and_piv,b,trans=0,overwrite_b=False,check_finite=True)
jax.scipy.linalg.qr(a,overwrite_a=False,lwork=None,mode='full',pivoting=False,check_finite=True)
jax.scipy.linalg.solve(a,b,sym_pos=False,lower=False,overwrite_a=False,overwrite_b=False,debug=False,check_finite=True)
jax.scipy.linalg.solve_triangular(a,b,trans=0,lower=False,unit_diagonal=False,overwrite_b=False,debug=None,check_finite=True)
jax.scipy.linalg.svd(a,full_matrices=True,compute_uv=True,overwrite_a=False,check_finite=True,lapack_driver='gesdd')
jax.scipy.linalg.tril(m,k=0)
jax.scipy.linalg.triu(m,k=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/special.py----------------------------------------
A:jax.scipy.special.(x,)->_promote_args_inexact('entr', x)
A:jax.scipy.special.(x, y)->_promote_args_inexact('xlog1py', x, y)
A:jax.scipy.special.(a, b, x)->_promote_args_inexact('betainc', a, b, x)
A:jax.scipy.special.(a, x)->_promote_args_inexact('gammaincc', a, x)
A:jax.scipy.special.x->numpy.lax_numpy.asarray(x)
A:jax.scipy.special.one->lax._const(x, 1)
A:jax.scipy.special.dims->_reduction_dims(a, axis)
A:jax.scipy.special.shape->numpy.lax_numpy.shape(p)
A:jax.scipy.special.amax->lax.select(lax.is_finite(amax), amax, lax.full_like(amax, 0))
A:jax.scipy.special.amax_singletons->dimadd(amax)
A:jax.scipy.special.out->lax.add(lax.log(lax.reduce(lax.exp(lax.sub(a, amax_singletons)), _constant_like(a, 0), lax.add, dims)), amax)
A:jax.scipy.special.safe_x->numpy.lax_numpy.where(x_ok, x, 1.0)
A:jax.scipy.special.safe_y->numpy.lax_numpy.where(x_ok, y, 1.0)
A:jax.scipy.special.(a,)->_promote_args_inexact('multigammaln', a)
A:jax.scipy.special.d->lax.convert_element_type(d, lax.dtype(a))
A:jax.scipy.special.constant->lax.mul(lax.mul(lax.mul(_constant_like(a, 0.25), d), lax.sub(d, _constant_like(a, 1))), lax.log(_constant_like(a, np.pi)))
A:jax.scipy.special.res->numpy.lax_numpy.sum(gammaln(jnp.expand_dims(a, axis=-1) - lax.div(jnp.arange(d), _constant_like(a, 2))), axis=-1)
A:jax.scipy.special._LOGNDTR_FLOAT64_LOWER->numpy.array(-20, np.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_LOWER->numpy.array(-10, np.float32)
A:jax.scipy.special._LOGNDTR_FLOAT64_UPPER->numpy.array(8, np.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_UPPER->numpy.array(5, np.float32)
A:jax.scipy.special.dtype->lax.dtype(x)
A:jax.scipy.special.z->lax.sqrt(dtype(-2.0) * lax.log(sanitized_mcp))
A:jax.scipy.special.y->lax.select(lax.lt(z, half_sqrt_2), dtype(1.0) + lax.erf(w), lax.select(lax.gt(w, dtype(0.0)), dtype(2.0) - lax.erfc(z), lax.erfc(z)))
A:jax.scipy.special.p0->list(reversed([-59.96335010141079, 98.00107541859997, -56.67628574690703, 13.931260938727968, -1.2391658386738125]))
A:jax.scipy.special.q0->list(reversed([1.0, 1.9544885833814176, 4.676279128988815, 86.36024213908905, -225.46268785411937, 200.26021238006066, -82.03722561683334, 15.90562251262117, -1.1833162112133]))
A:jax.scipy.special.p1->list(reversed([4.0554489230596245, 31.525109459989388, 57.16281922464213, 44.08050738932008, 14.684956192885803, 2.1866330685079025, -0.1402560791713545, -0.03504246268278482, -0.0008574567851546854]))
A:jax.scipy.special.q1->list(reversed([1.0, 15.779988325646675, 45.39076351288792, 41.3172038254672, 15.04253856929075, 2.504649462083094, -0.14218292285478779, -0.03808064076915783, -0.0009332594808954574]))
A:jax.scipy.special.p2->list(reversed([3.2377489177694603, 6.915228890689842, 3.9388102529247444, 1.3330346081580755, 0.20148538954917908, 0.012371663481782003, 0.00030158155350823543, 2.6580697468673755e-06, 6.239745391849833e-09]))
A:jax.scipy.special.q2->list(reversed([1.0, 6.02427039364742, 3.6798356385616087, 1.3770209948908132, 0.21623699359449663, 0.013420400608854318, 0.00032801446468212774, 2.8924786474538068e-06, 6.790194080099813e-09]))
A:jax.scipy.special.coeffs->numpy.array(coeffs, dtype)
A:jax.scipy.special.maybe_complement_p->numpy.lax_numpy.where(p > dtype(-np.expm1(-2.0)), dtype(1.0) - p, p)
A:jax.scipy.special.sanitized_mcp->numpy.lax_numpy.where(maybe_complement_p <= dtype(0.0), jnp.full(shape, dtype(0.5)), maybe_complement_p)
A:jax.scipy.special.ww->lax.square(w)
A:jax.scipy.special.infinity->numpy.lax_numpy.full(shape, dtype(np.inf))
A:jax.scipy.special.x_nan_replaced->numpy.lax_numpy.where(p <= dtype(0.0), -infinity, jnp.where(p >= dtype(1.0), infinity, x))
A:jax.scipy.special.x_2->lax.square(x)
A:jax.scipy.special.even_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special.odd_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special._norm_logpdf_constant->numpy.log(np.sqrt(2 * np.pi))
A:jax.scipy.special.neg_half->_constant_like(x, -0.5)
A:jax.scipy.special.log_normalizer->_constant_like(x, _norm_logpdf_constant)
jax.scipy.special._double_factorial(n)
jax.scipy.special._log_ndtr_asymptotic_series(x,series_order)
jax.scipy.special._log_ndtr_lower(x,series_order)
jax.scipy.special._ndtr(x)
jax.scipy.special._ndtri(p)
jax.scipy.special._norm_logpdf(x)
jax.scipy.special.betainc(a,b,x)
jax.scipy.special.betaln(x,y)
jax.scipy.special.digamma(x)
jax.scipy.special.entr(x)
jax.scipy.special.erf(x)
jax.scipy.special.erfc(x)
jax.scipy.special.erfinv(x)
jax.scipy.special.expit(x)
jax.scipy.special.gammainc(a,x)
jax.scipy.special.gammaincc(a,x)
jax.scipy.special.gammaln(x)
jax.scipy.special.i0e(x)
jax.scipy.special.i1e(x)
jax.scipy.special.log_ndtr(x,series_order=3)
jax.scipy.special.logit(x)
jax.scipy.special.logsumexp(a,axis=None,b=None,keepdims=False,return_sign=False)
jax.scipy.special.multigammaln(a,d)
jax.scipy.special.ndtr(x)
jax.scipy.special.ndtri(p)
jax.scipy.special.xlog1py(x,y)
jax.scipy.special.xlogy(x,y)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/poisson.py----------------------------------------
A:jax.scipy.stats.poisson.(k, mu, loc)->numpy.lax_numpy._promote_args_inexact('poisson.logpmf', k, mu, loc)
A:jax.scipy.stats.poisson.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.poisson.x->lax.sub(k, loc)
jax.scipy.stats.poisson.logpmf(k,mu,loc=0)
jax.scipy.stats.poisson.pmf(k,mu,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/pareto.py----------------------------------------
A:jax.scipy.stats.pareto.(x, b, loc, scale)->_promote_args_inexact('pareto.logpdf', x, b, loc, scale)
A:jax.scipy.stats.pareto.one->_constant_like(x, 1)
A:jax.scipy.stats.pareto.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.pareto.normalize_term->lax.log(lax.div(scale, b))
A:jax.scipy.stats.pareto.log_probs->lax.neg(lax.add(normalize_term, lax.mul(lax.add(b, one), lax.log(scaled_x))))
jax.scipy.stats.pareto.logpdf(x,b,loc=0,scale=1)
jax.scipy.stats.pareto.pdf(x,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/cauchy.py----------------------------------------
A:jax.scipy.stats.cauchy.(x, loc, scale)->_promote_args_inexact('cauchy.logpdf', x, loc, scale)
A:jax.scipy.stats.cauchy.one->_constant_like(x, 1)
A:jax.scipy.stats.cauchy.pi->_constant_like(x, np.pi)
A:jax.scipy.stats.cauchy.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.cauchy.normalize_term->lax.log(lax.mul(pi, scale))
jax.scipy.stats.cauchy.logpdf(x,loc=0,scale=1)
jax.scipy.stats.cauchy.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/expon.py----------------------------------------
A:jax.scipy.stats.expon.(x, loc, scale)->_promote_args_inexact('expon.logpdf', x, loc, scale)
A:jax.scipy.stats.expon.log_scale->lax.log(scale)
A:jax.scipy.stats.expon.linear_term->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.expon.log_probs->lax.neg(lax.add(linear_term, log_scale))
jax.scipy.stats.expon.logpdf(x,loc=0,scale=1)
jax.scipy.stats.expon.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/logistic.py----------------------------------------
jax.scipy.stats.logistic.cdf(x)
jax.scipy.stats.logistic.isf(x)
jax.scipy.stats.logistic.logpdf(x)
jax.scipy.stats.logistic.pdf(x)
jax.scipy.stats.logistic.ppf(x)
jax.scipy.stats.logistic.sf(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/laplace.py----------------------------------------
A:jax.scipy.stats.laplace.(x, loc, scale)->_promote_args_inexact('laplace.cdf', x, loc, scale)
A:jax.scipy.stats.laplace.two->_constant_like(x, 2)
A:jax.scipy.stats.laplace.linear_term->lax.div(lax.abs(lax.sub(x, loc)), scale)
A:jax.scipy.stats.laplace.half->_constant_like(x, 0.5)
A:jax.scipy.stats.laplace.one->_constant_like(x, 1)
A:jax.scipy.stats.laplace.zero->_constant_like(x, 0)
A:jax.scipy.stats.laplace.diff->lax.div(lax.sub(x, loc), scale)
jax.scipy.stats.laplace.cdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.logpdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/multivariate_normal.py----------------------------------------
A:jax.scipy.stats.multivariate_normal.(x, mean, cov)->_promote_dtypes_inexact(x, mean, cov)
A:jax.scipy.stats.multivariate_normal.L->cholesky(cov)
A:jax.scipy.stats.multivariate_normal.y->triangular_solve(L, x - mean, lower=True, transpose_a=True)
jax.scipy.stats.multivariate_normal.logpdf(x,mean,cov)
jax.scipy.stats.multivariate_normal.pdf(x,mean,cov)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/gamma.py----------------------------------------
A:jax.scipy.stats.gamma.(x, a, loc, scale)->_promote_args_inexact('gamma.logpdf', x, a, loc, scale)
A:jax.scipy.stats.gamma.one->_constant_like(x, 1)
A:jax.scipy.stats.gamma.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.gamma.log_linear_term->lax.sub(lax.mul(lax.sub(a, one), lax.log(y)), y)
A:jax.scipy.stats.gamma.shape_terms->lax.add(gammaln(a), lax.log(scale))
A:jax.scipy.stats.gamma.log_probs->lax.sub(log_linear_term, shape_terms)
jax.scipy.stats.gamma.logpdf(x,a,loc=0,scale=1)
jax.scipy.stats.gamma.pdf(x,a,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/bernoulli.py----------------------------------------
A:jax.scipy.stats.bernoulli.(k, p, loc)->numpy.lax_numpy._promote_args_inexact('bernoulli.logpmf', k, p, loc)
A:jax.scipy.stats.bernoulli.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.bernoulli.one->numpy.lax_numpy._constant_like(k, 1)
A:jax.scipy.stats.bernoulli.x->lax.sub(k, loc)
jax.scipy.stats.bernoulli.logpmf(k,p,loc=0)
jax.scipy.stats.bernoulli.pmf(k,p,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/dirichlet.py----------------------------------------
A:jax.scipy.stats.dirichlet.x_sum->numpy.lax_numpy.sum(x, axis=-1)
A:jax.scipy.stats.dirichlet.to_dtype->lax.dtype(osp_stats.dirichlet.logpdf(*args))
A:jax.scipy.stats.dirichlet.one->numpy.lax_numpy._constant_like(x, 1)
A:jax.scipy.stats.dirichlet.log_probs->lax.sub(jnp.sum(xlogy(lax.sub(alpha, one), x), axis=-1), normalize_term)
jax.scipy.stats.dirichlet._is_simplex(x)
jax.scipy.stats.dirichlet.logpdf(x,alpha)
jax.scipy.stats.dirichlet.pdf(x,alpha)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/uniform.py----------------------------------------
A:jax.scipy.stats.uniform.(x, loc, scale)->_promote_args_inexact('uniform.logpdf', x, loc, scale)
A:jax.scipy.stats.uniform.log_probs->lax.neg(lax.log(scale))
jax.scipy.stats.uniform.logpdf(x,loc=0,scale=1)
jax.scipy.stats.uniform.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/t.py----------------------------------------
A:jax.scipy.stats.t.(x, df, loc, scale)->_promote_args_inexact('t.logpdf', x, df, loc, scale)
A:jax.scipy.stats.t.two->_constant_like(x, 2)
A:jax.scipy.stats.t.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.t.df_over_two->lax.div(df, two)
A:jax.scipy.stats.t.df_plus_one_over_two->lax.add(df_over_two, _constant_like(x, 0.5))
A:jax.scipy.stats.t.normalize_term_const->lax.mul(lax.mul(scale, scale), _constant_like(x, np.pi))
A:jax.scipy.stats.t.normalize_term_tmp->lax.div(lax.log(lax.mul(normalize_term_const, df)), two)
A:jax.scipy.stats.t.normalize_term->lax.sub(lax.add(lax.lgamma(df_over_two), normalize_term_tmp), lax.lgamma(df_plus_one_over_two))
A:jax.scipy.stats.t.quadratic->lax.div(lax.mul(scaled_x, scaled_x), df)
jax.scipy.stats.t.logpdf(x,df,loc=0,scale=1)
jax.scipy.stats.t.pdf(x,df,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/beta.py----------------------------------------
A:jax.scipy.stats.beta.(x, a, b, loc, scale)->_promote_args_inexact('beta.logpdf', x, a, b, loc, scale)
A:jax.scipy.stats.beta.one->_constant_like(x, 1)
A:jax.scipy.stats.beta.shape_term->lax.neg(betaln(a, b))
A:jax.scipy.stats.beta.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.beta.log_linear_term->lax.add(lax.mul(lax.sub(a, one), lax.log(y)), lax.mul(lax.sub(b, one), lax.log1p(lax.neg(y))))
A:jax.scipy.stats.beta.log_probs->lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
jax.scipy.stats.beta.logpdf(x,a,b,loc=0,scale=1)
jax.scipy.stats.beta.pdf(x,a,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/scipy/stats/norm.py----------------------------------------
A:jax.scipy.stats.norm.(x, loc, scale)->_promote_args_inexact('norm.logcdf', x, loc, scale)
A:jax.scipy.stats.norm.two->_constant_like(x, 2)
A:jax.scipy.stats.norm.scale_sqrd->lax.pow(scale, two)
A:jax.scipy.stats.norm.log_normalizer->lax.log(lax.mul(_constant_like(x, 2 * np.pi), scale_sqrd))
A:jax.scipy.stats.norm.quadratic->lax.div(lax.pow(lax.sub(x, loc), two), scale_sqrd)
jax.scipy.stats.norm.cdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logcdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logpdf(x,loc=0,scale=1)
jax.scipy.stats.norm.pdf(x,loc=0,scale=1)
jax.scipy.stats.norm.ppf(q,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lib/xla_bridge.py----------------------------------------
A:jax.lib.xla_bridge.compile_options->xla_client.CompileOptions()
A:jax.lib.xla_bridge.device_assignment->xla_client.DeviceAssignment.create(device_assignment)
A:jax.lib.xla_bridge.backend->_backends.get(FLAGS.jax_xla_backend)
A:jax.lib.xla_bridge._tpu_backend->tpu_client.TpuBackend.create(worker=backend_target)
A:jax.lib.xla_bridge._backend_lock->threading.Lock()
A:jax.lib.xla_bridge.host_id->get_backend(backend).host_id()
A:jax.lib.xla_bridge.value->normalize_to_xla_dtypes(value)
A:jax.lib.xla_bridge.example_value->numpy.asarray(example_value)
A:jax.lib.xla_bridge.py_type->type(py_val)
A:jax.lib.xla_bridge.(zero_stride_axes,)->numpy.where(onp.equal(0, val.strides))
A:jax.lib.xla_bridge.(other_axes,)->numpy.where(onp.not_equal(0, val.strides))
A:jax.lib.xla_bridge.xla_val->c.Broadcast(c.NumpyArrayConstant(collapsed_val, canonicalize_types), onp.take(val.shape, zero_stride_axes))
A:jax.lib.xla_bridge.permutation->numpy.argsort(tuple(zero_stride_axes) + tuple(other_axes))
jax.lib.xla_bridge._JaxComputationBuilder(xla_client.ComputationBuilder)
jax.lib.xla_bridge._JaxComputationBuilder.AllToAll(self,operand,split_axis,concat_axis,replica_groups)
jax.lib.xla_bridge._JaxComputationBuilder.Build(self,*args,**kwargs)
jax.lib.xla_bridge._JaxComputationBuilder.Constant(self,py_val,canonicalize_types=True)
jax.lib.xla_bridge._JaxComputationBuilder.ConstantLike(self,example_value,value,canonicalize_types=True)
jax.lib.xla_bridge._JaxComputationBuilder.CrossReplicaSum(self,operand,replica_groups)
jax.lib.xla_bridge._JaxComputationBuilder.NumpyArrayConstant(self,value,canonicalize_types=True)
jax.lib.xla_bridge._get_local_backend(platform=None)
jax.lib.xla_bridge._get_tpu_driver_backend(platform)
jax.lib.xla_bridge._ndarray_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge._python_scalar_handler(dtype,c,val,canonicalize_dtypes=True)
jax.lib.xla_bridge._scalar_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge.device_count(backend=None)
jax.lib.xla_bridge.devices(backend=None)
jax.lib.xla_bridge.dtype_to_etype(dtype)
jax.lib.xla_bridge.get_backend(platform=None)
jax.lib.xla_bridge.get_compile_options(num_replicas,num_partitions,device_assignment=None)
jax.lib.xla_bridge.get_device_backend(device=None)
jax.lib.xla_bridge.host_count(backend=None)
jax.lib.xla_bridge.host_id(backend=None)
jax.lib.xla_bridge.host_ids(backend=None)
jax.lib.xla_bridge.local_device_count(backend=None)
jax.lib.xla_bridge.local_devices(host_id=None,backend=None)
jax.lib.xla_bridge.make_computation_builder(name)
jax.lib.xla_bridge.normalize_to_xla_dtypes(val)
jax.lib.xla_bridge.register_backend(name,factory)
jax.lib.xla_bridge.register_constant_handler(type_,handler_fun)
jax.lib.xla_bridge.supported_numpy_dtypes()


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lib/__init__.py----------------------------------------
A:jax.lib.__init__.version->tuple((int(x) for x in jaxlib_version.__version__.split('.')))
jax.lib.__init__._check_jaxlib_version()


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/tools/jax_to_hlo.py----------------------------------------
A:jax.tools.jax_to_hlo.shape_with_default_layout->jax.lib.xla_client.Shape.array_shape(shape.xla_element_type(), shape.dimensions()).with_major_to_minor_layout_if_absent()
A:jax.tools.jax_to_hlo.fn_curried->functools.partial(fn, **constants)
A:jax.tools.jax_to_hlo.comp->jax.api.xla_computation(ordered_wrapper)(*args)
A:jax.tools.jax_to_hlo.(module_name, fn_name)->FLAGS.fn.rsplit('.', 1)
A:jax.tools.jax_to_hlo.module->importlib.import_module(module_name)
A:jax.tools.jax_to_hlo.fn->getattr(module, fn_name)
A:jax.tools.jax_to_hlo.v->jax.numpy.asarray(v)
A:jax.tools.jax_to_hlo.(hlo_proto, hlo_text)->jax_to_hlo(fn, input_shapes, constants)
jax.tools.jax_to_hlo.jax_to_hlo(fn,input_shapes,constants=None)
jax.tools.jax_to_hlo.main(argv)
jax.tools.jax_to_hlo.set_up_flags()


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/tools/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/optix.py----------------------------------------
A:jax.experimental.optix.InitUpdate->collections.namedtuple('InitUpdate', ('init', 'update'))
A:jax.experimental.optix.ClipState->collections.namedtuple('ClipState', '')
A:jax.experimental.optix.updates->tree_multimap(lambda ga: emit * ga, grad_acc)
A:jax.experimental.optix.ClipByGlobalNormState->collections.namedtuple('ClipByGlobalNormState', '')
A:jax.experimental.optix.g_norm->global_norm(updates)
A:jax.experimental.optix.TraceState->collections.namedtuple('TraceState', 'trace')
A:jax.experimental.optix.update_trace->tree_multimap(f, updates, state.trace)
A:jax.experimental.optix.ScaleByRmsState->collections.namedtuple('ScaleByRmsState', 'nu')
A:jax.experimental.optix.nu->_update_moment(updates, state.nu, b2, 2)
A:jax.experimental.optix.ScaleByRStdDevState->collections.namedtuple('ScaleByRStdDevState', 'mu nu')
A:jax.experimental.optix.mu->_update_moment(updates, state.mu, b1, 1)
A:jax.experimental.optix.ScaleByAdamState->collections.namedtuple('ScaleByAdamState', 'count mu nu')
A:jax.experimental.optix.mu_hat->tree_multimap(lambda t: t / (1 - b1 ** (state.count + 1)), mu)
A:jax.experimental.optix.nu_hat->tree_multimap(lambda t: t / (1 - b2 ** (state.count + 1)), nu)
A:jax.experimental.optix.ScaleState->collections.namedtuple('ScaleState', '')
A:jax.experimental.optix.ScaleByScheduleState->collections.namedtuple('ScaleByScheduleState', 'count')
A:jax.experimental.optix.AddNoiseState->collections.namedtuple('AddNoiseState', 'count rng_key')
A:jax.experimental.optix.num_vars->len(tree_leaves(updates))
A:jax.experimental.optix.treedef->tree_structure(updates)
A:jax.experimental.optix.all_keys->jax.random.split(state.rng_key, num=num_vars + 1)
A:jax.experimental.optix.noise->tree_multimap(lambda g, k: jrandom.normal(k, shape=g.shape), updates, tree_unflatten(treedef, all_keys[1:]))
A:jax.experimental.optix.ApplyEvery->collections.namedtuple('ApplyEvery', 'count grad_acc')
A:jax.experimental.optix.grad_acc->tree_multimap(lambda g, ga: acc * ga + g, updates, state.grad_acc)
A:jax.experimental.optix.(init_fns, update_fns)->zip(*args)
A:jax.experimental.optix.(updates, new_s)->fn(updates, s)
jax.experimental.optix._update_moment(updates,moments,decay,order)
jax.experimental.optix.adam(learning_rate,b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optix.add_noise(eta,gamma,seed)
jax.experimental.optix.apply_every(k=1)
jax.experimental.optix.apply_updates(params,updates)
jax.experimental.optix.chain(*args)
jax.experimental.optix.clip(max_delta)
jax.experimental.optix.clip_by_global_norm(max_norm)
jax.experimental.optix.global_norm(items)
jax.experimental.optix.noisy_sgd(learning_rate,eta=0.01,gamma=0.55,seed=42)
jax.experimental.optix.rmsprop(learning_rate,decay=0.9,eps=1e-08,centered=False)
jax.experimental.optix.scale(step_size)
jax.experimental.optix.scale_by_adam(b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optix.scale_by_rms(decay=0.9,eps=1e-08)
jax.experimental.optix.scale_by_schedule(step_size_fn)
jax.experimental.optix.scale_by_stddev(decay=0.9,eps=1e-08)
jax.experimental.optix.sgd(learning_rate,momentum=0.0,nesterov=False)
jax.experimental.optix.trace(decay,nesterov)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/ode.py----------------------------------------
A:jax.experimental.ode.dps_c_mid->jax.numpy.array([6025192743 / 30085553152 / 2, 0, 51252292925 / 65400821598 / 2, -2691868925 / 45128329728 / 2, 187940372067 / 1594534317056 / 2, -1776094331 / 19743644256 / 2, 11237099 / 235043384 / 2])
A:jax.experimental.ode.v->jax.numpy.stack([dy0, dy1, y0, y1, y_mid])
A:jax.experimental.ode.a->jax.numpy.dot(np.hstack([-2.0 * dt, 2.0 * dt, np.array([-8.0, -8.0, 16.0])]), v)
A:jax.experimental.ode.b->jax.numpy.dot(np.hstack([5.0 * dt, -3.0 * dt, np.array([18.0, 14.0, -32.0])]), v)
A:jax.experimental.ode.c->jax.numpy.dot(np.hstack([-4.0 * dt, dt, np.array([-11.0, -5.0, 16.0])]), v)
A:jax.experimental.ode.d0->jax.numpy.linalg.norm(y0 / scale)
A:jax.experimental.ode.d1->jax.numpy.linalg.norm(f0 / scale)
A:jax.experimental.ode.h0->jax.numpy.where(np.any(np.asarray([d0 < 1e-05, d1 < 1e-05])), 1e-06, 0.01 * d0 / d1)
A:jax.experimental.ode.f1->fun(y1, t0 + h0)
A:jax.experimental.ode.h1->jax.numpy.where(np.all(np.asarray([d1 <= 1e-15, d2 <= 1e-15])), np.maximum(1e-06, h0 * 0.001), (0.01 / np.max(d1 + d2)) ** order_pow)
A:jax.experimental.ode.alpha->jax.numpy.array([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1.0, 1.0, 0])
A:jax.experimental.ode.beta->jax.numpy.array([[1 / 5, 0, 0, 0, 0, 0, 0], [3 / 40, 9 / 40, 0, 0, 0, 0, 0], [44 / 45, -56 / 15, 32 / 9, 0, 0, 0, 0], [19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729, 0, 0, 0], [9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656, 0, 0], [35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0]])
A:jax.experimental.ode.c_sol->jax.numpy.array([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0])
A:jax.experimental.ode.c_error->jax.numpy.array([35 / 384 - 1951 / 21600, 0, 500 / 1113 - 22642 / 50085, 125 / 192 - 451 / 720, -2187 / 6784 - -12231 / 42400, 11 / 84 - 649 / 6300, -1.0 / 60.0])
A:jax.experimental.ode.ft->func(yi, ti)
A:jax.experimental.ode.k->jax.lax.fori_loop(1, 7, _fori_body_fun, jax.ops.index_update(np.zeros((7, f0.shape[0])), jax.ops.index[0, :], f0))
A:jax.experimental.ode.mean_error_ratio->jax.numpy.max(mean_error_ratio)
A:jax.experimental.ode.dfactor->jax.numpy.where(mean_error_ratio < 1, 1.0, dfactor)
A:jax.experimental.ode.err_ratio->jax.numpy.sqrt(mean_error_ratio)
A:jax.experimental.ode.factor->jax.numpy.maximum(1.0 / ifactor, np.minimum(err_ratio ** (1.0 / order) / safety, 1.0 / dfactor))
A:jax.experimental.ode.rtol->kwargs.get('rtol', 1.4e-08)
A:jax.experimental.ode.atol->kwargs.get('atol', 1.4e-08)
A:jax.experimental.ode.mxstep->kwargs.get('mxstep', np.inf)
A:jax.experimental.ode.(cur_y, cur_f, cur_t, dt, last_t, interp_coeff, _)->jax.lax.while_loop(lambda x: (x[2] < t[i]) & (x[-1] < mxstep), functools.partial(_while_body_fun, func), (cur_y, cur_f, cur_t, dt, last_t, interp_coeff, 0.0))
A:jax.experimental.ode.out_x->jax.numpy.polyval(interp_coeff, relative_output_time)
A:jax.experimental.ode.(next_y, next_f, next_y_error, k)->runge_kutta_step(func, cur_y, cur_f, cur_t, dt)
A:jax.experimental.ode.error_ratios->error_ratio(next_y_error, rtol, atol, cur_y, next_y)
A:jax.experimental.ode.new_interp_coeff->interp_fit_dopri(cur_y, next_y, k, dt)
A:jax.experimental.ode.dt->initial_step_size(func, t[0], y0, 4, rtol, atol, f0)
A:jax.experimental.ode.(new_rav, unravel)->ravel_pytree((next_y, next_f, next_t, dt, cur_t, new_interp_coeff, next_j))
A:jax.experimental.ode.(old_rav, _)->ravel_pytree((cur_y, cur_f, cur_t, dt, last_t, interp_coeff, next_j))
A:jax.experimental.ode.f0->func(y0, t[0])
A:jax.experimental.ode.interp_coeff->jax.numpy.array([y0] * 5)
A:jax.experimental.ode.(flat_args, unravel_args)->ravel_pytree(args)
A:jax.experimental.ode.state_len->int(np.floor_divide(augmented_state.shape[0] - flat_args.shape[0] - 1, 2))
A:jax.experimental.ode.(dy_dt, vjpfun)->jax.vjp(flat_func, y, t, flat_args)
A:jax.experimental.ode.vjp_cur_t->jax.numpy.dot(flat_func(this_yt, this_t, flat_args), this_gi)
A:jax.experimental.ode.aug_y0->jax.numpy.hstack((this_yt, vjp_y, vjp_t0, vjp_args))
A:jax.experimental.ode.aug_ans->odeint(rev_aug_dynamics, aug_y0, this_tarray, flat_args, rtol=rtol, atol=atol, mxstep=mxstep)
A:jax.experimental.ode.time_vjp_list->jax.ops.index_update(result[-1], -1, result[-3])
A:jax.experimental.ode.vjp_args->unravel_args(result[-2])
A:jax.experimental.ode.result->jax.lax.fori_loop(0, rev_t.shape[0] - 1, _fori_body_fun, (rev_yt, rev_t, rev_tarray, rev_gi, vjp_y, vjp_t0, vjp_args, time_vjp_list))
A:jax.experimental.ode.primals_out->odeint(flat_func, y0, t, flat_args, rtol=rtol, atol=atol, mxstep=mxstep)
A:jax.experimental.ode.ct_odeint->jax.custom_transforms(lambda y0, t, *args: odeint(ofunc, y0, t, *args, rtol=rtol, atol=atol, mxstep=mxstep))
A:jax.experimental.ode.(ys, pullback)->vjp_odeint(fun, *args, **kwargs)
A:jax.experimental.ode.my_grad->pullback(np.ones_like(ys))
A:jax.experimental.ode.my_jac->jax.api.tree_transpose(jax.api.tree_structure(args), jax.api.tree_structure(ys), my_jac)
A:jax.experimental.ode.(flat_x, unravel)->ravel_pytree(x)
A:jax.experimental.ode.dim->len(flat_x)
A:jax.experimental.ode.g->numpy.zeros_like(flat_x)
A:jax.experimental.ode.d->numpy.zeros_like(flat_x)
A:jax.experimental.ode.y0->jax.numpy.linspace(0.1, 0.9, 10)
A:jax.experimental.ode.numerical_grad->nd(onearg_odeint, args)
A:jax.experimental.ode.(exact_grad, _)->ravel_pytree(my_odeint_grad(func)(*args))
A:jax.experimental.ode.x->jax.numpy.linspace(*xlimits, num=numticks)
A:jax.experimental.ode.y->jax.numpy.array([0.1])
A:jax.experimental.ode.(x_mesh, y_mesh)->jax.numpy.meshgrid(x, y)
A:jax.experimental.ode.zs->jax.vmap(func)(y_mesh.ravel(), x_mesh.ravel())
A:jax.experimental.ode.z_mesh->jax.vmap(func)(y_mesh.ravel(), x_mesh.ravel()).reshape(x_mesh.shape)
A:jax.experimental.ode.dydt->jax.numpy.array([omega, -arg1 * omega - arg2 * np.sin(theta)])
A:jax.experimental.ode.start->time.time()
A:jax.experimental.ode.scipy_result->scipy.integrate.odeint(fun, y0, tspace, args)
A:jax.experimental.ode.end->time.time()
A:jax.experimental.ode.jax_result->odeint(fun, np.array(y0), np.array(tspace), *args)
A:jax.experimental.ode.(_, _)->benchmark_odeint(pend, (onp.pi - 0.1, 0.0), onp.linspace(0.0, 10.0, 101), 0.25, 9.8)
A:jax.experimental.ode.ts->jax.numpy.array((0.1, 0.2))
A:jax.experimental.ode.big_y0->jax.numpy.linspace(1.1, 10.9, 10)
A:jax.experimental.ode.t->jax.numpy.linspace(0.0, 2.0, 11)
A:jax.experimental.ode.swoop_build->build_odeint(swoop)
A:jax.experimental.ode.jacswoop->jax.jit(jax.jacrev(swoop_build))
A:jax.experimental.ode.rslt->jacswoop(*wrap_args)
jax.experimental.ode.benchmark_odeint(fun,y0,tspace,*args)
jax.experimental.ode.build_odeint(ofunc,rtol=1.4e-08,atol=1.4e-08,mxstep=onp.inf)
jax.experimental.ode.decay(y,t,arg1,arg2)
jax.experimental.ode.error_ratio(error_estimate,rtol,atol,y0,y1)
jax.experimental.ode.fit_4th_order_polynomial(y0,y1,y_mid,dy0,dy1,dt)
jax.experimental.ode.initial_step_size(fun,t0,y0,order,rtol,atol,f0)
jax.experimental.ode.interp_fit_dopri(y0,y1,k,dt)
jax.experimental.ode.my_odeint_grad(fun)
jax.experimental.ode.my_odeint_jacrev(fun)
jax.experimental.ode.nd(f,x,eps=0.0001)
jax.experimental.ode.odeint(ofunc,y0,t,*args,**kwargs)
jax.experimental.ode.optimal_step_size(last_step,mean_error_ratio,safety=0.9,ifactor=10.0,dfactor=0.2,order=5.0)
jax.experimental.ode.pend(y,t,arg1,arg2)
jax.experimental.ode.pend_benchmark_odeint()
jax.experimental.ode.plot_gradient_field(ax,func,xlimits,ylimits,numticks=30)
jax.experimental.ode.runge_kutta_step(func,y0,f0,t0,dt)
jax.experimental.ode.swoop(y,t,arg1,arg2)
jax.experimental.ode.test_defvjp_all()
jax.experimental.ode.test_grad_vjp_odeint()
jax.experimental.ode.test_odeint_grad()
jax.experimental.ode.test_odeint_vjp()
jax.experimental.ode.vjp_odeint(ofunc,y0,t,*args,**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/stax.py----------------------------------------
A:jax.experimental.stax.(k1, k2)->jax.random.split(rng)
A:jax.experimental.stax.filter_shape_iter->iter(filter_shape)
A:jax.experimental.stax.output_shape->jax.lax.conv_transpose_shape_tuple(input_shape, kernel_shape, strides, padding, dimension_numbers)
A:jax.experimental.stax.bias_shape->tuple(itertools.dropwhile(lambda x: x == 1, bias_shape))
A:jax.experimental.stax.Conv->functools.partial(GeneralConv, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.Conv1DTranspose->functools.partial(GeneralConvTranspose, ('NHC', 'HIO', 'NHC'))
A:jax.experimental.stax.ConvTranspose->functools.partial(GeneralConvTranspose, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.shape->tuple((d for (i, d) in enumerate(input_shape) if i not in axis))
A:jax.experimental.stax.ed->tuple((None if i in axis else slice(None) for i in range(np.ndim(x))))
A:jax.experimental.stax.z->normalize(x, axis, epsilon=epsilon)
A:jax.experimental.stax.Tanh->elementwise(np.tanh)
A:jax.experimental.stax.Relu->elementwise(relu)
A:jax.experimental.stax.Exp->elementwise(np.exp)
A:jax.experimental.stax.LogSoftmax->elementwise(log_softmax, axis=-1)
A:jax.experimental.stax.Softmax->elementwise(softmax, axis=-1)
A:jax.experimental.stax.Softplus->elementwise(softplus)
A:jax.experimental.stax.Sigmoid->elementwise(sigmoid)
A:jax.experimental.stax.Elu->elementwise(elu)
A:jax.experimental.stax.LeakyRelu->elementwise(leaky_relu)
A:jax.experimental.stax.Selu->elementwise(selu)
A:jax.experimental.stax.Gelu->elementwise(gelu)
A:jax.experimental.stax.out_shape->jax.lax.reduce_window_shape_tuple(input_shape, window_shape, strides, padding)
A:jax.experimental.stax.out->jax.lax.reduce_window(inputs, init_val, reducer, window_shape, strides, padding)
A:jax.experimental.stax.MaxPool->_pooling_layer(lax.max, -np.inf)
A:jax.experimental.stax.SumPool->_pooling_layer(lax.add, 0.0)
A:jax.experimental.stax.spatial_shape->tuple((inputs.shape[i] for i in range(inputs.ndim) if i not in non_spatial_axes))
A:jax.experimental.stax.one->jax.numpy.ones(spatial_shape, dtype=inputs.dtype)
A:jax.experimental.stax.window_sizes->jax.numpy.expand_dims(window_sizes, i)
A:jax.experimental.stax.AvgPool->_pooling_layer(lax.add, 0.0, _normalize_by_window_size)
A:jax.experimental.stax.Flatten->Flatten()
A:jax.experimental.stax.Identity->Identity()
A:jax.experimental.stax.FanInSum->FanInSum()
A:jax.experimental.stax.concat_size->sum((shape[ax] for shape in input_shape))
A:jax.experimental.stax.rng->kwargs.pop('rng', None)
A:jax.experimental.stax.keep->jax.random.bernoulli(rng, rate, inputs.shape)
A:jax.experimental.stax.nlayers->len(layers)
A:jax.experimental.stax.(init_funs, apply_funs)->zip(*layers)
A:jax.experimental.stax.(rng, layer_rng)->jax.random.split(rng)
A:jax.experimental.stax.(input_shape, param)->init_fun(layer_rng, input_shape)
A:jax.experimental.stax.inputs->fun(param, inputs, rng=rng, **kwargs)
A:jax.experimental.stax.rngs->jax.random.split(rng, nlayers)
jax.experimental.stax.BatchNorm(axis=(0,1,2),epsilon=1e-05,center=True,scale=True,beta_init=zeros,gamma_init=ones)
jax.experimental.stax.Dense(out_dim,W_init=glorot_normal(),b_init=normal())
jax.experimental.stax.Dropout(rate,mode='train')
jax.experimental.stax.FanInConcat(axis=-1)
jax.experimental.stax.FanInSum()
jax.experimental.stax.FanOut(num)
jax.experimental.stax.Flatten()
jax.experimental.stax.GeneralConv(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.GeneralConvTranspose(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.Identity()
jax.experimental.stax._normalize_by_window_size(dims,strides,padding)
jax.experimental.stax._pooling_layer(reducer,init_val,rescaler=None)
jax.experimental.stax.elementwise(fun,**fun_kwargs)
jax.experimental.stax.parallel(*layers)
jax.experimental.stax.serial(*layers)
jax.experimental.stax.shape_dependent(make_layer)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/loops.py----------------------------------------
A:jax.experimental.loops.start->int(first)
A:jax.experimental.loops.stop->int(first)
A:jax.experimental.loops.step->int(third)
A:jax.experimental.loops.pred_dtype->numpy.result_type(pred)
A:jax.experimental.loops.mt_val->self._mutable_state.get(key)
A:jax.experimental.loops.self.stack->traceback.StackSummary.from_list(cast(List[Any], traceback.extract_stack()[:-2]))
A:jax.experimental.loops.self.carried_state_initial->copy.copy(self.scope._mutable_state)
A:jax.experimental.loops.self.carried_state_names->sorted(self.scope._mutable_state.keys())
A:jax.experimental.loops.self.trace->_BodyTracer.start_subtrace()
A:jax.experimental.loops.mt_aval->_BodyTracer.abstractify(initial)
A:jax.experimental.loops.mt_pval->jax.interpreters.partial_eval.PartialVal((mt_aval, core.unit))
A:jax.experimental.loops.mt_var->self.trace.new_arg(mt_pval)
A:jax.experimental.loops.index_var_aval->_BodyTracer.abstractify(0)
A:jax.experimental.loops.index_var_pval->jax.interpreters.partial_eval.PartialVal((index_var_aval, core.unit))
A:jax.experimental.loops.self._index_var->self.trace.new_arg(index_var_pval)
A:jax.experimental.loops.body_out_tracers->tuple([self.scope._mutable_state[ms] for ms in self.carried_state_names])
A:jax.experimental.loops.(body_typed_jaxpr, body_const_vals)->_BodyTracer.trace_to_jaxpr_finalize(in_tracers=in_tracers, out_tracers=body_out_tracers, trace=self.trace)
A:jax.experimental.loops.carried_init_val->tuple([self.carried_state_initial[ms] for ms in self.carried_state_names])
A:jax.experimental.loops.(carried_init_vals, carried_tree)->jax.tree_util.tree_flatten(carried_init_val)
A:jax.experimental.loops.carried_out_vals->self.loop_builder.build_output_vals(self.scope, self.carried_state_names, carried_tree, carried_init_vals, body_typed_jaxpr, body_const_vals)
A:jax.experimental.loops.carried_mutable_state_unflattened->jax.tree_util.tree_unflatten(carried_tree, carried_out_vals)
A:jax.experimental.loops.level->jax.core.trace_state.trace_stack.next_level(False)
A:jax.experimental.loops.master->jax.core.MasterTrace(level, pe.JaxprTrace)
A:jax.experimental.loops.out_tracers->safe_map(partial(pe.instantiate_const_at, trace), instantiate, out_tracers)
A:jax.experimental.loops.(jaxpr, consts, env)->jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.experimental.loops.out_avals->safe_map(abstract_arrays.raise_to_shaped, unzip2(out_pvals)[0])
A:jax.experimental.loops.const_avals->tuple((abstract_arrays.raise_to_shaped(core.get_aval(c)) for c in consts))
A:jax.experimental.loops.in_avals->tuple(safe_map(abstract_arrays.raise_to_shaped, unzip2(in_pvals)[0]))
A:jax.experimental.loops.typed_jaxpr->jax.core.TypedJaxpr(pe.convert_constvars_jaxpr(jaxpr), (), const_avals + in_avals, out_avals)
A:jax.experimental.loops.arange_val->jax.numpy.arange(self.start, stop=self.stop, step=self.step)
A:jax.experimental.loops.init_avals->safe_map(_BodyTracer.abstractify, init_vals)
A:jax.experimental.loops.(false_body_typed_jaxpr, false_body_const_vals, _)->jax.lax.lax_control_flow._initial_style_jaxpr(lambda *args: args, carried_tree, tuple(init_avals))
A:jax.experimental.loops.args->list(itertools.chain(body_const_vals, init_vals, false_body_const_vals, init_vals))
A:jax.experimental.loops.res->self.cond_func()
A:jax.experimental.loops.(cond_jaxpr, cond_consts, cond_tree)->jax.lax.lax_control_flow._initial_style_jaxpr(cond_func_wrapped, carried_tree, tuple(init_avals))
jax.experimental.loops.Scope(self)
jax.experimental.loops.Scope.__enter__(self)
jax.experimental.loops.Scope.__exit__(self,exc_type,exc_val,exc_tb)
jax.experimental.loops.Scope.__getattr__(self,key)
jax.experimental.loops.Scope.__setattr__(self,key,value)
jax.experimental.loops.Scope._error_premature_exit_range(self)
jax.experimental.loops.Scope._pop_range(self,range_)
jax.experimental.loops.Scope._push_range(self,range_)
jax.experimental.loops.Scope.cond_range(self,pred)
jax.experimental.loops.Scope.range(self,first,second=None,third=None)
jax.experimental.loops.Scope.while_range(self,cond_func)
jax.experimental.loops._BodyTracer(self,scope,loop_builder)
jax.experimental.loops._BodyTracer.__iter__(self)
jax.experimental.loops._BodyTracer.__next__(self)
jax.experimental.loops._BodyTracer.abstractify(x)
jax.experimental.loops._BodyTracer.end_subtrace()
jax.experimental.loops._BodyTracer.end_tracing_body(self)
jax.experimental.loops._BodyTracer.location(self)
jax.experimental.loops._BodyTracer.next(self)
jax.experimental.loops._BodyTracer.start_subtrace()
jax.experimental.loops._BodyTracer.start_tracing_body(self)
jax.experimental.loops._BodyTracer.trace_to_jaxpr_finalize(in_tracers,out_tracers,trace,instantiate=True)
jax.experimental.loops._BoundedLoopBuilder(self,start,stop,step)
jax.experimental.loops._BoundedLoopBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._BoundedLoopBuilder.can_use_index_var(self)
jax.experimental.loops._CondBuilder(self,pred)
jax.experimental.loops._CondBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._CondBuilder.can_use_index_var(self)
jax.experimental.loops._LoopBuilder(object)
jax.experimental.loops._LoopBuilder.__str__(self)
jax.experimental.loops._LoopBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._LoopBuilder.can_use_index_var(self)
jax.experimental.loops._WhileBuilder(self,cond_func)
jax.experimental.loops._WhileBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._WhileBuilder.can_use_index_var(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/optimizers.py----------------------------------------
A:jax.experimental.optimizers.OptimizerState->namedtuple('OptimizerState', ['packed_state', 'tree_def', 'subtree_defs'])
A:jax.experimental.optimizers.(init, update, get_params)->opt_maker(*args, **kwargs)
A:jax.experimental.optimizers.(x0_flat, tree)->tree_flatten(x0_tree)
A:jax.experimental.optimizers.(states_flat, subtrees)->unzip2(map(tree_flatten, initial_states))
A:jax.experimental.optimizers.packed_state->pack(map(pack, states_flat))
A:jax.experimental.optimizers.(grad_flat, tree2)->tree_flatten(grad_tree)
A:jax.experimental.optimizers.states->map(tree_unflatten, subtrees, packed_state)
A:jax.experimental.optimizers.new_states->map(partial(update, i), grad_flat, states)
A:jax.experimental.optimizers.(new_states_flat, subtrees2)->unzip2(map(tree_flatten, new_states))
A:jax.experimental.optimizers.new_packed_state->pack(map(pack, new_states_flat))
A:jax.experimental.optimizers.params->map(get_params, states)
A:jax.experimental.optimizers.step_size->make_schedule(step_size)
A:jax.experimental.optimizers.v0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq_inv_sqrt->jax.numpy.where(g_sq > 0, 1.0 / np.sqrt(g_sq), 0.0)
A:jax.experimental.optimizers.avg_sq_grad->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.mom->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.lst->list(seq)
A:jax.experimental.optimizers.idx->splice([None] * ndim, axis, [slice(None)])
A:jax.experimental.optimizers.accum_inv_sqrt->jax.numpy.where(accum > 0, 1.0 / np.sqrt(accum), 0)
A:jax.experimental.optimizers.step_num->jax.numpy.minimum(step_num, decay_steps)
A:jax.experimental.optimizers.boundaries->jax.numpy.array(boundaries)
A:jax.experimental.optimizers.values->jax.numpy.array(values)
A:jax.experimental.optimizers.(leaves, _)->tree_flatten(tree)
A:jax.experimental.optimizers.norm->l2_norm(grad_tree)
A:jax.experimental.optimizers.subtrees->map(tree_unflatten, subtree_defs, packed_state)
A:jax.experimental.optimizers.(sentinels, tree_def)->tree_flatten(marked_pytree)
A:jax.experimental.optimizers.(states_flat, subtree_defs)->unzip2(map(tree_flatten, subtrees))
jax.experimental.optimizers.JoinPoint(self,subtree)
jax.experimental.optimizers.JoinPoint.__iter__(self)
jax.experimental.optimizers.adagrad(step_size,momentum=0.9)
jax.experimental.optimizers.adam(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optimizers.clip_grads(grad_tree,max_norm)
jax.experimental.optimizers.constant(step_size)
jax.experimental.optimizers.exponential_decay(step_size,decay_steps,decay_rate)
jax.experimental.optimizers.inverse_time_decay(step_size,decay_steps,decay_rate,staircase=False)
jax.experimental.optimizers.l2_norm(tree)
jax.experimental.optimizers.make_schedule(scalar_or_schedule)
jax.experimental.optimizers.momentum(step_size,mass)
jax.experimental.optimizers.nesterov(step_size,mass)
jax.experimental.optimizers.optimizer(opt_maker)
jax.experimental.optimizers.pack_optimizer_state(marked_pytree)
jax.experimental.optimizers.piecewise_constant(boundaries,values)
jax.experimental.optimizers.polynomial_decay(step_size,decay_steps,final_step_size,power=1.0)
jax.experimental.optimizers.rmsprop(step_size,gamma=0.9,eps=1e-08)
jax.experimental.optimizers.rmsprop_momentum(step_size,gamma=0.9,eps=1e-08,momentum=0.9)
jax.experimental.optimizers.sgd(step_size)
jax.experimental.optimizers.sm3(step_size,momentum=0.9)
jax.experimental.optimizers.unpack_optimizer_state(opt_state)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/jet.py----------------------------------------
A:jax.experimental.jet.(order,)->set(map(len, series))
A:jax.experimental.jet.treedef->tree_structure(t)
A:jax.experimental.jet.(f, out_tree)->flatten_fun_output(lu.wrap_init(fun))
A:jax.experimental.jet.(out_primals, out_terms)->unzip2(((t.primal, t.terms) for t in out_tracers))
A:jax.experimental.jet.trace->JetTrace(master, core.cur_sublevel())
A:jax.experimental.jet.in_tracers->map(partial(JetTracer, trace), primals, series)
A:jax.experimental.jet.out_tracers->map(trace.full_raise, ans)
A:jax.experimental.jet.(primals_in, series_in)->unzip2(((t.primal, t.terms) for t in tracers))
A:jax.experimental.jet.(primal_out, terms_out)->rule(primals_in, series_in, **params)
A:jax.experimental.jet.zero_term->ZeroTerm()
A:jax.experimental.jet.zero_series->ZeroSeries()
A:jax.experimental.jet.jet_rules[prim]->partial(linear_prop, prim)
A:jax.experimental.jet.primal_out->jax.lax.lax.reduce_max_p.bind(operand, **params)
A:jax.experimental.jet.conv->sum([scale(k, j) * v[j] * w[k - j] for j in range(0, k)])
A:jax.experimental.jet.op->partial(prim.bind, **params)
A:jax.experimental.jet.jet_rules[lax.dot_general_p]->partial(_bilinear_taylor_rule, lax.dot_general_p)
A:jax.experimental.jet.jet_rules[lax.mul_p]->partial(_bilinear_taylor_rule, lax.mul_p)
A:jax.experimental.jet.jet_rules[lax.conv_general_dilated_p]->partial(_bilinear_taylor_rule, lax.conv_general_dilated_p)
A:jax.experimental.jet.axes->params.pop('axes', None)
A:jax.experimental.jet.location_indicators->jax.lax.lax.convert_element_type(lax._eq_meet(operand, lax.reshape(primal_out, shape)), primal_dtype)
A:jax.experimental.jet.counts->jax.lax.lax._reduce_sum(location_indicators, axes)
jax.experimental.jet.JetTrace(core.Trace)
jax.experimental.jet.JetTrace.join(self,xt,yt)
jax.experimental.jet.JetTrace.lift(self,val)
jax.experimental.jet.JetTrace.post_process_call(self,call_primitive,out_tracer,params)
jax.experimental.jet.JetTrace.process_call(self,call_primitive,f,tracers,params)
jax.experimental.jet.JetTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.jet.JetTrace.pure(self,val)
jax.experimental.jet.JetTrace.sublift(self,val)
jax.experimental.jet.JetTracer(self,trace,primal,terms)
jax.experimental.jet.JetTracer.aval(self)
jax.experimental.jet.JetTracer.full_lower(self)
jax.experimental.jet.ZeroSeries(object)
jax.experimental.jet.ZeroTerm(object)
jax.experimental.jet._bilinear_taylor_rule(prim,primals_in,series_in,**params)
jax.experimental.jet._div_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet._exp_taylor(primals_in,series_in)
jax.experimental.jet._gather_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet._log_taylor(primals_in,series_in)
jax.experimental.jet._reduce_max_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet.deflinear(prim)
jax.experimental.jet.fact(n)
jax.experimental.jet.jet(fun,primals,series)
jax.experimental.jet.jet_transform(primals,series)
jax.experimental.jet.linear_prop(prim,primals_in,series_in,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/vectorize.py----------------------------------------
A:jax.experimental.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax.experimental.vectorize._ARGUMENT->'\\({}\\)'.format(_CORE_DIMENSION_LIST)
A:jax.experimental.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax.experimental.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax.experimental.vectorize.num_core_dims->len(core_dims)
A:jax.experimental.vectorize.dummy_array->numpy.lib.stride_tricks.as_strided(0, arg.shape[:ndim])
A:jax.experimental.vectorize.broadcast_shape->numpy.lib.stride_tricks._broadcast_shape(*broadcast_args)
A:jax.experimental.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims)
A:jax.experimental.vectorize.input_shapes->_calculate_shapes(broadcast_shape, dim_sizes, input_core_dims)
A:jax.experimental.vectorize.all_core_dims->set()
A:jax.experimental.vectorize.result->reorder_outputs(result, axis, output_core_dims)
A:jax.experimental.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax.experimental.vectorize.axis->kwargs.get('axis')
A:jax.experimental.vectorize.args->reorder_inputs(args, axis, input_core_dims)
A:jax.experimental.vectorize.broadcast_args->broadcast_with_core_dims(args, input_core_dims, output_core_dims)
A:jax.experimental.vectorize.vectorized_func->vmap(vectorized_func)
jax.experimental.vectorize._calculate_shapes(broadcast_shape,dim_sizes,list_of_core_dims)
jax.experimental.vectorize._parse_gufunc_signature(signature)
jax.experimental.vectorize._parse_input_dimensions(args,input_core_dims)
jax.experimental.vectorize._update_dim_sizes(dim_sizes,arg,core_dims)
jax.experimental.vectorize.broadcast_with_core_dims(args,input_core_dims,output_core_dims)
jax.experimental.vectorize.reorder_inputs(args,axis,input_core_dims)
jax.experimental.vectorize.reorder_outputs(result,axis,output_core_dims)
jax.experimental.vectorize.vectorize(signature)
jax.experimental.vectorize.verify_axis_is_supported(input_core_dims,output_core_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/experimental/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/third_party/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/third_party/numpy/linalg.py----------------------------------------
A:jax.third_party.numpy.linalg.s->jax.numpy.linalg.svd(x, compute_uv=False)
A:jax.third_party.numpy.linalg.invx->jax.numpy.linalg.inv(x)
A:jax.third_party.numpy.linalg.orig_nan_check->jax.numpy.lax_numpy.full_like(r, ~np.isnan(r).any())
A:jax.third_party.numpy.linalg.nan_mask->jax.numpy.lax_numpy.logical_and(np.isnan(r), ~np.isnan(x).any(axis=(-2, -1)))
A:jax.third_party.numpy.linalg.r->jax.numpy.lax_numpy.where(orig_nan_check, np.where(nan_mask, np.inf, r), r)
A:jax.third_party.numpy.linalg.a->a.reshape(-1, prod).reshape(-1, prod)
A:jax.third_party.numpy.linalg.ia->jax.numpy.linalg.inv(a)
A:jax.third_party.numpy.linalg.b->b.ravel().ravel()
A:jax.third_party.numpy.linalg.allaxes->list(range(0, an))
A:jax.third_party.numpy.linalg.res->res.reshape(Q).reshape(Q)
jax.third_party.numpy.linalg._assertNdSquareness(*arrays)
jax.third_party.numpy.linalg._assertNoEmpty2d(*arrays)
jax.third_party.numpy.linalg._assertRankAtLeast2(*arrays)
jax.third_party.numpy.linalg._isEmpty2d(arr)
jax.third_party.numpy.linalg.cond(x,p=None)
jax.third_party.numpy.linalg.tensorinv(a,ind=2)
jax.third_party.numpy.linalg.tensorsolve(a,b,axes=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/third_party/numpy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/numpy/linalg.py----------------------------------------
A:jax.numpy.linalg.dtype->lax.dtype(a)
A:jax.numpy.linalg.a->_promote_arg_dtypes(np.asarray(a))
A:jax.numpy.linalg.n->np.abs(n)
A:jax.numpy.linalg.(n, bit)->divmod(n, 2)
A:jax.numpy.linalg.M->_promote_arg_dtypes(np.asarray(M))
A:jax.numpy.linalg.S->svd(M, full_matrices=False, compute_uv=False)
A:jax.numpy.linalg.jvp_sign->np.zeros(x.shape[:-2])
A:jax.numpy.linalg.jvp_logdet->np.trace(solve(x, g), axis1=-1, axis2=-2)
A:jax.numpy.linalg.a_shape->np.shape(a)
A:jax.numpy.linalg.(lu, pivot)->lax_linalg.lu(a)
A:jax.numpy.linalg.diag->np.diagonal(lu, axis1=-2, axis2=-1)
A:jax.numpy.linalg.is_zero->np.any(diag == np.array(0, dtype=dtype), axis=-1)
A:jax.numpy.linalg.parity->np.count_nonzero(pivot != np.arange(a_shape[-1]), axis=-1)
A:jax.numpy.linalg.sign->np.where(is_zero, np.array(0, dtype=dtype), sign * np.array(-2 * (parity % 2) + 1, dtype=dtype))
A:jax.numpy.linalg.logdet->np.where(is_zero, np.array(-np.inf, dtype=dtype), np.sum(np.log(np.abs(diag)), axis=-1))
A:jax.numpy.linalg.(sign, logdet)->slogdet(a)
A:jax.numpy.linalg.(w, vl, vr)->lax_linalg.eig(a)
A:jax.numpy.linalg.(w, _)->eigh(a, UPLO)
A:jax.numpy.linalg.msg->"UPLO must be one of None, 'L', or 'U', got {}".format(UPLO)
A:jax.numpy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input)
A:jax.numpy.linalg.max_rows_cols->max(a.shape[-2:])
A:jax.numpy.linalg.rcond->np.asarray(rcond)
A:jax.numpy.linalg.(u, s, v)->svd(a, full_matrices=False)
A:jax.numpy.linalg.s->np.where(large, s, 0)
A:jax.numpy.linalg.vT->np.swapaxes(v, -1, -2)
A:jax.numpy.linalg.uT->np.swapaxes(u, -1, -2)
A:jax.numpy.linalg.res->np.matmul(vT, np.multiply(s[..., np.newaxis], uT))
A:jax.numpy.linalg.x->np.moveaxis(x, axis, (-2, -1))
A:jax.numpy.linalg.x_shape->np.shape(x)
A:jax.numpy.linalg.ndim->len(x_shape)
A:jax.numpy.linalg.axis->tuple((np._canonicalize_axis(x, ndim) for x in axis))
A:jax.numpy.linalg.num_axes->len(axis)
A:jax.numpy.linalg.abs_x->np.abs(x)
A:jax.numpy.linalg.ord->lax._const(abs_x, ord)
A:jax.numpy.linalg.out->np.sum(abs_x ** ord, axis=axis, keepdims=keepdims)
A:jax.numpy.linalg.(row_axis, col_axis)->cast(Tuple[int, ...], axis)
A:jax.numpy.linalg.y->np.reshape(y, result_shape)
A:jax.numpy.linalg.result_shape->list(x_shape)
A:jax.numpy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.numpy.linalg.(a, b)->_promote_arg_dtypes(np.asarray(a), np.asarray(b))
A:jax.numpy.linalg.(lu, pivots)->lax.stop_gradient(lax_linalg.lu)(a)
A:jax.numpy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: _matvec_multiply(a, x), solve=lambda _, x: lax_linalg.lu_solve(lu, pivots, x, trans=0), transpose_solve=lambda _, x: lax_linalg.lu_solve(lu, pivots, x, trans=1))
A:jax.numpy.linalg.globals()[func.__name__]->_not_implemented(func)
jax.numpy.linalg._check_solve_shapes(a,b)
jax.numpy.linalg._jvp_slogdet(g,ans,x)
jax.numpy.linalg._matvec_multiply(a,b)
jax.numpy.linalg._norm(x,ord,axis:Union[None,Tuple[int,...],int],keepdims)
jax.numpy.linalg._promote_arg_dtypes(*args)
jax.numpy.linalg.cholesky(a)
jax.numpy.linalg.det(a)
jax.numpy.linalg.eig(a)
jax.numpy.linalg.eigh(a,UPLO=None,symmetrize_input=True)
jax.numpy.linalg.eigvals(a)
jax.numpy.linalg.eigvalsh(a,UPLO='L')
jax.numpy.linalg.inv(a)
jax.numpy.linalg.matrix_power(a,n)
jax.numpy.linalg.matrix_rank(M,tol=None)
jax.numpy.linalg.norm(x,ord=None,axis=None,keepdims=False)
jax.numpy.linalg.pinv(a,rcond=None)
jax.numpy.linalg.qr(a,mode='reduced')
jax.numpy.linalg.slogdet(a)
jax.numpy.linalg.solve(a,b)
jax.numpy.linalg.svd(a,full_matrices=True,compute_uv=True)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/numpy/lax_numpy.py----------------------------------------
A:jax.numpy.lax_numpy.bool_->_make_scalar_type(onp.bool_)
A:jax.numpy.lax_numpy.uint8->_make_scalar_type(onp.uint8)
A:jax.numpy.lax_numpy.uint16->_make_scalar_type(onp.uint16)
A:jax.numpy.lax_numpy.uint32->_make_scalar_type(onp.uint32)
A:jax.numpy.lax_numpy.uint64->_make_scalar_type(onp.uint64)
A:jax.numpy.lax_numpy.int8->_make_scalar_type(onp.int8)
A:jax.numpy.lax_numpy.int16->_make_scalar_type(onp.int16)
A:jax.numpy.lax_numpy.int32->_make_scalar_type(onp.int32)
A:jax.numpy.lax_numpy.int64->_make_scalar_type(onp.int64)
A:jax.numpy.lax_numpy.bfloat16->_make_scalar_type(dtypes.bfloat16)
A:jax.numpy.lax_numpy.float16->_make_scalar_type(onp.float16)
A:jax.numpy.lax_numpy.float32single->_make_scalar_type(onp.float32)
A:jax.numpy.lax_numpy.float64double->_make_scalar_type(onp.float64)
A:jax.numpy.lax_numpy.complex64csingle->_make_scalar_type(onp.complex64)
A:jax.numpy.lax_numpy.complex128cdouble->_make_scalar_type(onp.complex128)
A:jax.numpy.lax_numpy.result_rank->len(lax.broadcast_shapes(*shapes))
A:jax.numpy.lax_numpy.to_dtype->_to_inexact_dtype(result_type(*args))
A:jax.numpy.lax_numpy.(pos, arg)->next(((i, arg) for (i, arg) in enumerate(args) if not _arraylike(arg)))
A:jax.numpy.lax_numpy.lines->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).split('\n')
A:jax.numpy.lax_numpy.lines[idx]->line.replace('    ', '', 1)
A:jax.numpy.lax_numpy.docstr->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body)
A:jax.numpy.lax_numpy.begin_idx->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).find('Parameters')
A:jax.numpy.lax_numpy.end_idx->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).find('Returns', begin_idx)
A:jax.numpy.lax_numpy.param_list->'\n'.join(param_list).replace('@@', '\n    ').replace('\n    ', '@@').split('\n')
A:jax.numpy.lax_numpy.parameters->'\n'.join(param_list).replace('@@', '\n    ')
A:jax.numpy.lax_numpy._numpy_signature_re->lax.real(x).compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*\\)$')
A:jax.numpy.lax_numpy.sections->fun.__doc__.split('\n\n')
A:jax.numpy.lax_numpy.summary->sections[i].strip()
A:jax.numpy.lax_numpy.body->update_numpydoc(body, fun, op)
A:jax.numpy.lax_numpy.axis->_canonicalize_axis(axis, ndim(a))
A:jax.numpy.lax_numpy.x->remainder(x, a_shape[i] or 1)
A:jax.numpy.lax_numpy.(x1, x2)->_promote_dtypes(x1, x2)
A:jax.numpy.lax_numpy.absoluteabs->_one_to_one_unop(onp.absolute, lax.abs)
A:jax.numpy.lax_numpy.fabs->_one_to_one_unop(onp.fabs, lax.abs, True)
A:jax.numpy.lax_numpy.bitwise_not->_one_to_one_unop(onp.bitwise_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.negative->_one_to_one_unop(onp.negative, lax.neg)
A:jax.numpy.lax_numpy.positive->_one_to_one_unop(onp.positive, lambda x: x)
A:jax.numpy.lax_numpy.floor->_one_to_one_unop(onp.floor, lax.floor, True)
A:jax.numpy.lax_numpy.ceil->_one_to_one_unop(onp.ceil, lax.ceil, True)
A:jax.numpy.lax_numpy.exp->_one_to_one_unop(onp.exp, lax.exp, True)
A:jax.numpy.lax_numpy.log->_one_to_one_unop(onp.log, lax.log, True)
A:jax.numpy.lax_numpy.expm1->_one_to_one_unop(onp.expm1, lax.expm1, True)
A:jax.numpy.lax_numpy.log1p->_one_to_one_unop(onp.log1p, lax.log1p, True)
A:jax.numpy.lax_numpy.sin->_one_to_one_unop(onp.sin, lax.sin, True)
A:jax.numpy.lax_numpy.cos->_one_to_one_unop(onp.cos, lax.cos, True)
A:jax.numpy.lax_numpy.tan->_one_to_one_unop(onp.tan, lax.tan, True)
A:jax.numpy.lax_numpy.arcsin->_one_to_one_unop(onp.arcsin, lax.asin, True)
A:jax.numpy.lax_numpy.arccos->_one_to_one_unop(onp.arccos, lax.acos, True)
A:jax.numpy.lax_numpy.arctan->_one_to_one_unop(onp.arctan, lax.atan, True)
A:jax.numpy.lax_numpy.sinh->_one_to_one_unop(onp.sinh, lax.sinh, True)
A:jax.numpy.lax_numpy.cosh->_one_to_one_unop(onp.cosh, lax.cosh, True)
A:jax.numpy.lax_numpy.tanh->_one_to_one_unop(onp.tanh, lax.tanh, True)
A:jax.numpy.lax_numpy.arcsinh->_one_to_one_unop(onp.arcsinh, lax.asinh, True)
A:jax.numpy.lax_numpy.arccosh->_one_to_one_unop(onp.arccosh, lax.acosh, True)
A:jax.numpy.lax_numpy.arctanh->_one_to_one_unop(onp.arctanh, lax.atanh, True)
A:jax.numpy.lax_numpy.sqrt->_one_to_one_unop(onp.sqrt, lax.sqrt, True)
A:jax.numpy.lax_numpy.add->_maybe_bool_binop(onp.add, lax.add, lax.bitwise_or)
A:jax.numpy.lax_numpy.bitwise_and->_one_to_one_binop(onp.bitwise_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.bitwise_or->_one_to_one_binop(onp.bitwise_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.bitwise_xor->_one_to_one_binop(onp.bitwise_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.right_shift->_one_to_one_binop(onp.right_shift, lax.shift_right_arithmetic)
A:jax.numpy.lax_numpy.left_shift->_one_to_one_binop(onp.left_shift, lax.shift_left)
A:jax.numpy.lax_numpy.equal->_one_to_one_binop(onp.equal, lax.eq)
A:jax.numpy.lax_numpy.multiply->_maybe_bool_binop(onp.multiply, lax.mul, lax.bitwise_and)
A:jax.numpy.lax_numpy.not_equal->_one_to_one_binop(onp.not_equal, lax.ne)
A:jax.numpy.lax_numpy.subtract->_one_to_one_binop(onp.subtract, lax.sub)
A:jax.numpy.lax_numpy.arctan2->_one_to_one_binop(onp.arctan2, lax.atan2, True)
A:jax.numpy.lax_numpy.minimum->_one_to_one_binop(onp.minimum, lax.min)
A:jax.numpy.lax_numpy.maximum->_one_to_one_binop(onp.maximum, lax.max)
A:jax.numpy.lax_numpy.float_power->_one_to_one_binop(onp.float_power, lax.pow, True)
A:jax.numpy.lax_numpy.nextafter->_one_to_one_binop(onp.nextafter, lax.nextafter, True)
A:jax.numpy.lax_numpy.rx->lax.real(x1)
A:jax.numpy.lax_numpy.ry->lax.real(x2)
A:jax.numpy.lax_numpy.greater_equal->_comparison_op(onp.greater_equal, lax.ge)
A:jax.numpy.lax_numpy.greater->_comparison_op(onp.greater, lax.gt)
A:jax.numpy.lax_numpy.less_equal->_comparison_op(onp.less_equal, lax.le)
A:jax.numpy.lax_numpy.less->_comparison_op(onp.less, lax.lt)
A:jax.numpy.lax_numpy.logical_and->_logical_op(onp.logical_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.logical_not->_logical_op(onp.logical_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.logical_or->_logical_op(onp.logical_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.logical_xor->_logical_op(onp.logical_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.dtype->_dtype(x)
A:jax.numpy.lax_numpy.re->lax.real(x)
A:jax.numpy.lax_numpy.result_dtype->_result_dtype(onp.divide, x1, x2)
A:jax.numpy.lax_numpy.quotient->lax.div(x1, x2)
A:jax.numpy.lax_numpy.select->logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
A:jax.numpy.lax_numpy.x1r->lax.real(x1)
A:jax.numpy.lax_numpy.x1i->lax.imag(x1)
A:jax.numpy.lax_numpy.x2r->lax.real(x2)
A:jax.numpy.lax_numpy.x2i->lax.imag(x2)
A:jax.numpy.lax_numpy.which->lax.ge(lax.abs(x2r), lax.abs(x2i))
A:jax.numpy.lax_numpy.rat1->where(which, lax._const(x2i, 1), lax.div(x2r, x2i))
A:jax.numpy.lax_numpy.rat2->where(which, lax.div(x2i, x2r), lax._const(x2i, 1))
A:jax.numpy.lax_numpy.out->round(number, decimals=ndigits or 0)
A:jax.numpy.lax_numpy.mod->lax.select(ind, mod + x2, mod)
A:jax.numpy.lax_numpy.div->lax.select(ind, div - _constant_like(div, 1), div)
A:jax.numpy.lax_numpy.ind->lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
A:jax.numpy.lax_numpy.acc->where(lax.bitwise_and(x2, _constant_like(x2, 1)), lax.mul(acc, x1), acc)
A:jax.numpy.lax_numpy.x1->lax.mul(x1, x1)
A:jax.numpy.lax_numpy.x2->lax.shift_right_logical(x2, _constant_like(x2, 1))
A:jax.numpy.lax_numpy.amax->lax.max(x1, x2)
A:jax.numpy.lax_numpy.delta->lax.sub(x1, x2)
A:jax.numpy.lax_numpy.(x,)->_promote_dtypes_inexact(x)
A:jax.numpy.lax_numpy.info->finfo(dtypes.canonicalize_dtype(dtype))
A:jax.numpy.lax_numpy.zero->lax._const(x, 0)
A:jax.numpy.lax_numpy.trunc_mod->lax.rem(x1, x2)
A:jax.numpy.lax_numpy.trunc_mod_not_zero->lax.ne(trunc_mod, zero)
A:jax.numpy.lax_numpy.do_plus->lax.bitwise_and(lax.ne(lax.lt(trunc_mod, zero), lax.lt(x2, zero)), trunc_mod_not_zero)
A:jax.numpy.lax_numpy.fmod->_wraps(onp.fmod)(lambda x1, x2: lax.rem(x1, x2))
A:jax.numpy.lax_numpy.eq_zero->lax.eq(x, lax._const(x, 0))
A:jax.numpy.lax_numpy.safe_x->where(eq_zero, lax._const(x, 0), x)
A:jax.numpy.lax_numpy.pi_x->lax.mul(lax._const(x, pi), safe_x)
A:jax.numpy.lax_numpy.ax1->_canonicalize_axis(ax1, m.ndim)
A:jax.numpy.lax_numpy.ax2->_canonicalize_axis(ax2, m.ndim)
A:jax.numpy.lax_numpy.perm->tuple([names.index(name) for name in result_names])
A:jax.numpy.lax_numpy.i->array(i)
A:jax.numpy.lax_numpy.im->lax.imag(x)
A:jax.numpy.lax_numpy.slice1[axis]->slice(1, None)
A:jax.numpy.lax_numpy.slice2[axis]->slice(None, -1)
A:jax.numpy.lax_numpy.slice1->tuple(slice1)
A:jax.numpy.lax_numpy.slice2->tuple(slice2)
A:jax.numpy.lax_numpy.a->lax.sort(a, dimension=axis)
A:jax.numpy.lax_numpy.sliced->partial(lax.slice_in_dim, a, axis=axis)
A:jax.numpy.lax_numpy.a_grad->concatenate((sliced(1, 2) - sliced(0, 1), (sliced(2, None) - sliced(0, -2)) * 0.5, sliced(-1, None) - sliced(-2, -1)), axis)
A:jax.numpy.lax_numpy.newsize->_prod(newshape)
A:jax.numpy.lax_numpy.computed_newshape->_compute_newshape(a, newshape)
A:jax.numpy.lax_numpy.order->kwargs.pop('order', 'C')
A:jax.numpy.lax_numpy.invalid_kwargs->"'{}'".format("'".join(kwargs))
A:jax.numpy.lax_numpy.shape_a->shape(a)
A:jax.numpy.lax_numpy.shape->tuple(map(int, shape))
A:jax.numpy.lax_numpy.source->tuple((_canonicalize_axis(i, ndim(a)) for i in source))
A:jax.numpy.lax_numpy.destination->tuple((_canonicalize_axis(i, ndim(a)) for i in destination))
A:jax.numpy.lax_numpy.(a, b)->_promote_dtypes(a, b)
A:jax.numpy.lax_numpy.rtol->lax.convert_element_type(rtol, dtype)
A:jax.numpy.lax_numpy.atol->lax.convert_element_type(atol, dtype)
A:jax.numpy.lax_numpy.numpy_version->tuple(map(int, onp.version.version.split('.')[:2]))
A:jax.numpy.lax_numpy.condition->lax.ne(condition, zeros_like(condition))
A:jax.numpy.lax_numpy.(x, y)->_promote_dtypes(x, y)
A:jax.numpy.lax_numpy.(condition, x, y)->broadcast_arrays(condition, x, y)
A:jax.numpy.lax_numpy.choices->_promote_dtypes(default, *choicelist)
A:jax.numpy.lax_numpy.output->where(cond, choice, output)
A:jax.numpy.lax_numpy.result_shape->lax.broadcast_shapes(*shapes)
A:jax.numpy.lax_numpy.arr_shape->replace(arr.shape, 1)
A:jax.numpy.lax_numpy.(diff,)->numpy.where(onp.not_equal(shape[nlead:], arr_shape))
A:jax.numpy.lax_numpy.kept_dims->tuple(onp.delete(onp.arange(len(shape)), new_dims))
A:jax.numpy.lax_numpy.dummy_val->numpy.broadcast_to(0, ary.shape)
A:jax.numpy.lax_numpy.subarrays->numpy.split(dummy_val, indices_or_sections, axis)
A:jax.numpy.lax_numpy.split_indices->numpy.cumsum([0] + [onp.shape(sub)[axis] for sub in subarrays])
A:jax.numpy.lax_numpy.vsplit->_split_on_axis(onp.vsplit, axis=0)
A:jax.numpy.lax_numpy.hsplit->_split_on_axis(onp.hsplit, axis=1)
A:jax.numpy.lax_numpy.dsplit->_split_on_axis(onp.dsplit, axis=2)
A:jax.numpy.lax_numpy.a_min->lax.convert_element_type(a_min, _dtype(a))
A:jax.numpy.lax_numpy.a_max->lax.convert_element_type(a_max, _dtype(a))
A:jax.numpy.lax_numpy.half->lax._const(x, 0.5)
A:jax.numpy.lax_numpy.one->lax._const(x, 1)
A:jax.numpy.lax_numpy.round_val->lax.floor(x)
A:jax.numpy.lax_numpy.nearest_even_int->lax.sub(round_val, lax.mul(lax._const(x, 2), lax.floor(lax.mul(half, x))))
A:jax.numpy.lax_numpy.is_odd->lax.eq(nearest_even_int, one)
A:jax.numpy.lax_numpy.factor->_constant_like(x, 10 ** decimals)
A:jax.numpy.lax_numpy.isposinf->_wraps(onp.isposinf)(partial(_isposneginf, inf))
A:jax.numpy.lax_numpy.isneginf->_wraps(onp.isneginf)(partial(_isposneginf, -inf))
A:jax.numpy.lax_numpy.dims->shape(a)
A:jax.numpy.lax_numpy.computation_dtype->promote_types(dtype, float32)
A:jax.numpy.lax_numpy.result->lax.dot_general(a, b, dim_numbers, precision)
A:jax.numpy.lax_numpy.shape_with_singletons->subvals(shape(a), zip(dims, (1,) * len(dims)))
A:jax.numpy.lax_numpy.a_dtype->promote_types(a_dtype, float32)
A:jax.numpy.lax_numpy._cast_to_bool->partial(lax.convert_element_type, new_dtype=bool_)
A:jax.numpy.lax_numpy.sum->_make_reduction(onp.sum, lax.add, 0, upcast_f16_for_computation=True, bool_op=lax.bitwise_or)
A:jax.numpy.lax_numpy.productprod->_make_reduction(onp.prod, lax.mul, 1, bool_op=lax.bitwise_and, upcast_f16_for_computation=True)
A:jax.numpy.lax_numpy.amaxmax->_make_reduction(onp.max, lax.max, -onp.inf)
A:jax.numpy.lax_numpy.aminmin->_make_reduction(onp.min, lax.min, onp.inf)
A:jax.numpy.lax_numpy.allalltrue->_make_reduction(onp.all, lax.bitwise_and, True, _cast_to_bool)
A:jax.numpy.lax_numpy.anysometrue->_make_reduction(onp.any, lax.bitwise_or, False, _cast_to_bool)
A:jax.numpy.lax_numpy.normalizer->lax.convert_element_type(normalizer, dtype)
A:jax.numpy.lax_numpy.avg->mean(a, axis=axis)
A:jax.numpy.lax_numpy.weights_sum->broadcast_to(weights_sum, avg.shape)
A:jax.numpy.lax_numpy.weights->moveaxis(weights, -1, axis)
A:jax.numpy.lax_numpy.out_dtype->dtypes.canonicalize_dtype(out_dtype)
A:jax.numpy.lax_numpy.a_shape->shape(a)
A:jax.numpy.lax_numpy.a_ndim->len(a_shape)
A:jax.numpy.lax_numpy.weights_shape->shape(weights)
A:jax.numpy.lax_numpy.a_mean->mean(a, axis, dtype=a_dtype, keepdims=True)
A:jax.numpy.lax_numpy.centered->lax.square(centered)
A:jax.numpy.lax_numpy.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.numpy.lax_numpy.ndims->len(dims)
A:jax.numpy.lax_numpy.d->diag(c)
A:jax.numpy.lax_numpy.nanmin->_make_nan_reduction(onp.nanmin, min, inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nanmax->_make_nan_reduction(onp.nanmax, max, -inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nansum->_make_nan_reduction(onp.nansum, sum, 0, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.nanprod->_make_nan_reduction(onp.nanprod, prod, 1, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.nan_mask->logical_not(isnan(a))
A:jax.numpy.lax_numpy.td->lax.div(nansum(a, axis, dtype=dtype, keepdims=keepdims), normalizer)
A:jax.numpy.lax_numpy.num_dims->len(a_shape)
A:jax.numpy.lax_numpy.cumsum->_make_cumulative_reduction(onp.cumsum, lax._reduce_window_sum, 0, squash_nan=False)
A:jax.numpy.lax_numpy.cumprod->_make_cumulative_reduction(onp.cumprod, lax._reduce_window_prod, 1, squash_nan=False)
A:jax.numpy.lax_numpy.nancumsum->_make_cumulative_reduction(onp.nancumsum, lax._reduce_window_sum, 0, squash_nan=True)
A:jax.numpy.lax_numpy.nancumprod->_make_cumulative_reduction(onp.nancumprod, lax._reduce_window_prod, 1, squash_nan=True)
A:jax.numpy.lax_numpy.nd->ndim(array)
A:jax.numpy.lax_numpy.constant_values->lax.convert_element_type(constant_values, array.dtype)
A:jax.numpy.lax_numpy.array->asarray(array)
A:jax.numpy.lax_numpy.(repeats, (left_remainder, right_remainder))->_divmod(pad_width[i], size)
A:jax.numpy.lax_numpy.rarray->lax.rev(array, dimensions=(i,))
A:jax.numpy.lax_numpy.parts->reversed(build_padding(pad_width[i, 0], forward=True))
A:jax.numpy.lax_numpy.edge_before->lax.slice_in_dim(array, 0, 1, axis=i)
A:jax.numpy.lax_numpy.pad_before->repeat(edge_before, npad_before, axis=i)
A:jax.numpy.lax_numpy.edge_after->lax.slice_in_dim(array, n - 1, n, axis=i)
A:jax.numpy.lax_numpy.pad_after->repeat(edge_after, npad_after, axis=i)
A:jax.numpy.lax_numpy.pad_width->tuple(pad_width)
A:jax.numpy.lax_numpy.shape0->shape(arrays[0])
A:jax.numpy.lax_numpy.new_shape->list(shape0)
A:jax.numpy.lax_numpy.arrays->_promote_dtypes(*arrays)
A:jax.numpy.lax_numpy.arr->asarray(arr)
A:jax.numpy.lax_numpy.m->ndim(x)
A:jax.numpy.lax_numpy.(xs, depths)->unzip2([_block(x) for x in xs])
A:jax.numpy.lax_numpy.rank->ndim(arr)
A:jax.numpy.lax_numpy.(out, _)->_block(arrays)
A:jax.numpy.lax_numpy.view->memoryview(object)
A:jax.numpy.lax_numpy.k->int(k)
A:jax.numpy.lax_numpy.k_dtype->_dtype(k)
A:jax.numpy.lax_numpy.dt->result_type(start, stop, float(num))
A:jax.numpy.lax_numpy.bounds_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax.numpy.lax_numpy.broadcast_start->broadcast_to(start, bounds_shape)
A:jax.numpy.lax_numpy.empty_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax.numpy.lax_numpy.start->asarray(start, dtype=computation_dtype)
A:jax.numpy.lax_numpy.stop->asarray(stop, dtype=computation_dtype)
A:jax.numpy.lax_numpy.lin->linspace(start, stop, num, endpoint=endpoint, retstep=False, dtype=None, axis=axis)
A:jax.numpy.lax_numpy.res->moveaxis(res, 0, axis)
A:jax.numpy.lax_numpy.indexing->kwargs.get('indexing', 'xy')
A:jax.numpy.lax_numpy.sparse->kwargs.get('sparse', False)
A:jax.numpy.lax_numpy.copy->kwargs.get('copy', True)
A:jax.numpy.lax_numpy.args->list(args)
A:jax.numpy.lax_numpy.args[i]a->asarray(a)
A:jax.numpy.lax_numpy.s->list(s)
A:jax.numpy.lax_numpy.n->len(args)
A:jax.numpy.lax_numpy.broadcast_shape->list(a_shape)
A:jax.numpy.lax_numpy.broadcast_dims->numpy.concatenate((onp.arange(0, axis + 1), onp.arange(axis + 2, num_dims + 1)))
A:jax.numpy.lax_numpy.repeats_raveled->ravel(array(repeats))
A:jax.numpy.lax_numpy.total->sum(repeats_raveled)
A:jax.numpy.lax_numpy.a_flattened->ravel(a)
A:jax.numpy.lax_numpy.chunks->product(a_shape[:axis + 1]).item()
A:jax.numpy.lax_numpy.a_splitted->split(a_flattened, chunks)
A:jax.numpy.lax_numpy.repeats_tiled->tile(repeats_raveled, chunks // len(repeats_raveled))
A:jax.numpy.lax_numpy.ret->concatenate((ret, tile(a_splitted[i], repeat)))
A:jax.numpy.lax_numpy.repeat->repeat.item().item()
A:jax.numpy.lax_numpy.m_shape->shape(m)
A:jax.numpy.lax_numpy.mask->tri(*m_shape[-2:], k=k - 1, dtype=bool)
A:jax.numpy.lax_numpy.axis1->_canonicalize_axis(axis1, a_ndims)
A:jax.numpy.lax_numpy.axis2->_canonicalize_axis(axis2, a_ndims)
A:jax.numpy.lax_numpy.default_int->dtypes.canonicalize_dtype(onp.int_)
A:jax.numpy.lax_numpy.tril_indices->_wrap_indices_function(onp.tril_indices)
A:jax.numpy.lax_numpy.triu_indices->_wrap_indices_function(onp.triu_indices)
A:jax.numpy.lax_numpy.mask_indices->_wrap_indices_function(onp.mask_indices)
A:jax.numpy.lax_numpy.a_ndims->len(a_shape)
A:jax.numpy.lax_numpy.diag_size->_max(0, _min(a_shape[axis1] + _min(offset, 0), a_shape[axis2] - _max(offset, 0)))
A:jax.numpy.lax_numpy.v_shape->shape(v)
A:jax.numpy.lax_numpy.v->lax.pad(v, zero(v), ((_max(0, k), _max(0, -k), 0),))
A:jax.numpy.lax_numpy.p->numpy.asarray(p)
A:jax.numpy.lax_numpy.batch_shape->lax.broadcast_shapes(shape(a)[:-2], shape(b)[:-2])
A:jax.numpy.lax_numpy.b->reshape(b, (1,) * (ndim(a) - ndim(b)) + shape(b))
A:jax.numpy.lax_numpy.batch_dims->tuple(lhs_batch)
A:jax.numpy.lax_numpy.b_ndim->ndim(b)
A:jax.numpy.lax_numpy.optimize->kwargs.pop('optimize', 'greedy')
A:jax.numpy.lax_numpy.precision->kwargs.pop('precision', None)
A:jax.numpy.lax_numpy.(operands, contractions)->opt_einsum.contract_path(*operands, einsum_call=True, use_blas=True, optimize=optimize)
A:jax.numpy.lax_numpy.contractions->tuple((data[:3] for data in contractions))
A:jax.numpy.lax_numpy.operands->list(_promote_dtypes(*operands))
A:jax.numpy.lax_numpy.operand->lax.transpose(operand, perm)
A:jax.numpy.lax_numpy.names->names.replace(name, '', count - 1).replace(name, '', count - 1)
A:jax.numpy.lax_numpy.eye->lax._delta(operand.dtype, operand.shape, axes)
A:jax.numpy.lax_numpy.(input_str, result_names)->einstr.split('->')
A:jax.numpy.lax_numpy.input_names->input_str.split(',')
A:jax.numpy.lax_numpy.counts->collections.Counter(names)
A:jax.numpy.lax_numpy.(operand, names)->sum_repeats(operand, names, counts, result_names)
A:jax.numpy.lax_numpy.(lhs, rhs)->map(operands.pop, operand_indices)
A:jax.numpy.lax_numpy.(lhs_counts, rhs_counts)->map(collections.Counter, input_names)
A:jax.numpy.lax_numpy.(lhs, lhs_names)->sum_repeats(lhs, lhs_names, lhs_counts, result_names + rhs_names)
A:jax.numpy.lax_numpy.(rhs, rhs_names)->sum_repeats(rhs, rhs_names, rhs_counts, result_names + lhs_names)
A:jax.numpy.lax_numpy.(lhs_batch, rhs_batch)->unzip2(((lhs_names.find(n), rhs_names.find(n)) for n in batch_names))
A:jax.numpy.lax_numpy.lhs->moveaxis(lhs, lhs_batch, batch_dims)
A:jax.numpy.lax_numpy.lhs_names->_movechars(lhs_names, lhs_batch, batch_dims)
A:jax.numpy.lax_numpy.rhs->moveaxis(rhs, rhs_batch, batch_dims)
A:jax.numpy.lax_numpy.rhs_names->_movechars(rhs_names, rhs_batch, batch_dims)
A:jax.numpy.lax_numpy.batch_names->''.join((lhs_names[i] for i in range(len(lhs_names)) if i in batch_dims))
A:jax.numpy.lax_numpy.(lhs_cont, rhs_cont)->unzip2(((lhs_names.index(n), rhs_names.index(n)) for n in contracted_names))
A:jax.numpy.lax_numpy.bdims->tuple(range(len(batch_dims)))
A:jax.numpy.lax_numpy.c->lax.complex(real_part, complex_part)
A:jax.numpy.lax_numpy.a_reshaped->reshape(a, [i for d in shape(a) for i in (d, 1)])
A:jax.numpy.lax_numpy.b_reshaped->reshape(b, [i for d in shape(b) for i in (1, d)])
A:jax.numpy.lax_numpy.out_shape->lax.broadcast_shapes(idx_shape, arr_shape)
A:jax.numpy.lax_numpy.x_shape->shape(x)
A:jax.numpy.lax_numpy.iota->lax.broadcast_in_dim(iota, gather_index_shape, (j,))
A:jax.numpy.lax_numpy.idxs->lax.tie_in(a, arange(a.shape[axis])).reshape(shape)
A:jax.numpy.lax_numpy.maxval->lax.tie_in(a, maxval)
A:jax.numpy.lax_numpy.mask_idxs->where(lax._eq_meet(a, op(a, axis, keepdims=True)), idxs, maxval)
A:jax.numpy.lax_numpy.(_, perm)->lax.sort_key_val(a, iota, dimension=axis)
A:jax.numpy.lax_numpy.shift->asarray(shift)
A:jax.numpy.lax_numpy.b_shape->lax.broadcast_shapes(shift.shape, axis.shape, (1,))
A:jax.numpy.lax_numpy.indices->_normalize_index(indices, axis_size)
A:jax.numpy.lax_numpy.index_dims->len(shape(indices))
A:jax.numpy.lax_numpy.slice_sizes->list(a_shape)
A:jax.numpy.lax_numpy.dnums->lax.GatherDimensionNumbers(offset_dims=tuple(range(q_ndim, len(a_shape) + q_ndim if keepdims else len(a_shape) + q_ndim - 1)), collapsed_slice_dims=() if keepdims else (axis,), start_index_map=(axis,))
A:jax.numpy.lax_numpy.lst->list(tup)
A:jax.numpy.lax_numpy.bcast_shape->lax.broadcast_shapes(replace(arr.shape, 1), replace(indices.shape, 1))
A:jax.numpy.lax_numpy.gather_indices->concatenate((gather_indices, i), len(gather_indices_shape))
A:jax.numpy.lax_numpy.(treedef, static_idx, dynamic_idx)->_split_index_for_jit(idx)
A:jax.numpy.lax_numpy.idx->tuple(idx)
A:jax.numpy.lax_numpy.indexer->_index_to_gather(shape(arr), idx)
A:jax.numpy.lax_numpy._Indexer->collections.namedtuple('_Indexer', ['slice_shape', 'gather_slice_shape', 'gather_indices', 'dnums', 'reversed_y_dims', 'newaxis_dims'])
A:jax.numpy.lax_numpy.(leaves, treedef)->lib.pytree.flatten(idx)
A:jax.numpy.lax_numpy.(advanced_indexes, idx_advanced_axes, x_advanced_axes)->zip(*advanced_pairs)
A:jax.numpy.lax_numpy.advanced_axes_are_contiguous->numpy.all(onp.diff(idx_advanced_axes) == 1)
A:jax.numpy.lax_numpy.advanced_indexes->broadcast_arrays(*advanced_indexes)
A:jax.numpy.lax_numpy.ndim->len(shape)
A:jax.numpy.lax_numpy.abstract_i->core.get_aval(i)
A:jax.numpy.lax_numpy.(start, limit, stride, needs_rev)->_static_idx(i, x_shape[x_axis])
A:jax.numpy.lax_numpy.len_without_none->_sum((1 for e in idx if e is not None and e is not Ellipsis))
A:jax.numpy.lax_numpy.ellipsis_index->next(ellipses, None)
A:jax.numpy.lax_numpy.(start, stop, step)->tuple(idx).indices(size)
A:jax.numpy.lax_numpy.blackman->_wrap_numpy_nullary_function(onp.blackman)
A:jax.numpy.lax_numpy.bartlett->_wrap_numpy_nullary_function(onp.bartlett)
A:jax.numpy.lax_numpy.hamming->_wrap_numpy_nullary_function(onp.hamming)
A:jax.numpy.lax_numpy.hanning->_wrap_numpy_nullary_function(onp.hanning)
A:jax.numpy.lax_numpy.kaiser->_wrap_numpy_nullary_function(onp.kaiser)
A:jax.numpy.lax_numpy.(gcd, _)->lax.while_loop(_gcd_cond_fn, _gcd_body_fn, (lax.abs(x1), lax.abs(x2)))
A:jax.numpy.lax_numpy.X->array(m, ndmin=2, dtype=dtypes.canonicalize_dtype(result_type(m, float_)))
A:jax.numpy.lax_numpy.w->asarray(fweights)
A:jax.numpy.lax_numpy.(avg, w_sum)->average(X, axis=1, weights=w, returned=True)
A:jax.numpy.lax_numpy.stddev->sqrt(real(d))
A:jax.numpy.lax_numpy.real_part->clip(real(c), -1, 1)
A:jax.numpy.lax_numpy.complex_part->clip(imag(c), -1, 1)
A:jax.numpy.lax_numpy.q_ndim->ndim(q)
A:jax.numpy.lax_numpy.q->true_divide(asarray(q), float32(100.0))
A:jax.numpy.lax_numpy.low->lax.convert_element_type(low, int64)
A:jax.numpy.lax_numpy.high->lax.convert_element_type(high, int64)
A:jax.numpy.lax_numpy.high_weight->lax.broadcast_in_dim(high_weight, high_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_weight->lax.broadcast_in_dim(low_weight, low_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_value->lax.gather(a, low, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.high_value->lax.gather(a, high, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.globals()[func.__name__]->_not_implemented(func)
jax.numpy._ArrayMeta(type(onp.ndarray))
jax.numpy._ArrayMeta.__instancecheck__(self,instance)
jax.numpy._ScalarMeta(self,x)
jax.numpy._ScalarMeta.__eq__(self,other)
jax.numpy._ScalarMeta.__hash__(self)
jax.numpy._ScalarMeta.__ne__(self,other)
jax.numpy._argminmax(op,a,axis)
jax.numpy._arraylike(x)
jax.numpy._astype(arr,dtype)
jax.numpy._atleast_nd(x,n)
jax.numpy._block(xs)
jax.numpy._canonicalize_axis(axis,num_dims)
jax.numpy._canonicalize_tuple_index(arr_ndim,idx)
jax.numpy._check_arraylike(fun_name,*args)
jax.numpy._check_no_padding(axis_padding,mode)
jax.numpy._comparison_op(numpy_fn,lax_fn)
jax.numpy._complex_elem_type(dtype)
jax.numpy._compute_newshape(a,newshape)
jax.numpy._constant_like(x,const)
jax.numpy._cross(a,b,axisa,axisb,axisc)
jax.numpy._defer_to_unrecognized_arg(binary_op)
jax.numpy._dtype_info(dtype)
jax.numpy._einsum(operands,contractions,precision)
jax.numpy._eliminate_deprecated_list_indexing(idx)
jax.numpy._expand_bool_indices(idx)
jax.numpy._float_divmod(x1,x2)
jax.numpy._gather(arr,treedef,static_idx,dynamic_idx)
jax.numpy._gcd_body_fn(xs)
jax.numpy._gcd_cond_fn(xs)
jax.numpy._gradient(a,axis)
jax.numpy._index_to_gather(x_shape,idx)
jax.numpy._int(aval)
jax.numpy._is_advanced_int_indexer(idx)
jax.numpy._is_int_arraylike(x)
jax.numpy._is_slice_none(idx)
jax.numpy._isposneginf(infinity,x)
jax.numpy._logical_op(np_op,bitwise_op)
jax.numpy._make_cumulative_reduction(onp_reduction,window_reduce,init_val,squash_nan=False)
jax.numpy._make_nan_reduction(onp_reduction,np_reduction,init_val,nan_if_all_nan)
jax.numpy._make_reduction(np_fun,op,init_val,preproc=None,bool_op=None,upcast_f16_for_computation=False)
jax.numpy._make_scalar_type(onp_scalar_type)
jax.numpy._maybe_bool_binop(numpy_fn,lax_fn,bool_lax_fn)
jax.numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax.numpy._movechars(s,src,dst)
jax.numpy._normalize_index(index,axis_size)
jax.numpy._not_implemented(fun)
jax.numpy._one_to_one_binop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy._one_to_one_unop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy._operator_round(number,ndigits=None)
jax.numpy._pad(array,pad_width,mode,constant_values)
jax.numpy._pad_constant(array,pad_width,constant_values)
jax.numpy._pad_edge(array,pad_width)
jax.numpy._pad_symmetric_or_reflect(array,pad_width,mode)
jax.numpy._pad_wrap(array,pad_width)
jax.numpy._promote_args(fun_name,*args)
jax.numpy._promote_args_inexact(fun_name,*args)
jax.numpy._promote_dtypes(*args)
jax.numpy._promote_dtypes_inexact(*args)
jax.numpy._promote_shapes(fun_name,*args)
jax.numpy._quantile(a,q,axis,keepdims)
jax.numpy._rank_promotion_warning_or_error(fun_name,shapes)
jax.numpy._reduction_dims(a,axis)
jax.numpy._reduction_init_val(a,init_val)
jax.numpy._removechars(s,chars)
jax.numpy._repeat_scalar(a,repeats,axis=None)
jax.numpy._reshape(a,newshape,order='C')
jax.numpy._reshape_method(a,*newshape,**kwargs)
jax.numpy._result_dtype(op,*args)
jax.numpy._rewriting_take(arr,idx)
jax.numpy._roll(a,shift,axis)
jax.numpy._round_to_nearest_even(x)
jax.numpy._should_unpack_list_index(x)
jax.numpy._split_index_for_jit(idx)
jax.numpy._split_on_axis(onp_fun,axis)
jax.numpy._static_idx(idx,size)
jax.numpy._swap_args(f)
jax.numpy._take_along_axis(arr,indices,axis)
jax.numpy._to_inexact_dtype(dtype)
jax.numpy._unimplemented_setitem(self,i,x)
jax.numpy._unstack(x)
jax.numpy._where(condition,x=None,y=None)
jax.numpy._wrap_indices_function(f)
jax.numpy._wrap_numpy_nullary_function(f)
jax.numpy._wraps(fun,update_doc=True,lax_description='')
jax.numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.angle(z)
jax.numpy.append(arr,values,axis=None)
jax.numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.argmax(a,axis=None)
jax.numpy.argmin(a,axis=None)
jax.numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.array_equal(a1,a2)
jax.numpy.asarray(a,dtype=None,order=None)
jax.numpy.atleast_1d(*arys)
jax.numpy.atleast_2d(*arys)
jax.numpy.atleast_3d(*arys)
jax.numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.block(arrays)
jax.numpy.broadcast_arrays(*args)
jax.numpy.broadcast_to(arr,shape)
jax.numpy.cbrt(x)
jax.numpy.clip(a,a_min=None,a_max=None)
jax.numpy.column_stack(tup)
jax.numpy.concatenate(arrays,axis=0)
jax.numpy.conjugate(x)
jax.numpy.corrcoef(x,y=None,rowvar=True,bias=None,ddof=None)
jax.numpy.count_nonzero(a,axis=None)
jax.numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.deg2rad(x)
jax.numpy.diag(v,k=0)
jax.numpy.diag_indices(n,ndim=2)
jax.numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.diff(a,n=1,axis=-1)
jax.numpy.divide(x1,x2)
jax.numpy.divmod(x1,x2)
jax.numpy.dot(a,b,precision=None)
jax.numpy.dstack(tup)
jax.numpy.einsum(*operands,**kwargs)
jax.numpy.einsum_path(subscripts,*operands,**kwargs)
jax.numpy.exp2(x)
jax.numpy.expand_dims(a,axis)
jax.numpy.eye(N,M=None,k=0,dtype=None)
jax.numpy.finfo(dtype)
jax.numpy.fix(x,out=None)
jax.numpy.flip(m,axis=None)
jax.numpy.fliplr(m)
jax.numpy.flipud(m)
jax.numpy.floor_divide(x1,x2)
jax.numpy.full(shape,fill_value,dtype=None)
jax.numpy.full_like(a,fill_value,dtype=None)
jax.numpy.gcd(x1,x2)
jax.numpy.geomspace(start,stop,num=50,endpoint=True,dtype=None,axis=0)
jax.numpy.gradient(a,*args,**kwargs)
jax.numpy.heaviside(x1,x2)
jax.numpy.hstack(tup)
jax.numpy.hypot(x1,x2)
jax.numpy.identity(n,dtype=None)
jax.numpy.imag(val)
jax.numpy.inner(a,b,precision=None)
jax.numpy.isclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.iscomplex(x)
jax.numpy.isfinite(x)
jax.numpy.isinf(x)
jax.numpy.isnan(x)
jax.numpy.isreal(x)
jax.numpy.isrealobj(x)
jax.numpy.isscalar(num)
jax.numpy.issubdtype(arg1,arg2)
jax.numpy.ix_(*args)
jax.numpy.kron(a,b)
jax.numpy.lax_numpy._ArrayMeta(type(onp.ndarray))
jax.numpy.lax_numpy._ArrayMeta.__instancecheck__(self,instance)
jax.numpy.lax_numpy._ScalarMeta(self,x)
jax.numpy.lax_numpy._ScalarMeta.__eq__(self,other)
jax.numpy.lax_numpy._ScalarMeta.__hash__(self)
jax.numpy.lax_numpy._ScalarMeta.__ne__(self,other)
jax.numpy.lax_numpy._argminmax(op,a,axis)
jax.numpy.lax_numpy._arraylike(x)
jax.numpy.lax_numpy._astype(arr,dtype)
jax.numpy.lax_numpy._atleast_nd(x,n)
jax.numpy.lax_numpy._block(xs)
jax.numpy.lax_numpy._canonicalize_axis(axis,num_dims)
jax.numpy.lax_numpy._canonicalize_tuple_index(arr_ndim,idx)
jax.numpy.lax_numpy._check_arraylike(fun_name,*args)
jax.numpy.lax_numpy._check_no_padding(axis_padding,mode)
jax.numpy.lax_numpy._comparison_op(numpy_fn,lax_fn)
jax.numpy.lax_numpy._complex_elem_type(dtype)
jax.numpy.lax_numpy._compute_newshape(a,newshape)
jax.numpy.lax_numpy._constant_like(x,const)
jax.numpy.lax_numpy._cross(a,b,axisa,axisb,axisc)
jax.numpy.lax_numpy._defer_to_unrecognized_arg(binary_op)
jax.numpy.lax_numpy._dtype_info(dtype)
jax.numpy.lax_numpy._einsum(operands,contractions,precision)
jax.numpy.lax_numpy._eliminate_deprecated_list_indexing(idx)
jax.numpy.lax_numpy._expand_bool_indices(idx)
jax.numpy.lax_numpy._float_divmod(x1,x2)
jax.numpy.lax_numpy._gather(arr,treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._gcd_body_fn(xs)
jax.numpy.lax_numpy._gcd_cond_fn(xs)
jax.numpy.lax_numpy._gradient(a,axis)
jax.numpy.lax_numpy._index_to_gather(x_shape,idx)
jax.numpy.lax_numpy._int(aval)
jax.numpy.lax_numpy._is_advanced_int_indexer(idx)
jax.numpy.lax_numpy._is_int_arraylike(x)
jax.numpy.lax_numpy._is_slice_none(idx)
jax.numpy.lax_numpy._isposneginf(infinity,x)
jax.numpy.lax_numpy._logical_op(np_op,bitwise_op)
jax.numpy.lax_numpy._make_cumulative_reduction(onp_reduction,window_reduce,init_val,squash_nan=False)
jax.numpy.lax_numpy._make_nan_reduction(onp_reduction,np_reduction,init_val,nan_if_all_nan)
jax.numpy.lax_numpy._make_reduction(np_fun,op,init_val,preproc=None,bool_op=None,upcast_f16_for_computation=False)
jax.numpy.lax_numpy._make_scalar_type(onp_scalar_type)
jax.numpy.lax_numpy._maybe_bool_binop(numpy_fn,lax_fn,bool_lax_fn)
jax.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._movechars(s,src,dst)
jax.numpy.lax_numpy._normalize_index(index,axis_size)
jax.numpy.lax_numpy._not_implemented(fun)
jax.numpy.lax_numpy._one_to_one_binop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy.lax_numpy._one_to_one_unop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy.lax_numpy._operator_round(number,ndigits=None)
jax.numpy.lax_numpy._pad(array,pad_width,mode,constant_values)
jax.numpy.lax_numpy._pad_constant(array,pad_width,constant_values)
jax.numpy.lax_numpy._pad_edge(array,pad_width)
jax.numpy.lax_numpy._pad_symmetric_or_reflect(array,pad_width,mode)
jax.numpy.lax_numpy._pad_wrap(array,pad_width)
jax.numpy.lax_numpy._promote_args(fun_name,*args)
jax.numpy.lax_numpy._promote_args_inexact(fun_name,*args)
jax.numpy.lax_numpy._promote_dtypes(*args)
jax.numpy.lax_numpy._promote_dtypes_inexact(*args)
jax.numpy.lax_numpy._promote_shapes(fun_name,*args)
jax.numpy.lax_numpy._quantile(a,q,axis,keepdims)
jax.numpy.lax_numpy._rank_promotion_warning_or_error(fun_name,shapes)
jax.numpy.lax_numpy._reduction_dims(a,axis)
jax.numpy.lax_numpy._reduction_init_val(a,init_val)
jax.numpy.lax_numpy._removechars(s,chars)
jax.numpy.lax_numpy._repeat_scalar(a,repeats,axis=None)
jax.numpy.lax_numpy._reshape(a,newshape,order='C')
jax.numpy.lax_numpy._reshape_method(a,*newshape,**kwargs)
jax.numpy.lax_numpy._result_dtype(op,*args)
jax.numpy.lax_numpy._rewriting_take(arr,idx)
jax.numpy.lax_numpy._roll(a,shift,axis)
jax.numpy.lax_numpy._round_to_nearest_even(x)
jax.numpy.lax_numpy._should_unpack_list_index(x)
jax.numpy.lax_numpy._split_index_for_jit(idx)
jax.numpy.lax_numpy._split_on_axis(onp_fun,axis)
jax.numpy.lax_numpy._static_idx(idx,size)
jax.numpy.lax_numpy._swap_args(f)
jax.numpy.lax_numpy._take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy._to_inexact_dtype(dtype)
jax.numpy.lax_numpy._unimplemented_setitem(self,i,x)
jax.numpy.lax_numpy._unstack(x)
jax.numpy.lax_numpy._where(condition,x=None,y=None)
jax.numpy.lax_numpy._wrap_indices_function(f)
jax.numpy.lax_numpy._wrap_numpy_nullary_function(f)
jax.numpy.lax_numpy._wraps(fun,update_doc=True,lax_description='')
jax.numpy.lax_numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.lax_numpy.angle(z)
jax.numpy.lax_numpy.append(arr,values,axis=None)
jax.numpy.lax_numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.lax_numpy.argmax(a,axis=None)
jax.numpy.lax_numpy.argmin(a,axis=None)
jax.numpy.lax_numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.lax_numpy.array_equal(a1,a2)
jax.numpy.lax_numpy.asarray(a,dtype=None,order=None)
jax.numpy.lax_numpy.atleast_1d(*arys)
jax.numpy.lax_numpy.atleast_2d(*arys)
jax.numpy.lax_numpy.atleast_3d(*arys)
jax.numpy.lax_numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.lax_numpy.block(arrays)
jax.numpy.lax_numpy.broadcast_arrays(*args)
jax.numpy.lax_numpy.broadcast_to(arr,shape)
jax.numpy.lax_numpy.cbrt(x)
jax.numpy.lax_numpy.clip(a,a_min=None,a_max=None)
jax.numpy.lax_numpy.column_stack(tup)
jax.numpy.lax_numpy.concatenate(arrays,axis=0)
jax.numpy.lax_numpy.conjugate(x)
jax.numpy.lax_numpy.corrcoef(x,y=None,rowvar=True,bias=None,ddof=None)
jax.numpy.lax_numpy.count_nonzero(a,axis=None)
jax.numpy.lax_numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.lax_numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.lax_numpy.deg2rad(x)
jax.numpy.lax_numpy.diag(v,k=0)
jax.numpy.lax_numpy.diag_indices(n,ndim=2)
jax.numpy.lax_numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.lax_numpy.diff(a,n=1,axis=-1)
jax.numpy.lax_numpy.divide(x1,x2)
jax.numpy.lax_numpy.divmod(x1,x2)
jax.numpy.lax_numpy.dot(a,b,precision=None)
jax.numpy.lax_numpy.dstack(tup)
jax.numpy.lax_numpy.einsum(*operands,**kwargs)
jax.numpy.lax_numpy.einsum_path(subscripts,*operands,**kwargs)
jax.numpy.lax_numpy.exp2(x)
jax.numpy.lax_numpy.expand_dims(a,axis)
jax.numpy.lax_numpy.eye(N,M=None,k=0,dtype=None)
jax.numpy.lax_numpy.finfo(dtype)
jax.numpy.lax_numpy.fix(x,out=None)
jax.numpy.lax_numpy.flip(m,axis=None)
jax.numpy.lax_numpy.fliplr(m)
jax.numpy.lax_numpy.flipud(m)
jax.numpy.lax_numpy.floor_divide(x1,x2)
jax.numpy.lax_numpy.full(shape,fill_value,dtype=None)
jax.numpy.lax_numpy.full_like(a,fill_value,dtype=None)
jax.numpy.lax_numpy.gcd(x1,x2)
jax.numpy.lax_numpy.geomspace(start,stop,num=50,endpoint=True,dtype=None,axis=0)
jax.numpy.lax_numpy.gradient(a,*args,**kwargs)
jax.numpy.lax_numpy.heaviside(x1,x2)
jax.numpy.lax_numpy.hstack(tup)
jax.numpy.lax_numpy.hypot(x1,x2)
jax.numpy.lax_numpy.identity(n,dtype=None)
jax.numpy.lax_numpy.imag(val)
jax.numpy.lax_numpy.inner(a,b,precision=None)
jax.numpy.lax_numpy.isclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.lax_numpy.iscomplex(x)
jax.numpy.lax_numpy.isfinite(x)
jax.numpy.lax_numpy.isinf(x)
jax.numpy.lax_numpy.isnan(x)
jax.numpy.lax_numpy.isreal(x)
jax.numpy.lax_numpy.isrealobj(x)
jax.numpy.lax_numpy.isscalar(num)
jax.numpy.lax_numpy.issubdtype(arg1,arg2)
jax.numpy.lax_numpy.ix_(*args)
jax.numpy.lax_numpy.kron(a,b)
jax.numpy.lax_numpy.lcm(x1,x2)
jax.numpy.lax_numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.lax_numpy.log10(x)
jax.numpy.lax_numpy.log2(x)
jax.numpy.lax_numpy.logaddexp(x1,x2)
jax.numpy.lax_numpy.logaddexp2(x1,x2)
jax.numpy.lax_numpy.logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis=0)
jax.numpy.lax_numpy.matmul(a,b,precision=None)
jax.numpy.lax_numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.lax_numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.lax_numpy.meshgrid(*args,**kwargs)
jax.numpy.lax_numpy.moveaxis(a,source,destination)
jax.numpy.lax_numpy.msort(a)
jax.numpy.lax_numpy.nan_to_num(x,copy=True)
jax.numpy.lax_numpy.nanmean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.lax_numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.lax_numpy.nonzero(a)
jax.numpy.lax_numpy.ones(shape,dtype=None)
jax.numpy.lax_numpy.ones_like(x,dtype=None)
jax.numpy.lax_numpy.outer(a,b,out=None)
jax.numpy.lax_numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.lax_numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.polyval(p,x)
jax.numpy.lax_numpy.power(x1,x2)
jax.numpy.lax_numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.lax_numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.rad2deg(x)
jax.numpy.lax_numpy.ravel(a,order='C')
jax.numpy.lax_numpy.real(val)
jax.numpy.lax_numpy.reciprocal(x)
jax.numpy.lax_numpy.remainder(x1,x2)
jax.numpy.lax_numpy.repeat(a,repeats,axis=None)
jax.numpy.lax_numpy.reshape(a,newshape,order='C')
jax.numpy.lax_numpy.result_type(*args)
jax.numpy.lax_numpy.roll(a,shift,axis=None)
jax.numpy.lax_numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.lax_numpy.round(a,decimals=0)
jax.numpy.lax_numpy.select(condlist,choicelist,default=0)
jax.numpy.lax_numpy.sign(x)
jax.numpy.lax_numpy.signbit(x)
jax.numpy.lax_numpy.sinc(x)
jax.numpy.lax_numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.lax_numpy.square(x)
jax.numpy.lax_numpy.squeeze(a,axis=None)
jax.numpy.lax_numpy.stack(arrays,axis=0)
jax.numpy.lax_numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.swapaxes(a,axis1,axis2)
jax.numpy.lax_numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.lax_numpy.take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy.tensordot(a,b,axes=2,precision=None)
jax.numpy.lax_numpy.tile(a,reps)
jax.numpy.lax_numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.lax_numpy.transpose(a,axes=None)
jax.numpy.lax_numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.lax_numpy.tril(m,k=0)
jax.numpy.lax_numpy.triu(m,k=0)
jax.numpy.lax_numpy.true_divide(x1,x2)
jax.numpy.lax_numpy.update_numpydoc(docstr,fun,op)
jax.numpy.lax_numpy.vander(x,N=None,increasing=False)
jax.numpy.lax_numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.vdot(a,b,precision=None)
jax.numpy.lax_numpy.vstack(tup)
jax.numpy.lax_numpy.where(condition,x=None,y=None)
jax.numpy.lax_numpy.zeros(shape,dtype=None)
jax.numpy.lax_numpy.zeros_like(x,dtype=None)
jax.numpy.lcm(x1,x2)
jax.numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.log10(x)
jax.numpy.log2(x)
jax.numpy.logaddexp(x1,x2)
jax.numpy.logaddexp2(x1,x2)
jax.numpy.logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis=0)
jax.numpy.matmul(a,b,precision=None)
jax.numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.meshgrid(*args,**kwargs)
jax.numpy.moveaxis(a,source,destination)
jax.numpy.msort(a)
jax.numpy.nan_to_num(x,copy=True)
jax.numpy.nanmean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.nonzero(a)
jax.numpy.ones(shape,dtype=None)
jax.numpy.ones_like(x,dtype=None)
jax.numpy.outer(a,b,out=None)
jax.numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.polyval(p,x)
jax.numpy.power(x1,x2)
jax.numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.rad2deg(x)
jax.numpy.ravel(a,order='C')
jax.numpy.real(val)
jax.numpy.reciprocal(x)
jax.numpy.remainder(x1,x2)
jax.numpy.repeat(a,repeats,axis=None)
jax.numpy.reshape(a,newshape,order='C')
jax.numpy.result_type(*args)
jax.numpy.roll(a,shift,axis=None)
jax.numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.round(a,decimals=0)
jax.numpy.select(condlist,choicelist,default=0)
jax.numpy.sign(x)
jax.numpy.signbit(x)
jax.numpy.sinc(x)
jax.numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.square(x)
jax.numpy.squeeze(a,axis=None)
jax.numpy.stack(arrays,axis=0)
jax.numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.swapaxes(a,axis1,axis2)
jax.numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.take_along_axis(arr,indices,axis)
jax.numpy.tensordot(a,b,axes=2,precision=None)
jax.numpy.tile(a,reps)
jax.numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.transpose(a,axes=None)
jax.numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.tril(m,k=0)
jax.numpy.triu(m,k=0)
jax.numpy.true_divide(x1,x2)
jax.numpy.update_numpydoc(docstr,fun,op)
jax.numpy.vander(x,N=None,increasing=False)
jax.numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.vdot(a,b,precision=None)
jax.numpy.vstack(tup)
jax.numpy.where(condition,x=None,y=None)
jax.numpy.zeros(shape,dtype=None)
jax.numpy.zeros_like(x,dtype=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/numpy/vectorize.py----------------------------------------
A:jax.numpy.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax.numpy.vectorize._ARGUMENT->'\\({}\\)'.format(_CORE_DIMENSION_LIST)
A:jax.numpy.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax.numpy.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax.numpy.vectorize.num_core_dims->len(core_dims)
A:jax.numpy.vectorize.broadcast_shape->lax.broadcast_shapes(*shapes)
A:jax.numpy.vectorize.out->func(*args)
A:jax.numpy.vectorize.out_shapes->map(np.shape, out if isinstance(out, tuple) else [out])
A:jax.numpy.vectorize.sizes->dict(dim_sizes)
A:jax.numpy.vectorize.args->tuple(map(np.asarray, args))
A:jax.numpy.vectorize.error_context->'on vectorized function with excluded={!r} and signature={!r}'.format(excluded, signature)
A:jax.numpy.vectorize.(excluded_func, args)->_apply_excluded(pyfunc, excluded, args)
A:jax.numpy.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax.numpy.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims, error_context)
A:jax.numpy.vectorize.checked_func->_check_output_dims(excluded_func, dim_sizes, output_core_dims, error_context)
A:jax.numpy.vectorize.core_shape->tuple((dim_sizes[dim] for dim in core_dims))
A:jax.numpy.vectorize.vec_arg->np.broadcast_to(arg, vec_shape)
A:jax.numpy.vectorize.in_axes->tuple((0 if c > 0 else None for c in vmap_counts))
A:jax.numpy.vectorize.vectorized_func->api.vmap(vectorized_func, in_axes)
jax.numpy.vectorize(pyfunc,*,excluded=frozenset(),signature=None)
jax.numpy.vectorize._apply_excluded(func,excluded,args)
jax.numpy.vectorize._check_output_dims(func:Callable,dim_sizes:Dict[str,int],expected_output_core_dims:List[CoreDims],error_context:str='')->Callable
jax.numpy.vectorize._parse_gufunc_signature(signature:str)->Tuple[List[CoreDims], List[CoreDims]]
jax.numpy.vectorize._parse_input_dimensions(args:Tuple[NDArray,...],input_core_dims:List[CoreDims],error_context:str='')->Tuple[Tuple[int, ...], Dict[str, int]]
jax.numpy.vectorize._update_dim_sizes(dim_sizes:Dict[str,int],shape:Tuple[int,...],core_dims:CoreDims,error_context:str='',*,is_input:bool)
jax.numpy.vectorize.vectorize(pyfunc,*,excluded=frozenset(),signature=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/numpy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/numpy/fft.py----------------------------------------
A:jax.numpy.fft.axes->tuple(range(x.ndim))
A:jax.numpy.fft.a->np.moveaxis(a, orig_axes, axes)
A:jax.numpy.fft.transformed->np.moveaxis(transformed, axes, orig_axes)
A:jax.numpy.fft.k->np.arange(0, (n - 1) // 2 + 1)
A:jax.numpy.fft.x->np.asarray(x)
A:jax.numpy.fft.globals()[func.__name__]->_not_implemented(func)
jax.numpy.fft._fft_core(func_name,fft_type,a,s,axes,norm)
jax.numpy.fft._fft_core_1d(func_name,fft_type,a,s,axis,norm)
jax.numpy.fft._fft_core_2d(func_name,fft_type,a,s,axes,norm)
jax.numpy.fft.fft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.fft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.fftfreq(n,d=1.0)
jax.numpy.fft.fftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.fftshift(x,axes=None)
jax.numpy.fft.ifft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.ifft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.ifftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.ifftshift(x,axes=None)
jax.numpy.fft.irfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.irfft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.irfftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.rfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.rfft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.rfftfreq(n,d=1.0)
jax.numpy.fft.rfftn(a,s=None,axes=None,norm=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/ops/scatter.py----------------------------------------
A:jax.ops.scatter.x->numpy.lax_numpy.asarray(x)
A:jax.ops.scatter.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.ops.scatter.(treedef, static_idx, dynamic_idx)->numpy.lax_numpy._split_index_for_jit(idx)
A:jax.ops.scatter.idx->numpy.lax_numpy._merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)
A:jax.ops.scatter.indexer->numpy.lax_numpy._index_to_gather(np.shape(x), idx)
A:jax.ops.scatter.dnums->lax.ScatterDimensionNumbers(update_window_dims=indexer.dnums.offset_dims, inserted_window_dims=indexer.dnums.collapsed_slice_dims, scatter_dims_to_operand_dims=indexer.dnums.start_index_map)
A:jax.ops.scatter.index->_Indexable()
A:jax.ops.scatter.num_segments->int(num_segments)
A:jax.ops.scatter.out->numpy.lax_numpy.zeros((num_segments,) + data.shape[1:], dtype=data.dtype)
A:jax.ops.scatter.segment_ids->numpy.lax_numpy.mod(segment_ids, num_segments)
jax.ops.index_add(x,idx,y)
jax.ops.index_max(x,idx,y)
jax.ops.index_min(x,idx,y)
jax.ops.index_update(x,idx,y)
jax.ops.scatter._Indexable(object)
jax.ops.scatter._Indexable.__getitem__(self,index)
jax.ops.scatter._scatter_impl(x,y,scatter_op,treedef,static_idx,dynamic_idx)
jax.ops.scatter._scatter_update(x,idx,y,scatter_op)
jax.ops.scatter.index_add(x,idx,y)
jax.ops.scatter.index_max(x,idx,y)
jax.ops.scatter.index_min(x,idx,y)
jax.ops.scatter.index_update(x,idx,y)
jax.ops.scatter.segment_sum(data,segment_ids,num_segments=None)
jax.ops.segment_sum(data,segment_ids,num_segments=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/ops/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/pxla.py----------------------------------------
A:jax.interpreters.pxla.prev_assignments->assign_shards_to_replicas(len(arg.device_buffers), axis_size)
A:jax.interpreters.pxla.candidates->defaultdict(list)
A:jax.interpreters.pxla.buffers[r][a]->buf.copy_to_device(devices[r])
A:jax.interpreters.pxla.bufs->shard_arg_handlers[type(arg)](arg, devices, assignments)
A:jax.interpreters.pxla.nrep->len(devices)
A:jax.interpreters.pxla.xs->x._unstack()
A:jax.interpreters.pxla.full_aval->ShapedArray((size,) + aval.shape, aval.dtype)
A:jax.interpreters.pxla.(groupsize, ragged)->divmod(nrep, size)
A:jax.interpreters.pxla.indices->numpy.tile(onp.arange(size)[:, None], (1, groupsize))
A:jax.interpreters.pxla.self.dynamic_axis_env->DynamicAxisEnv()
A:jax.interpreters.pxla._thread_local_state->_ThreadLocalState()
A:jax.interpreters.pxla.mapped->prod((frame.hard_size for frame in dynamic_axis_env))
A:jax.interpreters.pxla.(unmapped, ragged)->divmod(xb.device_count(backend), mapped)
A:jax.interpreters.pxla.axis_name->params.pop('axis_name')
A:jax.interpreters.pxla.shape->tuple((logical_size(dynamic_axis_env[name]) for name in axis_name))
A:jax.interpreters.pxla.dummy_arg->frame.soft_trace.pure(dummy_arg)
A:jax.interpreters.pxla.out_aval->ShapedArray((), onp.int32)
A:jax.interpreters.pxla.out_tracer->pe.JaxprTracer(trace, pe.PartialVal((out_aval, core.unit)), None)
A:jax.interpreters.pxla.eqn->pe.new_eqn_recipe([], [out_tracer], axis_index_p, params)
A:jax.interpreters.pxla.div->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Constant(onp.array(axis_env.nreps // prod(axis_env.sizes), onp.uint32))
A:jax.interpreters.pxla.mod->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Constant(onp.array(axis_env.sizes[-1], onp.uint32))
A:jax.interpreters.pxla.unsigned_index->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Rem(c.Div(c.ReplicaId(), div), mod)
A:jax.interpreters.pxla.axis_index_p->core.Primitive('axis_index')
A:jax.interpreters.pxla._collect->staticmethod(onp.concatenate)
A:jax.interpreters.pxla.num_bufs->len(self.device_buffers)
A:jax.interpreters.pxla.assignments->assign_shards_to_replicas(num_bufs, self.axis_size)
A:jax.interpreters.pxla.(_, ids)->numpy.unique(assignments, return_index=True)
A:jax.interpreters.pxla.ids->self._ids()
A:jax.interpreters.pxla.self._npy_value->self._collect([self.device_buffers[i].to_py() for i in ids])
A:jax.interpreters.pxla.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.pxla.n->len(devices)
A:jax.interpreters.pxla.xla.pytype_aval_mappings[ShardedDeviceArray]->operator.attrgetter('aval')
A:jax.interpreters.pxla.xla.pytype_aval_mappings[ChunkedDeviceArray]->operator.attrgetter('aval')
A:jax.interpreters.pxla.abstract_args->map(xla.abstractify, args)
A:jax.interpreters.pxla.compiled_fun->parallel_callable(fun, backend, axis_name, axis_size, global_axis_size, devices, name, *abstract_args)
A:jax.interpreters.pxla.global_axis_size->lib.xla_bridge.device_count()
A:jax.interpreters.pxla.avals->tuple(map(partial(shard_aval, axis_size), avals))
A:jax.interpreters.pxla.pval->pe.PartialVal([core.abstract_unit, core.unit])
A:jax.interpreters.pxla.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(dynamic_fun, [pval] + pvals, instantiate=False, stage_out_calls=True, bottom=True)
A:jax.interpreters.pxla.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.pxla.is_multi_host_pmap->any((d.host_id != xb.host_id() for d in devices))
A:jax.interpreters.pxla.used_collectives->set(xla.jaxpr_collectives(jaxpr))
A:jax.interpreters.pxla.jaxpr_replicas->xla.jaxpr_replicas(jaxpr)
A:jax.interpreters.pxla.axis_env->xla.AxisEnv(num_global_replicas, (axis_name,), (global_axis_size,), devices)
A:jax.interpreters.pxla.c->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__))
A:jax.interpreters.pxla.xla_consts->_map(c.Constant, consts)
A:jax.interpreters.pxla.xla_args->xla._xla_callable_args(c, avals, tuple_args)
A:jax.interpreters.pxla.out_nodes->xla.jaxpr_subcomp(c, jaxpr, backend, axis_env, xla_consts, extend_name_stack(wrap_name(name, 'pmap')), *xla_args)
A:jax.interpreters.pxla.built->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Build(c.Tuple(*out_nodes))
A:jax.interpreters.pxla.devices->lib.xla_bridge.get_backend(backend).get_default_device_assignment(nrep)
A:jax.interpreters.pxla.local_devices_str->', '.join(map(str, local_devices))
A:jax.interpreters.pxla.device_assignment->tuple((d.id for d in devices))
A:jax.interpreters.pxla.compiled->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Build(c.Tuple(*out_nodes)).Compile(compile_options=xb.get_compile_options(num_replicas=num_global_replicas, num_partitions=1, device_assignment=device_assignment), backend=xb.get_backend(backend))
A:jax.interpreters.pxla.handle_args->partial(shard_args, backend, compiled.local_devices(), assign_shards_to_replicas(num_local_replicas, axis_size), axis_size)
A:jax.interpreters.pxla.handle_outs->_pvals_to_results_handler(axis_size, num_local_replicas, out_pvals, compiled.local_devices(), backend)
A:jax.interpreters.pxla.result_to_populate->ResultToPopulate()
A:jax.interpreters.pxla.nouts->len(out_pvals)
A:jax.interpreters.pxla.input_bufs->in_handler(args)
A:jax.interpreters.pxla.out_bufs->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Build(c.Tuple(*out_nodes)).Compile(compile_options=xb.get_compile_options(num_replicas=num_global_replicas, num_partitions=1, device_assignment=device_assignment), backend=xb.get_backend(backend)).ExecuteOnLocalDevices(list(input_bufs), tuple_arguments=tuple_args)
A:jax.interpreters.pxla.xla_pmap_p->core.Primitive('xla_pmap')
A:jax.interpreters.pxla.xla_pmap->partial(core.call_bind, xla_pmap_p)
A:jax.interpreters.pxla.new_env->xla.extend_axis_env(axis_env, axis_name, global_axis_size)
A:jax.interpreters.pxla.sharded_outs->xla.jaxpr_subcomp(c, call_jaxpr, backend, new_env, (), extend_name_stack(name_stack, wrap_name(name, 'pmap')), *in_nodes_sharded)
A:jax.interpreters.pxla.ad.primitive_transposes[xla_pmap_p]->partial(ad.map_transpose, xla_pmap_p)
A:jax.interpreters.pxla.dims->list(c.GetShape(x).dimensions())
A:jax.interpreters.pxla.zero->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).Constant(onp.zeros((), dtype=onp.uint32))
A:jax.interpreters.pxla.padded->lib.xla_bridge.make_computation_builder('pmap_{}'.format(fun.__name__)).DynamicUpdateSlice(padded, c.Reshape(x, None, [1] + dims), idxs)
A:jax.interpreters.pxla.trace->SplitAxisTrace(master, core.cur_sublevel())
A:jax.interpreters.pxla.in_tracers->list(map(partial(SplitAxisTracer, trace, axis_name), args))
A:jax.interpreters.pxla.out_tracers->list(map(trace.full_raise, outs))
A:jax.interpreters.pxla.(out_vals, out_names)->unzip2(((t.val, t.axis_name) for t in out_tracers))
A:jax.interpreters.pxla.(vals_in, names_in)->unzip2(((t.val, t.axis_name) for t in tracers))
A:jax.interpreters.pxla.hard_idx->primitive.bind(dummy, **params)
A:jax.interpreters.pxla.(name,)->set((n for n in names_in if n is not not_mapped))
A:jax.interpreters.pxla.(val_out, is_mapped)->rule(vals_in, which_mapped, **params)
A:jax.interpreters.pxla.val_out->primitive.bind(*vals_in, **params)
A:jax.interpreters.pxla.rule->batching.get_primitive_batcher(primitive)
A:jax.interpreters.pxla.(val_out, axis_out)->rule(vals_in, axes_in, **params)
A:jax.interpreters.pxla.(vals, names)->unzip2(((t.val, t.axis_name) for t in tracers))
A:jax.interpreters.pxla.(f, names_out)->split_axis_subtrace(f, self.master, names)
A:jax.interpreters.pxla.vals_out->call_primitive.bind(f, *vals, **params)
A:jax.interpreters.pxla.vals_out_trans->map_primitive.bind(f, *vals_trans, **params)
jax.interpreters.pxla.ChunkedDeviceArray(self,axis_size,aval,device_buffers)
jax.interpreters.pxla.ChunkedDeviceArray.__getitem__(self,idx)
jax.interpreters.pxla.DynamicAxisEnv(list)
jax.interpreters.pxla.DynamicAxisEnv.__contains__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.__getitem__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.nreps(self)
jax.interpreters.pxla.DynamicAxisEnv.sizes(self)
jax.interpreters.pxla.DynamicAxisEnvFrame(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.ResultToPopulate(object)
jax.interpreters.pxla.ShardedDeviceArray(self,aval,device_buffers)
jax.interpreters.pxla.ShardedDeviceArray.__getitem__(self,idx)
jax.interpreters.pxla.ShardedDeviceArray._check_if_deleted(self)
jax.interpreters.pxla.ShardedDeviceArray._ids(self)
jax.interpreters.pxla.ShardedDeviceArray._value(self)
jax.interpreters.pxla.ShardedDeviceArray.block_until_ready(self)
jax.interpreters.pxla.ShardedDeviceArray.copy_to_host_async(self)
jax.interpreters.pxla.ShardedDeviceArray.delete(self)
jax.interpreters.pxla.SplitAxisTrace(core.Trace)
jax.interpreters.pxla.SplitAxisTrace.lift(self,val)
jax.interpreters.pxla.SplitAxisTrace.post_process_call(self,call_primitive,out_tracer,params)
jax.interpreters.pxla.SplitAxisTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.pxla.SplitAxisTrace.pure(self,val)
jax.interpreters.pxla.SplitAxisTrace.sublift(self,val)
jax.interpreters.pxla.SplitAxisTracer(self,trace,axis_name,val)
jax.interpreters.pxla.SplitAxisTracer.aval(self)
jax.interpreters.pxla.SplitAxisTracer.full_lower(self)
jax.interpreters.pxla._ThreadLocalState(self)
jax.interpreters.pxla._axis_index_partial_eval(trace,_,**params)
jax.interpreters.pxla._axis_index_translation_rule(c,nreps,sizes,soft_size,axis_name)
jax.interpreters.pxla._pmap_translation_rule(c,axis_env,in_nodes,name_stack,axis_name,axis_size,global_axis_size,devices,name,call_jaxpr,*,backend=None,mapped_invars)
jax.interpreters.pxla._pval_to_result_handler(axis_size,nrep,pval,devices,backend)
jax.interpreters.pxla._pvals_to_results_handler(size,nrep,out_pvals,devices,backend)
jax.interpreters.pxla._shard_abstract_array(size,x)
jax.interpreters.pxla._shard_array(x,devices,assignments)
jax.interpreters.pxla._shard_device_array(x,devices,assignments)
jax.interpreters.pxla._shard_sharded_device_array(x,devices,assignments)
jax.interpreters.pxla._sharded_device_array_constant_handler(c,val,canonicalize_types=True)
jax.interpreters.pxla._unravel_index(c,axis_env)
jax.interpreters.pxla._xla_shard(c,aval,axis_env,x)
jax.interpreters.pxla._xla_unshard(c,aval,axis_env,x)
jax.interpreters.pxla.add_chunk_to_axis_env(axis_name,soft_trace,soft_size)
jax.interpreters.pxla.apply_parallel_primitive(prim,*args,**params)
jax.interpreters.pxla.array_result_handler(size,nrep,aval)
jax.interpreters.pxla.assign_shards_to_replicas(nrep,size)
jax.interpreters.pxla.aval_to_result_handler(size,nrep,aval)
jax.interpreters.pxla.axis_index(axis_name)
jax.interpreters.pxla.execute_replicated(compiled,backend,in_handler,out_handler,tuple_args,*args)
jax.interpreters.pxla.extend_dynamic_axis_env(axis_name,pmap_trace,hard_size)
jax.interpreters.pxla.identity(x)
jax.interpreters.pxla.parallel_callable(fun,backend,axis_name,axis_size,global_axis_size,devices,name,*avals)
jax.interpreters.pxla.replicate(val,axis_size,nrep,devices=None,backend=None)
jax.interpreters.pxla.shard_args(backend,devices,assignments,axis_size,args)
jax.interpreters.pxla.shard_aval(size,aval)
jax.interpreters.pxla.split_axis(axis_name,chunk_size,*args)
jax.interpreters.pxla.split_axis_subtrace(master,names,*vals)
jax.interpreters.pxla.unmapped_device_count(backend=None)
jax.interpreters.pxla.xla_pmap_impl(fun:lu.WrappedFun,*args,backend,axis_name,axis_size,global_axis_size,devices,name,mapped_invars=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/batching.py----------------------------------------
A:jax.interpreters.batching.(out_vals, out_dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(fun, out_dims)->batch_subtrace(fun, master, in_dims)
A:jax.interpreters.batching.out_vals->fun.call_wrapped(*in_vals)
A:jax.interpreters.batching.trace->BatchTrace(master, core.cur_sublevel())
A:jax.interpreters.batching.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.batching.NotMapped->type(None)
A:jax.interpreters.batching.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.batching.new_shape->tuple(onp.delete(aval.shape, self.batch_dim))
A:jax.interpreters.batching.(vals_in, dims_in)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.batched_primitive->get_primitive_batcher(primitive)
A:jax.interpreters.batching.(val_out, dim_out)->batched_primitive(vals_in, dims_in, **params)
A:jax.interpreters.batching.name->dict(params, input_shape=operand.shape).get('name', f.__name__)
A:jax.interpreters.batching.params->dict(params, input_shape=operand.shape)
A:jax.interpreters.batching.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(f, dims_out)->batch_subtrace(f, self.master, dims)
A:jax.interpreters.batching.vals_out->map_primitive.bind(f, *vals, **params)
A:jax.interpreters.batching.is_batched->tuple((d is not not_mapped for d in dims))
A:jax.interpreters.batching.dims->tuple((not_mapped if d is not_mapped else 0 for d in dims))
A:jax.interpreters.batching.dims_out->tuple((d + 1 if d is not not_mapped else d for d in dims_out()))
A:jax.interpreters.batching.primitive_batchers[prim]->partial(reducer_batcher, prim)
A:jax.interpreters.batching.d->next((d for d in dims if d is not not_mapped))
A:jax.interpreters.batching.out->prim.bind(*args, **params)
A:jax.interpreters.batching.ndim->max((onp.ndim(x) for x in args))
A:jax.interpreters.batching.axes->tuple(onp.where(onp.less(axes, bdim), axes, onp.add(axes, 1)))
A:jax.interpreters.batching.bdim_out->int(list(onp.delete(onp.arange(operand.ndim), axes)).index(bdim))
A:jax.interpreters.batching.x->moveaxis(x, bdx, bdy)
A:jax.interpreters.batching.y->broadcast(y, x.shape[bdx], bdx)
A:jax.interpreters.batching.last->_Last()
A:jax.interpreters.batching.axis->numpy.ndim(x)
A:jax.interpreters.batching.shape->list(onp.shape(x))
A:jax.interpreters.batching.broadcast_dims->tuple(onp.delete(onp.arange(len(shape)), axis))
A:jax.interpreters.batching.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.batching.(f, batched_out)->batched_traceable(f, size, batched, instantiate)
A:jax.interpreters.batching.(jaxpr_out, pvals_out, consts_out)->pe.trace_to_jaxpr(f, in_pvals, instantiate=True)
A:jax.interpreters.batching.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.batching.jaxpr_out->core.TypedJaxpr(jaxpr_out, consts_out, avals_in, avals_out)
jax.interpreters.batching.BatchTrace(Trace)
jax.interpreters.batching.BatchTrace.lift(self,val)
jax.interpreters.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.batching.BatchTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.batching.BatchTrace.pure(self,val)
jax.interpreters.batching.BatchTrace.sublift(self,val)
jax.interpreters.batching.BatchTracer(self,trace,val,batch_dim:Optional[int])
jax.interpreters.batching.BatchTracer.aval(self)
jax.interpreters.batching.BatchTracer.full_lower(self)
jax.interpreters.batching._Last(object)
jax.interpreters.batching._handle_scalar_broadcasting(nd,x,d)
jax.interpreters.batching._promote_aval_rank(sz,aval)
jax.interpreters.batching.add_batched(batched_args,batch_dims)
jax.interpreters.batching.batch(fun:lu.WrappedFun,in_vals,in_dims,out_dim_dests)
jax.interpreters.batching.batch_fun(fun:lu.WrappedFun,in_vals,in_dims)
jax.interpreters.batching.batch_jaxpr(jaxpr,size,batched,instantiate)
jax.interpreters.batching.batch_subtrace(master,in_dims,*in_vals)
jax.interpreters.batching.batched_traceable(size,batched,instantiate,*vals)
jax.interpreters.batching.bdim_at_front(x,bdim,size)
jax.interpreters.batching.broadcast(x,sz,axis)
jax.interpreters.batching.broadcast_batcher(prim,args,dims,**params)
jax.interpreters.batching.defbroadcasting(prim)
jax.interpreters.batching.defreducer(prim)
jax.interpreters.batching.defvectorized(prim)
jax.interpreters.batching.get_primitive_batcher(p)
jax.interpreters.batching.matchaxis(sz,src,dst,x)
jax.interpreters.batching.moveaxis(x,src,dst)
jax.interpreters.batching.reducer_batcher(prim,batched_args,batch_dims,axes,**params)
jax.interpreters.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax.interpreters.batching.zeros_like_batched(batched_args,batch_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/partial_eval.py----------------------------------------
A:jax.interpreters.partial_eval.aval->raise_to_shaped(get_aval(const), onp.isscalar(const))
A:jax.interpreters.partial_eval.(pvs, consts)->unzip2((t.pval for t in tracers))
A:jax.interpreters.partial_eval.tracers->map(self.instantiate_const_abstracted, tracers)
A:jax.interpreters.partial_eval.out_aval->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.eqn->new_eqn_recipe(tuple(it.chain(const_tracers, instantiated_tracers)), out_tracers, remat_call_p, new_params)
A:jax.interpreters.partial_eval.out_tracer->JaxprTracer(self, PartialVal((out_aval, unit)), None)
A:jax.interpreters.partial_eval.out_tracer.recipe->new_eqn_recipe(tracers, [out_tracer], primitive, params)
A:jax.interpreters.partial_eval.name->wrap_name(name, 'pe')
A:jax.interpreters.partial_eval.params->dict(params, name=name)
A:jax.interpreters.partial_eval.(in_pvs, in_consts)->unzip2((t.pval for t in instantiated_tracers))
A:jax.interpreters.partial_eval.(fun, aux)->partial_eval(f, trace, in_pvs)
A:jax.interpreters.partial_eval.out_flat->core.Primitive('remat_call').bind(fun, *in_consts, **params)
A:jax.interpreters.partial_eval.(out_pvs, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.env_tracers->map(trace.full_raise, env)
A:jax.interpreters.partial_eval.(out_pv_consts, consts)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.const_tracers->map(trace.new_instantiated_const, consts)
A:jax.interpreters.partial_eval.lifted_jaxpr->convert_constvars_jaxpr(typed_jaxpr.jaxpr)
A:jax.interpreters.partial_eval.new_params->dict(params, call_jaxpr=lifted_jaxpr)
A:jax.interpreters.partial_eval.(out_pvs_reduced, jaxpr, env)->aux()
A:jax.interpreters.partial_eval.(jaxpr, consts, env)->tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.interpreters.partial_eval.(out_pvs, out_pv_consts)->unzip2((t.pval for t in out_tracers))
A:jax.interpreters.partial_eval.n->len(jaxpr.outvars)
A:jax.interpreters.partial_eval.trace->JaxprTrace(master, core.cur_sublevel())
A:jax.interpreters.partial_eval.(out_pvs_reduced, out_pv_consts)->unzip2((t.pval for t in out_tracers))
A:jax.interpreters.partial_eval.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.partial_eval.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.(_, pvals_out, _)->trace_to_jaxpr(lu.wrap_init(fun, params), pvals_in, instantiate=True)
A:jax.interpreters.partial_eval.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.partial_eval.fun->trace_to_subjaxpr(fun, master, instantiate)
A:jax.interpreters.partial_eval.(jaxpr, (out_pvals, consts, env))->trace_to_subjaxpr(fun, master, instantiate).call_wrapped(pvals)
A:jax.interpreters.partial_eval.in_tracers->map(trace.new_arg, pvals)
A:jax.interpreters.partial_eval.out_tracers->map(partial(instantiate_const_at, trace), instantiate, out_tracers)
A:jax.interpreters.partial_eval.FreeVar->namedtuple('FreeVar', ['val'])
A:jax.interpreters.partial_eval.ConstVar->namedtuple('ConstVar', ['val'])
A:jax.interpreters.partial_eval.LambdaBinding->namedtuple('LambdaBinding', [])
A:jax.interpreters.partial_eval.JaxprEqnRecipe->namedtuple('JaxprEqnRecipe', ['eqn_id', 'invars', 'outvars', 'primitive', 'params'])
A:jax.interpreters.partial_eval.newvar->core.gensym('')
A:jax.interpreters.partial_eval.var->newvar(get_aval(c))
A:jax.interpreters.partial_eval.sorted_tracers->toposort(out_tracers)
A:jax.interpreters.partial_eval.invars->map(getvar, in_tracers)
A:jax.interpreters.partial_eval.processed_eqn_ids->set()
A:jax.interpreters.partial_eval.vt_to_var[id(t)]->getconstvar(recipe.val)
A:jax.interpreters.partial_eval.(env_vars, env_vals)->unzip2(env.items())
A:jax.interpreters.partial_eval.(const_vars, const_vals)->unzip2(consts.items())
A:jax.interpreters.partial_eval.jaxpr->Jaxpr(const_vars, list(it.chain(env_vars, invars)), list(map(getvar, out_tracers)), eqns)
A:jax.interpreters.partial_eval.(jaxpr_2, out_pvals_2, consts_2)->trace_to_jaxpr(f, pvals, instantiate=instantiate)
A:jax.interpreters.partial_eval.(out_pvs_2, out_consts_2)->unzip2(out_pvals_2)
A:jax.interpreters.partial_eval.(jaxpr_1, out_pvals, consts_1)->trace_to_jaxpr(lu.wrap_init(fun), pvals, instantiate=True)
A:jax.interpreters.partial_eval.jaxpr_2->convert_constvars_jaxpr(jaxpr_2)
A:jax.interpreters.partial_eval.(in_avals_1, in_avals_2)->unzip2(map(_split_aval, unknowns, jaxpr.in_avals))
A:jax.interpreters.partial_eval.(out_avals_1, out_avals_2)->unzip2(map(_split_aval, uk_out, jaxpr.out_avals))
A:jax.interpreters.partial_eval.(out_pvs, _)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.typed_jaxpr_1->TypedJaxpr(jaxpr_1, consts_1, in_avals_1, out_avals_1)
A:jax.interpreters.partial_eval.typed_jaxpr_2->TypedJaxpr(jaxpr_2, (), in_avals_2, out_avals_2)
A:jax.interpreters.partial_eval.remat_call_p->core.Primitive('remat_call')
A:jax.interpreters.partial_eval.remat_call->partial(core.call_bind, remat_call_p)
A:jax.interpreters.partial_eval.instantiated_tracers->map(trace.instantiate_const_abstracted, tracers)
A:jax.interpreters.partial_eval.env->map(trace.full_raise, env)
A:jax.interpreters.partial_eval.(out_pval_consts1, consts)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.typed_jaxpr->_dce_jaxpr(typed_jaxpr, out_unknowns)
A:jax.interpreters.partial_eval.(jaxpr_1, jaxpr_2, out_unknowns)->partial_eval_jaxpr(typed_jaxpr, in_unknowns, False)
A:jax.interpreters.partial_eval.jaxpr_1_primals->_dce_jaxpr(jaxpr_1, to_compute + [False] * num_res)
A:jax.interpreters.partial_eval.(_, in_consts)->unzip2((t.pval for t in it.chain(env, tracers)))
A:jax.interpreters.partial_eval.out_pvals->map(_reconstruct_pval, out_pvals1, out_pval_consts2, out_unknowns)
A:jax.interpreters.partial_eval.(new_outvars, new_out_avals)->unzip2(out_pairs)
A:jax.interpreters.partial_eval.new_jaxpr->core.Jaxpr((), new_invars, typed_jaxpr.jaxpr.outvars, typed_jaxpr.jaxpr.eqns)
A:jax.interpreters.partial_eval._thread_local_state->_ThreadLocalState()
A:jax.interpreters.partial_eval.new_invars->_move_to_front(typed_jaxpr.jaxpr.invars, to_move)
A:jax.interpreters.partial_eval.new_in_avals->_move_to_front(typed_jaxpr.in_avals, to_move)
A:jax.interpreters.partial_eval.new_typed_jaxpr->core.TypedJaxpr(new_jaxpr, typed_jaxpr.literals, new_in_avals, typed_jaxpr.out_avals)
jax.interpreters.partial_eval.JaxprTrace(Trace)
jax.interpreters.partial_eval.JaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.instantiate_const(self,tracer)
jax.interpreters.partial_eval.JaxprTrace.instantiate_const_abstracted(self,tracer)
jax.interpreters.partial_eval.JaxprTrace.lift(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_arg(self,pval)
jax.interpreters.partial_eval.JaxprTrace.new_const(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_const(self,val)
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_literal(self,val)
jax.interpreters.partial_eval.JaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.pure(self,val)
jax.interpreters.partial_eval.JaxprTrace.sublift(self,val)
jax.interpreters.partial_eval.JaxprTracer(self,trace,pval,recipe)
jax.interpreters.partial_eval.JaxprTracer.__repr__(self)
jax.interpreters.partial_eval.JaxprTracer.aval(self)
jax.interpreters.partial_eval.JaxprTracer.full_lower(self)
jax.interpreters.partial_eval.JaxprTracer.ispure(self)
jax.interpreters.partial_eval.JaxprTracer.parents(self)
jax.interpreters.partial_eval.PartialVal(cls,xs)
jax.interpreters.partial_eval.StagingJaxprTrace(JaxprTrace)
jax.interpreters.partial_eval._ThreadLocalState(self)
jax.interpreters.partial_eval._dce_jaxpr(typed_jaxpr,outputs)
jax.interpreters.partial_eval._mapped_aval(aval)
jax.interpreters.partial_eval._move_to_front(lst,to_move)
jax.interpreters.partial_eval._reconstruct_pval(pval1,const2,unknown)
jax.interpreters.partial_eval._remat_partial_eval(trace,f,tracers,params)
jax.interpreters.partial_eval._split_aval(unknown,aval)
jax.interpreters.partial_eval._unmapped_aval(size,aval)
jax.interpreters.partial_eval.abstract_eval_fun(fun,*avals,**params)
jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr)
jax.interpreters.partial_eval.identity(x)
jax.interpreters.partial_eval.instantiate_const_at(trace,instantiate,tracer)
jax.interpreters.partial_eval.merge_pvals(val,pval:PartialVal)
jax.interpreters.partial_eval.move_binders_to_front(typed_jaxpr,to_move)
jax.interpreters.partial_eval.new_eqn_recipe(invars,outvars,primitive,params)
jax.interpreters.partial_eval.partial_eval(f,trace,pvs)
jax.interpreters.partial_eval.partial_eval_jaxpr(jaxpr,unknowns,instantiate)
jax.interpreters.partial_eval.partial_eval_wrapper(avals,*consts)
jax.interpreters.partial_eval.partial_val_aval(pv,const)
jax.interpreters.partial_eval.recipe_to_eqn(unused_var,getvar,recipe)
jax.interpreters.partial_eval.remat_context()
jax.interpreters.partial_eval.trace_to_jaxpr(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate=False,stage_out_calls=False,bottom=False)
jax.interpreters.partial_eval.trace_to_subjaxpr(master,instantiate,pvals)
jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers,out_tracers)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/parallel.py----------------------------------------
A:jax.interpreters.parallel.(fun, _)->papply_transform(fun, name, axis_size)
A:jax.interpreters.parallel.trace->PapplyTrace(master, core.cur_sublevel())
A:jax.interpreters.parallel.in_tracers->map(partial(PapplyTracer, trace, name, axis_size, axis=0), args)
A:jax.interpreters.parallel.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.parallel.(out_vals, out_axes)->unzip2(((t.val, t.axis) for t in out_tracers))
A:jax.interpreters.parallel.NotSharded->type(None)
A:jax.interpreters.parallel.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.parallel.new_shape->list(aval.shape)
A:jax.interpreters.parallel.(names, vals, axes)->unzip3(((t.name, t.val, t.axis) for t in tracers))
A:jax.interpreters.parallel.(val_out, axis_out)->rule(name, size, vals, axes, **params)
A:jax.interpreters.parallel.(f_papply, axes_out)->papply_subtrace(f, self.master, name, size, axes)
A:jax.interpreters.parallel.vals_out->call_primitive.bind(f_papply, *vals, **params)
jax.interpreters.parallel.PapplyTrace(Trace)
jax.interpreters.parallel.PapplyTrace.lift(self,val)
jax.interpreters.parallel.PapplyTrace.post_process_call(self,call_primitive,out_tracer)
jax.interpreters.parallel.PapplyTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.parallel.PapplyTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.parallel.PapplyTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.parallel.PapplyTrace.pure(self,val)
jax.interpreters.parallel.PapplyTrace.sublift(self,val)
jax.interpreters.parallel.PapplyTracer(self,trace,name,axis_size,val,axis)
jax.interpreters.parallel.PapplyTracer.aval(self)
jax.interpreters.parallel.PapplyTracer.full_lower(self)
jax.interpreters.parallel.identity(x)
jax.interpreters.parallel.papply(fun,name,in_vals,axis_size)
jax.interpreters.parallel.papply_subtrace(master,name,axis_size,axes,*vals)
jax.interpreters.parallel.papply_transform(name,axis_size,*args)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/masking.py----------------------------------------
A:jax.interpreters.masking.xs->list(xs)
A:jax.interpreters.masking.ShapeEnvs->namedtuple('ShapeEnvs', ['logical', 'padded'])
A:jax.interpreters.masking.shape_envs->ShapeEnvs({}, {})
A:jax.interpreters.masking.new_logical->dict(it.chain(shape_envs.logical.items(), logical_env.items()))
A:jax.interpreters.masking.new_padded->dict(it.chain(shape_envs.padded.items(), padded_env.items()))
A:jax.interpreters.masking.(fun, out_shapes)->mask_subtrace(fun, master)
A:jax.interpreters.masking.out_vals->fun.call_wrapped(in_vals, shape_exprs)
A:jax.interpreters.masking.trace->ShapeCheckTrace(master, core.cur_sublevel())
A:jax.interpreters.masking.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.masking.(out_vals, out_shapes)->unzip2(((t.val, t.shape_expr) for t in out_tracers))
A:jax.interpreters.masking.coeffs->dict()
A:jax.interpreters.masking.mon->Mon(mon1 + mon2)
A:jax.interpreters.masking.(q, _)->divmod(self, divisor)
A:jax.interpreters.masking.(_, r)->divmod(self, divisor)
A:jax.interpreters.masking.(q, r)->divmod(count, divisor)
A:jax.interpreters.masking.other->ensure_poly(other)
A:jax.interpreters.masking.deg->int(deg)
A:jax.interpreters.masking.coeff->int(coeff)
A:jax.interpreters.masking.dims->map(parse_dim, spec.replace(' ', '').strip(',').split(','))
A:jax.interpreters.masking.terms->map(parse_dim, spec.split('*'))
A:jax.interpreters.masking.digits->frozenset(string.digits)
A:jax.interpreters.masking.identifiers->frozenset(string.ascii_lowercase)
A:jax.interpreters.masking.monomorphic_dim->MonomorphicDim()
A:jax.interpreters.masking.s_->S_()
A:jax.interpreters.masking.(vals, shape_exprs)->unzip2(((t.val, t.shape_expr) for t in tracers))
A:jax.interpreters.masking.(out, out_shape)->rule(shape_envs, vals, shape_exprs, **params)
A:jax.interpreters.masking.out_shape->shape_rule(*avals, **params)
A:jax.interpreters.masking.logical_shapes->map(partial(eval_shape_expr, shape_envs.logical), shape_exprs)
A:jax.interpreters.masking.out->masking_rules[primitive](vals, logical_shapes, **params)
A:jax.interpreters.masking.masking_rules[prim]->partial(naryop_masking_rule, prim)
A:jax.interpreters.masking.out_shapes->check_subtrace(fun, master).call_wrapped(in_shapes)
A:jax.interpreters.masking.in_tracers->map(partial(ShapeCheckTracer, trace), in_shapes)
A:jax.interpreters.masking.shape_rule->shape_rules.get(primitive)
jax.interpreters.masking.MaskTrace(Trace)
jax.interpreters.masking.MaskTrace.lift(self,val)
jax.interpreters.masking.MaskTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.masking.MaskTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.masking.MaskTrace.pure(self,val)
jax.interpreters.masking.MaskTrace.sublift(self,val)
jax.interpreters.masking.MaskTracer(self,trace,val,shape_expr)
jax.interpreters.masking.MaskTracer.aval(self)
jax.interpreters.masking.MaskTracer.full_lower(self)
jax.interpreters.masking.MaskTracer.is_pure(self)
jax.interpreters.masking.Mon(Counter)
jax.interpreters.masking.Mon.__hash__(self)
jax.interpreters.masking.Mon.__lt__(self,other)
jax.interpreters.masking.Mon.__str__(self)
jax.interpreters.masking.Mon.degree(self)
jax.interpreters.masking.MonomorphicDim(object)
jax.interpreters.masking.MonomorphicDim.__str__(self)
jax.interpreters.masking.Poly(self,coeffs)
jax.interpreters.masking.Poly.__add__(self,other)
jax.interpreters.masking.Poly.__divmod__(self,divisor)
jax.interpreters.masking.Poly.__eq__(self,other)
jax.interpreters.masking.Poly.__floordiv__(self,divisor)
jax.interpreters.masking.Poly.__ge__(self,other)
jax.interpreters.masking.Poly.__gt__(self,other)
jax.interpreters.masking.Poly.__hash__(self)
jax.interpreters.masking.Poly.__int__(self)
jax.interpreters.masking.Poly.__le__(self,other)
jax.interpreters.masking.Poly.__lt__(self,other)
jax.interpreters.masking.Poly.__mod__(self,divisor)
jax.interpreters.masking.Poly.__mul__(self,other)
jax.interpreters.masking.Poly.__ne__(self,other)
jax.interpreters.masking.Poly.__neg__(self)
jax.interpreters.masking.Poly.__radd__(self,other)
jax.interpreters.masking.Poly.__rmul__(self,other)
jax.interpreters.masking.Poly.__rsub__(self,other)
jax.interpreters.masking.Poly.__str__(self)
jax.interpreters.masking.Poly.__sub__(self,other)
jax.interpreters.masking.Poly.is_constant(self)
jax.interpreters.masking.S_(object)
jax.interpreters.masking.S_.__getitem__(self,idx)
jax.interpreters.masking.ShapeCheckTrace(Trace)
jax.interpreters.masking.ShapeCheckTrace.lift(self,val)
jax.interpreters.masking.ShapeCheckTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.masking.ShapeCheckTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.masking.ShapeCheckTrace.pure(self,val)
jax.interpreters.masking.ShapeCheckTrace.sublift(self,val)
jax.interpreters.masking.ShapeCheckTracer(self,trace,shape_expr)
jax.interpreters.masking.ShapeCheckTracer.aval(self)
jax.interpreters.masking.ShapeCheckTracer.full_lower(self)
jax.interpreters.masking.ShapeError(Exception)
jax.interpreters.masking.ShapeSpec(tuple)
jax.interpreters.masking.ShapeSpec.__str__(self)
jax.interpreters.masking.ShapeSyntaxError(Exception)
jax.interpreters.masking.check_subtrace(master,in_shapes)
jax.interpreters.masking.constant_poly(val)
jax.interpreters.masking.defnaryop(prim)
jax.interpreters.masking.defvectorized(prim)
jax.interpreters.masking.ensure_poly(p)
jax.interpreters.masking.eval_dim_expr(env,poly)
jax.interpreters.masking.eval_shape_expr(env,expr)
jax.interpreters.masking.extend_shape_envs(logical_env,padded_env)
jax.interpreters.masking.finalize_spec(spec,shape)
jax.interpreters.masking.is_polymorphic(shape)
jax.interpreters.masking.mask_fun(fun,logical_env,padded_env,in_vals,shape_exprs)
jax.interpreters.masking.mask_subtrace(master,in_vals,shape_exprs)
jax.interpreters.masking.mul(coeff,mon)
jax.interpreters.masking.naryop_masking_rule(prim,padded_vals,logical_shapes)
jax.interpreters.masking.padded_shape_as_value(expr)
jax.interpreters.masking.parse_dim(spec)
jax.interpreters.masking.parse_id(name)
jax.interpreters.masking.parse_lit(val_str)
jax.interpreters.masking.parse_spec(spec='')
jax.interpreters.masking.pow(x,deg)
jax.interpreters.masking.prod(xs)
jax.interpreters.masking.shape_as_value(expr)
jax.interpreters.masking.shapecheck(fun:lu.WrappedFun,in_shapes)
jax.interpreters.masking.vectorized_masking_rule(prim,padded_vals,logical_shapes,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/ad.py----------------------------------------
A:jax.interpreters.ad.(fun, aux)->jvp_subtrace_aux(fun)
A:jax.interpreters.ad.trace->JVPTrace(master, core.cur_sublevel())
A:jax.interpreters.ad.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.ans_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.aux_tracers->map(trace.full_raise, aux)
A:jax.interpreters.ad.(out_primals, out_tangents)->unzip2(((t.primal, t.tangent) for t in ans_tracers))
A:jax.interpreters.ad.(aux_primals, _)->unzip2(((t.primal, t.tangent) for t in aux_tracers))
A:jax.interpreters.ad.aux_primals->map(core.full_lower, aux_primals)
A:jax.interpreters.ad.has_aux->kwargs.pop('has_aux', False)
A:jax.interpreters.ad.jvpfun->jvp(traceable)
A:jax.interpreters.ad.(jvpfun, aux)->jvp(traceable, has_aux=True)
A:jax.interpreters.ad.(_, in_tree)->tree_flatten(((primals, primals), {}))
A:jax.interpreters.ad.(jvpfun_flat, out_tree)->flatten_fun(jvpfun, in_tree)
A:jax.interpreters.ad.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(jvpfun_flat, in_pvals)
A:jax.interpreters.ad.(pval_primals, pval_tangents)->tree_unflatten(out_tree(), out_pvals)
A:jax.interpreters.ad.(aval_primals, const_primals)->unzip2(pval_primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts)->linearize(traceable, *primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts, aux)->linearize(traceable, *primals, has_aux=True)
A:jax.interpreters.ad.cts->map(instantiate_zeros_aval, kwargs['out_avals'], cts)
A:jax.interpreters.ad.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax.interpreters.ad.in_vals->map(read_primal, eqn.invars)
A:jax.interpreters.ad.ans->prim.bind(*primals)
A:jax.interpreters.ad.(call_jaxpr, params)->core.extract_call_jaxpr(eqn.primitive, eqn.params)
A:jax.interpreters.ad.invals->map(read_primal, eqn.invars)
A:jax.interpreters.ad.cts_in->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.(cts_in,)->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.cts_out->get_primitive_transpose(eqn.primitive)(cts_in, *invals, **eqn.params)
A:jax.interpreters.ad.cotangents_out->map(read_cotangent, jaxpr.invars)
A:jax.interpreters.ad.(all_args, in_tree_def)->tree_flatten(((), args, ct))
A:jax.interpreters.ad.fun->lu.hashable_partial(lu.wrap_init(backward_pass), call_jaxpr)
A:jax.interpreters.ad.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax.interpreters.ad.out_flat->primitive.bind(fun, *all_args, **params)
A:jax.interpreters.ad.(primals_in, tangents_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax.interpreters.ad.(primal_out, tangent_out)->tree_unflatten(out_tree_def(), result)
A:jax.interpreters.ad.(nonzero_tangents, in_tree_def)->tree_flatten(tangents)
A:jax.interpreters.ad.(f_jvp, out_tree_def)->traceable(jvp_subtrace(f, self.master), len(primals), in_tree_def)
A:jax.interpreters.ad.name->dict(params, name=wrap_name(params['name'], 'transpose')).get('name', f.__name__)
A:jax.interpreters.ad.params->dict(params, name=wrap_name(params['name'], 'transpose'))
A:jax.interpreters.ad.result->call_primitive.bind(f_jvp, *primals + nonzero_tangents, **params)
A:jax.interpreters.ad.(primals, tangents)->split_list(tracers, [len(tracers) // 2])
A:jax.interpreters.ad.primal_aval->raise_to_shaped(get_aval(primal))
A:jax.interpreters.ad.tangent_aval->raise_to_shaped(get_aval(tangent))
A:jax.interpreters.ad.primitive_jvps[primitive]->partial(zero_jvp, primitive)
A:jax.interpreters.ad.primitive_transposes[primitive]->partial(linear_transpose2, transpose_rule)
A:jax.interpreters.ad.val_out->primitive.bind(*primals, **params)
A:jax.interpreters.ad.tangents->map(instantiate_zeros, primals, tangents)
A:jax.interpreters.ad.ts->map(instantiate_zeros, xs, ts)
A:jax.interpreters.ad.primals_and_tangents->core.Primitive('{name}_jvp'.format(name=name)).bind(*it.chain(xs, ts), **params)
A:jax.interpreters.ad.fun_jvp_p->core.Primitive('{name}_jvp'.format(name=name))
A:jax.interpreters.ad.(primals_out, vjp_py)->custom_vjp(*primals, **params)
A:jax.interpreters.ad.(jaxpr, _, res)->pe.trace_to_jaxpr(lu.wrap_init(vjp_py), ct_pvals, instantiate=True)
A:jax.interpreters.ad.tangents_out->core.Primitive('{name}_lin'.format(name=name)).bind(*it.chain(res, tangents), trans_jaxpr=jaxpr, num_res=len(res), out_avals=out_avals)
A:jax.interpreters.ad.fun_lin_p->core.Primitive('{name}_lin'.format(name=name))
A:jax.interpreters.ad.(res, _)->split_list(args, [num_res])
A:jax.interpreters.ad.outs->core.eval_jaxpr(trans_jaxpr, res, *cts)
A:jax.interpreters.ad.primitive_transposes[prim]->partial(bilinear_transpose, lhs_rule, rhs_rule)
A:jax.interpreters.ad.defbilinear->partial(defbilinear_broadcasting, lambda g, x: g)
A:jax.interpreters.ad.new_tangents->tree_unflatten(in_tree_def, new_tangents)
A:jax.interpreters.ad.(out_flat, tree_def)->tree_flatten((primal_out, tangent_out))
A:jax.interpreters.ad.primitive_transposes[core.call_p]->partial(call_transpose, call_p)
A:jax.interpreters.ad.primitive_transposes[pe.remat_call_p]->partial(call_transpose, pe.remat_call_p)
A:jax.interpreters.ad.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.ad.(f_jvp, out_nonzeros)->f_jvp_traceable(jvp(f, instantiate=instantiate), nonzeros)
A:jax.interpreters.ad.avals_in->list(it.chain(jaxpr.in_avals, tangent_avals))
A:jax.interpreters.ad.(jaxpr_out, pvals_out, literals_out)->pe.trace_to_jaxpr(f_jvp, pvals, instantiate=True)
A:jax.interpreters.ad.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.ad.jaxpr_out->core.TypedJaxpr(jaxpr_out, literals_out, avals_in, avals_out)
A:jax.interpreters.ad.num_primals->len(nonzeros)
A:jax.interpreters.ad.primals->list(primals_and_nztangents[:num_primals])
A:jax.interpreters.ad.nonzero_tangents->iter(primals_and_nztangents[num_primals:])
A:jax.interpreters.ad.new_invars->_perm(primals_in, tangents_in, jaxpr.jaxpr.invars)
A:jax.interpreters.ad.new_outvars->_perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)
A:jax.interpreters.ad.new_jaxpr->core.Jaxpr(jaxpr.jaxpr.constvars, new_invars, new_outvars, jaxpr.jaxpr.eqns)
A:jax.interpreters.ad.new_in_avals->_perm(primals_in, tangents_in, jaxpr.in_avals)
A:jax.interpreters.ad.new_out_avals->_perm(primals_out, tangents_out, jaxpr.out_avals)
A:jax.interpreters.ad.new_typed_jaxpr->core.TypedJaxpr(new_jaxpr, jaxpr.literals, new_in_avals, new_out_avals)
A:jax.interpreters.ad.n->sum(primal_counts)
A:jax.interpreters.ad.primal_groups->split_list(primals, primal_counts[:-1])
A:jax.interpreters.ad.tangent_groups->split_list(tangents, tangent_counts[:-1])
jax.interpreters.ad.JVPTrace(Trace)
jax.interpreters.ad.JVPTrace.join(self,xt,yt)
jax.interpreters.ad.JVPTrace.lift(self,val)
jax.interpreters.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.ad.JVPTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.ad.JVPTrace.pure(self,val)
jax.interpreters.ad.JVPTrace.sublift(self,val)
jax.interpreters.ad.JVPTracer(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.aval(self)
jax.interpreters.ad.JVPTracer.full_lower(self)
jax.interpreters.ad.UndefinedPrimal(self,aval)
jax.interpreters.ad.UndefinedPrimal.__repr__(self)
jax.interpreters.ad._eval_primals(jaxpr,args)
jax.interpreters.ad._eval_subjaxpr_primals(prim,jaxpr,in_vals,params)
jax.interpreters.ad._interleave(xs,ys)
jax.interpreters.ad._perm(primal_counts,tangent_counts,lst)
jax.interpreters.ad._primal_tangent_shapes_match(primal,tangent)
jax.interpreters.ad.add_tangents(x,y)
jax.interpreters.ad.backward_pass(jaxpr:core.Jaxpr,consts,args,cotangents_in)
jax.interpreters.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax.interpreters.ad.call_transpose(primitive,params,call_jaxpr,args,ct)
jax.interpreters.ad.defbilinear_broadcasting(bcast,prim,lhs_rule,rhs_rule)
jax.interpreters.ad.defjvp(primitive,*jvprules)
jax.interpreters.ad.defjvp2(primitive,*jvprules)
jax.interpreters.ad.defjvp_zero(primitive)
jax.interpreters.ad.deflinear(primitive,transpose_rule)
jax.interpreters.ad.deflinear2(primitive,transpose_rule)
jax.interpreters.ad.defvjp(prim,*vjps)
jax.interpreters.ad.defvjp2(prim,*vjps)
jax.interpreters.ad.defvjp_all(prim,custom_vjp)
jax.interpreters.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax.interpreters.ad.get_primitive_transpose(p)
jax.interpreters.ad.identity(x)
jax.interpreters.ad.ignore_consts(ct,pval)
jax.interpreters.ad.instantiate_zeros(example,tangent)
jax.interpreters.ad.instantiate_zeros_aval(aval,tangent)
jax.interpreters.ad.is_undefined_primal(x)
jax.interpreters.ad.jvp(fun:lu.WrappedFun,has_aux=False,instantiate=True)->Any
jax.interpreters.ad.jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax.interpreters.ad.jvp_subtrace(master,primals,tangents)
jax.interpreters.ad.jvp_subtrace_aux(master,primals,tangents)
jax.interpreters.ad.jvpfun(instantiate,primals,tangents)
jax.interpreters.ad.linear_jvp(primitive,primals,tangents,**params)
jax.interpreters.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linear_transpose2(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linearize(traceable,*primals,**kwargs)
jax.interpreters.ad.map_transpose(primitive,params,call_jaxpr,args,ct)
jax.interpreters.ad.rearrange_binders(jaxpr:core.TypedJaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax.interpreters.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.traceable(num_primals,in_tree_def,*primals_and_tangents)
jax.interpreters.ad.unpair_pval(pval)
jax.interpreters.ad.vjp(traceable,primals,has_aux=False)
jax.interpreters.ad.zero_jvp(primitive,primals,tangents,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/sharded_jit.py----------------------------------------
A:jax.interpreters.sharded_jit.nargs->len(args)
A:jax.interpreters.sharded_jit.bufs->_partition_array(arg, devices, assignments, partition)
A:jax.interpreters.sharded_jit.shards->_flatten((onp.split(s, parts, i) for s in shards))
A:jax.interpreters.sharded_jit.bufs[r][p]->xla.device_put(shards[p], devices[device])
A:jax.interpreters.sharded_jit.nouts->len(out_pvals)
A:jax.interpreters.sharded_jit.handlers->_map(_pval_to_result_handler, partitions, out_pvals)
A:jax.interpreters.sharded_jit.(r, p)->numpy.unravel_index(raw_idx, (nrep, npar))
A:jax.interpreters.sharded_jit.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(fun, in_pvals, instantiate=False, bottom=True)
A:jax.interpreters.sharded_jit.c->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__))
A:jax.interpreters.sharded_jit.xla_consts->_map(c.Constant, consts)
A:jax.interpreters.sharded_jit.xla_args->_xla_sharded_args(c, abstract_args, partitions[0])
A:jax.interpreters.sharded_jit.axis_env->xla.AxisEnv(nrep, [], [])
A:jax.interpreters.sharded_jit.out_nodes->xla.jaxpr_subcomp(subc, call_jaxpr, backend, axis_env, (), freevars, name_stack, *args)
A:jax.interpreters.sharded_jit.out_tuple->subc.Build(out_tuple).Tuple(*out_nodes)
A:jax.interpreters.sharded_jit.built->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__)).Build(out_tuple)
A:jax.interpreters.sharded_jit.num_partitions->numpy.prod(onp.max(partitions, axis=0))
A:jax.interpreters.sharded_jit.device_assignment->numpy.reshape(device_assignment, (-1, num_partitions))
A:jax.interpreters.sharded_jit.compiled->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__)).Build(out_tuple).Compile(compile_options=xb.get_compile_options(nrep, num_partitions, device_assignment), backend=xb.get_backend(None))
A:jax.interpreters.sharded_jit.handle_args->partial(_spatial_partitioned_args, compiled.local_devices(), device_assignment, partitions[0])
A:jax.interpreters.sharded_jit.handle_outs->_pvals_to_results_handler(nrep, num_partitions, partitions[1], out_pvals)
A:jax.interpreters.sharded_jit.subc->subc.Build(out_tuple).Build(out_tuple)
A:jax.interpreters.sharded_jit.input_bufs->in_handler(args)
A:jax.interpreters.sharded_jit.out_bufs->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__)).Build(out_tuple).Compile(compile_options=xb.get_compile_options(nrep, num_partitions, device_assignment), backend=xb.get_backend(None)).ExecuteOnLocalDevices(list(input_bufs), tuple_arguments=False)
A:jax.interpreters.sharded_jit.shape->lib.xla_client.Shape.array_shape(a.dtype, (4, 8))
A:jax.interpreters.sharded_jit.proto->lib.xla_client.OpSharding()
A:jax.interpreters.sharded_jit.proto.tile_assignment_dimensions->list(sharding)
A:jax.interpreters.sharded_jit.proto.tile_assignment_devices->list(range(onp.product(sharding)))
A:jax.interpreters.sharded_jit.num_partitions_set->set((_get_num_partitions(parts) for parts in partitions))
A:jax.interpreters.sharded_jit.partitions->params.pop('partitions')
A:jax.interpreters.sharded_jit.name->params.pop('name')
A:jax.interpreters.sharded_jit.compiled_fun->_sharded_callable(fun, partitions, name, *map(xla.abstractify, args))
A:jax.interpreters.sharded_jit.sharded_call_p->core.Primitive('sharded_call')
A:jax.interpreters.sharded_jit.sharded_call->partial(core.call_bind, sharded_call_p)
A:jax.interpreters.sharded_jit.f->lu.wrap_init(fun)
A:jax.interpreters.sharded_jit.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax.interpreters.sharded_jit.(flat_fun, out_tree)->flatten_fun(f, in_tree)
A:jax.interpreters.sharded_jit.out->sharded_call(flat_fun, *args_flat, partitions=partitions, name=flat_fun.__name__)
jax.interpreters.sharded_jit._array_result_handler(partition,aval)
jax.interpreters.sharded_jit._aval_to_result_handler(partition,aval)
jax.interpreters.sharded_jit._chunk(lst,sz)
jax.interpreters.sharded_jit._execute_spatially_partitioned(compiled,in_handler,out_handler,*args)
jax.interpreters.sharded_jit._flatten(lst)
jax.interpreters.sharded_jit._get_num_partitions(partitions)
jax.interpreters.sharded_jit._map(f,*xs)
jax.interpreters.sharded_jit._partition_array(x,devices,assignments,partition)
jax.interpreters.sharded_jit._pval_to_result_handler(partition,pval)
jax.interpreters.sharded_jit._pvals_to_results_handler(nrep,npar,partitions,out_pvals)
jax.interpreters.sharded_jit._sharded_call_impl(fun:lu.WrappedFun,*args,**params)
jax.interpreters.sharded_jit._sharded_callable(fun:lu.WrappedFun,partitions,name,*abstract_args)
jax.interpreters.sharded_jit._sharded_jit_translation_rule(c,axis_env,freevar_nodes,in_nodes,name_stack,partitions,backend,name,call_jaxpr)
jax.interpreters.sharded_jit._sharding_to_proto(sharding)
jax.interpreters.sharded_jit._spatial_partitioned_args(devices,assignments,partitions,args)
jax.interpreters.sharded_jit._xla_sharded_args(c,avals,partitions)
jax.interpreters.sharded_jit.get_num_partitions(partitions)
jax.interpreters.sharded_jit.jaxpr_partitions(jaxpr)
jax.interpreters.sharded_jit.sharded_jit(fun,partitions)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/interpreters/xla.py----------------------------------------
A:jax.interpreters.xla.x->_copy_device_array_to_device(x, device)
A:jax.interpreters.xla.typ->type(x)
A:jax.interpreters.xla.handler->aval_to_result_handler(device, a)
A:jax.interpreters.xla.canonicalize_dtype_handlers[_t]->partial(_canonicalize_python_scalar_dtype, _t)
A:jax.interpreters.xla.aval_fn->pytype_aval_mappings.get(typ)
A:jax.interpreters.xla.pytype_aval_mappings[_t]->partial(_make_abstract_python_scalar, _t)
A:jax.interpreters.xla.aval->abstractify(x)
A:jax.interpreters.xla.compiled_fun->_xla_callable(fun, device, backend, name, *map(arg_spec, args))
A:jax.interpreters.xla.(avals, arg_devices)->unzip2(arg_specs)
A:jax.interpreters.xla.device->_device_from_arg_devices([device])
A:jax.interpreters.xla.backend->lib.xla_bridge.get_device_backend(device)
A:jax.interpreters.xla.aval_out->prim.abstract_eval(*avals, **params)
A:jax.interpreters.xla.handle_result->aval_to_result_handler(device, aval_out)
A:jax.interpreters.xla.handlers->tuple(map(partial(aval_to_result_handler, device), aval_out))
A:jax.interpreters.xla.nreps->jaxpr_replicas(jaxpr)
A:jax.interpreters.xla.built_c->lib.xla_bridge.make_computation_builder('lazy_force').Build(xla_out)
A:jax.interpreters.xla.options->lib.xla_bridge.get_compile_options(num_replicas=1, num_partitions=1, device_assignment=device and (device.id,))
A:jax.interpreters.xla.compiled->lib.xla_bridge.make_computation_builder('lazy_force').Build(xla_out).Compile(compile_options=options, backend=backend)
A:jax.interpreters.xla.c->lib.xla_bridge.make_computation_builder('lazy_force')
A:jax.interpreters.xla.xla_args->_xla_callable_args(c, abstract_args, tuple_args)
A:jax.interpreters.xla.(device,)->lib.xla_bridge.make_computation_builder('lazy_force').Build(xla_out).Compile(compile_options=options, backend=backend).local_devices()
A:jax.interpreters.xla.out_bufs->lib.xla_bridge.make_computation_builder('lazy_force').Build(xla_out).Compile(compile_options=options, backend=backend).Execute(input_bufs, tuple_arguments=tuple_args)
A:jax.interpreters.xla.in_nodes->list(map(read, eqn.invars))
A:jax.interpreters.xla.ans->rule(c, axis_env, in_nodes, name_stack, backend=backend, **new_params)
A:jax.interpreters.xla.new_params->check_backend_params(eqn.params, backend)
A:jax.interpreters.xla.replica_groups->axis_groups(axis_env, eqn.params['axis_name'])
A:jax.interpreters.xla.num_elements->len(c.GetShape(ans).tuple_shapes())
A:jax.interpreters.xla.inner_backend->params.get('backend', None)
A:jax.interpreters.xla.mesh_axes->tuple(map(partial(axis_read, axis_env), name))
A:jax.interpreters.xla.(trailing_size, ragged)->divmod(nrep, prod(mesh_spec))
A:jax.interpreters.xla.iota->numpy.arange(prod(full_spec)).reshape(full_spec)
A:jax.interpreters.xla.groups->numpy.reshape(onp.moveaxis(iota, mesh_axes, onp.arange(len(mesh_axes))), (prod(onp.take(full_spec, mesh_axes)), -1))
A:jax.interpreters.xla.call_jaxpr->eqn.params.get('call_jaxpr')
A:jax.interpreters.xla.(abstract_args, arg_devices)->unzip2(arg_specs)
A:jax.interpreters.xla.(jaxpr, pvals, consts)->pe.trace_to_jaxpr(fun, pvals, instantiate=False, stage_out_calls=True, bottom=True)
A:jax.interpreters.xla.result_handlers->tuple(map(partial(_pval_to_result_handler, device), pvals))
A:jax.interpreters.xla.xla_consts->_map(c.Constant, consts)
A:jax.interpreters.xla.out_nodes->jaxpr_subcomp(remat_subc, call_jaxpr, backend, axis_env, (), extend_name_stack(name_stack, wrap_name(name, 'remat')), *args)
A:jax.interpreters.xla.built->lib.xla_bridge.make_computation_builder('lazy_force').Build(_make_unit(c))
A:jax.interpreters.xla.tuple_param->lib.xla_bridge.make_computation_builder('lazy_force').ParameterWithShape(xc.Shape.tuple_shape([aval_to_xla_shape(a) for a in avals if a is not abstract_token]))
A:jax.interpreters.xla.xla_inputs->iter(xla_destructure(c, tuple_param))
A:jax.interpreters.xla.(out,)->lib.xla_bridge.make_computation_builder('lazy_force').Build(xla_out).Compile(compile_options=options, backend=backend).local_devices()
A:jax.interpreters.xla.xla_call_p->core.Primitive('xla_call')
A:jax.interpreters.xla.xla_call->partial(core.call_bind, xla_call_p)
A:jax.interpreters.xla.subc->subc.Build(subc.Tuple(*out_nodes)).Build(subc.Tuple(*out_nodes))
A:jax.interpreters.xla.ad.primitive_transposes[xla_call_p]->partial(ad.call_transpose, xla_call_p)
A:jax.interpreters.xla.shape->lib.xla_bridge.make_computation_builder('lazy_force').GetShape(x)
A:jax.interpreters.xla.zero->dummy_subc.Build(dummy_subc.Tuple(*out_nodes)).Constant(onp.array(0, dtype=dtype))
A:jax.interpreters.xla.(jaxpr, _, consts)->pe.trace_to_jaxpr(lu.wrap_init(fun, params), pvals, instantiate=True)
A:jax.interpreters.xla.consts->_map(c.Constant, consts)
A:jax.interpreters.xla.outs->jaxpr_subcomp(c, jaxpr, backend, axis_env, consts, name_stack, *xla_args)
A:jax.interpreters.xla.token->Token()
A:jax.interpreters.xla._forward_to_value->partial(_forward_method, '_value')
A:jax.interpreters.xla.self._npy_value->_force(self).device_buffer.to_py()
A:jax.interpreters.xla.prefix->'{}('.format(self.__class__.__name__)
A:jax.interpreters.xla.s->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width)
A:jax.interpreters.xla.dtype_str->'dtype={})'.format(self.dtype.name)
A:jax.interpreters.xla.__str__->partialmethod(_forward_to_value, str)
A:jax.interpreters.xla.__bool____nonzero__->partialmethod(_forward_to_value, bool)
A:jax.interpreters.xla.__hex__->partialmethod(_forward_to_value, hex)
A:jax.interpreters.xla.__oct__->partialmethod(_forward_to_value, oct)
A:jax.interpreters.xla.__index__->partialmethod(_forward_to_value, op.index)
A:jax.interpreters.xla.__reduce__->partialmethod(_forward_to_value, op.methodcaller('__reduce__'))
A:jax.interpreters.xla.deleted_buffer->DeletedBuffer()
A:jax.interpreters.xla.pytype_aval_mappings[DeviceArray]->operator.attrgetter('aval')
A:jax.interpreters.xla.base_val->lib.xla_bridge.make_computation_builder('lazy_force').Constant(val.device_buffer.to_py())
A:jax.interpreters.xla.moved_buf->lib.xla_client.Buffer.from_pyval(x.device_buffer.to_py(), device, backend=xb.get_device_backend(device))
A:jax.interpreters.xla.force_fun->_lazy_force_computation(sticky, x.aval, device, x._lazy_expr)
A:jax.interpreters.xla.param->lib.xla_bridge.make_computation_builder('lazy_force').ParameterWithShape(xc.Shape.array_shape(aval.dtype, param_shape))
A:jax.interpreters.xla.xla_out->lazy.stage_lexpr(c, lexpr, param)
A:jax.interpreters.xla.a->abstractify(x)
A:jax.interpreters.xla.device_put_p->core.Primitive('device_put')
A:jax.interpreters.xla.rng->lib.xla_bridge.make_computation_builder('lazy_force').RngUniform(c.Constant(onp.array(0, dtype=onp.float32)), c.Constant(onp.array(1, dtype=onp.float32)), [])
A:jax.interpreters.xla.pred->lib.xla_bridge.make_computation_builder('lazy_force').Lt(rng, c.Constant(onp.array(2, dtype=onp.float32)))
A:jax.interpreters.xla.true_op->lib.xla_bridge.make_computation_builder('lazy_force').Tuple(*in_nodes)
A:jax.interpreters.xla.remat_subc->remat_subc.Build(remat_subc.Tuple(*out_nodes)).Build(remat_subc.Tuple(*out_nodes))
A:jax.interpreters.xla.input_op->remat_subc.Build(remat_subc.Tuple(*out_nodes)).Build(remat_subc.Tuple(*out_nodes)).ParameterWithShape(c.GetShape(true_op), replicated=[])
A:jax.interpreters.xla.dummy_subc->dummy_subc.Build(dummy_subc.Tuple(*out_nodes)).Build(dummy_subc.Tuple(*out_nodes))
jax.interpreters.xla.AxisEnv(self,nreps,names=(),sizes=(),devices=None)
jax.interpreters.xla.DeletedBuffer(object)
jax.interpreters.xla.DeviceArray(self,aval,device,lazy_expr,device_buffer)
jax.interpreters.xla.DeviceArray.__array__(self,dtype=None,context=None)
jax.interpreters.xla.DeviceArray.__complex__(self)
jax.interpreters.xla.DeviceArray.__cuda_array_interface__(self)
jax.interpreters.xla.DeviceArray.__eq__(self,other)
jax.interpreters.xla.DeviceArray.__float__(self)
jax.interpreters.xla.DeviceArray.__format__(self,format_spec)
jax.interpreters.xla.DeviceArray.__getitem__(self,i)
jax.interpreters.xla.DeviceArray.__hash__(self)
jax.interpreters.xla.DeviceArray.__int__(self)
jax.interpreters.xla.DeviceArray.__iter__(self)
jax.interpreters.xla.DeviceArray.__len__(self)
jax.interpreters.xla.DeviceArray.__repr__(self)
jax.interpreters.xla.DeviceArray.__reversed__(self)
jax.interpreters.xla.DeviceArray._value(self)
jax.interpreters.xla.DeviceArray.copy(self)
jax.interpreters.xla.DeviceArray.copy_to_host_async(self)
jax.interpreters.xla.DeviceArray.delete(self)
jax.interpreters.xla.DeviceArray.dtype(self)
jax.interpreters.xla.DeviceArray.item(self)
jax.interpreters.xla.DeviceArray.ndim(self)
jax.interpreters.xla.DeviceArray.shape(self)
jax.interpreters.xla.DeviceArray.size(self)
jax.interpreters.xla.DeviceConstant(self,device=None)
jax.interpreters.xla.DeviceConstant.device(self)
jax.interpreters.xla.DeviceConstant.to_py(self)
jax.interpreters.xla.DeviceValue(self,aval,device_buffer)
jax.interpreters.xla.DeviceValue._check_if_deleted(self)
jax.interpreters.xla.DeviceValue.block_until_ready(self)
jax.interpreters.xla.Token(object)
jax.interpreters.xla._array_aval_from_xla_shape(xla_shape)
jax.interpreters.xla._axis_groups(nrep,mesh_spec,mesh_axes)
jax.interpreters.xla._canonicalize_ndarray_dtype(x)
jax.interpreters.xla._canonicalize_python_scalar_dtype(typ,x)
jax.interpreters.xla._check_nans(name,xla_shape,buf)
jax.interpreters.xla._copy_device_array_to_device(x,device)
jax.interpreters.xla._device_array_constant_handler(c,val,canonicalize_types=True)
jax.interpreters.xla._device_from_arg_devices(devices)
jax.interpreters.xla._device_put_array(x,device)
jax.interpreters.xla._device_put_device_array(x,device)
jax.interpreters.xla._device_put_impl(x,device=None)
jax.interpreters.xla._device_put_scalar(x,device)
jax.interpreters.xla._device_put_unit(_,device)
jax.interpreters.xla._execute_compiled(compiled,backend,handlers,tuple_args,*args)
jax.interpreters.xla._execute_compiled_primitive(prim,compiled,backend,tuple_args,result_handler,*args)
jax.interpreters.xla._execute_replicated(compiled,backend,handlers,tuple_args,*args)
jax.interpreters.xla._execute_replicated_primitive(prim,compiled,backend,tuple_args,result_handler,*args)
jax.interpreters.xla._execute_trivial(jaxpr,device,consts,handlers,*args)
jax.interpreters.xla._force(x:DeviceArray)->DeviceArray
jax.interpreters.xla._forward_method(attrname,self,fun,*args)
jax.interpreters.xla._get_device(device,backend)
jax.interpreters.xla._lazy_force_computation(sticky,aval,device,lexpr)->Callable[[DeviceArray], DeviceArray]
jax.interpreters.xla._make_abstract_python_scalar(typ,_)
jax.interpreters.xla._make_abstract_unit(_)
jax.interpreters.xla._make_array_shape(a)
jax.interpreters.xla._make_unit(c)
jax.interpreters.xla._map(f,*xs)
jax.interpreters.xla._pval_to_result_handler(device,pval)
jax.interpreters.xla._remat_translation_rule(c,axis_env,in_nodes,name_stack,backend,name,call_jaxpr,device=None,concrete=None)
jax.interpreters.xla._xla_call_impl(fun:lu.WrappedFun,*args,device,backend,name)
jax.interpreters.xla._xla_call_translation_rule(c,axis_env,in_nodes,name_stack,backend,name,call_jaxpr,device=None)
jax.interpreters.xla._xla_callable(fun:lu.WrappedFun,device,backend,name,*arg_specs)
jax.interpreters.xla._xla_callable_args(c,avals,tuple_args)
jax.interpreters.xla._xla_callable_device(nreps,backend,device,arg_devices)
jax.interpreters.xla.abstractify(x)->core.AbstractValue
jax.interpreters.xla.add_jaxvals_translation_rule(c,x,y)
jax.interpreters.xla.apply_primitive(prim,*args,**params)
jax.interpreters.xla.arg_spec(x)
jax.interpreters.xla.array_result_handler(device,aval)
jax.interpreters.xla.aval_to_result_handler(device,aval)
jax.interpreters.xla.aval_to_xla_shape(aval)
jax.interpreters.xla.axis_groups(axis_env,name)
jax.interpreters.xla.axis_read(axis_env,axis_name)
jax.interpreters.xla.canonicalize_dtype(x)
jax.interpreters.xla.check_backend_params(params,outer_backend)
jax.interpreters.xla.check_nans(prim,bufs)
jax.interpreters.xla.device_put(x,device=None)
jax.interpreters.xla.eqn_replicas(eqn)
jax.interpreters.xla.extend_axis_env(env,name,size)
jax.interpreters.xla.identity(x)
jax.interpreters.xla.initial_style_primitive_replicas(params)
jax.interpreters.xla.is_device_constant(x)
jax.interpreters.xla.jaxpr_collectives(jaxpr)
jax.interpreters.xla.jaxpr_has_pmap(jaxpr)
jax.interpreters.xla.jaxpr_literals(jaxpr)
jax.interpreters.xla.jaxpr_replicas(jaxpr)
jax.interpreters.xla.jaxpr_subcomp(c,jaxpr,backend,axis_env,consts,name_stack,*args)
jax.interpreters.xla.lower_fun(fun)
jax.interpreters.xla.lower_fun_initial_style(fun)
jax.interpreters.xla.prefetch(x)
jax.interpreters.xla.primitive_computation(prim,axis_env,backend,tuple_args,*avals,**params)
jax.interpreters.xla.primitive_subcomputation(prim,*avals,**params)
jax.interpreters.xla.xla_destructure(c,ans)
jax.interpreters.xla.xla_primitive_callable(prim,*arg_specs,**params)
jax.interpreters.xla.zeros_like_translation_rule(c,x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/nn/functions.py----------------------------------------
A:jax.nn.functions.safe_x->jax.numpy.where(x > 0, 0.0, x)
A:jax.nn.functions.sqrt_2_over_pi->numpy.sqrt(2 / onp.pi).astype(x.dtype)
A:jax.nn.functions.unnormalized->jax.numpy.exp(x - x.max(axis, keepdims=True))
A:jax.nn.functions.mean->jax.numpy.mean(x, axis, keepdims=True)
A:jax.nn.functions.dtype->jax.dtypes.canonicalize_dtype(dtype)
A:jax.nn.functions.x->jax.numpy.asarray(x)
jax.nn.celu(x,alpha=1.0)
jax.nn.elu(x,alpha=1.0)
jax.nn.functions.celu(x,alpha=1.0)
jax.nn.functions.elu(x,alpha=1.0)
jax.nn.functions.gelu(x)
jax.nn.functions.glu(x,axis=-1)
jax.nn.functions.hard_tanh(x)
jax.nn.functions.leaky_relu(x,negative_slope=0.01)
jax.nn.functions.log_sigmoid(x)
jax.nn.functions.log_softmax(x,axis=-1)
jax.nn.functions.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.functions.one_hot(x,num_classes,*,dtype=np.float64)
jax.nn.functions.relu(x)
jax.nn.functions.selu(x)
jax.nn.functions.sigmoid(x)
jax.nn.functions.soft_sign(x)
jax.nn.functions.softmax(x,axis=-1)
jax.nn.functions.softplus(x)
jax.nn.functions.swish(x)
jax.nn.gelu(x)
jax.nn.glu(x,axis=-1)
jax.nn.hard_tanh(x)
jax.nn.leaky_relu(x,negative_slope=0.01)
jax.nn.log_sigmoid(x)
jax.nn.log_softmax(x,axis=-1)
jax.nn.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.one_hot(x,num_classes,*,dtype=np.float64)
jax.nn.relu(x)
jax.nn.selu(x)
jax.nn.sigmoid(x)
jax.nn.soft_sign(x)
jax.nn.softmax(x,axis=-1)
jax.nn.softplus(x)
jax.nn.swish(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/nn/initializers.py----------------------------------------
A:jax.nn.initializers.(fan_in, fan_out)->_compute_fans(shape, in_axis, out_axis)
A:jax.nn.initializers.variance->jax.numpy.array(scale / denominator, dtype=dtype)
A:jax.nn.initializers.xavier_uniformglorot_uniform->partial(variance_scaling, 1.0, 'fan_avg', 'uniform')
A:jax.nn.initializers.xavier_normalglorot_normal->partial(variance_scaling, 1.0, 'fan_avg', 'truncated_normal')
A:jax.nn.initializers.lecun_uniform->partial(variance_scaling, 1.0, 'fan_in', 'uniform')
A:jax.nn.initializers.lecun_normal->partial(variance_scaling, 1.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.kaiming_uniformhe_uniform->partial(variance_scaling, 2.0, 'fan_in', 'uniform')
A:jax.nn.initializers.kaiming_normalhe_normal->partial(variance_scaling, 2.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.A->jax.random.normal(key, matrix_shape, dtype)
A:jax.nn.initializers.(Q, R)->jax.numpy.linalg.qr(A)
A:jax.nn.initializers.Q->jax.numpy.moveaxis(Q, -1, column_axis)
A:jax.nn.initializers.ortho_init->orthogonal(scale=scale, column_axis=column_axis, dtype=dtype)
A:jax.nn.initializers.ortho_matrix->ortho_init(key, shape[-2:])
A:jax.nn.initializers.W->jax.numpy.zeros(shape, dtype=dtype)
jax.nn.initializers._compute_fans(shape,in_axis=-2,out_axis=-1)
jax.nn.initializers.delta_orthogonal(scale=1.0,column_axis=-1,dtype=np.float32)
jax.nn.initializers.normal(stddev=0.01,dtype=np.float32)
jax.nn.initializers.ones(key,shape,dtype=np.float32)
jax.nn.initializers.orthogonal(scale=1.0,column_axis=-1,dtype=np.float32)
jax.nn.initializers.uniform(scale=0.01,dtype=np.float32)
jax.nn.initializers.variance_scaling(scale,mode,distribution,in_axis=-2,out_axis=-1,dtype=np.float32)
jax.nn.initializers.zeros(key,shape,dtype=np.float32)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax/lax.py----------------------------------------
A:jax.lax.lax.ndim->_max((len(shape) for shape in shapes))
A:jax.lax.lax.shapes->numpy.array([operand.shape for operand in operands])
A:jax.lax.lax.is_zero->numpy.any(shapes == 0, axis=0)
A:jax.lax.lax.max_shape->numpy.max(shapes, axis=0)
A:jax.lax.lax.result_shape->numpy.floor_divide(onp.add(onp.subtract(limit_indices, start_indices), strides) - 1, strides)
A:jax.lax.lax.a->convert_element_type(a, b_dtype)
A:jax.lax.lax.b->convert_element_type(b, a_dtype)
A:jax.lax.lax.x->interpreters.batching.bdim_at_front(x, x_bdim, size)
A:jax.lax.lax.new_dtype->dtypes.canonicalize_dtype(new_dtype)
A:jax.lax.lax.operand->interpreters.batching.moveaxis(operand, o_bdims, 0)
A:jax.lax.lax.old_dtype->_dtype(operand)
A:jax.lax.lax.dimension_numbers->_conv_general_proto(dimension_numbers)
A:jax.lax.lax.padding->list(map(onp.sum, padding))
A:jax.lax.lax.contract_dims->tuple(map(tuple, contract_dims))
A:jax.lax.lax.batch_dims->tuple(map(tuple, batch_dims))
A:jax.lax.lax.lhs_noncontract_dims->tuple(sorted(set(range(onp.ndim(lhs))) - set(lhs_batch_dims) - set(lhs_contract_dims)))
A:jax.lax.lax.rhs_noncontract_dims->tuple(sorted(set(range(onp.ndim(rhs))) - set(rhs_batch_dims) - set(rhs_contract_dims)))
A:jax.lax.lax.lhs->interpreters.batching.moveaxis(lhs, lbd, lhs.ndim - 1)
A:jax.lax.lax.rhs->interpreters.batching.moveaxis(rhs, rbd, rhs.ndim - 1)
A:jax.lax.lax.new_lhs_shape->numpy.insert(onp.array(onp.shape(lhs), dtype=onp.int64), len(lhs_batch_dims) + len(lhs_noncontract_dims), (1,) * len(rhs_noncontract_dims))
A:jax.lax.lax.new_rhs_shape->numpy.insert(onp.array(onp.shape(rhs), dtype=onp.int64), len(lhs_batch_dims), (1,) * len(lhs_noncontract_dims))
A:jax.lax.lax.dims->tuple(range(len(update_shape)))
A:jax.lax.lax.shape->tuple(map(xla.aval_to_xla_shape, shapes))
A:jax.lax.lax.new_sizes->tuple(new_sizes)
A:jax.lax.lax.start_indices->concatenate([counts, start_indices], len(count_shape) - 1)
A:jax.lax.lax.(jaxpr, consts)->_reduction_jaxpr(mul, _abstractify(init_value))
A:jax.lax.lax.indices->concatenate([broadcast_in_dim(x, (size, 1), broadcast_dimensions=(0,) if i is not None else ()) for (x, i) in zip(indices, bdims)], dimension=1)
A:jax.lax.lax.slice_sizes->iter(onp.delete(slice_sizes, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.offset_dims->tuple(onp.add(1, dimension_numbers.offset_dims))
A:jax.lax.lax.dnums->ScatterDimensionNumbers(update_window_dims=update_window_dims, inserted_window_dims=inserted_window_dims, scatter_dims_to_operand_dims=scatter_dims_to_operand_dims)
A:jax.lax.lax.permutation->tuple(permutation)
A:jax.lax.lax.monoid_reducer->_get_monoid_window_reducer(computation, init_value)
A:jax.lax.lax.pval->interpreters.partial_eval.PartialVal((aval, core.unit))
A:jax.lax.lax.comp->lu.wrap_init(lambda x, y: (computation(x, y),))
A:jax.lax.lax.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(comp, (pval, pval), instantiate=False)
A:jax.lax.lax.aval->ShapedArray(lazy_expr.shape, operand.dtype)
A:jax.lax.lax.dtype->tuple(map(xla.aval_to_xla_shape, shapes)).numpy_dtype()
A:jax.lax.lax.init_value->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Constant(onp.array(0, dtype))
A:jax.lax.lax.(select_jaxpr, select_consts)->_reduction_jaxpr(select, _abstractify(init_value))
A:jax.lax.lax.(scatter_jaxpr, scatter_consts)->_reduction_jaxpr(scatter, _abstractify(init_value))
A:jax.lax.lax.result->_select_and_scatter_add(t, operand, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.k->int(k)
A:jax.lax.lax.fill_value->tie_in(x, fill_value)
A:jax.lax.lax.size->next((a.shape[bdim] for (a, bdim) in zip(batched_args, batch_dims) if bdim is not None))
A:jax.lax.lax.lazy_expr->lazy.transpose(operand._lazy_expr, permutation)
A:jax.lax.lax.dimension->kwargs.pop('dimension')
A:jax.lax.lax.(N, M)->tuple(map(int, shape))
A:jax.lax.lax.offset->int(offset)
A:jax.lax.lax.axes->tuple(onp.delete(range(len(shape)), broadcast_dimensions))
A:jax.lax.lax.base_shape->tuple(onp.take(shape, axes))
A:jax.lax.lax.pads->padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
A:jax.lax.lax.pad_a->int(onp.ceil(pad_len / 2))
A:jax.lax.lax.ndims->len(lhs.shape)
A:jax.lax.lax.dn->conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)
A:jax.lax.lax.k_shape->numpy.take(rhs.shape, dn.rhs_spec)
A:jax.lax.lax.effective_k_size->map(lambda k, r: (k - 1) * r + 1, k_sdims, rhs_dilation)
A:jax.lax.lax.limit_indices->list(operand.shape)
A:jax.lax.lax.axis->int(axis)
A:jax.lax.lax.start_indices[axis]->int(start_index)
A:jax.lax.lax.limit_indices[axis]->int(limit_index)
A:jax.lax.lax.strides[axis]->int(stride)
A:jax.lax.lax.slice_sizes[axis]->int(slice_size)
A:jax.lax.lax.update->reshape(update, operand.shape[:ax] + (1,) + operand.shape[ax + 1:])
A:jax.lax.lax.batch->tuple(range(lhs.ndim - 2))
A:jax.lax.lax.ShapedArray.broadcast->core.aval_method(broadcast)
A:jax.lax.lax.ShapedArray.transpose->core.aval_method(transpose)
A:jax.lax.lax.ShapedArray.reshape->core.aval_method(reshape)
A:jax.lax.lax.ShapedArray._iter->staticmethod(_iter)
A:jax.lax.lax.prim->standard_primitive(shape_rule, dtype_rule, name, translation_rule=translation_rule)
A:jax.lax.lax.least_specialized->_max(map(type, args), key=operator.attrgetter('array_abstraction_level'))
A:jax.lax.lax.xla_opname->''.join((term.capitalize() for term in name.split('_')))
A:jax.lax.lax.typename->str(onp.dtype(aval_dtype).name)
A:jax.lax.lax.dtype_rule->partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name)
A:jax.lax.lax.standard_unop->partial(unop, _identity)
A:jax.lax.lax.typenames->', '.join((t.__name__ for t in types))
A:jax.lax.lax.shape_rule->partial(_broadcasting_shape_rule, name)
A:jax.lax.lax.standard_naryop->partial(naryop, _input_dtype)
A:jax.lax.lax.x_shape->numpy.shape(x)
A:jax.lax.lax.(broadcast_dimensions,)->numpy.where(onp.equal(x_shape, shape))
A:jax.lax.lax.(squeezed_dimensions,)->numpy.where(onp.not_equal(x_shape, shape))
A:jax.lax.lax.inshape->numpy.delete(x_shape, squeezed_dimensions)
A:jax.lax.lax.neg_p->standard_unop(_num, 'neg')
A:jax.lax.lax.zero->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Constant(onp.array(0, dtype))
A:jax.lax.lax.sign_p->standard_unop(_num, 'sign', translation_rule=_sign_translation_rule)
A:jax.lax.lax.nextafter_p->standard_naryop([_float, _float], 'nextafter', translation_rule=lambda c, x1, x2: c.NextAfter(x1, x2))
A:jax.lax.lax.floor_p->standard_unop(_float, 'floor')
A:jax.lax.lax.ceil_p->standard_unop(_float, 'ceil')
A:jax.lax.lax.round_p->standard_unop(_float, 'round')
A:jax.lax.lax.is_finite_p->unop(_fixed_dtype(onp.bool_), _float, 'is_finite')
A:jax.lax.lax.exp_p->standard_unop(_float | _complex, 'exp')
A:jax.lax.lax.log_p->standard_unop(_float | _complex, 'log')
A:jax.lax.lax.expm1_p->standard_unop(_float | _complex, 'expm1')
A:jax.lax.lax.log1p_p->standard_unop(_float | _complex, 'log1p')
A:jax.lax.lax.tanh_p->standard_unop(_float | _complex, 'tanh')
A:jax.lax.lax.sin_p->standard_unop(_float | _complex, 'sin')
A:jax.lax.lax.cos_p->standard_unop(_float | _complex, 'cos')
A:jax.lax.lax.atan2_p->standard_naryop([_float, _float], 'atan2')
A:jax.lax.lax.sinh_p->standard_unop(_float | _complex, 'sinh')
A:jax.lax.lax.cosh_p->standard_unop(_float | _complex, 'cosh')
A:jax.lax.lax.asinh_p->standard_unop(_float | _complex, 'asinh')
A:jax.lax.lax.acosh_p->standard_unop(_float | _complex, 'acosh')
A:jax.lax.lax.atanh_p->standard_unop(_float | _complex, 'atanh')
A:jax.lax.lax.regularized_incomplete_beta_p->standard_naryop([_float, _float, _float], 'regularized_incomplete_beta')
A:jax.lax.lax.partial_x->exp((b - 1) * log1p(-x) + (a - 1) * log(x) - lbeta)
A:jax.lax.lax.lgamma_p->standard_unop(_float, 'lgamma')
A:jax.lax.lax.digamma_p->standard_unop(_float, 'digamma')
A:jax.lax.lax.igamma_p->standard_naryop([_float, _float], 'igamma')
A:jax.lax.lax.igammac_p->standard_naryop([_float, _float], 'igammac')
A:jax.lax.lax.bessel_i0e_p->standard_unop(_float, 'bessel_i0e')
A:jax.lax.lax.bessel_i1e_p->standard_unop(_float, 'bessel_i1e')
A:jax.lax.lax.safe_x->select(x_is_not_tiny, x, full_like(x, eps))
A:jax.lax.lax.dy_dx->select(x_is_not_tiny, dy_dx, full_like(x, 0.5))
A:jax.lax.lax.erf_p->standard_unop(_float, 'erf')
A:jax.lax.lax.erfc_p->standard_unop(_float, 'erfc')
A:jax.lax.lax.erf_inv_p->standard_unop(_float, 'erf_inv')
A:jax.lax.lax.real_p->unop(_complex_basetype, _complex, 'real')
A:jax.lax.lax.imag_p->unop(_complex_basetype, _complex, 'imag')
A:jax.lax.lax.complex_p->naryop(_complex_dtype, [_complex_elem_types, _complex_elem_types], 'complex')
A:jax.lax.lax.conj_p->unop(_complex_dtype, _complex_elem_types | _complex, 'conj')
A:jax.lax.lax.ad.primitive_jvps[conj_p]->partial(ad.linear_jvp, conj_p)
A:jax.lax.lax.abs_p->unop(_complex_basetype, _num, 'abs')
A:jax.lax.lax.sqrt_p->standard_unop(_float | _complex, 'sqrt')
A:jax.lax.lax.rsqrt_p->standard_unop(_float | _complex, 'rsqrt')
A:jax.lax.lax.pow_p->standard_naryop([_float | _complex, _float | _complex], 'pow')
A:jax.lax.lax.jac->mul(y, pow(x, select(eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))
A:jax.lax.lax.not_p->standard_unop(_bool_or_int, 'not')
A:jax.lax.lax.and_p->standard_naryop([_bool_or_int, _bool_or_int], 'and')
A:jax.lax.lax.or_p->standard_naryop([_bool_or_int, _bool_or_int], 'or')
A:jax.lax.lax.xor_p->standard_naryop([_bool_or_int, _bool_or_int], 'xor')
A:jax.lax.lax.add_p->standard_naryop([_num, _num], 'add')
A:jax.lax.lax.sub_p->standard_naryop([_num, _num], 'sub')
A:jax.lax.lax.mul_p->standard_naryop([_num, _num], 'mul')
A:jax.lax.lax.div_p->standard_naryop([_num, _num], 'div')
A:jax.lax.lax.rem_p->standard_naryop([_num, _num], 'rem')
A:jax.lax.lax.out_shape->_ceil_divide(in_shape, window_strides)
A:jax.lax.lax.which->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').BroadcastInDim(which, out_shape, bcast_dims(which_shape))
A:jax.lax.lax.y->tie_in(*batched_args)
A:jax.lax.lax.comparator->cmp(c)
A:jax.lax.lax.rx->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Real(x)
A:jax.lax.lax.ry->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Real(y)
A:jax.lax.lax.max_p->standard_naryop([_any, _any], 'max', translation_rule=partial(_minmax_translation_rule, minmax=lambda c: c.Max, cmp=lambda c: c.Gt))
A:jax.lax.lax.min_p->standard_naryop([_any, _any], 'min', translation_rule=partial(_minmax_translation_rule, minmax=lambda c: c.Min, cmp=lambda c: c.Lt))
A:jax.lax.lax.shift_left_p->standard_naryop([_int, _int], 'shift_left')
A:jax.lax.lax.shift_right_arithmetic_p->standard_naryop([_int, _int], 'shift_right_arithmetic')
A:jax.lax.lax.shift_right_logical_p->standard_naryop([_int, _int], 'shift_right_logical')
A:jax.lax.lax.eq_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'eq')
A:jax.lax.lax.ne_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'ne')
A:jax.lax.lax.ge_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'ge')
A:jax.lax.lax.gt_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'gt')
A:jax.lax.lax.le_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'le')
A:jax.lax.lax.lt_p->naryop(_fixed_dtype(onp.bool_), [_any, _any], 'lt')
A:jax.lax.lax.new_etype->lib.xla_bridge.dtype_to_etype(new_dtype)
A:jax.lax.lax.convert_element_type_p->standard_primitive(_convert_element_type_shape_rule, _convert_element_type_dtype_rule, 'convert_element_type', _convert_element_type_translation_rule)
A:jax.lax.lax.bitcast_convert_type_p->standard_primitive(_bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule, 'bitcast_convert_type', _bitcast_convert_type_translation_rule)
A:jax.lax.lax.(quot, rem)->divmod(lhs_feature_count, feature_group_count)
A:jax.lax.lax.lhs_trans->numpy.take(lhs_shape, lhs_perm)
A:jax.lax.lax.rhs_trans->numpy.take(rhs_shape, rhs_perm)
A:jax.lax.lax.out_trans->tuple((lhs_trans[0], rhs_trans[0]) + tuple(out_space))
A:jax.lax.lax.(lhs_sdims, rhs_sdims, out_sdims)->map(_conv_sdims, dimension_numbers)
A:jax.lax.lax.t_rhs_spec->_conv_spec_transpose(rhs_spec)
A:jax.lax.lax.trans_dimension_numbers->ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
A:jax.lax.lax.revd_weights->rev(rhs, rhs_sdims)
A:jax.lax.lax.(lhs_trans, rhs_trans, out_trans)->map(_conv_spec_transpose, dimension_numbers)
A:jax.lax.lax.new_lhs->_reshape_axis_into(lhs_bdim, lhs_spec[0], lhs)
A:jax.lax.lax.new_rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[0], new_rhs)
A:jax.lax.lax.out->_select_and_gather_add(t, x, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.conv_general_dilated_p->standard_primitive(_conv_general_dilated_shape_rule, _conv_general_dilated_dtype_rule, 'conv_general_dilated', _conv_general_dilated_translation_rule)
A:jax.lax.lax.new_shape->list(onp.delete(x.shape, src))
A:jax.lax.lax.(size2, ragged)->divmod(shape[src], size1)
A:jax.lax.lax.config->lib.xla_client.PrecisionConfig()
A:jax.lax.lax.lhs_batch_shape->numpy.take(lhs.shape, lhs_batch)
A:jax.lax.lax.rhs_batch_shape->numpy.take(rhs.shape, rhs_batch)
A:jax.lax.lax.lhs_contracting_shape->numpy.take(lhs.shape, lhs_contracting)
A:jax.lax.lax.rhs_contracting_shape->numpy.take(rhs.shape, rhs_contracting)
A:jax.lax.lax.batch_shape->tuple(onp.take(lhs.shape, lhs_batch))
A:jax.lax.lax.lhs_tensored_shape->tuple(onp.delete(lhs.shape, lhs_contract_or_batch))
A:jax.lax.lax.rhs_tensored_shape->tuple(onp.delete(rhs.shape, rhs_contract_or_batch))
A:jax.lax.lax.x_kept->remaining(range(x_ndim), x_contract, x_batch)
A:jax.lax.lax.y_kept->remaining(range(y.ndim), y_contract, y_batch)
A:jax.lax.lax.(ans_batch, ans_y, _)->ranges_like(x_batch, y_kept, x_kept)
A:jax.lax.lax.(ans_batch, _, ans_y)->ranges_like(x_batch, x_kept, y_kept)
A:jax.lax.lax.x_contract_sorted_by_y->list(onp.take(x_contract, onp.argsort(y_contract)))
A:jax.lax.lax.out_axes->numpy.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
A:jax.lax.lax.lhs_contract->tuple(onp.add(lhs_contract, bump_lhs_contract))
A:jax.lax.lax.rhs_contract->tuple(onp.add(rhs_contract, bump_rhs_contract))
A:jax.lax.lax.bump_lhs_contract->numpy.greater_equal(lhs_contract, lbd)
A:jax.lax.lax.bump_rhs_contract->numpy.greater_equal(rhs_contract, rbd)
A:jax.lax.lax.batched_out->dot_general(lhs, rhs, new_dimension_numbers, precision=precision)
A:jax.lax.lax.masked_lhs->select(mask_intersection, lhs, zeros_like_array(lhs))
A:jax.lax.lax.dot_general_p->standard_primitive(_dot_general_shape_rule, _dot_general_dtype_rule, 'dot_general', _dot_general_translation_rule)
A:jax.lax.lax.broadcast_p->standard_primitive(_broadcast_shape_rule, _input_dtype, 'broadcast')
A:jax.lax.lax.operand_ndim->numpy.ndim(operand)
A:jax.lax.lax.new_operand->pad(new_operand, _zero(operand), ((0, 1, 0),) + tuple(((0, 0, 0) for _ in operand_shape)))
A:jax.lax.lax.broadcast_in_dim_p->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
A:jax.lax.lax._clamp_dtype_rule->partial(naryop_dtype_rule, _input_dtype, [_any, _any, _any], 'clamp')
A:jax.lax.lax.clamp_p->standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')
A:jax.lax.lax.op->next((op for op in operands if not isinstance(op, UnshapedArray)))
A:jax.lax.lax.concat_size->sum((o.shape[dimension] for o in operands))
A:jax.lax.lax.limit_points->numpy.cumsum([shape[dimension] for shape in operand_shapes])
A:jax.lax.lax.starts->numpy.zeros((len(operands), t.ndim), dtype=int)
A:jax.lax.lax.limits->numpy.tile(t.shape, (len(operands), 1))
A:jax.lax.lax.concatenate_p->standard_primitive(_concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate', _concatenate_translation_rule)
A:jax.lax.lax.(lo, hi, interior)->zip(*padding_config)
A:jax.lax.lax.unpad_config->zip(onp.negative(lo), onp.negative(hi), onp.zeros_like(interior))
A:jax.lax.lax.unpadded->pad(t, onp.array(0.0, t.dtype), unpad_config)
A:jax.lax.lax.padding_config->list(padding_config)
A:jax.lax.lax.pad_p->standard_primitive(_pad_shape_rule, _pad_dtype_rule, 'pad')
A:jax.lax.lax.old_sizes->numpy.shape(operand)
A:jax.lax.lax.bcast_dims->_is_singleton_reshape(old_sizes, new_sizes)
A:jax.lax.lax.d2->next(new, None)
A:jax.lax.lax.reshape_p->standard_primitive(_reshape_shape_rule, _reshape_dtype_rule, 'reshape', _reshape_translation_rule)
A:jax.lax.lax.rev_p->standard_primitive(_rev_shape_rule, _input_dtype, 'rev')
A:jax.lax.lax.transpose_p->standard_primitive(_transpose_shape_rule, _input_dtype, 'transpose')
A:jax.lax.lax.zeros->full(operand_shape, tie_in(t, _zero(t)))
A:jax.lax.lax.pred->broadcast_in_dim(pred, on_true.shape, [0])
A:jax.lax.lax.on_false->broadcast(on_false, pred.shape)
A:jax.lax.lax.on_true->broadcast(on_true, pred.shape)
A:jax.lax.lax.select_p->standard_primitive(_select_shape_rule, _select_dtype_rule, 'select')
A:jax.lax.lax.strides->numpy.ones(operand.ndim, onp.int32)
A:jax.lax.lax.real_limits->numpy.add(onp.add(start_indices, 1), onp.multiply(onp.subtract(t.shape, 1), strides))
A:jax.lax.lax.new_start_indices->list(start_indices)
A:jax.lax.lax.new_limit_indices->list(limit_indices)
A:jax.lax.lax.new_strides->list(strides)
A:jax.lax.lax.slice_p->standard_primitive(_slice_shape_rule, _input_dtype, 'slice', _slice_translation_rule)
A:jax.lax.lax.tangent_out->_select_and_gather_add(g_source, operand, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.(index, index_bdim)->_batch_dynamic_slice_indices(start_idx, start_idx_bd)
A:jax.lax.lax.dynamic_slice_p->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', _dynamic_slice_translation_rule)
A:jax.lax.lax.val_out->sort_key_val(keys, values, dimension)
A:jax.lax.lax.g_operand->interpreters.ad.instantiate_zeros(operand, g_operand)
A:jax.lax.lax.g_update->interpreters.ad.instantiate_zeros(update, g_update)
A:jax.lax.lax.dynamic_update_slice_p->standard_primitive(_dynamic_update_slice_shape_rule, _dynamic_update_slice_dtype_rule, 'dynamic_update_slice', _dynamic_update_slice_translation_rule)
A:jax.lax.lax.proto->lib.xla_client.ConvolutionDimensionNumbers()
A:jax.lax.lax.msg->'slice_sizes must have rank equal to the gather operand; operand.shape={}, slice_sizes={}'.format(operand.shape, slice_sizes)
A:jax.lax.lax.start_indices_shape->iter(start_indices.shape[:-1])
A:jax.lax.lax.indices_shape->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').GetShape(scatter_indices)
A:jax.lax.lax.scatter_dnums->ScatterDimensionNumbers(update_window_dims=dimension_numbers.offset_dims, inserted_window_dims=dimension_numbers.collapsed_slice_dims, scatter_dims_to_operand_dims=dimension_numbers.start_index_map)
A:jax.lax.lax.collapsed_slice_dims->tuple(onp.add(1, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.start_index_map->tuple(onp.add(1, dimension_numbers.start_index_map))
A:jax.lax.lax.count_shape->list(scatter_indices.shape)
A:jax.lax.lax.counts->_reduce_sum(location_indicators, axes)
A:jax.lax.lax.gather_p->standard_primitive(_gather_shape_rule, _gather_dtype_rule, 'gather', _gather_translation_rule)
A:jax.lax.lax.update_computation->_reduction_computation(c, update_jaxpr, update_consts, init_value)
A:jax.lax.lax.g_updates->interpreters.ad.instantiate_zeros(updates, g_updates)
A:jax.lax.lax.gather_dnums->GatherDimensionNumbers(offset_dims=dnums.update_window_dims, collapsed_slice_dims=dnums.inserted_window_dims, start_index_map=dnums.scatter_dims_to_operand_dims)
A:jax.lax.lax.update_t->gather(t, scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.updates->reshape(updates, (1,) + updates_shape)
A:jax.lax.lax.inserted_window_dims->tuple(onp.add(1, dimension_numbers.inserted_window_dims))
A:jax.lax.lax.scatter_dims_to_operand_dims->tuple(onp.add(1, dimension_numbers.scatter_dims_to_operand_dims))
A:jax.lax.lax.scatter_indices->concatenate([counts, scatter_indices], len(count_shape) - 1)
A:jax.lax.lax.update_window_dims->tuple(onp.add(1, dimension_numbers.update_window_dims))
A:jax.lax.lax.scatter_add_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-add', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_add_p]->partial(_scatter_batching_rule, scatter_add)
A:jax.lax.lax.scatter_min_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-min', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_min_p]->partial(_scatter_batching_rule, scatter_min)
A:jax.lax.lax.scatter_max_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-max', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_max_p]->partial(_scatter_batching_rule, scatter_max)
A:jax.lax.lax.updates_dtype->_dtype(updates)
A:jax.lax.lax.ids_shape->numpy.array(updates_shape, dtype=onp.int32)
A:jax.lax.lax.num_ids->numpy.prod(ids_shape)
A:jax.lax.lax.update_ids->add(reshape(iota(updates_dtype, num_ids), ids_shape), _ones(updates))
A:jax.lax.lax.reshaped_update_ids->reshape(update_ids, (1,) + updates_shape)
A:jax.lax.lax.updates_and_ids->concatenate((updates, reshaped_update_ids), 0)
A:jax.lax.lax.new_dnums->ScatterDimensionNumbers(update_window_dims=(0,) + tuple((d + 1 for d in dnums.update_window_dims)), inserted_window_dims=tuple((d + 1 for d in dnums.inserted_window_dims)), scatter_dims_to_operand_dims=tuple((d + 1 for d in dnums.scatter_dims_to_operand_dims)))
A:jax.lax.lax.outputs->concatenate(outputs, 0)
A:jax.lax.lax.scattered_ids->index_in_dim(outputs, 1, keepdims=False)
A:jax.lax.lax.gathered_update_ids->gather(scattered_ids, scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.masked_g_operand->select(eq(scattered_ids, _zeros(scattered_ids)), g_operand, _zeros(g_operand))
A:jax.lax.lax.masked_g_updates->select(eq(update_ids, gathered_update_ids), g_updates, _zeros(g_updates))
A:jax.lax.lax.scatter_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_p]->partial(_scatter_batching_rule, scatter)
A:jax.lax.lax.xla_computation->_reduction_computation(c, jaxpr, consts, init_value)
A:jax.lax.lax.axis_env->interpreters.xla.AxisEnv(1)
A:jax.lax.lax.subc->lib.xla_bridge.make_computation_builder('reduction_computation')
A:jax.lax.lax.(out,)->interpreters.xla.jaxpr_subcomp(subc, jaxpr, None, axis_env, consts, '', *args)
A:jax.lax.lax.masking.masking_rules[prim]->partial(_reducer_masking_rule, prim, identity)
A:jax.lax.lax.padded_shape->interpreters.masking.padded_shape_as_value(padded_val.shape)
A:jax.lax.lax.mask->_reduce(operator.and_, masks)
A:jax.lax.lax.masked_val->select(mask, padded_val, identity(padded_shape, padded_val.dtype))
A:jax.lax.lax.reduce_p->standard_primitive(_reduce_shape_rule, _input_dtype, 'reduce', _reduce_translation_rule)
A:jax.lax.lax.scalar->ShapedArray((), dtype)
A:jax.lax.lax.broadcast_dimensions->numpy.delete(onp.arange(keys.ndim), keys_bdim)
A:jax.lax.lax.reduce_sum_p->standard_primitive(_reduce_sum_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_sum'), 'reduce_sum', _reduce_sum_translation_rule)
A:jax.lax.lax.input_shape->numpy.array(operand.shape)
A:jax.lax.lax.n->numpy.prod(input_shape[list(axes)])
A:jax.lax.lax.non_axes->numpy.delete(onp.arange(len(input_shape)), axes)
A:jax.lax.lax.tangent->reshape(tangent, new_shape, permutation)
A:jax.lax.lax.one->_const(operand, 1)
A:jax.lax.lax.left_products->_reduce_window_prod(pad(operand, one, left_padding), window_dims, window_strides, xla_client.PaddingType.VALID)
A:jax.lax.lax.right_products->_reduce_window_prod(pad(operand, one, right_padding), window_dims, window_strides, xla_client.PaddingType.VALID)
A:jax.lax.lax.reduce_prod_p->standard_primitive(_reduce_op_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_prod'), 'reduce_prod', _reduce_prod_translation_rule)
A:jax.lax.lax.location_indicators->convert_element_type(_eq_meet(operand, reshape(ans, shape)), g.dtype)
A:jax.lax.lax._reduce_max_translation_rule->partial(_reduce_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_max_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_max', _reduce_max_translation_rule)
A:jax.lax.lax._reduce_min_translation_rule->partial(_reduce_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_min_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_min', _reduce_min_translation_rule)
A:jax.lax.lax._reduce_or_translation_rule->partial(_reduce_logical_translation_rule, or_p, _get_max_identity)
A:jax.lax.lax.reduce_or_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(onp.bool_), 'reduce_or', _reduce_or_translation_rule)
A:jax.lax.lax._reduce_and_translation_rule->partial(_reduce_logical_translation_rule, and_p, _get_min_identity)
A:jax.lax.lax.reduce_and_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(onp.bool_), 'reduce_and', _reduce_and_translation_rule)
A:jax.lax.lax.reduce_window_p->standard_primitive(_reduce_window_shape_rule, _input_dtype, 'reduce_window', _reduce_window_translation_rule)
A:jax.lax.lax.in_pads->padtype_to_pads(input_shape, window_dimensions, window_strides, padding)
A:jax.lax.lax.pad_cotangent->pad(cotangent, _zero(cotangent), padding_config)
A:jax.lax.lax.reduce_window_sum_p->standard_primitive(_reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum', _reduce_window_sum_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_sum_p]->partial(_reduce_window_batch_rule, _reduce_window_sum)
A:jax.lax.lax.operand_padded->numpy.add(operand_shape, onp.add(*zip(*pads)))
A:jax.lax.lax._reduce_window_max_translation_rule->partial(_reduce_window_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_window_max_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max', _reduce_window_max_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_max_p]->partial(_reduce_window_batch_rule, _reduce_window_max)
A:jax.lax.lax._reduce_window_min_translation_rule->partial(_reduce_window_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_window_min_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min', _reduce_window_min_translation_rule)
A:jax.lax.lax._reduce_window_min_batch_rule->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_min_p]->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.select->interpreters.xla.primitive_subcomputation(select_prim, scalar, scalar)
A:jax.lax.lax.scatter->interpreters.xla.primitive_subcomputation(add_p, scalar, scalar)
A:jax.lax.lax.select_and_scatter_p->standard_primitive(_select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter', _select_and_scatter_translation)
A:jax.lax.lax.source_t->_select_and_gather_add(t, operand, select_prim, window_dimensions, window_strides, padding)
A:jax.lax.lax.source->interpreters.batching.moveaxis(source, s_bdims, 0)
A:jax.lax.lax.select_and_scatter_add_p->standard_primitive(_select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add', _select_and_scatter_add_translation)
A:jax.lax.lax.etype->tuple(map(xla.aval_to_xla_shape, shapes)).xla_element_type()
A:jax.lax.lax.word_type->lib.xla_client.dtype_to_etype(word_dtype)
A:jax.lax.lax.double_word_type->lib.xla_client.dtype_to_etype(double_word_dtype)
A:jax.lax.lax.st->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').And(t, const(c, word_dtype, (1 << r_nbits) - 1 << r_nbits))
A:jax.lax.lax.c->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer')
A:jax.lax.lax.t->interpreters.batching.bdim_at_front(t, t_bdim, size)
A:jax.lax.lax.select_and_gather_add_p->standard_primitive(_select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add', _select_and_gather_add_translation)
A:jax.lax.lax.xla.backend_specific_translations['tpu'][select_and_gather_add_p]->partial(_select_and_gather_add_translation, max_bits=32)
A:jax.lax.lax.(_, g_out)->sort_key_val(operand, g, dimension)
A:jax.lax.lax.sort_p->standard_primitive(sort_shape, _input_dtype, 'sort')
A:jax.lax.lax.keys_tangents_out->_sort_jvp_rule(keys_tangents, keys, dimension)
A:jax.lax.lax.values_tangents_out->_sort_jvp_rule(values_tangents, keys, dimension)
A:jax.lax.lax.iota->tuple(range(len(lhs_shape)))
A:jax.lax.lax.(_, perm)->sort_key_val(keys, iota)
A:jax.lax.lax.keys_trans->interpreters.batching.moveaxis(keys, keys_bdim, values_bdim)
A:jax.lax.lax.new_keys->broadcast_in_dim(keys, values.shape, broadcast_dimensions)
A:jax.lax.lax.new_values->broadcast_in_dim(values, keys.shape, broadcast_dimensions)
A:jax.lax.lax.sort_key_val_p->Primitive('sort_key_val')
A:jax.lax.lax.xla.translations[sort_key_val_p]->partial(standard_translate, 'sort_key_val')
A:jax.lax.lax.top_k_p->Primitive('top_k')
A:jax.lax.lax.xla.translations[top_k_p]->partial(standard_translate, 'top_k')
A:jax.lax.lax.tie_in_p->Primitive('tie_in')
A:jax.lax.lax.stop_gradient_p->Primitive('stop_gradient')
A:jax.lax.lax.create_token_p->Primitive('create_token')
A:jax.lax.lax.after_all_p->Primitive('after_all')
A:jax.lax.lax.(flat_shapes, treedef)->lib.pytree.flatten(shape)
A:jax.lax.lax.xs_and_token->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').Infeed(xla_client.Shape.tuple_shape(shape), token)
A:jax.lax.lax.xs->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').GetTupleElement(xs_and_token, 0)
A:jax.lax.lax.token->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').GetTupleElement(xs_and_token, 1)
A:jax.lax.lax.infeed_p->Primitive('infeed')
A:jax.lax.lax.(flat_xs, _)->lib.pytree.flatten(xs)
A:jax.lax.lax.outfeed_p->Primitive('outfeed')
A:jax.lax.lax.rng_uniform_p->Primitive('rng_uniform')
A:jax.lax.lax.pad_sizes->numpy.maximum(0, (out_shape - 1) * window_strides + window_shape - in_shape)
A:jax.lax.lax.types->list(map(onp.dtype, ttypes))
A:jax.lax.lax.lhs_padded->numpy.add(lhs_shape[2:], onp.sum(onp.array(pads).reshape(-1, 2), axis=1))
A:jax.lax.lax.out_space->numpy.sum([unpad_out_space, padding], axis=0).tolist()
A:jax.lax.lax.(lhs_perm, rhs_perm, out_perm)->map(getperm, dimension_numbers, charpairs)
A:jax.lax.lax.obj_arr->numpy.array(obj)
A:jax.lax.lax.x_len->len(x)
A:jax.lax.lax.blacklist->set(itertools.chain(*removed_lists))
A:jax.lax.lax.(lhs_spec, rhs_spec, out_spec)->conv_general_permutations(dimension_numbers)
A:jax.lax.lax.spatial->sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
A:jax.lax.lax.lhs_dilated_shape->_dilate_shape(in_shape, lhs_dilation)
A:jax.lax.lax.rhs_dilated_shape->_dilate_shape(window_dimensions, rhs_dilation)
A:jax.lax.lax.out_dilated_shape->_dilate_shape(out_shape, window_strides)
A:jax.lax.lax.higher_dtype->dtypes.promote_types(a_dtype, b_dtype)
jax.ConvDimensionNumbers(collections.namedtuple('ConvDimensionNumbers',['lhs_spec','rhs_spec','out_spec']))
jax.GatherDimensionNumbers(collections.namedtuple('GatherDimensionNumbers',['offset_dims','collapsed_slice_dims','start_index_map']))
jax.ScatterDimensionNumbers(collections.namedtuple('ScatterDimensionNumbers',['update_window_dims','inserted_window_dims','scatter_dims_to_operand_dims']))
jax._abs_jvp_rule(g,ans,x)
jax._abstractify(x)
jax._add_transpose(t,x,y)
jax._after_all_abstract_eval(*operands)
jax._after_all_translation_rule(c,*operands)
jax._balanced_eq(x,z,y)
jax._batch_dynamic_slice_indices(indices,bdims)
jax._bessel_i1e_jvp(g,y,x)
jax._bitcast_convert_type_dtype_rule(operand,new_dtype)
jax._bitcast_convert_type_shape_rule(operand,new_dtype)
jax._bitcast_convert_type_translation_rule(c,operand,new_dtype)
jax._brcast(x,*others)
jax._brcast_to(x,shape)
jax._broadcast_batch_rule(batched_args,batch_dims,sizes)
jax._broadcast_in_dim_batch_rule(batched_args,batch_dims,shape,broadcast_dimensions)
jax._broadcast_in_dim_impl(operand,shape,broadcast_dimensions)
jax._broadcast_in_dim_shape_rule(operand,shape,broadcast_dimensions)
jax._broadcast_in_dim_transpose_rule(t,shape,broadcast_dimensions)
jax._broadcast_shape_rule(operand,sizes)
jax._canonicalize_precision(precision)
jax._ceil_divide(x1,x2)
jax._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax._check_same_dtypes(name,ignore_fp_precision,*ttypes)
jax._check_shapelike(fun_name,arg_name,obj)
jax._clamp_shape_rule(min,operand,max)
jax._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding)
jax._concatenate_batch_rule(batched_args,batch_dims,dimension)
jax._concatenate_dtype_rule(*operands,**kwargs)
jax._concatenate_shape_rule(*operands,**kwargs)
jax._concatenate_translation_rule(c,*operands,**kwargs)
jax._concatenate_transpose_rule(t,*operands,dimension)
jax._conj_transpose_rule(t,x,input_dtype)
jax._conv_general_dilated_batch_rule(batched_args,batch_dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax._conv_general_dilated_dtype_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,**unused_kwargs)
jax._conv_general_dilated_shape_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,**unused_kwargs)
jax._conv_general_dilated_translation_rule(c,lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax._conv_general_dilated_transpose_lhs(g,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax._conv_general_dilated_transpose_rhs(g,lhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax._conv_general_proto(dimension_numbers)
jax._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax._conv_transpose_padding(k,s,padding)
jax._convert_element_type_dtype_rule(operand,new_dtype,old_dtype)
jax._convert_element_type_shape_rule(operand,new_dtype,old_dtype)
jax._convert_element_type_translation_rule(c,operand,new_dtype,old_dtype)
jax._dilate_shape(shape,dilation)
jax._div_transpose_rule(cotangent,x,y)
jax._dot_general_batch_rule(batched_args,batch_dims,dimension_numbers,precision)
jax._dot_general_dtype_rule(lhs,rhs,dimension_numbers,precision)
jax._dot_general_masking_rule(padded_vals,logical_shapes,dimension_numbers,precision)
jax._dot_general_shape_rule(lhs,rhs,dimension_numbers,precision)
jax._dot_general_translation_rule(c,lhs,rhs,dimension_numbers,precision)
jax._dot_general_transpose_lhs(g,y,dimension_numbers,precision,swap_ans=False)
jax._dot_general_transpose_rhs(g,x,dimension_numbers,precision)
jax._dynamic_slice_batching_rule(batched_args,batch_dims,slice_sizes)
jax._dynamic_slice_dtype_rule(operand,*start_indices,slice_sizes)
jax._dynamic_slice_indices(operand,start_indices)
jax._dynamic_slice_jvp(primals,tangents,slice_sizes)
jax._dynamic_slice_shape_rule(operand,*start_indices,slice_sizes)
jax._dynamic_slice_translation_rule(c,operand,*start_indices,slice_sizes)
jax._dynamic_slice_transpose_rule(t,operand,*start_indices,slice_sizes)
jax._dynamic_update_slice_batching_rule(batched_args,batch_dims)
jax._dynamic_update_slice_dtype_rule(operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_jvp(primals,tangents)
jax._dynamic_update_slice_shape_rule(operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_translation_rule(c,operand,update,*start_indices,**kwargs)
jax._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices)
jax._flip_axes(x,axes)
jax._gather_batching_rule(batched_args,batch_dims,dimension_numbers,slice_sizes)
jax._gather_dimensions_proto(indices_shape,dimension_numbers)
jax._gather_dtype_rule(operand,start_indices,**kwargs)
jax._gather_jvp_rule(g,operand,start_indices,dimension_numbers,slice_sizes)
jax._gather_shape_rule(operand,start_indices,dimension_numbers,slice_sizes)
jax._gather_translation_rule(c,operand,start_indices,dimension_numbers,slice_sizes)
jax._gather_transpose_rule(t,operand,start_indices,dimension_numbers,slice_sizes)
jax._generic_reduce_window_batch_rule(batched_args,batch_dims,jaxpr,consts,window_dimensions,window_strides,padding)
jax._get_max_identity(dtype)
jax._get_min_identity(dtype)
jax._get_monoid_reducer(monoid_op,x)
jax._get_monoid_window_reducer(monoid_op,x)
jax._identity(x)
jax._infeed_abstract_eval(token,shapes=None)
jax._infeed_translation_rule(c,token,shapes=None)
jax._is_axis_merge(s1,s2)
jax._is_axis_split(s1,s2)
jax._is_singleton_reshape(old,new)
jax._iscomplex(x)->bool
jax._iter(tracer)
jax._masking_defreducer(prim,identity)
jax._minmax_translation_rule(c,x,y,minmax=None,cmp=None)
jax._outfeed_abstract_eval(token,*xs)
jax._outfeed_translation_rule(c,token,*xs)
jax._pad_batch_rule(batched_args,batch_dims,padding_config)
jax._pad_dtype_rule(operand,padding_value,padding_config)
jax._pad_shape_rule(operand,padding_value,padding_config)
jax._pad_transpose(t,operand,padding_value,padding_config)
jax._pow_jvp_lhs(g,ans,x,y)
jax._pow_jvp_rhs(g,ans,x,y)
jax._precision_config(precision)
jax._reduce_batch_rule(batched_args,batch_dims,computation,jaxpr,consts,dimensions)
jax._reduce_chooser_jvp_rule(g,ans,operand,axes)
jax._reduce_chooser_shape_rule(operand,axes)
jax._reduce_chooser_translation_rule(prim,identity,c,operand,axes)
jax._reduce_logical_shape_rule(operand,axes)
jax._reduce_logical_translation_rule(prim,identity,c,operand,axes)
jax._reduce_number_dtype_rule(name,operand,*args,**kw)
jax._reduce_op_shape_rule(operand,axes)
jax._reduce_prod(operand,axes)
jax._reduce_prod_jvp_rule(tangent,operand,axes)
jax._reduce_prod_translation_rule(c,operand,axes)
jax._reduce_shape_rule(operand,init_value,computation,jaxpr,consts,dimensions)
jax._reduce_sum_shape_rule(operand,axes)
jax._reduce_sum_translation_rule(c,operand,axes)
jax._reduce_sum_transpose_rule(cotangent,operand,axes)
jax._reduce_translation_rule(c,operand,init_value,computation,jaxpr,consts,dimensions)
jax._reduce_window_batch_rule(reduce_window,batched_args,bdims,window_dimensions,window_strides,padding)
jax._reduce_window_chooser_jvp_rule(prim,g,operand,window_dimensions,window_strides,padding)
jax._reduce_window_chooser_translation_rule(prim,identity,c,operand,window_dimensions,window_strides,padding)
jax._reduce_window_shape_rule(operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax._reduce_window_sum_shape_rule(operand,window_dimensions,window_strides,padding)
jax._reduce_window_sum_translation_rule(c,operand,window_dimensions,window_strides,padding)
jax._reduce_window_sum_transpose_rule(cotangent,operand,window_dimensions,window_strides,padding)
jax._reduce_window_translation_rule(c,operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax._reducer_masking_rule(prim,identity,padded_vals,logical_shapes,axes)
jax._reduction_computation(c,jaxpr,consts,init_value)
jax._reduction_jaxpr(computation,aval)
jax._reshape_axis_into(src,dst,x)
jax._reshape_axis_out_of(src,size1,x)
jax._reshape_batch_rule(batched_args,batch_dims,new_sizes,dimensions,**unused)
jax._reshape_dtype_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax._reshape_impl(operand,new_sizes,dimensions)
jax._reshape_shape_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax._reshape_translation_rule(c,operand,new_sizes,dimensions)
jax._reshape_transpose_rule(t,operand,new_sizes,dimensions)
jax._rev_batch_rule(batched_args,batch_dims,dimensions)
jax._rev_shape_rule(operand,dimensions)
jax._rng_uniform_abstract_eval(a,b,shape)
jax._rng_uniform_translation_rule(c,a,b,shape)
jax._scatter_add_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers)
jax._scatter_add_transpose_rule(t,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers)
jax._scatter_batching_rule(scatter_op,batched_args,batch_dims,update_jaxpr,update_consts,dimension_numbers)
jax._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax._scatter_dtype_rule(operand,scatter_indices,updates,**kwargs)
jax._scatter_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers)
jax._scatter_shape_rule(operand,scatter_indices,updates,**kwargs)
jax._scatter_translation_rule(c,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers)
jax._select_and_gather_add_batching_rule(batched_args,batch_dims,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_shape_rule(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_gather_add_translation(c,tangents,operand,select_prim,window_dimensions,window_strides,padding,max_bits=64)
jax._select_and_gather_add_transpose(t,tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter(operand,select,window_dimensions,window_strides,padding,source,init_value,scatter)
jax._select_and_scatter_add(source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_batch_rule(batched_args,batch_dims,**kwargs)
jax._select_and_scatter_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_shape_rule(source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_translation(c,source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_add_transpose(t,source,operand,select_prim,window_dimensions,window_strides,padding)
jax._select_and_scatter_shape_rule(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._select_and_scatter_translation(c,operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax._select_dtype_rule(pred,on_true,on_false)
jax._select_shape_rule(pred,on_true,on_false)
jax._select_transpose_rule(t,pred,on_true,on_false)
jax._sign_translation_rule(c,x)
jax._slice_batching_rule(batched_args,batch_dims,start_indices,limit_indices,strides)
jax._slice_shape_rule(operand,start_indices,limit_indices,strides)
jax._slice_translation_rule(c,operand,start_indices,limit_indices,strides)
jax._slice_transpose_rule(t,operand,start_indices,limit_indices,strides)
jax._sort_batch_rule(batched_args,batch_dims,dimension)
jax._sort_jvp_rule(g,operand,dimension)
jax._sort_key_val_abstract_eval(keys,values,dimension)
jax._sort_key_val_batch_rule(batched_args,batch_dims,dimension)
jax._sort_key_val_jvp(primals,tangents,dimension)
jax._sort_key_val_transpose_rule(t,keys,values,dimension)
jax._stop_gradient_batch_rule(batched_args,batch_dims)
jax._stop_gradient_jvp_rule(primals,tangents)
jax._sub_transpose(t,x,y)
jax._tie_in_batch_rule(batched_args,batch_dims)
jax._tie_in_transpose_rule(t)
jax._top_k_abstract_eval(operand,k)
jax._transpose_batch_rule(batched_args,batch_dims,permutation)
jax._transpose_impl(operand,permutation)
jax._transpose_shape_rule(operand,permutation)
jax.abs(x)
jax.acos(x)
jax.acosh(x)
jax.add(x,y)
jax.after_all(*operands)
jax.asin(x)
jax.asinh(x)
jax.atan(x)
jax.atan2(x,y)
jax.atanh(x)
jax.batch_matmul(lhs,rhs,precision=None)
jax.bessel_i0e(x)
jax.bessel_i1e(x)
jax.betainc(a,b,x)
jax.betainc_grad_not_implemented(g,a,b,x)
jax.betainc_gradx(g,a,b,x)
jax.bitcast_convert_type(operand,new_dtype)
jax.bitwise_and(x,y)
jax.bitwise_not(x)
jax.bitwise_or(x,y)
jax.bitwise_xor(x,y)
jax.broadcast(operand,sizes)
jax.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.broadcast_shapes(*shapes)
jax.broadcasted_iota(dtype,shape,dimension)
jax.ceil(x)
jax.clamp(min,x,max)
jax.collapse(operand,start_dimension,stop_dimension)
jax.complex(x,y)
jax.concatenate(operands,dimension)
jax.conj(x)
jax.conv(lhs,rhs,window_strides,padding,precision=None)
jax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,precision=None)
jax.conv_general_permutations(dimension_numbers)
jax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads)
jax.conv_transpose(lhs,rhs,strides,padding,rhs_dilation=None,dimension_numbers=None,transpose_kernel=False,precision=None)
jax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,precision=None)
jax.convert_element_type(operand,new_dtype)
jax.cos(x)
jax.cosh(x)
jax.create_token(x)
jax.digamma(x)
jax.div(x,y)
jax.dot(lhs,rhs,precision=None)
jax.dot_general(lhs,rhs,dimension_numbers,precision=None)
jax.dynamic_index_in_dim(operand,index,axis=0,keepdims=True)
jax.dynamic_slice(operand,start_indices,slice_sizes)
jax.dynamic_slice_in_dim(operand,start_index,slice_size,axis=0)
jax.dynamic_update_index_in_dim(operand,update,index,axis)
jax.dynamic_update_slice(operand,update,start_indices)
jax.dynamic_update_slice_in_dim(operand,update,start_index,axis)
jax.eq(x,y)
jax.erf(x)
jax.erf_inv(x)
jax.erfc(x)
jax.exp(x)
jax.expm1(x)
jax.floor(x)
jax.full(shape,fill_value,dtype=None)
jax.full_like(x,fill_value,dtype=None,shape=None)
jax.gamma_grad_not_implemented(a,b,x)
jax.gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.ge(x,y)
jax.gt(x,y)
jax.igamma(a,x)
jax.igamma_gradx(g,a,x)
jax.igammac(a,x)
jax.igammac_gradx(g,a,x)
jax.imag(x)
jax.index_in_dim(operand,index,axis=0,keepdims=True)
jax.index_take(src,idxs,axes)
jax.infeed(token,shape=None)
jax.iota(dtype,size)
jax.is_finite(x)
jax.lax._broadcasting_select(c,which,x,y)
jax.lax._broadcasting_shape_rule(name,*avals)
jax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax._const(example,val)
jax.lax._delta(dtype,shape,axes)
jax.lax._eq_meet(a,b)
jax.lax._eye(dtype,shape,offset)
jax.lax._reduce_and(operand,axes)
jax.lax._reduce_max(operand,axes)
jax.lax._reduce_min(operand,axes)
jax.lax._reduce_or(operand,axes)
jax.lax._reduce_sum(operand,axes)
jax.lax._reduce_window_max(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_min(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_prod(operand,window_dimensions,window_strides,padding)
jax.lax._reduce_window_sum(operand,window_dimensions,window_strides,padding)
jax.lax._select_and_gather_add(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax._tri(dtype,shape,offset)
jax.lax._upcast_fp16_for_computation(f)
jax.lax.lax.ConvDimensionNumbers(collections.namedtuple('ConvDimensionNumbers',['lhs_spec','rhs_spec','out_spec']))
jax.lax.lax.GatherDimensionNumbers(collections.namedtuple('GatherDimensionNumbers',['offset_dims','collapsed_slice_dims','start_index_map']))
jax.lax.lax.ScatterDimensionNumbers(collections.namedtuple('ScatterDimensionNumbers',['update_window_dims','inserted_window_dims','scatter_dims_to_operand_dims']))
jax.lax.lax._abs_jvp_rule(g,ans,x)
jax.lax.lax._abstractify(x)
jax.lax.lax._add_transpose(t,x,y)
jax.lax.lax._after_all_abstract_eval(*operands)
jax.lax.lax._after_all_translation_rule(c,*operands)
jax.lax.lax._balanced_eq(x,z,y)
jax.lax.lax._batch_dynamic_slice_indices(indices,bdims)
jax.lax.lax._bessel_i1e_jvp(g,y,x)
jax.lax.lax._bitcast_convert_type_dtype_rule(operand,new_dtype)
jax.lax.lax._bitcast_convert_type_shape_rule(operand,new_dtype)
jax.lax.lax._bitcast_convert_type_translation_rule(c,operand,new_dtype)
jax.lax.lax._brcast(x,*others)
jax.lax.lax._brcast_to(x,shape)
jax.lax.lax._broadcast_batch_rule(batched_args,batch_dims,sizes)
jax.lax.lax._broadcast_in_dim_batch_rule(batched_args,batch_dims,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_impl(operand,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_shape_rule(operand,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_transpose_rule(t,shape,broadcast_dimensions)
jax.lax.lax._broadcast_shape_rule(operand,sizes)
jax.lax.lax._broadcasting_select(c,which,x,y)
jax.lax.lax._broadcasting_shape_rule(name,*avals)
jax.lax.lax._canonicalize_precision(precision)
jax.lax.lax._ceil_divide(x1,x2)
jax.lax.lax._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax.lax.lax._check_same_dtypes(name,ignore_fp_precision,*ttypes)
jax.lax.lax._check_shapelike(fun_name,arg_name,obj)
jax.lax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax.lax._clamp_shape_rule(min,operand,max)
jax.lax.lax._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding)
jax.lax.lax._concatenate_batch_rule(batched_args,batch_dims,dimension)
jax.lax.lax._concatenate_dtype_rule(*operands,**kwargs)
jax.lax.lax._concatenate_shape_rule(*operands,**kwargs)
jax.lax.lax._concatenate_translation_rule(c,*operands,**kwargs)
jax.lax.lax._concatenate_transpose_rule(t,*operands,dimension)
jax.lax.lax._conj_transpose_rule(t,x,input_dtype)
jax.lax.lax._const(example,val)
jax.lax.lax._conv_general_dilated_batch_rule(batched_args,batch_dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax._conv_general_dilated_dtype_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,**unused_kwargs)
jax.lax.lax._conv_general_dilated_shape_rule(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,**unused_kwargs)
jax.lax.lax._conv_general_dilated_translation_rule(c,lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax._conv_general_dilated_transpose_lhs(g,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_dilated_transpose_rhs(g,lhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_proto(dimension_numbers)
jax.lax.lax._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax.lax.lax._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax.lax.lax._conv_transpose_padding(k,s,padding)
jax.lax.lax._convert_element_type_dtype_rule(operand,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_shape_rule(operand,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_translation_rule(c,operand,new_dtype,old_dtype)
jax.lax.lax._delta(dtype,shape,axes)
jax.lax.lax._dilate_shape(shape,dilation)
jax.lax.lax._div_transpose_rule(cotangent,x,y)
jax.lax.lax._dot_general_batch_rule(batched_args,batch_dims,dimension_numbers,precision)
jax.lax.lax._dot_general_dtype_rule(lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_masking_rule(padded_vals,logical_shapes,dimension_numbers,precision)
jax.lax.lax._dot_general_shape_rule(lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_translation_rule(c,lhs,rhs,dimension_numbers,precision)
jax.lax.lax._dot_general_transpose_lhs(g,y,dimension_numbers,precision,swap_ans=False)
jax.lax.lax._dot_general_transpose_rhs(g,x,dimension_numbers,precision)
jax.lax.lax._dynamic_slice_batching_rule(batched_args,batch_dims,slice_sizes)
jax.lax.lax._dynamic_slice_dtype_rule(operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_indices(operand,start_indices)
jax.lax.lax._dynamic_slice_jvp(primals,tangents,slice_sizes)
jax.lax.lax._dynamic_slice_shape_rule(operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_translation_rule(c,operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_transpose_rule(t,operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_update_slice_batching_rule(batched_args,batch_dims)
jax.lax.lax._dynamic_update_slice_dtype_rule(operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_jvp(primals,tangents)
jax.lax.lax._dynamic_update_slice_shape_rule(operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_translation_rule(c,operand,update,*start_indices,**kwargs)
jax.lax.lax._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices)
jax.lax.lax._eq_meet(a,b)
jax.lax.lax._eye(dtype,shape,offset)
jax.lax.lax._flip_axes(x,axes)
jax.lax.lax._gather_batching_rule(batched_args,batch_dims,dimension_numbers,slice_sizes)
jax.lax.lax._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._gather_dtype_rule(operand,start_indices,**kwargs)
jax.lax.lax._gather_jvp_rule(g,operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax._gather_shape_rule(operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax._gather_translation_rule(c,operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax._gather_transpose_rule(t,operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax._generic_reduce_window_batch_rule(batched_args,batch_dims,jaxpr,consts,window_dimensions,window_strides,padding)
jax.lax.lax._get_max_identity(dtype)
jax.lax.lax._get_min_identity(dtype)
jax.lax.lax._get_monoid_reducer(monoid_op,x)
jax.lax.lax._get_monoid_window_reducer(monoid_op,x)
jax.lax.lax._identity(x)
jax.lax.lax._infeed_abstract_eval(token,shapes=None)
jax.lax.lax._infeed_translation_rule(c,token,shapes=None)
jax.lax.lax._is_axis_merge(s1,s2)
jax.lax.lax._is_axis_split(s1,s2)
jax.lax.lax._is_singleton_reshape(old,new)
jax.lax.lax._iscomplex(x)->bool
jax.lax.lax._iter(tracer)
jax.lax.lax._masking_defreducer(prim,identity)
jax.lax.lax._minmax_translation_rule(c,x,y,minmax=None,cmp=None)
jax.lax.lax._outfeed_abstract_eval(token,*xs)
jax.lax.lax._outfeed_translation_rule(c,token,*xs)
jax.lax.lax._pad_batch_rule(batched_args,batch_dims,padding_config)
jax.lax.lax._pad_dtype_rule(operand,padding_value,padding_config)
jax.lax.lax._pad_shape_rule(operand,padding_value,padding_config)
jax.lax.lax._pad_transpose(t,operand,padding_value,padding_config)
jax.lax.lax._pow_jvp_lhs(g,ans,x,y)
jax.lax.lax._pow_jvp_rhs(g,ans,x,y)
jax.lax.lax._precision_config(precision)
jax.lax.lax._reduce_and(operand,axes)
jax.lax.lax._reduce_batch_rule(batched_args,batch_dims,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_chooser_jvp_rule(g,ans,operand,axes)
jax.lax.lax._reduce_chooser_shape_rule(operand,axes)
jax.lax.lax._reduce_chooser_translation_rule(prim,identity,c,operand,axes)
jax.lax.lax._reduce_logical_shape_rule(operand,axes)
jax.lax.lax._reduce_logical_translation_rule(prim,identity,c,operand,axes)
jax.lax.lax._reduce_max(operand,axes)
jax.lax.lax._reduce_min(operand,axes)
jax.lax.lax._reduce_number_dtype_rule(name,operand,*args,**kw)
jax.lax.lax._reduce_op_shape_rule(operand,axes)
jax.lax.lax._reduce_or(operand,axes)
jax.lax.lax._reduce_prod(operand,axes)
jax.lax.lax._reduce_prod_jvp_rule(tangent,operand,axes)
jax.lax.lax._reduce_prod_translation_rule(c,operand,axes)
jax.lax.lax._reduce_shape_rule(operand,init_value,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_sum(operand,axes)
jax.lax.lax._reduce_sum_shape_rule(operand,axes)
jax.lax.lax._reduce_sum_translation_rule(c,operand,axes)
jax.lax.lax._reduce_sum_transpose_rule(cotangent,operand,axes)
jax.lax.lax._reduce_translation_rule(c,operand,init_value,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_window_batch_rule(reduce_window,batched_args,bdims,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_chooser_jvp_rule(prim,g,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_chooser_translation_rule(prim,identity,c,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_max(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_min(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_prod(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_shape_rule(operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum_shape_rule(operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum_translation_rule(c,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_sum_transpose_rule(cotangent,operand,window_dimensions,window_strides,padding)
jax.lax.lax._reduce_window_translation_rule(c,operand,init_value,jaxpr,consts,window_dimensions,window_strides,padding)
jax.lax.lax._reducer_masking_rule(prim,identity,padded_vals,logical_shapes,axes)
jax.lax.lax._reduction_computation(c,jaxpr,consts,init_value)
jax.lax.lax._reduction_jaxpr(computation,aval)
jax.lax.lax._reshape_axis_into(src,dst,x)
jax.lax.lax._reshape_axis_out_of(src,size1,x)
jax.lax.lax._reshape_batch_rule(batched_args,batch_dims,new_sizes,dimensions,**unused)
jax.lax.lax._reshape_dtype_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax.lax.lax._reshape_impl(operand,new_sizes,dimensions)
jax.lax.lax._reshape_shape_rule(operand,new_sizes,dimensions,**unused_kwargs)
jax.lax.lax._reshape_translation_rule(c,operand,new_sizes,dimensions)
jax.lax.lax._reshape_transpose_rule(t,operand,new_sizes,dimensions)
jax.lax.lax._rev_batch_rule(batched_args,batch_dims,dimensions)
jax.lax.lax._rev_shape_rule(operand,dimensions)
jax.lax.lax._rng_uniform_abstract_eval(a,b,shape)
jax.lax.lax._rng_uniform_translation_rule(c,a,b,shape)
jax.lax.lax._scatter_add_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers)
jax.lax.lax._scatter_add_transpose_rule(t,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers)
jax.lax.lax._scatter_batching_rule(scatter_op,batched_args,batch_dims,update_jaxpr,update_consts,dimension_numbers)
jax.lax.lax._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._scatter_dtype_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_jvp(primals,tangents,update_jaxpr,update_consts,dimension_numbers)
jax.lax.lax._scatter_shape_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_translation_rule(c,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers)
jax.lax.lax._select_and_gather_add(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_batching_rule(batched_args,batch_dims,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_shape_rule(tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_gather_add_translation(c,tangents,operand,select_prim,window_dimensions,window_strides,padding,max_bits=64)
jax.lax.lax._select_and_gather_add_transpose(t,tangents,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter(operand,select,window_dimensions,window_strides,padding,source,init_value,scatter)
jax.lax.lax._select_and_scatter_add(source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_batch_rule(batched_args,batch_dims,**kwargs)
jax.lax.lax._select_and_scatter_add_jvp(primals,tangents,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_shape_rule(source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_translation(c,source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_transpose(t,source,operand,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_shape_rule(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_translation(c,operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.lax.lax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax.lax.lax._select_dtype_rule(pred,on_true,on_false)
jax.lax.lax._select_shape_rule(pred,on_true,on_false)
jax.lax.lax._select_transpose_rule(t,pred,on_true,on_false)
jax.lax.lax._sign_translation_rule(c,x)
jax.lax.lax._slice_batching_rule(batched_args,batch_dims,start_indices,limit_indices,strides)
jax.lax.lax._slice_shape_rule(operand,start_indices,limit_indices,strides)
jax.lax.lax._slice_translation_rule(c,operand,start_indices,limit_indices,strides)
jax.lax.lax._slice_transpose_rule(t,operand,start_indices,limit_indices,strides)
jax.lax.lax._sort_batch_rule(batched_args,batch_dims,dimension)
jax.lax.lax._sort_jvp_rule(g,operand,dimension)
jax.lax.lax._sort_key_val_abstract_eval(keys,values,dimension)
jax.lax.lax._sort_key_val_batch_rule(batched_args,batch_dims,dimension)
jax.lax.lax._sort_key_val_jvp(primals,tangents,dimension)
jax.lax.lax._sort_key_val_transpose_rule(t,keys,values,dimension)
jax.lax.lax._stop_gradient_batch_rule(batched_args,batch_dims)
jax.lax.lax._stop_gradient_jvp_rule(primals,tangents)
jax.lax.lax._sub_transpose(t,x,y)
jax.lax.lax._tie_in_batch_rule(batched_args,batch_dims)
jax.lax.lax._tie_in_transpose_rule(t)
jax.lax.lax._top_k_abstract_eval(operand,k)
jax.lax.lax._transpose_batch_rule(batched_args,batch_dims,permutation)
jax.lax.lax._transpose_impl(operand,permutation)
jax.lax.lax._transpose_shape_rule(operand,permutation)
jax.lax.lax._tri(dtype,shape,offset)
jax.lax.lax._upcast_fp16_for_computation(f)
jax.lax.lax.abs(x)
jax.lax.lax.acos(x)
jax.lax.lax.acosh(x)
jax.lax.lax.add(x,y)
jax.lax.lax.after_all(*operands)
jax.lax.lax.asin(x)
jax.lax.lax.asinh(x)
jax.lax.lax.atan(x)
jax.lax.lax.atan2(x,y)
jax.lax.lax.atanh(x)
jax.lax.lax.batch_matmul(lhs,rhs,precision=None)
jax.lax.lax.bessel_i0e(x)
jax.lax.lax.bessel_i1e(x)
jax.lax.lax.betainc(a,b,x)
jax.lax.lax.betainc_grad_not_implemented(g,a,b,x)
jax.lax.lax.betainc_gradx(g,a,b,x)
jax.lax.lax.bitcast_convert_type(operand,new_dtype)
jax.lax.lax.bitwise_and(x,y)
jax.lax.lax.bitwise_not(x)
jax.lax.lax.bitwise_or(x,y)
jax.lax.lax.bitwise_xor(x,y)
jax.lax.lax.broadcast(operand,sizes)
jax.lax.lax.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.lax.lax.broadcast_shapes(*shapes)
jax.lax.lax.broadcasted_iota(dtype,shape,dimension)
jax.lax.lax.ceil(x)
jax.lax.lax.clamp(min,x,max)
jax.lax.lax.collapse(operand,start_dimension,stop_dimension)
jax.lax.lax.complex(x,y)
jax.lax.lax.concatenate(operands,dimension)
jax.lax.lax.conj(x)
jax.lax.lax.conv(lhs,rhs,window_strides,padding,precision=None)
jax.lax.lax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.lax.lax.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation=None,rhs_dilation=None,dimension_numbers=None,feature_group_count=1,precision=None)
jax.lax.lax.conv_general_permutations(dimension_numbers)
jax.lax.lax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads)
jax.lax.lax.conv_transpose(lhs,rhs,strides,padding,rhs_dilation=None,dimension_numbers=None,transpose_kernel=False,precision=None)
jax.lax.lax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,precision=None)
jax.lax.lax.convert_element_type(operand,new_dtype)
jax.lax.lax.cos(x)
jax.lax.lax.cosh(x)
jax.lax.lax.create_token(x)
jax.lax.lax.digamma(x)
jax.lax.lax.div(x,y)
jax.lax.lax.dot(lhs,rhs,precision=None)
jax.lax.lax.dot_general(lhs,rhs,dimension_numbers,precision=None)
jax.lax.lax.dynamic_index_in_dim(operand,index,axis=0,keepdims=True)
jax.lax.lax.dynamic_slice(operand,start_indices,slice_sizes)
jax.lax.lax.dynamic_slice_in_dim(operand,start_index,slice_size,axis=0)
jax.lax.lax.dynamic_update_index_in_dim(operand,update,index,axis)
jax.lax.lax.dynamic_update_slice(operand,update,start_indices)
jax.lax.lax.dynamic_update_slice_in_dim(operand,update,start_index,axis)
jax.lax.lax.eq(x,y)
jax.lax.lax.erf(x)
jax.lax.lax.erf_inv(x)
jax.lax.lax.erfc(x)
jax.lax.lax.exp(x)
jax.lax.lax.expm1(x)
jax.lax.lax.floor(x)
jax.lax.lax.full(shape,fill_value,dtype=None)
jax.lax.lax.full_like(x,fill_value,dtype=None,shape=None)
jax.lax.lax.gamma_grad_not_implemented(a,b,x)
jax.lax.lax.gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.lax.lax.ge(x,y)
jax.lax.lax.gt(x,y)
jax.lax.lax.igamma(a,x)
jax.lax.lax.igamma_gradx(g,a,x)
jax.lax.lax.igammac(a,x)
jax.lax.lax.igammac_gradx(g,a,x)
jax.lax.lax.imag(x)
jax.lax.lax.index_in_dim(operand,index,axis=0,keepdims=True)
jax.lax.lax.index_take(src,idxs,axes)
jax.lax.lax.infeed(token,shape=None)
jax.lax.lax.iota(dtype,size)
jax.lax.lax.is_finite(x)
jax.lax.lax.le(x,y)
jax.lax.lax.lgamma(x)
jax.lax.lax.log(x)
jax.lax.lax.log1p(x)
jax.lax.lax.lt(x,y)
jax.lax.lax.max(x,y)
jax.lax.lax.min(x,y)
jax.lax.lax.mul(x,y)
jax.lax.lax.naryop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.lax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.lax.lax.ne(x,y)
jax.lax.lax.neg(x)
jax.lax.lax.nextafter(x1,x2)
jax.lax.lax.outfeed(token,xs)
jax.lax.lax.pad(operand,padding_value,padding_config)
jax.lax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.lax.lax.pow(x,y)
jax.lax.lax.ranges_like(*xs)
jax.lax.lax.real(x)
jax.lax.lax.reciprocal(x)
jax.lax.lax.reduce(operand,init_value,computation,dimensions)
jax.lax.lax.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.lax.lax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding)
jax.lax.lax.rem(x,y)
jax.lax.lax.remaining(original,*removed_lists)
jax.lax.lax.reshape(operand,new_sizes,dimensions=None)
jax.lax.lax.rev(operand,dimensions)
jax.lax.lax.rng_uniform(a,b,shape)
jax.lax.lax.round(x)
jax.lax.lax.rsqrt(x)
jax.lax.lax.scatter(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_add(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_max(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.scatter_min(operand,scatter_indices,updates,dimension_numbers)
jax.lax.lax.select(pred,on_true,on_false)
jax.lax.lax.shift_left(x,y)
jax.lax.lax.shift_right_arithmetic(x,y)
jax.lax.lax.shift_right_logical(x,y)
jax.lax.lax.sign(x)
jax.lax.lax.sin(x)
jax.lax.lax.sinh(x)
jax.lax.lax.slice(operand:Any,start_indices,limit_indices,strides=None)
jax.lax.lax.slice_in_dim(operand,start_index,limit_index,stride=1,axis=0)
jax.lax.lax.sort(operand,dimension=-1)
jax.lax.lax.sort_key_val(keys,values,dimension=-1)
jax.lax.lax.sqrt(x)
jax.lax.lax.square(x)
jax.lax.lax.standard_abstract_eval(prim,shape_rule,dtype_rule,*args,**kwargs)
jax.lax.lax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.lax.lax.standard_translate(name,c,*args,**kwargs)
jax.lax.lax.stop_gradient(x)
jax.lax.lax.sub(x,y)
jax.lax.lax.tan(x)
jax.lax.lax.tanh(x)
jax.lax.lax.tie_in(x,y)
jax.lax.lax.top_k(operand,k)
jax.lax.lax.transpose(operand,permutation)
jax.lax.lax.unop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.lax.lax.zeros_like_array(x)
jax.le(x,y)
jax.lgamma(x)
jax.log(x)
jax.log1p(x)
jax.lt(x,y)
jax.max(x,y)
jax.min(x,y)
jax.mul(x,y)
jax.naryop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.ne(x,y)
jax.neg(x)
jax.nextafter(x1,x2)
jax.outfeed(token,xs)
jax.pad(operand,padding_value,padding_config)
jax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.pow(x,y)
jax.ranges_like(*xs)
jax.real(x)
jax.reciprocal(x)
jax.reduce(operand,init_value,computation,dimensions)
jax.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding)
jax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding)
jax.rem(x,y)
jax.remaining(original,*removed_lists)
jax.reshape(operand,new_sizes,dimensions=None)
jax.rev(operand,dimensions)
jax.rng_uniform(a,b,shape)
jax.round(x)
jax.rsqrt(x)
jax.scatter(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_add(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_max(operand,scatter_indices,updates,dimension_numbers)
jax.scatter_min(operand,scatter_indices,updates,dimension_numbers)
jax.select(pred,on_true,on_false)
jax.shift_left(x,y)
jax.shift_right_arithmetic(x,y)
jax.shift_right_logical(x,y)
jax.sign(x)
jax.sin(x)
jax.sinh(x)
jax.slice(operand:Any,start_indices,limit_indices,strides=None)
jax.slice_in_dim(operand,start_index,limit_index,stride=1,axis=0)
jax.sort(operand,dimension=-1)
jax.sort_key_val(keys,values,dimension=-1)
jax.sqrt(x)
jax.square(x)
jax.standard_abstract_eval(prim,shape_rule,dtype_rule,*args,**kwargs)
jax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.standard_translate(name,c,*args,**kwargs)
jax.stop_gradient(x)
jax.sub(x,y)
jax.tan(x)
jax.tanh(x)
jax.tie_in(x,y)
jax.top_k(operand,k)
jax.transpose(operand,permutation)
jax.unop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.zeros_like_array(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax/lax_parallel.py----------------------------------------
A:jax.lax.lax_parallel.(leaves, treedef)->jax.tree_util.tree_flatten(x)
A:jax.lax.lax_parallel.(x, n)->psum((x, 1), axis_name=axis_name)
A:jax.lax.lax_parallel.prim->jax.core.Primitive(name)
A:jax.lax.lax_parallel.dtype->c.GetShape(val).numpy_dtype()
A:jax.lax.lax_parallel.scalar->ShapedArray((), c.GetShape(dtype_args[0]).numpy_dtype())
A:jax.lax.lax_parallel.computation->jax.interpreters.xla.primitive_subcomputation(lax.add_p, scalar, scalar)
A:jax.lax.lax_parallel.args_by_type->collections.defaultdict(lambda : ([], []))
A:jax.lax.lax_parallel.is_complex->jax.dtypes.issubdtype(dtype, onp.complexfloating)
A:jax.lax.lax_parallel.n->len(dtype_args)
A:jax.lax.lax_parallel.all_reduce->c.AllReduce(c.Tuple(*dtype_args), computation, replica_groups=replica_groups)
A:jax.lax.lax_parallel.psum->partial(_allreduce_translation_rule, lax.add_p, c, replica_groups=replica_groups)
A:jax.lax.lax_parallel.psum_p->standard_pmap_primitive('psum', multiple_results=True)
A:jax.lax.lax_parallel.pxla.split_axis_rules[psum_p]->partial(_allreduce_split_axis_rule, psum_p, lax._reduce_sum)
A:jax.lax.lax_parallel.pmax_p->standard_pmap_primitive('pmax')
A:jax.lax.lax_parallel.xla.parallel_translations[pmax_p]->partial(_allreduce_translation_rule, lax.max_p)
A:jax.lax.lax_parallel.pxla.split_axis_rules[pmax_p]->partial(_allreduce_split_axis_rule, pmax_p, lax._reduce_max)
A:jax.lax.lax_parallel.pmin_p->standard_pmap_primitive('pmin')
A:jax.lax.lax_parallel.xla.parallel_translations[pmin_p]->partial(_allreduce_translation_rule, lax.min_p)
A:jax.lax.lax_parallel.pxla.split_axis_rules[pmin_p]->partial(_allreduce_split_axis_rule, pmin_p, lax._reduce_min)
A:jax.lax.lax_parallel.group_size->len(replica_groups[0])
A:jax.lax.lax_parallel.(srcs, dsts)->unzip2(perm)
A:jax.lax.lax_parallel.grp->list(sorted(grp))
A:jax.lax.lax_parallel.inverse_perm->list(zip(dsts, srcs))
A:jax.lax.lax_parallel.ppermute_p->standard_pmap_primitive('ppermute')
A:jax.lax.lax_parallel.stacked->standard_pmap_primitive('all_to_all').bind(x, split_axis=split_axis + 1, concat_axis=0, axis_name=axis_name)
A:jax.lax.lax_parallel.out->jax.lax.lax.gather(operand, start_indices, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.lax.lax_parallel.all_to_all_p->standard_pmap_primitive('all_to_all')
A:jax.lax.lax_parallel.shape->list(x.shape)
A:jax.lax.lax_parallel.outs->jax.tree_util.tree_map(partial(_expand, dim, size, axis_name), x)
A:jax.lax.lax_parallel.x->all_to_all(x, name, x_tosplit, xdim)
A:jax.lax.lax_parallel.y->_allgather(y, ydim, size, name)
A:jax.lax.lax_parallel.parallel.papply_primitive_rules[prim]->partial(_identity_papply, prim, argnum)
A:jax.lax.lax_parallel.result->jax.core.Primitive(name).bind(operand, axes=tuple(other_axes), **kwargs)
A:jax.lax.lax_parallel.xbatch->adjust_dims(xbatch, xdim)
A:jax.lax.lax_parallel.xcontract->adjust_dims(xcontract, xdim)
A:jax.lax.lax_parallel.ybatch->adjust_dims(ybatch, ydim)
A:jax.lax.lax_parallel.ycontract->adjust_dims(ycontract, ydim)
A:jax.lax.lax_parallel.z->jax.lax.lax.dot_general(x, y, sub_dims(xdim, None, xc, yc, xb, yb), precision)
A:jax.lax.lax_parallel.(ok, out)->cases(x, y, xdim, ydim, lhs_contract, rhs_contract, lhs_batch, rhs_batch)
A:jax.lax.lax_parallel.old_sizes->tuple(onp.insert(operand.shape, axis, size))
A:jax.lax.lax_parallel.left->numpy.prod(old_sizes[:old_axis])
A:jax.lax.lax_parallel.new_axis->find_new_axis(axis, old_sizes, new_sizes)
A:jax.lax.lax_parallel.lhs->jax.lax.lax.reshape(lhs, tuple(onp.insert(lhs.shape, lhs_dim, 1)))
A:jax.lax.lax_parallel.sub_bdims->tuple(onp.delete(broadcast_dimensions, dim))
A:jax.lax.lax_parallel.sub_shape->tuple(onp.delete(shape, out_dim))
A:jax.lax.lax_parallel.padding_config->list(padding_config)
A:jax.lax.lax_parallel.padded->jax.lax.lax.pad(operand, padding_value, padding_config[:operand_dim] + padding_config[operand_dim + 1:])
A:jax.lax.lax_parallel.start_indices->list(start_indices)
A:jax.lax.lax_parallel.limit_indices->list(limit_indices)
A:jax.lax.lax_parallel.offset_dims->tuple((i - 1 if i > start_indices_dim else i for i in dimension_numbers.offset_dims))
A:jax.lax.lax_parallel.dnums->jax.lax.lax.GatherDimensionNumbers(offset_dims=offset_dims, collapsed_slice_dims=dimension_numbers.collapsed_slice_dims, start_index_map=dimension_numbers.start_index_map)
jax.lax._add_jaxvals_papply_rule(name,size,vals,dims)
jax.lax._all_to_all_split_axis_rule(vals,which_mapped,split_axis,concat_axis,axis_name)
jax.lax._all_to_all_translation_rule(c,x,split_axis,concat_axis,replica_groups,platform=None)
jax.lax._allgather(x,dim,size,axis_name)
jax.lax._allreduce_split_axis_rule(prim,reducer,vals,which_mapped,axis_name)
jax.lax._allreduce_translation_rule(prim,c,val,replica_groups,platform=None)
jax.lax._broadcast_in_dim_papply_rule(name,size,vals,dims,shape,broadcast_dimensions)
jax.lax._broadcasting_papply(prim,name,size,vals,axes,**params)
jax.lax._conv_general_dilated_papply_rule(name,size,vals,dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax._convert_element_type_papply_rule(name,size,vals,dims,new_dtype,**params)
jax.lax._cpu_psum_translation_rule(c,*args,replica_groups)
jax.lax._defbroadcasting(prim)
jax.lax._defidentity(prim,argnum=0)
jax.lax._defreducer(prim,collective_prim)
jax.lax._defvectorized(prim)
jax.lax._dot_general_papply_rule(name,size,vals,dims,dimension_numbers,precision)
jax.lax._drop(x,dim,axis_name)
jax.lax._expand(dim,size,axis_name,x)
jax.lax._gather_papply_rule(name,size,vals,dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax._identity_papply(prim,argnum,name,size,vals,axes,**params)
jax.lax._moveaxis(src,dst,x)
jax.lax._pad_papply_rule(name,size,vals,dims,padding_config)
jax.lax._ppermute_translation_rule(c,x,replica_groups,perm,platform=None)
jax.lax._ppermute_transpose_rule(t,perm,axis_name)
jax.lax._psum_translation_rule(c,*args,replica_groups=None,platform=None)
jax.lax._reducer_papply(prim,collective,name,size,vals,papply_axes,axes,**kwargs)
jax.lax._reshape_papply_rule(name,size,vals,axes,new_sizes,dimensions)
jax.lax._select_papply_rule(name,size,vals,dims)
jax.lax._slice_papply_rule(name,size,vals,dims,start_indices,limit_indices,strides,**kwargs)
jax.lax._transpose_papply_rule(name,size,vals,dims,permutation)
jax.lax._vectorized_papply(prim,name,size,vals,axes,**params)
jax.lax.all_gather(x,axis_name)
jax.lax.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel._add_jaxvals_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._all_to_all_split_axis_rule(vals,which_mapped,split_axis,concat_axis,axis_name)
jax.lax.lax_parallel._all_to_all_translation_rule(c,x,split_axis,concat_axis,replica_groups,platform=None)
jax.lax.lax_parallel._allgather(x,dim,size,axis_name)
jax.lax.lax_parallel._allreduce_split_axis_rule(prim,reducer,vals,which_mapped,axis_name)
jax.lax.lax_parallel._allreduce_translation_rule(prim,c,val,replica_groups,platform=None)
jax.lax.lax_parallel._broadcast_in_dim_papply_rule(name,size,vals,dims,shape,broadcast_dimensions)
jax.lax.lax_parallel._broadcasting_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel._conv_general_dilated_papply_rule(name,size,vals,dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax_parallel._convert_element_type_papply_rule(name,size,vals,dims,new_dtype,**params)
jax.lax.lax_parallel._cpu_psum_translation_rule(c,*args,replica_groups)
jax.lax.lax_parallel._defbroadcasting(prim)
jax.lax.lax_parallel._defidentity(prim,argnum=0)
jax.lax.lax_parallel._defreducer(prim,collective_prim)
jax.lax.lax_parallel._defvectorized(prim)
jax.lax.lax_parallel._dot_general_papply_rule(name,size,vals,dims,dimension_numbers,precision)
jax.lax.lax_parallel._drop(x,dim,axis_name)
jax.lax.lax_parallel._expand(dim,size,axis_name,x)
jax.lax.lax_parallel._gather_papply_rule(name,size,vals,dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax_parallel._identity_papply(prim,argnum,name,size,vals,axes,**params)
jax.lax.lax_parallel._moveaxis(src,dst,x)
jax.lax.lax_parallel._pad_papply_rule(name,size,vals,dims,padding_config)
jax.lax.lax_parallel._ppermute_translation_rule(c,x,replica_groups,perm,platform=None)
jax.lax.lax_parallel._ppermute_transpose_rule(t,perm,axis_name)
jax.lax.lax_parallel._psum_translation_rule(c,*args,replica_groups=None,platform=None)
jax.lax.lax_parallel._reducer_papply(prim,collective,name,size,vals,papply_axes,axes,**kwargs)
jax.lax.lax_parallel._reshape_papply_rule(name,size,vals,axes,new_sizes,dimensions)
jax.lax.lax_parallel._select_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._slice_papply_rule(name,size,vals,dims,start_indices,limit_indices,strides,**kwargs)
jax.lax.lax_parallel._transpose_papply_rule(name,size,vals,dims,permutation)
jax.lax.lax_parallel._vectorized_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel.all_gather(x,axis_name)
jax.lax.lax_parallel.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel.pmax(x,axis_name)
jax.lax.lax_parallel.pmean(x,axis_name)
jax.lax.lax_parallel.pmin(x,axis_name)
jax.lax.lax_parallel.ppermute(x,axis_name,perm)
jax.lax.lax_parallel.pshuffle(x,axis_name,perm)
jax.lax.lax_parallel.psum(x,axis_name)
jax.lax.lax_parallel.pswapaxes(x,axis_name,axis)
jax.lax.lax_parallel.standard_pmap_primitive(name,multiple_results=False)
jax.lax.pmax(x,axis_name)
jax.lax.pmean(x,axis_name)
jax.lax.pmin(x,axis_name)
jax.lax.ppermute(x,axis_name,perm)
jax.lax.pshuffle(x,axis_name,perm)
jax.lax.psum(x,axis_name)
jax.lax.pswapaxes(x,axis_name,axis)
jax.lax.standard_pmap_primitive(name,multiple_results=False)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax/lax_control_flow.py----------------------------------------
A:jax.lax.lax_control_flow.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.lax.lax_control_flow.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(wrapped_fun, in_pvals, instantiate=True, stage_out_calls=True)
A:jax.lax.lax_control_flow.out_avals->_map(raise_to_shaped, true_jaxpr_2.out_avals)
A:jax.lax.lax_control_flow.const_avals->tuple((raise_to_shaped(core.get_aval(c)) for c in consts))
A:jax.lax.lax_control_flow.typed_jaxpr->jax.core.TypedJaxpr(pe.convert_constvars_jaxpr(jaxpr), (), const_avals + in_avals, out_avals)
A:jax.lax.lax_control_flow.aval->ShapedArray((), dtypes.int_)
A:jax.lax.lax_control_flow.lower_dtype->jax.dtypes.canonicalize_dtype(lax.dtype(lower))
A:jax.lax.lax_control_flow.upper_dtype->jax.dtypes.canonicalize_dtype(lax.dtype(upper))
A:jax.lax.lax_control_flow.lower_->int(lower)
A:jax.lax.lax_control_flow.upper_->int(upper)
A:jax.lax.lax_control_flow.((_, _, result), _)->scan(_fori_scan_body_fun(body_fun), (lower, upper, init_val), None, length=upper_ - lower_)
A:jax.lax.lax_control_flow.(_, _, result)->while_loop(_fori_cond_fun, _fori_body_fun(body_fun), (lower, upper, init_val))
A:jax.lax.lax_control_flow.(init_vals, in_tree)->tree_flatten((init_val,))
A:jax.lax.lax_control_flow.init_avals->tuple(_map(_abstractify, init_vals))
A:jax.lax.lax_control_flow.(cond_jaxpr, cond_consts, cond_tree)->_initial_style_jaxpr(cond_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.(body_jaxpr, body_consts, body_tree)->_initial_style_jaxpr(body_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.in_tree_children->in_tree.children()
A:jax.lax.lax_control_flow.outs->jax.core.Primitive('custom_linear_solve').bind(*new_params + new_b, const_lengths=const_lengths, jaxprs=batched_jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(cond_consts, body_consts, init_vals)->split_list(args, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.batched->bool(cond_jaxpr.out_avals[0].shape)
A:jax.lax.lax_control_flow.init_carry->jax.lib.xla_bridge.make_computation_builder(name + '_comp').Tuple(*cond_consts + body_consts + init_vals)
A:jax.lax.lax_control_flow.cond_c->jax.lib.xla_bridge.make_computation_builder('cond_computation')
A:jax.lax.lax_control_flow.cond_carry->jax.lib.xla_bridge.make_computation_builder('cond_computation').ParameterWithShape(c.GetShape(init_carry))
A:jax.lax.lax_control_flow.(x, _, z)->split_list(cond_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(pred,)->jax.interpreters.xla.jaxpr_subcomp(cond_c, cond_jaxpr.jaxpr, backend, axis_env, _map(cond_c.Constant, cond_jaxpr.literals), extend_name_stack(name_stack, 'cond'), *x + z)
A:jax.lax.lax_control_flow.scalar->ShapedArray((), onp.bool_)
A:jax.lax.lax_control_flow.or_->jax.interpreters.xla.primitive_subcomputation(lax.or_p, scalar, scalar)
A:jax.lax.lax_control_flow.pred->jax.lib.xla_bridge.make_computation_builder('cond_computation').Reduce(pred, cond_c.Constant(onp.array(False)), or_, list(range(cond_jaxpr.out_avals[0].ndim)))
A:jax.lax.lax_control_flow.body_c->jax.lib.xla_bridge.make_computation_builder('body_computation')
A:jax.lax.lax_control_flow.body_carry->jax.lib.xla_bridge.make_computation_builder('body_computation').ParameterWithShape(c.GetShape(init_carry))
A:jax.lax.lax_control_flow.(x, y, z)->split_list(body_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.new_z->_map(partial(_pred_bcast_select, body_c, body_pred), new_z, z)
A:jax.lax.lax_control_flow.(body_pred,)->jax.interpreters.xla.jaxpr_subcomp(body_c, cond_jaxpr.jaxpr, backend, axis_env, _map(body_c.Constant, cond_jaxpr.literals), extend_name_stack(name_stack, 'body_pred'), *x + z)
A:jax.lax.lax_control_flow.new_carry->jax.lib.xla_bridge.make_computation_builder('body_computation').Tuple(*itertools.chain(x, y, new_z))
A:jax.lax.lax_control_flow.ans->jax.lib.xla_bridge.make_computation_builder(name + '_comp').While(cond_c.Build(pred), body_c.Build(new_carry), init_carry)
A:jax.lax.lax_control_flow.(_, _, z)->split_list(ans_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.pred_shape->jax.lib.xla_bridge.make_computation_builder(name + '_comp').GetShape(pred).dimensions()
A:jax.lax.lax_control_flow.x_shape->jax.lib.xla_bridge.make_computation_builder(name + '_comp').GetShape(x).dimensions()
A:jax.lax.lax_control_flow.y_shape->jax.lib.xla_bridge.make_computation_builder(name + '_comp').GetShape(y).dimensions()
A:jax.lax.lax_control_flow.bcast_pred->jax.lax.lax.broadcast_in_dim(pred, onp.shape(x), list(range(onp.ndim(pred))))
A:jax.lax.lax_control_flow.(cconst_bat, bconst_bat, init_bat)->split_list(orig_batched, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jaxpr_batched, carry_bat_out)->jax.interpreters.batching.batch_jaxpr(body_jaxpr, size, batched, instantiate=carry_bat)
A:jax.lax.lax_control_flow.(cond_jaxpr_batched, (pred_bat,))->jax.interpreters.batching.batch_jaxpr(cond_jaxpr, size, cconst_bat + carry_bat, instantiate=False)
A:jax.lax.lax_control_flow.carry_bat_out->_map(partial(operator.or_, pred_bat), carry_bat_out)
A:jax.lax.lax_control_flow.carry_bat->_map(operator.or_, carry_bat, carry_bat_out)
A:jax.lax.lax_control_flow.(consts, init)->split_list(args, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.(const_dims, init_dims)->split_list(dims, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.(cconst_nz, bconst_nz, init_nz)->split_list(nonzeros, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(body_jaxpr, body_nonzeros, instantiate=carry_nz)
A:jax.lax.lax_control_flow.carry_nz->_map(operator.or_, carry_nz, carry_nz_out)
A:jax.lax.lax_control_flow.(cconst, bconst, init)->split_list(primals, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(_, bconst_dot, init_dot)->split_list(tangents, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.bconst_dot->_prune_zeros(bconst_dot)
A:jax.lax.lax_control_flow.init_dot->_prune_zeros(init_dot)
A:jax.lax.lax_control_flow.body_jvp_rearranged->jax.interpreters.ad.rearrange_binders(body_jvp, [body_nconsts, num_carry], [len(bconst_dot), len(init_dot)], [num_carry], [len(init_dot)])
A:jax.lax.lax_control_flow.newvar->jax.core.gensym('')
A:jax.lax.lax_control_flow.cond_jaxpr_augmented->jax.core.TypedJaxpr(cond_jaxpr_augmented, cond_jaxpr.literals, in_avals_aug, cond_jaxpr.out_avals)
A:jax.lax.lax_control_flow.out->jax.core.jaxpr_as_fun(jaxpr)(*lax.stop_gradient(consts + tuple(args_flat)))
A:jax.lax.lax_control_flow.(out_carry, out_carry_dot)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.out_tangents_iter->iter(out_tangents)
A:jax.lax.lax_control_flow.while_p->jax.lax.lax.Primitive('while')
A:jax.lax.lax_control_flow.pred_dtype->jax.dtypes.result_type(pred)
A:jax.lax.lax_control_flow.(true_ops, true_tree)->tree_flatten((true_operand,))
A:jax.lax.lax_control_flow.true_avals->tuple(_map(_abstractify, true_ops))
A:jax.lax.lax_control_flow.(true_jaxpr, true_consts, true_out_tree)->_initial_style_jaxpr(true_fun, true_tree, true_avals)
A:jax.lax.lax_control_flow.(false_ops, false_tree)->tree_flatten((false_operand,))
A:jax.lax.lax_control_flow.false_avals->tuple(_map(_abstractify, false_ops))
A:jax.lax.lax_control_flow.(false_jaxpr, false_consts, false_out_tree)->_initial_style_jaxpr(false_fun, false_tree, false_avals)
A:jax.lax.lax_control_flow.(true_ops, false_ops)->split_list(args, [len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.c->jax.lib.xla_bridge.make_computation_builder(name + '_comp')
A:jax.lax.lax_control_flow.op->jax.lib.xla_bridge.make_computation_builder(name + '_comp').ParameterWithShape(op_shape)
A:jax.lax.lax_control_flow.true_op->jax.lib.xla_bridge.make_computation_builder(name + '_comp').Tuple(*true_ops)
A:jax.lax.lax_control_flow.true_c->make_computation('true', true_jaxpr, c.GetShape(true_op))
A:jax.lax.lax_control_flow.false_op->jax.lib.xla_bridge.make_computation_builder(name + '_comp').Tuple(*false_ops)
A:jax.lax.lax_control_flow.false_c->make_computation('false', false_jaxpr, c.GetShape(false_op))
A:jax.lax.lax_control_flow.((pred,), true_ops, false_ops)->split_list(args, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.((pred_bat,), t_bat, f_bat)->split_list(orig_bat, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.(_, true_out_bat)->jax.interpreters.batching.batch_jaxpr(true_jaxpr, size, t_bat, False)
A:jax.lax.lax_control_flow.(_, false_out_bat)->jax.interpreters.batching.batch_jaxpr(false_jaxpr, size, f_bat, False)
A:jax.lax.lax_control_flow.(true_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr(true_jaxpr, size, t_bat, out_bat)
A:jax.lax.lax_control_flow.(false_jaxpr_batched, _)->jax.interpreters.batching.batch_jaxpr(false_jaxpr, size, f_bat, out_bat)
A:jax.lax.lax_control_flow.true_out->jax.core.jaxpr_as_fun(true_jaxpr_batched)(*true_ops)
A:jax.lax.lax_control_flow.false_out->jax.core.jaxpr_as_fun(false_jaxpr_batched)(*false_ops)
A:jax.lax.lax_control_flow.((pred_nz,), t_nz, f_nz)->split_list(nonzeros, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.(_, true_out_nz)->jax.interpreters.ad.jvp_jaxpr(true_jaxpr, t_nz, instantiate=False)
A:jax.lax.lax_control_flow.(_, false_out_nz)->jax.interpreters.ad.jvp_jaxpr(false_jaxpr, f_nz, instantiate=False)
A:jax.lax.lax_control_flow.(true_jvp, _)->jax.interpreters.ad.jvp_jaxpr(true_jaxpr, t_nz, instantiate=out_nz)
A:jax.lax.lax_control_flow.(false_jvp, _)->jax.interpreters.ad.jvp_jaxpr(false_jaxpr, f_nz, instantiate=out_nz)
A:jax.lax.lax_control_flow.((pred,), tops, fops)->split_list(args, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.(_, tops_dot, fops_dot)->split_list(tangents, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.tops_dot->_prune_zeros(tops_dot)
A:jax.lax.lax_control_flow.fops_dot->_prune_zeros(fops_dot)
A:jax.lax.lax_control_flow.(tops_lin, fops_lin)->split_list(linear, [len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.(out_primals, out_tangents)->split_list(out, [len(out_nz)])
A:jax.lax.lax_control_flow.((pred_uk,), t_uk, f_uk)->split_list(unknowns, [1, len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.params->dict(true_jaxpr=true_jaxpr_2, false_jaxpr=false_jaxpr_2, linear=linear_2)
A:jax.lax.lax_control_flow.(_, _, t_out_uks)->jax.interpreters.partial_eval.partial_eval_jaxpr(true_jaxpr, t_uk, instantiate=False)
A:jax.lax.lax_control_flow.(_, _, f_out_uks)->jax.interpreters.partial_eval.partial_eval_jaxpr(false_jaxpr, f_uk, instantiate=False)
A:jax.lax.lax_control_flow.(true_jaxpr_1, true_jaxpr_2, _)->jax.interpreters.partial_eval.partial_eval_jaxpr(true_jaxpr, t_uk, instantiate=out_uks)
A:jax.lax.lax_control_flow.(false_jaxpr_1, false_jaxpr_2, _)->jax.interpreters.partial_eval.partial_eval_jaxpr(false_jaxpr, f_uk, instantiate=out_uks)
A:jax.lax.lax_control_flow.true_jaxpr_2->jax.interpreters.partial_eval.move_binders_to_front(true_jaxpr_2, move)
A:jax.lax.lax_control_flow.false_jaxpr_2->jax.interpreters.partial_eval.move_binders_to_front(false_jaxpr_2, move)
A:jax.lax.lax_control_flow.t_res_avals->_map(raise_to_shaped, true_jaxpr_2.in_avals[:num_t_res])
A:jax.lax.lax_control_flow.f_res_avals->_map(raise_to_shaped, false_jaxpr_2.in_avals[:num_f_res])
A:jax.lax.lax_control_flow.num_outs->len(true_jaxpr_2.out_avals)
A:jax.lax.lax_control_flow.true_jaxpr_1->_join_cond_outputs(true_jaxpr_1, num_outs, f_res_avals, zeros_on_left=False)
A:jax.lax.lax_control_flow.false_jaxpr_1->_join_cond_outputs(false_jaxpr_1, num_outs, t_res_avals, zeros_on_left=True)
A:jax.lax.lax_control_flow.(_, in_consts)->unzip2([t.pval for t in tracers])
A:jax.lax.lax_control_flow.out_consts_res->jax.lax.lax.Primitive('cond').bind(*in_consts, true_jaxpr=true_jaxpr_1, false_jaxpr=false_jaxpr_1, linear=linear)
A:jax.lax.lax_control_flow.(out_consts, res)->split_list(out_consts_res, [len(out_consts_res) - num_res])
A:jax.lax.lax_control_flow.pred_tracer->trace.instantiate_const(tracers[0])
A:jax.lax.lax_control_flow.(true_ops_tracers, false_ops_tracers)->split_list(ops_tracers, [len(true_jaxpr.in_avals)])
A:jax.lax.lax_control_flow.res_tracers->_map(trace.new_instantiated_const, res)
A:jax.lax.lax_control_flow.(true_res_tracers, false_res_tracers)->split_list(res_tracers, [num_t_res])
A:jax.lax.lax_control_flow.eqn->jax.interpreters.partial_eval.new_eqn_recipe(int_res_tracers + new_tracers + ext_res_tracers, out_tracers, scan_p, dict(forward=forward, length=length, jaxpr=jaxpr_2_opt, num_consts=num_consts_2, num_carry=num_carry, linear=tuple(linear_2)))
A:jax.lax.lax_control_flow.prefix_and_rest->jax.core.jaxpr_as_fun(jaxpr)(*args)
A:jax.lax.lax_control_flow.(prefix, rest)->split_list(prefix_and_rest, [num_prefix])
A:jax.lax.lax_control_flow.(res_avals, primal_avals)->split_list(jaxpr.in_avals, [num_res])
A:jax.lax.lax_control_flow.primal_avals->_map(raise_to_shaped, primal_avals)
A:jax.lax.lax_control_flow.(res, cts_out)->split_list(args, [num_res])
A:jax.lax.lax_control_flow.cts_in->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, jaxpr.literals, primals, cts_out)
A:jax.lax.lax_control_flow.(_, cts_in)->split_list(cts_in, [num_res])
A:jax.lax.lax_control_flow.in_avals->_map(raise_to_shaped, true_jaxpr.in_avals + false_jaxpr.in_avals)
A:jax.lax.lax_control_flow.t_jaxpr_trans->_transpose_cond_jaxpr(true_jaxpr, num_t_res)
A:jax.lax.lax_control_flow.f_jaxpr_trans->_transpose_cond_jaxpr(false_jaxpr, num_f_res)
A:jax.lax.lax_control_flow.lin_in_avals->_map(raise_to_shaped, [a for (a, l) in zip(in_avals, linear) if l])
A:jax.lax.lax_control_flow.t_jaxpr_trans_->_join_cond_outputs(t_jaxpr_trans, 0, f_jaxpr_trans.out_avals, zeros_on_left=False)
A:jax.lax.lax_control_flow.f_jaxpr_trans_->_join_cond_outputs(f_jaxpr_trans, 0, t_jaxpr_trans.out_avals, zeros_on_left=True)
A:jax.lax.lax_control_flow.(t_res, _)->split_list(tops, [num_t_res])
A:jax.lax.lax_control_flow.(f_res, _)->split_list(fops, [num_f_res])
A:jax.lax.lax_control_flow.cts->_map(ad.instantiate_zeros_aval, true_jaxpr.out_avals, cts)
A:jax.lax.lax_control_flow.out_iter->iter(out)
A:jax.lax.lax_control_flow.cond_p->jax.lax.lax.Primitive('cond')
A:jax.lax.lax_control_flow.(init_flat, init_tree)->tree_flatten(init)
A:jax.lax.lax_control_flow.(xs_flat, _)->tree_flatten(xs)
A:jax.lax.lax_control_flow.(in_flat, in_tree)->tree_flatten((init, xs))
A:jax.lax.lax_control_flow.length->int(length)
A:jax.lax.lax_control_flow.unique_lengths->set(lengths)
A:jax.lax.lax_control_flow.carry_avals->tuple(_map(_abstractify, init_flat))
A:jax.lax.lax_control_flow.x_avals->tuple(_map(ShapedArray, x_shapes, x_dtypes))
A:jax.lax.lax_control_flow.(jaxpr, consts, out_tree)->_initial_style_jaxpr(g, in_args_tree, args_avals)
A:jax.lax.lax_control_flow.out_tree_children->out_tree.children()
A:jax.lax.lax_control_flow.(consts, init, xs)->split_list(args, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(_, _, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(_, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.([i], carry, ys)->split_list(vals, [1, num_carry])
A:jax.lax.lax_control_flow.x->jax.core.Primitive('custom_linear_solve').bind(*primals, **kwargs)
A:jax.lax.lax_control_flow.out_flat->jax.core.Primitive('custom_linear_solve').bind(*_flatten(all_consts) + b_flat, const_lengths=const_lengths, jaxprs=jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(carry_out, y_updates)->split_list(out_flat, [num_carry])
A:jax.lax.lax_control_flow.ys_out->_map(partial(_update_array, i_), y_avals, ys, y_updates)
A:jax.lax.lax_control_flow.ys_init->_map(partial(_empty_array, length), y_avals)
A:jax.lax.lax_control_flow.(_, *outs)->while_loop(cond_fun, body_fun, init_val)
A:jax.lax.lax_control_flow.(carry_avals, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.(const_nz, init_nz, xs_nz)->split_list(nonzeros, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(jaxpr, nonzeros, instantiate=carry_nz + [False] * num_ys)
A:jax.lax.lax_control_flow.all_tangents->split_list(tangents, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts_dot, init_dot, xs_dot)->_map(_prune_zeros, all_tangents)
A:jax.lax.lax_control_flow.jaxpr_jvp_rearranged->jax.interpreters.ad.rearrange_binders(jaxpr_jvp, [num_consts, num_carry, num_xs], [len(consts_dot), len(init_dot), len(xs_dot)], [num_carry, num_ys], [len(init_dot), sum(nonzeros_out) - len(init_dot)])
A:jax.lax.lax_control_flow.(consts_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.jaxpr_jvp_linear->tuple(consts_linear + [True] * len(consts_dot) + init_linear + [True] * len(init_dot) + xs_linear + [True] * len(xs_dot))
A:jax.lax.lax_control_flow.(carry, carry_dot, ys, ys_dot)->split_list(out_flat, [num_carry, len(init_dot), num_ys])
A:jax.lax.lax_control_flow.tangents_out_iter->iter(carry_dot + ys_dot)
A:jax.lax.lax_control_flow.(const_uk, init_uk, xs_uk)->split_list(unknowns, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_1, jaxpr_2, out_uk)->jax.interpreters.partial_eval.partial_eval_jaxpr(jaxpr, unknowns, instantiate=carry_uk + [False] * num_ys)
A:jax.lax.lax_control_flow.carry_uk->_map(operator.or_, carry_uk, carry_uk_out)
A:jax.lax.lax_control_flow.(untyped_jaxpr_1, out_pvals_1, consts_1)->jax.interpreters.partial_eval.trace_to_jaxpr(lu.wrap_init(core.jaxpr_as_fun(jaxpr_1)), in_pvals_1, instantiate=[True] * (num_carry + num_ys) + [False] * num_res)
A:jax.lax.lax_control_flow.jaxpr_1_opt->jax.interpreters.partial_eval.TypedJaxpr(pe.convert_constvars_jaxpr(untyped_jaxpr_1), (), const_avals_1 + in_avals_1, out_avals_1)
A:jax.lax.lax_control_flow.(_, _, res_pvals)->split_list(out_pvals_1, [num_carry, num_ys])
A:jax.lax.lax_control_flow.jaxpr_2_opt->jax.interpreters.partial_eval.move_binders_to_front(jaxpr_2, move)
A:jax.lax.lax_control_flow.(out_carry, ys, res_and_units)->split_list(out_flat, [num_carry, num_ys])
A:jax.lax.lax_control_flow.ys_avals->_map(partial(_promote_aval_rank, length), y_avals)
A:jax.lax.lax_control_flow.int_res_tracers->_map(trace.new_instantiated_const, intensive_residuals)
A:jax.lax.lax_control_flow.ext_res_tracers->_map(trace.new_instantiated_const, extensive_residuals)
A:jax.lax.lax_control_flow.(consts_lin, init_lin, xs_lin)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts, _, xs)->split_list(args, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(ires, _)->split_list(consts, [num_ires])
A:jax.lax.lax_control_flow.(_, eres)->split_list(xs, [sum(xs_lin)])
A:jax.lax.lax_control_flow.(ct_carry, ct_ys)->split_list(cts, [num_carry])
A:jax.lax.lax_control_flow.ct_carry->_map(ad.instantiate_zeros_aval, carry_avals, ct_carry)
A:jax.lax.lax_control_flow.ct_ys->_map(ad.instantiate_zeros_aval, ys_avals, ct_ys)
A:jax.lax.lax_control_flow.ct_consts->_map(ad_util.zeros_like_aval, jaxpr.in_avals[num_ires:num_consts])
A:jax.lax.lax_control_flow.jaxpr_trans->_transpose_scan_jaxpr(num_ires, num_consts - num_ires, num_eres, jaxpr)
A:jax.lax.lax_control_flow.(ct_consts, ct_init, ct_xs)->split_list(outs, [num_consts - num_ires, num_carry])
A:jax.lax.lax_control_flow.(res1_avals, c_avals, a_avals, res2_avals)->split_list(jaxpr.in_avals, [num_res1, num_c, num_a])
A:jax.lax.lax_control_flow.num_b->len(jaxpr.out_avals)
A:jax.lax.lax_control_flow.b_avals->tuple(_map(_abstractify, b_flat))
A:jax.lax.lax_control_flow.(res1, c_bar, b_bar, res2)->split_list(res1_cbar_bbar_res2, [num_res1, num_c, num_b])
A:jax.lax.lax_control_flow.cbar_abar->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, jaxpr.literals, primals, b_bar)
A:jax.lax.lax_control_flow.(_, new_c_bar, a_bar, _)->split_list(cbar_abar, [num_res1, num_c, num_a])
A:jax.lax.lax_control_flow.a_bar->_map(ad.instantiate_zeros_aval, a_avals, a_bar)
A:jax.lax.lax_control_flow.c_bar->_map(ad.instantiate_zeros_aval, c_avals, _map(ad.add_tangents, c_bar, new_c_bar))
A:jax.lax.lax_control_flow.(jaxpr, pvals_out, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(traceable, pvals, instantiate=True)
A:jax.lax.lax_control_flow.(out_avals, _)->unzip2(pvals_out)
A:jax.lax.lax_control_flow.(const_batched, init_batched, xs_batched)->split_list(orig_batched, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_batched, batched_out)->jax.interpreters.batching.batch_jaxpr(jaxpr, size, batched, instantiate=carry_batched + [False] * num_ys)
A:jax.lax.lax_control_flow.carry_batched->_map(operator.or_, carry_batched, carry_batched_out)
A:jax.lax.lax_control_flow.(consts_bdims, init_bdims, xs_bdims)->split_list(dims, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(const_shexprs, init_shexprs, xs_shexprs)->split_list(shapes, [num_consts, num_carry])
A:jax.lax.lax_control_flow.out_shape->_scan_shape_rule(shape_exprs, forward, length, jaxpr, num_consts, num_carry, linear)
A:jax.lax.lax_control_flow.dynamic_length->jax.interpreters.masking.eval_dim_expr(shape_envs.logical, length)
A:jax.lax.lax_control_flow.masked_jaxpr->_masked_scan_jaxpr(jaxpr, num_consts, num_carry)
A:jax.lax.lax_control_flow.(const_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.out_vals->jax.core.Primitive('scan').bind(*itertools.chain([dynamic_length] + consts, [0], init, xs), forward=forward, length=max_length, jaxpr=masked_jaxpr, num_consts=1 + num_consts, num_carry=1 + num_carry, linear=tuple([False] + const_linear + [False] + init_linear + xs_linear))
A:jax.lax.lax_control_flow.fun->jax.core.jaxpr_as_fun(jaxpr)
A:jax.lax.lax_control_flow.([dynamic_length], consts, [i], carry, xs)->split_list(args, [1, num_consts, 1, num_carry])
A:jax.lax.lax_control_flow.(new_carry, ys)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.(const_avals, carry_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts_avals, init_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.xs_avals->_map(partial(_promote_aval_rank, length), x_avals)
A:jax.lax.lax_control_flow.(carry_avals, _)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.scan_p->jax.core.Primitive('scan')
A:jax.lax.lax_control_flow.xla.initial_style_translations[scan_p]->jax.interpreters.xla.lower_fun_initial_style(_scan_impl)
A:jax.lax.lax_control_flow.(_, ys)->scan(g, (), xs)
A:jax.lax.lax_control_flow.result->_memcpy(dimension, logical_shape[dimension], padded_val, result, offset)
A:jax.lax.lax_control_flow.update->jax.lax.lax.dynamic_index_in_dim(src, i, axis)
A:jax.lax.lax_control_flow.(args_flat, in_args_tree)->tree_flatten((args, kwargs))
A:jax.lax.lax_control_flow.args_avals->tuple(_map(_abstractify, args_flat))
A:jax.lax.lax_control_flow._RootTuple->collections.namedtuple('_RootTuple', 'f, solve, l_and_s')
A:jax.lax.lax_control_flow.params_list->split_list(args, list(const_lengths))
A:jax.lax.lax_control_flow.(guess_flat, in_args_tree)->tree_flatten((initial_guess,))
A:jax.lax.lax_control_flow.guess_avals->tuple(_map(_abstractify, guess_flat))
A:jax.lax.lax_control_flow.(f_jaxpr, f_consts, out_tree)->_initial_style_jaxpr(f, in_args_tree, guess_avals)
A:jax.lax.lax_control_flow.(in_tree,)->treedef_children(in_args_tree)
A:jax.lax.lax_control_flow.(solve_jaxpr, solve_consts, solution_tree)->_initial_style_jaxpr(partial(solve, _stop_gradient_fun(f)), in_args_tree, guess_avals)
A:jax.lax.lax_control_flow.(unchecked_zeros, f_jvp)->jax.api.linearize(f, x)
A:jax.lax.lax_control_flow.(l_and_s_jaxpr, l_and_s_consts, out_tree)->_initial_style_jaxpr(linearize_and_solve, treedef_tuple((in_tree,) * 2), guess_avals * 2)
A:jax.lax.lax_control_flow.const_lengths->_LinearSolveTuple(*_map(len, all_consts))
A:jax.lax.lax_control_flow.jaxprs->_LinearSolveTuple(matvec_jaxpr, vecmat_jaxpr, solve_jaxpr, tr_solve_jaxpr)
A:jax.lax.lax_control_flow.(const_lengths, jaxprs)->split_dict(kwargs, ['const_lengths', 'jaxprs'])
A:jax.lax.lax_control_flow.(params, initial_guess)->_split_root_args(args, const_lengths)
A:jax.lax.lax_control_flow.solution->tuple(root_p.bind(*primals, const_lengths=const_lengths, jaxprs=jaxprs))
A:jax.lax.lax_control_flow.(params, _)->_split_linear_solve_args(primals, const_lengths)
A:jax.lax.lax_control_flow.(params_dot, _)->_split_root_args(tangents, const_lengths)
A:jax.lax.lax_control_flow.f->jax.core.jaxpr_as_fun(jaxprs.f)
A:jax.lax.lax_control_flow.linearize_and_solve->partial(core.jaxpr_as_fun(jaxprs.l_and_s), *params.l_and_s)
A:jax.lax.lax_control_flow.(_, rhs)->jax.interpreters.ad.jvp(lu.wrap_init(f_at_solution)).call_wrapped(params.f, params_dot.f)
A:jax.lax.lax_control_flow.solution_dot->_map(operator.neg, linearize_and_solve(*itertools.chain(solution, rhs)))
A:jax.lax.lax_control_flow.root_p->jax.core.Primitive('root')
A:jax.lax.lax_control_flow.xla.initial_style_translations[root_p]->jax.interpreters.xla.lower_fun_initial_style(_root_impl)
A:jax.lax.lax_control_flow.(_, vjp_fun)->jax.api.vjp(linear_fun, primals)
A:jax.lax.lax_control_flow.(y,)->vjp_fun(x)
A:jax.lax.lax_control_flow.actual_shapes->_map(onp.shape, actual)
A:jax.lax.lax_control_flow.expected_shapes->_map(onp.shape, expected)
A:jax.lax.lax_control_flow.actual_shape_tree->tree_unflatten(tree, actual_shapes)
A:jax.lax.lax_control_flow.act_shape_tree->tree_unflatten(tree, actual_shapes)
A:jax.lax.lax_control_flow.(b_flat, in_args_tree)->tree_flatten((b,))
A:jax.lax.lax_control_flow.(matvec_jaxpr, matvec_consts, out_tree)->_initial_style_jaxpr(matvec, in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(tree,)->treedef_children(in_args_tree)
A:jax.lax.lax_control_flow.(solve_jaxpr, solve_consts, out_tree)->_initial_style_jaxpr(partial(solve, matvec), in_args_tree, b_avals)
A:jax.lax.lax_control_flow.vecmat->_transpose_function(matvec, b)
A:jax.lax.lax_control_flow.(vecmat_jaxpr, vecmat_consts, out_tree)->_initial_style_jaxpr(vecmat, in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(tr_solve_jaxpr, tr_solve_consts, out_tree)->_initial_style_jaxpr(partial(transpose_solve, vecmat), in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(const_lengths, jaxprs, tree)->split_dict(kwargs, ['const_lengths', 'jaxprs', 'tree'])
A:jax.lax.lax_control_flow.(params, b)->_split_linear_solve_args(args, const_lengths)
A:jax.lax.lax_control_flow.(_, out_tangent)->jax.interpreters.ad.jvp(lu.wrap_init(func)).call_wrapped(params + list(x), params_dot + zeros)
A:jax.lax.lax_control_flow.kwargs->dict(const_lengths=const_lengths, jaxprs=jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(params_dot, b_dot)->_split_linear_solve_args(tangents, const_lengths)
A:jax.lax.lax_control_flow.matvec_tangents->_tangent_linear_map(core.jaxpr_as_fun(jaxprs.matvec), params.matvec, params_dot.matvec, *x)
A:jax.lax.lax_control_flow.rhs->_map(ad.add_tangents, b_dot, _map(operator.neg, matvec_tangents))
A:jax.lax.lax_control_flow.x_dot->jax.core.Primitive('custom_linear_solve').bind(*_flatten(params) + rhs, **kwargs)
A:jax.lax.lax_control_flow.cotangent_b->jax.core.Primitive('custom_linear_solve').bind(*_flatten(params.transpose()) + cotangent, const_lengths=const_lengths.transpose(), jaxprs=jaxprs.transpose(), tree=tree)
A:jax.lax.lax_control_flow.(params_dims, b_dims)->_split_linear_solve_args(dims, const_lengths)
A:jax.lax.lax_control_flow.(params_bat, orig_b_bat)->_split_linear_solve_args(orig_bat, const_lengths)
A:jax.lax.lax_control_flow.(solve_jaxpr_batched, solve_x_bat)->jax.interpreters.batching.batch_jaxpr(solve, size, solve_bat + b_bat, instantiate=x_bat)
A:jax.lax.lax_control_flow.(vecmat_jaxpr_batched, vecmat_x_bat)->jax.interpreters.batching.batch_jaxpr(vecmat, size, vecmat_bat + b_bat, instantiate=x_bat)
A:jax.lax.lax_control_flow.x_bat_out->_map(operator.or_, vecmat_x_bat, solve_x_bat)
A:jax.lax.lax_control_flow.(matvec_jaxpr_batched, matvec_b_bat)->jax.interpreters.batching.batch_jaxpr(matvec, size, matvec_bat + x_bat_out, instantiate=b_bat)
A:jax.lax.lax_control_flow.b_bat_out->_map(lambda m, s, o: m or s or o, matvec_b_bat, solve_t_b_bat, orig_b_bat)
A:jax.lax.lax_control_flow.(solve_t_jaxpr_batched, solve_t_b_bat)->jax.interpreters.batching.batch_jaxpr(solve_t, size, solve_t_bat + x_bat_out, instantiate=b_bat)
A:jax.lax.lax_control_flow.batched_jaxprs->_LinearSolveTuple(matvec_jaxpr_batched, vecmat_jaxpr_batched, solve_jaxpr_batched, solve_t_jaxpr_batched)
A:jax.lax.lax_control_flow.linear_solve_p->jax.core.Primitive('custom_linear_solve')
A:jax.lax.lax_control_flow.xla.initial_style_translations[linear_solve_p]->jax.interpreters.xla.lower_fun_initial_style(_custom_linear_solve_impl)
jax.lax.FixedPointError(Exception)
jax.lax._LinearSolveTuple(collections.namedtuple('_LinearSolveTuple','matvec,vecmat,solve,transpose_solve'))
jax.lax._LinearSolveTuple.transpose(self)
jax.lax._abstractify(x)
jax.lax._check_shapes(func_name,expected_name,actual,expected,tree)
jax.lax._check_tree(func_name,expected_name,actual_tree,expected_tree)
jax.lax._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax.lax._concat_masking_rule(padded_vals,logical_shapes,dimension)
jax.lax._cond_abstract_eval(*args,**kwargs)
jax.lax._cond_batching_rule(args,dims,true_jaxpr,false_jaxpr,linear)
jax.lax._cond_jvp(primals,tangents,true_jaxpr,false_jaxpr,linear)
jax.lax._cond_partial_eval(trace,*tracers,true_jaxpr,false_jaxpr,linear)
jax.lax._cond_pred_bcast_select(pred,x,y)
jax.lax._cond_translation_rule(c,axis_env,name_stack,avals,backend,pred,*args,true_jaxpr,false_jaxpr,linear)
jax.lax._cond_transpose(cts,*args,true_jaxpr,false_jaxpr,linear)
jax.lax._custom_linear_solve_impl(*args,**kwargs)
jax.lax._custom_linear_solve_jvp(primals,tangents,const_lengths,jaxprs,tree)
jax.lax._empty_array(sz,aval)
jax.lax._flatten(args)
jax.lax._fori_body_fun(body_fun)
jax.lax._fori_cond_fun(loop_carry)
jax.lax._fori_scan_body_fun(body_fun)
jax.lax._index_array(i,aval,x)
jax.lax._initial_style_jaxpr(fun:Callable,in_tree,in_avals)
jax.lax._join_cond_outputs(jaxpr,num_prefix,zeros_avals,zeros_on_left)
jax.lax._linear_solve_abstract_eval(*args,**kwargs)
jax.lax._linear_solve_batching_rule(args,dims,**kwargs)
jax.lax._linear_solve_transpose_rule(cotangent,*primals,**kwargs)
jax.lax._make_typed_jaxpr(traceable:lu.WrappedFun,in_avals)
jax.lax._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax.lax._memcpy(axis,num,src,dst,offset)
jax.lax._pred_bcast_select(c,pred,x,y)
jax.lax._promote_aval_rank(sz,aval)
jax.lax._prune_zeros(ts)
jax.lax._root_abstract_eval(*args,**kwargs)
jax.lax._root_impl(*args,**kwargs)
jax.lax._root_jvp(primals,tangents,const_lengths,jaxprs)
jax.lax._scan_abstract_eval(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax._scan_batching_rule(args,dims,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_impl(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax._scan_jvp(primals,tangents,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_masking_rule(shape_envs,padded_vals,shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_partial_eval(trace,*tracers,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax._scan_shape_rule(shapes,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax._scan_transpose(cts,*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax._split_linear_solve_args(args,const_lengths)
jax.lax._split_root_args(args,const_lengths)
jax.lax._stop_gradient_fun(f)
jax.lax._tangent_linear_map(func,params,params_dot,*x)
jax.lax._transpose_cond_jaxpr(jaxpr,num_res)
jax.lax._transpose_function(linear_fun,primals)
jax.lax._transpose_scan_jaxpr(num_res1,num_c,num_res2,jaxpr)
jax.lax._update_array(i,aval,xs,x)
jax.lax._while_loop_abstract_eval(*args,**kwargs)
jax.lax._while_loop_batching_rule(args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax._while_loop_jvp(primals,tangents,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax._while_loop_translation_rule(c,axis_env,name_stack,avals,backend,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax.lax.cond(pred,true_operand,true_fun,false_operand,false_fun)
jax.lax.cond_bind(*args,true_jaxpr,false_jaxpr,linear)
jax.lax.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False)
jax.lax.custom_root(f,initial_guess,solve,tangent_solve)
jax.lax.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow.FixedPointError(Exception)
jax.lax.lax_control_flow._LinearSolveTuple(collections.namedtuple('_LinearSolveTuple','matvec,vecmat,solve,transpose_solve'))
jax.lax.lax_control_flow._LinearSolveTuple.transpose(self)
jax.lax.lax_control_flow._abstractify(x)
jax.lax.lax_control_flow._check_shapes(func_name,expected_name,actual,expected,tree)
jax.lax.lax_control_flow._check_tree(func_name,expected_name,actual_tree,expected_tree)
jax.lax.lax_control_flow._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax.lax.lax_control_flow._concat_masking_rule(padded_vals,logical_shapes,dimension)
jax.lax.lax_control_flow._cond_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._cond_batching_rule(args,dims,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow._cond_jvp(primals,tangents,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow._cond_partial_eval(trace,*tracers,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow._cond_pred_bcast_select(pred,x,y)
jax.lax.lax_control_flow._cond_translation_rule(c,axis_env,name_stack,avals,backend,pred,*args,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow._cond_transpose(cts,*args,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow._custom_linear_solve_impl(*args,**kwargs)
jax.lax.lax_control_flow._custom_linear_solve_jvp(primals,tangents,const_lengths,jaxprs,tree)
jax.lax.lax_control_flow._empty_array(sz,aval)
jax.lax.lax_control_flow._flatten(args)
jax.lax.lax_control_flow._fori_body_fun(body_fun)
jax.lax.lax_control_flow._fori_cond_fun(loop_carry)
jax.lax.lax_control_flow._fori_scan_body_fun(body_fun)
jax.lax.lax_control_flow._index_array(i,aval,x)
jax.lax.lax_control_flow._initial_style_jaxpr(fun:Callable,in_tree,in_avals)
jax.lax.lax_control_flow._join_cond_outputs(jaxpr,num_prefix,zeros_avals,zeros_on_left)
jax.lax.lax_control_flow._linear_solve_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._linear_solve_batching_rule(args,dims,**kwargs)
jax.lax.lax_control_flow._linear_solve_transpose_rule(cotangent,*primals,**kwargs)
jax.lax.lax_control_flow._make_typed_jaxpr(traceable:lu.WrappedFun,in_avals)
jax.lax.lax_control_flow._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax.lax.lax_control_flow._memcpy(axis,num,src,dst,offset)
jax.lax.lax_control_flow._pred_bcast_select(c,pred,x,y)
jax.lax.lax_control_flow._promote_aval_rank(sz,aval)
jax.lax.lax_control_flow._prune_zeros(ts)
jax.lax.lax_control_flow._root_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._root_impl(*args,**kwargs)
jax.lax.lax_control_flow._root_jvp(primals,tangents,const_lengths,jaxprs)
jax.lax.lax_control_flow._scan_abstract_eval(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.lax_control_flow._scan_batching_rule(args,dims,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_impl(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.lax_control_flow._scan_jvp(primals,tangents,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_masking_rule(shape_envs,padded_vals,shape_exprs,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_partial_eval(trace,*tracers,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.lax_control_flow._scan_shape_rule(shapes,forward,length,jaxpr,num_consts,num_carry,linear)
jax.lax.lax_control_flow._scan_transpose(cts,*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.lax_control_flow._split_linear_solve_args(args,const_lengths)
jax.lax.lax_control_flow._split_root_args(args,const_lengths)
jax.lax.lax_control_flow._stop_gradient_fun(f)
jax.lax.lax_control_flow._tangent_linear_map(func,params,params_dot,*x)
jax.lax.lax_control_flow._transpose_cond_jaxpr(jaxpr,num_res)
jax.lax.lax_control_flow._transpose_function(linear_fun,primals)
jax.lax.lax_control_flow._transpose_scan_jaxpr(num_res1,num_c,num_res2,jaxpr)
jax.lax.lax_control_flow._update_array(i,aval,xs,x)
jax.lax.lax_control_flow._while_loop_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._while_loop_batching_rule(args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax.lax_control_flow._while_loop_jvp(primals,tangents,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax.lax_control_flow._while_loop_translation_rule(c,axis_env,name_stack,avals,backend,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax.lax.lax_control_flow.cond(pred,true_operand,true_fun,false_operand,false_fun)
jax.lax.lax_control_flow.cond_bind(*args,true_jaxpr,false_jaxpr,linear)
jax.lax.lax_control_flow.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False)
jax.lax.lax_control_flow.custom_root(f,initial_guess,solve,tangent_solve)
jax.lax.lax_control_flow.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow.map(f,xs)
jax.lax.lax_control_flow.scan(f,init,xs,length=None)
jax.lax.lax_control_flow.scan_bind(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.lax_control_flow.typecheck(aval,x)
jax.lax.lax_control_flow.typematch(aval1,aval2)
jax.lax.lax_control_flow.while_loop(cond_fun,body_fun,init_val)
jax.lax.map(f,xs)
jax.lax.scan(f,init,xs,length=None)
jax.lax.scan_bind(*args,forward,length,num_consts,num_carry,jaxpr,linear)
jax.lax.typecheck(aval,x)
jax.lax.typematch(aval1,aval2)
jax.lax.while_loop(cond_fun,body_fun,init_val)


----------------------------------------/home/zhang/Packages/jax/jax0.1.62/lax/lax_fft.py----------------------------------------
A:jax.lax.lax_fft.dtype->_real_dtype(x.dtype)
A:jax.lax.lax_fft.x->interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax.lax_fft.fft_lengths->tuple(fft_lengths)
A:jax.lax.lax_fft.y->fft(x, xla_client.FftType.FFT, fft_lengths)
A:jax.lax.lax_fft.dummy_primals->lax.full_like(t, 0.0, onp.float64, dummy_shape)
A:jax.lax.lax_fft.(_, jvpfun)->vjp(partial(_naive_rfft, fft_lengths=fft_lengths), dummy_primals)
A:jax.lax.lax_fft.(result,)->jvpfun(t)
A:jax.lax.lax_fft.full->partial(lax.full_like, t, dtype=onp.float64)
A:jax.lax.lax_fft.mask->lax.concatenate([full(1.0, shape=(1,)), full(2.0, shape=(n - 2 + is_odd,)), full(1.0, shape=(1 - is_odd,))], dimension=0)
A:jax.lax.lax_fft.result->fft(t, fft_type, fft_lengths)
A:jax.lax.lax_fft.fft_p->Primitive('fft')
jax.lax._irfft_transpose(t,fft_lengths)
jax.lax._naive_rfft(x,fft_lengths)
jax.lax._promote_to_complex(arg)
jax.lax._promote_to_real(arg)
jax.lax._rfft_transpose(t,fft_lengths)
jax.lax.fft(x,fft_type,fft_lengths)
jax.lax.fft_abstract_eval(x,fft_type,fft_lengths)
jax.lax.fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax.lax.fft_impl(x,fft_type,fft_lengths)
jax.lax.fft_translation_rule(c,x,fft_type,fft_lengths)
jax.lax.fft_transpose_rule(t,fft_type,fft_lengths)
jax.lax.lax_fft._irfft_transpose(t,fft_lengths)
jax.lax.lax_fft._naive_rfft(x,fft_lengths)
jax.lax.lax_fft._promote_to_complex(arg)
jax.lax.lax_fft._promote_to_real(arg)
jax.lax.lax_fft._rfft_transpose(t,fft_lengths)
jax.lax.lax_fft.fft(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_abstract_eval(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax.lax.lax_fft.fft_impl(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_translation_rule(c,x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_transpose_rule(t,fft_type,fft_lengths)

