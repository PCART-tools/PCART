
----------------------------------------/home/zhang/Packages/jax/jax0.1.75/flatten_util.py----------------------------------------
A:jax.flatten_util.(leaves, treedef)->tree_flatten(pytree)
A:jax.flatten_util.(flat, unravel_list)->vjp(_ravel_list, *leaves)
jax.flatten_util._ravel_list(*lst)
jax.flatten_util.ravel_pytree(pytree)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/util.py----------------------------------------
A:jax.util.n->len(args[0])
A:jax.util.args->list(args)
A:jax.util.lst->list(lst)
A:jax.util.dct->dict(dct)
A:jax.util.wrapped->functools.partial(fun, *args, **kwargs)
A:jax.util.end_nodes->_remove_duplicates(end_nodes)
A:jax.util.stack->list(end_nodes)
A:jax.util.node->childless_nodes.pop()
A:jax.util.visited->set()
A:jax.util.seen->set()
A:jax.util.sides->list(map(predicate, xs))
A:jax.util.memoize->functools.lru_cache(maxsize=None)
A:jax.util.attr->getattr(module, key)
jax.util.Hashable(self,val)
jax.util.Hashable.__eq__(self,other)
jax.util.Hashable.__hash__(self)
jax.util.WrapHashably(self,val)
jax.util.WrapHashably.__eq__(self,other)
jax.util.WrapHashably.__hash__(self)
jax.util._remove_duplicates(node_list)
jax.util.cache(max_size=4096)
jax.util.check_toposort(nodes)
jax.util.concatenate(xs)
jax.util.curry(f)
jax.util.extend_name_stack(stack,name='')
jax.util.get_module_functions(module)
jax.util.partial(fun,*args,**kwargs)
jax.util.partialmethod(functools.partial)
jax.util.partialmethod.__get__(self,instance,owner)
jax.util.prod(xs)
jax.util.safe_map(f,*args)
jax.util.safe_zip(*args)
jax.util.split_dict(dct,names)
jax.util.split_list(args,ns)
jax.util.split_merge(predicate,xs)
jax.util.subvals(lst,replace)
jax.util.toposort(end_nodes)
jax.util.unzip2(xys)
jax.util.unzip3(xyzs)
jax.util.unzip4(wxyzs)
jax.util.wrap_name(name,transform_name)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/abstract_arrays.py----------------------------------------
A:jax.abstract_arrays.dtype->dtypes.canonicalize_dtype(dtypes.result_type(x))
A:jax.abstract_arrays.core.pytype_aval_mappings[t]->partial(_make_concrete_python_scalar, t)
A:jax.abstract_arrays.ad_util.jaxval_zeros_likers[t]->partial(_zeros_like_python_scalar, t)
jax.abstract_arrays._make_concrete_python_scalar(t,x)
jax.abstract_arrays._zeros_like_python_scalar(t,x)
jax.abstract_arrays.make_shaped_array(x)
jax.abstract_arrays.zeros_like_array(x)
jax.abstract_arrays.zeros_like_shaped_array(aval)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax_reference.py----------------------------------------
A:jax.lax_reference.quotient->numpy.floor_divide(lhs, rhs)
A:jax.lax_reference.select->numpy.logical_and(np.sign(lhs) != np.sign(rhs), np.remainder(lhs, rhs) != 0)
A:jax.lax_reference.iinfo->numpy.iinfo(x.dtype)
A:jax.lax_reference.x->x.view(f'uint{np.iinfo(x.dtype).bits}').view(f'uint{np.iinfo(x.dtype).bits}')
A:jax.lax_reference.m->list(map(np.uint64, m))
A:jax.lax_reference.pads->padtype_to_pads(op.shape, dims, strides, padding)
A:jax.lax_reference.(lhs_perm, rhs_perm, out_perm)->_conv_general_permutations(dimension_numbers)
A:jax.lax_reference.padding->padtype_to_pads(np.take(lhs.shape, lhs_perm)[2:], np.take(rhs.shape, rhs_perm)[2:], window_strides, padding)
A:jax.lax_reference.trans_lhs->transpose(lhs, lhs_perm)
A:jax.lax_reference.trans_rhs->transpose(rhs, rhs_perm)
A:jax.lax_reference.out->numpy.full(operand.shape[:2] + tuple(outspace), fill_value, operand.dtype)
A:jax.lax_reference.new_id->itertools.count()
A:jax.lax_reference.shared_id->next(new_id)
A:jax.lax_reference.out_axis_ids->filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids)
A:jax.lax_reference.in_reshape->numpy.ones(len(shape), dtype=np.int32)
A:jax.lax_reference.dimensions->frozenset(dimensions)
A:jax.lax_reference.(lo, hi, interior)->zip(*padding_config)
A:jax.lax_reference.outshape->numpy.add(np.add(np.add(lo_pos, hi_pos), operand.shape), np.multiply(interior, np.subtract(operand.shape, 1)))
A:jax.lax_reference.lhs_slices->tuple((_slice(None, None, step) for step in factors))
A:jax.lax_reference.trim_slices->tuple((_slice(-l if l < 0 else 0, h if h < 0 else None) for (l, h) in zip(lo, hi)))
A:jax.lax_reference.strides->numpy.ones(len(start_indices)).astype(int)
A:jax.lax_reference.slices->tuple((_slice(abs(lo) if lo < 0 else 0, hi % dim if hi < 0 else None) for ((lo, hi), dim) in zip(pads, np.shape(arr))))
A:jax.lax_reference.idx->tuple((_slice(start, start + size) for (start, size) in zip(start_indices, slice_sizes)))
A:jax.lax_reference.updated_operand->numpy.copy(operand)
A:jax.lax_reference.reducer->_make_reducer(computation, init_value)
A:jax.lax_reference.op->_dilate(op, base_dilation, init_value)
A:jax.lax_reference.view->numpy.lib.stride_tricks.as_strided(lhs, view_shape, view_strides)
A:jax.lax_reference.idxs->list(np.ix_(*[np.arange(d) for d in keys.shape]))
A:jax.lax_reference.idxs[dimension]->numpy.argsort(keys, axis=dimension)
A:jax.lax_reference.(view, view_axes, rhs_axes, out_axes)->_conv_view(lhs, rhs.shape, window_strides, pads, 0.0)
A:jax.lax_reference.out_shape->numpy.ceil(np.true_divide(in_shape, window_strides)).astype(int)
A:jax.lax_reference.lhs->_pad(lhs, [(0, 0)] * 2 + list(pads), pad_value)
A:jax.lax_reference.dim->len(filter_shape)
A:jax.lax_reference.out_strides->numpy.multiply(window_strides, lhs.strides[2:])
A:jax.lax_reference.view_axes->list(range(view.ndim))
A:jax.lax_reference.outspace->numpy.add(operand.shape[2:], np.multiply(np.subtract(factors, 1), np.subtract(operand.shape[2:], 1)))
A:jax.lax_reference.monoid_record->_monoids.get(getattr(py_binop, '__name__'))
A:jax.lax_reference.MonoidRecord->collections.namedtuple('MonoidRecord', ['reducer', 'identity'])
A:jax.lax_reference.result->numpy.full(np.delete(np.shape(operand), axis), init_val, dtype=np.asarray(operand).dtype)
A:jax.lax_reference.out_idx->tuple(np.delete(idx, axis))
A:jax.lax_reference.result[out_idx]->py_binop(result[out_idx], operand[idx])
jax.lax_reference._conv(lhs,rhs,window_strides,pads)
jax.lax_reference._conv_general_permutations(dimension_numbers)
jax.lax_reference._conv_view(lhs,rhs_shape,window_strides,pads,pad_value)
jax.lax_reference._dilate(operand,factors,fill_value=0)
jax.lax_reference._get_max_identity(dt)
jax.lax_reference._get_min_identity(dt)
jax.lax_reference._identity_getter(op)
jax.lax_reference._make_reducer(py_binop,init_val)
jax.lax_reference._pad(arr,pads,pad_value)
jax.lax_reference._reducer_from_pyfunc(py_binop,init_val)
jax.lax_reference.bessel_i0e(x)
jax.lax_reference.bessel_i1e(x)
jax.lax_reference.betainc(a,b,x)
jax.lax_reference.bitcast_convert_type(operand,dtype)
jax.lax_reference.broadcast(operand,sizes)
jax.lax_reference.broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.lax_reference.clamp(min,operand,max)
jax.lax_reference.complex(x,y)
jax.lax_reference.concatenate(operands,dimension)
jax.lax_reference.conj(x)
jax.lax_reference.conv(lhs,rhs,window_strides,padding)
jax.lax_reference.conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers)
jax.lax_reference.conv_with_general_padding(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation)
jax.lax_reference.convert_element_type(operand,dtype)
jax.lax_reference.digamma(x)
jax.lax_reference.div(lhs,rhs)
jax.lax_reference.dot_general(lhs,rhs,dimension_numbers)
jax.lax_reference.dynamic_slice(operand,start_indices,slice_sizes)
jax.lax_reference.dynamic_update_slice(operand,update,start_indices)
jax.lax_reference.erf(x)
jax.lax_reference.erf_inv(x)
jax.lax_reference.erfc(x)
jax.lax_reference.lgamma(x)
jax.lax_reference.pad(operand,padding_value,padding_config)
jax.lax_reference.padtype_to_pads(in_shape,filter_shape,window_strides,padding)
jax.lax_reference.population_count(x)
jax.lax_reference.reduce(operand,init_value,computation,dimensions)
jax.lax_reference.reduce_window(operand,init_value,computation,window_dimensions,window_strides,padding,base_dilation)
jax.lax_reference.rem(lhs,rhs)
jax.lax_reference.reshape(operand,new_sizes,dimensions=None)
jax.lax_reference.rev(operand,dimensions)
jax.lax_reference.round(x)
jax.lax_reference.slice(operand,start_indices,limit_indices,strides=None)
jax.lax_reference.sort_key_val(keys,values,dimension=-1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/profiler.py----------------------------------------
A:jax.profiler.profile->device_memory_profile(backend)
jax.profiler.StepTraceContext(self,name:str,**kwargs)
jax.profiler.TraceContext(xla_client.profiler.TraceMe)
jax.profiler.device_memory_profile(backend:Optional[str]=None)->bytes
jax.profiler.save_device_memory_profile(filename,backend:Optional[str]=None)
jax.profiler.start_server(port:int)
jax.profiler.trace_function(func:Callable,name:str=None,**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lazy.py----------------------------------------
A:jax.lazy.class_namespace[f]->property(op.itemgetter(i + 1))
A:jax.lazy.LazyExpr->namedtuple('LazyExpr', ['input', 'shape', 'dims'])
A:jax.lazy.ArrayVar->taggedtuple('ArrayVar', [])
A:jax.lazy.Iota->taggedtuple('Iota', ['dtype', 'size'])
A:jax.lazy.Eye->taggedtuple('Eye', ['dtype', 'shape', 'offset'])
A:jax.lazy.Tri->taggedtuple('Tri', ['dtype', 'shape', 'offset'])
A:jax.lazy.Delta->taggedtuple('Delta', ['dtype', 'shape'])
A:jax.lazy.new_shape->tuple((lexpr.shape[i] for i in perm))
A:jax.lazy.new_dims->tuple((lexpr.dims[i] for i in perm))
A:jax.lazy.t->type(input_)
A:jax.lazy.x->xops.BroadcastInDim(x, shape, bcast_dims)
A:jax.lazy.xla_shape->lib.xla_client.Shape.array_shape(xc.PrimitiveType.S32, (N, M))
A:jax.lazy.bool_eye->xops.Eq(xops.Add(xops.Iota(c, xla_shape, 0), xb.constant(c, np.array(input_.offset, np.int32))), xops.Iota(c, xla_shape, 1))
A:jax.lazy.bool_tri->xops.Ge(xops.Add(xops.Iota(c, xla_shape, 0), xb.constant(c, np.array(input_.offset, np.int32))), xops.Iota(c, xla_shape, 1))
A:jax.lazy.etype->lib.xla_bridge.dtype_to_etype(input_.dtype)
A:jax.lazy.(bcast_dims, perm)->unzip2(((i, d) for (i, d) in enumerate(dims) if d is not None))
jax.lazy.array(shape)
jax.lazy.broadcast(lexpr,shape,broadcast_dimensions)
jax.lazy.delta(dtype,shape)
jax.lazy.eval_lexpr(lexpr,x)
jax.lazy.eye(dtype,shape,offset)
jax.lazy.iota(dtype,size)
jax.lazy.is_constant(lexpr:Optional[LazyExpr])
jax.lazy.is_trivial(lexpr:LazyExpr)->bool
jax.lazy.stage_lexpr(c,lexpr:Optional[LazyExpr],x)
jax.lazy.taggedtuple(name,fields)->Callable[..., Any]
jax.lazy.transpose(lexpr:LazyExpr,perm:Sequence[int])
jax.lazy.tri(dtype,shape,offset)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/test_util.py----------------------------------------
A:jax.test_util.tol->_default_tolerance.copy()
A:jax.test_util.dtype->numpy.dtype(dtype)
A:jax.test_util.tol1->_normalize_tolerance(tol1)
A:jax.test_util.tol2->_normalize_tolerance(tol2)
A:jax.test_util.out[k]->max(v, tol1.get(k, 0))
A:jax.test_util.atol->max(tolerance(_dtype(x), atol), tolerance(_dtype(y), atol))
A:jax.test_util.rtol->max(tolerance(_dtype(x), rtol), tolerance(_dtype(y), rtol))
A:jax.test_util.assert_close->partial(_assert_numpy_close, atol=atol, rtol=rtol)
A:jax.test_util.add->partial(tree_multimap, lambda x, y: np.add(x, y, dtype=_dtype(x)))
A:jax.test_util.sub->partial(tree_multimap, lambda x, y: np.subtract(x, y, dtype=_dtype(x)))
A:jax.test_util.safe_sub->partial(tree_multimap, lambda x, y: _safe_subtract(x, y, dtype=_dtype(x)))
A:jax.test_util.conj->partial(tree_map, lambda x: np.conj(x, dtype=_dtype(x)))
A:jax.test_util.shape->tuple(shape)
A:jax.test_util.delta->scalar_mul(tangents, eps)
A:jax.test_util.f_pos->f(*add(primals, delta))
A:jax.test_util.f_neg->f(*sub(primals, delta))
A:jax.test_util.out->default.copy()
A:jax.test_util.rng->numpy.random.RandomState(42)
A:jax.test_util.tangent->tree_map(_rand_like, args)
A:jax.test_util.(v_out, t_out)->f_jvp(args, tangent)
A:jax.test_util.v_out_expected->f(*args)
A:jax.test_util.t_out_expected->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util._rand_like->partial(rand_like, np.random.RandomState(0))
A:jax.test_util.(v_out, vjpfun)->f_vjp(*args)
A:jax.test_util.tangent_out->numerical_jvp(f, args, tangent, eps=eps)
A:jax.test_util.cotangent->tree_map(_rand_like, v_out)
A:jax.test_util.cotangent_out->conj(vjpfun(conj(cotangent)))
A:jax.test_util.ip->inner_prod(tangent, cotangent_out)
A:jax.test_util.ip_expected->inner_prod(tangent_out, cotangent)
A:jax.test_util.args->args_maker()
A:jax.test_util._check_jvp->partial(check_jvp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util._check_vjp->partial(check_vjp, atol=atol, rtol=rtol, eps=eps)
A:jax.test_util.(out_primal_py, vjp_py)->api.vjp(f, *args)
A:jax.test_util.device->device_under_test()
A:jax.test_util.test_name->getattr(test_method, '__name__', '[unknown test]')
A:jax.test_util.flag_value->getattr(FLAGS, flag_name)
A:jax.test_util.NUMPY_SCALAR_SHAPE->_NumpyScalar()
A:jax.test_util.PYTHON_SCALAR_SHAPE->_PythonScalar()
A:jax.test_util.shapestr->','.join((str(dim) for dim in shape))
A:jax.test_util.vals->numpy.where(zeros, np.array(0, dtype=dtype), vals)
A:jax.test_util.x_ravel->numpy.asarray(x).ravel()
A:jax.test_util.base_rand->rand_default(rng)
A:jax.test_util.dims->_dims_of_shape(shape)
A:jax.test_util.jaxpr->api.make_jaxpr(fun)(*args)
A:jax.test_util.msg->'Unexpected precision: {} != {}'.format(expected_precision, precision)
A:jax.test_util.xs->list(xs)
A:jax.test_util.n->len(xs)
A:jax.test_util.k->min(n, FLAGS.num_generated_cases)
A:jax.test_util.indices->_CACHED_INDICES.get(n)
A:jax.test_util._CACHED_INDICES[n]indices->numpy.random.RandomState(42).permutation(n)
A:jax.test_util.names->super().getTestCaseNames(testCaseClass)
A:jax.test_util.pattern->re.compile(FLAGS.test_targets)
A:jax.test_util.self._rng->numpy.random.RandomState(zlib.adler32(self._testMethodName.encode()))
A:jax.test_util.x->numpy.asarray(x)
A:jax.test_util.y->numpy.asarray(y)
A:jax.test_util.ignore_space_re->re.compile('\\s*\\n\\s*')
A:jax.test_util.expected_clean->re.sub(ignore_space_re, '\n', expected.strip())
A:jax.test_util.what_clean->re.sub(ignore_space_re, '\n', what.strip())
A:jax.test_util.python_ans->fun(*args)
A:jax.test_util.python_shapes->tree_map(lambda x: np.shape(x), python_ans)
A:jax.test_util.np_shapes->tree_map(lambda x: np.shape(np.asarray(x)), python_ans)
A:jax.test_util.cfun->api.jit(wrapped_fun)
A:jax.test_util.monitored_ans->cfun(*args)
A:jax.test_util.compiled_ans->cfun(*args)
A:jax.test_util.lax_ans->lax_op(*args)
A:jax.test_util.numpy_ans->numpy_reference_op(*args)
A:jax.test_util.null->object()
A:jax.test_util.self._value->self._method(obj)
A:jax.test_util.supported->supported_dtypes()
A:jax.test_util.dtypes->_LazyDtypes()
jax.test_util.JaxTestCase(parameterized.TestCase)
jax.test_util.JaxTestCase._CheckAgainstNumpy(self,numpy_reference_op,lax_op,args_maker,check_dtypes=True,tol=None,canonicalize_dtypes=True)
jax.test_util.JaxTestCase._CompileAndCheck(self,fun,args_maker,*,check_dtypes=True,rtol=None,atol=None)
jax.test_util.JaxTestCase.assertAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None,canonicalize_dtypes=True)
jax.test_util.JaxTestCase.assertArraysAllClose(self,x,y,*,check_dtypes=True,atol=None,rtol=None)
jax.test_util.JaxTestCase.assertArraysEqual(self,x,y,*,check_dtypes=True)
jax.test_util.JaxTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax.test_util.JaxTestCase.assertMultiLineStrippedEqual(self,expected,what)
jax.test_util.JaxTestCase.rng(self)
jax.test_util.JaxTestCase.setUp(self)
jax.test_util.JaxTestLoader(absltest.TestLoader)
jax.test_util.JaxTestLoader.getTestCaseNames(self,testCaseClass)
jax.test_util.ScalarShape(object)
jax.test_util.ScalarShape.__len__(self)
jax.test_util._LazyDtypes
jax.test_util._LazyDtypes.all(self)
jax.test_util._LazyDtypes.all_floating(self)
jax.test_util._LazyDtypes.all_inexact(self)
jax.test_util._LazyDtypes.all_integer(self)
jax.test_util._LazyDtypes.all_unsigned(self)
jax.test_util._LazyDtypes.boolean(self)
jax.test_util._LazyDtypes.complex(self)
jax.test_util._LazyDtypes.floating(self)
jax.test_util._LazyDtypes.inexact(self)
jax.test_util._LazyDtypes.integer(self)
jax.test_util._LazyDtypes.numeric(self)
jax.test_util._LazyDtypes.supported(self,dtypes)
jax.test_util._LazyDtypes.unsigned(self)
jax.test_util._NumpyScalar(ScalarShape)
jax.test_util._PythonScalar(ScalarShape)
jax.test_util._assert_numpy_allclose(a,b,atol=None,rtol=None)
jax.test_util._assert_numpy_close(a,b,atol=None,rtol=None)
jax.test_util._cached_property(self,method)
jax.test_util._cached_property.__get__(self,obj,cls)
jax.test_util._cast_to_shape(value,shape,dtype)
jax.test_util._check_dtypes_match(xs,ys)
jax.test_util._dims_of_shape(shape)
jax.test_util._dtype(x)
jax.test_util._format_shape_dtype_string(shape,dtype)
jax.test_util._merge_tolerance(tol,default)
jax.test_util._normalize_tolerance(tol)
jax.test_util._rand_dtype(rand,shape,dtype,scale=1.0,post=lambdax:x)
jax.test_util._safe_subtract(x,y,*,dtype)
jax.test_util.assert_dot_precision(expected_precision,fun,*args)
jax.test_util.cases_from_gens(*gens)
jax.test_util.cases_from_list(xs)
jax.test_util.check_close(xs,ys,atol=None,rtol=None)
jax.test_util.check_eq(xs,ys)
jax.test_util.check_grads(f,args,order,modes=['fwd','rev'],atol=None,rtol=None,eps=None)
jax.test_util.check_jvp(f,f_jvp,args,atol=None,rtol=None,eps=EPS)
jax.test_util.check_raises(thunk,err_type,msg)
jax.test_util.check_raises_regexp(thunk,err_type,pattern)
jax.test_util.check_vjp(f,f_vjp,args,atol=None,rtol=None,eps=EPS)
jax.test_util.count_jit_and_pmap_compiles()
jax.test_util.count_primitive_compiles()
jax.test_util.default_tolerance()
jax.test_util.device_under_test()
jax.test_util.dtype_str(dtype)
jax.test_util.format_shape_dtype_string(shape,dtype)
jax.test_util.format_test_name_suffix(opname,shapes,dtypes)
jax.test_util.if_device_under_test(device_type:Union[str,Sequence[str]],if_true,if_false)
jax.test_util.ignore_warning(**kw)
jax.test_util.inner_prod(xs,ys)
jax.test_util.is_sequence(x)
jax.test_util.iter_eqns(jaxpr)
jax.test_util.join_tolerance(tol1,tol2)
jax.test_util.num_float_bits(dtype)
jax.test_util.numerical_jvp(f,primals,tangents,eps=EPS)
jax.test_util.rand_bool(rng)
jax.test_util.rand_default(rng,scale=3)
jax.test_util.rand_fullrange(rng,standardize_nans=False)
jax.test_util.rand_int(rng,low=0,high=None)
jax.test_util.rand_like(rng,x)
jax.test_util.rand_nonzero(rng)
jax.test_util.rand_not_small(rng,offset=10.0)
jax.test_util.rand_positive(rng)
jax.test_util.rand_small(rng)
jax.test_util.rand_small_positive(rng)
jax.test_util.rand_some_equal(rng)
jax.test_util.rand_some_inf(rng)
jax.test_util.rand_some_inf_and_nan(rng)
jax.test_util.rand_some_nan(rng)
jax.test_util.rand_some_zero(rng)
jax.test_util.rand_uniform(rng,low=0.0,high=1.0)
jax.test_util.rand_unique_int(rng,high=None)
jax.test_util.scalar_mul(xs,a)
jax.test_util.skip_if_unsupported_type(dtype)
jax.test_util.skip_on_devices(*disabled_devices)
jax.test_util.skip_on_flag(flag_name,skip_value)
jax.test_util.supported_dtypes()
jax.test_util.tolerance(dtype,tol=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/dtypes.py----------------------------------------
A:jax.dtypes._bfloat16_dtype->numpy.dtype(bfloat16)
A:jax.dtypes.eps->bfloat16(float.fromhex('0x1p-7'))
A:jax.dtypes.epsneg->bfloat16(float.fromhex('0x1p-8'))
A:jax.dtypes.max->bfloat16(float.fromhex('0x1.FEp127'))
A:jax.dtypes.tiny->bfloat16(float.fromhex('0x1p-126'))
A:jax.dtypes.dtype->python_scalar_dtypes.get(type(x), None)
A:jax.dtypes.typ->dtype(x)
A:jax.dtypes._type_promotion_table->_make_type_promotion_table()
A:jax.dtypes.a->numpy.dtype(a)
A:jax.dtypes.b->numpy.dtype(b)
jax.dtypes._bfloat16_finfo(object)
jax.dtypes._dtype_priority(dtype)
jax.dtypes._issubclass(a,b)
jax.dtypes._make_type_promotion_table()
jax.dtypes.canonicalize_dtype(dtype)
jax.dtypes.coerce_to_array(x)
jax.dtypes.dtype(x)
jax.dtypes.finfo(dtype)
jax.dtypes.is_python_scalar(x)
jax.dtypes.issubdtype(a,b)
jax.dtypes.promote_types(a,b)
jax.dtypes.result_type(*args)
jax.dtypes.scalar_type_of(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/core.py----------------------------------------
A:jax.core.self.constvars->list(constvars)
A:jax.core.self.invars->list(invars)
A:jax.core.self.outvars->list(outvars)
A:jax.core.self.eqns->list(eqns)
A:jax.core.self.literals->list(literals)
A:jax.core.self.in_avals->list(in_avals)
A:jax.core.self.out_avals->list(out_avals)
A:jax.core.self.aval->raise_to_shaped(aval)
A:jax.core.all_vars->itertools.chain.from_iterable((_jaxpr_vars(j) for j in jaxprs))
A:jax.core.counter->itertools.count(start=start)
A:jax.core.dropvar->DropVar()
A:jax.core.self.hash->hash((val.item(), val.dtype))
A:jax.core.top_trace->find_top_trace(args)
A:jax.core.tracers->map(top_trace.full_raise, args)
A:jax.core.out_tracer->find_top_trace(args).process_primitive(self, tracers, kwargs)
A:jax.core.new_params->dict(params)
A:jax.core.in_vals->map(read, eqn.invars)
A:jax.core.(call_jaxpr, params)->extract_call_jaxpr(eqn.primitive, eqn.params)
A:jax.core.ans->max(tracers, key=lambda x: x._trace.level)
A:jax.core.attr->getattr(self.aval, name)
A:jax.core.t->ref(master)
A:jax.core.base->pp('Traced<{}>with<{}>'.format(self.aval, self._trace))
A:jax.core.contents->self._contents()
A:jax.core.aval_property->namedtuple('aval_property', ['fget'])
A:jax.core.aval_method->namedtuple('aval_method', ['fun'])
A:jax.core.new->self.__new__(TraceState)
A:jax.core.AxisEnvFrame->namedtuple('AxisEnvFrame', ['name', 'size'])
A:jax.core.self.trace_stack->TraceStack()
A:jax.core.new.trace_stack->self.trace_stack.copy()
A:jax.core.self.trace_state->TraceState()
A:jax.core.thread_local_state->ThreadLocalState()
A:jax.core.level->stack.next_level()
A:jax.core.master->MasterTrace(0, trace_type)
A:jax.core.sublevel->Sublevel(len(thread_local_state.trace_state.substack))
A:jax.core.bot->Bot()
A:jax.core.abstract_unit->AbstractUnit()
A:jax.core.handler->pytype_aval_mappings.get(typ)
A:jax.core.unit->Unit()
A:jax.core.unitvar->UnitVar()
A:jax.core.fname->getattr(fun, '__name__', fun)
A:jax.core.self.dtype->numpy.dtype(dtypes.canonicalize_dtype(dtype))
A:jax.core._bool_nonzero->partialmethod(_forward_to_value, bool)
A:jax.core._float->concretization_function_error(float, 'Try using `x.astype(float)` instead.')
A:jax.core._int->partialmethod(_forward_to_value, int)
A:jax.core._complex->concretization_function_error(complex, 'Try using `x.astype(complex)` instead.')
A:jax.core._hex->partialmethod(_forward_to_value, hex)
A:jax.core._oct->partialmethod(_forward_to_value, oct)
A:jax.core.self.shape->canonicalize_shape(shape)
A:jax.core.ndim->property(lambda self: len(self.shape))
A:jax.core.size->property(lambda self: prod(self.shape))
A:jax.core.shapestr->','.join(map(str, self.shape))
A:jax.core.abstract_token->AbstractToken()
A:jax.core.todos_list->list(todos)
A:jax.core.outs->primitive.process(top_trace, fun, tracers, params)
A:jax.core.params->dict(params_tuple)
A:jax.core.trace->type(ans._trace)(ans._trace.master, cur_sublevel())
A:jax.core.(outs, cur_todo)->primitive.post_process(trace, outs, params)
A:jax.core.params_tuple->tuple(params.items())
A:jax.core.(fun, env_trace_todo)->process_env_traces(fun, primitive, top_trace and top_trace.level, params_tuple)
A:jax.core.call_p->CallPrimitive('call')
A:jax.core.aval_ref->raise_to_shaped(aval_ref).strip_weak_type()
A:jax.core.jaxpr_str->str(pp_jaxpr_eqn_range(jaxpr, 0, 20))
A:jax.core.msg->'\n\n'.join([msg, 'in equation:', str(pp_eqn(eqn).indent(2)), f'from source: {src}'])
A:jax.core.in_avals->map(read, eqn.invars)
A:jax.core.out_avals->prim.abstract_eval(*in_avals, **params)
A:jax.core.src->source_info_util.summarize(eqn.source_info)
A:jax.core.lhs->pp_vars(eqn.outvars, print_shapes)
A:jax.core.pp_lhs->pp(f'{lhs} =')
A:jax.core.pps->pp_eqns(jaxpr.eqns, source_info=source_info)
A:jax.core.l->max((i + len(s) for x in pps for (i, s) in x.lines), default=None)
A:jax.core.str_outvars->str(tuple(jaxpr.outvars))
A:jax.core.lo->max(lo, 0)
A:jax.core.hi->max(lo, min(hi, len(jaxpr.eqns)))
A:jax.core.eval_trace->MasterTrace(0, EvalTrace)
A:jax.core.stack_str->map('  {}\n'.format, self.stack[::-1])
A:jax.core.top_master->max((x._trace.master for x in xs if isinstance(x, Tracer)), default=None, key=attrgetter('level'))
A:jax.core.out->find_top_trace(args).process_primitive(self, tracers, params)
A:jax.core.frame->AxisEnvFrame(axis_name, size)
A:jax.core.frame_->ThreadLocalState().trace_state.axis_env.pop()
A:jax.core.axis_index_p->Primitive('axis_index')
jax.core.AbstractToken(AbstractValue)
jax.core.AbstractToken.join(self,other)
jax.core.AbstractToken.str_short(self)
jax.core.AbstractUnit(AbstractValue)
jax.core.AbstractUnit._eq(self,self_traced,other)
jax.core.AbstractUnit.join(self,other)
jax.core.AbstractUnit.str_short(self)
jax.core.AbstractValue
jax.core.AbstractValue.__repr__(self)
jax.core.AbstractValue.at_least_vspace(self)
jax.core.AbstractValue.join(self,other)
jax.core.AbstractValue.strip_weak_type(self)->'AbstractValue'
jax.core.Bot(AbstractValue)
jax.core.CallPrimitive(Primitive)
jax.core.CallPrimitive.bind(self,fun,*args,**params)
jax.core.CallPrimitive.post_process(self,trace,out_tracers,params)
jax.core.CallPrimitive.process(self,trace,fun,tracers,params)
jax.core.ConcreteArray(self,val,weak_type=False)
jax.core.ConcreteArray.__eq__(self,other)
jax.core.ConcreteArray.__hash__(self)
jax.core.ConcreteArray.at_least_vspace(self)
jax.core.ConcreteArray.join(self,other)->UnshapedArray
jax.core.ConcreteArray.str_short(self)->str
jax.core.ConcreteArray.strip_weak_type(self)->'ConcreteArray'
jax.core.ConcretizationTypeError(TypeError)
jax.core.DropVar(self)
jax.core.DropVar.__repr__(self)
jax.core.DropVar.aval(self)
jax.core.EvalTrace(Trace)
jax.core.EvalTrace.process_call(self,primitive,f,tracers,params)
jax.core.EvalTrace.process_primitive(self,primitive,tracers,params)
jax.core.EvalTrace.pure(self,x)
jax.core.Jaxpr(self,constvars:Sequence['Var'],invars:Sequence['Var'],outvars:Sequence['Atom'],eqns:Sequence['JaxprEqn'])
jax.core.Jaxpr.__str__(self)
jax.core.JaxprEqn(NamedTuple)
jax.core.JaxprEqn.__repr__(self)
jax.core.JaxprTypeError(TypeError)
jax.core.Literal(self,val)
jax.core.Literal.__eq__(self,other)
jax.core.Literal.__hash__(self)
jax.core.Literal.__repr__(self)
jax.core.Literal.aval(self)
jax.core.MapPrimitive(Primitive)
jax.core.MapPrimitive.bind(self,fun,*args,**params)
jax.core.MapPrimitive.post_process(self,trace,out_tracers,params)
jax.core.MapPrimitive.process(self,trace,fun,tracers,params)
jax.core.MasterTrace(self,level,trace_type)
jax.core.MasterTrace.__eq__(self,other:object)->bool
jax.core.MasterTrace.__hash__(self)->int
jax.core.MasterTrace.__repr__(self)->str
jax.core.Primitive(self,name:str)
jax.core.Primitive.__repr__(self)
jax.core.Primitive.abstract_eval(self,*args,**params)
jax.core.Primitive.bind(self,*args,**kwargs)
jax.core.Primitive.def_abstract_eval(self,abstract_eval)
jax.core.Primitive.def_custom_bind(self,bind)
jax.core.Primitive.def_impl(self,impl)
jax.core.Primitive.impl(self,*args,**params)
jax.core.ShapedArray(self,shape,dtype,weak_type=False)
jax.core.ShapedArray.__eq__(self,other)
jax.core.ShapedArray.__hash__(self)
jax.core.ShapedArray.__len__(self)
jax.core.ShapedArray._len(self,ignored_tracer)
jax.core.ShapedArray.at_least_vspace(self)
jax.core.ShapedArray.join(self,other)
jax.core.ShapedArray.str_short(self)
jax.core.ShapedArray.strip_weak_type(self)
jax.core.Sublevel(int)
jax.core.ThreadLocalState(self)
jax.core.Trace(self,master:'MasterTrace',sublevel:'Sublevel')
jax.core.Trace.__repr__(self)
jax.core.Trace.full_raise(self,val)->'Tracer'
jax.core.Trace.lift(self,tracer)
jax.core.Trace.process_call(self,call_primitive,f,tracers,params)
jax.core.Trace.process_custom_jvp_call(self,primitive,fun,jvp,tracers)
jax.core.Trace.process_custom_vjp_call(self,primitive,fun,fwd,bwd,tracers,out_trees)
jax.core.Trace.process_map(self,call_primitive,f,tracers,params)
jax.core.Trace.process_primitive(self,primitive,tracers,params)
jax.core.Trace.pure(self,val)
jax.core.Trace.sublift(self,tracer)
jax.core.TraceStack(self)
jax.core.TraceStack.__repr__(self)->str
jax.core.TraceStack.copy(self)
jax.core.TraceStack.next_level(self,bottom:bool)->int
jax.core.TraceStack.pop(self,bottom:bool)->None
jax.core.TraceStack.push(self,master_trace:MasterTrace,bottom:bool)->None
jax.core.TraceState(self)
jax.core.TraceState.copy(self)
jax.core.Tracer(self,trace:Trace)
jax.core.Tracer.__abs__(self)
jax.core.Tracer.__add__(self,other)
jax.core.Tracer.__and__(self,other)
jax.core.Tracer.__array__(self,*args,**kw)
jax.core.Tracer.__bool__(self)
jax.core.Tracer.__complex__(self)
jax.core.Tracer.__copy__(self)
jax.core.Tracer.__deepcopy__(self,unused_memo)
jax.core.Tracer.__div__(self,other)
jax.core.Tracer.__divmod__(self,other)
jax.core.Tracer.__eq__(self,other)
jax.core.Tracer.__float__(self)
jax.core.Tracer.__floordiv__(self,other)
jax.core.Tracer.__ge__(self,other)
jax.core.Tracer.__getattr__(self,name)
jax.core.Tracer.__getitem__(self,idx)
jax.core.Tracer.__gt__(self,other)
jax.core.Tracer.__hex__(self)
jax.core.Tracer.__int__(self)
jax.core.Tracer.__invert__(self)
jax.core.Tracer.__iter__(self)
jax.core.Tracer.__le__(self,other)
jax.core.Tracer.__len__(self)
jax.core.Tracer.__long__(self)
jax.core.Tracer.__lshift__(self,other)
jax.core.Tracer.__lt__(self,other)
jax.core.Tracer.__matmul__(self,other)
jax.core.Tracer.__mod__(self,other)
jax.core.Tracer.__mul__(self,other)
jax.core.Tracer.__ne__(self,other)
jax.core.Tracer.__neg__(self)
jax.core.Tracer.__nonzero__(self)
jax.core.Tracer.__oct__(self)
jax.core.Tracer.__or__(self,other)
jax.core.Tracer.__pos__(self)
jax.core.Tracer.__pow__(self,other)
jax.core.Tracer.__radd__(self,other)
jax.core.Tracer.__rand__(self,other)
jax.core.Tracer.__rdiv__(self,other)
jax.core.Tracer.__rdivmod__(self,other)
jax.core.Tracer.__repr__(self)
jax.core.Tracer.__rfloordiv__(self,other)
jax.core.Tracer.__rmatmul__(self,other)
jax.core.Tracer.__rmod__(self,other)
jax.core.Tracer.__rmul__(self,other)
jax.core.Tracer.__ror__(self,other)
jax.core.Tracer.__rpow__(self,other)
jax.core.Tracer.__rshift__(self,other)
jax.core.Tracer.__rsub__(self,other)
jax.core.Tracer.__rtruediv__(self,other)
jax.core.Tracer.__rxor__(self,other)
jax.core.Tracer.__setitem__(self,idx,val)
jax.core.Tracer.__sub__(self,other)
jax.core.Tracer.__truediv__(self,other)
jax.core.Tracer.__xor__(self,other)
jax.core.Tracer._contents(self)
jax.core.Tracer.aval(self)
jax.core.TypedJaxpr(self,jaxpr:Jaxpr,literals:Sequence,in_avals:Sequence['AbstractValue'],out_avals:Sequence['AbstractValue'])
jax.core.TypedJaxpr.__iter__(self)
jax.core.TypedJaxpr.__str__(self)
jax.core.UnexpectedTracerError(Exception)
jax.core.Unit
jax.core.Unit.__repr__(self)
jax.core.UnitVar(self)
jax.core.UnitVar.__repr__(self)
jax.core.UnitVar.aval(self)
jax.core.UnshapedArray(self,dtype,weak_type=False)
jax.core.UnshapedArray.__eq__(self,other)
jax.core.UnshapedArray.__hash__(self)
jax.core.UnshapedArray.__ne__(self,other)
jax.core.UnshapedArray.__repr__(self)
jax.core.UnshapedArray.at_least_vspace(self)->AbstractValue
jax.core.UnshapedArray.join(self,other)
jax.core.UnshapedArray.shape(self)
jax.core.UnshapedArray.str_short(self)->str
jax.core.UnshapedArray.strip_weak_type(self)->'UnshapedArray'
jax.core.Var(self,count:int,suffix:str,aval:'AbstractValue')
jax.core.Var.__lt__(self,other)
jax.core.Var.__repr__(self)
jax.core._canonicalize_dimension(dim)
jax.core._check_jaxpr(jaxpr:Jaxpr,in_avals:Sequence[AbstractValue])
jax.core._forward_to_value(self,fun,ignored_tracer,*args)
jax.core._jaxpr_vars(jaxpr)
jax.core.apply_todos(todos,outs)
jax.core.call_bind(primitive:Union['CallPrimitive','MapPrimitive'],fun:lu.WrappedFun,*args,**params)
jax.core.call_impl(f:lu.WrappedFun,*args,**params)
jax.core.canonicalize_shape(shape)
jax.core.check_call(prim,in_avals,params)
jax.core.check_eqn(prim,in_avals,params)
jax.core.check_jaxpr(jaxpr:Jaxpr)
jax.core.check_map(prim,in_avals,params)
jax.core.check_valid_jaxtype(x)
jax.core.concrete_aval(x)
jax.core.concrete_or_error(force:Any,val:Any,context='')
jax.core.concretization_function_error(fun,context='')
jax.core.cur_sublevel()->Sublevel
jax.core.escaped_tracer_error(detail)
jax.core.eval_jaxpr(jaxpr:Jaxpr,consts,*args)
jax.core.extract_call_jaxpr(primitive:Primitive,params:Dict[str,Any])->Tuple[Optional[Jaxpr], Dict[str, Any]]
jax.core.find_top_trace(xs)->Optional[Trace]
jax.core.full_lower(val)
jax.core.gensym(jaxprs:Optional[Sequence[Jaxpr]]=None,suffix:str='')->Callable[['AbstractValue'], Var]
jax.core.get_aval(x)
jax.core.initial_style_staging()
jax.core.jaxpr_as_fun(typed_jaxpr:TypedJaxpr,*args)
jax.core.jaxprs_in_params(params)->Iterator[Jaxpr]
jax.core.lattice_join(x:Optional[AbstractValue],y:Optional[AbstractValue])->AbstractValue
jax.core.mapped_aval(size:int,aval:AbstractValue)->AbstractValue
jax.core.new_jaxpr_eqn(invars,outvars,primitive,params,source_info=None)
jax.core.new_master(trace_type:Type[Trace],bottom=False)->Generator[MasterTrace, None, None]
jax.core.new_sublevel()->Generator[None, None, None]
jax.core.omnistaging_enabler()->None
jax.core.pp_eqn(eqn:JaxprEqn,print_shapes:bool=False)->PrettyPrint
jax.core.pp_eqn_compact(primitive_name:str,params:Dict)->PrettyPrint
jax.core.pp_eqns(eqns:Sequence[JaxprEqn],source_info:bool=False)->Sequence[PrettyPrint]
jax.core.pp_jaxpr(jaxpr:Jaxpr,source_info:bool=False)->PrettyPrint
jax.core.pp_jaxpr_eqn_range(jaxpr:Jaxpr,lo:int,hi:int,source_info:bool=False)->PrettyPrint
jax.core.pp_jaxprs(jaxprs)->PrettyPrint
jax.core.pp_kv_pair(k,v)
jax.core.pp_kv_pairs(kv_pairs)
jax.core.pp_vars(vs:Sequence[Any],print_shapes:bool=False)->str
jax.core.process_env_traces(primitive:Union['CallPrimitive','MapPrimitive'],level:int,params_tuple:tuple,*args)
jax.core.raise_concretization_error(val,context='')
jax.core.raise_to_shaped(aval:AbstractValue,weak_type=False)
jax.core.reset_trace_state()->bool
jax.core.skipping_checks()
jax.core.subjaxprs(jaxpr:Jaxpr)->Iterator[Jaxpr]
jax.core.typecheck(aval:AbstractValue,x)->bool
jax.core.typecheck_assert(pred,msg)
jax.core.typecompat(aval_ref:AbstractValue,aval:AbstractValue)->bool
jax.core.typematch(aval1:UnshapedArray,aval2:UnshapedArray)->bool
jax.core.unmapped_aval(size:int,aval:AbstractValue)->AbstractValue
jax.core.valid_jaxtype(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/ad_util.py----------------------------------------
A:jax.ad_util.add_jaxvals_p->Primitive('add_any')
A:jax.ad_util.zeros_like_p->Primitive('zeros_like')
A:jax.ad_util.stop_gradient_p->Primitive('stop_gradient')
jax.ad_util.Zero(self,aval)
jax.ad_util.Zero.__repr__(self)
jax.ad_util.Zero.from_value(val)
jax.ad_util._stop_gradient_impl(x)
jax.ad_util.add_abstract(xs,ys)
jax.ad_util.add_impl(xs,ys)
jax.ad_util.add_jaxvals(x,y)
jax.ad_util.zeros_like_aval(aval)
jax.ad_util.zeros_like_impl(example)
jax.ad_util.zeros_like_jaxval(val)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/config.py----------------------------------------
A:jax.config.val->val.lower().lower()
A:jax.config.self.FLAGS->NameSpace(self.read)
A:jax.config.config->Config()
jax.config.Config(self)
jax.config.Config.DEFINE_bool(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_enum(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_integer(self,name,default,*args,**kwargs)
jax.config.Config.DEFINE_string(self,name,default,*args,**kwargs)
jax.config.Config.add_option(self,name,default,opt_type,meta_args,meta_kwargs)
jax.config.Config.check_exists(self,name)
jax.config.Config.complete_absl_config(self,absl_flags)
jax.config.Config.config_with_absl(self)
jax.config.Config.enable_omnistaging(self)
jax.config.Config.parse_flags_with_absl(self)
jax.config.Config.read(self,name)
jax.config.Config.update(self,name,val)
jax.config.NameSpace(self,getter)
jax.config.NameSpace.__getattr__(self,name)
jax.config.bool_env(varname:str,default:bool)->bool


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/custom_derivatives.py----------------------------------------
A:jax.custom_derivatives.ba->inspect.signature(fun).bind(*args, **kwargs)
A:jax.custom_derivatives.extra_args->tuple([arg.val for arg in extra_args])
A:jax.custom_derivatives.saved_state->core.thread_local_state.trace_state.copy()
A:jax.custom_derivatives.(jaxpr, out_pvals, consts)->interpreters.partial_eval.trace_to_jaxpr(fun, in_pvals, instantiate=True, bottom=True, stage_out=False)
A:jax.custom_derivatives.out_avals->map(raise_to_shaped, unzip2(out_pvals)[0])
A:jax.custom_derivatives.typed_jaxpr->core.TypedJaxpr(jaxpr, consts, in_avals, out_avals)
A:jax.custom_derivatives.primal_out->self(*primals)
A:jax.custom_derivatives.zeros->_zeros_like_pytree(primal_out)
A:jax.custom_derivatives.tangent_out->tree_multimap(_sum_tangents, primal_out, *all_tangents_out)
A:jax.custom_derivatives.args->map(core.full_lower, args)
A:jax.custom_derivatives.(f_, dyn_args)->argnums_partial(lu.wrap_init(self.fun), dyn_argnums, args)
A:jax.custom_derivatives.jvp->lu.wrap_init(self.jvp)
A:jax.custom_derivatives.(args_flat, in_tree)->tree_flatten(dyn_args)
A:jax.custom_derivatives.(flat_fun, out_tree1)->flatten_fun_nokwargs(f_, in_tree)
A:jax.custom_derivatives.(flat_jvp, out_tree2)->_flatten_jvp(jvp, in_tree)
A:jax.custom_derivatives.out_flat->custom_vjp_call(flat_fun, flat_fwd, flat_bwd, *args_flat, out_trees=out_trees)
A:jax.custom_derivatives.out_tree->out_tree()
A:jax.custom_derivatives.(_, out_tree)->lu.merge_linear_aux(out_tree1, out_tree2)
A:jax.custom_derivatives.(primals_in, tangents_in)->split_list(args, [len(args) // 2])
A:jax.custom_derivatives.py_primals->tree_unflatten(in_tree, primals_in)
A:jax.custom_derivatives.py_tangents->tree_unflatten(in_tree, tangents_in)
A:jax.custom_derivatives.(primals_out, out_tree)->tree_flatten(py_primals_out)
A:jax.custom_derivatives.(tangents_out, out_tree2)->tree_flatten(py_tangents_out)
A:jax.custom_derivatives.top_trace->core.find_top_trace(args)
A:jax.custom_derivatives.(fun, env_trace_todo1)->core.process_env_traces(fun, self, top_trace and top_trace.level, ())
A:jax.custom_derivatives.(jvp, env_trace_todo2)->core.process_env_traces(jvp, self, top_trace and top_trace.level, ())
A:jax.custom_derivatives.outs->core.find_top_trace(args).process_custom_jvp_call(self, fun, jvp, tracers)
A:jax.custom_derivatives.tracers->map(top_trace.full_raise, args)
A:jax.custom_derivatives.(_, env_trace_todo)->lu.merge_linear_aux(env_trace_todo1, env_trace_todo2)
A:jax.custom_derivatives.custom_jvp_call_p->CustomJVPCallPrimitive('custom_jvp_call')
A:jax.custom_derivatives.fun_jaxpr->_initial_style_jaxpr(fun, in_avals)
A:jax.custom_derivatives.jvp_jaxpr_thunk->_memoize(lambda : _initial_style_jaxpr(jvp, in_avals * 2))
A:jax.custom_derivatives.custom_jvp_call_jaxpr_p->core.Primitive('custom_jvp_call_jaxpr')
A:jax.custom_derivatives.jvp_jaxpr->jvp_jaxpr_thunk()
A:jax.custom_derivatives.tangents->map(ad.instantiate_zeros, tangents)
A:jax.custom_derivatives.num_out->len(fun_jaxpr.out_avals)
A:jax.custom_derivatives.(batched_fun_jaxpr, out_batched)->interpreters.batching.batch_jaxpr(fun_jaxpr, size, in_batched, False)
A:jax.custom_derivatives.(_, all_batched)->interpreters.batching.batch_jaxpr(jvp_jaxpr, size, in_batched * 2, False)
A:jax.custom_derivatives.(primals_batched, tangents_batched)->split_list(all_batched, [num_out])
A:jax.custom_derivatives.out_batched->map(op.or_, primals_batched, tangents_batched)
A:jax.custom_derivatives.(batched_jvp_jaxpr, _)->interpreters.batching.batch_jaxpr(jvp_jaxpr, size, in_batched * 2, out_batched * 2)
A:jax.custom_derivatives.batched_outs->core.Primitive('custom_vjp_call_jaxpr').bind(*args, fun_jaxpr=batched_fun_jaxpr, fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd, out_trees=out_trees)
A:jax.custom_derivatives.xla.initial_style_translations[custom_jvp_call_jaxpr_p]->interpreters.xla.lower_fun_initial_style(_custom_jvp_call_jaxpr_impl)
A:jax.custom_derivatives.(fwd, _)->argnums_partial(lu.wrap_init(self.fwd), dyn_argnums, args)
A:jax.custom_derivatives.bwd->_add_args(lu.wrap_init(self.bwd), static_args, left=True)
A:jax.custom_derivatives.(flat_fun, out_tree)->flatten_fun_nokwargs(f_, in_tree)
A:jax.custom_derivatives.(flat_fwd, out_trees)->_flatten_fwd(fwd, in_tree)
A:jax.custom_derivatives.flat_bwd->_flatten_bwd(bwd, in_tree, out_trees)
A:jax.custom_derivatives.(fst, aux)->lu.merge_linear_aux(out_tree, out_trees)
A:jax.custom_derivatives.py_args->tree_unflatten(in_tree, args)
A:jax.custom_derivatives.(out, out_tree)->tree_flatten(py_outs)
A:jax.custom_derivatives.(res, res_tree)->tree_flatten(res)
A:jax.custom_derivatives.(out_tree, res_tree)->out_trees()
A:jax.custom_derivatives.(res, cts_out)->split_list(args, [res_tree.num_leaves])
A:jax.custom_derivatives.py_res->tree_unflatten(res_tree, res)
A:jax.custom_derivatives.py_cts_out->tree_unflatten(out_tree, cts_out)
A:jax.custom_derivatives.(cts_in, in_tree2)->tree_flatten(py_cts_in)
A:jax.custom_derivatives.custom_vjp_call_p->CustomVJPCallPrimitive('custom_vjp_call')
A:jax.custom_derivatives.fwd_jaxpr_thunk->_memoize(lambda : _initial_style_jaxpr(fwd, in_avals))
A:jax.custom_derivatives.custom_vjp_call_jaxpr_p->core.Primitive('custom_vjp_call_jaxpr')
A:jax.custom_derivatives.fwd_jaxpr->fwd_jaxpr_thunk()
A:jax.custom_derivatives.res_and_primals_out->core.jaxpr_as_fun(fwd_jaxpr)(*primals)
A:jax.custom_derivatives.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax.custom_derivatives.tangents_out->interpreters.ad.custom_lin_p.bind(*res, *tangents, num_res=res_tree.num_leaves, bwd=bwd, avals_out=avals_out)
A:jax.custom_derivatives.(batched_fwd_jaxpr, out_batched)->interpreters.batching.batch_jaxpr(fwd_jaxpr, size, in_batched, False)
A:jax.custom_derivatives.batched_bwd->interpreters.batching.batch_fun(bwd, fwd_out_dims, fwd_in_dims, sum_match=True)
A:jax.custom_derivatives.xla.initial_style_translations[custom_vjp_call_jaxpr_p]->interpreters.xla.lower_fun_initial_style(_custom_vjp_call_jaxpr_impl)
A:jax.custom_derivatives.(jaxpr, out_avals, consts)->interpreters.partial_eval.trace_to_jaxpr_dynamic(fun, in_avals)
jax.custom_derivatives.CustomJVPCallPrimitive(core.CallPrimitive)
jax.custom_derivatives.CustomJVPCallPrimitive.bind(self,fun,jvp,*args)
jax.custom_derivatives.CustomJVPCallPrimitive.impl(self,fun,_,*args)
jax.custom_derivatives.CustomVJPCallPrimitive(core.CallPrimitive)
jax.custom_derivatives.CustomVJPCallPrimitive.bind(self,fun,fwd,bwd,*args,out_trees)
jax.custom_derivatives.CustomVJPCallPrimitive.impl(self,fun,fwd,bwd,*args,out_trees)
jax.custom_derivatives._add_args(f,extra_args,left)
jax.custom_derivatives._add_args_(extra_args,left,*args,**kwargs)
jax.custom_derivatives._custom_jvp_call_jaxpr_abstract_eval(*_,fun_jaxpr,**__)
jax.custom_derivatives._custom_jvp_call_jaxpr_impl(*args,fun_jaxpr,**_)
jax.custom_derivatives._custom_jvp_call_jaxpr_jvp(primals,tangents,*,fun_jaxpr,jvp_jaxpr_thunk)
jax.custom_derivatives._custom_jvp_call_jaxpr_transpose(cts,*args,fun_jaxpr,jvp_jaxpr_thunk)
jax.custom_derivatives._custom_jvp_call_jaxpr_vmap(args,in_dims,*,fun_jaxpr,jvp_jaxpr_thunk)
jax.custom_derivatives._custom_vjp_call_jaxpr_abstract_eval(*_,fun_jaxpr,**__)
jax.custom_derivatives._custom_vjp_call_jaxpr_impl(*args,fun_jaxpr,**_)
jax.custom_derivatives._custom_vjp_call_jaxpr_jvp(primals,tangents,*,fun_jaxpr,fwd_jaxpr_thunk,bwd,out_trees)
jax.custom_derivatives._custom_vjp_call_jaxpr_vmap(args,in_dims,*,fun_jaxpr,fwd_jaxpr_thunk,bwd,out_trees)
jax.custom_derivatives._flatten_bwd(in_tree,out_trees,*args)
jax.custom_derivatives._flatten_fwd(in_tree,*args)
jax.custom_derivatives._flatten_jvp(in_tree,*args)
jax.custom_derivatives._initial_style_jaxpr(fun,in_avals)
jax.custom_derivatives._initial_style_staging()->bool
jax.custom_derivatives._memoize(thunk)
jax.custom_derivatives._resolve_kwargs(fun,args,kwargs)
jax.custom_derivatives._stop_gradient(x)
jax.custom_derivatives._sum_tangents(_,x,*xs)
jax.custom_derivatives._zeros_like_pytree(x)
jax.custom_derivatives.custom_jvp(self,fun,nondiff_argnums=())
jax.custom_derivatives.custom_jvp.defjvp(self,jvp)
jax.custom_derivatives.custom_jvp.defjvps(self,*jvps)
jax.custom_derivatives.custom_jvp_call_jaxpr(fun,jvp,*args)
jax.custom_derivatives.custom_vjp(self,fun,nondiff_argnums=())
jax.custom_derivatives.custom_vjp.defvjp(self,fwd,bwd)
jax.custom_derivatives.custom_vjp_call_jaxpr(fun,fwd,bwd,*args,out_trees)
jax.custom_derivatives.omnistaging_enabler()->None


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/dlpack.py----------------------------------------
A:jax.dlpack.client->getattr(backend, 'client', backend)
A:jax.dlpack.buf->lib.xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, client)
A:jax.dlpack.xla_shape->lib.xla_client._xla.dlpack_managed_tensor_to_buffer(dlpack, client).shape()
A:jax.dlpack.aval->core.ShapedArray(xla_shape.dimensions(), xla_shape.numpy_dtype())
jax.dlpack.from_dlpack(dlpack,backend=None)
jax.dlpack.to_dlpack(x:xla.DeviceArray)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/api.py----------------------------------------
A:jax.api._thread_local_state->_ThreadLocalState()
A:jax.api.static_argnums->_ensure_tuple(static_argnums)
A:jax.api.donate_argnums->rebase_donate_argnums(donate_argnums, static_argnums)
A:jax.api.f->lu.wrap_init(fun)
A:jax.api.(f, dyn_args)->argnums_partial(f, dyn_argnums, args)
A:jax.api.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax.api.donated_invars->donation_vector(donate_tuple, dyn_args, kwargs)
A:jax.api.(flat_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax.api.out->interpreters.partial_eval.abstract_eval_fun(wrapped_fun.call_wrapped, *map(abstractify, args_flat))
A:jax.api.fun_name->getattr(fun, '__name__', 'unknown')
A:jax.api.(names, sizes)->zip(*axis_env)
A:jax.api.wrapped->lu.wrap_init(fun)
A:jax.api.(wrapped, _)->argnums_partial(wrapped, dyn_argnums, args)
A:jax.api.(jax_args, in_tree)->tree_flatten((args, kwargs))
A:jax.api.(jaxtree_fun, out_tree)->flatten_fun(wrapped, in_tree)
A:jax.api.avals->map(partial(ShapedArray, dtype=np.float32), in_shapes)
A:jax.api.(jaxpr, out_avals, consts)->interpreters.partial_eval.trace_to_jaxpr_dynamic(jaxtree_fun, in_avals)
A:jax.api.(jaxpr, out_pvals, consts)->interpreters.partial_eval.trace_to_jaxpr(jaxtree_fun, in_pvals, instantiate=True, stage_out=True)
A:jax.api.jaxpr->interpreters.xla.apply_outfeed_rewriter(jaxpr)
A:jax.api.axis_env_->make_axis_env(xla.jaxpr_replicas(jaxpr))
A:jax.api.c->lib.xla_bridge.make_computation_builder('xla_computation_{}'.format(fun_name))
A:jax.api.xla_consts->map(partial(xb.constant, c), consts)
A:jax.api.xla_args->interpreters.xla._xla_callable_args(c, avals, tuple_args)
A:jax.api.outs->self.prim.bind(*it.chain(consts, args_flat), jaxpr=jaxpr, in_tree=in_tree, out_tree=out_tree(), num_consts=len(consts))
A:jax.api.built->lib.xla_bridge.make_computation_builder('xla_computation_{}'.format(fun_name)).build(xc.ops.Tuple(c, outs))
A:jax.api.out_shape->tree_unflatten(out_tree(), out_shapes_flat)
A:jax.api.value_and_grad_f->value_and_grad(fun, argnums, has_aux=has_aux, holomorphic=holomorphic)
A:jax.api.(_, g)->value_and_grad_f(*args, **kwargs)
A:jax.api.((_, aux), g)->value_and_grad_f(*args, **kwargs)
A:jax.api.(f_partial, dyn_args)->argnums_partial(f, argnums, args)
A:jax.api.(ans, vjp_py)->_vjp(f_partial, *dyn_args)
A:jax.api.(ans, vjp_py, aux)->_vjp(f_partial, *dyn_args, has_aux=True)
A:jax.api.dtype->dtypes.result_type(*leaves)
A:jax.api.g->vjp_py(np.ones((), dtype=dtype))
A:jax.api.aval->core.get_aval(x)
A:jax.api._check_input_dtype_grad->partial(_check_input_dtype_revderiv, 'grad')
A:jax.api._check_output_dtype_grad->partial(_check_output_dtype_revderiv, 'grad')
A:jax.api.pushfwd->partial(_jvp, f_partial, dyn_args)
A:jax.api.(y, jac)->vmap(pushfwd, out_axes=(None, batching.last))(_std_basis(dyn_args))
A:jax.api.(y, pullback)->_vjp(f_partial, *dyn_args)
A:jax.api.jac->tree_map(partial(_unravel_array_into_pytree, y, 0), jac)
A:jax.api._check_input_dtype_jacrev->partial(_check_input_dtype_revderiv, 'jacrev')
A:jax.api._check_output_dtype_jacrev->partial(_check_output_dtype_revderiv, 'jacrev')
A:jax.api.(leaves, _)->tree_flatten(pytree)
A:jax.api.ndim->property(lambda self: len(self.shape))
A:jax.api.flat_basis->numpy.eye(ndim, dtype=dtype)
A:jax.api.(leaves, treedef)->tree_flatten(pytree)
A:jax.api.parts->_split(arr, np.cumsum(map(np.size, leaves[:-1])), axis)
A:jax.api.in_axes->tuple(in_axes)
A:jax.api.in_axes_flat->flatten_axes('soft_pmap in_axes', in_tree, (in_axes, 0))
A:jax.api._->_mapped_axis_size(in_tree, args_flat, in_axes_flat, 'vmap')
A:jax.api.out_flat->interpreters.invertible_ad.invertible_call(flat_fun, *args_flat, name=flat_fun.__name__, concrete=concrete)
A:jax.api.sizes->tree_unflatten(tree, sizes)
A:jax.api.static_broadcasted_tuple->_ensure_tuple(static_broadcasted_argnums)
A:jax.api.donate_tuple->rebase_donate_argnums(_ensure_tuple(donate_argnums), static_broadcasted_tuple)
A:jax.api.dyn_in_axes->tuple((in_axes[i] for i in dyn_argnums))
A:jax.api.(args, in_tree)->tree_flatten(py_args)
A:jax.api.local_axis_size->_mapped_axis_size(in_tree, args, in_axes_flat, 'pmap')
A:jax.api.self.obj->id(obj)
A:jax.api.self.hash->hash(obj)
A:jax.api.mapped_invars->tuple((axis is not None for axis in in_axes_flat))
A:jax.api.axis_size->_mapped_axis_size(in_tree, args_flat, (0,) * len(args_flat), 'papply')
A:jax.api.axis_name->_TempAxisName(fun)
A:jax.api.unique_ids->interpreters.masking.UniqueIds()
A:jax.api.(in_specs, in_shapes_tree)->tree_flatten(in_shapes)
A:jax.api.in_specs->map(partial(masking.remap_ids, unique_ids), in_specs)
A:jax.api.(out_specs, out_spec_tree)->tree_flatten(out_shape)
A:jax.api.out_specs->map(masking.parse_spec, out_specs)
A:jax.api.in_shapes->map(masking.parse_spec, in_shapes)
A:jax.api.padded_env->interpreters.masking.bind_shapes(in_shapes, [x.shape for x in args_flat])
A:jax.api.(flat_fun, out_tree_thunk)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.api.(outs, out_shapes)->interpreters.masking.mask_fun(flat_fun, logical_env, padded_env, args_flat, in_shapes)
A:jax.api.out_tree->out_tree()
A:jax.api.(in_shapes, in_tree)->tree_flatten(in_shapes)
A:jax.api.(ps_flat, tree_def)->tree_flatten(primals)
A:jax.api.(ts_flat, tree_def_2)->tree_flatten(tangents)
A:jax.api.(out_primals, out_tangents)->interpreters.ad.jvp(flat_fun).call_wrapped(ps_flat, ts_flat)
A:jax.api.(primals_flat, in_tree)->tree_flatten(primals)
A:jax.api.(out_primals, out_pvals, jaxpr, consts)->interpreters.ad.linearize(jaxtree_fun, *primals_flat)
A:jax.api.out_primal_py->tree_unflatten(out_tree, out_primal)
A:jax.api.primal_avals->list(map(core.get_aval, primals_flat))
A:jax.api.lifted_jvp->partial(_lift_linearized, jaxpr, primal_avals, consts, (in_tree, out_tree), out_pvals)
A:jax.api.tangent_avals->list(map(core.get_aval, tangents))
A:jax.api.tangents_out->eval_jaxpr(jaxpr, consts, *tangents)
A:jax.api.ans->fun(*primals)
A:jax.api.has_aux->kwargs.pop('has_aux', False)
A:jax.api.(out_primal, out_vjp)->interpreters.ad.vjp(flat_fun, primals_flat)
A:jax.api.(flat_fun, out_aux_trees)->flatten_fun_nokwargs2(fun, in_tree)
A:jax.api.(out_primal, out_vjp, aux)->interpreters.ad.vjp(flat_fun, primals_flat, has_aux=True)
A:jax.api.(out_tree, aux_tree)->out_aux_trees()
A:jax.api.vjp_py->Partial(partial(_vjp_pullback_wrapper, [_dtype(x) for x in out_primal], (out_tree, in_tree)), out_vjp)
A:jax.api.in_avals->map(xla.abstractify, jax_args)
A:jax.api.out_avals->map(raise_to_shaped, unzip2(out_pvals)[0])
A:jax.api.typed_jaxpr->core.TypedJaxpr(jaxpr, consts, in_avals, out_avals)
A:jax.api.jaxpr_maker.__name__->'make_jaxpr({})'.format(jaxpr_maker.__name__)
A:jax.api.self.dtype->numpy.dtype(dtype)
A:jax.api.size->property(lambda self: np.prod(self.shape))
A:jax.api.(wrapped_fun, out_tree)->flatten_fun(lu.wrap_init(fun), in_tree)
A:jax.api.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(flat_fun, in_pvals, instantiate=True)
A:jax.api.name->getattr(fun, '__name__', '<unnamed custom_transforms primitive>')
A:jax.api.fun_p->core.Primitive(name)
A:jax.api.(consts, args)->split_list(args, [params['num_consts']])
A:jax.api.(batched, out_dims)->interpreters.batching.batch_fun2(lu.wrap_init(fun_impl, params), dims)
A:jax.api.(_, args_flat)->split_list(primals, [num_consts])
A:jax.api.(consts_dot, args_dot_flat)->split_list(tangents, [num_consts])
A:jax.api.args_dot_flat->map(ad.instantiate_zeros, args_dot_flat)
A:jax.api.args->tree_unflatten(params['in_tree'], args_flat)
A:jax.api.args_dot->tree_unflatten(in_tree, args_dot_flat)
A:jax.api.(out, out_dot)->custom_jvp(args, args_dot)
A:jax.api.(out_flat, out_tree)->tree_flatten(out)
A:jax.api.(out_dot_flat, out_tree2)->tree_flatten(out_dot)
A:jax.api.(consts, args_flat)->split_list(consts_and_args, [num_consts])
A:jax.api.(out, vjp)->custom_vjp(*args)
A:jax.api.msg->"Output of the `vjp`: {} doesn't match the structure of args of `fun`: {}\n{}\nvs\n{}\n".format(vjp, fun, in_tree2, in_tree)
A:jax.api.cts->tree_unflatten(out_tree, cts_flat)
A:jax.api.(args_cts_flat, in_tree2)->tree_flatten(vjp(cts))
A:jax.api.(ans, _)->fun(*args, **kwargs)
A:jax.api.primal_fun->custom_transforms(primal_fun)
jax.ShapeDtypeStruct(self,shape,dtype)
jax.api.CustomTransformsFunction(self,fun,prim)
jax.api.CustomTransformsFunction.__repr__(self)
jax.api.ShapeDtypeStruct(self,shape,dtype)
jax.api.ShapeDtypeStruct.__eq__(self,other)
jax.api.ShapeDtypeStruct.__hash__(self)
jax.api.ShapeDtypeStruct.__len__(self)
jax.api.ShapeDtypeStruct.__repr__(self)
jax.api._TempAxisName(self,obj)
jax.api._TempAxisName.__eq__(self,other)
jax.api._TempAxisName.__hash__(self)
jax.api._TempAxisName.__repr__(self)
jax.api._ThreadLocalState(self)
jax.api._check_arg(arg)
jax.api._check_callable(fun)
jax.api._check_custom_transforms_type(name,fun)
jax.api._check_inexact_input_vjp(x)
jax.api._check_input_dtype_jacfwd(holomorphic,x)
jax.api._check_input_dtype_revderiv(name,holomorphic,x)
jax.api._check_output_dtype_jacfwd(holomorphic,x)
jax.api._check_output_dtype_revderiv(name,holomorphic,x)
jax.api._check_scalar(x)
jax.api._device_get(x)
jax.api._dtype(x)
jax.api._ensure_tuple(x:Union[int,Iterable[int]])->Tuple[int, ...]
jax.api._get_axis_size(name:str,i:int,shape:Tuple[int,...],axis:int)
jax.api._jit_is_disabled()
jax.api._jvp(fun:lu.WrappedFun,primals,tangents)
jax.api._lift_linearized(jaxpr,primal_avals,consts,io_tree,out_pvals,*py_args)
jax.api._mapped_axis_size(tree,vals,dims,name)
jax.api._papply(fun)
jax.api._split(x,indices,axis)
jax.api._std_basis(pytree)
jax.api._unravel_array_into_pytree(pytree,axis,arr)
jax.api._valid_jaxtype(arg)
jax.api._vjp(fun:lu.WrappedFun,*primals,**kwargs)
jax.api._vjp_pullback_wrapper(cotangent_dtypes,io_tree,fun,py_args)
jax.api.checkpoint(fun:Callable,concrete:bool=False)->Callable
jax.api.custom_gradient(fun)
jax.api.custom_transforms(fun)
jax.api.defjvp(fun,*jvprules)
jax.api.defjvp_all(fun,custom_jvp)
jax.api.defvjp(fun,*vjprules)
jax.api.defvjp_all(fun,custom_vjp)
jax.api.device_get(x)
jax.api.device_put(x,device:Optional[xc.Device]=None)
jax.api.disable_jit()
jax.api.eval_shape(fun:Callable,*args,**kwargs)
jax.api.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.api.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.invertible(fun:Callable,concrete:bool=False)->Callable
jax.api.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.api.jit(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),device=None,backend:Optional[str]=None,donate_argnums:Union[int,Iterable[int]]=())->Callable
jax.api.jvp(fun:Callable,primals,tangents)->Tuple[Any, Any]
jax.api.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax.api.make_jaxpr(fun:Callable,static_argnums:Union[int,Iterable[int]]=())->Callable[..., core.TypedJaxpr]
jax.api.mask(fun:Callable,in_shapes,out_shape)->Callable
jax.api.pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=())->Callable
jax.api.shapecheck(in_shapes,out_shape,fun:Callable)
jax.api.soft_pmap(fun:Callable,axis_name:Optional[AxisName]=None,in_axes=0)->Callable
jax.api.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable[..., Tuple[Any, Any]]
jax.api.vjp(fun:Callable,*primals,**kwargs)->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax.api.vmap(fun:Callable,in_axes=0,out_axes=0)->Callable
jax.api.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:Optional[bool]=None,return_shape:bool=False)->Callable
jax.checkpoint(fun:Callable,concrete:bool=False)->Callable
jax.custom_gradient(fun)
jax.custom_transforms(fun)
jax.defjvp(fun,*jvprules)
jax.defjvp_all(fun,custom_jvp)
jax.defvjp(fun,*vjprules)
jax.defvjp_all(fun,custom_vjp)
jax.device_get(x)
jax.device_put(x,device:Optional[xc.Device]=None)
jax.disable_jit()
jax.eval_shape(fun:Callable,*args,**kwargs)
jax.grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable
jax.hessian(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.invertible(fun:Callable,concrete:bool=False)->Callable
jax.jacfwd(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.jacrev(fun:Callable,argnums:Union[int,Sequence[int]]=0,holomorphic:bool=False)->Callable
jax.jit(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),device=None,backend:Optional[str]=None,donate_argnums:Union[int,Iterable[int]]=())->Callable
jax.jvp(fun:Callable,primals,tangents)->Tuple[Any, Any]
jax.linearize(fun:Callable,*primals)->Tuple[Any, Callable]
jax.make_jaxpr(fun:Callable,static_argnums:Union[int,Iterable[int]]=())->Callable[..., core.TypedJaxpr]
jax.mask(fun:Callable,in_shapes,out_shape)->Callable
jax.pmap(fun:Callable,axis_name:Optional[AxisName]=None,*,in_axes=0,static_broadcasted_argnums:Union[int,Iterable[int]]=(),devices=None,backend:Optional[str]=None,axis_size:Optional[int]=None,donate_argnums:Union[int,Iterable[int]]=())->Callable
jax.shapecheck(in_shapes,out_shape,fun:Callable)
jax.soft_pmap(fun:Callable,axis_name:Optional[AxisName]=None,in_axes=0)->Callable
jax.value_and_grad(fun:Callable,argnums:Union[int,Sequence[int]]=0,has_aux:bool=False,holomorphic:bool=False)->Callable[..., Tuple[Any, Any]]
jax.vjp(fun:Callable,*primals,**kwargs)->Union[Tuple[Any, Callable], Tuple[Any, Callable, Any]]
jax.vmap(fun:Callable,in_axes=0,out_axes=0)->Callable
jax.xla_computation(fun:Callable,static_argnums:Union[int,Iterable[int]]=(),axis_env:Optional[Sequence[Tuple[AxisName,int]]]=None,backend:Optional[str]=None,tuple_args:bool=False,instantiate_const_outputs:Optional[bool]=None,return_shape:bool=False)->Callable


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/linear_util.py----------------------------------------
A:jax.linear_util._EMPTY_STORE_VALUE->EmptyStoreValue()
A:jax.linear_util.gen->gen(*gen_static_args + tuple(args), **kwargs)
A:jax.linear_util.(args, kwargs)->next(gen)
A:jax.linear_util.ans->call(fun, *args)
A:jax.linear_util.(gen, out_store)->stack.pop()
A:jax.linear_util.transformation_stack->map(transform_to_str, enumerate(self.transforms))
A:jax.linear_util.out_store->Store()
A:jax.linear_util.cache->fun_caches.setdefault(fun.f, {})
A:jax.linear_util.result->fun_caches.setdefault(fun.f, {}).get(key, None)
A:jax.linear_util.out1->aux1()
A:jax.linear_util.out2->aux2()
jax.linear_util.EmptyStoreValue(object)
jax.linear_util.Store(self)
jax.linear_util.Store.__nonzero__(self)
jax.linear_util.Store.store(self,val)
jax.linear_util.Store.val(self)
jax.linear_util.StoreException(Exception)
jax.linear_util.WrappedFun(self,f,transforms,stores,params)
jax.linear_util.WrappedFun.__eq__(self,other)
jax.linear_util.WrappedFun.__hash__(self)
jax.linear_util.WrappedFun.__name__(self)
jax.linear_util.WrappedFun.__repr__(self)
jax.linear_util.WrappedFun.call_wrapped(self,*args,**kwargs)
jax.linear_util.WrappedFun.populate_stores(self,stores)
jax.linear_util.WrappedFun.wrap(self,gen,gen_static_args,out_store)->'WrappedFun'
jax.linear_util.cache(call:Callable)
jax.linear_util.fun_name(f)
jax.linear_util.hashable_partial(x,*args)
jax.linear_util.merge_linear_aux(aux1,aux2)
jax.linear_util.transformation(gen,fun:WrappedFun,*gen_static_args)->WrappedFun
jax.linear_util.transformation_with_aux(gen,fun:WrappedFun,*gen_static_args)->Tuple[WrappedFun, Any]
jax.linear_util.wrap_init(f,params={})->WrappedFun


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/version.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/random.py----------------------------------------
A:jax.random.k1->convert(lax.shift_right_logical(seed, lax._const(seed, 32)))
A:jax.random.k2->convert(jnp.bitwise_and(seed, 4294967295))
A:jax.random.nbits->numpy.array(jnp.iinfo(dtype).bits, dtype)
A:jax.random.d->lax.sub(alpha, one_over_three)
A:jax.random.shape->numpy.shape(df)
A:jax.random.aval->jax.abstract_arrays.UnshapedArray(jnp.dtype(jnp.uint32))
A:jax.random.rotate_left->_make_rotate_left(np.uint32)
A:jax.random.v[1]->rotate_left(v[1], rot)
A:jax.random.x->uniform(key, shape, dtype)
A:jax.random.(x, _, _)->lax.fori_loop(0, 5, rolled_loop_step, (x, rotate_list(ks), rotations))
A:jax.random.rank->len(shape)
A:jax.random.ndims->lax.div(one_over_three, lax.pow(d, one_over_two)).get_shape(x).rank()
A:jax.random.threefry2x32_p->jax.core.Primitive('threefry2x32')
A:jax.random.xla.translations[threefry2x32_p]->jax.interpreters.xla.lower_fun(partial(_threefry2x32_lowering, use_rolled_loops=False), multiple_results=True)
A:jax.random.xla.backend_specific_translations['cpu'][threefry2x32_p]->jax.interpreters.xla.lower_fun(partial(_threefry2x32_lowering, use_rolled_loops=True), multiple_results=True)
A:jax.random.out->jnp.concatenate(x)
A:jax.random.counts->lax.iota(np.uint32, max_count)
A:jax.random.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax.random.max_count->int(np.ceil(bit_width * size / 32))
A:jax.random.bits->_random_bits(key, nbits, shape)
A:jax.random.shape_->lax.broadcast_shapes(shape, *param_shapes)
A:jax.random.dtype->dtypes.canonicalize_dtype(dtype)
A:jax.random.minval->lax.convert_element_type(minval, dtype)
A:jax.random.maxval->lax.max(lax.add(minval, np.array(1, dtype)), maxval)
A:jax.random.finfo->jnp.finfo(dtype)
A:jax.random.float_bits->lax.bitwise_or(lax.shift_right_logical(bits, np.array(nbits - nmant, lax.dtype(bits))), np.array(1.0, dtype).view(_UINT_DTYPES[nbits]))
A:jax.random.(k1, k2)->split(key)
A:jax.random.span->lax.convert_element_type(maxval - minval, unsigned_dtype)
A:jax.random.multiplier->lax.rem(lax.mul(multiplier, multiplier), span)
A:jax.random.random_offset->lax.rem(random_offset, span)
A:jax.random.ind->jnp.searchsorted(p_cuml, r)
A:jax.random.num_rounds->int(np.ceil(exponent * np.log(x.size) / np.log(uint32max)))
A:jax.random.(key, subkey)->split(key)
A:jax.random.sort_keys->_random_bits(subkey, 32, x.shape)
A:jax.random.(_, x)->lax.sort_key_val(sort_keys, x, axis)
A:jax.random.a->jnp.broadcast_to(a, shape)
A:jax.random.n_draws->numpy.prod(shape).astype(int)
A:jax.random.p->lax.convert_element_type(p, dtype)
A:jax.random.p_cuml->jnp.cumsum(p)
A:jax.random.lo->numpy.nextafter(np.array(-1.0, dtype), 0.0, dtype=dtype)
A:jax.random.hi->numpy.array(1.0, dtype)
A:jax.random.u->uniform(key, shape, dtype, minval=-1.0 + jnp.finfo(dtype).epsneg, maxval=1.0)
A:jax.random.chol_factor->cholesky(cov)
A:jax.random.normal_samples->normal(key, shape + mean.shape[-1:], dtype)
A:jax.random.sqrt2->numpy.array(np.sqrt(2), dtype)
A:jax.random.b->lax.convert_element_type(b, dtype)
A:jax.random.(key_a, key_b)->split(key)
A:jax.random.gamma_a->gamma(key_a, a, shape, dtype)
A:jax.random.gamma_b->gamma(key_b, b, shape, dtype)
A:jax.random.pi->_constant_like(u, np.pi)
A:jax.random.alpha->lax.select(lax.ge(alpha, one), alpha, lax.add(alpha, one))
A:jax.random.gamma_samples->gamma(key, alpha, shape + np.shape(alpha)[-1:], dtype)
A:jax.random.zero->_constant_like(alpha, 0)
A:jax.random.one->_constant_like(alpha, 1)
A:jax.random.minus_one->_constant_like(alpha, -1)
A:jax.random.one_over_two->_constant_like(alpha, 0.5)
A:jax.random.one_over_three->_constant_like(alpha, 1.0 / 3.0)
A:jax.random.squeeze_const->_constant_like(alpha, 0.0331)
A:jax.random.boost->lax.select(lax.ge(alpha, one), one, lax.pow(uniform(subkey, (), dtype=dtype), lax.div(one, alpha)))
A:jax.random.c->lax.div(one_over_three, lax.pow(d, one_over_two))
A:jax.random.cond->lax.bitwise_and(lax.ge(U, lax.sub(one, lax.mul(squeeze_const, lax.mul(X, X)))), lax.ge(lax.log(U), lax.add(lax.mul(X, one_over_two), lax.mul(d, lax.add(lax.sub(one, V), lax.log(V))))))
A:jax.random.v->uniform(subkey_1, shape, lam.dtype)
A:jax.random.(key, x_key, U_key)->split(key, 3)
A:jax.random.(_, x, v)->lax.while_loop(lambda kxv: lax.le(kxv[2], zero), _next_kxv, (x_key, zero, minus_one))
A:jax.random.X->lax.mul(x, x)
A:jax.random.V->lax.mul(lax.mul(v, v), v)
A:jax.random.U->uniform(U_key, (), dtype=dtype)
A:jax.random.(_, _, V, _)->lax.while_loop(_cond_fn, _body_fn, (key, zero, one, _constant_like(alpha, 2)))
A:jax.random.z->lax.mul(lax.mul(d, V), boost)
A:jax.random.samples->vmap(_gamma_one)(keys, alphas)
A:jax.random.alphas->jnp.reshape(a, -1)
A:jax.random.grads->vmap(lax.random_gamma_grad)(alphas, samples)
A:jax.random.a_shape->jnp.shape(a)
A:jax.random.key->vmap(split, in_axes=(0, None))(key, prod(a_shape[key_ndim:]))
A:jax.random.keys->jnp.reshape(key, (-1, 2))
A:jax.random.k->lax.floor((2 * a / u_shifted + b) * u + lam + 0.43)
A:jax.random.random_gamma_p->jax.core.Primitive('random_gamma')
A:jax.random.xla.translations[random_gamma_p]->jax.interpreters.xla.lower_fun(_gamma_impl, multiple_results=False)
A:jax.random.(rng, subkey)->split(rng)
A:jax.random.k_init->lax.full_like(lam, -1, lam.dtype, shape)
A:jax.random.log_rate_init->lax.full_like(lam, 0, np.float32, shape)
A:jax.random.log_lam->lax.log(lam)
A:jax.random.(key, subkey_0, subkey_1)->split(key, 3)
A:jax.random.s->lax.log(v * inv_alpha / (a / (u_shifted * u_shifted) + b))
A:jax.random.k_out->lax.select(accept, k, k_out)
A:jax.random.accepted->lax.full_like(lam, False, jnp.bool_, shape)
A:jax.random.lam_knuth->lax.select(use_knuth, lam, lax.full_like(lam, 0.0))
A:jax.random.lam_rejection->lax.select(use_knuth, lax.full_like(lam, 100000.0), lam)
A:jax.random.max_iters->dtypes.canonicalize_dtype(dtype).type(jnp.iinfo(dtype).max)
A:jax.random.lam->lax.convert_element_type(lam, np.float32)
A:jax.random.batch_shape->tuple(np.delete(logits.shape, axis))
A:jax.random.e->exponential(key, shape, dtype)
A:jax.random.df->lax.convert_element_type(df, dtype)
A:jax.random.(key_n, key_g)->split(key)
A:jax.random.n->normal(key_n, shape, dtype)
A:jax.random.two->_constant_like(n, 2)
A:jax.random.half_df->lax.div(df, two)
A:jax.random.g->gamma(key_n, half_df, shape, dtype)
jax.random.PRNGKey(seed:int)->jnp.ndarray
jax.random._bernoulli(key,p,shape)
jax.random._beta(key,a,b,shape,dtype)
jax.random._bit_stats(bits)
jax.random._cauchy(key,shape,dtype)
jax.random._check_shape(name,shape,*param_shapes)
jax.random._dirichlet(key,alpha,shape,dtype)
jax.random._exponential(key,shape,dtype)
jax.random._fold_in(key,data)
jax.random._gamma(key,a,shape,dtype)
jax.random._gamma_batching_rule(batched_args,batch_dims)
jax.random._gamma_grad(sample,a)
jax.random._gamma_impl(key,a)
jax.random._gamma_one(key,alpha)
jax.random._gumbel(key,shape,dtype)
jax.random._is_prng_key(key:jnp.ndarray)->bool
jax.random._laplace(key,shape,dtype)
jax.random._logistic(key,shape,dtype)
jax.random._make_rotate_left(dtype)
jax.random._multivariate_normal(key,mean,cov,shape,dtype)
jax.random._normal(key,shape,dtype)
jax.random._pareto(key,b,shape,dtype)
jax.random._poisson(key,lam,shape,dtype)
jax.random._poisson_knuth(key,lam,shape,dtype,max_iters)
jax.random._poisson_rejection(key,lam,shape,dtype,max_iters)
jax.random._randint(key,shape,minval,maxval,dtype)
jax.random._random_bits(key,bit_width,shape)
jax.random._shuffle(key,x,axis)
jax.random._split(key,num)
jax.random._t(key,df,shape,dtype)
jax.random._threefry2x32_abstract_eval(*args)
jax.random._threefry2x32_gpu_translation_rule(c,k1,k2,x1,x2)
jax.random._threefry2x32_lowering(key1,key2,x1,x2,use_rolled_loops=True)
jax.random._truncated_normal(key,lower,upper,shape,dtype)
jax.random._uniform(key,shape,dtype,minval,maxval)
jax.random.apply_round(v,rot)
jax.random.bernoulli(key:jnp.ndarray,p:jnp.ndarray=np.float32(0.5),shape:Optional[Sequence[int]]=None)->jnp.ndarray
jax.random.beta(key:jnp.ndarray,a:Union[float,jnp.ndarray],b:Union[float,jnp.ndarray],shape:Optional[Sequence[int]]=None,dtype:np.dtype=dtypes.float_)->jnp.ndarray
jax.random.categorical(key,logits,axis=-1,shape=None)
jax.random.cauchy(key,shape=(),dtype=dtypes.float_)
jax.random.choice(key,a,shape=(),replace=True,p=None)
jax.random.dirichlet(key,alpha,shape=None,dtype=dtypes.float_)
jax.random.exponential(key,shape=(),dtype=dtypes.float_)
jax.random.fold_in(key,data)
jax.random.gamma(key,a,shape=None,dtype=dtypes.float_)
jax.random.gumbel(key,shape=(),dtype=dtypes.float_)
jax.random.laplace(key,shape=(),dtype=dtypes.float_)
jax.random.logistic(key,shape=(),dtype=dtypes.float_)
jax.random.multivariate_normal(key:jnp.ndarray,mean:jnp.ndarray,cov:jnp.ndarray,shape:Optional[Sequence[int]]=None,dtype:np.dtype=dtypes.float_)->jnp.ndarray
jax.random.normal(key:jnp.ndarray,shape:Sequence[int]=(),dtype:np.dtype=dtypes.float_)->jnp.ndarray
jax.random.pareto(key,b,shape=None,dtype=dtypes.float_)
jax.random.permutation(key,x)
jax.random.poisson(key,lam,shape=(),dtype=dtypes.int_)
jax.random.randint(key:jnp.ndarray,shape:Sequence[int],minval:Union[int,jnp.ndarray],maxval:Union[int,jnp.ndarray],dtype:np.dtype=dtypes.int_)
jax.random.rolled_loop_step(i,state)
jax.random.rotate_list(xs)
jax.random.shuffle(key:jnp.ndarray,x:jnp.ndarray,axis:int=0)->jnp.ndarray
jax.random.split(key:jnp.ndarray,num:int=2)->jnp.ndarray
jax.random.t(key,df,shape=(),dtype=dtypes.float_)
jax.random.threefry_2x32(keypair,count)
jax.random.truncated_normal(key:jnp.ndarray,lower:Union[float,jnp.ndarray],upper:Union[float,jnp.ndarray],shape:Optional[Sequence[int]]=None,dtype:np.dtype=dtypes.float_)->jnp.ndarray
jax.random.uniform(key:jnp.ndarray,shape:Sequence[int]=(),dtype:np.dtype=dtypes.float_,minval:Union[float,jnp.ndarray]=0.0,maxval:Union[float,jnp.ndarray]=1.0)->jnp.ndarray


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/__init__.py----------------------------------------
jax.__init__._init()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/source_info_util.py----------------------------------------
A:jax.source_info_util._jax_path->os.path.dirname(__file__)
A:jax.source_info_util.frame->user_frame(source_info)
A:jax.source_info_util._source_info_context->_SourceInfoContext()
jax.source_info_util._SourceInfoContext(self)
jax.source_info_util.current()->Optional[Traceback]
jax.source_info_util.summarize(source_info:Optional[Traceback])->str
jax.source_info_util.user_context(c)
jax.source_info_util.user_frame(source_info:Optional[Traceback])->Optional[Frame]


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/tree_util.py----------------------------------------
A:jax.tree_util._registry[nodetype]->_RegistryEntry(flatten_func, unflatten_func)
A:jax.tree_util.(leaves, treedef)->lib.pytree.flatten(tree)
A:jax.tree_util.(flat, treedef)->tree_flatten(pytree_to_transpose)
A:jax.tree_util.expected_treedef->outer_treedef.compose(inner_treedef)
A:jax.tree_util.flat->iter(flat)
A:jax.tree_util.transposed_lol->zip(*lol)
A:jax.tree_util.subtrees->map(partial(tree_unflatten, outer_treedef), transposed_lol)
A:jax.tree_util._RegistryEntry->collections.namedtuple('RegistryEntry', ['to_iter', 'from_iter'])
A:jax.tree_util.handler->_registry.get(type(tree))
A:jax.tree_util.(children, metadata)->_registry.get(type(tree)).to_iter(tree)
A:jax.tree_util.children->iter(tree)
A:jax.tree_util.no_initializer->object()
jax.tree_util.Partial(functools.partial)
jax.tree_util._process_pytree(process_node,tree)
jax.tree_util._replace_nones(sentinel,tree)
jax.tree_util.all_leaves(iterable)
jax.tree_util.build_tree(treedef,xs)
jax.tree_util.register_pytree_node(nodetype,flatten_func,unflatten_func)
jax.tree_util.register_pytree_node_class(cls)
jax.tree_util.tree_all(tree)
jax.tree_util.tree_flatten(tree)
jax.tree_util.tree_leaves(tree)
jax.tree_util.tree_map(f,tree)
jax.tree_util.tree_multimap(f,tree,*rest)
jax.tree_util.tree_reduce(function,tree,initializer=no_initializer)
jax.tree_util.tree_structure(tree)
jax.tree_util.tree_transpose(outer_treedef,inner_treedef,pytree_to_transpose)
jax.tree_util.tree_unflatten(treedef,leaves)
jax.tree_util.treedef_children(treedef)
jax.tree_util.treedef_is_leaf(treedef)
jax.tree_util.treedef_tuple(treedefs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax_linalg.py----------------------------------------
A:jax.lax_linalg.x->jax.interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax_linalg.(w, vl, vr)->Primitive('eig').bind(x)
A:jax.lax_linalg.(v, w)->jax.interpreters.xla.apply_primitive(eigh_p, operand, lower=lower)
A:jax.lax_linalg.(lu, pivots)->Primitive('lu').bind(a)
A:jax.lax_linalg.(q, r)->Primitive('qr').bind(x, full_matrices=False)
A:jax.lax_linalg.(s, u, v)->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.b->min(r - k, block_size)
A:jax.lax_linalg.out->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve', translation_rule=_triangular_solve_translation_rule).bind(a, b, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.lax_linalg.t->f(c, *args, **kwargs)
A:jax.lax_linalg.L->jax.numpy.lax_numpy.tril(cholesky_p.bind(x))
A:jax.lax_linalg.l->jax.lax.pad(jnp.tril(lu[..., :, :k], -1), zero, l_padding)
A:jax.lax_linalg.tmp->triangular_solve(L, sigma_dot, left_side=False, transpose_a=True, conjugate_a=True, lower=True)
A:jax.lax_linalg.L_dot->jax.lax.batch_matmul(L, phi(triangular_solve(L, tmp, left_side=True, transpose_a=False, lower=True)), precision=lax.Precision.HIGHEST)
A:jax.lax_linalg.cholesky_p->standard_unop(_float | _complex, 'cholesky')
A:jax.lax_linalg.shape->c.get_shape(operand)
A:jax.lax_linalg.dtype->jax.lax.dtype(a)
A:jax.lax_linalg.nan->jax.lib.xla_bridge.constant(c, np.array(np.nan, dtype=dtype))
A:jax.lax_linalg.(result, info)->potrf_impl(c, operand, lower=True)
A:jax.lax_linalg.ok->xops.Eq(info, xops.ConstantLiteral(c, np.array(0, np.int32)))
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][cholesky_p]->partial(_cholesky_cpu_gpu_translation_rule, lapack.potrf)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][cholesky_p]->partial(_cholesky_cpu_gpu_translation_rule, cusolver.potrf)
A:jax.lax_linalg.vlvr->ShapedArray(batch_dims + (n, n), dtype)
A:jax.lax_linalg.w->w_real.astype(a.dtype)
A:jax.lax_linalg.(w, vl, vr, info)->_cpu_geev(c, operand)
A:jax.lax_linalg.vl->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), vl, _nan_like(c, vl))
A:jax.lax_linalg.vr->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), vr, _nan_like(c, vr))
A:jax.lax_linalg.eig_p->Primitive('eig')
A:jax.lax_linalg.dims->c.get_shape(operand).dimensions()
A:jax.lax_linalg.n->len(dims)
A:jax.lax_linalg.operand->xops.Transpose(operand, list(range(n - 2)) + [n - 1, n - 2])
A:jax.lax_linalg.v->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), v, _nan_like(c, v))
A:jax.lax_linalg.(v, w, info)->syevd_impl(c, operand, lower=lower)
A:jax.lax_linalg.(v, w_real)->Primitive('eigh').bind(symmetrize(a), lower=lower)
A:jax.lax_linalg.eye_n->jax.numpy.lax_numpy.eye(a.shape[-1], dtype=a.dtype)
A:jax.lax_linalg.dot->partial(lax.dot if g_a.ndim == 2 else lax.batch_matmul, precision=lax.Precision.HIGHEST)
A:jax.lax_linalg.vdag_adot_v->dot(dot(_H(v), a_dot), v)
A:jax.lax_linalg.dv->dot(v, jnp.multiply(Fmat, vdag_adot_v))
A:jax.lax_linalg.dw->jax.numpy.lax_numpy.real(jnp.diagonal(vdag_adot_v, axis1=-2, axis2=-1))
A:jax.lax_linalg.eigh_p->Primitive('eigh')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, _cpu_syevd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][eigh_p]->partial(_eigh_cpu_gpu_translation_rule, cusolver.syevd)
A:jax.lax_linalg.triangular_solve_dtype_rule->partial(naryop_dtype_rule, _input_dtype, (_float | _complex, _float | _complex), 'triangular_solve')
A:jax.lax_linalg.g_a->jax.lax.neg(g_a)
A:jax.lax_linalg.cotangent_b->triangular_solve(a, cotangent, left_side, lower, not transpose_a, conjugate_a, unit_diagonal)
A:jax.lax_linalg.y->jax.interpreters.batching.bdim_at_front(y, by, size)
A:jax.lax_linalg.y_flat->jax.interpreters.batching.bdim_at_front(y, by, size).reshape(y.shape[:-3] + (y.shape[-3] * y.shape[-2], y.shape[-1]))
A:jax.lax_linalg.out_flat->triangular_solve(x, y_flat, left_side=left_side, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.lax_linalg.size->next((t.shape[i] for (t, i) in zip(batched_args, batch_dims) if i is not None))
A:jax.lax_linalg.a->jax.ops.index_add(a, ops.index[k + b:, k + b:], -lax.dot(a[k + b:, k:k + b], a[k:k + b, k + b:], precision=lax.Precision.HIGHEST))
A:jax.lax_linalg.triangular_solve_p->standard_primitive(triangular_solve_shape_rule, triangular_solve_dtype_rule, 'triangular_solve', translation_rule=_triangular_solve_translation_rule)
A:jax.lax_linalg.batch->prod(dims[:-2])
A:jax.lax_linalg.m_idx->jax.numpy.lax_numpy.arange(m)
A:jax.lax_linalg.n_idx->jax.numpy.lax_numpy.arange(n)
A:jax.lax_linalg.magnitude->jax.numpy.lax_numpy.abs(a[:, k])
A:jax.lax_linalg.i->jax.numpy.lax_numpy.argmax(jnp.where(m_idx >= k, magnitude, -jnp.inf))
A:jax.lax_linalg.pivot->xops.Sub(pivot, xops.ConstantLiteral(c, np.array(1, np.int32)))
A:jax.lax_linalg.perm->jax.numpy.lax_numpy.arange(m, dtype=jnp.int32)
A:jax.lax_linalg.r->jax.interpreters.xla.lower_fun(jnp.triu, multiple_results=False)(c, r)
A:jax.lax_linalg.(block_pivot, perm, lu_block)->_lu_unblocked(a[k:, k:k + b])
A:jax.lax_linalg.batch_size->numpy.prod(batch_dims, dtype=np.int64)
A:jax.lax_linalg.(pivot, lu)->_lu_blocked(x)
A:jax.lax_linalg.lu->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), lu, _nan_like(c, lu))
A:jax.lax_linalg.(lu, pivot)->jax.interpreters.xla.apply_primitive(lu_p, operand)
A:jax.lax_linalg.a_shape->jax.numpy.lax_numpy.shape(a)
A:jax.lax_linalg.k->min(m, n)
A:jax.lax_linalg.permutation->list(range(len(shape)))
A:jax.lax_linalg.iotas->jax.numpy.lax_numpy.ix_(*(lax.iota(jnp.int32, b) for b in batch_dims))
A:jax.lax_linalg.ndims->len(a_shape)
A:jax.lax_linalg.zero->jax.numpy.lax_numpy._constant_like(lu, 0)
A:jax.lax_linalg.u_eye->jax.lax.pad(jnp.eye(n - k, n - k, dtype=dtype), zero, ((k, 0, 0), (k, 0, 0)))
A:jax.lax_linalg.la->triangular_solve(l, x, left_side=True, transpose_a=False, lower=True, unit_diagonal=True)
A:jax.lax_linalg.lau->triangular_solve(u, la, left_side=False, transpose_a=False, lower=False)
A:jax.lax_linalg.l_dot->jax.numpy.lax_numpy.matmul(l, jnp.tril(lau, -1))
A:jax.lax_linalg.u_dot->jax.numpy.lax_numpy.matmul(jnp.triu(lau), u)
A:jax.lax_linalg.(lu, pivot, info)->getrf_impl(c, operand)
A:jax.lax_linalg.lu_p->Primitive('lu')
A:jax.lax_linalg.xla.translations[lu_p]->jax.interpreters.xla.lower_fun(_lu_python, multiple_results=True)
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, lapack.getrf)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][lu_p]->partial(_lu_cpu_gpu_translation_rule, cusolver.getrf)
A:jax.lax_linalg.(result, _)->jax.lax.fori_loop(np.array(0, np.int32), np.array(k, np.int32), _lu_pivots_body_fn, (permutation, swaps))
A:jax.lax_linalg.q->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), q, _nan_like(c, q))
A:jax.lax_linalg.dx_rinv->triangular_solve(r, dx)
A:jax.lax_linalg.qt_dx_rinv->jax.numpy.lax_numpy.matmul(_H(q), dx_rinv)
A:jax.lax_linalg.qt_dx_rinv_lower->jax.numpy.lax_numpy.tril(qt_dx_rinv, -1)
A:jax.lax_linalg.dr->jax.numpy.lax_numpy.matmul(qt_dx_rinv - do, r)
A:jax.lax_linalg.(r, tau, info_geqrf)->geqrf_impl(c, operand)
A:jax.lax_linalg.(q, info_orgqr)->orgqr_impl(c, q, tau)
A:jax.lax_linalg.qr_p->Primitive('qr')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, lapack.geqrf, lapack.orgqr)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][qr_p]->partial(_qr_cpu_gpu_translation_rule, cusolver.geqrf, cusolver.orgqr)
A:jax.lax_linalg.(s, u, vt)->jax.interpreters.xla.apply_primitive(svd_p, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.(u, s, v)->xops.SVD(operand)
A:jax.lax_linalg.vt->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), vt, _nan_like(c, vt))
A:jax.lax_linalg.u->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1, 1)), u, _nan_like(c, u))
A:jax.lax_linalg.s->_broadcasting_select(c, xops.Reshape(ok, batch_dims + (1,)), s, _nan_like(c, s))
A:jax.lax_linalg.(s, U, Vt)->Primitive('svd').bind(A, full_matrices=False, compute_uv=True)
A:jax.lax_linalg.dS->jax.numpy.lax_numpy.matmul(jnp.matmul(Ut, dA), V)
A:jax.lax_linalg.ds->jax.numpy.lax_numpy.real(jnp.diagonal(dS, 0, -2, -1))
A:jax.lax_linalg.dU->jax.numpy.lax_numpy.matmul(U, F * (dSS + _T(dSS)))
A:jax.lax_linalg.dV->jax.numpy.lax_numpy.matmul(V, F * (SdS + _T(SdS)))
A:jax.lax_linalg.(s, u, vt, info)->gesvd_impl(c, operand, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.outs->Primitive('svd').bind(x, full_matrices=full_matrices, compute_uv=compute_uv)
A:jax.lax_linalg.svd_p->Primitive('svd')
A:jax.lax_linalg.xla.backend_specific_translations['cpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, lapack.gesdd)
A:jax.lax_linalg.xla.backend_specific_translations['gpu'][svd_p]->partial(_svd_cpu_gpu_translation_rule, cusolver.gesvd)
jax.lax_linalg._H(x)
jax.lax_linalg._T(x)
jax.lax_linalg._cholesky_cpu_gpu_translation_rule(potrf_impl,c,operand)
jax.lax_linalg._eigh_cpu_gpu_translation_rule(syevd_impl,c,operand,lower)
jax.lax_linalg._lu_abstract_eval(operand)
jax.lax_linalg._lu_batching_rule(batched_args,batch_dims)
jax.lax_linalg._lu_blocked(a,block_size=128)
jax.lax_linalg._lu_cpu_gpu_translation_rule(getrf_impl,c,operand)
jax.lax_linalg._lu_impl(operand)
jax.lax_linalg._lu_jvp_rule(primals,tangents)
jax.lax_linalg._lu_pivots_body_fn(i,permutation_and_swaps)
jax.lax_linalg._lu_python(x)
jax.lax_linalg._lu_solve(lu,pivots,b,trans)
jax.lax_linalg._lu_solve_core(lu,pivots,b,trans)
jax.lax_linalg._lu_unblocked(a)
jax.lax_linalg._nan_like(c,operand)
jax.lax_linalg._qr_cpu_gpu_translation_rule(geqrf_impl,orgqr_impl,c,operand,full_matrices)
jax.lax_linalg._svd_cpu_gpu_translation_rule(gesvd_impl,c,operand,full_matrices,compute_uv)
jax.lax_linalg._triangular_solve_cpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._triangular_solve_gpu_translation_rule(c,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._triangular_solve_translation_rule(c,a,b,*,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg._unpack_tuple(f,n)
jax.lax_linalg.cholesky(x,symmetrize_input=True)
jax.lax_linalg.cholesky_batching_rule(batched_args,batch_dims)
jax.lax_linalg.cholesky_jvp_rule(primals,tangents)
jax.lax_linalg.eig(x)
jax.lax_linalg.eig_abstract_eval(operand)
jax.lax_linalg.eig_batching_rule(batched_args,batch_dims)
jax.lax_linalg.eig_cpu_translation_rule(c,operand)
jax.lax_linalg.eig_impl(operand)
jax.lax_linalg.eig_translation_rule(c,operand)
jax.lax_linalg.eigh(x,lower=True,symmetrize_input=True)
jax.lax_linalg.eigh_abstract_eval(operand,lower)
jax.lax_linalg.eigh_batching_rule(batched_args,batch_dims,lower)
jax.lax_linalg.eigh_impl(operand,lower)
jax.lax_linalg.eigh_jvp_rule(primals,tangents,lower)
jax.lax_linalg.eigh_translation_rule(c,operand,lower)
jax.lax_linalg.lu(x)
jax.lax_linalg.lu_pivots_to_permutation(swaps,m)
jax.lax_linalg.lu_solve(lu,pivots,b,trans=0)
jax.lax_linalg.qr(x,full_matrices=True)
jax.lax_linalg.qr_abstract_eval(operand,full_matrices)
jax.lax_linalg.qr_batching_rule(batched_args,batch_dims,full_matrices)
jax.lax_linalg.qr_impl(operand,full_matrices)
jax.lax_linalg.qr_jvp_rule(primals,tangents,full_matrices)
jax.lax_linalg.qr_translation_rule(c,operand,full_matrices)
jax.lax_linalg.svd(x,full_matrices=True,compute_uv=True)
jax.lax_linalg.svd_abstract_eval(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_batching_rule(batched_args,batch_dims,full_matrices,compute_uv)
jax.lax_linalg.svd_impl(operand,full_matrices,compute_uv)
jax.lax_linalg.svd_jvp_rule(primals,tangents,full_matrices,compute_uv)
jax.lax_linalg.svd_translation_rule(c,operand,full_matrices,compute_uv)
jax.lax_linalg.symmetrize(x)
jax.lax_linalg.triangular_solve(a,b,left_side=False,lower=False,transpose_a=False,conjugate_a=False,unit_diagonal=False)
jax.lax_linalg.triangular_solve_batching_rule(batched_args,batch_dims,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_jvp_rule_a(g_a,ans,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)
jax.lax_linalg.triangular_solve_shape_rule(a,b,left_side=False,**unused_kwargs)
jax.lax_linalg.triangular_solve_transpose_rule(cotangent,a,b,left_side,lower,transpose_a,conjugate_a,unit_diagonal)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/api_util.py----------------------------------------
A:jax.api_util.fun.__name__->namestr.format(fun=get_name(wrapped))
A:jax.api_util.fun.__module__->get_module(wrapped)
A:jax.api_util.fun.__doc__->docstr.format(fun=get_name(wrapped), doc=get_doc(wrapped), **kwargs)
A:jax.api_util.(py_args, py_kwargs)->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(args, in_tree)->tree_flatten(py_args)
A:jax.api_util.ans->fun(*args)
A:jax.api_util.py_args->tree_unflatten(in_tree, args_flat)
A:jax.api_util.(ans_flat, ans_tree)->tree_flatten(ans)
A:jax.api_util.(aux_flat, aux_tree)->tree_flatten(aux)
A:jax.api_util.dyn_argnums->tuple(dyn_argnums)
A:jax.api_util.fixed_args->tuple([unit if i in dyn_argnums else wrap_hashably(arg) for (i, arg) in enumerate(args)])
A:jax.api_util.dyn_args->tuple((args[i] for i in dyn_argnums))
A:jax.api_util.donate->bool(i in donate_argnums)
A:jax.api_util.static_argnums->sorted(set(static_argnums))
A:jax.api_util.donate_argnums->sorted(set(donate_argnums))
A:jax.api_util.proxy->object()
A:jax.api_util.dummy->tree_unflatten(treedef, [object()] * treedef.num_leaves)
jax.api_util._argnums_partial(dyn_argnums,fixed_args,*dyn_args,**kwargs)
jax.api_util.apply_flat_fun(fun,io_tree,*py_args)
jax.api_util.apply_flat_fun_nokwargs(fun,io_tree,py_args)
jax.api_util.argnums_partial(f,dyn_argnums,args)
jax.api_util.donation_vector(donate_argnums,args,kwargs)->Tuple[bool, ...]
jax.api_util.flatten_axes(name,treedef,axis_tree)
jax.api_util.flatten_fun(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs(in_tree,*args_flat)
jax.api_util.flatten_fun_nokwargs2(in_tree,*args_flat)
jax.api_util.get_doc(fun)
jax.api_util.get_module(fun)
jax.api_util.get_name(fun)
jax.api_util.rebase_donate_argnums(donate_argnums,static_argnums)->Tuple[int, ...]
jax.api_util.wrap_hashably(arg)
jax.api_util.wraps(wrapped,fun,namestr='{fun}',docstr='{doc}',**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/pprint_util.py----------------------------------------
A:jax.pprint_util.indented_block->rhs.indent(indent + len(s))
jax.pprint_util.PrettyPrint(self,lines)
jax.pprint_util.PrettyPrint.__add__(self,rhs)
jax.pprint_util.PrettyPrint.__rshift__(self,rhs)
jax.pprint_util.PrettyPrint.__str__(self)
jax.pprint_util.PrettyPrint.annotate(self,length,msg)
jax.pprint_util.PrettyPrint.indent(self,indent)
jax.pprint_util.hcat(ps)
jax.pprint_util.pp(s)
jax.pprint_util.vcat(ps)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/ndimage.py----------------------------------------
A:jax.scipy.ndimage._nonempty_prod->functools.partial(functools.reduce, operator.mul)
A:jax.scipy.ndimage._nonempty_sum->functools.partial(functools.reduce, operator.add)
A:jax.scipy.ndimage.index->numpy.lax_numpy.floor(coordinate).astype(jnp.int32)
A:jax.scipy.ndimage.weight->coordinate.dtype.type(1)
A:jax.scipy.ndimage.lower->numpy.lax_numpy.floor(coordinate)
A:jax.scipy.ndimage.input->numpy.lax_numpy.asarray(input)
A:jax.scipy.ndimage.cval->numpy.lax_numpy.asarray(cval, input.dtype)
A:jax.scipy.ndimage.index_fixer->_INDEX_FIXERS.get(mode)
A:jax.scipy.ndimage.interp_nodes->interp_fun(coordinate)
A:jax.scipy.ndimage.fixed_index->index_fixer(index, size)
A:jax.scipy.ndimage.valid->is_valid(index, size)
A:jax.scipy.ndimage.(indices, validities, weights)->zip(*items)
A:jax.scipy.ndimage.all_valid->functools.reduce(operator.and_, validities)
A:jax.scipy.ndimage.contribution->numpy.lax_numpy.where(all_valid, input[indices], cval)
A:jax.scipy.ndimage.result->_round_half_away_from_zero(result)
jax.scipy.ndimage._linear_indices_and_weights(coordinate)
jax.scipy.ndimage._map_coordinates(input,coordinates,order,mode,cval)
jax.scipy.ndimage._nearest_indices_and_weights(coordinate)
jax.scipy.ndimage._round_half_away_from_zero(a)
jax.scipy.ndimage.map_coordinates(input,coordinates,order,mode='constant',cval=0.0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/signal.py----------------------------------------
A:jax.scipy.signal.(in1, in2)->_promote_dtypes_inexact(in1, in2)
A:jax.scipy.signal.no_swap->all((s1 >= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax.scipy.signal.swap->all((s1 <= s2 for (s1, s2) in zip(in1.shape, in2.shape)))
A:jax.scipy.signal.strides->tuple((1 for s in shape))
A:jax.scipy.signal.result->lax.conv_general_dilated(in1[None, None], in2[None, None], strides, padding, precision=precision)
A:jax.scipy.signal.(data,)->_promote_dtypes_inexact(jnp.asarray(data))
A:jax.scipy.signal.bp->numpy.sort(np.unique(np.r_[0, bp, N]))
A:jax.scipy.signal.data->data.at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST)).at[sl].add(-jnp.matmul(A, coef, precision=lax.Precision.HIGHEST))
A:jax.scipy.signal.sl->slice(bp[m], bp[m + 1])
A:jax.scipy.signal.(coef, *_)->numpy.linalg.lstsq(A, data[sl])
jax.scipy.signal._convolve_nd(in1,in2,mode,*,precision)
jax.scipy.signal.convolve(in1,in2,mode='full',method='auto',precision=None)
jax.scipy.signal.convolve2d(in1,in2,mode='full',boundary='fill',fillvalue=0,precision=None)
jax.scipy.signal.correlate(in1,in2,mode='full',method='auto',precision=None)
jax.scipy.signal.correlate2d(in1,in2,mode='full',boundary='fill',fillvalue=0,precision=None)
jax.scipy.signal.detrend(data,axis=-1,type='linear',bp=0,overwrite_data=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/linalg.py----------------------------------------
A:jax.scipy.linalg.a->lax.pad(a, dtype.type(0), ((0, 0, 0), (acc.shape[-1], 0, 0)))
A:jax.scipy.linalg.l->lax_linalg.cholesky(a if lower else jnp.conj(_T(a)), symmetrize_input=False)
A:jax.scipy.linalg.(c, b)->numpy.linalg._promote_arg_dtypes(jnp.asarray(c), jnp.asarray(b))
A:jax.scipy.linalg.b->lax_linalg.triangular_solve(c, b, left_side=True, lower=lower, transpose_a=lower, conjugate_a=lower)
A:jax.scipy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower)
A:jax.scipy.linalg.(lu, pivots)->lax_linalg.lu(a)
A:jax.scipy.linalg.dtype->lax.dtype(acc)
A:jax.scipy.linalg.(m, n)->numpy.lax_numpy.shape(a)
A:jax.scipy.linalg.permutation->lax_linalg.lu_pivots_to_permutation(pivots, m)
A:jax.scipy.linalg.p->numpy.lax_numpy.real(jnp.array(permutation == jnp.arange(m)[:, None], dtype=dtype))
A:jax.scipy.linalg.k->min(m, n)
A:jax.scipy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.scipy.linalg.(a, b)->numpy.linalg._promote_arg_dtypes(jnp.asarray(a), jnp.asarray(b))
A:jax.scipy.linalg.factors->cho_factor(lax.stop_gradient(a), lower=lower)
A:jax.scipy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: np_linalg._matvec_multiply(a, x), solve=lambda _, x: cho_solve(factors, x), symmetric=True)
A:jax.scipy.linalg.out->lax_linalg.triangular_solve(a, b, left_side=True, lower=lower, transpose_a=transpose_a, conjugate_a=conjugate_a, unit_diagonal=unit_diagonal)
A:jax.scipy.linalg._expm_description->textwrap.dedent('\nIn addition to the original NumPy argument(s) listed below,\nalso supports the optional boolean argument ``upper_triangular``\nto specify whether the ``A`` matrix is upper triangular.\n')
A:jax.scipy.linalg.(P, Q, n_squarings)->_calc_P_Q(A)
A:jax.scipy.linalg.R->_precise_dot(R, R)
A:jax.scipy.linalg.A->numpy.lax_numpy.asarray(A)
A:jax.scipy.linalg.A_L1->numpy.linalg.norm(A, 1)
A:jax.scipy.linalg.(U3, V3)->_pade3(A)
A:jax.scipy.linalg.(U5, V5)->_pade5(A)
A:jax.scipy.linalg.(U7, V7)->_pade7(A)
A:jax.scipy.linalg.(U9, V9)->_pade9(A)
A:jax.scipy.linalg.n_squarings->numpy.lax_numpy.maximum(0, jnp.floor(jnp.log2(A_L1 / maxnorm)))
A:jax.scipy.linalg.(U13, V13)->_pade13(A)
A:jax.scipy.linalg.conds->numpy.lax_numpy.array([0.4258730016922831, 1.880152677804762])
A:jax.scipy.linalg.U->_precise_dot(A, W)
A:jax.scipy.linalg.V->numpy.lax_numpy.select(A_norm_1 <= ell_table_61_local99, (V3579, V3579), V13)
A:jax.scipy.linalg.lower->numpy.lax_numpy.zeros(1, dtype=s.dtype)
A:jax.scipy.linalg.ident->numpy.lax_numpy.eye(*A.shape, dtype=A.dtype)
A:jax.scipy.linalg.A2->_precise_dot(A, A)
A:jax.scipy.linalg.A4->_precise_dot(A2, A2)
A:jax.scipy.linalg.A6->_precise_dot(A2, A4)
A:jax.scipy.linalg.A8->_precise_dot(A4, A4)
A:jax.scipy.linalg._expm_frechet_description->textwrap.dedent("\nDoes not currently support the Scipy argument ``jax.numpy.asarray_chkfinite``,\nbecause `jax.numpy.asarray_chkfinite` does not exist at the moment. Does not\nsupport the ``method='blockEnlarge'`` argument.\n")
A:jax.scipy.linalg.E->numpy.lax_numpy.asarray(E)
A:jax.scipy.linalg.(expm_A, expm_frechet_AE)->expm_frechet_algo_64(A, E)
A:jax.scipy.linalg.A_norm_1->numpy.linalg.norm(A, 1)
A:jax.scipy.linalg.(U3579, V3579, Lu3579, Lv3579, s3579)->lax.cond(A_norm_1 <= ell_table_61[3], args, lambda args: _diff_pade3(args), args, lambda args: lax.cond(A_norm_1 <= ell_table_61[5], args, lambda args: _diff_pade5(args), args, lambda args: lax.cond(A_norm_1 <= ell_table_61[7], args, lambda args: _diff_pade7(args), args, lambda args: _diff_pade9(args))))
A:jax.scipy.linalg.(U13, V13, Lu13, Lv13, s13)->_diff_pade13(args)
A:jax.scipy.linalg.ell_table_61_local99->numpy.lax_numpy.array([ell_table_61[9], ell_table_61[9]])
A:jax.scipy.linalg.Lu->numpy.lax_numpy.select(A_norm_1 <= ell_table_61_local99, (Lu3579, Lu3579), Lu13)
A:jax.scipy.linalg.Lv->numpy.lax_numpy.select(A_norm_1 <= ell_table_61_local99, (Lv3579, Lv3579), Lv13)
A:jax.scipy.linalg.s->numpy.lax_numpy.maximum(0, jnp.floor_divide(lax.ceil(jnp.log2(A_norm_1 / ell_table_61[13])), 1))
A:jax.scipy.linalg.lu_piv->lu_factor(-U + V)
A:jax.scipy.linalg.L->lu_solve(lu_piv, Lu + Lv + _precise_dot(Lu - Lv, R))
A:jax.scipy.linalg.(R, L)->lax.fori_loop(lower[0], s, my_body_fun, (R, L))
A:jax.scipy.linalg.two->numpy.lax_numpy.array([2.0], A.dtype)
A:jax.scipy.linalg.arrs->numpy.lax_numpy._promote_dtypes(*arrs)
A:jax.scipy.linalg.acc->lax.concatenate([acc, a], dimension=0)
jax.scipy.linalg._calc_P_Q(A)
jax.scipy.linalg._cho_solve(c,b,lower)
jax.scipy.linalg._cholesky(a,lower)
jax.scipy.linalg._diff_pade13(args)
jax.scipy.linalg._diff_pade3(args)
jax.scipy.linalg._diff_pade5(args)
jax.scipy.linalg._diff_pade7(args)
jax.scipy.linalg._diff_pade9(args)
jax.scipy.linalg._expm(A,upper_triangular)
jax.scipy.linalg._expm_frechet(A,E,method=None,compute_expm=True)
jax.scipy.linalg._expm_jvp(upper_triangular,primals,tangents)
jax.scipy.linalg._lu(a,permute_l)
jax.scipy.linalg._pade13(A)
jax.scipy.linalg._pade3(A)
jax.scipy.linalg._pade5(A)
jax.scipy.linalg._pade7(A)
jax.scipy.linalg._pade9(A)
jax.scipy.linalg._precise_dot(A,B)
jax.scipy.linalg._qr(a,mode,pivoting)
jax.scipy.linalg._solve(a,b,sym_pos,lower)
jax.scipy.linalg._solve_P_Q(P,Q,upper_triangular=False)
jax.scipy.linalg._solve_triangular(a,b,trans,lower,unit_diagonal)
jax.scipy.linalg._squaring(R,n_squarings)
jax.scipy.linalg.block_diag(*arrs)
jax.scipy.linalg.cho_factor(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.cho_solve(c_and_lower,b,overwrite_b=False,check_finite=True)
jax.scipy.linalg.cholesky(a,lower=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.det(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.eigh(a,b=None,lower=True,eigvals_only=False,overwrite_a=False,overwrite_b=False,turbo=True,eigvals=None,type=1,check_finite=True)
jax.scipy.linalg.expm(A,*,upper_triangular=False)
jax.scipy.linalg.expm_frechet(A,E,*,method=None,compute_expm=True)
jax.scipy.linalg.expm_frechet_algo_64(A,E)
jax.scipy.linalg.inv(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu(a,permute_l=False,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_factor(a,overwrite_a=False,check_finite=True)
jax.scipy.linalg.lu_solve(lu_and_piv,b,trans=0,overwrite_b=False,check_finite=True)
jax.scipy.linalg.qr(a,overwrite_a=False,lwork=None,mode='full',pivoting=False,check_finite=True)
jax.scipy.linalg.solve(a,b,sym_pos=False,lower=False,overwrite_a=False,overwrite_b=False,debug=False,check_finite=True)
jax.scipy.linalg.solve_triangular(a,b,trans=0,lower=False,unit_diagonal=False,overwrite_b=False,debug=None,check_finite=True)
jax.scipy.linalg.svd(a,full_matrices=True,compute_uv=True,overwrite_a=False,check_finite=True,lapack_driver='gesdd')
jax.scipy.linalg.tril(m,k=0)
jax.scipy.linalg.triu(m,k=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/special.py----------------------------------------
A:jax.scipy.special.(x,)->_promote_args_inexact('entr', x)
A:jax.scipy.special.(x, y)->_promote_args_inexact('xlog1py', x, y)
A:jax.scipy.special.(a, b, x)->_promote_args_inexact('betainc', a, b, x)
A:jax.scipy.special.(a, x)->_promote_args_inexact('gammaincc', a, x)
A:jax.scipy.special.x->numpy.lax_numpy.asarray(x)
A:jax.scipy.special.one->lax._const(x, 1)
A:jax.scipy.special.(a, b)->numpy.lax_numpy.broadcast_arrays(a, b)
A:jax.scipy.special.dims->_reduction_dims(a, axis)
A:jax.scipy.special.amax->lax.stop_gradient(lax.select(lax.is_finite(amax), amax, lax.full_like(amax, 0)))
A:jax.scipy.special.amax_singletons->dimadd(amax)
A:jax.scipy.special.out->numpy.lax_numpy.where(sign < 0, np.nan, out)
A:jax.scipy.special.sign->lax.stop_gradient(lax.sign(sumexp))
A:jax.scipy.special.sumexp->lax.reduce(lax.mul(lax.exp(lax.sub(a, amax_singletons)), b), _constant_like(a, 0), lax.add, dims)
A:jax.scipy.special.safe_x->numpy.lax_numpy.where(x_ok, x, 1.0)
A:jax.scipy.special.safe_y->numpy.lax_numpy.where(x_ok, y, 1.0)
A:jax.scipy.special.(a,)->_promote_args_inexact('multigammaln', a)
A:jax.scipy.special.d->lax.convert_element_type(d, lax.dtype(a))
A:jax.scipy.special.constant->lax.mul(lax.mul(lax.mul(_constant_like(a, 0.25), d), lax.sub(d, _constant_like(a, 1))), lax.log(_constant_like(a, np.pi)))
A:jax.scipy.special.res->numpy.lax_numpy.sum(gammaln(jnp.expand_dims(a, axis=-1) - lax.div(jnp.arange(d), _constant_like(a, 2))), axis=-1)
A:jax.scipy.special.(s, a)->_promote_args_inexact('zeta', x, q)
A:jax.scipy.special.k->numpy.arange(N, dtype=N.dtype)
A:jax.scipy.special.S->numpy.lax_numpy.sum((a_ + k) ** (-s_), -1)
A:jax.scipy.special.I->lax.div((a + N) ** (dtype(1) - s), s - dtype(1))
A:jax.scipy.special.T1->numpy.lax_numpy.clip(T1, a_max=jnp.finfo(dtype).max)
A:jax.scipy.special.coefs->numpy.array(_BERNOULLI_COEFS[:T1.shape[-1]], dtype=dtype)
A:jax.scipy.special.(n, x)->_promote_args_inexact('polygamma', n, x)
A:jax.scipy.special.shape->numpy.lax_numpy.shape(p)
A:jax.scipy.special._LOGNDTR_FLOAT64_LOWER->numpy.array(-20, np.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_LOWER->numpy.array(-10, np.float32)
A:jax.scipy.special._LOGNDTR_FLOAT64_UPPER->numpy.array(8, np.float64)
A:jax.scipy.special._LOGNDTR_FLOAT32_UPPER->numpy.array(5, np.float32)
A:jax.scipy.special.dtype->lax.dtype(x)
A:jax.scipy.special.z->lax.sqrt(dtype(-2.0) * lax.log(sanitized_mcp))
A:jax.scipy.special.y->lax.select(lax.lt(z, half_sqrt_2), dtype(1.0) + lax.erf(w), lax.select(lax.gt(w, dtype(0.0)), dtype(2.0) - lax.erfc(z), lax.erfc(z)))
A:jax.scipy.special.p0->list(reversed([-59.96335010141079, 98.00107541859997, -56.67628574690703, 13.931260938727968, -1.2391658386738125]))
A:jax.scipy.special.q0->list(reversed([1.0, 1.9544885833814176, 4.676279128988815, 86.36024213908905, -225.46268785411937, 200.26021238006066, -82.03722561683334, 15.90562251262117, -1.1833162112133]))
A:jax.scipy.special.p1->list(reversed([4.0554489230596245, 31.525109459989388, 57.16281922464213, 44.08050738932008, 14.684956192885803, 2.1866330685079025, -0.1402560791713545, -0.03504246268278482, -0.0008574567851546854]))
A:jax.scipy.special.q1->list(reversed([1.0, 15.779988325646675, 45.39076351288792, 41.3172038254672, 15.04253856929075, 2.504649462083094, -0.14218292285478779, -0.03808064076915783, -0.0009332594808954574]))
A:jax.scipy.special.p2->list(reversed([3.2377489177694603, 6.915228890689842, 3.9388102529247444, 1.3330346081580755, 0.20148538954917908, 0.012371663481782003, 0.00030158155350823543, 2.6580697468673755e-06, 6.239745391849833e-09]))
A:jax.scipy.special.q2->list(reversed([1.0, 6.02427039364742, 3.6798356385616087, 1.3770209948908132, 0.21623699359449663, 0.013420400608854318, 0.00032801446468212774, 2.8924786474538068e-06, 6.790194080099813e-09]))
A:jax.scipy.special.coeffs->numpy.array(coeffs, dtype)
A:jax.scipy.special.maybe_complement_p->numpy.lax_numpy.where(p > dtype(-np.expm1(-2.0)), dtype(1.0) - p, p)
A:jax.scipy.special.sanitized_mcp->numpy.lax_numpy.where(maybe_complement_p <= dtype(0.0), jnp.full(shape, dtype(0.5)), maybe_complement_p)
A:jax.scipy.special.ww->lax.square(w)
A:jax.scipy.special.infinity->numpy.lax_numpy.full(shape, dtype(np.inf))
A:jax.scipy.special.x_nan_replaced->numpy.lax_numpy.where(p <= dtype(0.0), -infinity, jnp.where(p >= dtype(1.0), infinity, x))
A:jax.scipy.special.ans->log_ndtr(x, series_order=series_order)
A:jax.scipy.special.t_out->lax.mul(t, lax.exp(lax.sub(_norm_logpdf(x), ans)))
A:jax.scipy.special.x_2->lax.square(x)
A:jax.scipy.special.even_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special.odd_sum->numpy.lax_numpy.zeros_like(x)
A:jax.scipy.special._norm_logpdf_constant->numpy.log(np.sqrt(2 * np.pi))
A:jax.scipy.special.neg_half->_constant_like(x, -0.5)
A:jax.scipy.special.log_normalizer->_constant_like(x, _norm_logpdf_constant)
jax.scipy.special._double_factorial(n)
jax.scipy.special._log_ndtr_asymptotic_series(x,series_order)
jax.scipy.special._log_ndtr_jvp(series_order,primals,tangents)
jax.scipy.special._log_ndtr_lower(x,series_order)
jax.scipy.special._ndtr(x)
jax.scipy.special._ndtri(p)
jax.scipy.special._norm_logpdf(x)
jax.scipy.special._polygamma(n,x)
jax.scipy.special.betainc(a,b,x)
jax.scipy.special.betaln(x,y)
jax.scipy.special.digamma(x)
jax.scipy.special.entr(x)
jax.scipy.special.erf(x)
jax.scipy.special.erfc(x)
jax.scipy.special.erfinv(x)
jax.scipy.special.expit(x)
jax.scipy.special.gammainc(a,x)
jax.scipy.special.gammaincc(a,x)
jax.scipy.special.gammaln(x)
jax.scipy.special.i0e(x)
jax.scipy.special.i1e(x)
jax.scipy.special.log_ndtr(x,series_order=3)
jax.scipy.special.logit(x)
jax.scipy.special.logsumexp(a,axis=None,b=None,keepdims=False,return_sign=False)
jax.scipy.special.multigammaln(a,d)
jax.scipy.special.ndtr(x)
jax.scipy.special.ndtri(p)
jax.scipy.special.polygamma(n,x)
jax.scipy.special.xlog1py(x,y)
jax.scipy.special.xlogy(x,y)
jax.scipy.special.zeta(x,q=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/poisson.py----------------------------------------
A:jax.scipy.stats.poisson.(k, mu, loc)->numpy.lax_numpy._promote_args_inexact('poisson.logpmf', k, mu, loc)
A:jax.scipy.stats.poisson.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.poisson.x->lax.sub(k, loc)
jax.scipy.stats.poisson.logpmf(k,mu,loc=0)
jax.scipy.stats.poisson.pmf(k,mu,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/pareto.py----------------------------------------
A:jax.scipy.stats.pareto.(x, b, loc, scale)->_promote_args_inexact('pareto.logpdf', x, b, loc, scale)
A:jax.scipy.stats.pareto.one->_constant_like(x, 1)
A:jax.scipy.stats.pareto.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.pareto.normalize_term->lax.log(lax.div(scale, b))
A:jax.scipy.stats.pareto.log_probs->lax.neg(lax.add(normalize_term, lax.mul(lax.add(b, one), lax.log(scaled_x))))
jax.scipy.stats.pareto.logpdf(x,b,loc=0,scale=1)
jax.scipy.stats.pareto.pdf(x,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/cauchy.py----------------------------------------
A:jax.scipy.stats.cauchy.(x, loc, scale)->_promote_args_inexact('cauchy.logpdf', x, loc, scale)
A:jax.scipy.stats.cauchy.pi->_constant_like(x, np.pi)
A:jax.scipy.stats.cauchy.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.cauchy.normalize_term->lax.log(lax.mul(pi, scale))
jax.scipy.stats.cauchy.logpdf(x,loc=0,scale=1)
jax.scipy.stats.cauchy.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/expon.py----------------------------------------
A:jax.scipy.stats.expon.(x, loc, scale)->_promote_args_inexact('expon.logpdf', x, loc, scale)
A:jax.scipy.stats.expon.log_scale->lax.log(scale)
A:jax.scipy.stats.expon.linear_term->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.expon.log_probs->lax.neg(lax.add(linear_term, log_scale))
jax.scipy.stats.expon.logpdf(x,loc=0,scale=1)
jax.scipy.stats.expon.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/logistic.py----------------------------------------
jax.scipy.stats.logistic.cdf(x)
jax.scipy.stats.logistic.isf(x)
jax.scipy.stats.logistic.logpdf(x)
jax.scipy.stats.logistic.pdf(x)
jax.scipy.stats.logistic.ppf(x)
jax.scipy.stats.logistic.sf(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/laplace.py----------------------------------------
A:jax.scipy.stats.laplace.(x, loc, scale)->_promote_args_inexact('laplace.cdf', x, loc, scale)
A:jax.scipy.stats.laplace.two->_constant_like(x, 2)
A:jax.scipy.stats.laplace.linear_term->lax.div(lax.abs(lax.sub(x, loc)), scale)
A:jax.scipy.stats.laplace.half->_constant_like(x, 0.5)
A:jax.scipy.stats.laplace.one->_constant_like(x, 1)
A:jax.scipy.stats.laplace.zero->_constant_like(x, 0)
A:jax.scipy.stats.laplace.diff->lax.div(lax.sub(x, loc), scale)
jax.scipy.stats.laplace.cdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.logpdf(x,loc=0,scale=1)
jax.scipy.stats.laplace.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/multivariate_normal.py----------------------------------------
A:jax.scipy.stats.multivariate_normal.(x, mean, cov)->_promote_dtypes_inexact(x, mean, cov)
A:jax.scipy.stats.multivariate_normal.L->cholesky(cov)
A:jax.scipy.stats.multivariate_normal.y->triangular_solve(L, x - mean, lower=True, transpose_a=True)
jax.scipy.stats.multivariate_normal.logpdf(x,mean,cov)
jax.scipy.stats.multivariate_normal.pdf(x,mean,cov)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/gamma.py----------------------------------------
A:jax.scipy.stats.gamma.(x, a, loc, scale)->_promote_args_inexact('gamma.logpdf', x, a, loc, scale)
A:jax.scipy.stats.gamma.one->_constant_like(x, 1)
A:jax.scipy.stats.gamma.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.gamma.log_linear_term->lax.sub(lax.mul(lax.sub(a, one), lax.log(y)), y)
A:jax.scipy.stats.gamma.shape_terms->lax.add(gammaln(a), lax.log(scale))
A:jax.scipy.stats.gamma.log_probs->lax.sub(log_linear_term, shape_terms)
jax.scipy.stats.gamma.logpdf(x,a,loc=0,scale=1)
jax.scipy.stats.gamma.pdf(x,a,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/geom.py----------------------------------------
A:jax.scipy.stats.geom.(k, p, loc)->numpy.lax_numpy._promote_args_inexact('geom.logpmf', k, p, loc)
A:jax.scipy.stats.geom.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.geom.one->numpy.lax_numpy._constant_like(k, 1)
A:jax.scipy.stats.geom.x->lax.sub(k, loc)
jax.scipy.stats.geom.logpmf(k,p,loc=0)
jax.scipy.stats.geom.pmf(k,p,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/bernoulli.py----------------------------------------
A:jax.scipy.stats.bernoulli.(k, p, loc)->numpy.lax_numpy._promote_args_inexact('bernoulli.logpmf', k, p, loc)
A:jax.scipy.stats.bernoulli.zero->numpy.lax_numpy._constant_like(k, 0)
A:jax.scipy.stats.bernoulli.one->numpy.lax_numpy._constant_like(k, 1)
A:jax.scipy.stats.bernoulli.x->lax.sub(k, loc)
jax.scipy.stats.bernoulli.logpmf(k,p,loc=0)
jax.scipy.stats.bernoulli.pmf(k,p,loc=0)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/dirichlet.py----------------------------------------
A:jax.scipy.stats.dirichlet.x_sum->numpy.lax_numpy.sum(x, axis=-1)
A:jax.scipy.stats.dirichlet.to_dtype->lax.dtype(osp_stats.dirichlet.logpdf(*args))
A:jax.scipy.stats.dirichlet.one->numpy.lax_numpy._constant_like(x, 1)
A:jax.scipy.stats.dirichlet.log_probs->lax.sub(jnp.sum(xlogy(lax.sub(alpha, one), x), axis=-1), normalize_term)
jax.scipy.stats.dirichlet._is_simplex(x)
jax.scipy.stats.dirichlet.logpdf(x,alpha)
jax.scipy.stats.dirichlet.pdf(x,alpha)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/uniform.py----------------------------------------
A:jax.scipy.stats.uniform.(x, loc, scale)->_promote_args_inexact('uniform.logpdf', x, loc, scale)
A:jax.scipy.stats.uniform.log_probs->lax.neg(lax.log(scale))
jax.scipy.stats.uniform.logpdf(x,loc=0,scale=1)
jax.scipy.stats.uniform.pdf(x,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/t.py----------------------------------------
A:jax.scipy.stats.t.(x, df, loc, scale)->_promote_args_inexact('t.logpdf', x, df, loc, scale)
A:jax.scipy.stats.t.two->_constant_like(x, 2)
A:jax.scipy.stats.t.scaled_x->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.t.df_over_two->lax.div(df, two)
A:jax.scipy.stats.t.df_plus_one_over_two->lax.add(df_over_two, _constant_like(x, 0.5))
A:jax.scipy.stats.t.normalize_term_const->lax.mul(lax.mul(scale, scale), _constant_like(x, np.pi))
A:jax.scipy.stats.t.normalize_term_tmp->lax.div(lax.log(lax.mul(normalize_term_const, df)), two)
A:jax.scipy.stats.t.normalize_term->lax.sub(lax.add(lax.lgamma(df_over_two), normalize_term_tmp), lax.lgamma(df_plus_one_over_two))
A:jax.scipy.stats.t.quadratic->lax.div(lax.mul(scaled_x, scaled_x), df)
jax.scipy.stats.t.logpdf(x,df,loc=0,scale=1)
jax.scipy.stats.t.pdf(x,df,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/beta.py----------------------------------------
A:jax.scipy.stats.beta.(x, a, b, loc, scale)->_promote_args_inexact('beta.logpdf', x, a, b, loc, scale)
A:jax.scipy.stats.beta.one->_constant_like(x, 1)
A:jax.scipy.stats.beta.shape_term->lax.neg(betaln(a, b))
A:jax.scipy.stats.beta.y->lax.div(lax.sub(x, loc), scale)
A:jax.scipy.stats.beta.log_linear_term->lax.add(lax.mul(lax.sub(a, one), lax.log(y)), lax.mul(lax.sub(b, one), lax.log1p(lax.neg(y))))
A:jax.scipy.stats.beta.log_probs->lax.sub(lax.add(shape_term, log_linear_term), lax.log(scale))
jax.scipy.stats.beta.logpdf(x,a,b,loc=0,scale=1)
jax.scipy.stats.beta.pdf(x,a,b,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/stats/norm.py----------------------------------------
A:jax.scipy.stats.norm.(x, loc, scale)->_promote_args_inexact('norm.logcdf', x, loc, scale)
A:jax.scipy.stats.norm.two->_constant_like(x, 2)
A:jax.scipy.stats.norm.scale_sqrd->lax.pow(scale, two)
A:jax.scipy.stats.norm.log_normalizer->lax.log(lax.mul(_constant_like(x, 2 * np.pi), scale_sqrd))
A:jax.scipy.stats.norm.quadratic->lax.div(lax.pow(lax.sub(x, loc), two), scale_sqrd)
jax.scipy.stats.norm.cdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logcdf(x,loc=0,scale=1)
jax.scipy.stats.norm.logpdf(x,loc=0,scale=1)
jax.scipy.stats.norm.pdf(x,loc=0,scale=1)
jax.scipy.stats.norm.ppf(q,loc=0,scale=1)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/sparse/linalg.py----------------------------------------
A:jax.scipy.sparse.linalg.vdot->partial(jnp.vdot, precision=lax.Precision.HIGHEST)
A:jax.scipy.sparse.linalg.result->vdot(x.real, y.real)
A:jax.scipy.sparse.linalg._add->partial(tree_multimap, operator.add)
A:jax.scipy.sparse.linalg._sub->partial(tree_multimap, operator.sub)
A:jax.scipy.sparse.linalg.bs->_vdot_tree(b, b)
A:jax.scipy.sparse.linalg.atol2->jax.numpy.maximum(jnp.square(tol) * bs, jnp.square(atol))
A:jax.scipy.sparse.linalg.Ap->A(p)
A:jax.scipy.sparse.linalg.x_->_add(x, _mul(alpha, p))
A:jax.scipy.sparse.linalg.r_->_sub(r, _mul(alpha, Ap))
A:jax.scipy.sparse.linalg.z_->M(r_)
A:jax.scipy.sparse.linalg.gamma_->_vdot_tree(r_, z_)
A:jax.scipy.sparse.linalg.p_->_add(z_, _mul(beta_, p))
A:jax.scipy.sparse.linalg.r0->_sub(b, A(x0))
A:jax.scipy.sparse.linalg.p0z0->M(r0)
A:jax.scipy.sparse.linalg.gamma0->_vdot_tree(r0, z0)
A:jax.scipy.sparse.linalg.(x_final, *_)->jax.lax.while_loop(cond_fun, body_fun, initial_value)
A:jax.scipy.sparse.linalg.x0->tree_map(jnp.zeros_like, b)
A:jax.scipy.sparse.linalg.(b, x0)->device_put((b, x0))
A:jax.scipy.sparse.linalg.size->sum((bi.size for bi in tree_leaves(b)))
A:jax.scipy.sparse.linalg.cg_solve->partial(_cg_solve, x0=x0, tol=tol, atol=atol, maxiter=maxiter, M=M)
A:jax.scipy.sparse.linalg.symmetric->all(map(real_valued, tree_leaves(b)))
A:jax.scipy.sparse.linalg.x->jax.lax.custom_linear_solve(A, b, solve=cg_solve, transpose_solve=cg_solve, symmetric=symmetric)
jax.scipy.sparse.linalg._cg_solve(A,b,x0=None,*,maxiter,tol=1e-05,atol=0.0,M=_identity)
jax.scipy.sparse.linalg._identity(x)
jax.scipy.sparse.linalg._mul(scalar,tree)
jax.scipy.sparse.linalg._shapes(pytree)
jax.scipy.sparse.linalg._vdot_real_part(x,y)
jax.scipy.sparse.linalg._vdot_tree(x,y)
jax.scipy.sparse.linalg.cg(A,b,x0=None,*,tol=1e-05,atol=0.0,maxiter=None,M=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/sparse/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/optimize/_line_search.py----------------------------------------
A:jax.scipy.optimize._line_search.d1->jax.numpy.array([[dc ** 2, -db ** 2], [-dc ** 3, db ** 3]])
A:jax.scipy.optimize._line_search.keys->new_dict.keys()
A:jax.scipy.optimize._line_search.out->dict()
A:jax.scipy.optimize._line_search.out[key]->jax.numpy.where(replace_bit, new_dict[key], original_dict[key])
A:jax.scipy.optimize._line_search.state->while_loop(lambda state: ~state.done & (state.i <= maxiter) & ~state.failed, body, state)
A:jax.scipy.optimize._line_search.a->jax.numpy.minimum(state.a_hi, state.a_lo)
A:jax.scipy.optimize._line_search.b->jax.numpy.maximum(state.a_hi, state.a_lo)
A:jax.scipy.optimize._line_search.a_j_cubic->_cubicmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi, state.a_rec, state.phi_rec)
A:jax.scipy.optimize._line_search.a_j_quad->_quadmin(state.a_lo, state.phi_lo, state.dphi_lo, state.a_hi, state.phi_hi)
A:jax.scipy.optimize._line_search.a_j->jax.numpy.where(use_bisection, a_j_bisection, a_j)
A:jax.scipy.optimize._line_search.(phi_j, dphi_j, g_j)->restricted_func_and_grad(a_j)
A:jax.scipy.optimize._line_search.(phi, g)->jax.value_and_grad(f)(xk + t * pk)
A:jax.scipy.optimize._line_search.dphi->jax.numpy.dot(g, pk)
A:jax.scipy.optimize._line_search.(phi_0, dphi_0, gfk)->restricted_func_and_grad(0.0)
A:jax.scipy.optimize._line_search.dphi_0->jax.numpy.dot(gfk, pk)
A:jax.scipy.optimize._line_search.a_i->jax.numpy.where(state.i == 1, 1.0, state.a_i1 * 2.0)
A:jax.scipy.optimize._line_search.(phi_i, dphi_i, g_i)->restricted_func_and_grad(a_i)
A:jax.scipy.optimize._line_search.zoom1->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, state.a_i1, state.phi_i1, state.dphi_i1, a_i, phi_i, dphi_i, gfk, ~star_to_zoom1)
A:jax.scipy.optimize._line_search.zoom2->_zoom(restricted_func_and_grad, wolfe_one, wolfe_two, a_i, phi_i, dphi_i, state.a_i1, state.phi_i1, state.dphi_i1, gfk, ~star_to_zoom2)
A:jax.scipy.optimize._line_search.status->jax.numpy.where(state.failed & ~state.saddle_point, jnp.array(1), jnp.where(state.failed & state.saddle_point, jnp.array(2), jnp.where(state.i > maxiter, jnp.array(3), jnp.array(0))))
A:jax.scipy.optimize._line_search.results->_LineSearchResults(failed=state.failed | ~state.done, nit=state.i - 1, nfev=state.nfev, ngev=state.ngev, k=state.i, a_k=state.a_star, f_k=state.phi_star, g_k=state.g_star, status=status)
jax.scipy.optimize._line_search._LineSearchResults(NamedTuple)
jax.scipy.optimize._line_search._LineSearchState(NamedTuple)
jax.scipy.optimize._line_search._ZoomState(NamedTuple)
jax.scipy.optimize._line_search._binary_replace(replace_bit,original_dict,new_dict,keys=None)
jax.scipy.optimize._line_search._cubicmin(a,fa,fpa,b,fb,c,fc)
jax.scipy.optimize._line_search._quadmin(a,fa,fpa,b,fb)
jax.scipy.optimize._line_search._zoom(restricted_func_and_grad,wolfe_one,wolfe_two,a_lo,phi_lo,dphi_lo,a_hi,phi_hi,dphi_hi,g_0,pass_through)
jax.scipy.optimize._line_search.line_search(f,xk,pk,old_fval=None,old_old_fval=None,gfk=None,c1=0.0001,c2=0.9,maxiter=20)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/optimize/_bfgs.py----------------------------------------
A:jax.scipy.optimize._bfgs._dot->partial(jnp.dot, precision=lax.Precision.HIGHEST)
A:jax.scipy.optimize._bfgs._einsum->partial(jnp.einsum, precision=lax.Precision.HIGHEST)
A:jax.scipy.optimize._bfgs.initial_H->jax.numpy.eye(d)
A:jax.scipy.optimize._bfgs.(f_0, g_0)->jax.value_and_grad(fun)(x0)
A:jax.scipy.optimize._bfgs.state->state._replace(status=status)._replace(status=status)
A:jax.scipy.optimize._bfgs.line_search_results->line_search(fun, state.x_k, p_k, old_fval=state.f_k, gfk=state.g_k, maxiter=line_search_maxiter)
A:jax.scipy.optimize._bfgs.rho_k->jax.numpy.reciprocal(_dot(y_k, s_k))
A:jax.scipy.optimize._bfgs.H_kp1->jax.numpy.where(jnp.isfinite(rho_k), H_kp1, state.H_k)
A:jax.scipy.optimize._bfgs.status->jax.numpy.where(state.converged, 0, jnp.where(state.k == maxiter, 1, jnp.where(state.failed, 2 + state.line_search_status, -1)))
jax.scipy.optimize._bfgs._BFGSResults(NamedTuple)
jax.scipy.optimize._bfgs.minimize_bfgs(fun:Callable,x0:jnp.ndarray,maxiter:Optional[int]=None,norm=jnp.inf,gtol:float=1e-05,line_search_maxiter:int=10)->_BFGSResults


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/optimize/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/scipy/optimize/_minimize.py----------------------------------------
A:jax.scipy.optimize._minimize.fun_with_args->partial(fun, *args)
A:jax.scipy.optimize._minimize.results->minimize_bfgs(fun_with_args, x0, **options)
jax.scipy.optimize._minimize.OptimizeResults(NamedTuple)
jax.scipy.optimize._minimize.minimize(fun:Callable,x0:jnp.ndarray,args:Tuple=(),*,method:str,tol:Optional[float]=None,options:Optional[Mapping[str,Any]]=None)->OptimizeResults
jax.scipy.optimize.minimize(fun:Callable,x0:jnp.ndarray,args:Tuple=(),*,method:str,tol:Optional[float]=None,options:Optional[Mapping[str,Any]]=None)->OptimizeResults


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lib/xla_bridge.py----------------------------------------
A:jax.lib.xla_bridge.compile_options->xla_client.CompileOptions()
A:jax.lib.xla_bridge.device_assignment->xla_client.DeviceAssignment.create(device_assignment)
A:jax.lib.xla_bridge.backend->_backends.get(FLAGS.jax_xla_backend)
A:jax.lib.xla_bridge._tpu_backend->tpu_client.TpuBackend.create(worker=backend_target)
A:jax.lib.xla_bridge._backend_lock->threading.Lock()
A:jax.lib.xla_bridge.host_id->get_backend(backend).host_id()
A:jax.lib.xla_bridge.value->normalize_to_xla_dtypes(value)
A:jax.lib.xla_bridge.py_type->type(py_val)
A:jax.lib.xla_bridge.proto->xla_client.OpSharding()
A:jax.lib.xla_bridge.proto.tile_assignment_dimensions->list(sharding)
A:jax.lib.xla_bridge.proto.tile_assignment_devices->list(range(np.product(sharding)))
A:jax.lib.xla_bridge.(zero_stride_axes,)->numpy.where(np.equal(0, val.strides))
A:jax.lib.xla_bridge.(other_axes,)->numpy.where(np.not_equal(0, val.strides))
A:jax.lib.xla_bridge.xla_val->xops.Broadcast(_numpy_array_constant(c, collapsed_val, canonicalize_types), np.take(val.shape, zero_stride_axes))
A:jax.lib.xla_bridge.permutation->numpy.argsort(tuple(zero_stride_axes) + tuple(other_axes))
jax.lib.xla_bridge._get_local_backend(platform=None)
jax.lib.xla_bridge._get_tpu_driver_backend(platform)
jax.lib.xla_bridge._ndarray_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge._numpy_array_constant(builder,value,canonicalize_types=True)
jax.lib.xla_bridge._python_scalar_handler(dtype,c,val,canonicalize_dtypes=True)
jax.lib.xla_bridge._scalar_constant_handler(c,val,canonicalize_types=True)
jax.lib.xla_bridge._sharding_to_proto(sharding:SpatialSharding)
jax.lib.xla_bridge.constant(builder,py_val,canonicalize_types=True)
jax.lib.xla_bridge.device_count(backend:str=None)
jax.lib.xla_bridge.devices(backend:str=None)
jax.lib.xla_bridge.dtype_to_etype(dtype)
jax.lib.xla_bridge.get_backend(platform=None)
jax.lib.xla_bridge.get_compile_options(num_replicas,num_partitions,device_assignment=None,use_spmd_partitioning=True)
jax.lib.xla_bridge.get_device_backend(device=None)
jax.lib.xla_bridge.host_count(backend:str=None)
jax.lib.xla_bridge.host_id(backend:str=None)
jax.lib.xla_bridge.host_ids(backend:str=None)
jax.lib.xla_bridge.local_device_count(backend:str=None)
jax.lib.xla_bridge.local_devices(host_id:int=None,backend:str=None)
jax.lib.xla_bridge.make_computation_builder(name)
jax.lib.xla_bridge.normalize_to_xla_dtypes(val)
jax.lib.xla_bridge.parameter(builder,num,shape,name=None,replicated=None)
jax.lib.xla_bridge.register_backend(name,factory)
jax.lib.xla_bridge.register_constant_handler(type_,handler_fun)
jax.lib.xla_bridge.set_sharding(builder,op,sharding:SpatialSharding)
jax.lib.xla_bridge.supported_numpy_dtypes()
jax.lib.xla_bridge.with_sharding(builder,sharding:SpatialSharding,op_fn,*args,**kwargs)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lib/__init__.py----------------------------------------
A:jax.lib.__init__.version->tuple((int(x) for x in jaxlib_version.__version__.split('.')))
jax.lib.__init__._check_jaxlib_version()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/image/scale.py----------------------------------------
A:jax.image.scale.x->numpy.abs(x)
A:jax.image.scale.out->numpy.where(x >= 1.0, ((-0.5 * x + 2.5) * x - 4.0) * x + 2.0, out)
A:jax.image.scale.span_size->min(2 * int(math.ceil(radius * kernel_scale)) + 1, input_size)
A:jax.image.scale.span_start->numpy.clip(span_start, 0, input_size - span_size)
A:jax.image.scale.weight->kernel_fn(kernel_pos / kernel_scale)
A:jax.image.scale.total_weight_sum->numpy.sum(weight, axis=1, keepdims=True)
A:jax.image.scale.weights->numpy.where(np.abs(total_weight_sum) > 1000.0 * np.finfo(np.float32).min, weight / total_weight_sum, 0)
A:jax.image.scale.(spatial_dims,)->numpy.nonzero(np.not_equal(input_shape, output_shape))
A:jax.image.scale.in_indices->list(range(len(output_shape)))
A:jax.image.scale.out_indices->list(range(len(output_shape)))
A:jax.image.scale.(starts, span_weights)->_compute_spans(m, n, scale[d], translate[d], kernel, antialias=antialias)
A:jax.image.scale.dnums->jax.lax.ScatterDimensionNumbers(update_window_dims=(1,), inserted_window_dims=(1,), scatter_dims_to_operand_dims=(0, 1))
A:jax.image.scale.w->jax.lax.scatter_add(jnp.zeros((m, n), x.dtype), np.stack([starts, np.arange(n)], axis=-1), span_weights.astype(x.dtype), dnums)
A:jax.image.scale.indices[d]->numpy.floor(offsets).astype(np.int32)
A:jax.image.scale._kernels[ResizeMethod.LINEAR]->_triangle_kernel()
A:jax.image.scale._kernels[ResizeMethod.LANCZOS3]->_lanczos_kernel(3.0)
A:jax.image.scale._kernels[ResizeMethod.LANCZOS5]->_lanczos_kernel(5.0)
A:jax.image.scale._kernels[ResizeMethod.CUBIC]->_keys_cubic_kernel()
A:jax.image.scale.method->ResizeMethod.from_string(method)
A:jax.image.scale.image->jax.lax.convert_element_type(image, jnp.result_type(image, jnp.float32))
jax.image.ResizeMethod(enum.Enum)
jax.image.resize(image,shape:Sequence[int],method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)
jax.image.scale.ResizeMethod(enum.Enum)
jax.image.scale.ResizeMethod.from_string(s:str)
jax.image.scale._compute_spans(input_size:int,output_size:int,scale:float,translate:float,kernel:Tuple[float,Callable[[np.ndarray],np.ndarray]],antialias:bool)->Tuple[np.ndarray, np.ndarray]
jax.image.scale._keys_cubic_kernel()
jax.image.scale._lanczos_kernel(radius:float)
jax.image.scale._resize(image,shape:Sequence[int],method:Union[str,ResizeMethod],antialias:bool,precision)
jax.image.scale._resize_nearest(x,output_shape)
jax.image.scale._scale_and_translate(x,output_shape,scale,translate,kernel,antialias,precision)
jax.image.scale._triangle_kernel()
jax.image.scale.resize(image,shape:Sequence[int],method:Union[str,ResizeMethod],antialias:bool=True,precision=lax.Precision.HIGHEST)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/image/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/tools/jax_to_hlo.py----------------------------------------
A:jax.tools.jax_to_hlo.shape_with_default_layout->jax.lib.xla_client.Shape.array_shape(shape.xla_element_type(), shape.dimensions()).with_major_to_minor_layout_if_absent()
A:jax.tools.jax_to_hlo.fn_curried->functools.partial(fn, **constants)
A:jax.tools.jax_to_hlo.comp->jax.api.xla_computation(ordered_wrapper)(*args)
A:jax.tools.jax_to_hlo.(module_name, fn_name)->FLAGS.fn.rsplit('.', 1)
A:jax.tools.jax_to_hlo.module->importlib.import_module(module_name)
A:jax.tools.jax_to_hlo.fn->getattr(module, fn_name)
A:jax.tools.jax_to_hlo.v->jax.numpy.asarray(v)
A:jax.tools.jax_to_hlo.(hlo_proto, hlo_text)->jax_to_hlo(fn, input_shapes, constants)
jax.tools.jax_to_hlo.jax_to_hlo(fn,input_shapes,constants=None)
jax.tools.jax_to_hlo.main(argv)
jax.tools.jax_to_hlo.set_up_flags()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/tools/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/doubledouble.py----------------------------------------
A:jax.experimental.doubledouble._zero->_Zeros()
A:jax.experimental.doubledouble.func->doubling_rules.get(primitive, None)
A:jax.experimental.doubledouble.out->tree_unflatten(out_tree(), out_flat)
A:jax.experimental.doubledouble.(heads, tails)->unzip2(((t.head, t.tail) for t in tracers))
A:jax.experimental.doubledouble.(nonzero_tails, in_tree_def)->tree_flatten(tails)
A:jax.experimental.doubledouble.(f_double, out_tree_def)->screen_nones(doubling_subtrace(f, self.master), len(heads), in_tree_def)
A:jax.experimental.doubledouble.name->params.get('name', f.__name__)
A:jax.experimental.doubledouble.new_params->dict(params, name=wrap_name(name, 'doubledouble'), donated_invars=(False,) * (len(heads) + len(nonzero_tails)))
A:jax.experimental.doubledouble.result->call_primitive.bind(f_double, *heads, *nonzero_tails, **new_params)
A:jax.experimental.doubledouble.(heads_out, tails_out)->tree_unflatten(out_tree_def(), result)
A:jax.experimental.doubledouble.trace->DoublingTrace(master, core.cur_sublevel())
A:jax.experimental.doubledouble.out_tracers->map(trace.full_raise, outputs)
A:jax.experimental.doubledouble.new_tails->tree_unflatten(in_tree_def, new_tails)
A:jax.experimental.doubledouble.(out_flat, tree_def)->tree_flatten((head_out, tail_out))
A:jax.experimental.doubledouble.out_tracer->DoublingTrace(master, core.cur_sublevel()).full_raise(outputs)
A:jax.experimental.doubledouble.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.doubledouble.(f_flat, out_tree)->flatten_fun_nokwargs(lu.wrap_init(f), in_tree)
A:jax.experimental.doubledouble.out_pairs_flat->doubling_transform(f_flat).call_wrapped(*arg_pairs)
A:jax.experimental.doubledouble.z->jax.numpy.where(jnp.isinf(x), x, x + y)
A:jax.experimental.doubledouble.zz->jax.numpy.where(lax.abs(x) > lax.abs(y), x - z + y, y - z + x)
A:jax.experimental.doubledouble.sign->jax.numpy.where(lax.sign(x) == lax.sign(xx), 1, -1)
A:jax.experimental.doubledouble.s->jax.numpy.where(lax.abs(x) > lax.abs(y), x - r - y - yy + xx, -y - r + x + xx - yy)
A:jax.experimental.doubledouble.dtype->jax.numpy.result_type(x, y)
A:jax.experimental.doubledouble.K->_mul_const(dtype)
A:jax.experimental.doubledouble.(c, cc)->_mul12(x, y)
A:jax.experimental.doubledouble.(u, uu)->_mul12(c, c)
A:jax.experimental.doubledouble.c->jax.lax.sqrt(x)
A:jax.experimental.doubledouble.(z, zz)->_sub2(x, y)
A:jax.experimental.doubledouble.head->head.astype(dtype).astype(dtype)
A:jax.experimental.doubledouble.tail->tail.astype(dtype).astype(dtype)
A:jax.experimental.doubledouble.val->decimal.Decimal(val)
A:jax.experimental.doubledouble.(self.head, self.tail)->_normalize(head, tail)
jax.experimental.doubledouble.DoublingTrace(core.Trace)
jax.experimental.doubledouble.DoublingTrace.lift(self,val:core.Tracer)
jax.experimental.doubledouble.DoublingTrace.process_call(self,call_primitive,f,tracers,params)
jax.experimental.doubledouble.DoublingTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.doubledouble.DoublingTrace.pure(self,val:Any)
jax.experimental.doubledouble.DoublingTrace.sublift(self,val:DoublingTracer)
jax.experimental.doubledouble.DoublingTracer(self,trace,head,tail)
jax.experimental.doubledouble.DoublingTracer.aval(self)
jax.experimental.doubledouble.DoublingTracer.full_lower(self)
jax.experimental.doubledouble._DoubleDouble(self,val,dtype=None)
jax.experimental.doubledouble._DoubleDouble.__abs__(self)
jax.experimental.doubledouble._DoubleDouble.__add__(self,other)
jax.experimental.doubledouble._DoubleDouble.__eq__(self,other)
jax.experimental.doubledouble._DoubleDouble.__ge__(self,other)
jax.experimental.doubledouble._DoubleDouble.__gt__(self,other)
jax.experimental.doubledouble._DoubleDouble.__le__(self,other)
jax.experimental.doubledouble._DoubleDouble.__lt__(self,other)
jax.experimental.doubledouble._DoubleDouble.__mul__(self,other)
jax.experimental.doubledouble._DoubleDouble.__ne__(self,other)
jax.experimental.doubledouble._DoubleDouble.__neg__(self)
jax.experimental.doubledouble._DoubleDouble.__radd__(self,other)
jax.experimental.doubledouble._DoubleDouble.__repr__(self)
jax.experimental.doubledouble._DoubleDouble.__rmul__(self,other)
jax.experimental.doubledouble._DoubleDouble.__rsub__(self,other)
jax.experimental.doubledouble._DoubleDouble.__rtruediv__(self,other)
jax.experimental.doubledouble._DoubleDouble.__sub__(self,other)
jax.experimental.doubledouble._DoubleDouble.__truediv__(self,other)
jax.experimental.doubledouble._DoubleDouble._tup(self)
jax.experimental.doubledouble._DoubleDouble._wrap(self,other)
jax.experimental.doubledouble._DoubleDouble.dtype(self)
jax.experimental.doubledouble._DoubleDouble.normalize(self)
jax.experimental.doubledouble._DoubleDouble.to_array(self,dtype=None)
jax.experimental.doubledouble._Zeros
jax.experimental.doubledouble._Zeros.__repr__(self)
jax.experimental.doubledouble._abs2(x)
jax.experimental.doubledouble._add2(x,y)
jax.experimental.doubledouble._add_jaxvals(xs,ys)
jax.experimental.doubledouble._convert_element_type(operand,new_dtype,old_dtype)
jax.experimental.doubledouble._def_inequality(prim,op)
jax.experimental.doubledouble._def_passthrough(prim,argnums=(0,))
jax.experimental.doubledouble._div2(x,y)
jax.experimental.doubledouble._mul12(x,y)
jax.experimental.doubledouble._mul2(x,y)
jax.experimental.doubledouble._mul_const(dtype)
jax.experimental.doubledouble._neg2(x)
jax.experimental.doubledouble._normalize(x,y)
jax.experimental.doubledouble._sqrt2(x)
jax.experimental.doubledouble._sub2(x,y)
jax.experimental.doubledouble.doubledouble(f)
jax.experimental.doubledouble.doubling_subtrace(master,heads,tails)
jax.experimental.doubledouble.doubling_transform(*args)
jax.experimental.doubledouble.screen_nones(num_heads,in_tree_def,*heads_and_tails)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/optix.py----------------------------------------
A:jax.experimental.optix.updates->tree_multimap(lambda ga: emit * ga, grad_acc)
A:jax.experimental.optix.g_norm->global_norm(updates)
A:jax.experimental.optix.update_trace->tree_multimap(f, updates, state.trace)
A:jax.experimental.optix.nu->_update_moment(updates, state.nu, b2, 2)
A:jax.experimental.optix.mu->_update_moment(updates, state.mu, b1, 1)
A:jax.experimental.optix.mu_hat->tree_multimap(lambda t: t / (1 - b1 ** (state.count + 1)), mu)
A:jax.experimental.optix.nu_hat->tree_multimap(lambda t: t / (1 - b2 ** (state.count + 1)), nu)
A:jax.experimental.optix.num_vars->len(tree_leaves(updates))
A:jax.experimental.optix.treedef->tree_structure(updates)
A:jax.experimental.optix.all_keys->jax.random.split(state.rng_key, num=num_vars + 1)
A:jax.experimental.optix.noise->tree_multimap(lambda g, k: jrandom.normal(k, shape=g.shape), updates, tree_unflatten(treedef, all_keys[1:]))
A:jax.experimental.optix.grad_acc->tree_multimap(lambda g, ga: acc * ga + g, updates, state.grad_acc)
A:jax.experimental.optix.(init_fns, update_fns)->zip(*args)
A:jax.experimental.optix.(updates, new_s)->fn(updates, s, params)
jax.experimental.optix.AddNoiseState(OptState)
jax.experimental.optix.ApplyEvery(OptState)
jax.experimental.optix.ClipByGlobalNormState(OptState)
jax.experimental.optix.ClipState(OptState)
jax.experimental.optix.GradientTransformation(NamedTuple)
jax.experimental.optix.ScaleByAdamState(OptState)
jax.experimental.optix.ScaleByRStdDevState(OptState)
jax.experimental.optix.ScaleByRmsState(OptState)
jax.experimental.optix.ScaleByScheduleState(OptState)
jax.experimental.optix.ScaleState(NamedTuple)
jax.experimental.optix.TraceState(OptState)
jax.experimental.optix._update_moment(updates,moments,decay,order)
jax.experimental.optix.adam(learning_rate:float,b1:float=0.9,b2:float=0.999,eps:float=1e-08)->GradientTransformation
jax.experimental.optix.add_noise(eta:float,gamma:float,seed:int)->GradientTransformation
jax.experimental.optix.apply_every(k:int=1)->GradientTransformation
jax.experimental.optix.apply_updates(params:Params,updates:Updates)->Params
jax.experimental.optix.chain(*args:GradientTransformation)->GradientTransformation
jax.experimental.optix.clip(max_delta)->GradientTransformation
jax.experimental.optix.clip_by_global_norm(max_norm)->GradientTransformation
jax.experimental.optix.global_norm(updates:Updates)->Updates
jax.experimental.optix.noisy_sgd(learning_rate:float,eta:float=0.01,gamma:float=0.55,seed:int=0)->GradientTransformation
jax.experimental.optix.rmsprop(learning_rate:float,decay:float=0.9,eps:float=1e-08,centered:bool=False)->GradientTransformation
jax.experimental.optix.scale(step_size:float)->GradientTransformation
jax.experimental.optix.scale_by_adam(b1:float=0.9,b2:float=0.999,eps:float=1e-08,eps_root:float=0.0)->GradientTransformation
jax.experimental.optix.scale_by_rms(decay:float=0.9,eps:float=1e-08)
jax.experimental.optix.scale_by_schedule(step_size_fn:Callable[[jnp.ndarray],jnp.ndarray])
jax.experimental.optix.scale_by_stddev(decay:float=0.9,eps:float=1e-08)->GradientTransformation
jax.experimental.optix.sgd(learning_rate:float,momentum:float=0.0,nesterov:bool=False)->GradientTransformation
jax.experimental.optix.trace(decay:float,nesterov:bool)->GradientTransformation


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/ode.py----------------------------------------
A:jax.experimental.ode.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.experimental.ode.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(wrapped_fun, in_pvals, instantiate=True, stage_out=False)
A:jax.experimental.ode.out_tree->out_tree()
A:jax.experimental.ode.((closure_consts, hoisted_consts), merge)->partition_list(is_float, consts)
A:jax.experimental.ode.num_consts->len(hoisted_consts)
A:jax.experimental.ode.(hoisted_consts, args)->split_list(hconsts_args, [num_consts])
A:jax.experimental.ode.consts->merge(closure_consts, hoisted_consts)
A:jax.experimental.ode.(all_args, in_tree2)->tree_flatten((y, t, *args))
A:jax.experimental.ode.out_flat->jax.core.eval_jaxpr(jaxpr, consts, *all_args)
A:jax.experimental.ode.y->unravel(y_flat)
A:jax.experimental.ode.(ans_flat, _)->ravel_pytree(ans)
A:jax.experimental.ode.dps_c_mid->jax.numpy.array([6025192743 / 30085553152 / 2, 0, 51252292925 / 65400821598 / 2, -2691868925 / 45128329728 / 2, 187940372067 / 1594534317056 / 2, -1776094331 / 19743644256 / 2, 11237099 / 235043384 / 2])
A:jax.experimental.ode.d0->jax.numpy.linalg.norm(y0 / scale)
A:jax.experimental.ode.d1->jax.numpy.linalg.norm(f0 / scale)
A:jax.experimental.ode.h0->jax.numpy.where((d0 < 1e-05) | (d1 < 1e-05), 1e-06, 0.01 * d0 / d1)
A:jax.experimental.ode.f1->fun(y1, t0 + h0)
A:jax.experimental.ode.h1->jax.numpy.where((d1 <= 1e-15) & (d2 <= 1e-15), jnp.maximum(1e-06, h0 * 0.001), (0.01 / jnp.max(d1 + d2)) ** (1.0 / (order + 1.0)))
A:jax.experimental.ode.alpha->jax.numpy.array([1 / 5, 3 / 10, 4 / 5, 8 / 9, 1.0, 1.0, 0])
A:jax.experimental.ode.beta->jax.numpy.array([[1 / 5, 0, 0, 0, 0, 0, 0], [3 / 40, 9 / 40, 0, 0, 0, 0, 0], [44 / 45, -56 / 15, 32 / 9, 0, 0, 0, 0], [19372 / 6561, -25360 / 2187, 64448 / 6561, -212 / 729, 0, 0, 0], [9017 / 3168, -355 / 33, 46732 / 5247, 49 / 176, -5103 / 18656, 0, 0], [35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0]])
A:jax.experimental.ode.c_sol->jax.numpy.array([35 / 384, 0, 500 / 1113, 125 / 192, -2187 / 6784, 11 / 84, 0])
A:jax.experimental.ode.c_error->jax.numpy.array([35 / 384 - 1951 / 21600, 0, 500 / 1113 - 22642 / 50085, 125 / 192 - 451 / 720, -2187 / 6784 - -12231 / 42400, 11 / 84 - 649 / 6300, -1.0 / 60.0])
A:jax.experimental.ode.ft->func(yi, ti)
A:jax.experimental.ode.k->jax.lax.fori_loop(1, 7, body_fun, k)
A:jax.experimental.ode.mean_error_ratio->jax.numpy.max(mean_error_ratio)
A:jax.experimental.ode.dfactor->jax.numpy.where(mean_error_ratio < 1, 1.0, dfactor)
A:jax.experimental.ode.err_ratio->jax.numpy.sqrt(mean_error_ratio)
A:jax.experimental.ode.factor->jax.numpy.maximum(1.0 / ifactor, jnp.minimum(err_ratio ** (1.0 / order) / safety, 1.0 / dfactor))
A:jax.experimental.ode.(flat_args, in_tree)->tree_flatten((y0, t[0], *args))
A:jax.experimental.ode.in_avals->tuple(map(abstractify, flat_args))
A:jax.experimental.ode.(converted, consts)->closure_convert(func, in_tree, in_avals)
A:jax.experimental.ode.(y0, unravel)->ravel_pytree(y0)
A:jax.experimental.ode.func->ravel_first_arg(func, unravel)
A:jax.experimental.ode.out->_odeint(func, rtol, atol, mxstep, y0, ts, *args)
A:jax.experimental.ode.(next_y, next_f, next_y_error, k)->runge_kutta_step(func_, y, f, t, dt)
A:jax.experimental.ode.error_ratios->error_ratio(next_y_error, rtol, atol, y, next_y)
A:jax.experimental.ode.new_interp_coeff->interp_fit_dopri(y, next_y, k, dt)
A:jax.experimental.ode.dt->initial_step_size(func_, ts[0], y0, 4, rtol, atol, f0)
A:jax.experimental.ode.(_, *carry)->jax.lax.while_loop(cond_fun, body_fun, [0] + carry)
A:jax.experimental.ode.y_target->jax.numpy.polyval(interp_coeff, relative_output_time)
A:jax.experimental.ode.f0->func_(y0, ts[0])
A:jax.experimental.ode.interp_coeff->jax.numpy.array([y0] * 5)
A:jax.experimental.ode.(_, ys)->jax.lax.scan(scan_fun, init_carry, ts[1:])
A:jax.experimental.ode.ys->_odeint(func, rtol, atol, mxstep, y0, ts, *args)
A:jax.experimental.ode.(y_dot, vjpfun)->jax.vjp(func, y, -t, *args)
A:jax.experimental.ode.t_bar->jax.numpy.dot(func(ys[i], ts[i], *args), g[i])
A:jax.experimental.ode.(_, y_bar, t0_bar, args_bar)->odeint(aug_dynamics, (ys[i], y_bar, t0_bar, args_bar), jnp.array([-ts[i], -ts[i - 1]]), *args, rtol=rtol, atol=atol, mxstep=mxstep)
A:jax.experimental.ode.(y_bar, t0_bar, args_bar)->tree_map(op.itemgetter(1), (y_bar, t0_bar, args_bar))
A:jax.experimental.ode.((y_bar, t0_bar, args_bar), rev_ts_bar)->jax.lax.scan(scan_fun, init_carry, jnp.arange(len(ts) - 1, 0, -1))
A:jax.experimental.ode.ts_bar->jax.numpy.concatenate([jnp.array([t0_bar]), rev_ts_bar[::-1]])
jax.experimental.ode._odeint(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode._odeint_fwd(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode._odeint_rev(func,rtol,atol,mxstep,res,g)
jax.experimental.ode._odeint_wrapper(func,rtol,atol,mxstep,y0,ts,*args)
jax.experimental.ode.abstractify(x)
jax.experimental.ode.closure_convert(fun,in_tree,in_avals)
jax.experimental.ode.error_ratio(error_estimate,rtol,atol,y0,y1)
jax.experimental.ode.fit_4th_order_polynomial(y0,y1,y_mid,dy0,dy1,dt)
jax.experimental.ode.initial_step_size(fun,t0,y0,order,rtol,atol,f0)
jax.experimental.ode.interp_fit_dopri(y0,y1,k,dt)
jax.experimental.ode.odeint(func,y0,t,*args,rtol=1.4e-08,atol=1.4e-08,mxstep=jnp.inf)
jax.experimental.ode.optimal_step_size(last_step,mean_error_ratio,safety=0.9,ifactor=10.0,dfactor=0.2,order=5.0)
jax.experimental.ode.partition_list(choice,lst)
jax.experimental.ode.ravel_first_arg(f,unravel)
jax.experimental.ode.ravel_first_arg_(unravel,y_flat,*args)
jax.experimental.ode.runge_kutta_step(func,y0,f0,t0,dt)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/stax.py----------------------------------------
A:jax.experimental.stax.(k1, k2)->jax.random.split(rng)
A:jax.experimental.stax.filter_shape_iter->iter(filter_shape)
A:jax.experimental.stax.output_shape->jax.lax.conv_transpose_shape_tuple(input_shape, kernel_shape, strides, padding, dimension_numbers)
A:jax.experimental.stax.bias_shape->tuple(itertools.dropwhile(lambda x: x == 1, bias_shape))
A:jax.experimental.stax.Conv->functools.partial(GeneralConv, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.Conv1DTranspose->functools.partial(GeneralConvTranspose, ('NHC', 'HIO', 'NHC'))
A:jax.experimental.stax.ConvTranspose->functools.partial(GeneralConvTranspose, ('NHWC', 'HWIO', 'NHWC'))
A:jax.experimental.stax.shape->tuple((d for (i, d) in enumerate(input_shape) if i not in axis))
A:jax.experimental.stax.ed->tuple((None if i in axis else slice(None) for i in range(jnp.ndim(x))))
A:jax.experimental.stax.z->normalize(x, axis, epsilon=epsilon)
A:jax.experimental.stax.Tanh->elementwise(jnp.tanh)
A:jax.experimental.stax.Relu->elementwise(relu)
A:jax.experimental.stax.Exp->elementwise(jnp.exp)
A:jax.experimental.stax.LogSoftmax->elementwise(log_softmax, axis=-1)
A:jax.experimental.stax.Softmax->elementwise(softmax, axis=-1)
A:jax.experimental.stax.Softplus->elementwise(softplus)
A:jax.experimental.stax.Sigmoid->elementwise(sigmoid)
A:jax.experimental.stax.Elu->elementwise(elu)
A:jax.experimental.stax.LeakyRelu->elementwise(leaky_relu)
A:jax.experimental.stax.Selu->elementwise(selu)
A:jax.experimental.stax.Gelu->elementwise(gelu)
A:jax.experimental.stax.padding_vals->jax.lax.padtype_to_pads(input_shape, window_shape, strides, padding)
A:jax.experimental.stax.out_shape->jax.lax.reduce_window_shape_tuple(input_shape, window_shape, strides, padding_vals, ones, ones)
A:jax.experimental.stax.out->jax.lax.reduce_window(inputs, init_val, reducer, window_shape, strides, padding)
A:jax.experimental.stax.MaxPool->_pooling_layer(lax.max, -jnp.inf)
A:jax.experimental.stax.SumPool->_pooling_layer(lax.add, 0.0)
A:jax.experimental.stax.spatial_shape->tuple((inputs.shape[i] for i in range(inputs.ndim) if i not in non_spatial_axes))
A:jax.experimental.stax.one->jax.numpy.ones(spatial_shape, dtype=inputs.dtype)
A:jax.experimental.stax.window_sizes->jax.numpy.expand_dims(window_sizes, i)
A:jax.experimental.stax.AvgPool->_pooling_layer(lax.add, 0.0, _normalize_by_window_size)
A:jax.experimental.stax.Flatten->Flatten()
A:jax.experimental.stax.Identity->Identity()
A:jax.experimental.stax.FanInSum->FanInSum()
A:jax.experimental.stax.concat_size->sum((shape[ax] for shape in input_shape))
A:jax.experimental.stax.rng->kwargs.pop('rng', None)
A:jax.experimental.stax.keep->jax.random.bernoulli(rng, rate, inputs.shape)
A:jax.experimental.stax.nlayers->len(layers)
A:jax.experimental.stax.(init_funs, apply_funs)->zip(*layers)
A:jax.experimental.stax.(rng, layer_rng)->jax.random.split(rng)
A:jax.experimental.stax.(input_shape, param)->init_fun(layer_rng, input_shape)
A:jax.experimental.stax.inputs->fun(param, inputs, rng=rng, **kwargs)
A:jax.experimental.stax.rngs->jax.random.split(rng, nlayers)
jax.experimental.stax.BatchNorm(axis=(0,1,2),epsilon=1e-05,center=True,scale=True,beta_init=zeros,gamma_init=ones)
jax.experimental.stax.Dense(out_dim,W_init=glorot_normal(),b_init=normal())
jax.experimental.stax.Dropout(rate,mode='train')
jax.experimental.stax.FanInConcat(axis=-1)
jax.experimental.stax.FanInSum()
jax.experimental.stax.FanOut(num)
jax.experimental.stax.Flatten()
jax.experimental.stax.GeneralConv(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.GeneralConvTranspose(dimension_numbers,out_chan,filter_shape,strides=None,padding='VALID',W_init=None,b_init=normal(1e-06))
jax.experimental.stax.Identity()
jax.experimental.stax._normalize_by_window_size(dims,strides,padding)
jax.experimental.stax._pooling_layer(reducer,init_val,rescaler=None)
jax.experimental.stax.elementwise(fun,**fun_kwargs)
jax.experimental.stax.parallel(*layers)
jax.experimental.stax.serial(*layers)
jax.experimental.stax.shape_dependent(make_layer)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/loops.py----------------------------------------
A:jax.experimental.loops.start->int(first)
A:jax.experimental.loops.stop->int(first)
A:jax.experimental.loops.step->int(third)
A:jax.experimental.loops.pred_dtype->numpy.result_type(pred)
A:jax.experimental.loops.mt_val->self._mutable_state.get(key)
A:jax.experimental.loops.level->jax.core.thread_local_state.trace_state.trace_stack.next_level(False)
A:jax.experimental.loops.master->jax.core.MasterTrace(level, pe.JaxprTrace)
A:jax.experimental.loops.self.stack->traceback.StackSummary.from_list(cast(List[Any], traceback.extract_stack()[:-2]))
A:jax.experimental.loops.self.carried_state_initial->copy.copy(self.scope._mutable_state)
A:jax.experimental.loops.self.carried_state_names->sorted(self.scope._mutable_state.keys())
A:jax.experimental.loops.self.trace->self.scope.start_subtrace()
A:jax.experimental.loops.mt_aval->_BodyTracer.abstractify(initial)
A:jax.experimental.loops.mt_pval->jax.interpreters.partial_eval.PartialVal.unknown(mt_aval)
A:jax.experimental.loops.mt_var->self.trace.new_arg(mt_pval)
A:jax.experimental.loops.index_var_aval->_BodyTracer.abstractify(0)
A:jax.experimental.loops.index_var_pval->jax.interpreters.partial_eval.PartialVal.unknown(index_var_aval)
A:jax.experimental.loops.self._index_var->self.trace.new_arg(index_var_pval)
A:jax.experimental.loops.body_out_tracers->tuple([self.scope._mutable_state[ms] for ms in self.carried_state_names])
A:jax.experimental.loops.(body_typed_jaxpr, body_const_vals)->_BodyTracer.trace_to_jaxpr_finalize(in_tracers=in_tracers, out_tracers=body_out_tracers, trace=self.trace)
A:jax.experimental.loops.carried_init_val->tuple([self.carried_state_initial[ms] for ms in self.carried_state_names])
A:jax.experimental.loops.(carried_init_vals, carried_tree)->jax.tree_util.tree_flatten(carried_init_val)
A:jax.experimental.loops.carried_out_vals->self.loop_builder.build_output_vals(self.scope, self.carried_state_names, carried_tree, carried_init_vals, body_typed_jaxpr, body_const_vals)
A:jax.experimental.loops.carried_mutable_state_unflattened->jax.tree_util.tree_unflatten(carried_tree, carried_out_vals)
A:jax.experimental.loops.out_tracers->safe_map(partial(pe.instantiate_const_at, trace), instantiate, out_tracers)
A:jax.experimental.loops.(jaxpr, consts, env)->jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.experimental.loops.out_avals->safe_map(abstract_arrays.raise_to_shaped, unzip2(out_pvals)[0])
A:jax.experimental.loops.const_avals->tuple((abstract_arrays.raise_to_shaped(core.get_aval(c)) for c in consts))
A:jax.experimental.loops.in_avals->safe_map(_BodyTracer.abstractify, in_vals)
A:jax.experimental.loops.typed_jaxpr->jax.core.TypedJaxpr(pe.convert_constvars_jaxpr(jaxpr), (), const_avals + in_avals, out_avals)
A:jax.experimental.loops.arange_val->jax.numpy.arange(self.start, stop=self.stop, step=self.step)
A:jax.experimental.loops.self.index->jax.lax.convert_element_type(pred, np.int32)
A:jax.experimental.loops.(in_vals, in_tree)->jax.tree_util.tree_flatten((body_const_vals, tree_util.tree_unflatten(carried_tree, init_vals)))
A:jax.experimental.loops.(pass_through_typed_jaxpr, pass_through_const_vals, _)->jax.lax.lax_control_flow._initial_style_jaxpr(lambda *args: args[1], in_tree, tuple(in_avals))
A:jax.experimental.loops.args->list(itertools.chain(body_const_vals, init_vals))
A:jax.experimental.loops.res->self.cond_func()
A:jax.experimental.loops.init_avals->safe_map(_BodyTracer.abstractify, init_vals)
A:jax.experimental.loops.(cond_jaxpr, cond_consts, cond_tree)->jax.lax.lax_control_flow._initial_style_jaxpr(cond_func_wrapped, carried_tree, tuple(init_avals))
jax.experimental.loops.Scope(self)
jax.experimental.loops.Scope.__enter__(self)
jax.experimental.loops.Scope.__exit__(self,exc_type,exc_val,exc_tb)
jax.experimental.loops.Scope.__getattr__(self,key)
jax.experimental.loops.Scope.__setattr__(self,key,value)
jax.experimental.loops.Scope._error_premature_exit_range(self)
jax.experimental.loops.Scope._pop_range(self,range_)
jax.experimental.loops.Scope._push_range(self,range_)
jax.experimental.loops.Scope.cond_range(self,pred)
jax.experimental.loops.Scope.end_subtrace(self)
jax.experimental.loops.Scope.range(self,first,second=None,third=None)
jax.experimental.loops.Scope.start_subtrace(self)
jax.experimental.loops.Scope.while_range(self,cond_func)
jax.experimental.loops._BodyTracer(self,scope,loop_builder)
jax.experimental.loops._BodyTracer.__iter__(self)
jax.experimental.loops._BodyTracer.__next__(self)
jax.experimental.loops._BodyTracer.abstractify(x)
jax.experimental.loops._BodyTracer.end_tracing_body(self)
jax.experimental.loops._BodyTracer.location(self)
jax.experimental.loops._BodyTracer.next(self)
jax.experimental.loops._BodyTracer.start_tracing_body(self)
jax.experimental.loops._BodyTracer.trace_to_jaxpr_finalize(in_tracers,out_tracers,trace,instantiate=True)
jax.experimental.loops._BoundedLoopBuilder(self,start,stop,step)
jax.experimental.loops._BoundedLoopBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._BoundedLoopBuilder.can_use_index_var(self)
jax.experimental.loops._CondBuilder(self,pred)
jax.experimental.loops._CondBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._CondBuilder.can_use_index_var(self)
jax.experimental.loops._LoopBuilder(object)
jax.experimental.loops._LoopBuilder.__str__(self)
jax.experimental.loops._LoopBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._LoopBuilder.can_use_index_var(self)
jax.experimental.loops._WhileBuilder(self,cond_func)
jax.experimental.loops._WhileBuilder.build_output_vals(self,scope,carried_state_names,carried_tree,init_vals,body_typed_jaxpr,body_const_vals)
jax.experimental.loops._WhileBuilder.can_use_index_var(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/optimizers.py----------------------------------------
A:jax.experimental.optimizers.OptimizerState->namedtuple('OptimizerState', ['packed_state', 'tree_def', 'subtree_defs'])
A:jax.experimental.optimizers.(init, update, get_params)->opt_maker(*args, **kwargs)
A:jax.experimental.optimizers.(x0_flat, tree)->tree_flatten(x0_tree)
A:jax.experimental.optimizers.(states_flat, subtrees)->unzip2(map(tree_flatten, initial_states))
A:jax.experimental.optimizers.(grad_flat, tree2)->tree_flatten(grad_tree)
A:jax.experimental.optimizers.states->map(tree_unflatten, subtrees, states_flat)
A:jax.experimental.optimizers.new_states->map(partial(update, i), grad_flat, states)
A:jax.experimental.optimizers.(new_states_flat, subtrees2)->unzip2(map(tree_flatten, new_states))
A:jax.experimental.optimizers.params->map(get_params, states)
A:jax.experimental.optimizers.step_size->make_schedule(step_size)
A:jax.experimental.optimizers.v0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.g_sq_inv_sqrt->jax.numpy.where(g_sq > 0, 1.0 / jnp.sqrt(g_sq), 0.0)
A:jax.experimental.optimizers.avg_sq_grad->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.mom->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.m0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.u0->jax.numpy.zeros_like(x0)
A:jax.experimental.optimizers.u->jax.numpy.maximum(b2 * u, jnp.abs(g))
A:jax.experimental.optimizers.lst->list(seq)
A:jax.experimental.optimizers.idx->splice([None] * ndim, axis, [slice(None)])
A:jax.experimental.optimizers.accum_inv_sqrt->jax.numpy.where(accum > 0, 1.0 / jnp.sqrt(accum), 0)
A:jax.experimental.optimizers.step_num->jax.numpy.minimum(step_num, decay_steps)
A:jax.experimental.optimizers.boundaries->jax.numpy.array(boundaries)
A:jax.experimental.optimizers.values->jax.numpy.array(values)
A:jax.experimental.optimizers.(leaves, _)->tree_flatten(tree)
A:jax.experimental.optimizers.norm->l2_norm(grad_tree)
A:jax.experimental.optimizers.subtrees->map(tree_unflatten, subtree_defs, states_flat)
A:jax.experimental.optimizers.(sentinels, tree_def)->tree_flatten(marked_pytree)
A:jax.experimental.optimizers.(states_flat, subtree_defs)->unzip2(map(tree_flatten, subtrees))
jax.experimental.optimizers.JoinPoint(self,subtree)
jax.experimental.optimizers.JoinPoint.__iter__(self)
jax.experimental.optimizers.adagrad(step_size,momentum=0.9)
jax.experimental.optimizers.adam(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optimizers.adamax(step_size,b1=0.9,b2=0.999,eps=1e-08)
jax.experimental.optimizers.clip_grads(grad_tree,max_norm)
jax.experimental.optimizers.constant(step_size)
jax.experimental.optimizers.exponential_decay(step_size,decay_steps,decay_rate)
jax.experimental.optimizers.inverse_time_decay(step_size,decay_steps,decay_rate,staircase=False)
jax.experimental.optimizers.l2_norm(tree)
jax.experimental.optimizers.make_schedule(scalar_or_schedule)
jax.experimental.optimizers.momentum(step_size,mass)
jax.experimental.optimizers.nesterov(step_size,mass)
jax.experimental.optimizers.optimizer(opt_maker)
jax.experimental.optimizers.pack_optimizer_state(marked_pytree)
jax.experimental.optimizers.piecewise_constant(boundaries,values)
jax.experimental.optimizers.polynomial_decay(step_size,decay_steps,final_step_size,power=1.0)
jax.experimental.optimizers.rmsprop(step_size,gamma=0.9,eps=1e-08)
jax.experimental.optimizers.rmsprop_momentum(step_size,gamma=0.9,eps=1e-08,momentum=0.9)
jax.experimental.optimizers.sgd(step_size)
jax.experimental.optimizers.sm3(step_size,momentum=0.9)
jax.experimental.optimizers.unpack_optimizer_state(opt_state)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/callback.py----------------------------------------
A:jax.experimental.callback.(args_flat, in_tree)->tree_flatten(args)
A:jax.experimental.callback.f->callback_subtrace(f, self.master)
A:jax.experimental.callback.(flat_fun, out_tree)->flatten_fun_nokwargs(f, in_tree)
A:jax.experimental.callback.out_flat->callback_fun(flat_fun, args_flat, callback, strip_calls)
A:jax.experimental.callback.vals->prim.bind(*vals, **params)
A:jax.experimental.callback.fun->_callback_fun(fun, callback, strip_calls)
A:jax.experimental.callback.trace->CallbackTrace(master, core.cur_sublevel())
A:jax.experimental.callback.out_tracers->map(trace.full_raise, outs)
A:jax.experimental.callback.vals_out->call_primitive.bind(f, *vals_in, **params)
jax.experimental.callback.CallbackTrace(Trace)
jax.experimental.callback.CallbackTrace.lift(self,val)
jax.experimental.callback.CallbackTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.experimental.callback.CallbackTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.callback.CallbackTrace.pure(self,val)
jax.experimental.callback.CallbackTrace.sublift(self,val)
jax.experimental.callback.CallbackTracer(self,trace,val)
jax.experimental.callback.CallbackTracer.aval(self)
jax.experimental.callback.CallbackTracer.full_lower(self)
jax.experimental.callback.FoundValue(Exception)
jax.experimental.callback._callback_fun(callback,strip_calls,*in_vals,**params)
jax.experimental.callback._check_callable(fun)
jax.experimental.callback._contains_query(vals,query)
jax.experimental.callback.callback_fun(fun:lu.WrappedFun,in_vals,callback,strip_calls)
jax.experimental.callback.callback_subtrace(master,*in_vals,**params)
jax.experimental.callback.callback_transform(fun:Callable,callback:Callable,strip_calls:bool=False)->Callable
jax.experimental.callback.find_by_value(fun:Callable,queries)->Callable
jax.experimental.callback.rewrite(fun:Callable,rules)->Callable


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jet.py----------------------------------------
A:jax.experimental.jet.(order,)->set(map(len, series))
A:jax.experimental.jet.treedef->tree_structure(t)
A:jax.experimental.jet.(f, out_tree)->flatten_fun_output(lu.wrap_init(fun))
A:jax.experimental.jet.(out_primals, out_terms)->unzip2(((t.primal, t.terms) for t in out_tracers))
A:jax.experimental.jet.trace->JetTrace(master, core.cur_sublevel())
A:jax.experimental.jet.in_tracers->map(partial(JetTracer, trace), primals, series)
A:jax.experimental.jet.out_tracers->map(trace.full_raise, ans)
A:jax.experimental.jet.(primals_in, series_in)->unzip2(((t.primal, t.terms) for t in tracers))
A:jax.experimental.jet.(out_flat, out_tree_def)->tree_flatten((primals_out, series_out))
A:jax.experimental.jet.(primal_out, terms_out)->rule(primals_in, series_in, **params)
A:jax.experimental.jet.(primals_and_series, in_tree_def)->tree_flatten((primals_in, series_in))
A:jax.experimental.jet.(f_jet, out_tree_def)->traceable(jet_subtrace(f, self.master), in_tree_def)
A:jax.experimental.jet.update_params->call_param_updaters.get(call_primitive)
A:jax.experimental.jet.result->call_primitive.bind(f_jet, *primals_and_series, **new_params)
A:jax.experimental.jet.(primals_out, series_out)->tree_unflatten(out_tree_def(), result)
A:jax.experimental.jet.(primals, series)->tree_unflatten(treedef, x)
A:jax.experimental.jet.(out, treedef)->tree_flatten((primals, series))
A:jax.experimental.jet.zero_term->ZeroTerm()
A:jax.experimental.jet.zero_series->ZeroSeries()
A:jax.experimental.jet.jet_rules[prim]->partial(jet, comp)
A:jax.experimental.jet.primal_out->jax.lax.lax.select(xgy, x, y)
A:jax.experimental.jet.(c0, cs)->jet(lambda x: lax.div(1, 1 + lax.square(x)), (x,), (series,))
A:jax.experimental.jet.k->len(series)
A:jax.experimental.jet.(x, series)->jet(lax.div, primals_in, series_in)
A:jax.experimental.jet.vu->sum((_scale(k, j) * v[k - j] * u[j] for j in range(1, k + 1)))
A:jax.experimental.jet.uv->sum((_scale(k, j) * u[k - j] * v[j] for j in range(1, k)))
A:jax.experimental.jet.v[k]->jax.numpy.where(x == 0, 0, fact(k - 1) * (y * vu - uv) / x)
A:jax.experimental.jet.(primal_out, series_out)->_expit_taylor((primals_in,), (series_in,))
A:jax.experimental.jet.conv->sum([scale(k, j) * v[j] * w[k - j] for j in range(0, k)])
A:jax.experimental.jet.jet_rules[lax.sin_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 0)
A:jax.experimental.jet.jet_rules[lax.cos_p]->_get_ind(partial(_sinusoidal_rule, -1, (lax.sin, lax.cos)), 1)
A:jax.experimental.jet.jet_rules[lax.sinh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 0)
A:jax.experimental.jet.jet_rules[lax.cosh_p]->_get_ind(partial(_sinusoidal_rule, 1, (lax.sinh, lax.cosh)), 1)
A:jax.experimental.jet.op->partial(prim.bind, **params)
A:jax.experimental.jet.jet_rules[lax.dot_general_p]->partial(_bilinear_taylor_rule, lax.dot_general_p)
A:jax.experimental.jet.jet_rules[lax.mul_p]->partial(_bilinear_taylor_rule, lax.mul_p)
A:jax.experimental.jet.jet_rules[lax.conv_general_dilated_p]->partial(_bilinear_taylor_rule, lax.conv_general_dilated_p)
A:jax.experimental.jet.axes->params.pop('axes', None)
A:jax.experimental.jet.location_indicators->jax.lax.lax.convert_element_type(lax._eq_meet(operand, lax.reshape(primal_out, shape)), primal_dtype)
A:jax.experimental.jet.counts->jax.lax.lax._reduce_sum(location_indicators, axes)
A:jax.experimental.jet.jet_rules[lax.reduce_max_p]->_gen_reduce_choose_taylor_rule(lax.reduce_max_p.bind)
A:jax.experimental.jet.jet_rules[lax.reduce_min_p]->_gen_reduce_choose_taylor_rule(lax.reduce_min_p.bind)
A:jax.experimental.jet.zero->jax.lax.lax.full_like(x, 0, shape=())
A:jax.experimental.jet.negs->jax.lax.lax.select(lax.lt(x, zero), lax.full_like(x, -1), lax.full_like(x, 1.0))
A:jax.experimental.jet.max_i->jax.lax.lax.select(xey, (x_i + y_i) / 2, max_i)
A:jax.experimental.jet.min_i->jax.lax.lax.select(xey, (x_i + y_i) / 2, min_i)
jax.experimental.jet.JetTrace(core.Trace)
jax.experimental.jet.JetTrace.join(self,xt,yt)
jax.experimental.jet.JetTrace.lift(self,val)
jax.experimental.jet.JetTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.experimental.jet.JetTrace.process_call(self,call_primitive,f,tracers,params)
jax.experimental.jet.JetTrace.process_primitive(self,primitive,tracers,params)
jax.experimental.jet.JetTrace.pure(self,val)
jax.experimental.jet.JetTrace.sublift(self,val)
jax.experimental.jet.JetTracer(self,trace,primal,terms)
jax.experimental.jet.JetTracer.aval(self)
jax.experimental.jet.JetTracer.full_lower(self)
jax.experimental.jet.ZeroSeries(object)
jax.experimental.jet.ZeroTerm(object)
jax.experimental.jet._abs_taylor_rule(x,series_in,**params)
jax.experimental.jet._atan2_taylor(primals_in,series_in)
jax.experimental.jet._bilinear_taylor_rule(prim,primals_in,series_in,**params)
jax.experimental.jet._custom_jvp_call_jaxpr_rule(primals_in,series_in,*,fun_jaxpr,jvp_jaxpr_thunk)
jax.experimental.jet._div_taylor_rule(primals_in,series_in)
jax.experimental.jet._erf_inv_rule(primals_in,series_in)
jax.experimental.jet._exp_taylor(primals_in,series_in)
jax.experimental.jet._expit_taylor(primals_in,series_in)
jax.experimental.jet._gather_taylor_rule(primals_in,series_in,**params)
jax.experimental.jet._gen_reduce_choose_taylor_rule(chooser_fun)
jax.experimental.jet._get_ind(f,ind)
jax.experimental.jet._integer_pow_taylor(primals_in,series_in,*,y)
jax.experimental.jet._lax_max_taylor_rule(primal_in,series_in)
jax.experimental.jet._lax_min_taylor_rule(primal_in,series_in)
jax.experimental.jet._log_taylor(primals_in,series_in)
jax.experimental.jet._pow_taylor(primals_in,series_in)
jax.experimental.jet._scale(k,j)
jax.experimental.jet._scale2(k,j)
jax.experimental.jet._select_taylor_rule(primal_in,series_in,**params)
jax.experimental.jet._sinusoidal_rule(sign,prims,primals_in,series_in)
jax.experimental.jet._tanh_taylor(primals_in,series_in)
jax.experimental.jet._xla_call_param_updater(params,num_inputs)
jax.experimental.jet.def_comp(prim,comp)
jax.experimental.jet.def_deriv(prim,deriv)
jax.experimental.jet.deflinear(prim)
jax.experimental.jet.defzero(prim)
jax.experimental.jet.deriv_prop(prim,deriv,primals_in,series_in)
jax.experimental.jet.fact(n)
jax.experimental.jet.jet(fun,primals,series)
jax.experimental.jet.jet_fun(order,primals,series)
jax.experimental.jet.jet_subtrace(master,primals,series)
jax.experimental.jet.linear_prop(prim,primals_in,series_in,**params)
jax.experimental.jet.traceable(in_tree_def,*primals_and_series)
jax.experimental.jet.zero_prop(prim,primals_in,series_in,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/host_callback.py----------------------------------------
A:jax.experimental.host_callback.(flat_args, arg_treedef)->jax.lib.pytree.flatten(arg)
A:jax.experimental.host_callback.params->dict(kwargs)
A:jax.experimental.host_callback.params['nr_tapped_args_']->len(flat_args)
A:jax.experimental.host_callback.(flat_results, result_treedef)->jax.lib.pytree.flatten(result)
A:jax.experimental.host_callback.nr_results->len(flat_results)
A:jax.experimental.host_callback.flat_outs->jax.core.Primitive('id_tap').bind(*all_args, **params)
A:jax.experimental.host_callback.kwargs->dict(self.kwargs)
A:jax.experimental.host_callback.transforms->dict(self.kwargs).get('transforms')
A:jax.experimental.host_callback.cons_id->_OutfeedReceiverData().consumer_registry.get(cons)
A:jax.experimental.host_callback.kv_pairs->' '.join([f'{k}: {v}' for (k, v) in sorted(kwargs.items())])
A:jax.experimental.host_callback.id_tap_p->jax.core.Primitive('id_tap')
A:jax.experimental.host_callback.out_primals->jax.core.Primitive('id_tap').bind(*primals, **params)
A:jax.experimental.host_callback.tangent_zeros->tuple(map(_instantiate_zeros, primals, tangents))
A:jax.experimental.host_callback.out_tangents_extra->jax.core.Primitive('id_tap').bind(*tangent_zeros, out_primals[0], **_add_transform(params, 'jvp'))
A:jax.experimental.host_callback.cts_zeros->tuple(map(_instantiate_zeros, args, cts))
A:jax.experimental.host_callback.ct_args->jax.core.Primitive('id_tap').bind(*cts_zeros, **_add_transform(params, 'transpose'))
A:jax.experimental.host_callback.new_params->_add_transform(params, 'mask', operands_logical_shapes)
A:jax.experimental.host_callback.res->jax.core.Primitive('id_tap').bind(*batched_args, **new_params)
A:jax.experimental.host_callback.consumer_id->_register_consumer(_ConsumerCallable(tap_func_, tuple(sorted(params.items())), arg_treedef_, comp.get_shape(xops.Tuple(comp, args_to_outfeed))))
A:jax.experimental.host_callback.next_token->_OutfeedReceiverData().receiver.add_outfeed(comp, current_token, consumer_id, args_to_outfeed)
A:jax.experimental.host_callback.new_jaxpr->_rewrite_typed_jaxpr(carry_jaxpr, True, True)
A:jax.experimental.host_callback.mk_new_var->jax.core.gensym([jaxpr])
A:jax.experimental.host_callback.last_token_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.output_token_var->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.(cond_jaxpr, _, body_jaxpr, _)->jax.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.(branches, linear)->jax.util.split_dict(eqn.params, ['branches', 'linear'])
A:jax.experimental.host_callback.(num_consts, num_carry, carry_jaxpr, linear, _, _, _)->jax.util.split_dict(eqn.params, ['num_consts', 'num_carry', 'jaxpr', 'linear', 'reverse', 'length', 'unroll'])
A:jax.experimental.host_callback.call_jaxpr->cast(core.Jaxpr, eqn.params['call_jaxpr'])
A:jax.experimental.host_callback.(cond_jaxpr, cond_nconsts, body_jaxpr, body_nconsts)->jax.util.split_dict(eqn.params, ['cond_jaxpr', 'cond_nconsts', 'body_jaxpr', 'body_nconsts'])
A:jax.experimental.host_callback.transformed_cond_jaxpr->_rewrite_typed_jaxpr(cond_jaxpr, True, True)
A:jax.experimental.host_callback.new_cond_pred_invar->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_cond_jaxpr->_mk_typed_jaxpr(core.Jaxpr([], new_cond_invars, [new_cond_pred_invar], []), [])
A:jax.experimental.host_callback.transformed_body_jaxpr->_rewrite_typed_jaxpr(body_jaxpr, True, True)
A:jax.experimental.host_callback.new_body_invars_pred->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_invars_token->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.new_body_token2->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.new_body_pred2->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.new_body_token3->mk_new_var(core.abstract_token)
A:jax.experimental.host_callback.new_body_jaxpr->_mk_typed_jaxpr(core.Jaxpr([], new_body_invars_cond_constvars + new_body_invars_body_constvars + [new_body_invars_pred] + new_body_invars_carry + [new_body_invars_token], [new_body_pred2] + new_body_carry2 + [new_body_token3], new_body_eqns), [])
A:jax.experimental.host_callback.pred_out->mk_new_var(cond_jaxpr.out_avals[0])
A:jax.experimental.host_callback.self.lock->threading.Lock()
A:jax.experimental.host_callback.self.consumer_registry->dict()
A:jax.experimental.host_callback.self.consumer_registry_by_id->dict()
A:jax.experimental.host_callback._outfeed_receiver->_OutfeedReceiverData()
A:jax.experimental.host_callback.consumer->_OutfeedReceiverData().consumer_registry_by_id.get(consumer_id)
A:jax.experimental.host_callback.arg->jax.api.tree_unflatten(consumer.arg_treedef, arrays)
A:jax.experimental.host_callback.clients->tuple([c for c in clients if c.platform != 'interpreter'])
A:jax.experimental.host_callback.devices->list(itertools.chain(*[backend.devices() for backend in clients]))
A:jax.experimental.host_callback._outfeed_receiver.receiver->outfeed_receiver_module.start(_outfeed_receiver_callback, tuple(clients), max_callback_queue_size_bytes)
A:jax.experimental.host_callback.lock->threading.Lock()
A:jax.experimental.host_callback.cv->threading.Condition(lock=lock)
A:jax.experimental.host_callback.num_at_large->len(_outfeed_receiver.devices)
A:jax.experimental.host_callback.x_on_dev->jax.api.device_put(d_idx, device=d)
jax.experimental.host_callback.TapFunctionException(Exception)
jax.experimental.host_callback._ConsumerCallable(NamedTuple)
jax.experimental.host_callback._ConsumerCallable.unpack_kwargs(self)
jax.experimental.host_callback._OutfeedReceiverData(self)
jax.experimental.host_callback._OutfeedReceiverData.stop(self)
jax.experimental.host_callback._add_transform(params:Dict,name:str,*transform_params)->Dict
jax.experimental.host_callback._id_tap_abstract_eval(*args_a:pe.AbstractValue,**params)->Sequence[pe.AbstractValue]
jax.experimental.host_callback._id_tap_batching_rule(batched_args,batch_dims,**params)
jax.experimental.host_callback._id_tap_impl(*arrays,**params)
jax.experimental.host_callback._id_tap_jvp_rule(primals,tangents,**params)
jax.experimental.host_callback._id_tap_masking_rule(operands,operands_logical_shapes,**params)
jax.experimental.host_callback._id_tap_translation_rule(comp:XlaComputationBuilder,*args_op:XlaOp,tap_func_=None,nr_tapped_args_,arg_treedef_=None,has_token_=False,**params)
jax.experimental.host_callback._id_tap_transpose_rule(cts,*args,**params)
jax.experimental.host_callback._initialize_outfeed_receiver(clients:Optional[List[XlaLocalClient]]=None,max_callback_queue_size_bytes:int=int(256*1000000.0))
jax.experimental.host_callback._instantiate_zeros(arg,tan)
jax.experimental.host_callback._mk_typed_jaxpr(jaxpr:core.Jaxpr,literals:Sequence)->core.TypedJaxpr
jax.experimental.host_callback._outfeed_receiver_callback(device,consumer_id,arrays)
jax.experimental.host_callback._print_consumer(arg,*,output_stream=None,threshold=1024,**kwargs)
jax.experimental.host_callback._register_consumer(cons:_ConsumerCallable)->int
jax.experimental.host_callback._rewrite_eqn(eqn:core.JaxprEqn,eqns:List[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,mk_new_var:Callable[[core.AbstractValue],core.Var])
jax.experimental.host_callback._rewrite_jaxpr(jaxpr:core.Jaxpr,has_input_token:bool,has_output_token:bool)->core.Jaxpr
jax.experimental.host_callback._rewrite_typed_jaxpr(tjaxpr:core.TypedJaxpr,has_input_token:bool,has_output_token:bool)->core.TypedJaxpr
jax.experimental.host_callback._rewrite_while_outfeed_cond(eqn:core.JaxprEqn,eqns:List[core.JaxprEqn],input_token_var:core.Var,output_token_var:core.Var,mk_new_var:Callable)
jax.experimental.host_callback.barrier_wait()
jax.experimental.host_callback.id_print(arg,*,result=None,output_stream=None,threshold=None,**kwargs)
jax.experimental.host_callback.id_tap(tap_func:Callable,arg,*,result=None,**kwargs)
jax.experimental.host_callback.outfeed_receiver()
jax.experimental.host_callback.stop_outfeed_receiver()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/vectorize.py----------------------------------------
A:jax.experimental.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax.experimental.vectorize._ARGUMENT->'\\({}\\)'.format(_CORE_DIMENSION_LIST)
A:jax.experimental.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax.experimental.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax.experimental.vectorize.num_core_dims->len(core_dims)
A:jax.experimental.vectorize.dummy_array->numpy.lib.stride_tricks.as_strided(0, arg.shape[:ndim])
A:jax.experimental.vectorize.broadcast_shape->numpy.lib.stride_tricks._broadcast_shape(*broadcast_args)
A:jax.experimental.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims)
A:jax.experimental.vectorize.input_shapes->_calculate_shapes(broadcast_shape, dim_sizes, input_core_dims)
A:jax.experimental.vectorize.all_core_dims->set()
A:jax.experimental.vectorize.result->reorder_outputs(result, axis, output_core_dims)
A:jax.experimental.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax.experimental.vectorize.axis->kwargs.get('axis')
A:jax.experimental.vectorize.args->reorder_inputs(args, axis, input_core_dims)
A:jax.experimental.vectorize.broadcast_args->broadcast_with_core_dims(args, input_core_dims, output_core_dims)
A:jax.experimental.vectorize.vectorized_func->vmap(vectorized_func)
jax.experimental.vectorize._calculate_shapes(broadcast_shape,dim_sizes,list_of_core_dims)
jax.experimental.vectorize._parse_gufunc_signature(signature)
jax.experimental.vectorize._parse_input_dimensions(args,input_core_dims)
jax.experimental.vectorize._update_dim_sizes(dim_sizes,arg,core_dims)
jax.experimental.vectorize.broadcast_with_core_dims(args,input_core_dims,output_core_dims)
jax.experimental.vectorize.reorder_inputs(args,axis,input_core_dims)
jax.experimental.vectorize.reorder_outputs(result,axis,output_core_dims)
jax.experimental.vectorize.vectorize(signature)
jax.experimental.vectorize.verify_axis_is_supported(input_core_dims,output_core_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/jax2tf.py----------------------------------------
A:jax.experimental.jax2tf.jax2tf.(args_flat, in_tree)->jax.tree_util.tree_flatten((args, {}))
A:jax.experimental.jax2tf.jax2tf.f->_interpret_subtrace(f, self.master)
A:jax.experimental.jax2tf.jax2tf.(flat_fun, out_tree_thunk)->flatten_fun(f, in_tree)
A:jax.experimental.jax2tf.jax2tf.variables->kwargs.get('variables', [])
A:jax.experimental.jax2tf.jax2tf.(_, pullback_jax)->jax.vjp(fun, *args_jax)
A:jax.experimental.jax2tf.jax2tf.out_cts->jax.tree_util.tree_unflatten(out_tree_thunk(), out_cts_flat)
A:jax.experimental.jax2tf.jax2tf.in_cts->convert(fun_vjp_jax, with_gradient=False)(args, out_cts)
A:jax.experimental.jax2tf.jax2tf.out_flat->converted_fun_flat_with_custom_gradient(*args_flat)
A:jax.experimental.jax2tf.jax2tf.out_flat_raw->_interpret_fun(flat_fun, args_flat)
A:jax.experimental.jax2tf.jax2tf.out->_xla_compile(lambda o, s, u: tfxla.scatter(o, s, u, xla_update_computation, proto, indices_are_sorted=indices_are_sorted), operand, scatter_indices, updates)
A:jax.experimental.jax2tf.jax2tf.fun->_interpret_subtrace(fun, master)
A:jax.experimental.jax2tf.jax2tf.trace->TensorFlowTrace(master, core.cur_sublevel())
A:jax.experimental.jax2tf.jax2tf.in_tracers->tuple((TensorFlowTracer(trace, val) for val in in_vals))
A:jax.experimental.jax2tf.jax2tf.self.val->tensorflow.convert_to_tensor(np.array(val, aval.dtype), dtype=aval.dtype)
A:jax.experimental.jax2tf.jax2tf.aval->jax.interpreters.xla.abstractify(val)
A:jax.experimental.jax2tf.jax2tf.impl->self.get_primitive_impl(primitive)
A:jax.experimental.jax2tf.jax2tf.vals->tuple((t.val for t in out_tracers))
A:jax.experimental.jax2tf.jax2tf.dtype->to_tf_dtype(functools.reduce(jnp.promote_types, (to_jax_dtype(v.dtype) for v in values)))
A:jax.experimental.jax2tf.jax2tf.tf_impl[unexpected]->functools.partial(_unexpected_primitive, unexpected)
A:jax.experimental.jax2tf.jax2tf.tf_impl[ad_util.add_jaxvals_p]->wrap_binary_op(tf.math.add)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.atan2_p]->wrap_binary_op(tf.math.atan2)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.igamma_p]->wrap_binary_op(tf.math.igamma)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.igammac_p]->wrap_binary_op(tf.math.igammac)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.add_p]->wrap_binary_op(tf.math.add)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.sub_p]->wrap_binary_op(tf.math.subtract)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.mul_p]->wrap_binary_op(tf.math.multiply)
A:jax.experimental.jax2tf.jax2tf.in_xla_context->tensorflow.python.ops.control_flow_util.GraphOrParentsInXlaContext(ops.get_default_graph())
A:jax.experimental.jax2tf.jax2tf.(res,)->tensorflow.xla.experimental.compile(func, args)
A:jax.experimental.jax2tf.jax2tf.quotient->tensorflow.math.floor_divide(lhs, rhs)
A:jax.experimental.jax2tf.jax2tf.select->tensorflow.math.logical_and(tf.math.sign(lhs) != tf.math.sign(rhs), tf.math.floormod(lhs, rhs) != 0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.div_p]->wrap_binary_op(_div)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.rem_p]->wrap_binary_op(_rem)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.max_p]->wrap_binary_op(tf.math.maximum)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.min_p]->wrap_binary_op(tf.math.minimum)
A:jax.experimental.jax2tf.jax2tf.x->tensorflow.cast(x, unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.y->tensorflow.cast(y, unsigned_dtype)
A:jax.experimental.jax2tf.jax2tf.res->_interpret_jaxpr(fun_jaxpr, *args)
A:jax.experimental.jax2tf.jax2tf.argnums->tensorflow.nest.flatten(argnums)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.or_p]->bool_to_int8(tf.bitwise.bitwise_or, argnums=(0, 1))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.and_p]->bool_to_int8(tf.bitwise.bitwise_and, argnums=(0, 1))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.xor_p]->bool_to_int8(tf.bitwise.bitwise_xor, argnums=(0, 1))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.eq_p]->wrap_binary_op(tf.math.equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.ne_p]->wrap_binary_op(tf.math.not_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.ge_p]->wrap_binary_op(tf.math.greater_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.gt_p]->wrap_binary_op(tf.math.greater)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.le_p]->wrap_binary_op(tf.math.less_equal)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.lt_p]->wrap_binary_op(tf.math.less)
A:jax.experimental.jax2tf.jax2tf.proto->_scatter_dimensions_proto(scatter_indices.shape, dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.avals->map(abstractify, vals)
A:jax.experimental.jax2tf.jax2tf.(out,)->_infer_shape_jax(lax.scatter, operand, scatter_indices, updates, dimension_numbers=dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.out_shape->_scatter_shape(operand, scatter_indices, updates, dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.dnums_proto->_conv_general_proto(dimension_numbers)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.conv_general_dilated_p]->wrap_binary_op(_conv_general_dilated)
A:jax.experimental.jax2tf.jax2tf.new_id->iter(string.ascii_letters)
A:jax.experimental.jax2tf.jax2tf.shared_id->next(new_id)
A:jax.experimental.jax2tf.jax2tf.out_axis_ids->list(filter(not_none, batch_ids + lhs_out_axis_ids + rhs_out_axis_ids))
A:jax.experimental.jax2tf.jax2tf.spec->'{},{}->{}'.format(''.join(lhs_axis_ids), ''.join(rhs_axis_ids), ''.join(out_axis_ids))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.dot_general_p]->wrap_binary_op(_dot_general)
A:jax.experimental.jax2tf.jax2tf.inshape->tuple((1 if i not in broadcast_dimensions else d for (i, d) in enumerate(shape)))
A:jax.experimental.jax2tf.jax2tf.dimensions->tensorflow.range(tf.rank(operand))
A:jax.experimental.jax2tf.jax2tf.op_shape->numpy.shape(operand)
A:jax.experimental.jax2tf.jax2tf.new_shape->tuple((d for (i, d) in enumerate(op_shape) if i not in dimensions))
A:jax.experimental.jax2tf.jax2tf.(low, high, interior)->jax.util.unzip3(padding_config)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.pad_p]->wrap_binary_op(_pad)
A:jax.experimental.jax2tf.jax2tf.slices->tuple(map(slice, start_indices, limit_indices, strides))
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_sum_p]->bool_to_int8(axes_to_axis(tf.reduce_sum), argnums=0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_prod_p]->bool_to_int8(axes_to_axis(tf.reduce_prod), argnums=0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_max_p]->bool_to_int8(axes_to_axis(tf.reduce_max), argnums=0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_min_p]->bool_to_int8(axes_to_axis(tf.reduce_min), argnums=0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_or_p]->axes_to_axis(tf.reduce_any)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_and_p]->axes_to_axis(tf.reduce_all)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.argmin_p]->functools.partial(_argminmax, tf.math.argmin)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.argmax_p]->functools.partial(_argminmax, tf.math.argmax)
A:jax.experimental.jax2tf.jax2tf._add_fn->tensorflow.function(tf.math.add)
A:jax.experimental.jax2tf.jax2tf._ge_fn->tensorflow.function(tf.math.greater_equal)
A:jax.experimental.jax2tf.jax2tf._min_fn->tensorflow.function(tf.math.minimum)
A:jax.experimental.jax2tf.jax2tf._max_fn->tensorflow.function(tf.math.maximum)
A:jax.experimental.jax2tf.jax2tf.params->dict(window_dimensions=window_dimensions, window_strides=window_strides, padding=padding, base_dilation=base_dilation, window_dilation=window_dilation, input_shape=input_shape)
A:jax.experimental.jax2tf.jax2tf.a->tensorflow.constant(0, operand.dtype)
A:jax.experimental.jax2tf.jax2tf.reducer_fn->reducer.get_concrete_function(a, a)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_window_sum_p]->functools.partial(_reduce_window, lax._reduce_window_sum, _add_fn, 0)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_window_min_p]->functools.partial(_reduce_window, lax._reduce_window_min, _min_fn, np.inf)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.reduce_window_max_p]->functools.partial(_reduce_window, lax._reduce_window_max, _max_fn, -np.inf)
A:jax.experimental.jax2tf.jax2tf.select_fn->tensorflow.function(tf.math.greater_equal).get_concrete_function(a, a)
A:jax.experimental.jax2tf.jax2tf.scatter_fn->tensorflow.function(tf.math.add).get_concrete_function(a, a)
A:jax.experimental.jax2tf.jax2tf.v1->uadd(v1, v2)
A:jax.experimental.jax2tf.jax2tf.v2->tensorflow.bitwise.bitwise_xor(v1, v2)
A:jax.experimental.jax2tf.jax2tf.magic_number->tensorflow.constant(np.uint32(466688986), dtype=tf.uint32)
A:jax.experimental.jax2tf.jax2tf.key3->tensorflow.bitwise.bitwise_xor(key1, tf.bitwise.bitwise_xor(key2, magic_number))
A:jax.experimental.jax2tf.jax2tf.x1->uadd(x1, key3)
A:jax.experimental.jax2tf.jax2tf.x2->uadd(x2, key1, np.uint32(5))
A:jax.experimental.jax2tf.jax2tf.(x1, x2)->apply_round(x1, x2, r)
A:jax.experimental.jax2tf.jax2tf.expected_offset_dims->tuple(list(range(axis)) + list(range(axis + index_dims, len(op_shape) + index_dims - 1)))
A:jax.experimental.jax2tf.jax2tf.start_indices_reshaped->tensorflow.reshape(start_indices, start_indices.shape[0:-1])
A:jax.experimental.jax2tf.jax2tf.o_spec->tensorflow.TensorSpec(None, dtype=operand.dtype)
A:jax.experimental.jax2tf.jax2tf.xla_update_computation->tensorflow.function(update_computation).get_concrete_function(o_spec, o_spec)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.scatter_p]->functools.partial(_scatter, lambda x, y: y)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.scatter_min_p]->functools.partial(_scatter, tf.math.minimum)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.scatter_max_p]->functools.partial(_scatter, tf.math.maximum)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.scatter_mul_p]->functools.partial(_scatter, tf.math.multiply)
A:jax.experimental.jax2tf.jax2tf.tf_impl[lax.scatter_add_p]->functools.partial(_scatter, tf.math.add)
A:jax.experimental.jax2tf.jax2tf.(cond_consts, body_consts, init_carry)->jax.util.split_list(args, [cond_nconsts, body_nconsts])
A:jax.experimental.jax2tf.jax2tf.(pred,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *args)
A:jax.experimental.jax2tf.jax2tf.body_tf_func->functools.partial(_interpret_jaxpr, body_jaxpr, *body_consts)
A:jax.experimental.jax2tf.jax2tf.res_tf->tensorflow.while_loop(cond_tf_func, body_tf_func, _tfval_remove_unit(init_carry))
A:jax.experimental.jax2tf.jax2tf.(init_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *init_carry)
A:jax.experimental.jax2tf.jax2tf.pred->tensorflow.reduce_any(pred_b, axis=list(range(len(pred_b.shape))))
A:jax.experimental.jax2tf.jax2tf.pred_b_bcast->_broadcast_in_dim(pred_b, new_c.shape, list(range(len(pred_b.shape))))
A:jax.experimental.jax2tf.jax2tf.(next_pred_b,)->_interpret_jaxpr(cond_jaxpr, *cond_consts, *selected_carry)
A:jax.experimental.jax2tf.jax2tf.(_, *res_carry)->tensorflow.while_loop(new_cond_tf_func, new_body_tf_func, _tfval_remove_unit((init_pred_b, *init_carry)))
A:jax.experimental.jax2tf.jax2tf.conversion_dtype->promote_tf_dtype(operand.dtype)
A:jax.experimental.jax2tf.jax2tf.(values, indices)->tensorflow.math.top_k(tf.dtypes.cast(operand, conversion_dtype), k=k, sorted=True)
A:jax.experimental.jax2tf.jax2tf.m->tensorflow.Module()
A:jax.experimental.jax2tf.jax2tf.tuple_wrapper->type(m.a)
A:jax.experimental.jax2tf.jax2tf.list_wrapper->type(m.b)
A:jax.experimental.jax2tf.jax2tf.dict_wrapper->type(m.c)
jax.experimental.jax2tf.convert(fun,with_gradient=False)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace(core.Trace)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.get_primitive_impl(self,p)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.lift(self,val:core.Tracer)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_call(self,call_primitive:core.Primitive,out_tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_call(self,call_primitive:core.Primitive,f,tracers:Sequence[TensorFlowTracer],params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_map(self,map_primitive,f,tracers,params)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.process_primitive(self,primitive:core.Primitive,tracers:Sequence[TensorFlowTracer],params)->TensorFlowTracer
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.pure(self,val:TfValOrUnit)
jax.experimental.jax2tf.jax2tf.TensorFlowTrace.sublift(self,val:TensorFlowTracer)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer(self,trace:'TensorFlowTrace',val:TfValOrUnit)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.aval(self)
jax.experimental.jax2tf.jax2tf.TensorFlowTracer.full_lower(self)
jax.experimental.jax2tf.jax2tf._argminmax(fn,operand,axes,index_dtype)
jax.experimental.jax2tf.jax2tf._batched_cond_while(*args:TfValOrUnit,cond_nconsts:int,cond_jaxpr:core.TypedJaxpr,body_nconsts:int,body_jaxpr:core.TypedJaxpr)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._bitcast_convert_type(operand,new_dtype)
jax.experimental.jax2tf.jax2tf._broadcast(operand,sizes)
jax.experimental.jax2tf.jax2tf._broadcast_in_dim(operand,shape,broadcast_dimensions)
jax.experimental.jax2tf.jax2tf._clamp(minval,operand,maxval)
jax.experimental.jax2tf.jax2tf._concatenate(*operands,dimension=None)
jax.experimental.jax2tf.jax2tf._cond(index:TfVal,*operands:TfValOrUnit,branches:Sequence[core.TypedJaxpr],linear:Sequence[bool])->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._conv_general_dilated(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,lhs_shape,rhs_shape,precision)
jax.experimental.jax2tf.jax2tf._conv_general_dilated_shape(lhs,rhs,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,lhs_shape,rhs_shape,precision)
jax.experimental.jax2tf.jax2tf._conv_general_proto(dimension_numbers)
jax.experimental.jax2tf.jax2tf._convert_element_type(operand,new_dtype,old_dtype)
jax.experimental.jax2tf.jax2tf._custom_jvp_call_jaxpr(*args:TfValOrUnit,fun_jaxpr:core.TypedJaxpr,jvp_jaxpr_thunk:Callable)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._custom_lin(*args:TfValOrUnit,**_)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._custom_vjp_call_jaxpr(*args:TfValOrUnit,fun_jaxpr:core.TypedJaxpr,**_)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._div(lhs,rhs)
jax.experimental.jax2tf.jax2tf._dot_general(lhs,rhs,dimension_numbers,precision)
jax.experimental.jax2tf.jax2tf._dynamic_slice(operand,*start_indices,slice_sizes=None)
jax.experimental.jax2tf.jax2tf._dynamic_update_slice(operand,update,*start_indices)
jax.experimental.jax2tf.jax2tf._gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.experimental.jax2tf.jax2tf._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._gather_shape(operand,start_indices,dimension_numbers,slice_sizes)
jax.experimental.jax2tf.jax2tf._get_shape_from_tensor_or_array(x)
jax.experimental.jax2tf.jax2tf._infer_shape_jax(f,*vals,**params)
jax.experimental.jax2tf.jax2tf._interpret_fun(fun:lu.WrappedFun,in_vals:Sequence[TfValOrUnit])->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._interpret_jaxpr(jaxpr:core.TypedJaxpr,*args:TfValOrUnit)->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._interpret_subtrace(master:core.MasterTrace,*in_vals:TfValOrUnit)
jax.experimental.jax2tf.jax2tf._is_tfval(v:TfVal)->bool
jax.experimental.jax2tf.jax2tf._is_tfvalorunit(v:TfValOrUnit)->bool
jax.experimental.jax2tf.jax2tf._not(x)
jax.experimental.jax2tf.jax2tf._pad(operand,padding_value,padding_config)
jax.experimental.jax2tf.jax2tf._pad_shape(operand,padding_value,padding_config)
jax.experimental.jax2tf.jax2tf._qr(operand,full_matrices)
jax.experimental.jax2tf.jax2tf._reduce_window(jax_f,reducer,init_val,operand,window_dimensions,window_strides,padding,base_dilation,window_dilation,input_shape=None)
jax.experimental.jax2tf.jax2tf._reduce_window_shape(jax_f,operand,window_dimensions,window_strides,padding,base_dilation,window_dilation,input_shape=None)
jax.experimental.jax2tf.jax2tf._register_checkpoint_pytrees()
jax.experimental.jax2tf.jax2tf._rem(lhs,rhs)
jax.experimental.jax2tf.jax2tf._reshape(operand,new_sizes,dimensions)
jax.experimental.jax2tf.jax2tf._rev(operand,dimensions)
jax.experimental.jax2tf.jax2tf._scan(*tf_args:TfValOrUnit,**kwargs)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._scatter(update_computation,operand,scatter_indices,updates,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.experimental.jax2tf.jax2tf._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.experimental.jax2tf.jax2tf._scatter_shape(operand,scatter_indices,updates,dimension_numbers)
jax.experimental.jax2tf.jax2tf._select_and_scatter_add(operand,source,init_value,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.experimental.jax2tf.jax2tf._shift_right_arithmetic(x,y)
jax.experimental.jax2tf.jax2tf._shift_right_logical(x,y)
jax.experimental.jax2tf.jax2tf._slice(operand,start_indices,limit_indices,strides)
jax.experimental.jax2tf.jax2tf._sort(*operand:TfVal,dimension:int,is_stable:bool,num_keys:int)->Tuple[TfVal, ...]
jax.experimental.jax2tf.jax2tf._squeeze(operand,dimensions)
jax.experimental.jax2tf.jax2tf._tfval_add_unit(vals:Sequence[TfValOrUnit],avals:Sequence[core.AbstractValue])->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._tfval_remove_unit(args:Sequence[TfValOrUnit])->Sequence[TfVal]
jax.experimental.jax2tf.jax2tf._threefry2x32(key1,key2,x1,x2)
jax.experimental.jax2tf.jax2tf._top_k(operand:TfVal,k:int)->Tuple[TfVal, TfVal]
jax.experimental.jax2tf.jax2tf._transpose(operand,permutation)
jax.experimental.jax2tf.jax2tf._try_tf_gather(operand,start_indices,dimension_numbers,slice_sizes)
jax.experimental.jax2tf.jax2tf._unexpected_primitive(p:core.Primitive,*args,**kwargs)
jax.experimental.jax2tf.jax2tf._while(*args:TfValOrUnit,cond_nconsts:int,cond_jaxpr:core.TypedJaxpr,body_nconsts:int,body_jaxpr:core.TypedJaxpr)->Sequence[TfValOrUnit]
jax.experimental.jax2tf.jax2tf._xla_compile(func:Callable,*args:TfVal)->TfVal
jax.experimental.jax2tf.jax2tf.abstractify(t:Union[tf.Tensor,tf.Variable])
jax.experimental.jax2tf.jax2tf.bool_to_int8(f,argnums)
jax.experimental.jax2tf.jax2tf.convert(fun,with_gradient=False)
jax.experimental.jax2tf.jax2tf.promote_types(*values)
jax.experimental.jax2tf.jax2tf.to_jax_dtype(tf_dtype)
jax.experimental.jax2tf.jax2tf.to_tf_dtype(jax_dtype)
jax.experimental.jax2tf.jax2tf.uadd(a,*b)
jax.experimental.jax2tf.jax2tf.wrap_binary_op(func)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/tf_test_util.py----------------------------------------
A:jax.experimental.jax2tf.tests.tf_test_util.result_jax->func_jax(*args)
A:jax.experimental.jax2tf.tests.tf_test_util.func_tf->jax.experimental.jax2tf.convert(func_jax)
A:jax.experimental.jax2tf.tests.tf_test_util.result_tf->run_tf(mode)
A:jax.experimental.jax2tf.tests.tf_test_util.t_arg->numpy.stack([arg] * 4)
A:jax.experimental.jax2tf.tests.tf_test_util.grad_func->jax.grad(lambda x: jnp.sum(jax.vmap(func)(x)))
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase(jtu.JaxTestCase)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.ConvertAndCompare(self,func_jax:Callable,*args,custom_assert:Optional[Callable]=None,expect_tf_exceptions:bool=False,atol=None,rtol=None)->Tuple[Any, Any]
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.TransformConvertAndCompare(self,func:Callable,arg,transform:Optional[str])
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.assertDtypesMatch(self,x,y,*,canonicalize_dtypes=True)
jax.experimental.jax2tf.tests.tf_test_util.JaxToTfTestCase.setUp(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/primitives_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitives_test.tf_impl->set(jax.experimental.jax2tf.jax2tf.tf_impl)
A:jax.experimental.jax2tf.tests.primitives_test.tf_not_yet_impl->set(jax.experimental.jax2tf.jax2tf.tf_not_yet_impl)
A:jax.experimental.jax2tf.tests.primitives_test.all_primitives->tuple(sorted(all_primitives, key=str))
A:jax.experimental.jax2tf.tests.primitives_test.x->numpy.array([1, 2], dtype=x_dtype)
A:jax.experimental.jax2tf.tests.primitives_test.y->numpy.array([3, 4], dtype=y_dtype)
A:jax.experimental.jax2tf.tests.primitives_test.f_jax->jax.jit(lambda key: jax.random.split(key, 2))
A:jax.experimental.jax2tf.tests.primitives_test.(arg,)->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.primitives_test.nr_special_cases->numpy.count_nonzero(special_cases)
A:jax.experimental.jax2tf.tests.primitives_test.signs->numpy.where(arg[special_cases] < 0.0, -1.0, 1.0)
A:jax.experimental.jax2tf.tests.primitives_test.(arg1, arg2)->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.primitives_test.args->harness.dyn_args_maker(self.rng())
A:jax.experimental.jax2tf.tests.primitives_test.values->numpy.array([True, False, True], dtype=np.bool_)
A:jax.experimental.jax2tf.tests.primitives_test.indices->jax.numpy.array([[1, 1, 2], [0, 1, 0]])
A:jax.experimental.jax2tf.tests.primitives_test.params->jax.numpy.array([[1.0, 1.5, 2.0], [2.0, 2.5, 3.0], [3.0, 3.5, 4.0]])
A:jax.experimental.jax2tf.tests.primitives_test.update->numpy.float32(6.0)
A:jax.experimental.jax2tf.tests.primitives_test.v->numpy.float32(2.0)
A:jax.experimental.jax2tf.tests.primitives_test.f->jax.experimental.jax2tf.convert(lax.stop_gradient)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_betainc(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_binary_elementwise(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_binary_elementwise_logical(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_bitwise_not(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_boolean_gather(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_concat(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_cumulated_ops(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_dynamic_slice(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_dynamic_update_slice(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_gather(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_gather_rank_change(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_pad(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_primitive_coverage(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_prngsplit(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_qr(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_boolean_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_reduce_ops_with_numerical_input(self,f_jax)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_scatter_static(self,op)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_shift_left(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_shift_right_arithmetic(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_shift_right_logical(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_slice(self,harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_sort(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_squeeze(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_stop_gradient(self)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_top_k(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_type_promotion(self,f_jax=jnp.add)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_unary_elementwise(self,harness:primitive_harness.Harness)
jax.experimental.jax2tf.tests.primitives_test.JaxPrimitiveTest.test_zeros_like(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/control_flow_ops_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.control_flow_ops_test.res->jax.lax.cond(True, lambda op: op * x, lambda op: op + x, x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.cond_const->numpy.ones(3, dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const1->numpy.full_like(cond_const, 1.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const2->numpy.full_like(cond_const, 2.0)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.xs->numpy.arange(4, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.ys->numpy.arange(5, dtype=np.int32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.body_const->numpy.ones((2,), dtype=np.float32)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.arg->numpy.full((5,), 0.7)
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(c_out, _)->jax.lax.scan(body, 0.0, (xs, ys))
A:jax.experimental.jax2tf.tests.control_flow_ops_test.(res1, res2)->jax.lax.scan(body_fun, 0.0, xs + 1.0)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_multiple_results(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_cond_units(self,with_function=True)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_custom_vjp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_partial_eval(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_scan_remat(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_batched_cond(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_custom_jvp(self)
jax.experimental.jax2tf.tests.control_flow_ops_test.ControlFlowOpsTest.test_while_single_carry(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/jax2tf_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.jax2tf_test.(_, res_tf)->self.ConvertAndCompare(f_jax, 0.7)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_tf->tensorflow.function(f_tf, autograph=False)
A:jax.experimental.jax2tf.tests.jax2tf_test.v->tensorflow.Variable(0.7)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.jax2tf_test.n->jax.local_device_count()
A:jax.experimental.jax2tf.tests.jax2tf_test.big_const->numpy.full((5,), 2 ** 33, dtype=dtype)
A:jax.experimental.jax2tf.tests.jax2tf_test.f_conv->tensorflow.function(f_conv)
A:jax.experimental.jax2tf.tests.jax2tf_test.x->jax.experimental.jax2tf.convert(jnp.sin)(1.0)
A:jax.experimental.jax2tf.tests.jax2tf_test.y->f_tf(x)
A:jax.experimental.jax2tf.tests.jax2tf_test._->tape.gradient(y, x)
A:jax.experimental.jax2tf.tests.jax2tf_test.(u, v)->f_tf(x, y)
A:jax.experimental.jax2tf.tests.jax2tf_test.uv->f_tf((x, y))
A:jax.experimental.jax2tf.tests.jax2tf_test.primal_out->f(x)
A:jax.experimental.jax2tf.tests.jax2tf_test.m->tensorflow.Module()
A:jax.experimental.jax2tf.tests.jax2tf_test.x2->jax.numpy.sin(x1)
A:jax.experimental.jax2tf.tests.jax2tf_test.x3->jax.numpy.sin(x2)
A:jax.experimental.jax2tf.tests.jax2tf_test.x4->jax.numpy.sin(x3)
A:jax.experimental.jax2tf.tests.jax2tf_test.arg->numpy.arange(3.0)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_argument_eager_tensor(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_basics(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_passed_by_tf(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_bfloat16_returned_by_jax(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_checkpoint_wrapper_types(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_callable_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_convert_argument_non_tensor_error(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_64bit(self,dtype=np.int64,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_converts_jax_arrays(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_jvp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_custom_vjp(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_function(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_disabled(self,with_function=False)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_pytree(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_jvp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_gradients_with_custom_vjp(self,with_function=True)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_nested_jit(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_pytrees(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat1(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_remat_free_var(self)
jax.experimental.jax2tf.tests.jax2tf_test.Jax2TfTest.test_variable_input(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/stax_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.stax_test.this_dir->os.path.dirname(os.path.abspath(__file__))
A:jax.experimental.jax2tf.tests.stax_test.examples_dir->os.path.abspath(os.path.join(this_dir, '..', '..', '..', '..', 'examples'))
A:jax.experimental.jax2tf.tests.stax_test.resnet50->from_examples_import_resnet50()
A:jax.experimental.jax2tf.tests.stax_test.key->jax.random.PRNGKey(0)
A:jax.experimental.jax2tf.tests.stax_test.(init_fn, apply_fn)->from_examples_import_resnet50().ResNet50(1000)
A:jax.experimental.jax2tf.tests.stax_test.(_, params)->init_fn(key, shape)
A:jax.experimental.jax2tf.tests.stax_test.infer->functools.partial(apply_fn, params)
A:jax.experimental.jax2tf.tests.stax_test.images->numpy.array(jax.random.normal(key, shape))
jax.experimental.jax2tf.tests.stax_test.StaxTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.stax_test.StaxTest.test_res_net(self)
jax.experimental.jax2tf.tests.stax_test.from_examples_import_resnet50()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/primitive_harness.py----------------------------------------
A:jax.experimental.jax2tf.tests.primitive_harness.all_args->self._args_from_dynargs(dyn_args)
A:jax.experimental.jax2tf.tests.primitive_harness.cases->tuple((dict(testcase_name=harness.name if one_containing is None else '', harness=harness) for harness in harness_group if one_containing is None or one_containing in harness.name))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_unary_elementwise->tuple((Harness(f'{f_lax.__name__}_{jtu.dtype_str(dtype)}', f_lax, [arg], lax_name=f_lax.__name__, dtype=dtype) for f_lax in _LAX_UNARY_ELEMENTWISE for dtype in jtu.dtypes.all_floating for arg in [np.array([-1.6, -1.4, -1.0, 0.0, 0.1, 0.2, 1.0, 1.4, 1.6], dtype=dtype)]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_bitwise_not->tuple([Harness(f'{jtu.dtype_str(dtype)}', lax.bitwise_not, [arg], dtype=dtype) for dtype in jtu.dtypes.all_integer + jtu.dtypes.all_unsigned for arg in [np.array([-1, -3, -2, 0, 0, 2, 1, 3], dtype=dtype)]] + [Harness('bool', f_lax, [arg], lax_name=f_lax.__name__, dtype=np.bool_) for f_lax in [lax.bitwise_not] for arg in [np.array([True, False])]])
A:jax.experimental.jax2tf.tests.primitive_harness.lax_binary_elementwise->tuple((Harness(f'{f_lax.__name__}_{jtu.dtype_str(dtype)}', f_lax, [arg1, arg2], lax_name=f_lax.__name__, dtype=dtype) for f_lax in _LAX_BINARY_ELEMENTWISE for dtype in jtu.dtypes.all_floating for (arg1, arg2) in [(np.array([-1.6, -1.4, -1.0, 0.0, 0.1, 0.2, 1.0, 1.4, 1.6], dtype=dtype), np.array([-1.6, 1.4, 1.0, 0.0, 0.1, 0.2, 1.0, 1.4, -1.6], dtype=dtype))]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_binary_elementwise_logical->tuple([Harness(f'{f_lax.__name__}_{jtu.dtype_str(dtype)}', f_lax, [arg1, arg2], lax_name=f_lax.__name__, dtype=dtype) for f_lax in _LAX_BINARY_ELEMENTWISE_LOGICAL for dtype in jtu.dtypes.all_integer + jtu.dtypes.all_unsigned for (arg1, arg2) in [(np.array([1, 3, 2, 0, 0, 2, 1, 3], dtype=dtype), np.array([1, 2, 3, 0, 1, 0, 2, 3], dtype=dtype))]] + [Harness(f'{f_lax.__name__}_bool', f_lax, [arg1, arg2], lax_name=f_lax.__name__, dtype=np.bool_) for f_lax in [lax.bitwise_and, lax.bitwise_or, lax.bitwise_xor] for (arg1, arg2) in [(np.array([True, True, False, False]), np.array([True, False, True, False]))]])
A:jax.experimental.jax2tf.tests.primitive_harness.lax_betainc->tuple((Harness(f'_{jtu.dtype_str(dtype)}', lax.betainc, [arg1, arg2, arg3], dtype=dtype) for dtype in jtu.dtypes.all_floating for (arg1, arg2, arg3) in [(np.array([-1.6, -1.4, -1.0, 0.0, 0.1, 0.3, 1, 1.4, 1.6], dtype=dtype), np.array([-1.6, 1.4, 1.0, 0.0, 0.2, 0.1, 1, 1.4, -1.6], dtype=dtype), np.array([1.0, -1.0, 2.0, 1.0, 0.3, 0.3, -1.0, 2.4, 1.6], dtype=np.float32))]))
A:jax.experimental.jax2tf.tests.primitive_harness._gather_input->numpy.arange(1000, dtype=np.float32).reshape((10, 10, 10))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_gather->tuple([Harness(f'from_take_indices_shape={indices.shape}_axis={axis}', lambda a, i, axis: jnp.take(a, i, axis=axis), [_gather_input, indices, StaticArg(axis)]) for indices in [np.array(2, dtype=np.int32), np.array([2], dtype=np.int32), np.array([2, 4], dtype=np.int32), np.array([[2, 4], [5, 6]], dtype=np.int32), np.array([0, 1, 10], dtype=np.int32), np.array([0, 1, 2, -1], dtype=np.int32)] for axis in [0, 1, 2]] + [Harness(f'_shape={shape}_idxs_shape={idxs.shape}_dnums={dnums}_slice_sizes={slice_sizes}', lambda op, idxs, dnums, slice_sizes: lax.gather(op, idxs, dimension_numbers=dnums, slice_sizes=slice_sizes), [RandArg(shape, np.float32), idxs, StaticArg(dnums), StaticArg(slice_sizes)]) for (shape, idxs, dnums, slice_sizes) in [((5,), np.array([[0], [2]]), lax.GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=(0,), start_index_map=(0,)), (1,)), ((10,), np.array([[0], [0], [0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(), start_index_map=(0,)), (2,)), ((10, 5), np.array([[0], [2], [1]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0,)), (1, 3)), ((10, 5), np.array([[0, 2], [1, 0]]), lax.GatherDimensionNumbers(offset_dims=(1,), collapsed_slice_dims=(0,), start_index_map=(0, 1)), (1, 3))]])
A:jax.experimental.jax2tf.tests.primitive_harness.lax_pad->tuple((Harness(f'_inshape={jtu.format_shape_dtype_string(arg_shape, dtype)}_pads={pads}', lax.pad, [RandArg(arg_shape, dtype), np.array(0, dtype), StaticArg(pads)], rng_factory=jtu.rand_small, arg_shape=arg_shape, dtype=dtype, pads=pads) for arg_shape in [(2, 3)] for dtype in jtu.dtypes.all_floating + jtu.dtypes.all_integer for pads in [[(0, 0, 0), (0, 0, 0)], [(1, 1, 0), (2, 2, 0)], [(1, 2, 1), (0, 1, 0)], [(0, 0, 0), (-1, -1, 0)], [(0, 0, 0), (-2, -2, 4)], [(0, 0, 0), (-2, -3, 1)]]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_linalg_qr->tuple((Harness(f'multi_array_shape={jtu.format_shape_dtype_string(shape, dtype)}_fullmatrices={full_matrices}', lax_linalg.qr, [RandArg(shape, dtype), StaticArg(full_matrices)], shape=shape, dtype=dtype, full_matrices=full_matrices) for dtype in jtu.dtypes.all for shape in [(1, 1), (3, 3), (3, 4), (2, 10, 5), (2, 200, 100)] for full_matrices in [False, True]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_slice->tuple((Harness(f'_shape={shape}_start_indices={start_indices}_limit_indices={limit_indices}_strides={strides}', lax.slice, [RandArg(shape, dtype), StaticArg(start_indices), StaticArg(limit_indices), StaticArg(strides)], shape=shape, start_indices=start_indices, limit_indices=limit_indices) for (shape, start_indices, limit_indices, strides) in [[(3,), (1,), (2,), None], [(7,), (4,), (7,), None], [(5,), (1,), (5,), (2,)], [(8,), (1,), (6,), (2,)], [(5, 3), (1, 1), (3, 2), None], [(5, 3), (1, 1), (3, 1), None], [(7, 5, 3), (4, 0, 1), (7, 1, 3), None], [(5, 3), (1, 1), (2, 1), (1, 1)], [(5, 3), (1, 1), (5, 3), (2, 1)], [(5,), (-1,), (0,), None], [(5,), (-1,), (1,), None], [(5,), (-4,), (-2,), None], [(5,), (-5,), (-2,), None], [(5,), (-6,), (-5,), None], [(5,), (-10,), (-9,), None], [(5,), (-100,), (-99,), None], [(5,), (5,), (6,), None], [(5,), (10,), (11,), None], [(5,), (0,), (100,), None]] for dtype in [np.float32]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_dynamic_update_slice->tuple((Harness(f'_operand={jtu.format_shape_dtype_string(shape, dtype)}_update={jtu.format_shape_dtype_string(update_shape, update_dtype)}_start_indices={start_indices}', lax.dynamic_update_slice, [RandArg(shape, dtype), RandArg(update_shape, update_dtype), np.array(start_indices)], shape=shape, start_indices=start_indices, update_shape=update_shape) for (shape, start_indices, update_shape) in [[(3,), (1,), (1,)], [(5, 3), (1, 1), (3, 1)], [(7, 5, 3), (4, 1, 0), (2, 0, 1)], [(3,), (-1,), (1,)], [(3,), (10,), (1,)], [(3,), (10,), (4,)], [(3,), (10,), (2,)]] for (dtype, update_dtype) in [(np.float32, np.float32), (np.float64, np.float32)]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_squeeze->tuple((Harness(f'_inshape={jtu.format_shape_dtype_string(arg_shape, dtype)}_dimensions={dimensions}', lax.squeeze, [RandArg(arg_shape, dtype), StaticArg(dimensions)], arg_shape=arg_shape, dtype=dtype, dimensions=dimensions) for (arg_shape, dimensions) in [[(1,), (0,)], [(1,), (-1,)], [(2, 1, 4), (1,)], [(2, 1, 4), (-2,)], [(2, 1, 3, 1), (1,)], [(2, 1, 3, 1), (1, 3)], [(2, 1, 3, 1), (3,)], [(2, 1, 3, 1), (1, -1)]] for dtype in [np.float32]))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_shift_left->tuple((Harness(f'_dtype={dtype.__name__}_shift_amount={shift_amount}', lax.shift_left, [arg, StaticArg(np.array([shift_amount], dtype=dtype))]) for (arg, dtype, shift_amount) in shift_inputs))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_shift_right_logical->tuple((Harness(f'_dtype={dtype.__name__}_shift_amount={shift_amount}', lax.shift_right_logical, [arg, StaticArg(np.array([shift_amount], dtype=dtype))]) for (arg, dtype, shift_amount) in shift_inputs))
A:jax.experimental.jax2tf.tests.primitive_harness.lax_shift_right_arithmetic->tuple((Harness(f'_dtype={dtype.__name__}_shift_amount={shift_amount}', lax.shift_right_arithmetic, [arg, StaticArg(np.array([shift_amount], dtype=dtype))]) for (arg, dtype, shift_amount) in shift_inputs))
jax.experimental.jax2tf.tests.primitive_harness.Harness(self,name,fun,arg_descriptors,*,rng_factory=jtu.rand_default,**params)
jax.experimental.jax2tf.tests.primitive_harness.Harness.__str__(self)
jax.experimental.jax2tf.tests.primitive_harness.Harness._arg_maker(self,arg_descriptor,rng:Rng)
jax.experimental.jax2tf.tests.primitive_harness.Harness._args_from_dynargs(self,dyn_args:Sequence)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_args_maker(self,rng:Rng)->Sequence
jax.experimental.jax2tf.tests.primitive_harness.Harness.dyn_fun(self,*dyn_args)
jax.experimental.jax2tf.tests.primitive_harness.RandArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.StaticArg(NamedTuple)
jax.experimental.jax2tf.tests.primitive_harness.parameterized(harness_group:Iterable[Harness],one_containing:Optional[str]=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/experimental/jax2tf/tests/savedmodel_test.py----------------------------------------
A:jax.experimental.jax2tf.tests.savedmodel_test.model_dir->os.path.join(absltest.get_default_test_tmpdir(), str(id(model)))
A:jax.experimental.jax2tf.tests.savedmodel_test.restored_model->self.save_and_load_model(model)
A:jax.experimental.jax2tf.tests.savedmodel_test.f_jax->jax.jit(lambda x: jnp.sin(jnp.cos(x)))
A:jax.experimental.jax2tf.tests.savedmodel_test.model->tensorflow.Module()
A:jax.experimental.jax2tf.tests.savedmodel_test.model.f->tensorflow.function(jax2tf.convert(f_jax, with_gradient=True), autograph=False, input_signature=[tf.TensorSpec([], tf.float32)])
A:jax.experimental.jax2tf.tests.savedmodel_test.x->numpy.array(0.7)
A:jax.experimental.jax2tf.tests.savedmodel_test.xv->tensorflow.Variable(0.7)
A:jax.experimental.jax2tf.tests.savedmodel_test._->tape.gradient(y, xv)
A:jax.experimental.jax2tf.tests.savedmodel_test.primal_out->f_jax(x)
A:jax.experimental.jax2tf.tests.savedmodel_test.y->self.save_and_load_model(model).f(xv)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest(tf_test_util.JaxToTfTestCase)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.save_and_load_model(self,model:tf.Module)->tf.Module
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_eval(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient(self)
jax.experimental.jax2tf.tests.savedmodel_test.SavedModelTest.test_gradient_disabled(self)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/third_party/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/third_party/numpy/linalg.py----------------------------------------
A:jax.third_party.numpy.linalg.s->numpy.empty((n, n), dtype=np.intp)
A:jax.third_party.numpy.linalg.invx->jax.numpy.linalg.inv(x)
A:jax.third_party.numpy.linalg.orig_nan_check->jax.numpy.lax_numpy.full_like(r, ~jnp.isnan(r).any())
A:jax.third_party.numpy.linalg.nan_mask->jax.numpy.lax_numpy.logical_and(jnp.isnan(r), ~jnp.isnan(x).any(axis=(-2, -1)))
A:jax.third_party.numpy.linalg.r->jax.numpy.lax_numpy.where(orig_nan_check, jnp.where(nan_mask, jnp.inf, r), r)
A:jax.third_party.numpy.linalg.a->a.reshape(-1, prod).reshape(-1, prod)
A:jax.third_party.numpy.linalg.ia->jax.numpy.linalg.inv(a)
A:jax.third_party.numpy.linalg.b->b.ravel().ravel()
A:jax.third_party.numpy.linalg.allaxes->list(range(0, an))
A:jax.third_party.numpy.linalg.res->res.reshape(Q).reshape(Q)
A:jax.third_party.numpy.linalg.n->len(arrays)
A:jax.third_party.numpy.linalg.arrays[0]->jax.numpy.lax_numpy.atleast_2d(arrays[0])
A:jax.third_party.numpy.linalg.result->_multi_dot(arrays, order, 0, n - 1, precision)
A:jax.third_party.numpy.linalg.order->_multi_dot_matrix_chain_order(arrays)
A:jax.third_party.numpy.linalg.m->numpy.zeros((n, n), dtype=np.double)
jax.third_party.numpy.linalg._assert2d(*arrays)
jax.third_party.numpy.linalg._assertNdSquareness(*arrays)
jax.third_party.numpy.linalg._assertNoEmpty2d(*arrays)
jax.third_party.numpy.linalg._assertRankAtLeast2(*arrays)
jax.third_party.numpy.linalg._isEmpty2d(arr)
jax.third_party.numpy.linalg._multi_dot(arrays,order,i,j,precision)
jax.third_party.numpy.linalg._multi_dot_matrix_chain_order(arrays,return_costs=False)
jax.third_party.numpy.linalg._multi_dot_three(A,B,C,precision)
jax.third_party.numpy.linalg.cond(x,p=None)
jax.third_party.numpy.linalg.multi_dot(arrays,*,precision=None)
jax.third_party.numpy.linalg.tensorinv(a,ind=2)
jax.third_party.numpy.linalg.tensorsolve(a,b,axes=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/third_party/numpy/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/linalg.py----------------------------------------
A:jax.numpy.linalg.dtype->lax.dtype(a)
A:jax.numpy.linalg.a->_promote_arg_dtypes(jnp.asarray(a))
A:jax.numpy.linalg.n->numpy.abs(n)
A:jax.numpy.linalg.(n, bit)->divmod(n, 2)
A:jax.numpy.linalg.M->_promote_arg_dtypes(jnp.asarray(M))
A:jax.numpy.linalg.S->svd(M, full_matrices=False, compute_uv=False)
A:jax.numpy.linalg.a_shape->jnp.shape(a)
A:jax.numpy.linalg.(lu, pivot)->lax_linalg.lu(a)
A:jax.numpy.linalg.diag->jnp.diagonal(lu, axis1=-2, axis2=-1)
A:jax.numpy.linalg.is_zero->jnp.any(diag == jnp.array(0, dtype=dtype), axis=-1)
A:jax.numpy.linalg.parity->jnp.count_nonzero(pivots != jnp.arange(a_shape[-1]), axis=-1)
A:jax.numpy.linalg.sign->jnp.array(-2 * (parity % 2) + 1, dtype=dtype)
A:jax.numpy.linalg.logdet->jnp.where(is_zero, jnp.array(-jnp.inf, dtype=dtype), jnp.sum(jnp.log(jnp.abs(diag)), axis=-1))
A:jax.numpy.linalg.(sign, ans)->slogdet(x)
A:jax.numpy.linalg.b->_promote_arg_dtypes(jnp.asarray(b))
A:jax.numpy.linalg.b_shape->jnp.shape(b)
A:jax.numpy.linalg.a_ndims->len(a_shape)
A:jax.numpy.linalg.(lu, pivots)->lax_linalg.lu(lax.stop_gradient(a))
A:jax.numpy.linalg.batch_dims->lax.broadcast_shapes(lu.shape[:-2], b.shape[:-2])
A:jax.numpy.linalg.x->x.ravel().ravel()
A:jax.numpy.linalg.lu->ops.index_update(lu, ops.index[..., -1, -1], 1.0 / partial_det[..., -2])
A:jax.numpy.linalg.permutation->jnp.broadcast_to(permutation, batch_dims + (a_shape[-1],))
A:jax.numpy.linalg.iotas->jnp.ix_(*(lax.iota(jnp.int32, b) for b in batch_dims + (1,)))
A:jax.numpy.linalg.d->jnp.tile(d[..., None, None], d.ndim * (1,) + x.shape[-2:])
A:jax.numpy.linalg.(sign, logdet)->slogdet(a)
A:jax.numpy.linalg.(y, z)->_cofactor_solve(x, g)
A:jax.numpy.linalg.(w, vl, vr)->lax_linalg.eig(a)
A:jax.numpy.linalg.(w, _)->eigh(a, UPLO)
A:jax.numpy.linalg.msg->"UPLO must be one of None, 'L', or 'U', got {}".format(UPLO)
A:jax.numpy.linalg.(v, w)->lax_linalg.eigh(a, lower=lower, symmetrize_input=symmetrize_input)
A:jax.numpy.linalg.max_rows_cols->max(a.shape[-2:])
A:jax.numpy.linalg.rcond->jnp.asarray(rcond)
A:jax.numpy.linalg.(u, s, v)->svd(a, full_matrices=False)
A:jax.numpy.linalg.s->jnp.where(s > cutoff, s, jnp.inf)
A:jax.numpy.linalg.res->jnp.matmul(_T(v), jnp.divide(_T(u), s[..., jnp.newaxis]))
A:jax.numpy.linalg.p->pinv(a, rcond=rcond)
A:jax.numpy.linalg.x_shape->jnp.shape(x)
A:jax.numpy.linalg.ndim->len(x_shape)
A:jax.numpy.linalg.axis->tuple((jnp._canonicalize_axis(x, ndim) for x in axis))
A:jax.numpy.linalg.num_axes->len(axis)
A:jax.numpy.linalg.abs_x->jnp.abs(x)
A:jax.numpy.linalg.ord->lax._const(abs_x, ord)
A:jax.numpy.linalg.out->jnp.sum(abs_x ** ord, axis=axis, keepdims=keepdims)
A:jax.numpy.linalg.(row_axis, col_axis)->cast(Tuple[int, ...], axis)
A:jax.numpy.linalg.y->jnp.reshape(y, result_shape)
A:jax.numpy.linalg.result_shape->list(x_shape)
A:jax.numpy.linalg.(q, r)->lax_linalg.qr(a, full_matrices)
A:jax.numpy.linalg.(a, b)->_promote_arg_dtypes(a, b)
A:jax.numpy.linalg.custom_solve->partial(lax.custom_linear_solve, lambda x: _matvec_multiply(a, x), solve=lambda _, x: lax_linalg.lu_solve(lu, pivots, x, trans=0), transpose_solve=lambda _, x: lax_linalg.lu_solve(lu, pivots, x, trans=1))
A:jax.numpy.linalg.(u, s, vt)->svd(a, full_matrices=False)
A:jax.numpy.linalg.rank->mask.sum()
A:jax.numpy.linalg.safe_s->jnp.where(mask, s, 1)
A:jax.numpy.linalg.uTb->jnp.matmul(u.conj().T, b, precision=lax.Precision.HIGHEST)
A:jax.numpy.linalg.resid->jnp.asarray([])
A:jax.numpy.linalg.b_estimate->jnp.matmul(a, x, precision=lax.Precision.HIGHEST)
A:jax.numpy.linalg.globals()[name]->_not_implemented(func)
jax.numpy.linalg._check_solve_shapes(a,b)
jax.numpy.linalg._cofactor_solve(a,b)
jax.numpy.linalg._det_jvp(primals,tangents)
jax.numpy.linalg._matvec_multiply(a,b)
jax.numpy.linalg._norm(x,ord,axis:Union[None,Tuple[int,...],int],keepdims)
jax.numpy.linalg._pinv_jvp(rcond,primals,tangents)
jax.numpy.linalg._promote_arg_dtypes(*args)
jax.numpy.linalg._slogdet_jvp(primals,tangents)
jax.numpy.linalg.cholesky(a)
jax.numpy.linalg.det(a)
jax.numpy.linalg.eig(a)
jax.numpy.linalg.eigh(a,UPLO=None,symmetrize_input=True)
jax.numpy.linalg.eigvals(a)
jax.numpy.linalg.eigvalsh(a,UPLO='L')
jax.numpy.linalg.inv(a)
jax.numpy.linalg.lstsq(a,b,rcond=None,*,numpy_resid=False)
jax.numpy.linalg.matrix_power(a,n)
jax.numpy.linalg.matrix_rank(M,tol=None)
jax.numpy.linalg.norm(x,ord=None,axis=None,keepdims=False)
jax.numpy.linalg.pinv(a,rcond=None)
jax.numpy.linalg.qr(a,mode='reduced')
jax.numpy.linalg.slogdet(a)
jax.numpy.linalg.solve(a,b)
jax.numpy.linalg.svd(a,full_matrices=True,compute_uv=True)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/_util.py----------------------------------------
A:jax.numpy._util.lines->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).split('\n')
A:jax.numpy._util.lines[idx]->line.replace('    ', '', 1)
A:jax.numpy._util.docstr->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body)
A:jax.numpy._util.begin_idx->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).find('Parameters')
A:jax.numpy._util.end_idx->'{summary}\n\nLAX-backend implementation of :func:`{fun}`.\n{lax_description}Original docstring below.\n\n{body}'.format(summary=summary, lax_description=desc, fun=fun.__name__, body=body).find('Returns', begin_idx)
A:jax.numpy._util.param_list->'\n'.join(param_list).replace('@@', '\n    ').replace('\n    ', '@@').split('\n')
A:jax.numpy._util.parameters->'\n'.join(param_list).replace('@@', '\n    ')
A:jax.numpy._util._numpy_signature_re->re.compile('^([\\w., ]+=)?\\s*[\\w\\.]+\\([\\w\\W]*\\)$')
A:jax.numpy._util.sections->fun.__doc__.split('\n\n')
A:jax.numpy._util.summary->sections[i].strip()
A:jax.numpy._util.body->update_numpydoc(body, fun, op)
jax.numpy._util._wraps(fun,update_doc=True,lax_description='')
jax.numpy._util.update_numpydoc(docstr,fun,op)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/lax_numpy.py----------------------------------------
A:jax.numpy.lax_numpy.bool_->_make_scalar_type(np.bool_)
A:jax.numpy.lax_numpy.uint8->_make_scalar_type(np.uint8)
A:jax.numpy.lax_numpy.uint16->_make_scalar_type(np.uint16)
A:jax.numpy.lax_numpy.uint32->_make_scalar_type(np.uint32)
A:jax.numpy.lax_numpy.uint64->_make_scalar_type(np.uint64)
A:jax.numpy.lax_numpy.int8->_make_scalar_type(np.int8)
A:jax.numpy.lax_numpy.int16->_make_scalar_type(np.int16)
A:jax.numpy.lax_numpy.int32->_make_scalar_type(np.int32)
A:jax.numpy.lax_numpy.int64->_make_scalar_type(np.int64)
A:jax.numpy.lax_numpy.bfloat16->_make_scalar_type(dtypes.bfloat16)
A:jax.numpy.lax_numpy.float16->_make_scalar_type(np.float16)
A:jax.numpy.lax_numpy.float32single->_make_scalar_type(np.float32)
A:jax.numpy.lax_numpy.float64double->_make_scalar_type(np.float64)
A:jax.numpy.lax_numpy.complex64csingle->_make_scalar_type(np.complex64)
A:jax.numpy.lax_numpy.complex128cdouble->_make_scalar_type(np.complex128)
A:jax.numpy.lax_numpy.arr->arr.astype(uint8).astype(uint8)
A:jax.numpy.lax_numpy.obj_dtype->getattr(obj, 'dtype', None)
A:jax.numpy.lax_numpy._np_asarray->partial(_np_array, copy=False)
A:jax.numpy.lax_numpy.result_rank->len(lax.broadcast_shapes(*shapes))
A:jax.numpy.lax_numpy.to_dtype->_to_inexact_dtype(result_type(*args))
A:jax.numpy.lax_numpy.(pos, arg)->next(((i, arg) for (i, arg) in enumerate(args) if not _arraylike(arg)))
A:jax.numpy.lax_numpy.x->remainder(x, a_shape[i] or 1)
A:jax.numpy.lax_numpy.(x1, x2)->_promote_dtypes(x1, x2)
A:jax.numpy.lax_numpy.fabs->_one_to_one_unop(np.fabs, lax.abs, True)
A:jax.numpy.lax_numpy.bitwise_not->_one_to_one_unop(np.bitwise_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.negative->_one_to_one_unop(np.negative, lax.neg)
A:jax.numpy.lax_numpy.positive->_one_to_one_unop(np.positive, lambda x: x)
A:jax.numpy.lax_numpy.floor->_one_to_one_unop(np.floor, lax.floor, True)
A:jax.numpy.lax_numpy.ceil->_one_to_one_unop(np.ceil, lax.ceil, True)
A:jax.numpy.lax_numpy.exp->_one_to_one_unop(np.exp, lax.exp, True)
A:jax.numpy.lax_numpy.log->_one_to_one_unop(np.log, lax.log, True)
A:jax.numpy.lax_numpy.expm1->_one_to_one_unop(np.expm1, lax.expm1, True)
A:jax.numpy.lax_numpy.log1p->_one_to_one_unop(np.log1p, lax.log1p, True)
A:jax.numpy.lax_numpy.sin->_one_to_one_unop(np.sin, lax.sin, True)
A:jax.numpy.lax_numpy.cos->_one_to_one_unop(np.cos, lax.cos, True)
A:jax.numpy.lax_numpy.tan->_one_to_one_unop(np.tan, lax.tan, True)
A:jax.numpy.lax_numpy.arcsin->_one_to_one_unop(np.arcsin, lax.asin, True)
A:jax.numpy.lax_numpy.arccos->_one_to_one_unop(np.arccos, lax.acos, True)
A:jax.numpy.lax_numpy.arctan->_one_to_one_unop(np.arctan, lax.atan, True)
A:jax.numpy.lax_numpy.sinh->_one_to_one_unop(np.sinh, lax.sinh, True)
A:jax.numpy.lax_numpy.cosh->_one_to_one_unop(np.cosh, lax.cosh, True)
A:jax.numpy.lax_numpy.arcsinh->_one_to_one_unop(np.arcsinh, lax.asinh, True)
A:jax.numpy.lax_numpy.tanh->_one_to_one_unop(np.tanh, lax.tanh, True)
A:jax.numpy.lax_numpy.arccosh->_one_to_one_unop(np.arccosh, lax.acosh, True)
A:jax.numpy.lax_numpy.arctanh->_one_to_one_unop(np.arctanh, lax.atanh, True)
A:jax.numpy.lax_numpy.sqrt->_one_to_one_unop(np.sqrt, lax.sqrt, True)
A:jax.numpy.lax_numpy.add->_maybe_bool_binop(np.add, lax.add, lax.bitwise_or)
A:jax.numpy.lax_numpy.bitwise_and->_one_to_one_binop(np.bitwise_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.bitwise_or->_one_to_one_binop(np.bitwise_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.bitwise_xor->_one_to_one_binop(np.bitwise_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.right_shift->_one_to_one_binop(np.right_shift, lax.shift_right_arithmetic)
A:jax.numpy.lax_numpy.left_shift->_one_to_one_binop(np.left_shift, lax.shift_left)
A:jax.numpy.lax_numpy.equal->_one_to_one_binop(np.equal, lax.eq)
A:jax.numpy.lax_numpy.multiply->_maybe_bool_binop(np.multiply, lax.mul, lax.bitwise_and)
A:jax.numpy.lax_numpy.not_equal->_one_to_one_binop(np.not_equal, lax.ne)
A:jax.numpy.lax_numpy.subtract->_one_to_one_binop(np.subtract, lax.sub)
A:jax.numpy.lax_numpy.arctan2->_one_to_one_binop(np.arctan2, lax.atan2, True)
A:jax.numpy.lax_numpy.minimum->_one_to_one_binop(np.minimum, lax.min)
A:jax.numpy.lax_numpy.maximum->_one_to_one_binop(np.maximum, lax.max)
A:jax.numpy.lax_numpy.float_power->_one_to_one_binop(np.float_power, lax.pow, True)
A:jax.numpy.lax_numpy.nextafter->_one_to_one_binop(np.nextafter, lax.nextafter, True)
A:jax.numpy.lax_numpy.rx->lax.real(x1)
A:jax.numpy.lax_numpy.ry->lax.real(x2)
A:jax.numpy.lax_numpy.greater_equal->_comparison_op(np.greater_equal, lax.ge)
A:jax.numpy.lax_numpy.greater->_comparison_op(np.greater, lax.gt)
A:jax.numpy.lax_numpy.less_equal->_comparison_op(np.less_equal, lax.le)
A:jax.numpy.lax_numpy.less->_comparison_op(np.less, lax.lt)
A:jax.numpy.lax_numpy.logical_and->_logical_op(np.logical_and, lax.bitwise_and)
A:jax.numpy.lax_numpy.logical_not->_logical_op(np.logical_not, lax.bitwise_not)
A:jax.numpy.lax_numpy.logical_or->_logical_op(np.logical_or, lax.bitwise_or)
A:jax.numpy.lax_numpy.logical_xor->_logical_op(np.logical_xor, lax.bitwise_xor)
A:jax.numpy.lax_numpy.abs->_wraps(np.abs)(absolute)
A:jax.numpy.lax_numpy.dtype->_dtype(x)
A:jax.numpy.lax_numpy.re->lax.real(x)
A:jax.numpy.lax_numpy.result_dtype->_result_dtype(np.divide, x1, x2)
A:jax.numpy.lax_numpy.quotient->lax.div(x1, x2)
A:jax.numpy.lax_numpy.select->logical_and(lax.sign(x1) != lax.sign(x2), lax.rem(x1, x2) != 0)
A:jax.numpy.lax_numpy.x1r->lax.real(x1)
A:jax.numpy.lax_numpy.x1i->lax.imag(x1)
A:jax.numpy.lax_numpy.x2r->lax.real(x2)
A:jax.numpy.lax_numpy.x2i->lax.imag(x2)
A:jax.numpy.lax_numpy.which->lax.ge(lax.abs(x2r), lax.abs(x2i))
A:jax.numpy.lax_numpy.rat1->where(which, lax._const(x2i, 1), lax.div(x2r, x2i))
A:jax.numpy.lax_numpy.rat2->where(which, lax.div(x2i, x2r), lax._const(x2i, 1))
A:jax.numpy.lax_numpy.out->round(number, decimals=ndigits or 0)
A:jax.numpy.lax_numpy.mod->_wraps(np.mod)(remainder)
A:jax.numpy.lax_numpy.div->lax.select(ind, div - _constant_like(div, 1), div)
A:jax.numpy.lax_numpy.ind->lax.bitwise_and(mod != 0, lax.sign(x2) != lax.sign(mod))
A:jax.numpy.lax_numpy.acc->where(lax.bitwise_and(x2, _constant_like(x2, 1)), lax.mul(acc, x1), acc)
A:jax.numpy.lax_numpy.x1->lax.bitcast_convert_type(x1, dtype)
A:jax.numpy.lax_numpy.x2->where(x2 == 0, 1, x2)
A:jax.numpy.lax_numpy.amax->lax.max(x1, x2)
A:jax.numpy.lax_numpy.delta->lax.sub(x1, x2)
A:jax.numpy.lax_numpy.(x1, x2, t1, t2)->broadcast_arrays(x1, x2, t1, t2)
A:jax.numpy.lax_numpy.primal_out->logaddexp2(x1, x2)
A:jax.numpy.lax_numpy.(x,)->_promote_dtypes_inexact(x)
A:jax.numpy.lax_numpy.info->finfo(dtypes.canonicalize_dtype(dtype))
A:jax.numpy.lax_numpy.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.numpy.lax_numpy.dx->moveaxis(diff(x, axis=axis), axis, -1)
A:jax.numpy.lax_numpy.(x, y)->_promote_dtypes(x, y)
A:jax.numpy.lax_numpy.out_order->slice(None, None, -1)
A:jax.numpy.lax_numpy.result->lax.mul(lax.add(low_value, high_value), _constant_like(low_value, 0.5))
A:jax.numpy.lax_numpy.(x, e)->_normalize_float(x1)
A:jax.numpy.lax_numpy.m->ndim(x)
A:jax.numpy.lax_numpy.zero->lax.full_like(normalizer, 0, shape=())
A:jax.numpy.lax_numpy.trunc_mod->lax.rem(x1, x2)
A:jax.numpy.lax_numpy.trunc_mod_not_zero->lax.ne(trunc_mod, zero)
A:jax.numpy.lax_numpy.do_plus->lax.bitwise_and(lax.ne(lax.lt(trunc_mod, zero), lax.lt(x2, zero)), trunc_mod_not_zero)
A:jax.numpy.lax_numpy.a->asarray(a)
A:jax.numpy.lax_numpy.b->reshape(b, (1,) * (ndim(a) - ndim(b)) + shape(b))
A:jax.numpy.lax_numpy.range->asarray(range)
A:jax.numpy.lax_numpy.weights->moveaxis(weights, -1, axis)
A:jax.numpy.lax_numpy.bin_edges->histogram_bin_edges(a, bins, range, weights)
A:jax.numpy.lax_numpy.bin_idx->where(a == bin_edges[-1], len(bin_edges) - 1, bin_idx)
A:jax.numpy.lax_numpy.bin_widths->diff(bin_edges)
A:jax.numpy.lax_numpy.eq_zero->lax.eq(x, lax._const(x, 0))
A:jax.numpy.lax_numpy.safe_x->where(eq_zero, lax._const(x, 0), x)
A:jax.numpy.lax_numpy.pi_x->lax.mul(lax._const(x, pi), safe_x)
A:jax.numpy.lax_numpy.ax1->_canonicalize_axis(ax1, m.ndim)
A:jax.numpy.lax_numpy.ax2->_canonicalize_axis(ax2, m.ndim)
A:jax.numpy.lax_numpy.perm->asarray(ar).flatten().argsort()
A:jax.numpy.lax_numpy.i->array(i)
A:jax.numpy.lax_numpy.im->lax.imag(x)
A:jax.numpy.lax_numpy.slice1[axis]->slice(1, None)
A:jax.numpy.lax_numpy.slice2[axis]->slice(None, -1)
A:jax.numpy.lax_numpy.slice1->tuple(slice1)
A:jax.numpy.lax_numpy.slice2->tuple(slice2)
A:jax.numpy.lax_numpy.ary->ravel(asarray(ary))
A:jax.numpy.lax_numpy.sliced->sliced.reshape(np.delete(sliced.shape, removed_dims)).reshape(np.delete(sliced.shape, removed_dims))
A:jax.numpy.lax_numpy.a_grad->concatenate((sliced(1, 2) - sliced(0, 1), (sliced(2, None) - sliced(None, -2)) * 0.5, sliced(-1, None) - sliced(-2, -1)), axis)
A:jax.numpy.lax_numpy.axis->_canonicalize_axis(axis, ndim(a))
A:jax.numpy.lax_numpy.len_axes->len(axis)
A:jax.numpy.lax_numpy.n->len(args)
A:jax.numpy.lax_numpy.newsize->_prod(newshape)
A:jax.numpy.lax_numpy.computed_newshape->_compute_newshape(a, newshape)
A:jax.numpy.lax_numpy.order->kwargs.pop('order', 'C')
A:jax.numpy.lax_numpy.invalid_kwargs->"'{}'".format("'".join(kwargs))
A:jax.numpy.lax_numpy.indices->_normalize_index(indices, axis_size)
A:jax.numpy.lax_numpy.sizes->pad(shape, (0, 1), constant_values=1)
A:jax.numpy.lax_numpy.clipped_indices->clip(indices, -total_size, total_size - 1)
A:jax.numpy.lax_numpy.cumulative_sizes->cumulative_sizes.reshape([-1] + [1] * indices.ndim).reshape([-1] + [1] * indices.ndim)
A:jax.numpy.lax_numpy.a_shape->shape(a)
A:jax.numpy.lax_numpy.source->tuple((_canonicalize_axis(i, ndim(a)) for i in source))
A:jax.numpy.lax_numpy.destination->tuple((_canonicalize_axis(i, ndim(a)) for i in destination))
A:jax.numpy.lax_numpy.(a, b)->_promote_dtypes(a, b)
A:jax.numpy.lax_numpy.rtol->lax.convert_element_type(rtol, dtype)
A:jax.numpy.lax_numpy.atol->lax.convert_element_type(atol, dtype)
A:jax.numpy.lax_numpy.a_inf->isinf(a)
A:jax.numpy.lax_numpy.b_inf->isinf(b)
A:jax.numpy.lax_numpy.any_inf->logical_or(a_inf, b_inf)
A:jax.numpy.lax_numpy.both_inf->logical_and(a_inf, b_inf)
A:jax.numpy.lax_numpy.same_value->lax.eq(a, b)
A:jax.numpy.lax_numpy.same_inf->logical_and(both_inf, same_value)
A:jax.numpy.lax_numpy.a_nan->isnan(a)
A:jax.numpy.lax_numpy.b_nan->isnan(b)
A:jax.numpy.lax_numpy.any_nan->logical_or(a_nan, b_nan)
A:jax.numpy.lax_numpy.both_nan->logical_and(a_nan, b_nan)
A:jax.numpy.lax_numpy.numpy_version->tuple(map(int, np.version.version.split('.')[:2]))
A:jax.numpy.lax_numpy.ar1->ravel(ar1)
A:jax.numpy.lax_numpy.ar2->ravel(ar2)
A:jax.numpy.lax_numpy.ar->asarray(ar).flatten()
A:jax.numpy.lax_numpy.iota->lax.broadcast_in_dim(iota, gather_index_shape, (j,))
A:jax.numpy.lax_numpy.(aux, indices)->lax.sort_key_val(ar, iota)
A:jax.numpy.lax_numpy.aux->asarray(ar).flatten().sort()
A:jax.numpy.lax_numpy.(ar1, ind1)->unique(ar1, return_index=True)
A:jax.numpy.lax_numpy.(ar2, ind2)->unique(ar2, return_index=True)
A:jax.numpy.lax_numpy.(aux, mask, aux_sort_indices)->_intersect1d_sorted_mask(ar1, ar2, return_indices)
A:jax.numpy.lax_numpy.(aux, mask)->_unique1d_sorted_mask(ar, optional_indices)
A:jax.numpy.lax_numpy.condition->array(condition).astype(bool)
A:jax.numpy.lax_numpy.(condition, x, y)->broadcast_arrays(condition, x, y)
A:jax.numpy.lax_numpy.choices->_promote_dtypes(default, *choicelist)
A:jax.numpy.lax_numpy.output->where(cond, choice, output)
A:jax.numpy.lax_numpy.length->_max(length, minlength)
A:jax.numpy.lax_numpy.result_shape->list(a.shape)
A:jax.numpy.lax_numpy.shape->canonicalize_shape(shape)
A:jax.numpy.lax_numpy.arr_shape->replace(arr.shape, 1)
A:jax.numpy.lax_numpy.(diff,)->numpy.where(np.not_equal(shape[nlead:], arr_shape))
A:jax.numpy.lax_numpy.kept_dims->tuple(np.delete(np.arange(len(shape)), new_dims))
A:jax.numpy.lax_numpy.split_indices->numpy.concatenate([[0], indices_or_sections, [size]])
A:jax.numpy.lax_numpy.indices_or_sections->core.concrete_or_error(int, indices_or_sections, 'in jax.numpy.split argument 1')
A:jax.numpy.lax_numpy.(part_size, r)->_divmod(size, indices_or_sections)
A:jax.numpy.lax_numpy.vsplit->_split_on_axis(np.vsplit, axis=0)
A:jax.numpy.lax_numpy.hsplit->_split_on_axis(np.hsplit, axis=1)
A:jax.numpy.lax_numpy.dsplit->_split_on_axis(np.dsplit, axis=2)
A:jax.numpy.lax_numpy.a_min->lax.convert_element_type(a_min, _dtype(a))
A:jax.numpy.lax_numpy.a_max->lax.convert_element_type(a_max, _dtype(a))
A:jax.numpy.lax_numpy.half->lax._const(x, 0.5)
A:jax.numpy.lax_numpy.one->lax._const(x, 1)
A:jax.numpy.lax_numpy.round_val->lax.floor(x)
A:jax.numpy.lax_numpy.nearest_even_int->lax.sub(round_val, lax.mul(lax._const(x, 2), lax.floor(lax.mul(half, x))))
A:jax.numpy.lax_numpy.is_odd->lax.eq(nearest_even_int, one)
A:jax.numpy.lax_numpy.factor->_constant_like(x, 10 ** decimals)
A:jax.numpy.lax_numpy.whole->fix(x)
A:jax.numpy.lax_numpy.isposinf->_wraps(np.isposinf)(lambda x: _isposneginf(inf, x))
A:jax.numpy.lax_numpy.isneginf->_wraps(np.isneginf)(lambda x: _isposneginf(-inf, x))
A:jax.numpy.lax_numpy.dims->shape(a)
A:jax.numpy.lax_numpy.computation_dtype->promote_types(dtype, dtypes.canonicalize_dtype(float_))
A:jax.numpy.lax_numpy.a_dtype->promote_types(a_dtype, float32)
A:jax.numpy.lax_numpy._cast_to_bool->partial(lax.convert_element_type, new_dtype=bool_)
A:jax.numpy.lax_numpy.sum->_make_reduction(np.sum, lax.add, 0, upcast_f16_for_computation=True, bool_op=lax.bitwise_or)
A:jax.numpy.lax_numpy.productprod->_make_reduction(np.prod, lax.mul, 1, bool_op=lax.bitwise_and, upcast_f16_for_computation=True)
A:jax.numpy.lax_numpy.amaxmax->_make_reduction(np.max, lax.max, -np.inf)
A:jax.numpy.lax_numpy.aminmin->_make_reduction(np.min, lax.min, np.inf)
A:jax.numpy.lax_numpy.allalltrue->_make_reduction(np.all, lax.bitwise_and, True, _cast_to_bool)
A:jax.numpy.lax_numpy.anysometrue->_make_reduction(np.any, lax.bitwise_or, False, _cast_to_bool)
A:jax.numpy.lax_numpy.normalizer->sum(logical_not(isnan(a)), axis=axis, keepdims=keepdims)
A:jax.numpy.lax_numpy.avg->mean(a, axis=axis)
A:jax.numpy.lax_numpy.weights_sum->broadcast_to(weights_sum, avg.shape)
A:jax.numpy.lax_numpy.out_dtype->dtypes.canonicalize_dtype(out_dtype)
A:jax.numpy.lax_numpy.a_ndim->ndim(a)
A:jax.numpy.lax_numpy.weights_shape->shape(weights)
A:jax.numpy.lax_numpy.(a_dtype, dtype)->_var_promote_types(_dtype(a), dtype)
A:jax.numpy.lax_numpy.a_mean->nanmean(a, axis, dtype=a_dtype, keepdims=True)
A:jax.numpy.lax_numpy.centered->lax.square(centered)
A:jax.numpy.lax_numpy.ndims->len(dims)
A:jax.numpy.lax_numpy.d->diag(c)
A:jax.numpy.lax_numpy.nanmin->_make_nan_reduction(np.nanmin, min, inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nanmax->_make_nan_reduction(np.nanmax, max, -inf, nan_if_all_nan=True)
A:jax.numpy.lax_numpy.nansum->_make_nan_reduction(np.nansum, sum, 0, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.nanprod->_make_nan_reduction(np.nanprod, prod, 1, nan_if_all_nan=False)
A:jax.numpy.lax_numpy.nan_mask->isnan(a)
A:jax.numpy.lax_numpy.td->lax.div(nansum(a, axis, dtype=dtype, keepdims=keepdims), normalizer)
A:jax.numpy.lax_numpy.normalizer_mask->lax.le(normalizer, zero)
A:jax.numpy.lax_numpy.divisor->where(normalizer_mask, 1, normalizer)
A:jax.numpy.lax_numpy.num_dims->len(a_shape)
A:jax.numpy.lax_numpy.cumsum->_make_cumulative_reduction(np.cumsum, lax.cumsum, fill_nan=False)
A:jax.numpy.lax_numpy.cumprod->_make_cumulative_reduction(np.cumprod, lax.cumprod, fill_nan=False)
A:jax.numpy.lax_numpy.nancumsum->_make_cumulative_reduction(np.nancumsum, lax.cumsum, fill_nan=True, fill_value=0)
A:jax.numpy.lax_numpy.nancumprod->_make_cumulative_reduction(np.nancumprod, lax.cumprod, fill_nan=True, fill_value=1)
A:jax.numpy.lax_numpy.dd->diff(p, axis=axis)
A:jax.numpy.lax_numpy.ddmod->where((ddmod == -pi) & (dd > 0), pi, ddmod)
A:jax.numpy.lax_numpy.ph_correct->where(abs(dd) < discont, 0, ddmod - dd)
A:jax.numpy.lax_numpy.up->concatenate((lax.slice_in_dim(p, 0, 1, axis=axis), lax.slice_in_dim(p, 1, None, axis=axis) + cumsum(ph_correct, axis=axis)), axis=axis)
A:jax.numpy.lax_numpy.nd->ndim(array)
A:jax.numpy.lax_numpy.constant_values->lax.convert_element_type(constant_values, array.dtype)
A:jax.numpy.lax_numpy.array->asarray(array)
A:jax.numpy.lax_numpy.(repeats, (left_remainder, right_remainder))->_divmod(pad_width[i], size)
A:jax.numpy.lax_numpy.rarray->lax.rev(array, dimensions=(i,))
A:jax.numpy.lax_numpy.parts->reversed(build_padding(pad_width[i, 0], forward=True))
A:jax.numpy.lax_numpy.edge_before->lax.slice_in_dim(array, 0, 1, axis=i)
A:jax.numpy.lax_numpy.pad_before->repeat(edge_before, npad_before, axis=i)
A:jax.numpy.lax_numpy.edge_after->lax.slice_in_dim(array, n - 1, n, axis=i)
A:jax.numpy.lax_numpy.pad_after->repeat(edge_after, npad_after, axis=i)
A:jax.numpy.lax_numpy.pad_width->tuple(pad_width)
A:jax.numpy.lax_numpy.shape0->shape(arrays[0])
A:jax.numpy.lax_numpy.A->concatenate([A] * int(rep), axis=i)
A:jax.numpy.lax_numpy.arrays->_promote_dtypes(*arrays)
A:jax.numpy.lax_numpy.(xs, depths)->unzip2([_block(x) for x in xs])
A:jax.numpy.lax_numpy.rank->ndim(arr)
A:jax.numpy.lax_numpy.(out, _)->_block(arrays)
A:jax.numpy.lax_numpy.object->_np_array(object, dtype=dtype, ndmin=ndmin)
A:jax.numpy.lax_numpy.view->memoryview(object)
A:jax.numpy.lax_numpy.eq->logical_or(eq, logical_and(isnan(a1), isnan(a2)))
A:jax.numpy.lax_numpy.k->int(k)
A:jax.numpy.lax_numpy.k_dtype->_dtype(k)
A:jax.numpy.lax_numpy.require->partial(core.concrete_or_error, _np_asarray)
A:jax.numpy.lax_numpy.start->sanitize(idx.start, default=upper if step_is_negative else lower)
A:jax.numpy.lax_numpy.stop->sanitize(idx.stop, default=lower if step_is_negative else upper)
A:jax.numpy.lax_numpy.bounds_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax.numpy.lax_numpy.broadcast_start->broadcast_to(start, bounds_shape)
A:jax.numpy.lax_numpy.broadcast_stop->broadcast_to(stop, bounds_shape)
A:jax.numpy.lax_numpy.empty_shape->list(lax.broadcast_shapes(shape(start), shape(stop)))
A:jax.numpy.lax_numpy.lin->linspace(start, stop, num, endpoint=endpoint, retstep=False, dtype=None, axis=axis)
A:jax.numpy.lax_numpy.res->argmin(a, axis=axis)
A:jax.numpy.lax_numpy.indexing->kwargs.get('indexing', 'xy')
A:jax.numpy.lax_numpy.sparse->kwargs.get('sparse', False)
A:jax.numpy.lax_numpy.copy->kwargs.get('copy', True)
A:jax.numpy.lax_numpy.args->list(args)
A:jax.numpy.lax_numpy.args[i]a->asarray(a)
A:jax.numpy.lax_numpy.s->shape(operand)
A:jax.numpy.lax_numpy.dimensions->tuple(dimensions)
A:jax.numpy.lax_numpy.N->len(dimensions)
A:jax.numpy.lax_numpy.idx->tuple(idx)
A:jax.numpy.lax_numpy.repeats->broadcast_to(repeats, [a.shape[axis]])
A:jax.numpy.lax_numpy.total_repeat_length->numpy.sum(repeats)
A:jax.numpy.lax_numpy.exclusive_repeats->roll(repeats, shift=1).at[0].set(0)
A:jax.numpy.lax_numpy.scatter_indices->cumsum(exclusive_repeats)
A:jax.numpy.lax_numpy.block_split_indicators->ops.index_add(x=zeros([total_repeat_length], dtype=int32), idx=scatter_indices, y=1)
A:jax.numpy.lax_numpy.m_shape->shape(m)
A:jax.numpy.lax_numpy.mask->ops.index_update(mask, ops.index[1:], aux[1:] != aux[:-1])
A:jax.numpy.lax_numpy.axis1->_canonicalize_axis(axis1, a_ndims)
A:jax.numpy.lax_numpy.axis2->_canonicalize_axis(axis2, a_ndims)
A:jax.numpy.lax_numpy.default_int->dtypes.canonicalize_dtype(np.int_)
A:jax.numpy.lax_numpy.tril_indices->_wrap_indices_function(np.tril_indices)
A:jax.numpy.lax_numpy.triu_indices->_wrap_indices_function(np.triu_indices)
A:jax.numpy.lax_numpy.mask_indices->_wrap_indices_function(np.mask_indices)
A:jax.numpy.lax_numpy.a_ndims->len(a_shape)
A:jax.numpy.lax_numpy.diag_size->_max(0, _min(a_shape[axis1] + _min(offset, 0), a_shape[axis2] - _max(offset, 0)))
A:jax.numpy.lax_numpy.v_shape->shape(v)
A:jax.numpy.lax_numpy.v->asarray(v)
A:jax.numpy.lax_numpy.v_length->len(v)
A:jax.numpy.lax_numpy.p->asarray(p)
A:jax.numpy.lax_numpy.a1->asarray([0.0])
A:jax.numpy.lax_numpy.a2->asarray([0.0])
A:jax.numpy.lax_numpy.coeff->(arange(len(p), m, -1) - 1 - arange(m)[:, newaxis]).prod(0)
A:jax.numpy.lax_numpy.val->convolve(a1, a2, mode='full')
A:jax.numpy.lax_numpy.num_batch_dims->_max(len(a_batch_dims), len(b_batch_dims))
A:jax.numpy.lax_numpy.b_ndim->ndim(b)
A:jax.numpy.lax_numpy.(operands, contractions)->opt_einsum.contract_path(*operands, einsum_call=True, use_blas=True, optimize=optimize)
A:jax.numpy.lax_numpy.contractions->tuple((data[:3] for data in contractions))
A:jax.numpy.lax_numpy.operands->list(_promote_dtypes(*operands))
A:jax.numpy.lax_numpy.operand->lax.transpose(operand, perm)
A:jax.numpy.lax_numpy.names->names.replace(name, '', count - 1).replace(name, '', count - 1)
A:jax.numpy.lax_numpy.eye->lax._delta(operand.dtype, operand.shape, axes)
A:jax.numpy.lax_numpy.other_i->other_names.find(d)
A:jax.numpy.lax_numpy.contracted_names->sorted(contracted_names_set)
A:jax.numpy.lax_numpy.(input_str, result_names)->einstr.split('->')
A:jax.numpy.lax_numpy.input_names->input_str.split(',')
A:jax.numpy.lax_numpy.counts->lax.expand_dims(counts, tuple(range(q_ndim)))
A:jax.numpy.lax_numpy.(operand, names)->sum_repeats(operand, names, counts, result_names)
A:jax.numpy.lax_numpy.(lhs, rhs)->map(operands.pop, operand_indices)
A:jax.numpy.lax_numpy.(lhs, lhs_names)->sum_repeats(lhs, lhs_names, lhs_counts, result_names + rhs_names)
A:jax.numpy.lax_numpy.(rhs, rhs_names)->sum_repeats(rhs, rhs_names, rhs_counts, result_names + lhs_names)
A:jax.numpy.lax_numpy.lhs_counts->collections.Counter(lhs_names)
A:jax.numpy.lax_numpy.rhs_counts->collections.Counter(rhs_names)
A:jax.numpy.lax_numpy.(lhs_batch, rhs_batch)->unzip2(((lhs_names.find(n), rhs_names.find(n)) for n in batch_names))
A:jax.numpy.lax_numpy.batch_names_str->''.join(batch_names)
A:jax.numpy.lax_numpy.(lhs_cont, rhs_cont)->unzip2(((lhs_names.index(n), rhs_names.index(n)) for n in contracted_names))
A:jax.numpy.lax_numpy.c->lax.complex(real_part, complex_part)
A:jax.numpy.lax_numpy.a_reshaped->reshape(a, [i for d in shape(a) for i in (d, 1)])
A:jax.numpy.lax_numpy.b_reshaped->reshape(b, [i for d in shape(b) for i in (1, d)])
A:jax.numpy.lax_numpy.out_shape->lax.broadcast_shapes(idx_shape, arr_shape)
A:jax.numpy.lax_numpy.x_shape->shape(x)
A:jax.numpy.lax_numpy.keys->tuple(keys)
A:jax.numpy.lax_numpy.(_, perm)->lax.sort_key_val(a, iota, dimension=axis)
A:jax.numpy.lax_numpy.shift->asarray(shift)
A:jax.numpy.lax_numpy.b_shape->lax.broadcast_shapes(shift.shape, axis.shape, (1,))
A:jax.numpy.lax_numpy.bits->arange(8, dtype='uint8')
A:jax.numpy.lax_numpy.packed->(a << bits).sum(-1).astype('uint8')
A:jax.numpy.lax_numpy.unpacked->(a[..., None] & bits > 0).astype('uint8')
A:jax.numpy.lax_numpy.index_dims->len(shape(indices))
A:jax.numpy.lax_numpy.slice_sizes->list(a_shape)
A:jax.numpy.lax_numpy.slice_sizes[axis]->_min(indices.size, 1)
A:jax.numpy.lax_numpy.dnums->lax.GatherDimensionNumbers(offset_dims=tuple(range(q_ndim, len(a_shape) + q_ndim if keepdims else len(a_shape) + q_ndim - 1)), collapsed_slice_dims=() if keepdims else (axis,), start_index_map=(axis,))
A:jax.numpy.lax_numpy.lst->list(tup)
A:jax.numpy.lax_numpy.bcast_shape->lax.broadcast_shapes(replace(arr.shape, 1), replace(indices.shape, 1))
A:jax.numpy.lax_numpy.gather_indices->concatenate((gather_indices, i), len(gather_indices_shape))
A:jax.numpy.lax_numpy.(aux, mask, perm)->_unique1d_sorted_mask(ar, optional_indices)
A:jax.numpy.lax_numpy.inv_idx->ops.index_update(inv_idx, perm, imask)
A:jax.numpy.lax_numpy.ret->_unique1d(ar, return_index, return_inverse, return_counts)
A:jax.numpy.lax_numpy.(treedef, static_idx, dynamic_idx)->_split_index_for_jit(idx)
A:jax.numpy.lax_numpy.indexer->_index_to_gather(shape(arr), idx)
A:jax.numpy.lax_numpy._Indexer->collections.namedtuple('_Indexer', ['slice_shape', 'gather_slice_shape', 'gather_indices', 'dnums', 'reversed_y_dims', 'newaxis_dims'])
A:jax.numpy.lax_numpy.(leaves, treedef)->tree_flatten(idx)
A:jax.numpy.lax_numpy.(advanced_indexes, idx_advanced_axes, x_advanced_axes)->zip(*advanced_pairs)
A:jax.numpy.lax_numpy.advanced_axes_are_contiguous->numpy.all(np.diff(idx_advanced_axes) == 1)
A:jax.numpy.lax_numpy.use_64bit_index->_any([type(d) is Poly or d >= 1 << 31 for d in x_shape])
A:jax.numpy.lax_numpy.advanced_indexes->broadcast_arrays(*advanced_indexes)
A:jax.numpy.lax_numpy.ndim->len(shape)
A:jax.numpy.lax_numpy.abstract_i->core.get_aval(i)
A:jax.numpy.lax_numpy.(start, limit, stride, needs_rev)->_static_idx(i, x_shape[x_axis])
A:jax.numpy.lax_numpy.len_without_none->_sum((1 for e in idx if e is not None and e is not Ellipsis))
A:jax.numpy.lax_numpy.ellipsis_index->next(ellipses, None)
A:jax.numpy.lax_numpy.(start, stop, step)->tuple(idx).indices(size)
A:jax.numpy.lax_numpy.blackman->_wrap_numpy_nullary_function(np.blackman)
A:jax.numpy.lax_numpy.bartlett->_wrap_numpy_nullary_function(np.bartlett)
A:jax.numpy.lax_numpy.hamming->_wrap_numpy_nullary_function(np.hamming)
A:jax.numpy.lax_numpy.hanning->_wrap_numpy_nullary_function(np.hanning)
A:jax.numpy.lax_numpy.kaiser->_wrap_numpy_nullary_function(np.kaiser)
A:jax.numpy.lax_numpy.(gcd, _)->lax.while_loop(_gcd_cond_fn, _gcd_body_fn, (lax.abs(x1), lax.abs(x2)))
A:jax.numpy.lax_numpy.X->array(m, ndmin=2, dtype=dtypes.canonicalize_dtype(result_type(m, float_)))
A:jax.numpy.lax_numpy.w->asarray(fweights)
A:jax.numpy.lax_numpy.(avg, w_sum)->average(X, axis=1, weights=w, returned=True)
A:jax.numpy.lax_numpy.stddev->sqrt(real(d))
A:jax.numpy.lax_numpy.real_part->clip(real(c), -1, 1)
A:jax.numpy.lax_numpy.complex_part->clip(imag(c), -1, 1)
A:jax.numpy.lax_numpy.q->true_divide(asarray(q), float32(100.0))
A:jax.numpy.lax_numpy.q_shape->shape(q)
A:jax.numpy.lax_numpy.q_ndim->ndim(q)
A:jax.numpy.lax_numpy.low->lax.convert_element_type(low, int64)
A:jax.numpy.lax_numpy.high->lax.convert_element_type(high, int64)
A:jax.numpy.lax_numpy.high_weight->lax.broadcast_in_dim(high_weight, high_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_weight->lax.broadcast_in_dim(low_weight, low_value.shape, broadcast_dimensions=(0,))
A:jax.numpy.lax_numpy.low_value->lax.gather(a, low[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.high_value->lax.gather(a, high[..., None], dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.numpy.lax_numpy.pred->lax.le(high_weight, _constant_like(high_weight, 0.5))
A:jax.numpy.lax_numpy.go_left->op(v, a[mid])
A:jax.numpy.lax_numpy.n_levels->int(np.ceil(np.log2(len(a) + 1)))
A:jax.numpy.lax_numpy.arr_dtype->_dtype(arr)
A:jax.numpy.lax_numpy.arr_bytes->arr_bytes.reshape(arr_bytes.shape[:-2] + (-1,)).reshape(arr_bytes.shape[:-2] + (-1,))
A:jax.numpy.lax_numpy.shifts->arange(0, nbits_in, nbits_out, dtype=dt_in)
A:jax.numpy.lax_numpy.argpartition->_not_implemented(np.argpartition)
jax.numpy.absolute(x)
jax.numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.angle(z)
jax.numpy.append(arr,values,axis=None)
jax.numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.argmax(a,axis=None)
jax.numpy.argmin(a,axis=None)
jax.numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.argwhere(a)
jax.numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.array_equal(a1,a2,equal_nan=False)
jax.numpy.asarray(a,dtype=None,order=None)
jax.numpy.atleast_1d(*arys)
jax.numpy.atleast_2d(*arys)
jax.numpy.atleast_3d(*arys)
jax.numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.bincount(x,weights=None,minlength=0,*,length=None)
jax.numpy.block(arrays)
jax.numpy.broadcast_arrays(*args)
jax.numpy.broadcast_to(arr,shape)
jax.numpy.cbrt(x)
jax.numpy.clip(a,a_min=None,a_max=None)
jax.numpy.column_stack(tup)
jax.numpy.compress(condition,a,axis=None,out=None)
jax.numpy.concatenate(arrays,axis=0)
jax.numpy.conjugate(x)
jax.numpy.convolve(a,v,mode='full',*,precision=None)
jax.numpy.copysign(x1,x2)
jax.numpy.corrcoef(x,y=None,rowvar=True)
jax.numpy.correlate(a,v,mode='valid',*,precision=None)
jax.numpy.count_nonzero(a,axis=None,keepdims=False)
jax.numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.deg2rad(x)
jax.numpy.diag(v,k=0)
jax.numpy.diag_indices(n,ndim=2)
jax.numpy.diag_indices_from(arr)
jax.numpy.diagflat(v,k=0)
jax.numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.diff(a,n=1,axis=-1)
jax.numpy.digitize(x,bins,right=False)
jax.numpy.divide(x1,x2)
jax.numpy.divmod(x1,x2)
jax.numpy.dot(a,b,*,precision=None)
jax.numpy.dstack(tup)
jax.numpy.ediff1d(ary,to_end=None,to_begin=None)
jax.numpy.einsum(*operands,optimize='greedy',precision=None)
jax.numpy.einsum_path(subscripts,*operands,optimize='greedy')
jax.numpy.exp2(x)
jax.numpy.expand_dims(a,axis:Union[int,Tuple[int,...]])
jax.numpy.extract(condition,arr)
jax.numpy.eye(N,M=None,k=0,dtype=None)
jax.numpy.finfo(dtype)
jax.numpy.fix(x,out=None)
jax.numpy.flatnonzero(a)
jax.numpy.flip(m,axis=None)
jax.numpy.fliplr(m)
jax.numpy.flipud(m)
jax.numpy.floor_divide(x1,x2)
jax.numpy.fmax(x1,x2)
jax.numpy.fmin(x1,x2)
jax.numpy.fmod(x1,x2)
jax.numpy.frexp(x)
jax.numpy.full(shape,fill_value,dtype=None)
jax.numpy.full_like(a,fill_value,dtype=None)
jax.numpy.gcd(x1,x2)
jax.numpy.geomspace(start,stop,num=50,endpoint=True,dtype=None,axis=0)
jax.numpy.gradient(f,*args,**kwargs)
jax.numpy.heaviside(x1,x2)
jax.numpy.histogram(a,bins=10,range=None,weights=None,density=None)
jax.numpy.histogram_bin_edges(a,bins=10,range=None,weights=None)
jax.numpy.hstack(tup)
jax.numpy.hypot(x1,x2)
jax.numpy.identity(n,dtype=None)
jax.numpy.imag(val)
jax.numpy.in1d(ar1,ar2,assume_unique=False,invert=False)
jax.numpy.indices(dimensions,dtype=int32,sparse=False)
jax.numpy.inner(a,b,*,precision=None)
jax.numpy.intersect1d(ar1,ar2,assume_unique=False,return_indices=False)
jax.numpy.isclose(a,b,rtol=1e-05,atol=1e-08,equal_nan=False)
jax.numpy.iscomplex(x)
jax.numpy.isfinite(x)
jax.numpy.isin(element,test_elements,assume_unique=False,invert=False)
jax.numpy.isinf(x)
jax.numpy.isnan(x)
jax.numpy.isreal(x)
jax.numpy.isrealobj(x)
jax.numpy.isscalar(element)
jax.numpy.issubdtype(arg1,arg2)
jax.numpy.ix_(*args)
jax.numpy.kron(a,b)
jax.numpy.lax_numpy._ArrayMeta(type(np.ndarray))
jax.numpy.lax_numpy._ArrayMeta.__instancecheck__(self,instance)
jax.numpy.lax_numpy._IndexUpdateHelper(self,array)
jax.numpy.lax_numpy._IndexUpdateHelper.__getitem__(self,index)
jax.numpy.lax_numpy._IndexUpdateHelper.__repr__(self)
jax.numpy.lax_numpy._IndexUpdateRef(self,array,index)
jax.numpy.lax_numpy._IndexUpdateRef.__repr__(self)
jax.numpy.lax_numpy._IndexUpdateRef.add(self,values,indices_are_sorted=False,unique_indices=False)
jax.numpy.lax_numpy._IndexUpdateRef.max(self,values,indices_are_sorted=False,unique_indices=False)
jax.numpy.lax_numpy._IndexUpdateRef.min(self,values,indices_are_sorted=False,unique_indices=False)
jax.numpy.lax_numpy._IndexUpdateRef.mul(self,values,indices_are_sorted=False,unique_indices=False)
jax.numpy.lax_numpy._IndexUpdateRef.set(self,values,indices_are_sorted=False,unique_indices=False)
jax.numpy.lax_numpy._ScalarMeta(self,x)
jax.numpy.lax_numpy._ScalarMeta.__eq__(self,other)
jax.numpy.lax_numpy._ScalarMeta.__hash__(self)
jax.numpy.lax_numpy._ScalarMeta.__ne__(self,other)
jax.numpy.lax_numpy._arraylike(x)
jax.numpy.lax_numpy._astype(arr,dtype)
jax.numpy.lax_numpy._atleast_nd(x,n)
jax.numpy.lax_numpy._block(xs)
jax.numpy.lax_numpy._can_call_numpy_array(x)
jax.numpy.lax_numpy._canonicalize_tuple_index(arr_ndim,idx)
jax.numpy.lax_numpy._check_arraylike(fun_name,*args)
jax.numpy.lax_numpy._check_no_padding(axis_padding,mode)
jax.numpy.lax_numpy._comparison_op(numpy_fn,lax_fn)
jax.numpy.lax_numpy._complex_elem_type(dtype)
jax.numpy.lax_numpy._compress_method(a,condition,axis=None,out=None)
jax.numpy.lax_numpy._compute_newshape(a,newshape)
jax.numpy.lax_numpy._constant_like(x,const)
jax.numpy.lax_numpy._conv(x,y,mode,op,precision)
jax.numpy.lax_numpy._cross(a,b,axisa,axisb,axisc)
jax.numpy.lax_numpy._defer_to_unrecognized_arg(binary_op)
jax.numpy.lax_numpy._einsum(operands:Sequence,contractions:Sequence[Tuple[Tuple[int,...],Set[str],str]],precision)
jax.numpy.lax_numpy._eliminate_deprecated_list_indexing(idx)
jax.numpy.lax_numpy._expand_bool_indices(idx)
jax.numpy.lax_numpy._float_divmod(x1,x2)
jax.numpy.lax_numpy._gather(arr,treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._gcd_body_fn(xs)
jax.numpy.lax_numpy._gcd_cond_fn(xs)
jax.numpy.lax_numpy._gradient(a,varargs,axis)
jax.numpy.lax_numpy._index_to_gather(x_shape,idx)
jax.numpy.lax_numpy._int(aval)
jax.numpy.lax_numpy._intersect1d_sorted_mask(ar1,ar2,return_indices=False)
jax.numpy.lax_numpy._is_advanced_int_indexer(idx)
jax.numpy.lax_numpy._is_int_arraylike(x)
jax.numpy.lax_numpy._is_slice_none(idx)
jax.numpy.lax_numpy._isposneginf(infinity,x)
jax.numpy.lax_numpy._logaddexp2_jvp(primals,tangents)
jax.numpy.lax_numpy._logaddexp_jvp(primals,tangents)
jax.numpy.lax_numpy._logical_op(np_op,bitwise_op)
jax.numpy.lax_numpy._make_cumulative_reduction(np_reduction,reduction,fill_nan=False,fill_value=0)
jax.numpy.lax_numpy._make_nan_reduction(np_reduction,jnp_reduction,init_val,nan_if_all_nan)
jax.numpy.lax_numpy._make_reduction(np_fun,op,init_val,preproc=None,bool_op=None,upcast_f16_for_computation=False)
jax.numpy.lax_numpy._make_scalar_type(np_scalar_type)
jax.numpy.lax_numpy._maybe_bool_binop(numpy_fn,lax_fn,bool_lax_fn)
jax.numpy.lax_numpy._merge_static_and_dynamic_indices(treedef,static_idx,dynamic_idx)
jax.numpy.lax_numpy._movechars(s,src,dst)
jax.numpy.lax_numpy._multi_slice(arr:DeviceArray,start_indices:Tuple[Tuple[int,...]],limit_indices:Tuple[Tuple[int,...]],removed_dims:Tuple[Tuple[int,...]])
jax.numpy.lax_numpy._normalize_float(x)
jax.numpy.lax_numpy._normalize_index(index,axis_size)
jax.numpy.lax_numpy._not_implemented(fun)
jax.numpy.lax_numpy._np_array(obj,dtype=None,**kwargs)
jax.numpy.lax_numpy._one_to_one_binop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy.lax_numpy._one_to_one_unop(numpy_fn,lax_fn,promote_to_inexact=False)
jax.numpy.lax_numpy._operator_round(number,ndigits=None)
jax.numpy.lax_numpy._pad(array,pad_width,mode,constant_values)
jax.numpy.lax_numpy._pad_constant(array,pad_width,constant_values)
jax.numpy.lax_numpy._pad_edge(array,pad_width)
jax.numpy.lax_numpy._pad_symmetric_or_reflect(array,pad_width,mode)
jax.numpy.lax_numpy._pad_wrap(array,pad_width)
jax.numpy.lax_numpy._polymorphic_slice_indices(idx:slice,size:Union[int,Poly])
jax.numpy.lax_numpy._promote_args(fun_name,*args)
jax.numpy.lax_numpy._promote_args_inexact(fun_name,*args)
jax.numpy.lax_numpy._promote_dtypes(*args)
jax.numpy.lax_numpy._promote_dtypes_inexact(*args)
jax.numpy.lax_numpy._promote_shapes(fun_name,*args)
jax.numpy.lax_numpy._quantile(a,q,axis,interpolation,keepdims,squash_nans)
jax.numpy.lax_numpy._rank_promotion_warning_or_error(fun_name,shapes)
jax.numpy.lax_numpy._reduction_dims(a,axis)
jax.numpy.lax_numpy._reduction_init_val(a,init_val)
jax.numpy.lax_numpy._removechars(s,chars)
jax.numpy.lax_numpy._replace_inf(x)
jax.numpy.lax_numpy._reshape(a,newshape,order='C')
jax.numpy.lax_numpy._reshape_method(a,*newshape,**kwargs)
jax.numpy.lax_numpy._result_dtype(op,*args)
jax.numpy.lax_numpy._rewriting_take(arr,idx)
jax.numpy.lax_numpy._roll(a,shift,axis)
jax.numpy.lax_numpy._round_to_nearest_even(x)
jax.numpy.lax_numpy._searchsorted(a,v,side)
jax.numpy.lax_numpy._should_unpack_list_index(x)
jax.numpy.lax_numpy._split_index_for_jit(idx)
jax.numpy.lax_numpy._split_on_axis(np_fun,axis)
jax.numpy.lax_numpy._static_idx(idx:slice,size:Union[int,Poly])
jax.numpy.lax_numpy._swap_args(f)
jax.numpy.lax_numpy._take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy._to_inexact_dtype(dtype)
jax.numpy.lax_numpy._trim_zeros(a)
jax.numpy.lax_numpy._unimplemented_setitem(self,i,x)
jax.numpy.lax_numpy._unique1d(ar,return_index=False,return_inverse=False,return_counts=False)
jax.numpy.lax_numpy._unique1d_sorted_mask(ar,optional_indices=False)
jax.numpy.lax_numpy._var_promote_types(a_dtype,dtype)
jax.numpy.lax_numpy._view(arr,dtype=None,type=None)
jax.numpy.lax_numpy._where(condition,x=None,y=None)
jax.numpy.lax_numpy._wrap_indices_function(f)
jax.numpy.lax_numpy._wrap_numpy_nullary_function(f)
jax.numpy.lax_numpy.absolute(x)
jax.numpy.lax_numpy.allclose(a,b,rtol=1e-05,atol=1e-08)
jax.numpy.lax_numpy.angle(z)
jax.numpy.lax_numpy.append(arr,values,axis=None)
jax.numpy.lax_numpy.arange(start,stop=None,step=None,dtype=None)
jax.numpy.lax_numpy.argmax(a,axis=None)
jax.numpy.lax_numpy.argmin(a,axis=None)
jax.numpy.lax_numpy.argsort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.argwhere(a)
jax.numpy.lax_numpy.array(object,dtype=None,copy=True,order='K',ndmin=0)
jax.numpy.lax_numpy.array_equal(a1,a2,equal_nan=False)
jax.numpy.lax_numpy.asarray(a,dtype=None,order=None)
jax.numpy.lax_numpy.atleast_1d(*arys)
jax.numpy.lax_numpy.atleast_2d(*arys)
jax.numpy.lax_numpy.atleast_3d(*arys)
jax.numpy.lax_numpy.average(a,axis=None,weights=None,returned=False)
jax.numpy.lax_numpy.bincount(x,weights=None,minlength=0,*,length=None)
jax.numpy.lax_numpy.block(arrays)
jax.numpy.lax_numpy.broadcast_arrays(*args)
jax.numpy.lax_numpy.broadcast_to(arr,shape)
jax.numpy.lax_numpy.cbrt(x)
jax.numpy.lax_numpy.clip(a,a_min=None,a_max=None)
jax.numpy.lax_numpy.column_stack(tup)
jax.numpy.lax_numpy.compress(condition,a,axis=None,out=None)
jax.numpy.lax_numpy.concatenate(arrays,axis=0)
jax.numpy.lax_numpy.conjugate(x)
jax.numpy.lax_numpy.convolve(a,v,mode='full',*,precision=None)
jax.numpy.lax_numpy.copysign(x1,x2)
jax.numpy.lax_numpy.corrcoef(x,y=None,rowvar=True)
jax.numpy.lax_numpy.correlate(a,v,mode='valid',*,precision=None)
jax.numpy.lax_numpy.count_nonzero(a,axis=None,keepdims=False)
jax.numpy.lax_numpy.cov(m,y=None,rowvar=True,bias=False,ddof=None,fweights=None,aweights=None)
jax.numpy.lax_numpy.cross(a,b,axisa=-1,axisb=-1,axisc=-1,axis=None)
jax.numpy.lax_numpy.deg2rad(x)
jax.numpy.lax_numpy.diag(v,k=0)
jax.numpy.lax_numpy.diag_indices(n,ndim=2)
jax.numpy.lax_numpy.diag_indices_from(arr)
jax.numpy.lax_numpy.diagflat(v,k=0)
jax.numpy.lax_numpy.diagonal(a,offset=0,axis1=0,axis2=1)
jax.numpy.lax_numpy.diff(a,n=1,axis=-1)
jax.numpy.lax_numpy.digitize(x,bins,right=False)
jax.numpy.lax_numpy.divide(x1,x2)
jax.numpy.lax_numpy.divmod(x1,x2)
jax.numpy.lax_numpy.dot(a,b,*,precision=None)
jax.numpy.lax_numpy.dstack(tup)
jax.numpy.lax_numpy.ediff1d(ary,to_end=None,to_begin=None)
jax.numpy.lax_numpy.einsum(*operands,optimize='greedy',precision=None)
jax.numpy.lax_numpy.einsum_path(subscripts,*operands,optimize='greedy')
jax.numpy.lax_numpy.exp2(x)
jax.numpy.lax_numpy.expand_dims(a,axis:Union[int,Tuple[int,...]])
jax.numpy.lax_numpy.extract(condition,arr)
jax.numpy.lax_numpy.eye(N,M=None,k=0,dtype=None)
jax.numpy.lax_numpy.finfo(dtype)
jax.numpy.lax_numpy.fix(x,out=None)
jax.numpy.lax_numpy.flatnonzero(a)
jax.numpy.lax_numpy.flip(m,axis=None)
jax.numpy.lax_numpy.fliplr(m)
jax.numpy.lax_numpy.flipud(m)
jax.numpy.lax_numpy.floor_divide(x1,x2)
jax.numpy.lax_numpy.fmax(x1,x2)
jax.numpy.lax_numpy.fmin(x1,x2)
jax.numpy.lax_numpy.fmod(x1,x2)
jax.numpy.lax_numpy.frexp(x)
jax.numpy.lax_numpy.full(shape,fill_value,dtype=None)
jax.numpy.lax_numpy.full_like(a,fill_value,dtype=None)
jax.numpy.lax_numpy.gcd(x1,x2)
jax.numpy.lax_numpy.geomspace(start,stop,num=50,endpoint=True,dtype=None,axis=0)
jax.numpy.lax_numpy.gradient(f,*args,**kwargs)
jax.numpy.lax_numpy.heaviside(x1,x2)
jax.numpy.lax_numpy.histogram(a,bins=10,range=None,weights=None,density=None)
jax.numpy.lax_numpy.histogram_bin_edges(a,bins=10,range=None,weights=None)
jax.numpy.lax_numpy.hstack(tup)
jax.numpy.lax_numpy.hypot(x1,x2)
jax.numpy.lax_numpy.identity(n,dtype=None)
jax.numpy.lax_numpy.imag(val)
jax.numpy.lax_numpy.in1d(ar1,ar2,assume_unique=False,invert=False)
jax.numpy.lax_numpy.indices(dimensions,dtype=int32,sparse=False)
jax.numpy.lax_numpy.inner(a,b,*,precision=None)
jax.numpy.lax_numpy.intersect1d(ar1,ar2,assume_unique=False,return_indices=False)
jax.numpy.lax_numpy.isclose(a,b,rtol=1e-05,atol=1e-08,equal_nan=False)
jax.numpy.lax_numpy.iscomplex(x)
jax.numpy.lax_numpy.isfinite(x)
jax.numpy.lax_numpy.isin(element,test_elements,assume_unique=False,invert=False)
jax.numpy.lax_numpy.isinf(x)
jax.numpy.lax_numpy.isnan(x)
jax.numpy.lax_numpy.isreal(x)
jax.numpy.lax_numpy.isrealobj(x)
jax.numpy.lax_numpy.isscalar(element)
jax.numpy.lax_numpy.issubdtype(arg1,arg2)
jax.numpy.lax_numpy.ix_(*args)
jax.numpy.lax_numpy.kron(a,b)
jax.numpy.lax_numpy.lcm(x1,x2)
jax.numpy.lax_numpy.ldexp(x1,x2)
jax.numpy.lax_numpy.lexsort(keys,axis=-1)
jax.numpy.lax_numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.lax_numpy.log10(x)
jax.numpy.lax_numpy.log2(x)
jax.numpy.lax_numpy.logaddexp(x1,x2)
jax.numpy.lax_numpy.logaddexp2(x1,x2)
jax.numpy.lax_numpy.logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis=0)
jax.numpy.lax_numpy.matmul(a,b,*,precision=None)
jax.numpy.lax_numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.lax_numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.lax_numpy.meshgrid(*args,**kwargs)
jax.numpy.lax_numpy.modf(x,out=None)
jax.numpy.lax_numpy.moveaxis(a,source,destination)
jax.numpy.lax_numpy.msort(a)
jax.numpy.lax_numpy.nan_to_num(x,copy=True,nan=0.0,posinf=None,neginf=None)
jax.numpy.lax_numpy.nanargmax(a,axis=None)
jax.numpy.lax_numpy.nanargmin(a,axis=None)
jax.numpy.lax_numpy.nanmean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.lax_numpy.nanmedian(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.lax_numpy.nanpercentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.nanquantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.nanstd(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.nanvar(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.lax_numpy.nonzero(a)
jax.numpy.lax_numpy.ones(shape,dtype=None)
jax.numpy.lax_numpy.ones_like(a,dtype=None)
jax.numpy.lax_numpy.outer(a,b,out=None)
jax.numpy.lax_numpy.packbits(a,axis=None,bitorder='big')
jax.numpy.lax_numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.lax_numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.polyadd(a1,a2)
jax.numpy.lax_numpy.polyder(p,m=1)
jax.numpy.lax_numpy.polymul(a1,a2,*,trim_leading_zeros=False)
jax.numpy.lax_numpy.polysub(a1,a2)
jax.numpy.lax_numpy.polyval(p,x)
jax.numpy.lax_numpy.power(x1,x2)
jax.numpy.lax_numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.lax_numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.lax_numpy.rad2deg(x)
jax.numpy.lax_numpy.ravel(a,order='C')
jax.numpy.lax_numpy.real(val)
jax.numpy.lax_numpy.reciprocal(x)
jax.numpy.lax_numpy.remainder(x1,x2)
jax.numpy.lax_numpy.repeat(a,repeats,axis=None,*,total_repeat_length=None)
jax.numpy.lax_numpy.reshape(a,newshape,order='C')
jax.numpy.lax_numpy.result_type(*args)
jax.numpy.lax_numpy.rint(x)
jax.numpy.lax_numpy.roll(a,shift,axis=None)
jax.numpy.lax_numpy.rollaxis(a,axis,start=0)
jax.numpy.lax_numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.lax_numpy.round(a,decimals=0)
jax.numpy.lax_numpy.searchsorted(a,v,side='left',sorter=None)
jax.numpy.lax_numpy.select(condlist,choicelist,default=0)
jax.numpy.lax_numpy.sign(x)
jax.numpy.lax_numpy.signbit(x)
jax.numpy.lax_numpy.sinc(x)
jax.numpy.lax_numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.lax_numpy.sort_complex(a)
jax.numpy.lax_numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.lax_numpy.square(x)
jax.numpy.lax_numpy.squeeze(a,axis:Union[int,Tuple[int,...]]=None)
jax.numpy.lax_numpy.stack(arrays,axis=0)
jax.numpy.lax_numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.swapaxes(a,axis1,axis2)
jax.numpy.lax_numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.lax_numpy.take_along_axis(arr,indices,axis)
jax.numpy.lax_numpy.tensordot(a,b,axes=2,*,precision=None)
jax.numpy.lax_numpy.tile(A,reps)
jax.numpy.lax_numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.lax_numpy.transpose(a,axes=None)
jax.numpy.lax_numpy.trapz(y,x=None,dx=1.0,axis=-1)
jax.numpy.lax_numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.lax_numpy.tril(m,k=0)
jax.numpy.lax_numpy.tril_indices_from(arr,k=0)
jax.numpy.lax_numpy.triu(m,k=0)
jax.numpy.lax_numpy.triu_indices_from(arr,k=0)
jax.numpy.lax_numpy.true_divide(x1,x2)
jax.numpy.lax_numpy.trunc(x)
jax.numpy.lax_numpy.unique(ar,return_index=False,return_inverse=False,return_counts=False,axis=None)
jax.numpy.lax_numpy.unpackbits(a,axis=None,count=None,bitorder='big')
jax.numpy.lax_numpy.unravel_index(indices,shape)
jax.numpy.lax_numpy.unwrap(p,discont=pi,axis=-1)
jax.numpy.lax_numpy.vander(x,N=None,increasing=False)
jax.numpy.lax_numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.lax_numpy.vdot(a,b,*,precision=None)
jax.numpy.lax_numpy.vstack(tup)
jax.numpy.lax_numpy.where(condition,x=None,y=None)
jax.numpy.lax_numpy.zeros(shape,dtype=None)
jax.numpy.lax_numpy.zeros_like(a,dtype=None)
jax.numpy.lcm(x1,x2)
jax.numpy.ldexp(x1,x2)
jax.numpy.lexsort(keys,axis=-1)
jax.numpy.linspace(start,stop,num=50,endpoint=True,retstep=False,dtype=None,axis=0)
jax.numpy.log10(x)
jax.numpy.log2(x)
jax.numpy.logaddexp(x1,x2)
jax.numpy.logaddexp2(x1,x2)
jax.numpy.logspace(start,stop,num=50,endpoint=True,base=10.0,dtype=None,axis=0)
jax.numpy.matmul(a,b,*,precision=None)
jax.numpy.mean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.median(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.meshgrid(*args,**kwargs)
jax.numpy.modf(x,out=None)
jax.numpy.moveaxis(a,source,destination)
jax.numpy.msort(a)
jax.numpy.nan_to_num(x,copy=True,nan=0.0,posinf=None,neginf=None)
jax.numpy.nanargmax(a,axis=None)
jax.numpy.nanargmin(a,axis=None)
jax.numpy.nanmean(a,axis=None,dtype=None,out=None,keepdims=False)
jax.numpy.nanmedian(a,axis=None,out=None,overwrite_input=False,keepdims=False)
jax.numpy.nanpercentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.nanquantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.nanstd(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.nanvar(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.ndarray(shape,dtype=None,buffer=None,offset=0,strides=None,order=None)
jax.numpy.nonzero(a)
jax.numpy.ones(shape,dtype=None)
jax.numpy.ones_like(a,dtype=None)
jax.numpy.outer(a,b,out=None)
jax.numpy.packbits(a,axis=None,bitorder='big')
jax.numpy.pad(array,pad_width,mode='constant',constant_values=0)
jax.numpy.percentile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.polyadd(a1,a2)
jax.numpy.polyder(p,m=1)
jax.numpy.polymul(a1,a2,*,trim_leading_zeros=False)
jax.numpy.polysub(a1,a2)
jax.numpy.polyval(p,x)
jax.numpy.power(x1,x2)
jax.numpy.ptp(a,axis=None,out=None,keepdims=False)
jax.numpy.quantile(a,q,axis=None,out=None,overwrite_input=False,interpolation='linear',keepdims=False)
jax.numpy.rad2deg(x)
jax.numpy.ravel(a,order='C')
jax.numpy.real(val)
jax.numpy.reciprocal(x)
jax.numpy.remainder(x1,x2)
jax.numpy.repeat(a,repeats,axis=None,*,total_repeat_length=None)
jax.numpy.reshape(a,newshape,order='C')
jax.numpy.result_type(*args)
jax.numpy.rint(x)
jax.numpy.roll(a,shift,axis=None)
jax.numpy.rollaxis(a,axis,start=0)
jax.numpy.rot90(m,k=1,axes=(0,1))
jax.numpy.round(a,decimals=0)
jax.numpy.searchsorted(a,v,side='left',sorter=None)
jax.numpy.select(condlist,choicelist,default=0)
jax.numpy.sign(x)
jax.numpy.signbit(x)
jax.numpy.sinc(x)
jax.numpy.sort(a,axis=-1,kind='quicksort',order=None)
jax.numpy.sort_complex(a)
jax.numpy.split(ary,indices_or_sections,axis=0)
jax.numpy.square(x)
jax.numpy.squeeze(a,axis:Union[int,Tuple[int,...]]=None)
jax.numpy.stack(arrays,axis=0)
jax.numpy.std(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.swapaxes(a,axis1,axis2)
jax.numpy.take(a,indices,axis=None,out=None,mode=None)
jax.numpy.take_along_axis(arr,indices,axis)
jax.numpy.tensordot(a,b,axes=2,*,precision=None)
jax.numpy.tile(A,reps)
jax.numpy.trace(a,offset=0,axis1=0,axis2=1,dtype=None,out=None)
jax.numpy.transpose(a,axes=None)
jax.numpy.trapz(y,x=None,dx=1.0,axis=-1)
jax.numpy.tri(N,M=None,k=0,dtype=None)
jax.numpy.tril(m,k=0)
jax.numpy.tril_indices_from(arr,k=0)
jax.numpy.triu(m,k=0)
jax.numpy.triu_indices_from(arr,k=0)
jax.numpy.true_divide(x1,x2)
jax.numpy.trunc(x)
jax.numpy.unique(ar,return_index=False,return_inverse=False,return_counts=False,axis=None)
jax.numpy.unpackbits(a,axis=None,count=None,bitorder='big')
jax.numpy.unravel_index(indices,shape)
jax.numpy.unwrap(p,discont=pi,axis=-1)
jax.numpy.vander(x,N=None,increasing=False)
jax.numpy.var(a,axis=None,dtype=None,out=None,ddof=0,keepdims=False)
jax.numpy.vdot(a,b,*,precision=None)
jax.numpy.vstack(tup)
jax.numpy.where(condition,x=None,y=None)
jax.numpy.zeros(shape,dtype=None)
jax.numpy.zeros_like(a,dtype=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/vectorize.py----------------------------------------
A:jax.numpy.vectorize._CORE_DIMENSION_LIST->'(?:{0:}(?:,{0:})*)?'.format(_DIMENSION_NAME)
A:jax.numpy.vectorize._ARGUMENT->'\\({}\\)'.format(_CORE_DIMENSION_LIST)
A:jax.numpy.vectorize._ARGUMENT_LIST->'{0:}(?:,{0:})*'.format(_ARGUMENT)
A:jax.numpy.vectorize._SIGNATURE->'^{0:}->{0:}$'.format(_ARGUMENT_LIST)
A:jax.numpy.vectorize.num_core_dims->len(core_dims)
A:jax.numpy.vectorize.broadcast_shape->lax.broadcast_shapes(*shapes)
A:jax.numpy.vectorize.out->func(*args)
A:jax.numpy.vectorize.out_shapes->map(jnp.shape, out if isinstance(out, tuple) else [out])
A:jax.numpy.vectorize.sizes->dict(dim_sizes)
A:jax.numpy.vectorize.args->tuple(map(jnp.asarray, args))
A:jax.numpy.vectorize.error_context->'on vectorized function with excluded={!r} and signature={!r}'.format(excluded, signature)
A:jax.numpy.vectorize.(excluded_func, args)->_apply_excluded(pyfunc, excluded, args)
A:jax.numpy.vectorize.(input_core_dims, output_core_dims)->_parse_gufunc_signature(signature)
A:jax.numpy.vectorize.(broadcast_shape, dim_sizes)->_parse_input_dimensions(args, input_core_dims, error_context)
A:jax.numpy.vectorize.checked_func->_check_output_dims(excluded_func, dim_sizes, output_core_dims, error_context)
A:jax.numpy.vectorize.core_shape->tuple((dim_sizes[dim] for dim in core_dims))
A:jax.numpy.vectorize.vec_arg->jnp.broadcast_to(arg, vec_shape)
A:jax.numpy.vectorize.in_axes->tuple((0 if c > 0 else None for c in vmap_counts))
A:jax.numpy.vectorize.vectorized_func->api.vmap(vectorized_func, in_axes)
jax.numpy.vectorize(pyfunc,*,excluded=frozenset(),signature=None)
jax.numpy.vectorize._apply_excluded(func,excluded,args)
jax.numpy.vectorize._check_output_dims(func:Callable,dim_sizes:Dict[str,int],expected_output_core_dims:List[CoreDims],error_context:str='')->Callable
jax.numpy.vectorize._parse_gufunc_signature(signature:str)->Tuple[List[CoreDims], List[CoreDims]]
jax.numpy.vectorize._parse_input_dimensions(args:Tuple[NDArray,...],input_core_dims:List[CoreDims],error_context:str='')->Tuple[Tuple[int, ...], Dict[str, int]]
jax.numpy.vectorize._update_dim_sizes(dim_sizes:Dict[str,int],shape:Tuple[int,...],core_dims:CoreDims,error_context:str='',*,is_input:bool)
jax.numpy.vectorize.vectorize(pyfunc,*,excluded=frozenset(),signature=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/__init__.py----------------------------------------
A:jax.numpy.__init__.globals()[name]->lax_numpy._not_implemented(func)
jax.numpy.__init__._init()


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/polynomial.py----------------------------------------
A:jax.numpy.polynomial.p->jnp.atleast_1d(p)
A:jax.numpy.polynomial.A->jaxops.index_update(A, jaxops.index[0, :], -p[1:] / p[0])
A:jax.numpy.polynomial.roots->jnp.hstack((roots, jnp.zeros(trailing_zeros, p.dtype)))
A:jax.numpy.polynomial.start->jnp.argmin(is_zero)
A:jax.numpy.polynomial.(start, end)->_nonzero_range(p)
A:jax.numpy.polynomial.globals()[name]->_not_implemented(func)
jax.numpy.polynomial._nonzero_range(arr)
jax.numpy.polynomial._promote_inexact(arr)
jax.numpy.polynomial._roots_no_zeros(p)
jax.numpy.polynomial._to_inexact_type(type)
jax.numpy.polynomial.roots(p,*,strip_zeros=True)
jax.numpy.roots(p,*,strip_zeros=True)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/numpy/fft.py----------------------------------------
A:jax.numpy.fft.axes->tuple(range(x.ndim))
A:jax.numpy.fft.a->jnp.moveaxis(a, orig_axes, axes)
A:jax.numpy.fft.transformed->jnp.moveaxis(transformed, axes, orig_axes)
A:jax.numpy.fft.conj_a->jnp.conj(a)
A:jax.numpy.fft.output->_fft_core_1d('ihfft', xla_client.FftType.RFFT, a, s=n, axis=axis, norm=norm)
A:jax.numpy.fft.k->jnp.arange(0, (n - 1) // 2 + 1)
A:jax.numpy.fft.x->jnp.asarray(x)
A:jax.numpy.fft.globals()[name]->_not_implemented(func)
jax.numpy.fft._axis_check_1d(func_name,axis)
jax.numpy.fft._fft_core(func_name,fft_type,a,s,axes,norm)
jax.numpy.fft._fft_core_1d(func_name,fft_type,a,s,axis,norm)
jax.numpy.fft._fft_core_2d(func_name,fft_type,a,s,axes,norm)
jax.numpy.fft.fft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.fft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.fftfreq(n,d=1.0)
jax.numpy.fft.fftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.fftshift(x,axes=None)
jax.numpy.fft.hfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.ifft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.ifft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.ifftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.ifftshift(x,axes=None)
jax.numpy.fft.ihfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.irfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.irfft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.irfftn(a,s=None,axes=None,norm=None)
jax.numpy.fft.rfft(a,n=None,axis=-1,norm=None)
jax.numpy.fft.rfft2(a,s=None,axes=(-2,-1),norm=None)
jax.numpy.fft.rfftfreq(n,d=1.0)
jax.numpy.fft.rfftn(a,s=None,axes=None,norm=None)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/ops/scatter.py----------------------------------------
A:jax.ops.scatter.x->numpy.lax_numpy.asarray(x)
A:jax.ops.scatter.y->lax.rev(y, indexer.reversed_y_dims)
A:jax.ops.scatter.(treedef, static_idx, dynamic_idx)->numpy.lax_numpy._split_index_for_jit(idx)
A:jax.ops.scatter.dtype->lax.dtype(x)
A:jax.ops.scatter.(x, y)->numpy.lax_numpy._promote_dtypes(x, y)
A:jax.ops.scatter.idx->numpy.lax_numpy._merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)
A:jax.ops.scatter.indexer->numpy.lax_numpy._index_to_gather(jnp.shape(x), idx)
A:jax.ops.scatter.dnums->lax.ScatterDimensionNumbers(update_window_dims=indexer.dnums.offset_dims, inserted_window_dims=indexer.dnums.collapsed_slice_dims, scatter_dims_to_operand_dims=indexer.dnums.start_index_map)
A:jax.ops.scatter.out->numpy.lax_numpy.zeros((num_segments,) + data.shape[1:], dtype=data.dtype)
A:jax.ops.scatter.index->_Indexable()
A:jax.ops.scatter.num_segments->int(num_segments)
A:jax.ops.scatter.segment_ids->numpy.lax_numpy.mod(segment_ids, num_segments)
jax.ops.index_add(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.index_max(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.index_min(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.index_mul(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.index_update(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter._Indexable(object)
jax.ops.scatter._Indexable.__getitem__(self,index)
jax.ops.scatter._scatter_impl(x,y,scatter_op,treedef,static_idx,dynamic_idx,indices_are_sorted,unique_indices)
jax.ops.scatter._scatter_update(x,idx,y,scatter_op,indices_are_sorted,unique_indices)
jax.ops.scatter.index_add(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter.index_max(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter.index_min(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter.index_mul(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter.index_update(x,idx,y,indices_are_sorted=False,unique_indices=False)
jax.ops.scatter.segment_sum(data,segment_ids,num_segments=None,indices_are_sorted=False,unique_indices=False)
jax.ops.segment_sum(data,segment_ids,num_segments=None,indices_are_sorted=False,unique_indices=False)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/ops/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/pxla.py----------------------------------------
A:jax.interpreters.pxla.total_replication_factor->int(prod((factor for (factor, index) in sharding_spec.replication_factors)))
A:jax.interpreters.pxla.replication_factors->sorted(sharding_spec.replication_factors, key=op.itemgetter(1))
A:jax.interpreters.pxla.(factor, _)->sorted(sharding_spec.replication_factors, key=op.itemgetter(1)).pop(0)
A:jax.interpreters.pxla.indices->spec_to_indices(unsharded_aval.shape, sharding_spec)
A:jax.interpreters.pxla.(shard_size, ragged)->divmod(axis_size, num_shards)
A:jax.interpreters.pxla.arg->xla.canonicalize_dtype(arg)
A:jax.interpreters.pxla.bufs->shard_arg_handlers[type(arg)](arg, devices, indices[a])
A:jax.interpreters.pxla.(start_indices, limit_indices, removed_dims)->map(tuple, unzip3((_as_slice_indices(x, idx) for idx in indices)))
A:jax.interpreters.pxla.shards->xops.ConvertElementType(x, xb.dtype_to_etype(np.float32))._multi_slice(start_indices, limit_indices, removed_dims)
A:jax.interpreters.pxla.limit_indices->list(arr.shape)
A:jax.interpreters.pxla.self.dynamic_axis_env->DynamicAxisEnv()
A:jax.interpreters.pxla._thread_local_state->_ThreadLocalState()
A:jax.interpreters.pxla.mapped->prod((frame.hard_size for frame in dynamic_axis_env))
A:jax.interpreters.pxla.(unmapped, ragged)->divmod(xb.device_count(backend), mapped)
A:jax.interpreters.pxla.axis_name->params.pop('axis_name')
A:jax.interpreters.pxla.axis_index_groups->params.pop('axis_index_groups')
A:jax.interpreters.pxla.shape->tuple((logical_size(dynamic_axis_env[name]) for name in axis_name))
A:jax.interpreters.pxla.out_aval->ShapedArray((), np.int32)
A:jax.interpreters.pxla.out_tracer->pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)
A:jax.interpreters.pxla.eqn->pe.new_eqn_recipe([], [out_tracer], axis_index_p, dict(nreps=nreps, sizes=sizes, axis_name=axis_name), source_info_util.current())
A:jax.interpreters.pxla.div->lib.xla_bridge.constant(c, np.array(axis_env.nreps // prod(axis_env.sizes), np.uint32))
A:jax.interpreters.pxla.mod->lib.xla_bridge.constant(c, np.array(axis_env.sizes[-1], np.uint32))
A:jax.interpreters.pxla.unsigned_index->xops.Rem(xops.Div(xops.ReplicaId(c), div), mod)
A:jax.interpreters.pxla.axis_index_p->core.Primitive('axis_index')
A:jax.interpreters.pxla.sharded_aval->ShapedArray(aval.shape[1:], aval.dtype)
A:jax.interpreters.pxla.sharding_spec->_pmap_sharding_spec(nrep, axis_size, npart, parts, pv, True)
A:jax.interpreters.pxla.seen_index_hashes->set()
A:jax.interpreters.pxla.hashed_index->_hashable_index(index)
A:jax.interpreters.pxla.npy_value->numpy.empty(self.aval.shape, self.aval.dtype)
A:jax.interpreters.pxla.npy_value[self.indices[i]]->self.device_buffers[i].to_py()
A:jax.interpreters.pxla.aval->xla.abstractify(val)
A:jax.interpreters.pxla.candidates->defaultdict(list)
A:jax.interpreters.pxla.xla.pytype_aval_mappings[ShardedDeviceArray]->operator.attrgetter('aval')
A:jax.interpreters.pxla.abstract_args->unsafe_map(xla.abstractify, args)
A:jax.interpreters.pxla.compiled_fun->_soft_pmap_callable(fun, axis_name, axis_size, mapped_invars, *abstract_args)
A:jax.interpreters.pxla.global_axis_size->len(devices)
A:jax.interpreters.pxla.sharded_avals->tuple((shard_aval(axis_size, aval) if m else aval for (m, aval) in zip(mapped_invars, avals)))
A:jax.interpreters.pxla.(jaxpr, out_avals, consts)->pe.trace_to_jaxpr_final(fun, mapped_avals)
A:jax.interpreters.pxla.jaxpr->xla.apply_outfeed_rewriter(jaxpr)
A:jax.interpreters.pxla.pval->pe.PartialVal.unknown(core.abstract_unit)
A:jax.interpreters.pxla.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(dynamic_fun, [pval] + pvals, instantiate=False, stage_out=True, bottom=True)
A:jax.interpreters.pxla.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.pxla.is_multi_host_pmap->any((d.host_id != xb.host_id() for d in devices))
A:jax.interpreters.pxla.used_collectives->set(xla.jaxpr_collectives(jaxpr))
A:jax.interpreters.pxla.jaxpr_replicas->xla.jaxpr_replicas(jaxpr)
A:jax.interpreters.pxla.(arg_parts, out_parts, num_partitions)->_find_partitions(jaxpr)
A:jax.interpreters.pxla.axis_env->xla.AxisEnv(num_devices, (axis_name,), (num_devices,), None)
A:jax.interpreters.pxla.c->lib.xla_bridge.make_computation_builder('soft_pmap_{}'.format(fun.__name__))
A:jax.interpreters.pxla.xla_consts->map(partial(xb.constant, c), consts)
A:jax.interpreters.pxla.xla_args->xla._xla_callable_args(c, chunked_avals, tuple_args)
A:jax.interpreters.pxla.out_nodes->xla.jaxpr_subcomp(c, jaxpr, None, axis_env, xla_consts, 'soft_pmap', *xla_args)
A:jax.interpreters.pxla.build_out_tuple->partial(xops.Tuple, c, out_nodes)
A:jax.interpreters.pxla.out_tuple->build_out_tuple()
A:jax.interpreters.pxla.backend->lib.xla_bridge.get_backend(None)
A:jax.interpreters.pxla.donated_invars->xla.set_up_aliases(c, xla_args, out_tuple, donated_invars, tuple_args)
A:jax.interpreters.pxla.built->lib.xla_bridge.make_computation_builder('soft_pmap_{}'.format(fun.__name__)).Build(xops.Tuple(c, out_nodes))
A:jax.interpreters.pxla.devices->lib.xla_bridge.get_backend(backend).get_default_device_assignment(nrep)
A:jax.interpreters.pxla.local_devices_str->', '.join(map(str, local_devices))
A:jax.interpreters.pxla.device_assignment->numpy.array(device_assignment).reshape((num_global_replicas, num_partitions))
A:jax.interpreters.pxla.compile_options->lib.xla_bridge.get_compile_options(num_replicas=num_devices, num_partitions=1, device_assignment=None)
A:jax.interpreters.pxla.compiled->lib.xla_bridge.get_backend(None).compile(built, compile_options=compile_options)
A:jax.interpreters.pxla.handle_args->partial(shard_args, compiled.local_devices(), input_indices)
A:jax.interpreters.pxla.handle_outs->soft_pmap_avals_to_results_handler(num_devices, chunk_size, out_avals)
A:jax.interpreters.pxla.num_partitions->reconcile_num_partitions(eqn.params['call_jaxpr'], eqn.params['num_partitions'])
A:jax.interpreters.pxla.inner_num_parts->_inner_partitions(jaxpr, outer_num_parts)
A:jax.interpreters.pxla.nparts->get_num_partitions(parts)
A:jax.interpreters.pxla.expected_num_parts->_inner_partitions(subjaxpr, expected_num_parts)
A:jax.interpreters.pxla.num_partitions_set->set((np.prod(spec) for spec in partition_specs))
A:jax.interpreters.pxla.result_to_populate->ResultToPopulate()
A:jax.interpreters.pxla.nouts->len(out_avals)
A:jax.interpreters.pxla.replicated_aval->ShapedArray((axis_size,) + aval.shape, aval.dtype)
A:jax.interpreters.pxla.unsharded_aval->ShapedArray((axis_size,) + pv.shape, pv.dtype)
A:jax.interpreters.pxla.(replication_factor, ragged)->divmod(nrep, axis_size)
A:jax.interpreters.pxla.shard_spec->partitioned_sharding_spec(npart, parts, sharded_aval)
A:jax.interpreters.pxla.input_bufs->in_handler(args)
A:jax.interpreters.pxla.out_bufs->lib.xla_bridge.get_backend(None).compile(built, compile_options=compile_options).execute_on_local_devices(list(input_bufs))
A:jax.interpreters.pxla.xla_pmap_p->core.MapPrimitive('xla_pmap')
A:jax.interpreters.pxla.new_env->xla.extend_axis_env(axis_env, axis_name, global_axis_size)
A:jax.interpreters.pxla.sharded_outs->xla.jaxpr_subcomp(c, call_jaxpr, backend, new_env, (), extend_name_stack(name_stack, wrap_name(name, 'pmap')), *in_nodes_sharded)
A:jax.interpreters.pxla.ad.primitive_transposes[xla_pmap_p]->partial(ad.map_transpose, xla_pmap_p)
A:jax.interpreters.pxla.dims->list(xla_shape.dimensions())
A:jax.interpreters.pxla.zero->lib.xla_bridge.constant(c, np.zeros((), dtype=np.uint32))
A:jax.interpreters.pxla.x->xops.ConvertElementType(x, xb.dtype_to_etype(np.float32))
A:jax.interpreters.pxla.xla_shape->lib.xla_bridge.make_computation_builder('soft_pmap_{}'.format(fun.__name__)).get_shape(x)
A:jax.interpreters.pxla.padded->xops.DynamicUpdateSlice(padded, xops.Reshape(x, [1] + dims), idxs)
A:jax.interpreters.pxla.replica_groups_protos->lib.xla_client.make_replica_groups(xla.axis_groups(axis_env, axis_env.names[-1]))
A:jax.interpreters.pxla.out->xops.ConvertElementType(nonzero, xb.dtype_to_etype(np.bool_))
A:jax.interpreters.pxla.nonzero->xops.Ne(out, xb.constant(c, np.array(0, dtype=np.float32)))
A:jax.interpreters.pxla.num_devices->lib.xla_bridge.local_device_count()
A:jax.interpreters.pxla.(chunk_size, ragged)->divmod(axis_size, num_devices)
A:jax.interpreters.pxla.(jaxpr, _, consts)->_soft_pmap_jaxpr(jaxpr, consts, mapped_invars, axis_name, chunk_size)
A:jax.interpreters.pxla.fun->partial(_soft_pmap_interp, chunk_size, jaxpr, consts, mapped_invars)
A:jax.interpreters.pxla.(in_vals, in_mapped)->unzip2(map(read, eqn.invars))
A:jax.interpreters.pxla.(out_vals, out_mapped)->unzip2(map(read, jaxpr.outvars))
A:jax.interpreters.pxla.(call_jaxpr, params)->core.extract_call_jaxpr(eqn.primitive, eqn.params)
A:jax.interpreters.pxla.out_vals->pe.new_eqn_recipe([], [out_tracer], axis_index_p, dict(nreps=nreps, sizes=sizes, axis_name=axis_name), source_info_util.current()).primitive.bind(*in_vals, **eqn.params)
A:jax.interpreters.pxla.rule->batching.get_primitive_batcher(eqn.primitive)
A:jax.interpreters.pxla.(out_vals, out_axes)->rule(in_vals, in_axes, **eqn.params)
A:jax.interpreters.pxla.new_aval->ShapedArray((axis_size,) + aval.shape, aval.dtype)
A:jax.interpreters.pxla.spec->ShardingSpec(shards_per_axis=(num_devices,) + (1,) * aval.ndim, is_axis_materialized=(True,) * new_aval.ndim, replication_factors=[])
A:jax.interpreters.pxla.soft_pmap_p->core.MapPrimitive('soft_pmap')
A:jax.interpreters.pxla.idx->core.axis_index(axis_name)
jax.interpreters.pxla.DynamicAxisEnv(list)
jax.interpreters.pxla.DynamicAxisEnv.__contains__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.__getitem__(self,axis_name)
jax.interpreters.pxla.DynamicAxisEnv.nreps(self)
jax.interpreters.pxla.DynamicAxisEnv.sizes(self)
jax.interpreters.pxla.DynamicAxisEnvFrame(self,name,pmap_trace,hard_size)
jax.interpreters.pxla.ResultToPopulate
jax.interpreters.pxla.ShardedDeviceArray(self,aval:ShapedArray,sharding_spec,device_buffers:List[xb.xla_client._xla.PyLocalBuffer]=None,indices:Optional[Tuple[Index,...]]=None)
jax.interpreters.pxla.ShardedDeviceArray.__getitem__(self,idx)
jax.interpreters.pxla.ShardedDeviceArray._check_if_deleted(self)
jax.interpreters.pxla.ShardedDeviceArray._value(self)
jax.interpreters.pxla.ShardedDeviceArray.block_until_ready(self)
jax.interpreters.pxla.ShardedDeviceArray.copy_to_host_async(self)
jax.interpreters.pxla.ShardedDeviceArray.delete(self)
jax.interpreters.pxla.ShardedDeviceArray.one_replica_buffer_indices(self)
jax.interpreters.pxla.ShardingSpec(self,shards_per_axis:Tuple[int,...],is_axis_materialized:Tuple[bool,...],replication_factors:List[Tuple[int,int]])
jax.interpreters.pxla.ShardingSpec.__eq__(self,other)
jax.interpreters.pxla.ShardingSpec.__repr__(self)
jax.interpreters.pxla._ThreadLocalState(self)
jax.interpreters.pxla._as_slice_indices(arr:xla.DeviceArray,idx:Index)->Tuple[Tuple[int, ...], Tuple[int, ...], Tuple[int, ...]]
jax.interpreters.pxla._axis_index_bind(*,axis_name)
jax.interpreters.pxla._axis_index_soft_pmap_rule(vals,mapped,chunk_size,*,axis_name)
jax.interpreters.pxla._axis_index_translation_rule(c,nreps,sizes,axis_name)
jax.interpreters.pxla._axis_indices(axis_size,num_shards,is_materialized)
jax.interpreters.pxla._find_partitions(jaxpr)->Tuple[Optional[Tuple[PartitionsOrReplicated, ...]], Optional[Tuple[PartitionsOrReplicated, ...]], int]
jax.interpreters.pxla._hashable_index(idx)
jax.interpreters.pxla._inner_partitions(jaxpr,expected_num_parts:Optional[int])
jax.interpreters.pxla._pmap_sharding_spec(nrep,axis_size,npart,parts,sharded_aval,mapped)
jax.interpreters.pxla._pmap_translation_rule(c,axis_env,in_nodes,name_stack,axis_name,axis_size,global_axis_size,devices,name,call_jaxpr,*,backend=None,mapped_invars,donated_invars)
jax.interpreters.pxla._pval_to_result_handler(axis_size,nrep,npart,parts,pval,devices,backend)
jax.interpreters.pxla._pvals_to_results_handler(size,nrep,npart,out_parts:Optional[Tuple[PartitionsOrReplicated,...]],out_pvals,devices,backend)
jax.interpreters.pxla._shard_abstract_array(size,x)
jax.interpreters.pxla._shard_array(x,devices,indices)
jax.interpreters.pxla._shard_device_array(x,devices,indices)
jax.interpreters.pxla._shard_sharded_device_array_slow_path(x,devices,indices)
jax.interpreters.pxla._sharded_device_array_constant_handler(c,val,canonicalize_types=True)
jax.interpreters.pxla._soft_pmap_callable(fun,axis_name,axis_size,mapped_invars,*avals)
jax.interpreters.pxla._soft_pmap_interp(chunk_size,jaxpr,consts,mapped_invars,*args)
jax.interpreters.pxla._soft_pmap_jaxpr(jaxpr,consts,mapped_invars,axis_name,chunk_size)
jax.interpreters.pxla._unravel_index(c,axis_env)
jax.interpreters.pxla._xla_shard(c,aval,axis_env,x)
jax.interpreters.pxla._xla_unshard(c,aval,axis_env,x,backend)
jax.interpreters.pxla.apply_parallel_primitive(prim,*args,**params)
jax.interpreters.pxla.array_result_handler(sharding_spec,indices,aval:ShapedArray)
jax.interpreters.pxla.aval_to_result_handler(sharding_spec:Optional[ShardingSpec],indices:Optional[Tuple[Index]],aval:core.AbstractValue)->Callable[[List[xb.xla_client._xla.PyLocalBuffer]], Any]
jax.interpreters.pxla.axis_index(axis_name)
jax.interpreters.pxla.execute_replicated(compiled,backend,in_handler,out_handler,*args)
jax.interpreters.pxla.extend_dynamic_axis_env(axis_name,pmap_trace,hard_size)
jax.interpreters.pxla.get_num_partitions(*partitions)
jax.interpreters.pxla.identity(x)
jax.interpreters.pxla.omnistaging_enable()->None
jax.interpreters.pxla.parallel_callable(fun,backend,axis_name,axis_size,global_axis_size,devices,name,mapped_invars,donated_invars,*avals)
jax.interpreters.pxla.partitioned_sharding_spec(num_partitions:int,partitions:Optional[Sequence[int]],aval)
jax.interpreters.pxla.reconcile_num_partitions(jaxpr,outer_num_parts:Optional[int])
jax.interpreters.pxla.replicate(val,axis_size,nrep,devices=None,backend=None)
jax.interpreters.pxla.shard_args(devices:Sequence[xb.xla_client.Device],indices:Sequence[Sequence[Index]],args)->Sequence[Sequence[xb.xla_client._xla.PyLocalBuffer]]
jax.interpreters.pxla.shard_aval(size,aval)
jax.interpreters.pxla.soft_pmap_aval_to_result_handler(chunk_size,num_devices,aval)
jax.interpreters.pxla.soft_pmap_avals_to_results_handler(num_devices,chunk_size,out_avals)
jax.interpreters.pxla.soft_pmap_impl(fun:lu.WrappedFun,*args,axis_name,axis_size,mapped_invars)
jax.interpreters.pxla.spec_to_indices(shape:Tuple[int,...],sharding_spec:ShardingSpec)->Tuple[Index, ...]
jax.interpreters.pxla.unmapped_device_count(backend=None)
jax.interpreters.pxla.xla_pmap_impl(fun:lu.WrappedFun,*args,backend,axis_name,axis_size,global_axis_size,devices,name,mapped_invars,donated_invars)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/batching.py----------------------------------------
A:jax.interpreters.batching.batched_fun->batch_fun(fun, in_dims, out_dim_dests)
A:jax.interpreters.batching.trace->BatchTrace(master, core.cur_sublevel())
A:jax.interpreters.batching.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.batching.(out_vals, out_dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(fun, out_dims)->batch_subtrace(fun)
A:jax.interpreters.batching.out_dims->map(_merge_bdims, out_primal_bds, out_tangent_bds)
A:jax.interpreters.batching.out_vals->prim.bind(fun, fwd, bwd, *in_vals, out_trees=out_trees)
A:jax.interpreters.batching.NotMapped->type(None)
A:jax.interpreters.batching.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.batching.new_shape->tuple(np.delete(aval.shape, self.batch_dim))
A:jax.interpreters.batching.(vals_in, dims_in)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.batched_primitive->get_primitive_batcher(primitive)
A:jax.interpreters.batching.(val_out, dim_out)->batched_primitive(vals_in, dims_in, **params)
A:jax.interpreters.batching.params->dict(params, input_shape=operand.shape)
A:jax.interpreters.batching.(vals, dims)->unzip2(((t.val, t.batch_dim) for t in out_tracers))
A:jax.interpreters.batching.(f, dims_out)->batch_subtrace(f, self.master, dims)
A:jax.interpreters.batching.vals_out->map_primitive.bind(f, *vals, **params)
A:jax.interpreters.batching.dims->tuple((not_mapped if d is not_mapped else max(0, d - mapped_invar) for (d, mapped_invar) in zip(dims, mapped_invars)))
A:jax.interpreters.batching.dims_out->tuple((d + 1 if d is not not_mapped else d for d in dims_out()))
A:jax.interpreters.batching.(in_vals, in_dims)->unzip2(((t.val, t.batch_dim) for t in tracers))
A:jax.interpreters.batching.(fun, out_dims1)->batch_subtrace(fun, self.master, in_dims)
A:jax.interpreters.batching.(jvp, out_dims2)->batch_custom_jvp_subtrace(jvp, self.master, in_dims)
A:jax.interpreters.batching.(fst, out_dims)->lu.merge_linear_aux(out_dims1, out_dims2)
A:jax.interpreters.batching.(fwd, out_dims2)->batch_subtrace(fwd, self.master, in_dims)
A:jax.interpreters.batching.bwd->batch_fun(bwd, out_dims2, in_dims, sum_match=True)
A:jax.interpreters.batching.primitive_batchers[prim]->partial(reducer_batcher, prim)
A:jax.interpreters.batching.d->next((d for d in dims if d is not not_mapped))
A:jax.interpreters.batching.out->prim.bind(*args, **params)
A:jax.interpreters.batching.ndim->max((np.ndim(x) for x in args))
A:jax.interpreters.batching.axes->tuple(np.where(np.less(axes, bdim), axes, np.add(axes, 1)))
A:jax.interpreters.batching.bdim_out->int(list(np.delete(np.arange(operand.ndim), axes)).index(bdim))
A:jax.interpreters.batching.x->moveaxis(x, bdx, bdy)
A:jax.interpreters.batching.y->broadcast(y, x.shape[bdx], bdx)
A:jax.interpreters.batching.last->_Last()
A:jax.interpreters.batching.axis->numpy.ndim(x)
A:jax.interpreters.batching.shape->list(np.shape(x))
A:jax.interpreters.batching.broadcast_dims->tuple(np.delete(np.arange(len(shape)), axis))
A:jax.interpreters.batching.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.batching.(f, batched_out)->batched_traceable(f, size, batched, instantiate)
A:jax.interpreters.batching.(jaxpr_out, pvals_out, consts_out)->pe.trace_to_jaxpr(f, in_pvals, instantiate=True)
A:jax.interpreters.batching.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.batching.jaxpr_out->core.TypedJaxpr(jaxpr_out, literals_out, avals_in, avals_out)
A:jax.interpreters.batching.(out_primals, out_tangents)->split_list(out_vals, [len(out_vals) // 2])
A:jax.interpreters.batching.(out_primal_bds, out_tangent_bds)->split_list(out_dims, [len(out_vals) // 2])
A:jax.interpreters.batching.out_primals->map(partial(matchaxis, size), out_primal_bds, out_dims, out_primals)
A:jax.interpreters.batching.out_tangents->map(partial(matchaxis, size), out_tangent_bds, out_dims, out_tangents)
A:jax.interpreters.batching.(jaxpr_out, avals_out, literals_out)->pe.trace_to_jaxpr_dynamic(f, avals_in)
jax.interpreters.batching.BatchTrace(Trace)
jax.interpreters.batching.BatchTrace.lift(self,val)
jax.interpreters.batching.BatchTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.post_process_map(self,call_primitive,out_tracers,params)
jax.interpreters.batching.BatchTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.batching.BatchTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.interpreters.batching.BatchTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,*,out_trees)
jax.interpreters.batching.BatchTrace.process_map(self,map_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.batching.BatchTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.batching.BatchTrace.pure(self,val)
jax.interpreters.batching.BatchTrace.sublift(self,val)
jax.interpreters.batching.BatchTracer(self,trace,val,batch_dim:Optional[int])
jax.interpreters.batching.BatchTracer.aval(self)
jax.interpreters.batching.BatchTracer.full_lower(self)
jax.interpreters.batching._Last(object)
jax.interpreters.batching._batch_fun(sum_match,in_dims,out_dims_thunk,out_dim_dests,*in_vals,**params)
jax.interpreters.batching._batch_fun2(in_dims,*in_vals,**params)
jax.interpreters.batching._handle_scalar_broadcasting(nd,x,d)
jax.interpreters.batching._merge_bdims(x,y)
jax.interpreters.batching._promote_aval_rank(sz,aval)
jax.interpreters.batching.add_batched(batched_args,batch_dims)
jax.interpreters.batching.batch(fun:lu.WrappedFun,in_vals,in_dims,out_dim_dests)
jax.interpreters.batching.batch_custom_jvp_subtrace(master,in_dims,*in_vals)
jax.interpreters.batching.batch_fun(fun:lu.WrappedFun,in_dims,out_dim_dests,sum_match=False)
jax.interpreters.batching.batch_fun2(fun:lu.WrappedFun,in_dims)
jax.interpreters.batching.batch_jaxpr(jaxpr,size,batched,instantiate)
jax.interpreters.batching.batch_subtrace(master,in_dims,*in_vals,**params)
jax.interpreters.batching.batched_traceable(size,batched,instantiate,*vals)
jax.interpreters.batching.bdim_at_front(x,bdim,size)
jax.interpreters.batching.broadcast(x,sz,axis)
jax.interpreters.batching.broadcast_batcher(prim,args,dims,**params)
jax.interpreters.batching.defbroadcasting(prim)
jax.interpreters.batching.defreducer(prim)
jax.interpreters.batching.defvectorized(prim)
jax.interpreters.batching.get_primitive_batcher(p)
jax.interpreters.batching.matchaxis(sz,src,dst,x,sum_match=False)
jax.interpreters.batching.moveaxis(x,src,dst)
jax.interpreters.batching.omnistaging_enabler()->None
jax.interpreters.batching.reducer_batcher(prim,batched_args,batch_dims,axes,**params)
jax.interpreters.batching.vectorized_batcher(prim,batched_args,batch_dims,**params)
jax.interpreters.batching.zeros_like_batched(batched_args,batch_dims)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/partial_eval.py----------------------------------------
A:jax.interpreters.partial_eval.known->self.pval.get_known()
A:jax.interpreters.partial_eval.const->DynamicJaxprTracer(self, raise_to_shaped(get_aval(val), weak_type=dtypes.is_python_scalar(val))).pval.get_known()
A:jax.interpreters.partial_eval.aval->raise_to_shaped(get_aval(const), np.isscalar(const))
A:jax.interpreters.partial_eval.tracers->map(self.instantiate_const_abstracted, tracers)
A:jax.interpreters.partial_eval.out_aval->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.source->source_info_util.current()
A:jax.interpreters.partial_eval.eqn->new_jaxpr_eqn([*constvars, *invars], outvars, map_primitive, new_params)
A:jax.interpreters.partial_eval.out_tracer->JaxprTracer(self, PartialVal.unknown(out_aval), None)
A:jax.interpreters.partial_eval.out_tracer.recipe->new_eqn_recipe(tracers, [out_tracer], primitive, params, source)
A:jax.interpreters.partial_eval.mapped_aval->partial(core.mapped_aval, params['axis_size'])
A:jax.interpreters.partial_eval.(jaxpr, out_pvals, consts, env_tracers)->self.partial_eval(f, in_pvals, partial(primitive.bind, **params))
A:jax.interpreters.partial_eval.unmapped_aval->partial(core.unmapped_aval, params['axis_size'])
A:jax.interpreters.partial_eval.in_knowns->tuple((t.pval.is_known() for t in it.chain(env_tracers, tracers)))
A:jax.interpreters.partial_eval.out_unknowns->tuple((not pval.is_known() for pval in out_pvals))
A:jax.interpreters.partial_eval.jaxpr->Jaxpr(constvars, invars, outvars, self.eqns)
A:jax.interpreters.partial_eval.const_tracers->map(trace.new_instantiated_const, consts)
A:jax.interpreters.partial_eval.new_params->update_params(new_params, [True] * len(tracers))
A:jax.interpreters.partial_eval.update_params->call_param_updaters.get(map_primitive)
A:jax.interpreters.partial_eval.(jaxpr, consts, env)->tracers_to_jaxpr(in_tracers, out_tracers)
A:jax.interpreters.partial_eval.(out_pvs, out_pv_consts)->unzip2((t.pval for t in out_tracers))
A:jax.interpreters.partial_eval.n->len(jaxpr.outvars)
A:jax.interpreters.partial_eval.trace->DynamicJaxprTrace(master, core.cur_sublevel())
A:jax.interpreters.partial_eval.(in_avals, in_consts)->unzip2(pvals)
A:jax.interpreters.partial_eval.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.partial_eval.(f, aux)->partial_eval_wrapper(f, tuple(in_avals))
A:jax.interpreters.partial_eval.(out_consts, consts)->split_list(out_flat, [len(out_flat) - len(jaxpr.constvars)])
A:jax.interpreters.partial_eval.out_pvs->map(PartialVal, zip(out_avals, out_consts))
A:jax.interpreters.partial_eval.env_tracers->map(self.full_raise, env)
A:jax.interpreters.partial_eval.py_args->map(PartialVal, zip(pvs, consts))
A:jax.interpreters.partial_eval.(out_pvs, out_consts)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.(_, pvals_out, _)->trace_to_jaxpr(lu.wrap_init(fun, params), pvals_in, instantiate=True, stage_out=True)
A:jax.interpreters.partial_eval.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.partial_eval.fun->trace_to_subjaxpr(fun, master, instantiate)
A:jax.interpreters.partial_eval.(jaxpr, (out_pvals, consts, env))->trace_to_subjaxpr(fun, master, instantiate).call_wrapped(pvals)
A:jax.interpreters.partial_eval.in_tracers->map(trace.new_arg, in_avals)
A:jax.interpreters.partial_eval.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.partial_eval.FreeVar->namedtuple('FreeVar', ['val'])
A:jax.interpreters.partial_eval.ConstVar->namedtuple('ConstVar', ['val'])
A:jax.interpreters.partial_eval.LambdaBinding->namedtuple('LambdaBinding', [])
A:jax.interpreters.partial_eval.newvar->core.gensym()
A:jax.interpreters.partial_eval.var->self.frame.constid_to_var.get(id(c))
A:jax.interpreters.partial_eval.vart_to_var[id(t)]->newvar(aval)
A:jax.interpreters.partial_eval.sorted_tracers->toposort(out_tracers)
A:jax.interpreters.partial_eval.invars->map(self.getvar, tracers)
A:jax.interpreters.partial_eval.varconst_to_var[id(c)]->newvar(get_aval(c))
A:jax.interpreters.partial_eval.processed_eqn_ids->set()
A:jax.interpreters.partial_eval.vt_to_var[id(t)]->getconstvar(recipe.val)
A:jax.interpreters.partial_eval.(env_vars, env_vals)->unzip2(env.items())
A:jax.interpreters.partial_eval.(const_vars, const_vals)->unzip2(consts.items())
A:jax.interpreters.partial_eval.lifted_jaxpr->Jaxpr(constvars=(), invars=jaxpr.constvars + jaxpr.invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns)
A:jax.interpreters.partial_eval.(jaxpr_2, out_pvals_2, consts_2)->trace_to_jaxpr(f, pvals, instantiate=instantiate)
A:jax.interpreters.partial_eval.(out_pvs_2, out_consts_2)->unzip2(out_pvals_2)
A:jax.interpreters.partial_eval.(jaxpr_1, out_pvals, consts_1)->trace_to_jaxpr(lu.wrap_init(fun), pvals, instantiate=True)
A:jax.interpreters.partial_eval.jaxpr_2->convert_constvars_jaxpr(jaxpr_2)
A:jax.interpreters.partial_eval.(in_avals_1, in_avals_2)->unzip2(map(_split_aval, unknowns, jaxpr.in_avals))
A:jax.interpreters.partial_eval.(out_avals_1, out_avals_2)->unzip2(map(_split_aval, uk_out, jaxpr.out_avals))
A:jax.interpreters.partial_eval.(out_pvs, _)->unzip2(out_pvals)
A:jax.interpreters.partial_eval.typed_jaxpr_1->TypedJaxpr(jaxpr_1, consts_1, in_avals_1, out_avals_1)
A:jax.interpreters.partial_eval.typed_jaxpr_2->TypedJaxpr(jaxpr_2, (), in_avals_2, out_avals_2)
A:jax.interpreters.partial_eval.remat_call_p->core.CallPrimitive('remat_call')
A:jax.interpreters.partial_eval.instantiated_tracers->map(trace.instantiate_const_abstracted, tracers)
A:jax.interpreters.partial_eval.(jaxpr, eval_out_pvals, consts, env_tracers)->DynamicJaxprTrace(master, core.cur_sublevel()).partial_eval(f, in_pvals, partial(remat_call_p.bind, **params))
A:jax.interpreters.partial_eval.typed_jaxpr->core.TypedJaxpr(jaxpr, (), [v.aval for v in jaxpr.invars], [v.aval for v in jaxpr.outvars])
A:jax.interpreters.partial_eval.(jaxpr_known, jaxpr_unknown, out_unknowns)->partial_eval_jaxpr(typed_jaxpr, in_unknowns, instantiate=False, trace_type=trace.master.trace_type)
A:jax.interpreters.partial_eval.(out_known_pvals, out_unknown_pvals)->_partition_knowns(eval_out_pvals, out_unknowns)
A:jax.interpreters.partial_eval.num_outputs->len(jaxpr_unknown.out_avals)
A:jax.interpreters.partial_eval.jaxpr_known_nores->_dce_jaxpr(jaxpr_known, out_knowns + [False] * num_res, drop_outputs=True)
A:jax.interpreters.partial_eval.jaxpr_known_comp->_dce_jaxpr(jaxpr_known_nores, to_compute)
A:jax.interpreters.partial_eval.(_, in_consts)->unzip2((t.pval for t in it.chain(env_tracers, tracers)))
A:jax.interpreters.partial_eval.reconstructed_consts->core.jaxpr_as_fun(jaxpr_known_comp)(*consts, *in_consts)
A:jax.interpreters.partial_eval.out_known_pvals->map(_reconstruct_pval, out_known_pvals, reconstructed_consts)
A:jax.interpreters.partial_eval.call_partial_eval_rules[remat_call_p]->partial(_remat_partial_eval, _remat_make_output_tracers)
A:jax.interpreters.partial_eval.new_jaxpr->Jaxpr(new_constvars, new_invars, new_outvars, new_eqns)
A:jax.interpreters.partial_eval.new_invars->_move_to_front(typed_jaxpr.jaxpr.invars, to_move)
A:jax.interpreters.partial_eval.new_in_avals->_move_to_front(typed_jaxpr.in_avals, to_move)
A:jax.interpreters.partial_eval.new_typed_jaxpr->core.TypedJaxpr(new_jaxpr, typed_jaxpr.literals, new_in_avals, typed_jaxpr.out_avals)
A:jax.interpreters.partial_eval.msgs->self._progenitor_messages()
A:jax.interpreters.partial_eval.progenitor_eqns->self._trace.frame.find_progenitors(self)
A:jax.interpreters.partial_eval.self.newvar->core.gensym()
A:jax.interpreters.partial_eval.(constvars, constvals)->unzip2(self.constvar_to_val.items())
A:jax.interpreters.partial_eval.(jaxpr, constvals)->_inline_literals(jaxpr, constvals)
A:jax.interpreters.partial_eval.consts->dict(zip(jaxpr.constvars, constvals))
A:jax.interpreters.partial_eval.new_vself[v]->newvar(v.aval)
A:jax.interpreters.partial_eval.val->dict(zip(jaxpr.constvars, constvals)).get(var)
A:jax.interpreters.partial_eval.tracer->DynamicJaxprTracer(self, raise_to_shaped(get_aval(val), weak_type=dtypes.is_python_scalar(val)))
A:jax.interpreters.partial_eval.self.frame.tracer_to_var[id(tracer)]->self.frame.newvar(aval)
A:jax.interpreters.partial_eval.varself.frame.tracer_to_var[id(tracer)]->self.frame.newvar(tracer.aval)
A:jax.interpreters.partial_eval.varself.frame.constid_to_var[id(c)]->self.frame.newvar(get_aval(c))
A:jax.interpreters.partial_eval.out_avals->primitive.abstract_eval(*avals, **params)
A:jax.interpreters.partial_eval.outvars->map(self.getvar, out_tracers)
A:jax.interpreters.partial_eval.(jaxpr, out_avals, consts)->trace_to_subjaxpr_dynamic(fun, master, in_avals)
A:jax.interpreters.partial_eval.constvars->map(self.getvar, map(self.instantiate_const, consts))
A:jax.interpreters.partial_eval.(jaxpr, reduced_out_avals, consts)->trace_to_subjaxpr_dynamic(f, self.master, reduced_in_avals)
A:jax.interpreters.partial_eval.master.source_info->fun_sourceinfo(fun.f)
A:jax.interpreters.partial_eval.frame->JaxprStackFrame()
A:jax.interpreters.partial_eval.ans->trace_to_subjaxpr(fun, master, instantiate).call_wrapped(*in_tracers)
A:jax.interpreters.partial_eval.(jaxpr_1, out_avals, consts_1)->trace_to_jaxpr_dynamic(lu.wrap_init(fun), in_avals)
jax.interpreters.partial_eval.DynamicJaxprTrace(core.Trace)
jax.interpreters.partial_eval.DynamicJaxprTrace.frame(self)
jax.interpreters.partial_eval.DynamicJaxprTrace.getconstvar(self,c)
jax.interpreters.partial_eval.DynamicJaxprTrace.getvar(self,tracer)
jax.interpreters.partial_eval.DynamicJaxprTrace.instantiate_const(self,val)
jax.interpreters.partial_eval.DynamicJaxprTrace.new_arg(self,aval)
jax.interpreters.partial_eval.DynamicJaxprTrace.new_const(self,val)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.post_process_map(self,map_primitive,out_tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_map(self,map_primitive,f,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.DynamicJaxprTracer(self,trace,aval,line_info=None)
jax.interpreters.partial_eval.DynamicJaxprTracer.__bool__(self)
jax.interpreters.partial_eval.DynamicJaxprTracer._concretization_error(self,name)
jax.interpreters.partial_eval.DynamicJaxprTracer._contents(self)
jax.interpreters.partial_eval.DynamicJaxprTracer._progenitor_messages(self)
jax.interpreters.partial_eval.DynamicJaxprTracer.full_lower(self)
jax.interpreters.partial_eval.JaxprEqnRecipe(NamedTuple)
jax.interpreters.partial_eval.JaxprStackFrame(self)
jax.interpreters.partial_eval.JaxprStackFrame.find_progenitors(self,tracer)
jax.interpreters.partial_eval.JaxprStackFrame.to_jaxpr(self,in_tracers,out_tracers)
jax.interpreters.partial_eval.JaxprTrace(Trace)
jax.interpreters.partial_eval.JaxprTrace.default_process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.instantiate_const(self,tracer)->Tracer
jax.interpreters.partial_eval.JaxprTrace.instantiate_const_abstracted(self,tracer)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.lift(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.new_arg(self,pval:PartialVal)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.new_const(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_const(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.new_instantiated_literal(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.partial_eval(self,f:lu.WrappedFun,pvals:Sequence[PartialVal],app:Callable[[lu.WrappedFun,Tuple[core.Value,...]],Tuple[core.Value]])
jax.interpreters.partial_eval.JaxprTrace.post_process_call(self,primitive,out_tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_call(self,primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.process_custom_jvp_call(self,prim,fun,jvp,tracers)
jax.interpreters.partial_eval.JaxprTrace.process_custom_vjp_call(self,prim,fun,fwd,bwd,tracers,out_trees)
jax.interpreters.partial_eval.JaxprTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.partial_eval.JaxprTrace.pure(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTrace.sublift(self,val)->'JaxprTracer'
jax.interpreters.partial_eval.JaxprTracer(self,trace:JaxprTrace,pval:PartialVal,recipe:Optional[JaxprTracerRecipe])
jax.interpreters.partial_eval.JaxprTracer.__repr__(self)
jax.interpreters.partial_eval.JaxprTracer.aval(self)->AbstractValue
jax.interpreters.partial_eval.JaxprTracer.full_lower(self)
jax.interpreters.partial_eval.JaxprTracer.is_known(self)
jax.interpreters.partial_eval.JaxprTracer.parents(self)->Sequence['JaxprTracer']
jax.interpreters.partial_eval.PartialVal(cls,xs:Tuple[Optional[AbstractValue],core.Value])
jax.interpreters.partial_eval.PartialVal.get_aval(self)->AbstractValue
jax.interpreters.partial_eval.PartialVal.get_known(self)->Optional[core.Value]
jax.interpreters.partial_eval.PartialVal.is_known(self)->bool
jax.interpreters.partial_eval.PartialVal.known(cls,const:core.Value)->'PartialVal'
jax.interpreters.partial_eval.PartialVal.merge_with_known(self,val:core.Value)->core.Value
jax.interpreters.partial_eval.PartialVal.unknown(cls,aval:AbstractValue)->'PartialVal'
jax.interpreters.partial_eval.StagingJaxprTrace(JaxprTrace)
jax.interpreters.partial_eval._dce_jaxpr(typed_jaxpr:TypedJaxpr,outputs:Sequence[bool],drop_outputs=False)->TypedJaxpr
jax.interpreters.partial_eval._dce_untyped_jaxpr(jaxpr:Jaxpr,outputs:Tuple[bool,...],drop_outputs=False)->Jaxpr
jax.interpreters.partial_eval._drop_invars(jaxpr:Jaxpr,drop:Tuple[bool,...])
jax.interpreters.partial_eval._inline_literals(jaxpr,constvals)
jax.interpreters.partial_eval._move_to_front(lst:Sequence,to_move:Sequence[bool])->Sequence
jax.interpreters.partial_eval._partition_knowns(pvals,unknowns:Sequence[bool])
jax.interpreters.partial_eval._reconstruct_pval(pval1:PartialVal,const2:core.Value)
jax.interpreters.partial_eval._remat_make_output_tracers(_,in_tracers,out_tracers,params)
jax.interpreters.partial_eval._remat_partial_eval(process_out,trace,_,f,tracers,params)
jax.interpreters.partial_eval._split_aval(unknown:bool,aval:AbstractValue)->Tuple[AbstractValue, AbstractValue]
jax.interpreters.partial_eval._zip_knowns(known_list,unknown_list,which_unknown:Sequence[bool])
jax.interpreters.partial_eval.abstract_eval_fun(fun,*avals,**params)
jax.interpreters.partial_eval.convert_constvars_jaxpr(jaxpr:Jaxpr)
jax.interpreters.partial_eval.extend_jaxpr_stack(master,frame)
jax.interpreters.partial_eval.fun_sourceinfo(fun)
jax.interpreters.partial_eval.identity(x)
jax.interpreters.partial_eval.instantiate_const_at(trace:JaxprTrace,instantiate:bool,tracer)
jax.interpreters.partial_eval.move_binders_to_front(typed_jaxpr:TypedJaxpr,to_move:Sequence[bool])->TypedJaxpr
jax.interpreters.partial_eval.new_eqn_recipe(invars:Sequence[JaxprTracer],outvars:Sequence[JaxprTracer],primitive:core.Primitive,params:Dict[str,Any],source_info:Optional[source_info_util.Traceback])->JaxprEqnRecipe
jax.interpreters.partial_eval.omnistaging_enabler()->None
jax.interpreters.partial_eval.partial_eval_jaxpr(jaxpr:TypedJaxpr,unknowns:Sequence[bool],instantiate:Union[bool,Sequence[bool]],trace_type:Optional[Type[core.Trace]])->Tuple[TypedJaxpr, TypedJaxpr, Sequence[bool]]
jax.interpreters.partial_eval.partial_eval_wrapper(pvs:Sequence[Optional[AbstractValue]],*consts)
jax.interpreters.partial_eval.recipe_to_eqn(getvar:Callable[[JaxprTracer],core.Atom],recipe:JaxprEqnRecipe)->core.JaxprEqn
jax.interpreters.partial_eval.trace_to_jaxpr(fun:lu.WrappedFun,pvals:Sequence[PartialVal],instantiate:Union[bool,Sequence[bool]]=False,stage_out=False,bottom=False,trace_type:Optional[Type[Trace]]=None)->Tuple[Jaxpr, Tuple[PartialVal, ...], Tuple[core.Value, ...]]
jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue])
jax.interpreters.partial_eval.trace_to_jaxpr_final(fun:lu.WrappedFun,in_avals:Sequence[AbstractValue])
jax.interpreters.partial_eval.trace_to_subjaxpr(master:core.MasterTrace,instantiate:Union[bool,Sequence[bool]],pvals:Sequence[PartialVal])
jax.interpreters.partial_eval.trace_to_subjaxpr_dynamic(fun:lu.WrappedFun,master:core.MasterTrace,in_avals:Sequence[AbstractValue])
jax.interpreters.partial_eval.tracers_to_jaxpr(in_tracers:List[JaxprTracer],out_tracers:List[JaxprTracer])->Tuple[Jaxpr, Tuple[Any, ...], Tuple[Any, ...]]


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/parallel.py----------------------------------------
A:jax.interpreters.parallel.(fun, _)->papply_transform(fun, name, axis_size)
A:jax.interpreters.parallel.trace->PapplyTrace(master, core.cur_sublevel())
A:jax.interpreters.parallel.in_tracers->map(partial(PapplyTracer, trace, name, axis_size, axis=0), args)
A:jax.interpreters.parallel.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.parallel.(out_vals, out_axes)->unzip2(((t.val, t.axis) for t in out_tracers))
A:jax.interpreters.parallel.NotSharded->type(None)
A:jax.interpreters.parallel.aval->raise_to_shaped(core.get_aval(self.val))
A:jax.interpreters.parallel.new_shape->list(aval.shape)
A:jax.interpreters.parallel.(names, vals, axes)->unzip3(((t.name, t.val, t.axis) for t in tracers))
A:jax.interpreters.parallel.(val_out, axis_out)->rule(name, size, vals, axes, **params)
A:jax.interpreters.parallel.(f_papply, axes_out)->papply_subtrace(f, self.master, name, size, axes)
A:jax.interpreters.parallel.vals_out->call_primitive.bind(f_papply, *vals, **params)
jax.interpreters.parallel.PapplyTrace(Trace)
jax.interpreters.parallel.PapplyTrace.lift(self,val)
jax.interpreters.parallel.PapplyTrace.post_process_call(self,call_primitive,out_tracer)
jax.interpreters.parallel.PapplyTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.parallel.PapplyTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.parallel.PapplyTrace.pure(self,val)
jax.interpreters.parallel.PapplyTrace.sublift(self,val)
jax.interpreters.parallel.PapplyTracer(self,trace,name,axis_size,val,axis)
jax.interpreters.parallel.PapplyTracer.aval(self)
jax.interpreters.parallel.PapplyTracer.full_lower(self)
jax.interpreters.parallel.identity(x)
jax.interpreters.parallel.papply(fun,name,in_vals,axis_size)
jax.interpreters.parallel.papply_subtrace(master,name,axis_size,axes,*vals)
jax.interpreters.parallel.papply_transform(name,axis_size,*args)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/masking.py----------------------------------------
A:jax.interpreters.masking.masking_rules[prim]->partial(naryop_masking_rule, prim)
A:jax.interpreters.masking.ShapeEnvs->namedtuple('ShapeEnvs', ['logical', 'padded'])
A:jax.interpreters.masking.shape_envs->ShapeEnvs({}, {})
A:jax.interpreters.masking.new_logical->dict(chain(shape_envs.logical.items(), logical_env.items()))
A:jax.interpreters.masking.new_padded->dict(chain(shape_envs.padded.items(), padded_env.items()))
A:jax.interpreters.masking.(env_keys, padded_env_vals)->unzip2(sorted(padded_env.items()))
A:jax.interpreters.masking.(fun, out_shapes)->mask_subtrace(fun, master, polymorphic_shapes, padded_env)
A:jax.interpreters.masking.out_vals->fun.call_wrapped(*logical_env_vals + in_vals)
A:jax.interpreters.masking.logical_env->dict(zip(env_keys, logical_env_vals))
A:jax.interpreters.masking.padded_env->dict(zip(*padded_env))
A:jax.interpreters.masking.trace->MaskTrace(master, core.cur_sublevel())
A:jax.interpreters.masking.out_tracers->map(trace.full_raise, outs)
A:jax.interpreters.masking.(out_vals, out_shapes)->unzip2(((t.val, t.polymorphic_shape) for t in out_tracers))
A:jax.interpreters.masking.coeffs->self.copy()
A:jax.interpreters.masking.other->_ensure_poly(other)
A:jax.interpreters.masking.(q, _)->divmod(self, divisor)
A:jax.interpreters.masking.(_, r)->divmod(self, divisor)
A:jax.interpreters.masking.(q, r)->divmod(count, divisor)
A:jax.interpreters.masking.deg->int(deg)
A:jax.interpreters.masking.coeff->int(coeff)
A:jax.interpreters.masking.dims->map(_parse_dim, spec.replace(' ', '').strip(',').split(','))
A:jax.interpreters.masking._identifiers->frozenset(string.ascii_lowercase)
A:jax.interpreters.masking._monomorphic_dim->MonomorphicDim()
A:jax.interpreters.masking.s_->S_()
A:jax.interpreters.masking.masking_rule->masking_rules.get(primitive)
A:jax.interpreters.masking.out_aval->primitive.abstract_eval(*(t.aval for t in tracers), **params)
A:jax.interpreters.masking.(vals, polymorphic_shapes)->unzip2(((t.val, t.polymorphic_shape) for t in tracers))
A:jax.interpreters.masking.logical_shapes->map(shape_as_value, polymorphic_shapes)
A:jax.interpreters.masking.out->masking_rule(vals, logical_shapes, **params)
A:jax.interpreters.masking.params->dict(params, donated_invars=(False,) * len(logical_env_vals) + params['donated_invars'])
A:jax.interpreters.masking.(vals, shapes)->unzip2(((t.val, t.polymorphic_shape) for t in out_tracers))
A:jax.interpreters.masking.logical_env_vals->tuple((logical_env[k] for k in env_keys))
A:jax.interpreters.masking.(f, shapes_out)->mask_subtrace(f, self.master, shapes, padded_env)
A:jax.interpreters.masking.vals_out->call_primitive.bind(f, *logical_env_vals + vals, **params)
A:jax.interpreters.masking.unique_id->UniqueId(key)
A:jax.interpreters.masking.poly->poly.copy().copy()
A:jax.interpreters.masking.const_coeff->poly.copy().copy().pop(Mon({}), 0)
A:jax.interpreters.masking.((mon, linear_coeff),)->poly.copy().copy().items()
A:jax.interpreters.masking.((id, index),)->mon.items()
A:jax.interpreters.masking.(d, r)->divmod(d - const_coeff, linear_coeff)
A:jax.interpreters.masking.specs->tree_unflatten(spec_tree, specs)
A:jax.interpreters.masking.shapes->tree_unflatten(tree, shapes)
jax.interpreters.masking.MaskTrace(Trace)
jax.interpreters.masking.MaskTrace.lift(self,val)
jax.interpreters.masking.MaskTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.masking.MaskTrace.process_call(self,call_primitive,f,tracers,params)
jax.interpreters.masking.MaskTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.masking.MaskTrace.pure(self,val)
jax.interpreters.masking.MaskTrace.sublift(self,val)
jax.interpreters.masking.MaskTracer(self,trace,val,polymorphic_shape)
jax.interpreters.masking.MaskTracer.aval(self)
jax.interpreters.masking.MaskTracer.dtype(self)
jax.interpreters.masking.MaskTracer.full_lower(self)
jax.interpreters.masking.MaskTracer.is_pure(self)
jax.interpreters.masking.Mon(dict)
jax.interpreters.masking.Mon.__hash__(self)
jax.interpreters.masking.Mon.__lt__(self,other)
jax.interpreters.masking.Mon.__mul__(self,other)
jax.interpreters.masking.Mon.__str__(self)
jax.interpreters.masking.Mon.degree(self)
jax.interpreters.masking.MonomorphicDim(object)
jax.interpreters.masking.MonomorphicDim.__str__(self)
jax.interpreters.masking.Poly(self,coeffs)
jax.interpreters.masking.Poly.__add__(self,other)
jax.interpreters.masking.Poly.__divmod__(self,divisor)
jax.interpreters.masking.Poly.__eq__(self,other)
jax.interpreters.masking.Poly.__floordiv__(self,divisor)
jax.interpreters.masking.Poly.__ge__(self,other)
jax.interpreters.masking.Poly.__gt__(self,other)
jax.interpreters.masking.Poly.__hash__(self)
jax.interpreters.masking.Poly.__int__(self)
jax.interpreters.masking.Poly.__le__(self,other)
jax.interpreters.masking.Poly.__lt__(self,other)
jax.interpreters.masking.Poly.__mod__(self,divisor)
jax.interpreters.masking.Poly.__mul__(self,other)
jax.interpreters.masking.Poly.__ne__(self,other)
jax.interpreters.masking.Poly.__neg__(self)
jax.interpreters.masking.Poly.__radd__(self,other)
jax.interpreters.masking.Poly.__repr__(self)
jax.interpreters.masking.Poly.__rmul__(self,other)
jax.interpreters.masking.Poly.__rsub__(self,other)
jax.interpreters.masking.Poly.__str__(self)
jax.interpreters.masking.Poly.__sub__(self,other)
jax.interpreters.masking.Poly.evaluate(self,env)
jax.interpreters.masking.Poly.is_constant(self)
jax.interpreters.masking.S_(object)
jax.interpreters.masking.S_.__getitem__(self,idx)
jax.interpreters.masking.ShapeError(Exception)
jax.interpreters.masking.ShapeSpec(tuple)
jax.interpreters.masking.ShapeSpec.__str__(self)
jax.interpreters.masking.ShapeSyntaxError(Exception)
jax.interpreters.masking.UniqueId(self,name)
jax.interpreters.masking.UniqueId.__lt__(self,other)
jax.interpreters.masking.UniqueId.__repr__(self)
jax.interpreters.masking.UniqueIds(dict)
jax.interpreters.masking.UniqueIds.__missing__(self,key)
jax.interpreters.masking._ensure_poly(p)
jax.interpreters.masking._parse_dim(spec)
jax.interpreters.masking._parse_id(name)
jax.interpreters.masking._parse_lit(val_str)
jax.interpreters.masking._shape_spec_consistent(spec,expr)
jax.interpreters.masking.bind_shapes(polymorphic_shapes,padded_shapes)
jax.interpreters.masking.check_shapes(specs,spec_tree,shapes,tree,message_prefix='Output')
jax.interpreters.masking.defnaryop(prim)
jax.interpreters.masking.defvectorized(prim)
jax.interpreters.masking.eval_poly(poly,values_dict)
jax.interpreters.masking.eval_polymorphic_shape(shape,values_dict)
jax.interpreters.masking.extend_shape_envs(logical_env,padded_env)
jax.interpreters.masking.finalize_spec(polymorphic_shape,padded_shape)
jax.interpreters.masking.is_polymorphic(shape:Sequence[Union[int,'Poly']])
jax.interpreters.masking.is_tracing()
jax.interpreters.masking.mask_fun(fun,logical_env,padded_env,in_vals,polymorphic_shapes)
jax.interpreters.masking.mask_subtrace(master,shapes,padded_env,*in_vals)
jax.interpreters.masking.mul(coeff,mon)
jax.interpreters.masking.naryop_masking_rule(prim,padded_vals,logical_shapes)
jax.interpreters.masking.padded_shape_as_value(shape)
jax.interpreters.masking.parse_spec(spec='')
jax.interpreters.masking.pow(x,deg)
jax.interpreters.masking.remap_ids(names,shape_spec)
jax.interpreters.masking.shape_as_value(shape)
jax.interpreters.masking.vectorized_masking_rule(prim,padded_vals,logical_shapes,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/ad.py----------------------------------------
A:jax.interpreters.ad.(fun, aux)->jvp_subtrace_aux(fun)
A:jax.interpreters.ad.trace->JVPTrace(master, core.cur_sublevel())
A:jax.interpreters.ad.out_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.ans_tracers->map(trace.full_raise, ans)
A:jax.interpreters.ad.(out_primals, out_tangents)->unzip2(((t.primal, t.tangent) for t in ans_tracers))
A:jax.interpreters.ad.has_aux->kwargs.pop('has_aux', False)
A:jax.interpreters.ad.jvpfun->jvp(traceable)
A:jax.interpreters.ad.(jvpfun, aux)->jvp(traceable, has_aux=True)
A:jax.interpreters.ad.(_, in_tree)->tree_flatten(((primals, primals), {}))
A:jax.interpreters.ad.(jvpfun_flat, out_tree)->flatten_fun(jvpfun, in_tree)
A:jax.interpreters.ad.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(jvpfun_flat, in_pvals)
A:jax.interpreters.ad.(out_primals_pvals, out_tangents_pvals)->tree_unflatten(out_tree(), out_pvals)
A:jax.interpreters.ad.(_, out_primals_consts)->unzip2(out_primals_pvals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts)->linearize(traceable, *primals)
A:jax.interpreters.ad.(out_primals, pvals, jaxpr, consts, aux)->linearize(traceable, *primals, has_aux=True)
A:jax.interpreters.ad.cts->map(instantiate_zeros_aval, kwargs['out_avals'], cts)
A:jax.interpreters.ad.arg_cts->tree_unflatten(out_tree(), out_flat)
A:jax.interpreters.ad.vjp_->Partial(partial(unbound_vjp, pvals, jaxpr), consts)
A:jax.interpreters.ad.ct_aval->core.get_aval(ct_env[v])
A:jax.interpreters.ad.read_set->set(eqn.outvars)
A:jax.interpreters.ad.invals->map(read_primal, eqn.invars)
A:jax.interpreters.ad.cts_in->bwd.call_wrapped(*res, *cts_out)
A:jax.interpreters.ad.(cts_in,)->map(read_cotangent, eqn.outvars)
A:jax.interpreters.ad.(call_jaxpr, params)->core.extract_call_jaxpr(eqn.primitive, eqn.params)
A:jax.interpreters.ad.cts_out->map(instantiate_zeros_aval, avals_out, cts_out)
A:jax.interpreters.ad.cotangents_out->backward_pass(tangent_jaxpr.jaxpr, (), primals_in + residuals, cotangents_in)
A:jax.interpreters.ad.(primals_in, tangents_in)->unzip2(((t.primal, t.tangent) for t in tracers))
A:jax.interpreters.ad.jvp->primitive_jvps.get(primitive)
A:jax.interpreters.ad.(primal_out, tangent_out)->tree_unflatten(out_tree_def(), result)
A:jax.interpreters.ad.(primals, tangents)->split_list(tracers, [len(tracers) // 2])
A:jax.interpreters.ad.(nonzero_tangents, tangent_tree_def)->tree_flatten(tangents)
A:jax.interpreters.ad.(f_jvp, out_tree_def)->traceable(jvp_subtrace(f, self.master), len(primals), tangent_tree_def)
A:jax.interpreters.ad.params->dict(params, mapped_invars=(*mapped_invars, *mapped_tangents))
A:jax.interpreters.ad.update_params->call_transpose_param_updaters.get(primitive)
A:jax.interpreters.ad.result->call_primitive.bind(f_jvp, *primals, *nonzero_tangents, **new_params)
A:jax.interpreters.ad.(out, treedef)->tree_flatten((primals, tangents))
A:jax.interpreters.ad.primals_in->map(core.full_lower, primals_in)
A:jax.interpreters.ad.tangents_in->map(instantiate_zeros, tangents_in)
A:jax.interpreters.ad.outs->core.eval_jaxpr(trans_jaxpr, res, *cts)
A:jax.interpreters.ad.(primals_out, tangents_out)->split_list(outs, [len(outs) // 2])
A:jax.interpreters.ad.res_and_primals_out->fwd.call_wrapped(*map(core.full_lower, primals_in))
A:jax.interpreters.ad.(out_tree, res_tree)->out_trees()
A:jax.interpreters.ad.(res, primals_out)->split_list(res_and_primals_out, [res_tree.num_leaves])
A:jax.interpreters.ad.tangents_out->core.Primitive('{name}_lin'.format(name=name)).bind(*it.chain(res, tangents), trans_jaxpr=jaxpr, num_res=len(res), out_avals=out_avals)
A:jax.interpreters.ad.primal_aval->raise_to_shaped(get_aval(primal))
A:jax.interpreters.ad.tangent_aval->raise_to_shaped(get_aval(tangent))
A:jax.interpreters.ad.primitive_jvps[primitive]->partial(zero_jvp, primitive)
A:jax.interpreters.ad.primitive_transposes[primitive]->partial(linear_transpose2, transpose_rule)
A:jax.interpreters.ad.val_out->primitive.bind(*primals, **params)
A:jax.interpreters.ad.tangents->map(instantiate_zeros, tangents)
A:jax.interpreters.ad.primitive_transposes[prim]->partial(bilinear_transpose, lhs_rule, rhs_rule)
A:jax.interpreters.ad.defbilinear->partial(defbilinear_broadcasting, lambda g, x: g)
A:jax.interpreters.ad.out->rhs_rule(cotangent, x, **kwargs)
A:jax.interpreters.ad.r->primitive.bind(*primals, **params)
A:jax.interpreters.ad.new_tangents->tree_unflatten(in_tree_def, new_tangents)
A:jax.interpreters.ad.(out_flat, tree_def)->tree_flatten((primal_out, tangent_out))
A:jax.interpreters.ad.(all_args, in_tree_def)->tree_flatten(((), args, ct))
A:jax.interpreters.ad.fun->lu.hashable_partial(lu.wrap_init(backward_pass), call_jaxpr)
A:jax.interpreters.ad.(fun, out_tree)->flatten_fun_nokwargs(fun, in_tree_def)
A:jax.interpreters.ad.new_params->update_params(new_params, map(is_undefined_primal, args), [type(x) is not Zero for x in ct])
A:jax.interpreters.ad.out_flat->primitive.bind(fun, *all_args, **new_params)
A:jax.interpreters.ad.primitive_transposes[core.call_p]->partial(call_transpose, call_p)
A:jax.interpreters.ad.typed_call_jaxpr->core.TypedJaxpr(call_jaxpr, [], in_avals, cotangent_in_avals)
A:jax.interpreters.ad.unknowns->map(is_undefined_primal, primals_in)
A:jax.interpreters.ad.(primal_jaxpr, tangent_jaxpr, out_unknowns)->pe.partial_eval_jaxpr(typed_call_jaxpr, unknowns=unknowns, instantiate=True, trace_type=None)
A:jax.interpreters.ad.(flat_args, in_tree_def)->tree_flatten((primals_in, cotangents_in))
A:jax.interpreters.ad.(flat_do_transpose, out_tree)->flatten_fun_nokwargs(lu.wrap_init(do_transpose), in_tree_def)
A:jax.interpreters.ad.flat_cotangents_out->pe.remat_call_p.bind(flat_do_transpose, *flat_args, **params)
A:jax.interpreters.ad.f->lu.wrap_init(core.jaxpr_as_fun(jaxpr))
A:jax.interpreters.ad.(f_jvp, out_nonzeros)->f_jvp_traceable(jvp(f, instantiate=instantiate), nonzeros)
A:jax.interpreters.ad.avals_in->list(it.chain(jaxpr.in_avals, tangent_avals))
A:jax.interpreters.ad.(jaxpr_out, pvals_out, literals_out)->pe.trace_to_jaxpr(f_jvp, pvals, instantiate=True)
A:jax.interpreters.ad.(avals_out, _)->unzip2(pvals_out)
A:jax.interpreters.ad.jaxpr_out->core.TypedJaxpr(jaxpr_out, literals_out, avals_in, avals_out)
A:jax.interpreters.ad.num_primals->len(nonzeros)
A:jax.interpreters.ad.primals->list(primals_and_nztangents[:num_primals])
A:jax.interpreters.ad.nonzero_tangents->iter(primals_and_nztangents[num_primals:])
A:jax.interpreters.ad.new_invars->_perm(primals_in, tangents_in, jaxpr.jaxpr.invars)
A:jax.interpreters.ad.new_outvars->_perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)
A:jax.interpreters.ad.new_jaxpr->core.Jaxpr(jaxpr.jaxpr.constvars, new_invars, new_outvars, jaxpr.jaxpr.eqns)
A:jax.interpreters.ad.new_in_avals->_perm(primals_in, tangents_in, jaxpr.in_avals)
A:jax.interpreters.ad.new_out_avals->_perm(primals_out, tangents_out, jaxpr.out_avals)
A:jax.interpreters.ad.new_typed_jaxpr->core.TypedJaxpr(new_jaxpr, jaxpr.literals, new_in_avals, new_out_avals)
A:jax.interpreters.ad.n->sum(primal_counts)
A:jax.interpreters.ad.primal_groups->split_list(primals, primal_counts[:-1])
A:jax.interpreters.ad.tangent_groups->split_list(tangents, tangent_counts[:-1])
A:jax.interpreters.ad.custom_lin_p->core.Primitive('custom_lin')
A:jax.interpreters.ad.(res, _)->split_list(args, [num_res])
A:jax.interpreters.ad.(cts_in_flat, _)->tree_flatten(cts_in)
A:jax.interpreters.ad.ts->map(instantiate_zeros, ts)
A:jax.interpreters.ad.primals_and_tangents->core.Primitive('{name}_jvp'.format(name=name)).bind(*it.chain(xs, ts), **params)
A:jax.interpreters.ad.fun_jvp_p->core.Primitive('{name}_jvp'.format(name=name))
A:jax.interpreters.ad.(primals_out, vjp_py)->custom_vjp(*primals, **params)
A:jax.interpreters.ad.(jaxpr, _, res)->pe.trace_to_jaxpr(lu.wrap_init(vjp_py), ct_pvals, instantiate=True)
A:jax.interpreters.ad.fun_lin_p->core.Primitive('{name}_lin'.format(name=name))
A:jax.interpreters.ad.ans->prim.bind(*primals)
A:jax.interpreters.ad.(jaxpr_out, avals_out, literals_out)->pe.trace_to_jaxpr_dynamic(f_jvp, avals_in)
jax.interpreters.ad.JVPTrace(Trace)
jax.interpreters.ad.JVPTrace.join(self,xt,yt)
jax.interpreters.ad.JVPTrace.lift(self,val)
jax.interpreters.ad.JVPTrace.post_process_call(self,call_primitive,out_tracers,params)
jax.interpreters.ad.JVPTrace.process_call(self,call_primitive,f:lu.WrappedFun,tracers,params)
jax.interpreters.ad.JVPTrace.process_custom_jvp_call(self,_,__,f_jvp,tracers)
jax.interpreters.ad.JVPTrace.process_custom_vjp_call(self,_,__,fwd,bwd,tracers,*,out_trees)
jax.interpreters.ad.JVPTrace.process_primitive(self,primitive,tracers,params)
jax.interpreters.ad.JVPTrace.pure(self,val)
jax.interpreters.ad.JVPTrace.sublift(self,val)
jax.interpreters.ad.JVPTracer(self,trace,primal,tangent)
jax.interpreters.ad.JVPTracer.aval(self)
jax.interpreters.ad.JVPTracer.full_lower(self)
jax.interpreters.ad.UndefinedPrimal(self,aval)
jax.interpreters.ad.UndefinedPrimal.__repr__(self)
jax.interpreters.ad._custom_lin_transpose(cts_out,*invals,num_res,bwd,avals_out)
jax.interpreters.ad._interleave(xs,ys)
jax.interpreters.ad._perm(primal_counts,tangent_counts,lst)
jax.interpreters.ad._primal_tangent_shapes_match(primal,tangent)
jax.interpreters.ad._raise_custom_vjp_error_on_jvp(*_,**__)
jax.interpreters.ad.add_tangents(x,y)
jax.interpreters.ad.backward_pass(jaxpr:core.Jaxpr,consts,primals_in,cotangents_in)
jax.interpreters.ad.bilinear_transpose(lhs_rule,rhs_rule,cotangent,x,y,**kwargs)
jax.interpreters.ad.call_transpose(primitive,params,call_jaxpr,args,ct,_)
jax.interpreters.ad.defbilinear_broadcasting(bcast,prim,lhs_rule,rhs_rule)
jax.interpreters.ad.defjvp(primitive,*jvprules)
jax.interpreters.ad.defjvp2(primitive,*jvprules)
jax.interpreters.ad.defjvp_zero(primitive)
jax.interpreters.ad.deflinear(primitive,transpose_rule)
jax.interpreters.ad.deflinear2(primitive,transpose_rule)
jax.interpreters.ad.defvjp(prim,*vjps)
jax.interpreters.ad.defvjp2(prim,*vjps)
jax.interpreters.ad.defvjp_all(prim,custom_vjp)
jax.interpreters.ad.f_jvp_traceable(nonzeros,*primals_and_nztangents)
jax.interpreters.ad.get_primitive_transpose(p)
jax.interpreters.ad.identity(x)
jax.interpreters.ad.ignore_consts(ct,pval)
jax.interpreters.ad.instantiate_zeros(tangent)
jax.interpreters.ad.instantiate_zeros_aval(aval,tangent)
jax.interpreters.ad.is_undefined_primal(x)
jax.interpreters.ad.jvp(fun:lu.WrappedFun,has_aux=False,instantiate=True)->Any
jax.interpreters.ad.jvp_jaxpr(jaxpr,nonzeros,instantiate)
jax.interpreters.ad.jvp_subtrace(master,primals,tangents)
jax.interpreters.ad.jvp_subtrace_aux(master,primals,tangents)
jax.interpreters.ad.jvpfun(instantiate,primals,tangents)
jax.interpreters.ad.linear_jvp(primitive,primals,tangents,**params)
jax.interpreters.ad.linear_transpose(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linear_transpose2(transpose_rule,cotangent,*args,**kwargs)
jax.interpreters.ad.linearize(traceable,*primals,**kwargs)
jax.interpreters.ad.map_transpose(primitive,params,call_jaxpr,args,ct,_)
jax.interpreters.ad.omnistaging_enabler()->None
jax.interpreters.ad.rearrange_binders(jaxpr:core.TypedJaxpr,primals_in,tangents_in,primals_out,tangents_out)
jax.interpreters.ad.remat_transpose(params,call_jaxpr,primals_in,cotangents_in,cotangent_in_avals)
jax.interpreters.ad.standard_jvp(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.standard_jvp2(jvprules,primitive,primals,tangents,**params)
jax.interpreters.ad.traceable(num_primals,in_tree_def,*primals_and_tangents)
jax.interpreters.ad.unpair_pval(pval)
jax.interpreters.ad.vjp(traceable,primals,has_aux=False)
jax.interpreters.ad.zero_jvp(primitive,primals,tangents,**params)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/invertible_ad.py----------------------------------------
A:jax.interpreters.invertible_ad.invertible_call_p->jax.core.CallPrimitive('invertible_call')
A:jax.interpreters.invertible_ad.(out_tracers_known, out_tracers_unknown)->_partition_knowns(out_tracers, uks)
A:jax.interpreters.invertible_ad.new_jaxpr->_append_invars(params['call_jaxpr'], tuple(out_known_avals))
A:jax.interpreters.invertible_ad.eqn->new_eqn_recipe(new_in_tracers, new_out_tracers, invertible_call_p, dict(params, call_jaxpr=new_jaxpr), source_info_util.current())
A:jax.interpreters.invertible_ad.pe.call_partial_eval_rules[invertible_call_p]->partial(pe._remat_partial_eval, _invertible_call_make_output_tracers)
A:jax.interpreters.invertible_ad.newvar->jax.core.gensym([jaxpr])
A:jax.interpreters.invertible_ad.is_tangent->map(ad.is_undefined_primal, args)
A:jax.interpreters.invertible_ad.first_tangent->map(ad.is_undefined_primal, args).index(True)
A:jax.interpreters.invertible_ad.args->custom_derivatives._resolve_kwargs(self.fun, args, kwargs)
A:jax.interpreters.invertible_ad.(args_flat, in_tree)->tree_flatten(args)
A:jax.interpreters.invertible_ad.(flat_fun, out_tree)->flatten_fun_nokwargs(fun, in_tree)
A:jax.interpreters.invertible_ad.flat_ivjp->_flatten_ivjp(ivjp, in_tree, out_tree)
A:jax.interpreters.invertible_ad.out_flat->_custom_ivjp(flat_fun, flat_ivjp, args_flat)
A:jax.interpreters.invertible_ad.out_tree->out_tree()
A:jax.interpreters.invertible_ad.arg_leaves->split_list(args, [num_inputs, num_outputs])
A:jax.interpreters.invertible_ad.py_args->zip_with(tree_unflatten, [in_tree, out_tree, out_tree], arg_leaves)
A:jax.interpreters.invertible_ad.fun_jaxpr->custom_derivatives._initial_style_jaxpr(fun, in_avals)
A:jax.interpreters.invertible_ad.ivjp_jaxpr->jax.core.TypedJaxpr(ivjp_jaxpr, [], in_avals, out_avals)
A:jax.interpreters.invertible_ad.custom_ivjp_p->jax.core.Primitive('custom_ivjp')
A:jax.interpreters.invertible_ad.primals_out->map(read_primal, eqn.outvars)
A:jax.interpreters.invertible_ad.fun->jax.core.jaxpr_as_fun(fun_jaxpr)
A:jax.interpreters.invertible_ad.(_, tangents_out)->ad.jvp(lu.wrap_init(fun)).call_wrapped(primals, tangents)
A:jax.interpreters.invertible_ad.(primal_invars, tangent_invars)->split(jaxpr.invars[:-len(primals_out)], parts=2)
A:jax.interpreters.invertible_ad.(primal_outvars, tangent_outvars)->split(jaxpr.outvars, parts=2)
A:jax.interpreters.invertible_ad.tangent_vars->set(tangent_invars)
A:jax.interpreters.invertible_ad.primals_in->map(lambda p, rp, unknown: rp if unknown else p, primals_in, rec_primals_in, unknown_primals)
A:jax.interpreters.invertible_ad.cts_in->map(read_cotangent, eqn.outvars)
A:jax.interpreters.invertible_ad.should_invert->any((type(primal) is not ad.UndefinedPrimal for primal in primals_out))
A:jax.interpreters.invertible_ad.should_vjp->any((type(ct) is not ad.Zero for ct in cts_in))
A:jax.interpreters.invertible_ad.complete_ivjp->jax.linear_util.wrap_init(partial(synthesize_ivjp, eqn, map(ad.is_undefined_primal, primals_in)))
A:jax.interpreters.invertible_ad.(_, in_tree)->tree_flatten(tuple((map(abstract, x) for x in (primals_in, primals_out, primals_out))))
A:jax.interpreters.invertible_ad.(complete_ivjp_flat, _)->flatten_fun_nokwargs(complete_ivjp, in_tree)
A:jax.interpreters.invertible_ad.in_avals->map(abstract, primals_in + primals_out + primals_out)
A:jax.interpreters.invertible_ad.(ivjp_jaxpr, out_pvals, _)->pe.trace_to_jaxpr(complete_ivjp_flat, map(PartialVal.unknown, in_avals), instantiate=True, stage_out=False)
A:jax.interpreters.invertible_ad.out_avals->map(raise_to_shaped, unzip2(out_pvals)[0])
A:jax.interpreters.invertible_ad.num_inputs->len(eqn.invars)
A:jax.interpreters.invertible_ad.(jaxpr_known, jaxpr_unknown, out_unknowns)->pe.partial_eval_jaxpr(ivjp_jaxpr, unknowns, instantiate=False, trace_type=None)
A:jax.interpreters.invertible_ad.(unknown_rec_primals_in, unknown_cotangents)->split_list(out_unknowns, [num_inputs])
A:jax.interpreters.invertible_ad.ivjp->jax.core.jaxpr_as_fun(jaxpr_known)
A:jax.interpreters.invertible_ad.(rec_primals_in, cts_out)->split_list(ivjp(*primals_in, *primals_out, *cts_in), [num_inputs])
A:jax.interpreters.invertible_ad.rec_primals_in->get_primitive_inverse(eqn.primitive)(primals_out, *primals_in)
A:jax.interpreters.invertible_ad.eqn_jaxpr->Jaxpr([], variable_invars, eqn.outvars, [eqn])
A:jax.interpreters.invertible_ad.(_, eqn_vjp)->jax.vjp(eqn_callable, variable_primals_in)
A:jax.interpreters.invertible_ad.(cts_out,)->eqn_vjp(cts_in)
jax.interpreters.invertible_ad._append_invars(jaxpr,avals)
jax.interpreters.invertible_ad._custom_ivjp(fun,ivjp,args)
jax.interpreters.invertible_ad._custom_ivjp_impl(*args,fun_jaxpr,**_)
jax.interpreters.invertible_ad._custom_ivjp_jvp(primals,tangents,*,fun_jaxpr,ivjp_jaxpr)
jax.interpreters.invertible_ad._flatten_ivjp(in_tree,out_tree,*args)
jax.interpreters.invertible_ad._invertible_call_make_output_tracers(trace,in_tracers,out_tracers,params)
jax.interpreters.invertible_ad._invertible_call_transpose(params,call_jaxpr,args,ct,_)
jax.interpreters.invertible_ad.custom_ivjp(self,fun)
jax.interpreters.invertible_ad.custom_ivjp.defivjp(self,ivjp)
jax.interpreters.invertible_ad.definverse(primitive,inverse_rule)
jax.interpreters.invertible_ad.get_primitive_inverse(p)
jax.interpreters.invertible_ad.inv_backward_pass(jaxpr:core.Jaxpr,consts,primals_in,primals_out,cotangents_in)
jax.interpreters.invertible_ad.split(l,parts)
jax.interpreters.invertible_ad.synthesize_ivjp(eqn,unknown_primals,primals_in,primals_out,cts_in)
jax.interpreters.invertible_ad.zero_vars(vs)
jax.interpreters.invertible_ad.zip_with(fun,*args)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/sharded_jit.py----------------------------------------
A:jax.interpreters.sharded_jit.result_to_populate->ResultToPopulate()
A:jax.interpreters.sharded_jit.nouts->len(out_avals)
A:jax.interpreters.sharded_jit.spec->pxla.partitioned_sharding_spec(npart, parts, aval)
A:jax.interpreters.sharded_jit.indices->pxla.spec_to_indices(aval.shape, spec)
A:jax.interpreters.sharded_jit.(jaxpr, out_avals, consts)->pe.trace_to_jaxpr_final(fun, abstract_args)
A:jax.interpreters.sharded_jit.(jaxpr, out_pvals, consts)->pe.trace_to_jaxpr(fun, in_pvals, instantiate=False, bottom=True)
A:jax.interpreters.sharded_jit.num_partitions->pxla.reconcile_num_partitions(jaxpr, num_partitions)
A:jax.interpreters.sharded_jit.out_parts->out_parts_thunk()
A:jax.interpreters.sharded_jit.c->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__))
A:jax.interpreters.sharded_jit.xla_consts->_map(partial(xb.constant, c), consts)
A:jax.interpreters.sharded_jit.xla_args->_xla_sharded_args(c, abstract_args, in_parts)
A:jax.interpreters.sharded_jit.axis_env->xla.AxisEnv(nrep, (), (), None)
A:jax.interpreters.sharded_jit.out_nodes->xla.jaxpr_subcomp(subc, call_jaxpr, backend, axis_env, (), extend_name_stack(name_stack, wrap_name(name, 'sharded_jit')), *args)
A:jax.interpreters.sharded_jit.out_tuple->lib.xla_bridge.with_sharding(c, out_parts, xops.Tuple, c, out_nodes)
A:jax.interpreters.sharded_jit.built->lib.xla_bridge.make_computation_builder('spjit_{}'.format(fun.__name__)).Build(out_tuple)
A:jax.interpreters.sharded_jit.device_assignment->numpy.reshape(device_assignment, (-1, num_partitions))
A:jax.interpreters.sharded_jit.compiled->lib.xla_bridge.get_backend().compile(built, compile_options=xb.get_compile_options(nrep, num_partitions, device_assignment))
A:jax.interpreters.sharded_jit.handle_args->partial(pxla.shard_args, compiled.local_devices(), input_indices)
A:jax.interpreters.sharded_jit.handle_outs->_pvals_to_results_handler(nrep, num_partitions, out_parts, out_pvals)
A:jax.interpreters.sharded_jit.subc->subc.build(xops.Tuple(subc, out_nodes)).build(xops.Tuple(subc, out_nodes))
A:jax.interpreters.sharded_jit.arg->lib.xla_bridge.parameter(subc, i, c.GetShape(n))
A:jax.interpreters.sharded_jit.input_bufs->in_handler(args)
A:jax.interpreters.sharded_jit.out_bufs->lib.xla_bridge.get_backend().compile(built, compile_options=xb.get_compile_options(nrep, num_partitions, device_assignment)).execute_on_local_devices(list(input_bufs))
A:jax.interpreters.sharded_jit.param->lib.xla_bridge.with_sharding(c, sharding, xb.parameter, c, i, xla.aval_to_xla_shape(aval))
A:jax.interpreters.sharded_jit.compiled_fun->_sharded_callable(fun, num_partitions, in_parts, out_parts_thunk, name, *map(xla.abstractify, args))
A:jax.interpreters.sharded_jit.sharded_call_p->core.CallPrimitive('sharded_call')
A:jax.interpreters.sharded_jit.num_parts->pxla.get_num_partitions(in_parts, out_parts)
A:jax.interpreters.sharded_jit.f->lu.wrap_init(fun)
A:jax.interpreters.sharded_jit.(args_flat, in_tree)->tree_flatten((args, kwargs))
A:jax.interpreters.sharded_jit.in_parts_flat->tuple(flatten_axes('sharded_jit in_parts', in_tree.children()[0], in_parts))
A:jax.interpreters.sharded_jit.(flat_fun, out_tree)->flatten_fun(f, in_tree)
A:jax.interpreters.sharded_jit.out->sharded_call(flat_fun, *args_flat, num_partitions=num_parts, in_parts=in_parts_flat, out_parts_thunk=out_parts_thunk, name=flat_fun.__name__)
A:jax.interpreters.sharded_jit.sharding_constraint_p->core.Primitive('sharding_constraint')
jax.interpreters.sharded_jit.PartitionSpec(cls,*partitions)
jax.interpreters.sharded_jit.PartitionSpec.__repr__(self)
jax.interpreters.sharded_jit.ResultToPopulate
jax.interpreters.sharded_jit._execute_spatially_partitioned(compiled,in_handler,out_handler,*args)
jax.interpreters.sharded_jit._map(f,*xs)
jax.interpreters.sharded_jit._pval_to_result_handler(npart,parts,pval)
jax.interpreters.sharded_jit._pvals_to_results_handler(nrep,npart,partitions,out_pvals)
jax.interpreters.sharded_jit._sharded_call_impl(fun,*args,num_partitions,in_parts,out_parts_thunk,name)
jax.interpreters.sharded_jit._sharded_callable(fun:lu.WrappedFun,num_partitions:Optional[int],in_parts:Tuple[pxla.PartitionsOrReplicated,...],out_parts_thunk:Callable[[],Tuple[pxla.PartitionsOrReplicated,...]],name:str,*abstract_args)
jax.interpreters.sharded_jit._sharded_jit_translation_rule(c,axis_env,in_nodes,name_stack,in_parts,out_parts_thunk,num_partitions,backend,name,call_jaxpr)
jax.interpreters.sharded_jit._sharding_constraint_impl(x,partitions)
jax.interpreters.sharded_jit._sharding_constraint_translation_rule(c,x_node,partitions)
jax.interpreters.sharded_jit._xla_sharded_args(c,avals,in_parts)
jax.interpreters.sharded_jit.omnistaging_enabler()->None
jax.interpreters.sharded_jit.sharded_jit(fun:Callable,in_parts,out_parts,num_partitions:int=None)
jax.interpreters.sharded_jit.with_sharding_constraint(x,partitions:Optional[PartitionSpec])


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/interpreters/xla.py----------------------------------------
A:jax.interpreters.xla._scalar_types->dtypes.python_scalar_dtypes.keys()
A:jax.interpreters.xla.backend->lib.xla_bridge.get_device_backend(device)
A:jax.interpreters.xla.x->_copy_device_array_to_device(x, device)
A:jax.interpreters.xla.typ->type(x)
A:jax.interpreters.xla.handler->aval_to_result_handler(device, a)
A:jax.interpreters.xla.aval_fn->pytype_aval_mappings.get(typ)
A:jax.interpreters.xla.aval->abstractify(x)
A:jax.interpreters.xla.compiled_fun->_xla_callable(fun, device, backend, name, donated_invars, *unsafe_map(arg_spec, args))
A:jax.interpreters.xla.(avals, arg_devices)->unzip2(arg_specs)
A:jax.interpreters.xla.device->_device_from_arg_devices([device])
A:jax.interpreters.xla.aval_out->prim.abstract_eval(*avals, **params)
A:jax.interpreters.xla.handle_result->aval_to_result_handler(device, aval_out)
A:jax.interpreters.xla.handlers->map(partial(aval_to_result_handler, device), aval_out)
A:jax.interpreters.xla.nreps->jaxpr_replicas(jaxpr)
A:jax.interpreters.xla.built_c->lib.xla_bridge.make_computation_builder('lazy_force').build(xla_out)
A:jax.interpreters.xla.options->lib.xla_bridge.get_compile_options(num_replicas=1, num_partitions=1, device_assignment=device and (device.id,))
A:jax.interpreters.xla.compiled->lib.xla_bridge.get_device_backend(device).compile(built_c, compile_options=options)
A:jax.interpreters.xla.c->lib.xla_bridge.make_computation_builder('lazy_force')
A:jax.interpreters.xla.xla_args->_xla_callable_args(c, abstract_args, tuple_args)
A:jax.interpreters.xla.ans->rule(c, axis_env, in_nodes, name_stack, backend=backend, **new_params)
A:jax.interpreters.xla.axis_env->AxisEnv(1, (), (), None)
A:jax.interpreters.xla.(device,)->lib.xla_bridge.get_device_backend(device).compile(built_c, compile_options=options).local_devices()
A:jax.interpreters.xla.out_bufs->lib.xla_bridge.get_device_backend(device).compile(built_c, compile_options=options).execute(input_bufs)
A:jax.interpreters.xla.frame->source_info_util.user_frame(eqn.source_info)
A:jax.interpreters.xla.in_nodes->map(read, eqn.invars)
A:jax.interpreters.xla.new_params->check_backend_params(eqn.params, backend)
A:jax.interpreters.xla.num_elements->len(c.get_shape(ans).tuple_shapes())
A:jax.interpreters.xla.inner_backend->params.get('backend', None)
A:jax.interpreters.xla.AxisEnv->namedtuple('AxisEnv', ['nreps', 'names', 'sizes', 'devices'])
A:jax.interpreters.xla.mesh_axes->tuple(unsafe_map(partial(axis_read, axis_env), name))
A:jax.interpreters.xla.(trailing_size, ragged)->divmod(nrep, prod(mesh_spec))
A:jax.interpreters.xla.iota->numpy.arange(prod(full_spec)).reshape(full_spec)
A:jax.interpreters.xla.groups->numpy.reshape(np.moveaxis(iota, mesh_axes, np.arange(len(mesh_axes))), (prod(np.take(full_spec, mesh_axes)), -1))
A:jax.interpreters.xla.call_jaxpr->eqn.params.get('call_jaxpr')
A:jax.interpreters.xla.(abstract_args, arg_devices)->unzip2(arg_specs)
A:jax.interpreters.xla.(jaxpr, out_avals, consts)->pe.trace_to_jaxpr_final(fun, abstract_args)
A:jax.interpreters.xla.(jaxpr, pvals, consts)->pe.trace_to_jaxpr(fun, pvals, instantiate=False, stage_out=True, bottom=True)
A:jax.interpreters.xla.jaxpr->apply_outfeed_rewriter(jaxpr)
A:jax.interpreters.xla.result_handlers->tuple(map(partial(_pval_to_result_handler, device), pvals))
A:jax.interpreters.xla.xla_consts->_xla_consts(c, consts)
A:jax.interpreters.xla.out_nodes->jaxpr_subcomp(subc, call_jaxpr, backend, axis_env, (), extend_name_stack(name_stack, 'core_call'), *args)
A:jax.interpreters.xla.out_tuple->xops.Tuple(c, out_nodes)
A:jax.interpreters.xla.donated_invars->set_up_aliases(c, xla_args, out_tuple, donated_invars, tuple_args)
A:jax.interpreters.xla.built->lib.xla_bridge.make_computation_builder('lazy_force').build(_make_unit(c))
A:jax.interpreters.xla.donations->defaultdict(deque)
A:jax.interpreters.xla.out_donated_args->list(donated_args)
A:jax.interpreters.xla.(param_number, param_index, arg_index)->donations[key].popleft()
A:jax.interpreters.xla._replicated_param->object()
A:jax.interpreters.xla.tuple_shape->lib.xla_client.Shape.tuple_shape([aval_to_xla_shape(a) for a in avals if a is not abstract_token])
A:jax.interpreters.xla.tuple_param->_xla_param(c, 0, tuple_shape, replicated, tuple_parts)
A:jax.interpreters.xla.xla_inputs->iter(xla_destructure(c, tuple_param))
A:jax.interpreters.xla.make_param->partial(xb.parameter, builder, param_num, xla_shape, replicated=replicated)
A:jax.interpreters.xla.(out,)->lib.xla_bridge.get_device_backend(device).compile(built_c, compile_options=options).local_devices()
A:jax.interpreters.xla.xla_call_p->core.CallPrimitive('xla_call')
A:jax.interpreters.xla.subc->subc.Build(xops.Tuple(subc, out_nodes)).Build(xops.Tuple(subc, out_nodes))
A:jax.interpreters.xla.ad.primitive_transposes[xla_call_p]->partial(ad.call_transpose, xla_call_p)
A:jax.interpreters.xla.shape->lib.xla_bridge.make_computation_builder('lazy_force').get_shape(x)
A:jax.interpreters.xla.zero->lib.xla_bridge.constant(dummy_subc, np.array(0, dtype=dtype))
A:jax.interpreters.xla.wrapped_fun->_tuple_output(wrapped_fun)
A:jax.interpreters.xla.(jaxpr, _, consts)->pe.trace_to_jaxpr(lu.wrap_init(fun, params), pvals, instantiate=True, stage_out=True)
A:jax.interpreters.xla.outs->jaxpr_subcomp(c, jaxpr, backend, axis_env, xla_consts, name_stack, *xla_args)
A:jax.interpreters.xla.token->Token()
A:jax.interpreters.xla._forward_to_value->partial(_forward_method, '_value')
A:jax.interpreters.xla.self._npy_value->_force(self).device_buffer.to_py()
A:jax.interpreters.xla.prefix->'{}('.format(self.__class__.__name__)
A:jax.interpreters.xla.s->numpy.array2string(self._value, prefix=prefix, suffix=',', separator=', ', max_line_width=line_width)
A:jax.interpreters.xla.dtype_str->'dtype={})'.format(self.dtype.name)
A:jax.interpreters.xla.__str__->partialmethod(_forward_to_value, str)
A:jax.interpreters.xla.__bool____nonzero__->partialmethod(_forward_to_value, bool)
A:jax.interpreters.xla.__hex__->partialmethod(_forward_to_value, hex)
A:jax.interpreters.xla.__oct__->partialmethod(_forward_to_value, oct)
A:jax.interpreters.xla.__index__->partialmethod(_forward_to_value, op.index)
A:jax.interpreters.xla.__reduce__->partialmethod(_forward_to_value, op.methodcaller('__reduce__'))
A:jax.interpreters.xla.deleted_buffer->DeletedBuffer()
A:jax.interpreters.xla.pytype_aval_mappings[DeviceArray]->operator.attrgetter('aval')
A:jax.interpreters.xla.base_val->lib.xla_bridge.constant(c, val.device_buffer.to_py())
A:jax.interpreters.xla.moved_buf->lib.xla_bridge.get_device_backend(device).buffer_from_pyval(x.device_buffer.to_py(), device)
A:jax.interpreters.xla.force_fun->_lazy_force_computation(x.aval, device, x._lazy_expr)
A:jax.interpreters.xla.result->force_fun(x)
A:jax.interpreters.xla.param->lib.xla_bridge.parameter(c, 0, xc.Shape.array_shape(aval.dtype, param_shape))
A:jax.interpreters.xla.xla_out->lazy.stage_lexpr(c, lexpr, param)
A:jax.interpreters.xla.a->abstractify(x)
A:jax.interpreters.xla.device_put_p->core.Primitive('device_put')
A:jax.interpreters.xla.rng->xops.RngUniform(xb.constant(c, np.array(0, dtype=np.float32)), xb.constant(c, np.array(1, dtype=np.float32)), xc.Shape.array_shape(xc.PrimitiveType.F32, []))
A:jax.interpreters.xla.pred->xops.Lt(rng, xb.constant(c, np.array(2, dtype=np.float32)))
A:jax.interpreters.xla.true_op->xops.Tuple(c, in_nodes)
A:jax.interpreters.xla.remat_subc->remat_subc.build(xops.Tuple(remat_subc, out_nodes)).build(xops.Tuple(remat_subc, out_nodes))
A:jax.interpreters.xla.input_op->lib.xla_bridge.parameter(remat_subc, 0, c.get_shape(true_op), replicated=[])
A:jax.interpreters.xla.dummy_subc->dummy_subc.build(xops.Tuple(dummy_subc, out_nodes)).build(xops.Tuple(dummy_subc, out_nodes))
A:jax.interpreters.xla.div->lib.xla_bridge.constant(c, np.array(axis_env.nreps // prod(axis_env.sizes), dtype=np.uint32))
A:jax.interpreters.xla.mod->lib.xla_bridge.constant(c, np.array(axis_env.sizes[-1], dtype=np.uint32))
A:jax.interpreters.xla.unsigned_index->xops.Rem(xops.Div(xops.ReplicaId(c), div), mod)
jax.interpreters.xla.DeletedBuffer(object)
jax.interpreters.xla.DeviceArray(self,aval:core.ShapedArray,device:Optional[Device],lazy_expr:lazy.LazyExpr,device_buffer:PyLocalBuffer)
jax.interpreters.xla.DeviceArray.__array__(self,dtype=None,context=None)
jax.interpreters.xla.DeviceArray.__complex__(self)
jax.interpreters.xla.DeviceArray.__cuda_array_interface__(self)
jax.interpreters.xla.DeviceArray.__eq__(self,other)
jax.interpreters.xla.DeviceArray.__float__(self)
jax.interpreters.xla.DeviceArray.__format__(self,format_spec)
jax.interpreters.xla.DeviceArray.__getitem__(self,i)
jax.interpreters.xla.DeviceArray.__hash__(self)
jax.interpreters.xla.DeviceArray.__int__(self)
jax.interpreters.xla.DeviceArray.__iter__(self)
jax.interpreters.xla.DeviceArray.__len__(self)
jax.interpreters.xla.DeviceArray.__repr__(self)
jax.interpreters.xla.DeviceArray.__reversed__(self)
jax.interpreters.xla.DeviceArray._value(self)
jax.interpreters.xla.DeviceArray.copy(self)
jax.interpreters.xla.DeviceArray.copy_to_host_async(self)
jax.interpreters.xla.DeviceArray.delete(self)
jax.interpreters.xla.DeviceArray.dtype(self)
jax.interpreters.xla.DeviceArray.item(self)
jax.interpreters.xla.DeviceArray.ndim(self)
jax.interpreters.xla.DeviceArray.shape(self)
jax.interpreters.xla.DeviceArray.size(self)
jax.interpreters.xla.DeviceArray.tobytes(self,order='C')
jax.interpreters.xla.DeviceArray.tolist(self)
jax.interpreters.xla.DeviceConstant(self,device=None)
jax.interpreters.xla.DeviceConstant.device(self)
jax.interpreters.xla.DeviceConstant.to_py(self)
jax.interpreters.xla.DeviceValue(self,aval,device_buffer)
jax.interpreters.xla.DeviceValue._check_if_deleted(self)
jax.interpreters.xla.DeviceValue.block_until_ready(self)
jax.interpreters.xla.Token(object)
jax.interpreters.xla._array_aval_from_xla_shape(xla_shape)
jax.interpreters.xla._axis_groups(nrep,mesh_spec,mesh_axes)
jax.interpreters.xla._backend_compile(backend,built_c,options)
jax.interpreters.xla._call_translation_rule(c,axis_env,in_nodes,name_stack,*,backend,call_jaxpr)
jax.interpreters.xla._canonicalize_ndarray_dtype(x)
jax.interpreters.xla._canonicalize_python_scalar_dtype(typ,x)
jax.interpreters.xla._check_nans(name,xla_shape,buf)
jax.interpreters.xla._copy_device_array_to_device(x:DeviceArray,device:Optional[xc.Device])->DeviceArray
jax.interpreters.xla._device_array_constant_handler(c,val,canonicalize_types=True)
jax.interpreters.xla._device_from_arg_devices(devices:Sequence[Optional[Device]])->Optional[Device]
jax.interpreters.xla._device_put_array(x,device:Optional[Device])
jax.interpreters.xla._device_put_device_array(x:DeviceArray,device:Optional[Device])
jax.interpreters.xla._device_put_impl(x,device:Optional[Device]=None)
jax.interpreters.xla._device_put_scalar(x,device)
jax.interpreters.xla._device_put_unit(_,device)
jax.interpreters.xla._execute_compiled(compiled:XlaExecutable,handlers,*args)
jax.interpreters.xla._execute_compiled_primitive(prim,compiled,result_handler,*args)
jax.interpreters.xla._execute_replicated(compiled:XlaExecutable,handlers,*args)
jax.interpreters.xla._execute_replicated_primitive(prim,compiled,result_handler,*args)
jax.interpreters.xla._execute_trivial(jaxpr,device:Optional[Device],consts,handlers,*args)
jax.interpreters.xla._force(x:DeviceArray)->DeviceArray
jax.interpreters.xla._forward_method(attrname,self,fun,*args)
jax.interpreters.xla._get_device(device,backend)
jax.interpreters.xla._lazy_force_computation(aval:core.ShapedArray,device:Device,lexpr:lazy.LazyExpr)->Callable[[DeviceArray], PyLocalBuffer]
jax.interpreters.xla._make_abstract_python_scalar(typ,_)
jax.interpreters.xla._make_abstract_unit(_)
jax.interpreters.xla._make_array_shape(a)
jax.interpreters.xla._make_unit(c)
jax.interpreters.xla._param_uses_outfeed(param)
jax.interpreters.xla._pval_to_result_handler(device,pval)
jax.interpreters.xla._remat_translation_rule(c,axis_env,in_nodes,name_stack,backend,name,call_jaxpr,device=None,concrete=None)
jax.interpreters.xla._tuple_output(*args,**kwargs)
jax.interpreters.xla._xla_call_impl(fun:lu.WrappedFun,*args,device,backend,name,donated_invars)
jax.interpreters.xla._xla_call_jvp_update_params(params,nz_tangents)
jax.interpreters.xla._xla_call_partial_eval_update_params(params,in_unknowns)
jax.interpreters.xla._xla_call_translation_rule(c,axis_env,in_nodes,name_stack,backend,name,call_jaxpr,donated_invars,device=None)
jax.interpreters.xla._xla_call_transpose_update_params(params,undef_primals,nonzero_cts)
jax.interpreters.xla._xla_callable(fun:lu.WrappedFun,device,backend,name,donated_invars,*arg_specs)
jax.interpreters.xla._xla_callable_args(c,avals,tuple_args,replicated=None,partitions:Optional[Sequence[Optional[Sequence[int]]]]=None)
jax.interpreters.xla._xla_callable_device(nreps,backend,device,arg_devices)
jax.interpreters.xla._xla_consts(c,consts)
jax.interpreters.xla._xla_param(builder,param_num,xla_shape,replicated,partitions)
jax.interpreters.xla.abstractify(x)->core.AbstractValue
jax.interpreters.xla.add_jaxvals_translation_rule(c,x,y)
jax.interpreters.xla.apply_outfeed_rewriter(jaxpr:core.Jaxpr)->core.Jaxpr
jax.interpreters.xla.apply_primitive(prim,*args,**params)
jax.interpreters.xla.arg_spec(x)
jax.interpreters.xla.array_result_handler(device:Optional[Device],aval:core.ShapedArray)
jax.interpreters.xla.aval_to_result_handler(device:Optional[Device],aval:core.ShapedArray)
jax.interpreters.xla.aval_to_xla_shape(aval)
jax.interpreters.xla.axis_groups(axis_env,name)
jax.interpreters.xla.axis_read(axis_env,axis_name)
jax.interpreters.xla.canonicalize_dtype(x)
jax.interpreters.xla.check_backend_params(params,outer_backend)
jax.interpreters.xla.check_nans(prim,bufs)
jax.interpreters.xla.device_put(x,device:Optional[Device]=None)
jax.interpreters.xla.eqn_replicas(eqn)
jax.interpreters.xla.extend_axis_env(env,name,size)
jax.interpreters.xla.flatten_shape(s:XlaShape)->Sequence[Tuple[Sequence[int], XlaShape]]
jax.interpreters.xla.identity(x)
jax.interpreters.xla.initial_style_primitive_replicas(params)
jax.interpreters.xla.is_device_constant(x)
jax.interpreters.xla.jaxpr_collectives(jaxpr)
jax.interpreters.xla.jaxpr_has_pmap(jaxpr)
jax.interpreters.xla.jaxpr_literals(jaxpr)
jax.interpreters.xla.jaxpr_replicas(jaxpr)
jax.interpreters.xla.jaxpr_subcomp(c,jaxpr,backend,axis_env,consts,name_stack,*args)
jax.interpreters.xla.jaxpr_uses_outfeed(jaxpr:core.Jaxpr)->bool
jax.interpreters.xla.lower_fun(fun,multiple_results)
jax.interpreters.xla.lower_fun_initial_style(fun)
jax.interpreters.xla.omnistaging_enabler()->None
jax.interpreters.xla.prefetch(x)
jax.interpreters.xla.primitive_computation(prim,axis_env,backend,tuple_args,*avals,**params)
jax.interpreters.xla.primitive_subcomputation(prim,*avals,**params)
jax.interpreters.xla.primitive_uses_outfeed(prim:core.Primitive,params:Dict)->bool
jax.interpreters.xla.set_up_aliases(c,xla_args,out_tuple,donated_args,tuple_args)
jax.interpreters.xla.xla_destructure(c,ans)
jax.interpreters.xla.xla_primitive_callable(prim,*arg_specs:Tuple[core.AbstractValue,Optional[Device]],**params)
jax.interpreters.xla.zeros_like_translation_rule(c,x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/nn/functions.py----------------------------------------
A:jax.nn.functions.safe_x->jax.numpy.where(x > 0, 0.0, x)
A:jax.nn.functions.sqrt_2_over_pi->numpy.sqrt(2 / np.pi).astype(x.dtype)
A:jax.nn.functions.(x1, x2)->jax.numpy.split(x, 2, axis)
A:jax.nn.functions.unnormalized->jax.numpy.exp(x - lax.stop_gradient(x.max(axis, keepdims=True)))
A:jax.nn.functions.mean->jax.numpy.mean(x, axis, keepdims=True)
A:jax.nn.functions.num_classes->jax.core.concrete_or_error(int, num_classes, 'in jax.nn.one_hot argument `num_classes`')
A:jax.nn.functions.dtype->jax.dtypes.canonicalize_dtype(dtype)
A:jax.nn.functions.x->jax.numpy.asarray(x)
A:jax.nn.functions.rhs->jax.lax.broadcast_to_rank(jnp.arange(num_classes, dtype=x.dtype), lhs.ndim)
jax.nn.celu(x,alpha=1.0)
jax.nn.elu(x,alpha=1.0)
jax.nn.functions.celu(x,alpha=1.0)
jax.nn.functions.elu(x,alpha=1.0)
jax.nn.functions.gelu(x)
jax.nn.functions.glu(x,axis=-1)
jax.nn.functions.hard_sigmoid(x)
jax.nn.functions.hard_silu(x)
jax.nn.functions.hard_tanh(x)
jax.nn.functions.leaky_relu(x,negative_slope=0.01)
jax.nn.functions.log_sigmoid(x)
jax.nn.functions.log_softmax(x,axis=-1)
jax.nn.functions.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.functions.one_hot(x,num_classes,*,dtype=jnp.float64)
jax.nn.functions.relu(x)
jax.nn.functions.relu6(x)
jax.nn.functions.selu(x)
jax.nn.functions.sigmoid(x)
jax.nn.functions.silu(x)
jax.nn.functions.soft_sign(x)
jax.nn.functions.softmax(x,axis=-1)
jax.nn.functions.softplus(x)
jax.nn.gelu(x)
jax.nn.glu(x,axis=-1)
jax.nn.hard_sigmoid(x)
jax.nn.hard_silu(x)
jax.nn.hard_tanh(x)
jax.nn.leaky_relu(x,negative_slope=0.01)
jax.nn.log_sigmoid(x)
jax.nn.log_softmax(x,axis=-1)
jax.nn.normalize(x,axis=-1,mean=None,variance=None,epsilon=1e-05)
jax.nn.one_hot(x,num_classes,*,dtype=jnp.float64)
jax.nn.relu(x)
jax.nn.relu6(x)
jax.nn.selu(x)
jax.nn.sigmoid(x)
jax.nn.silu(x)
jax.nn.soft_sign(x)
jax.nn.softmax(x,axis=-1)
jax.nn.softplus(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/nn/initializers.py----------------------------------------
A:jax.nn.initializers.(fan_in, fan_out)->_compute_fans(shape, in_axis, out_axis)
A:jax.nn.initializers.variance->jax.numpy.array(scale / denominator, dtype=dtype)
A:jax.nn.initializers.xavier_uniformglorot_uniform->partial(variance_scaling, 1.0, 'fan_avg', 'uniform')
A:jax.nn.initializers.xavier_normalglorot_normal->partial(variance_scaling, 1.0, 'fan_avg', 'truncated_normal')
A:jax.nn.initializers.lecun_uniform->partial(variance_scaling, 1.0, 'fan_in', 'uniform')
A:jax.nn.initializers.lecun_normal->partial(variance_scaling, 1.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.kaiming_uniformhe_uniform->partial(variance_scaling, 2.0, 'fan_in', 'uniform')
A:jax.nn.initializers.kaiming_normalhe_normal->partial(variance_scaling, 2.0, 'fan_in', 'truncated_normal')
A:jax.nn.initializers.A->jax.random.normal(key, matrix_shape, dtype)
A:jax.nn.initializers.(Q, R)->jax.numpy.linalg.qr(A)
A:jax.nn.initializers.diag_sign->jax.lax.broadcast_to_rank(jnp.sign(jnp.diag(R)), rank=Q.ndim)
A:jax.nn.initializers.Q->jax.numpy.moveaxis(Q, -1, column_axis)
A:jax.nn.initializers.ortho_init->orthogonal(scale=scale, column_axis=column_axis, dtype=dtype)
A:jax.nn.initializers.ortho_matrix->ortho_init(key, shape[-2:])
A:jax.nn.initializers.W->jax.numpy.zeros(shape, dtype=dtype)
jax.nn.initializers._compute_fans(shape,in_axis=-2,out_axis=-1)
jax.nn.initializers.delta_orthogonal(scale=1.0,column_axis=-1,dtype=jnp.float32)
jax.nn.initializers.normal(stddev=0.01,dtype=jnp.float32)
jax.nn.initializers.ones(key,shape,dtype=jnp.float32)
jax.nn.initializers.orthogonal(scale=1.0,column_axis=-1,dtype=jnp.float32)
jax.nn.initializers.uniform(scale=0.01,dtype=jnp.float32)
jax.nn.initializers.variance_scaling(scale,mode,distribution,in_axis=-2,out_axis=-1,dtype=jnp.float32)
jax.nn.initializers.zeros(key,shape,dtype=jnp.float32)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/nn/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax/lax.py----------------------------------------
A:jax.lax.lax.out_shape->_ceil_divide(in_shape, window_strides)
A:jax.lax.lax.ndim->numpy.ndim(array)
A:jax.lax.lax.shapes->' '.join((str(a.shape) for a in args))
A:jax.lax.lax.result_shape->numpy.floor_divide(np.add(np.subtract(limit_indices, start_indices), strides) - 1, strides)
A:jax.lax.lax.new_dtype->dtypes.canonicalize_dtype(new_dtype)
A:jax.lax.lax.operand->pad(operand, select_identity(operand.dtype), tuple(((0, 0, d - 1) for d in base_dilation)))
A:jax.lax.lax.old_dtype->_dtype(operand)
A:jax.lax.lax.dnums->GatherDimensionNumbers(offset_dims=(), collapsed_slice_dims=tuple(range(rank)), start_index_map=tuple(range(rank)))
A:jax.lax.lax.padding->list(map(np.sum, padding))
A:jax.lax.lax.contract_dims->tuple(map(lambda x: tuple(x), contract_dims_seq))
A:jax.lax.lax.batch_dims->tuple(map(lambda x: tuple(x), batch_dims_seq))
A:jax.lax.lax.dims->numpy.delete(np.arange(prototype_arg.ndim), new_bdim)
A:jax.lax.lax.shape->tuple((xla.aval_to_xla_shape(x).with_major_to_minor_layout_if_absent() for x in shapes))
A:jax.lax.lax.new_sizes->tuple(new_sizes)
A:jax.lax.lax.start_indices->concatenate([counts, start_indices], len(count_shape) - 1)
A:jax.lax.lax.(jaxpr, consts)->_reduction_jaxpr(mul, _abstractify(init_value))
A:jax.lax.lax.indices->concatenate([broadcast_in_dim(x, (size, 1), broadcast_dimensions=(0,) if i is not None else ()) for (x, i) in zip(indices, bdims)], dimension=1)
A:jax.lax.lax.slice_sizes->iter(np.delete(slice_sizes, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.offset_dims->tuple(np.add(1, dimension_numbers.offset_dims))
A:jax.lax.lax.permutation->tuple(permutation)
A:jax.lax.lax.monoid_reducer->_get_monoid_window_reducer(computation, init_value)
A:jax.lax.lax.pval->interpreters.partial_eval.PartialVal.unknown(aval)
A:jax.lax.lax.comp->lu.wrap_init(lambda x, y: (computation(x, y),))
A:jax.lax.lax.(jaxpr, _, consts)->interpreters.partial_eval.trace_to_jaxpr(comp, (pval, pval), instantiate=False)
A:jax.lax.lax.aval->ShapedArray(lazy_expr.shape, operand.dtype)
A:jax.lax.lax.dtype->tuple((xla.aval_to_xla_shape(x).with_major_to_minor_layout_if_absent() for x in shapes)).numpy_dtype()
A:jax.lax.lax.init_value->xb.constant(c, np.array(0, dtype))
A:jax.lax.lax.(select_jaxpr, select_consts)->_reduction_jaxpr(select, _abstractify(init_value))
A:jax.lax.lax.(scatter_jaxpr, scatter_consts)->_reduction_jaxpr(scatter, _abstractify(init_value))
A:jax.lax.lax.dimension->kwargs.pop('dimension')
A:jax.lax.lax.(k, v)->Primitive('sort').bind(keys, values, dimension=dimension, is_stable=is_stable, num_keys=1)
A:jax.lax.lax.k->int(k)
A:jax.lax.lax.fill_value->tie_in(x, fill_value)
A:jax.lax.lax.lazy_expr->lazy.transpose(operand._lazy_expr, permutation)
A:jax.lax.lax.(N, M)->tuple(map(int, shape))
A:jax.lax.lax.offset->int(offset)
A:jax.lax.lax.axes->tuple(np.delete(range(len(shape)), broadcast_dimensions))
A:jax.lax.lax.base_shape->tuple(np.take(shape, axes))
A:jax.lax.lax.pad_a->int(np.ceil(pad_len / 2))
A:jax.lax.lax.x->convert_element_type(x, np.float32)
A:jax.lax.lax.ndims->len(lhs.shape)
A:jax.lax.lax.dn->conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)
A:jax.lax.lax.k_shape->numpy.take(rhs.shape, dn.rhs_spec)
A:jax.lax.lax.effective_k_size->map(lambda k, r: (k - 1) * r + 1, k_sdims, rhs_dilation)
A:jax.lax.lax.rhs->expand_dims(rhs, tuple(range(rhs_start_expand, rhs_end_expand)))
A:jax.lax.lax.size->next((a.shape[bdim] for (a, bdim) in zip(batched_args, batch_dims) if bdim is not None))
A:jax.lax.lax.limit_indices->list(operand.shape)
A:jax.lax.lax.axis->operator.index(axis)
A:jax.lax.lax.strides[axis]->int(stride)
A:jax.lax.lax.result->interpreters.xla.lower_fun(partial(_sort_lt_comparator, num_keys=num_keys), multiple_results=False)(subc, *params)
A:jax.lax.lax.slice_sizes[axis]->int(slice_size)
A:jax.lax.lax.update->expand_dims(update, (axis,))
A:jax.lax.lax.batch->tuple(range(lhs.ndim - 2))
A:jax.lax.lax.ShapedArray.broadcast->core.aval_method(broadcast)
A:jax.lax.lax.ShapedArray.transpose->core.aval_method(transpose)
A:jax.lax.lax.ShapedArray.reshape->core.aval_method(reshape)
A:jax.lax.lax.n->numpy.prod(input_shape[list(axes)])
A:jax.lax.lax.ShapedArray._iter->staticmethod(_iter)
A:jax.lax.lax.prim->standard_primitive(shape_rule, dtype_rule, name, translation_rule=translation_rule)
A:jax.lax.lax.least_specialized->_max(map(type, args), key=operator.attrgetter('array_abstraction_level'))
A:jax.lax.lax.xla_opname->''.join((term.capitalize() for term in name.split('_')))
A:jax.lax.lax.typename->str(np.dtype(aval_dtype).name)
A:jax.lax.lax.dtype_rule->partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name)
A:jax.lax.lax.standard_unop->partial(unop, _identity)
A:jax.lax.lax.typenames->', '.join((t.__name__ for t in types))
A:jax.lax.lax.shape_rule->partial(_broadcasting_shape_rule, name)
A:jax.lax.lax.standard_naryop->partial(naryop, _input_dtype)
A:jax.lax.lax.bcast_dims->_is_singleton_reshape(old_sizes, new_sizes)
A:jax.lax.lax.x_shape->numpy.shape(x)
A:jax.lax.lax.(broadcast_dimensions,)->numpy.where(np.equal(x_shape, shape))
A:jax.lax.lax.(squeezed_dimensions,)->numpy.where(np.not_equal(x_shape, shape))
A:jax.lax.lax.squeezed->squeeze(x, squeezed_dimensions)
A:jax.lax.lax.neg_p->standard_unop(_num, 'neg')
A:jax.lax.lax.zero->xb.constant(c, np.array(0, dtype))
A:jax.lax.lax.sign_p->standard_unop(_num, 'sign', translation_rule=_sign_translation_rule)
A:jax.lax.lax.nextafter_p->standard_naryop([_float, _float], 'nextafter', translation_rule=lambda c, x1, x2: xops.NextAfter(x1, x2))
A:jax.lax.lax.floor_p->standard_unop(_float, 'floor')
A:jax.lax.lax.ceil_p->standard_unop(_float, 'ceil')
A:jax.lax.lax.round_p->standard_unop(_float, 'round')
A:jax.lax.lax.is_finite_p->unop(_fixed_dtype(np.bool_), _float, 'is_finite')
A:jax.lax.lax.exp_p->standard_unop(_float | _complex, 'exp')
A:jax.lax.lax.log_p->standard_unop(_float | _complex, 'log')
A:jax.lax.lax.expm1_p->standard_unop(_float | _complex, 'expm1')
A:jax.lax.lax.log1p_p->standard_unop(_float | _complex, 'log1p')
A:jax.lax.lax.tanh_p->standard_unop(_float | _complex, 'tanh')
A:jax.lax.lax.sin_p->standard_unop(_float | _complex, 'sin')
A:jax.lax.lax.cos_p->standard_unop(_float | _complex, 'cos')
A:jax.lax.lax.atan2_p->standard_naryop([_float, _float], 'atan2')
A:jax.lax.lax.sinh_p->standard_unop(_float | _complex, 'sinh')
A:jax.lax.lax.cosh_p->standard_unop(_float | _complex, 'cosh')
A:jax.lax.lax.asinh_p->standard_unop(_float | _complex, 'asinh')
A:jax.lax.lax.acosh_p->standard_unop(_float | _complex, 'acosh')
A:jax.lax.lax.atanh_p->standard_unop(_float | _complex, 'atanh')
A:jax.lax.lax.regularized_incomplete_beta_p->standard_naryop([_float, _float, _float], 'regularized_incomplete_beta', translation_rule=_broadcast_translate(partial(standard_translate, 'regularized_incomplete_beta')))
A:jax.lax.lax.partial_x->exp((b - 1) * log1p(-x) + (a - 1) * log(x) - lbeta)
A:jax.lax.lax.lgamma_p->standard_unop(_float, 'lgamma')
A:jax.lax.lax.digamma_p->standard_unop(_float, 'digamma')
A:jax.lax.lax.igamma_p->standard_naryop([_float, _float], 'igamma', translation_rule=_broadcast_translate(partial(standard_translate, 'igamma')))
A:jax.lax.lax.igamma_grad_a_p->standard_naryop([_float, _float], 'igamma_grad_a', translation_rule=_broadcast_translate(partial(standard_translate, 'igamma_grad_a')))
A:jax.lax.lax.igammac_p->standard_naryop([_float, _float], 'igammac', translation_rule=_broadcast_translate(partial(standard_translate, 'igammac')))
A:jax.lax.lax.random_gamma_grad_p->standard_naryop([_float, _float], 'random_gamma_grad', translation_rule=_broadcast_translate(partial(standard_translate, 'random_gamma_grad')))
A:jax.lax.lax.bessel_i0e_p->standard_unop(_float, 'bessel_i0e')
A:jax.lax.lax.bessel_i1e_p->standard_unop(_float, 'bessel_i1e')
A:jax.lax.lax.safe_x->select(x_is_not_tiny, x, full_like(x, eps))
A:jax.lax.lax.dy_dx->select(x_is_not_tiny, dy_dx, full_like(x, 0.5))
A:jax.lax.lax.erf_p->standard_unop(_float, 'erf')
A:jax.lax.lax.erfc_p->standard_unop(_float, 'erfc')
A:jax.lax.lax.erf_inv_p->standard_unop(_float, 'erf_inv')
A:jax.lax.lax.real_p->unop(_complex_basetype, _complex, 'real')
A:jax.lax.lax.imag_p->unop(_complex_basetype, _complex, 'imag')
A:jax.lax.lax.complex_p->naryop(_complex_dtype, [_complex_elem_types, _complex_elem_types], 'complex')
A:jax.lax.lax.conj_p->unop(_complex_dtype, _complex_elem_types | _complex, 'conj')
A:jax.lax.lax.ad.primitive_jvps[conj_p]->partial(ad.linear_jvp, conj_p)
A:jax.lax.lax.abs_p->unop(_complex_basetype, _num, 'abs')
A:jax.lax.lax.sqrt_p->standard_unop(_float | _complex, 'sqrt')
A:jax.lax.lax.rsqrt_p->standard_unop(_float | _complex, 'rsqrt')
A:jax.lax.lax.pow_p->standard_naryop([_float | _complex, _float | _complex], 'pow')
A:jax.lax.lax.jac->mul(y, pow(x, select(eq(y, _zeros(y)), _ones(y), sub(y, _ones(y)))))
A:jax.lax.lax.integer_pow_p->standard_primitive(_attrgetter('shape'), _integer_pow_dtype_rule, 'integer_pow', translation_rule=_integer_pow_translation_rule)
A:jax.lax.lax.not_p->standard_unop(_bool_or_int, 'not')
A:jax.lax.lax.and_p->standard_naryop([_bool_or_int, _bool_or_int], 'and')
A:jax.lax.lax.or_p->standard_naryop([_bool_or_int, _bool_or_int], 'or')
A:jax.lax.lax.xor_p->standard_naryop([_bool_or_int, _bool_or_int], 'xor')
A:jax.lax.lax.population_count_p->standard_unop(_int, 'population_count')
A:jax.lax.lax.add_p->standard_naryop([_num, _num], 'add')
A:jax.lax.lax.sub_p->standard_naryop([_num, _num], 'sub')
A:jax.lax.lax.mul_p->standard_naryop([_num, _num], 'mul')
A:jax.lax.lax.div_p->standard_naryop([_num, _num], 'div')
A:jax.lax.lax.rem_p->standard_naryop([_num, _num], 'rem')
A:jax.lax.lax.which->xops.BroadcastInDim(which, out_shape, bcast_dims(which_shape))
A:jax.lax.lax.y->tie_in(*batched_args)
A:jax.lax.lax.rx->xops.Real(x)
A:jax.lax.lax.ry->xops.Real(y)
A:jax.lax.lax.max_p->standard_naryop([_any, _any], 'max', translation_rule=partial(_minmax_translation_rule, minmax=xops.Max, cmp=xops.Gt))
A:jax.lax.lax.min_p->standard_naryop([_any, _any], 'min', translation_rule=partial(_minmax_translation_rule, minmax=xops.Min, cmp=xops.Lt))
A:jax.lax.lax.shift_left_p->standard_naryop([_int, _int], 'shift_left')
A:jax.lax.lax.shift_right_arithmetic_p->standard_naryop([_int, _int], 'shift_right_arithmetic')
A:jax.lax.lax.shift_right_logical_p->standard_naryop([_int, _int], 'shift_right_logical')
A:jax.lax.lax.eq_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'eq')
A:jax.lax.lax.ne_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'ne')
A:jax.lax.lax.ge_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'ge')
A:jax.lax.lax.gt_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'gt')
A:jax.lax.lax.le_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'le')
A:jax.lax.lax.lt_p->naryop(_fixed_dtype(np.bool_), [_any, _any], 'lt')
A:jax.lax.lax.new_etype->lib.xla_bridge.dtype_to_etype(new_dtype)
A:jax.lax.lax.convert_element_type_p->standard_primitive(_convert_element_type_shape_rule, _convert_element_type_dtype_rule, 'convert_element_type', _convert_element_type_translation_rule)
A:jax.lax.lax.bitcast_convert_type_p->standard_primitive(_bitcast_convert_type_shape_rule, _bitcast_convert_type_dtype_rule, 'bitcast_convert_type', _bitcast_convert_type_translation_rule)
A:jax.lax.lax.(quot, rem)->divmod(lhs_feature_count, feature_group_count)
A:jax.lax.lax.lhs_trans->numpy.take(lhs_shape, lhs_perm)
A:jax.lax.lax.rhs_trans->numpy.take(rhs_shape, rhs_perm)
A:jax.lax.lax.out_trans->tuple((lhs_trans[0], rhs_trans[0]) + tuple(out_space))
A:jax.lax.lax.(lhs_sdims, rhs_sdims, out_sdims)->map(_conv_sdims, dimension_numbers)
A:jax.lax.lax.t_rhs_spec->_conv_spec_transpose(rhs_spec)
A:jax.lax.lax.trans_dimension_numbers->ConvDimensionNumbers(lhs_trans, out_trans, rhs_trans)
A:jax.lax.lax.revd_weights->rev(rhs, rhs_sdims)
A:jax.lax.lax.out->xops.Sort(c, operands, dimension=dimension, is_stable=is_stable, comparator=comparator)
A:jax.lax.lax.(lhs_trans, rhs_trans, out_trans)->map(_conv_spec_transpose, dimension_numbers)
A:jax.lax.lax.dimension_numbers->_conv_general_proto(dimension_numbers)
A:jax.lax.lax.precision_config->_precision_config(precision)
A:jax.lax.lax.k1->conv(xops.Add(lhs_real, lhs_imag), rhs_real)
A:jax.lax.lax.k2->conv(lhs_real, xops.Sub(rhs_imag, rhs_real))
A:jax.lax.lax.k3->conv(lhs_imag, xops.Add(rhs_real, rhs_imag))
A:jax.lax.lax.new_lhs->_reshape_axis_into(lhs_spec[0], lhs_spec[0], new_lhs)
A:jax.lax.lax.new_rhs->_reshape_axis_into(rhs_spec[0], rhs_spec[0], new_rhs)
A:jax.lax.lax.conv_general_dilated_p->standard_primitive(_conv_general_dilated_shape_rule, _conv_general_dilated_dtype_rule, 'conv_general_dilated', partial(_conv_general_dilated_translation_rule, expand_complex_convolutions=False))
A:jax.lax.lax.xla.backend_specific_translations['cpu'][conv_general_dilated_p]->partial(_conv_general_dilated_translation_rule, expand_complex_convolutions=True)
A:jax.lax.lax.xla.backend_specific_translations['gpu'][conv_general_dilated_p]->partial(_conv_general_dilated_translation_rule, expand_complex_convolutions=True)
A:jax.lax.lax.new_shape->_compute_squeeze_shape(c.get_shape(arg).dimensions(), dimensions)
A:jax.lax.lax.(size2, ragged)->divmod(shape[src], size1)
A:jax.lax.lax.config->lib.xla_client.PrecisionConfig()
A:jax.lax.lax.lhs_batch_shape->numpy.take(lhs.shape, lhs_batch)
A:jax.lax.lax.rhs_batch_shape->numpy.take(rhs.shape, rhs_batch)
A:jax.lax.lax.lhs_contracting_shape->numpy.take(lhs.shape, lhs_contracting)
A:jax.lax.lax.rhs_contracting_shape->numpy.take(rhs.shape, rhs_contracting)
A:jax.lax.lax.batch_shape->tuple(lhs_batch_shape)
A:jax.lax.lax.lhs_contract_or_batch->tuple(sorted(tuple(lhs_contracting) + tuple(lhs_batch)))
A:jax.lax.lax.lhs_tensored_shape->tuple(np.delete(lhs.shape, lhs_contract_or_batch))
A:jax.lax.lax.rhs_contract_or_batch->tuple(sorted(tuple(rhs_contracting) + tuple(rhs_batch)))
A:jax.lax.lax.rhs_tensored_shape->tuple(np.delete(rhs.shape, rhs_contract_or_batch))
A:jax.lax.lax.x_kept->remaining(range(x_ndim), x_contract, x_batch)
A:jax.lax.lax.y_kept->remaining(range(y.ndim), y_contract, y_batch)
A:jax.lax.lax.(ans_batch, ans_y, _)->ranges_like(x_batch, y_kept, x_kept)
A:jax.lax.lax.(ans_batch, _, ans_y)->ranges_like(x_batch, x_kept, y_kept)
A:jax.lax.lax.x_contract_sorted_by_y->list(np.take(x_contract, np.argsort(y_contract)))
A:jax.lax.lax.out_axes->numpy.argsort(list(x_batch) + x_kept + x_contract_sorted_by_y)
A:jax.lax.lax.lhs_contract->bump_dims(lhs_contract, lbd)
A:jax.lax.lax.rhs_contract->bump_dims(rhs_contract, rbd)
A:jax.lax.lax.other->tuple((d for d in range(rhs.ndim) if d not in rhs_batch and d not in rhs_contract))
A:jax.lax.lax.lhs_batch->bump_dims(lhs_batch, lbd)
A:jax.lax.lax.rhs_batch->bump_dims(rhs_batch, rbd)
A:jax.lax.lax.batched_out->dot_general(lhs, rhs, new_dimension_numbers, precision=precision)
A:jax.lax.lax.lhs_noncontract_dims->tuple(sorted(set(range(np.ndim(lhs))) - set(lhs_batch_dims) - set(lhs_contract_dims)))
A:jax.lax.lax.rhs_noncontract_dims->tuple(sorted(set(range(np.ndim(rhs))) - set(rhs_batch_dims) - set(rhs_contract_dims)))
A:jax.lax.lax.lhs->expand_dims(lhs, tuple(range(lhs_start_expand, lhs_end_expand)))
A:jax.lax.lax.rhs_start_expand->len(lhs_batch_dims)
A:jax.lax.lax.translation->interpreters.xla.lower_fun(_dot_using_sum_of_products, multiple_results=False)
A:jax.lax.lax.dot_general_p->standard_primitive(_dot_general_shape_rule, _dot_general_dtype_rule, 'dot_general', _dot_general_translation_rule)
A:jax.lax.lax.broadcast_p->standard_primitive(_broadcast_shape_rule, _input_dtype, 'broadcast')
A:jax.lax.lax.operand_ndim->numpy.ndim(operand)
A:jax.lax.lax.new_operand->pad(new_operand, _zero(operand), ((0, 1, 0),) + tuple(((0, 0, 0) for _ in operand_shape)))
A:jax.lax.lax.broadcast_in_dim_p->standard_primitive(_broadcast_in_dim_shape_rule, _input_dtype, 'broadcast_in_dim')
A:jax.lax.lax._clamp_dtype_rule->partial(naryop_dtype_rule, _input_dtype, [_any, _any, _any], 'clamp')
A:jax.lax.lax.clamp_p->standard_primitive(_clamp_shape_rule, _clamp_dtype_rule, 'clamp')
A:jax.lax.lax.op->next((op for op in operands if not isinstance(op, UnshapedArray)))
A:jax.lax.lax.concat_size->sum((o.shape[dimension] for o in operands))
A:jax.lax.lax.limit_points->numpy.cumsum([shape[dimension] for shape in operand_shapes])
A:jax.lax.lax.starts->numpy.zeros((len(operands), t.ndim), dtype=int)
A:jax.lax.lax.limits->numpy.tile(t.shape, (len(operands), 1))
A:jax.lax.lax.concatenate_p->standard_primitive(_concatenate_shape_rule, _concatenate_dtype_rule, 'concatenate', _concatenate_translation_rule)
A:jax.lax.lax.(lo, hi, interior)->zip(*padding_config)
A:jax.lax.lax.unpad_config->safe_zip(np.negative(lo), np.negative(hi), np.zeros_like(interior))
A:jax.lax.lax.unpadded->pad(t, np.array(0.0, t.dtype), unpad_config)
A:jax.lax.lax.padding_config->list(padding_config)
A:jax.lax.lax.pad_p->standard_primitive(_pad_shape_rule, _pad_dtype_rule, 'pad', translation_rule=_pad_translation_rule)
A:jax.lax.lax.dimensions->tuple(np.add(1, dimensions))
A:jax.lax.lax.dims_set->frozenset((_canonicalize_axis(i, ndim_out) for i in dimensions))
A:jax.lax.lax.squeeze_p->standard_primitive(_squeeze_shape_rule, _squeeze_dtype_rule, 'squeeze', _squeeze_translation_rule)
A:jax.lax.lax.old_sizes->numpy.shape(operand)
A:jax.lax.lax.d2->next(new, None)
A:jax.lax.lax.reshape_p->standard_primitive(_reshape_shape_rule, _reshape_dtype_rule, 'reshape', _reshape_translation_rule)
A:jax.lax.lax.rev_p->standard_primitive(_rev_shape_rule, _input_dtype, 'rev')
A:jax.lax.lax.transpose_p->standard_primitive(_transpose_shape_rule, _input_dtype, 'transpose')
A:jax.lax.lax.zeros->full(operand_shape, tie_in(t, _zero(t)))
A:jax.lax.lax.pred->broadcast_in_dim(pred, on_true.shape, [0])
A:jax.lax.lax.on_false->broadcast(on_false, pred.shape)
A:jax.lax.lax.on_true->broadcast(on_true, pred.shape)
A:jax.lax.lax.select_p->standard_primitive(_select_shape_rule, _select_dtype_rule, 'select')
A:jax.lax.lax.strides->numpy.ones(operand.ndim, np.int32)
A:jax.lax.lax.pads->padtype_to_pads(lhs_shape[2:], rhs_shape[2:], strides, pads)
A:jax.lax.lax.real_limits->numpy.add(np.add(start_indices, 1), np.multiply(np.subtract(t.shape, 1), strides))
A:jax.lax.lax.new_start_indices->list(start_indices)
A:jax.lax.lax.new_limit_indices->list(limit_indices)
A:jax.lax.lax.new_strides->list(strides)
A:jax.lax.lax.slice_p->standard_primitive(_slice_shape_rule, _input_dtype, 'slice', _slice_translation_rule)
A:jax.lax.lax.tangent_out->gather(tangent, gather_indices, dnums, slice_sizes)
A:jax.lax.lax.(index, index_bdim)->_batch_dynamic_slice_indices(start_idx, start_idx_bd)
A:jax.lax.lax.dynamic_slice_p->standard_primitive(_dynamic_slice_shape_rule, _dynamic_slice_dtype_rule, 'dynamic_slice', _dynamic_slice_translation_rule)
A:jax.lax.lax.val_out->_select_and_gather_add(source, operand, select_prim, window_dimensions, window_strides, padding, base_dilation, window_dilation)
A:jax.lax.lax.g_operand->interpreters.ad.instantiate_zeros(g_operand)
A:jax.lax.lax.g_update->interpreters.ad.instantiate_zeros(g_update)
A:jax.lax.lax.dynamic_update_slice_p->standard_primitive(_dynamic_update_slice_shape_rule, _dynamic_update_slice_dtype_rule, 'dynamic_update_slice', _dynamic_update_slice_translation_rule)
A:jax.lax.lax.proto->lib.xla_client.ConvolutionDimensionNumbers()
A:jax.lax.lax.msg->'slice_sizes must have rank equal to the gather operand; operand.shape={}, slice_sizes={}'.format(operand.shape, slice_sizes)
A:jax.lax.lax.start_indices_shape->iter(start_indices.shape[:-1])
A:jax.lax.lax.indices_shape->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer').get_shape(scatter_indices)
A:jax.lax.lax.scatter_dnums->ScatterDimensionNumbers(update_window_dims=dimension_numbers.offset_dims, inserted_window_dims=dimension_numbers.collapsed_slice_dims, scatter_dims_to_operand_dims=dimension_numbers.start_index_map)
A:jax.lax.lax.collapsed_slice_dims->tuple(np.add(1, dimension_numbers.collapsed_slice_dims))
A:jax.lax.lax.start_index_map->tuple(np.add(1, dimension_numbers.start_index_map))
A:jax.lax.lax.count_shape->list(scatter_indices.shape)
A:jax.lax.lax.counts->_reduce_sum(location_indicators, axes)
A:jax.lax.lax.gather_p->standard_primitive(_gather_shape_rule, _gather_dtype_rule, 'gather', _gather_translation_rule)
A:jax.lax.lax.update_computation->_reduction_computation(c, update_jaxpr, update_consts, init_value)
A:jax.lax.lax.g_updates->interpreters.ad.instantiate_zeros(g_updates)
A:jax.lax.lax.gather_dnums->GatherDimensionNumbers(offset_dims=dnums.update_window_dims, collapsed_slice_dims=dnums.inserted_window_dims, start_index_map=dnums.scatter_dims_to_operand_dims)
A:jax.lax.lax.update_t->gather(mul(t, operand), scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.operand_t->scatter_mul(t, scatter_indices, updates, dimension_numbers=dimension_numbers, indices_are_sorted=indices_are_sorted, unique_indices=unique_indices)
A:jax.lax.lax.updates->reshape(updates, (1,) + updates_shape)
A:jax.lax.lax.inserted_window_dims->tuple(np.add(1, dimension_numbers.inserted_window_dims))
A:jax.lax.lax.scatter_dims_to_operand_dims->tuple(np.add(1, dimension_numbers.scatter_dims_to_operand_dims))
A:jax.lax.lax.scatter_indices->concatenate([counts, scatter_indices], len(count_shape) - 1)
A:jax.lax.lax.update_window_dims->tuple(np.add(1, dimension_numbers.update_window_dims))
A:jax.lax.lax.scatter_add_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-add', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_add_p]->partial(_scatter_batching_rule, scatter_add)
A:jax.lax.lax.scatter_mul_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-mul', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_mul_p]->partial(_scatter_batching_rule, scatter_mul)
A:jax.lax.lax.initial_vals->gather(operand, scatter_indices, gather_dnums, np.array(slice_sizes))
A:jax.lax.lax.target_vals->gather(val_out, scatter_indices, gather_dnums, np.array(slice_sizes))
A:jax.lax.lax.num_updates->gather(scatter_add(_zeros(operand), scatter_indices, select(successful_updates, _ones(updates), _zeros(updates)), scatter_dnums), scatter_indices, gather_dnums, np.array(slice_sizes))
A:jax.lax.lax.num_refs->gather(scatter_add(_zeros(operand), scatter_indices, _ones(updates), scatter_dnums), scatter_indices, gather_dnums, np.array(slice_sizes))
A:jax.lax.lax.updates_normalizer->select(retained_values, 1.0 / (num_updates + 1), 1.0 / num_updates)
A:jax.lax.lax.updates_coef->select(successful_updates, updates_normalizer, _zeros(updates))
A:jax.lax.lax.operand_normalizer->select(retained_values, 1.0 / (num_updates + 1), _zeros(num_updates))
A:jax.lax.lax.target_tangents->gather(g_operand, scatter_indices, gather_dnums, np.array(slice_sizes))
A:jax.lax.lax.scatter_min_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-min', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_min_p]->partial(_scatter_batching_rule, scatter_min)
A:jax.lax.lax.ad.primitive_jvps[scatter_min_p]->partial(_scatter_extremal_jvp, scatter_min_p)
A:jax.lax.lax.scatter_max_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter-max', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_max_p]->partial(_scatter_batching_rule, scatter_max)
A:jax.lax.lax.ad.primitive_jvps[scatter_max_p]->partial(_scatter_extremal_jvp, scatter_max_p)
A:jax.lax.lax.updates_dtype->_dtype(updates)
A:jax.lax.lax.ids_shape->numpy.array(updates_shape, dtype=np.int32)
A:jax.lax.lax.num_ids->numpy.prod(ids_shape)
A:jax.lax.lax.update_ids->add(reshape(iota(updates_dtype, num_ids), ids_shape), _ones(updates))
A:jax.lax.lax.reshaped_update_ids->reshape(update_ids, (1,) + updates_shape)
A:jax.lax.lax.updates_and_ids->concatenate((updates, reshaped_update_ids), 0)
A:jax.lax.lax.new_dnums->ScatterDimensionNumbers(update_window_dims=(0,) + tuple((d + 1 for d in dnums.update_window_dims)), inserted_window_dims=tuple((d + 1 for d in dnums.inserted_window_dims)), scatter_dims_to_operand_dims=tuple((d + 1 for d in dnums.scatter_dims_to_operand_dims)))
A:jax.lax.lax.outputs->concatenate(outputs, 0)
A:jax.lax.lax.scattered_ids->index_in_dim(outputs, 1, keepdims=False)
A:jax.lax.lax.gathered_update_ids->gather(scattered_ids, scatter_indices, dimension_numbers=gather_dnums, slice_sizes=slice_sizes)
A:jax.lax.lax.masked_g_operand->select(eq(scattered_ids, _zeros(scattered_ids)), g_operand, _zeros(g_operand))
A:jax.lax.lax.masked_g_updates->select(eq(update_ids, gathered_update_ids), g_updates, _zeros(g_updates))
A:jax.lax.lax.scatter_p->standard_primitive(_scatter_shape_rule, _scatter_dtype_rule, 'scatter', _scatter_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[scatter_p]->partial(_scatter_batching_rule, scatter)
A:jax.lax.lax.xla_computation->_reduction_computation(c, jaxpr, consts, init_value)
A:jax.lax.lax.axis_env->interpreters.xla.AxisEnv(1, (), (), None)
A:jax.lax.lax.subc->lib.xla_bridge.make_computation_builder('sort_lt_comparator')
A:jax.lax.lax.(out,)->interpreters.xla.jaxpr_subcomp(subc, jaxpr, None, axis_env, consts, '', *args)
A:jax.lax.lax.masking.masking_rules[prim]->partial(_reducer_masking_rule, prim, identity)
A:jax.lax.lax.padded_shape->interpreters.masking.padded_shape_as_value(padded_val.shape)
A:jax.lax.lax.mask->_reduce(operator.and_, masks)
A:jax.lax.lax.masked_val->select(mask, padded_val, identity(padded_shape, padded_val.dtype))
A:jax.lax.lax.reduce_p->standard_primitive(_reduce_shape_rule, _input_dtype, 'reduce', _reduce_translation_rule)
A:jax.lax.lax.scalar->ShapedArray((), dtype)
A:jax.lax.lax.broadcast_dimensions->tuple(np.delete(np.arange(len(input_shape)), axes))
A:jax.lax.lax.reduce_sum_p->standard_primitive(_reduce_sum_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_sum'), 'reduce_sum', _reduce_sum_translation_rule)
A:jax.lax.lax.input_shape->numpy.array(operand.shape)
A:jax.lax.lax.non_axes->numpy.delete(np.arange(len(input_shape)), axes)
A:jax.lax.lax.tangent->reshape(tangent, new_shape, permutation)
A:jax.lax.lax.x1->pad(x, _const(x, 0), pad_right)
A:jax.lax.lax.x2->pad(x, _const(x, 0), pad_left)
A:jax.lax.lax.reduce_prod_p->standard_primitive(_reduce_op_shape_rule, partial(_reduce_number_dtype_rule, 'reduce_prod'), 'reduce_prod', _reduce_prod_translation_rule)
A:jax.lax.lax.location_indicators->convert_element_type(_eq_meet(operand, reshape(ans, shape)), g.dtype)
A:jax.lax.lax._reduce_max_translation_rule->partial(_reduce_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_max_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_max', _reduce_max_translation_rule)
A:jax.lax.lax._reduce_min_translation_rule->partial(_reduce_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_min_p->standard_primitive(_reduce_op_shape_rule, _input_dtype, 'reduce_min', _reduce_min_translation_rule)
A:jax.lax.lax.value_shape->xc.Shape.array_shape(shape.xla_element_type(), ())
A:jax.lax.lax.index_shape->xc.Shape.array_shape(index_dtype, ())
A:jax.lax.lax.x_value->xb.parameter(subc, 0, value_shape)
A:jax.lax.lax.x_index->xb.parameter(subc, 1, index_shape)
A:jax.lax.lax.y_value->xb.parameter(subc, 2, value_shape)
A:jax.lax.lax.y_index->xb.parameter(subc, 3, index_shape)
A:jax.lax.lax.which_value->value_comparator(x_value, y_value)
A:jax.lax.lax.which_index->xops.Or(which_value, xops.And(xops.Eq(x_value, y_value), xops.Lt(x_index, y_index)))
A:jax.lax.lax.comparator->lib.xla_bridge.make_computation_builder('sort_lt_comparator').build(result)
A:jax.lax.lax.iota_shape->xc.Shape.array_shape(index_dtype, shape.dimensions())
A:jax.lax.lax.iota->tuple(range(len(lhs_shape)))
A:jax.lax.lax.idxs->tie_in(a, broadcasted_iota(index_dtype, a.shape, axis))
A:jax.lax.lax.maxval->broadcast(tie_in(a, maxval), a.shape)
A:jax.lax.lax.mask_idxs->select(eq(a, expand_dims(op(a, (axis,)), (axis,))), idxs, maxval)
A:jax.lax.lax._argmin_translation_rule->partial(_argminmax_translation_rule, xops.Lt, _get_min_identity)
A:jax.lax.lax._argmax_translation_rule->partial(_argminmax_translation_rule, xops.Gt, _get_max_identity)
A:jax.lax.lax.argmin_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmin', _argmin_translation_rule)
A:jax.lax.lax.xla.backend_specific_translations['gpu'][argmin_p]->interpreters.xla.lower_fun(partial(_argminmax_gpu_translation_rule, _reduce_min), multiple_results=False)
A:jax.lax.lax.argmax_p->standard_primitive(_argminmax_shape_rule, _argminmax_dtype_rule, 'argmax', _argmax_translation_rule)
A:jax.lax.lax.xla.backend_specific_translations['gpu'][argmax_p]->interpreters.xla.lower_fun(partial(_argminmax_gpu_translation_rule, _reduce_max), multiple_results=False)
A:jax.lax.lax._reduce_or_translation_rule->partial(_reduce_logical_translation_rule, or_p, _get_max_identity)
A:jax.lax.lax.reduce_or_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(np.bool_), 'reduce_or', _reduce_or_translation_rule)
A:jax.lax.lax._reduce_and_translation_rule->partial(_reduce_logical_translation_rule, and_p, _get_min_identity)
A:jax.lax.lax.reduce_and_p->standard_primitive(_reduce_logical_shape_rule, _fixed_dtype(np.bool_), 'reduce_and', _reduce_and_translation_rule)
A:jax.lax.lax.reduce_window_p->standard_primitive(_reduce_window_shape_rule, _input_dtype, 'reduce_window', _reduce_window_translation_rule)
A:jax.lax.lax.pad_cotangent->pad(cotangent, _zero(cotangent), padding_config)
A:jax.lax.lax.reduce_window_sum_p->standard_primitive(_reduce_window_sum_shape_rule, _input_dtype, 'reduce_window_sum', _reduce_window_sum_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_sum_p]->partial(_reduce_window_batch_rule, _reduce_window_sum)
A:jax.lax.lax.operand_shape->_dilate_shape(operand_shape, base_dilation)
A:jax.lax.lax.window_dimensions->_dilate_shape(window_dimensions, window_dilation)
A:jax.lax.lax.operand_padded->numpy.add(operand_shape, np.add(*zip(*padding)))
A:jax.lax.lax._reduce_window_max_translation_rule->partial(_reduce_window_chooser_translation_rule, max_p, _get_max_identity)
A:jax.lax.lax.reduce_window_max_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_max', _reduce_window_max_translation_rule)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_max_p]->partial(_reduce_window_batch_rule, _reduce_window_max)
A:jax.lax.lax._reduce_window_min_translation_rule->partial(_reduce_window_chooser_translation_rule, min_p, _get_min_identity)
A:jax.lax.lax.reduce_window_min_p->standard_primitive(_common_reduce_window_shape_rule, _input_dtype, 'reduce_window_min', _reduce_window_min_translation_rule)
A:jax.lax.lax._reduce_window_min_batch_rule->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.batching.primitive_batchers[reduce_window_min_p]->partial(_reduce_window_batch_rule, _reduce_window_min)
A:jax.lax.lax.select->interpreters.xla.primitive_subcomputation(select_prim, scalar, scalar)
A:jax.lax.lax.scatter->interpreters.xla.primitive_subcomputation(add_p, scalar, scalar)
A:jax.lax.lax.select_and_scatter_p->standard_primitive(_select_and_scatter_shape_rule, _input_dtype, 'select_and_scatter', _select_and_scatter_translation)
A:jax.lax.lax.source_t->_select_and_gather_add(t, operand, select_prim, window_dimensions, window_strides, padding, ones, ones)
A:jax.lax.lax.source->interpreters.batching.moveaxis(source, s_bdims, 0)
A:jax.lax.lax.select_and_scatter_add_p->standard_primitive(_select_and_scatter_add_shape_rule, _input_dtype, 'select_and_scatter_add', _select_and_scatter_add_translation)
A:jax.lax.lax.etype->tuple((xla.aval_to_xla_shape(x).with_major_to_minor_layout_if_absent() for x in shapes)).xla_element_type()
A:jax.lax.lax.word_type->lib.xla_client.dtype_to_etype(word_dtype)
A:jax.lax.lax.double_word_type->lib.xla_client.dtype_to_etype(double_word_dtype)
A:jax.lax.lax.a->convert_element_type(a, b_dtype)
A:jax.lax.lax.b->convert_element_type(b, a_dtype)
A:jax.lax.lax.st->xops.And(t, const(c, word_dtype, (1 << r_nbits) - 1 << r_nbits))
A:jax.lax.lax.c->lib.xla_bridge.make_computation_builder('select_and_gather_pair_reducer')
A:jax.lax.lax.has_base_dilation->any((d != 1 for d in base_dilation))
A:jax.lax.lax.t->xops.Tuple(c, xs)
A:jax.lax.lax.select_and_gather_add_p->standard_primitive(_select_and_gather_add_shape_rule, _input_dtype, 'select_and_gather_add', _select_and_gather_add_translation)
A:jax.lax.lax.xla.backend_specific_translations['tpu'][select_and_gather_add_p]->partial(_select_and_gather_add_translation, max_bits=32)
A:jax.lax.lax.w->pad(w, _const(x, 0), pad_left)
A:jax.lax.lax.nbits->numpy.prod(input_shape[list(axes)]).bit_length()
A:jax.lax.lax.(x, total)->_prescan_power_of_two(x, axis, op, unit)
A:jax.lax.lax._cumsum_prefix_scan->partial(_parallel_prefix_scan, op=add, unit=0)
A:jax.lax.lax._cumprod_prefix_scan->partial(_parallel_prefix_scan, op=mul, unit=1)
A:jax.lax.lax._cummax_prefix_scan->partial(_parallel_prefix_scan, op=max, unit=-np.inf)
A:jax.lax.lax._cummin_prefix_scan->partial(_parallel_prefix_scan, op=min, unit=np.inf)
A:jax.lax.lax.cumsum_p->standard_primitive(_cumred_shape_rule, partial(_reduce_number_dtype_rule, 'cumsum'), 'cumsum', xla.lower_fun(_cumsum_prefix_scan, multiple_results=False))
A:jax.lax.lax.xla.backend_specific_translations['tpu'][cumsum_p]->interpreters.xla.lower_fun(partial(_cumred_tpu_translation_rule, _reduce_window_sum), multiple_results=False)
A:jax.lax.lax.batching.primitive_batchers[cumsum_p]->partial(_cumred_batch_rule, cumsum_p)
A:jax.lax.lax.reducer_p->standard_primitive(_cumred_shape_rule, partial(_reduce_number_dtype_rule, name), name, xla.lower_fun(prefix_scan_fn, multiple_results=False))
A:jax.lax.lax.xla.backend_specific_translations['tpu'][reducer_p]->interpreters.xla.lower_fun(partial(_cumred_tpu_translation_rule, reduce_window_fn), multiple_results=False)
A:jax.lax.lax.batching.primitive_batchers[reducer_p]->partial(_cumred_batch_rule, reducer_p)
A:jax.lax.lax.cumprod_p->_cumulative_reduction_primitive('cumprod', _cumprod_prefix_scan, partial(_cumulative_jvp_rule, prefix_scan=_cumprod_prefix_scan), _reduce_window_prod)
A:jax.lax.lax.cummax_p->_cumulative_reduction_primitive('cummax', _cummax_prefix_scan, partial(_cumulative_jvp_rule, prefix_scan=_cummax_prefix_scan), _reduce_window_max)
A:jax.lax.lax.cummin_p->_cumulative_reduction_primitive('cummin', _cummin_prefix_scan, partial(_cumulative_jvp_rule, prefix_scan=_cummin_prefix_scan), _reduce_window_min)
A:jax.lax.lax.args->tuple((raise_to_shaped(arg) for arg in args))
A:jax.lax.lax.signed->bitcast_convert_type(x, signed_dtype)
A:jax.lax.lax.unsigned->bitcast_convert_type(x, unsigned_dtype)
A:jax.lax.lax.flipped->bitcast_convert_type(sub(unsigned_dtype(np.iinfo(signed_dtype).max), unsigned), signed_dtype)
A:jax.lax.lax.primals->Primitive('sort').bind(*primals + (iotas[dimension],), dimension=dimension, is_stable=is_stable, num_keys=num_keys)
A:jax.lax.lax.idx->tuple((primals[-1] if i == dimension else iotas[i] for i in range(len(shape))))
A:jax.lax.lax.tangents_out->tuple((t if type(t) is ad_util.Zero else t[idx] for t in tangents))
A:jax.lax.lax.(prototype_arg, new_bdim)->next(((a, b) for (a, b) in zip(batched_args, batch_dims) if b is not None))
A:jax.lax.lax.sort_p->Primitive('sort')
A:jax.lax.lax.primals_out->top_k(operand, k)
A:jax.lax.lax.rank->len(idx_shape)
A:jax.lax.lax._iota->broadcast_in_dim(_iota, gather_index_shape, (i,))
A:jax.lax.lax.gather_indices->concatenate(gather_indices, dimension=rank)
A:jax.lax.lax.perm->numpy.arange(operand.ndim)
A:jax.lax.lax.(top_k_v, top_k_i)->top_k(transpose(operand, perm), k=k)
A:jax.lax.lax.top_k_p->Primitive('top_k')
A:jax.lax.lax.xla.translations[top_k_p]->partial(standard_translate, 'top_k')
A:jax.lax.lax.tie_in_p->Primitive('tie_in')
A:jax.lax.lax.create_token_p->Primitive('create_token')
A:jax.lax.lax.after_all_p->Primitive('after_all')
A:jax.lax.lax.(flat_shapes, treedef)->lib.pytree.flatten(shape)
A:jax.lax.lax.xs_and_token->build_infeed()
A:jax.lax.lax.build_infeed->partial(xops.InfeedWithToken, token, xla_client.Shape.tuple_shape(shape))
A:jax.lax.lax.xs->xops.GetTupleElement(xs_and_token, 0)
A:jax.lax.lax.token->xops.GetTupleElement(xs_and_token, 1)
A:jax.lax.lax.infeed_p->Primitive('infeed')
A:jax.lax.lax.(flat_xs, _)->lib.pytree.flatten(xs)
A:jax.lax.lax.outfeed_p->Primitive('outfeed')
A:jax.lax.lax.xla_shape->xc.Shape.array_shape(c.get_shape(a).xla_element_type(), shape)
A:jax.lax.lax.rng_uniform_p->Primitive('rng_uniform')
A:jax.lax.lax.pad_sizes->numpy.maximum(0, (out_shape - 1) * window_strides + window_shape - in_shape)
A:jax.lax.lax.types->list(map(np.dtype, ttypes))
A:jax.lax.lax.lhs_padded->numpy.add(lhs_shape[2:], np.sum(np.array(pads).reshape(-1, 2), axis=1))
A:jax.lax.lax.out_space->numpy.sum([unpad_out_space, padding], axis=0).tolist()
A:jax.lax.lax.(lhs_perm, rhs_perm, out_perm)->map(getperm, dimension_numbers, charpairs)
A:jax.lax.lax.obj_arr->numpy.array(obj)
A:jax.lax.lax.x_len->len(x)
A:jax.lax.lax.removed->set(itertools.chain(*removed_lists))
A:jax.lax.lax.(lhs_spec, rhs_spec, out_spec)->conv_general_permutations(dimension_numbers)
A:jax.lax.lax.spatial->sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))
A:jax.lax.lax.lhs_dilated_shape->_dilate_shape(in_shape, lhs_dilation)
A:jax.lax.lax.rhs_dilated_shape->_dilate_shape(window_dimensions, rhs_dilation)
A:jax.lax.lax.out_dilated_shape->_dilate_shape(out_shape, window_strides)
A:jax.lax.lax.higher_dtype->dtypes.promote_types(a_dtype, b_dtype)
A:jax.lax.lax.np_dtype->numpy.dtype(dtype)
jax.lax.ConvDimensionNumbers(NamedTuple)
jax.lax.GatherDimensionNumbers(NamedTuple)
jax.lax.ScatterDimensionNumbers(NamedTuple)
jax.lax._broadcasting_select(c,which,x,y)
jax.lax._broadcasting_shape_rule(name,*avals)
jax.lax._canonicalize_axis(axis,num_dims)
jax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax._const(example,val)
jax.lax._delta(dtype:DType,shape:Shape,axes:Sequence[int])->Array
jax.lax._eq_meet(a,b)
jax.lax._eye(dtype:DType,shape:Shape,offset:int)->Array
jax.lax._reduce_and(operand:Array,axes:Sequence[int])->Array
jax.lax._reduce_max(operand:Array,axes:Sequence[int])->Array
jax.lax._reduce_min(operand:Array,axes:Sequence[int])->Array
jax.lax._reduce_or(operand:Array,axes:Sequence[int])->Array
jax.lax._reduce_sum(operand:Array,axes:Sequence[int])->Array
jax.lax._reduce_window_max(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax._reduce_window_min(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax._reduce_window_prod(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax._reduce_window_sum(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax._select_and_gather_add(tangents:Array,operand:Array,select_prim:core.Primitive,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Sequence[int],window_dilation:Sequence[int])->Array
jax.lax._tri(dtype:DType,shape:Shape,offset:int)->Array
jax.lax._upcast_fp16_for_computation(f)
jax.lax.abs(x:Array)->Array
jax.lax.acos(x:Array)->Array
jax.lax.acosh(x:Array)->Array
jax.lax.add(x:Array,y:Array)->Array
jax.lax.after_all(*operands)
jax.lax.argmax(operand:Array,axis:int,index_dtype:DType)->Tuple[Array, Array]
jax.lax.argmin(operand:Array,axis:int,index_dtype:DType)->Tuple[Array, Array]
jax.lax.asin(x:Array)->Array
jax.lax.asinh(x:Array)->Array
jax.lax.atan(x:Array)->Array
jax.lax.atan2(x:Array,y:Array)->Array
jax.lax.atanh(x:Array)->Array
jax.lax.batch_matmul(lhs:Array,rhs:Array,precision:Optional[PrecisionType]=None)->Array
jax.lax.bessel_i0e(x:Array)->Array
jax.lax.bessel_i1e(x:Array)->Array
jax.lax.betainc(a:Array,b:Array,x:Array)->Array
jax.lax.bitcast_convert_type(operand:Array,new_dtype:DType)->Array
jax.lax.bitwise_and(x:Array,y:Array)->Array
jax.lax.bitwise_not(x:Array)->Array
jax.lax.bitwise_or(x:Array,y:Array)->Array
jax.lax.bitwise_xor(x:Array,y:Array)->Array
jax.lax.broadcast(operand:Array,sizes:Sequence[int])->Array
jax.lax.broadcast_in_dim(operand:Array,shape:Shape,broadcast_dimensions:Sequence[int])->Array
jax.lax.broadcast_shapes(*shapes)
jax.lax.broadcast_to_rank(x:Array,rank:int)->Array
jax.lax.broadcasted_iota(dtype:DType,shape:Shape,dimension:int)->Array
jax.lax.ceil(x:Array)->Array
jax.lax.clamp(min:Array,x:Array,max:Array)->Array
jax.lax.collapse(operand:Array,start_dimension:int,stop_dimension:int)->Array
jax.lax.complex(x:Array,y:Array)->Array
jax.lax.concatenate(operands:Sequence[Array],dimension:int)->Array
jax.lax.conj(x:Array)->Array
jax.lax.conv(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:str,precision:Optional[PrecisionType]=None)->Array
jax.lax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.lax.conv_general_dilated(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,feature_group_count:int=1,batch_group_count:int=1,precision:Optional[PrecisionType]=None)->Array
jax.lax.conv_general_permutations(dimension_numbers)
jax.lax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads,batch_group_count=1)
jax.lax.conv_transpose(lhs:Array,rhs:Array,strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,transpose_kernel:bool=False,precision:Optional[PrecisionType]=None)->Array
jax.lax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.conv_with_general_padding(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]],rhs_dilation:Optional[Sequence[int]],precision:Optional[PrecisionType]=None)->Array
jax.lax.convert_element_type(operand:Array,new_dtype:DType)->Array
jax.lax.cos(x:Array)->Array
jax.lax.cosh(x:Array)->Array
jax.lax.create_token(x)
jax.lax.cummax(operand:Array,axis:int)->Array
jax.lax.cummin(operand:Array,axis:int)->Array
jax.lax.cumprod(operand:Array,axis:int)->Array
jax.lax.cumsum(operand:Array,axis:int)->Array
jax.lax.digamma(x:Array)->Array
jax.lax.div(x:Array,y:Array)->Array
jax.lax.dot(lhs:Array,rhs:Array,precision:Optional[PrecisionType]=None)->Array
jax.lax.dot_general(lhs:Array,rhs:Array,dimension_numbers:DotDimensionNumbers,precision:Optional[PrecisionType]=None)->Array
jax.lax.dynamic_index_in_dim(operand:Array,index:Array,axis:int=0,keepdims:bool=True)->Array
jax.lax.dynamic_slice(operand:Array,start_indices:Sequence[Array],slice_sizes:Shape)->Array
jax.lax.dynamic_slice_in_dim(operand:Array,start_index:Array,slice_size:int,axis:int=0)->Array
jax.lax.dynamic_update_index_in_dim(operand:Array,update:Array,index:Array,axis:int)->Array
jax.lax.dynamic_update_slice(operand:Array,update:Array,start_indices:Array)->Array
jax.lax.dynamic_update_slice_in_dim(operand:Array,update:Array,start_index:Array,axis:int)->Array
jax.lax.eq(x:Array,y:Array)->Array
jax.lax.erf(x:Array)->Array
jax.lax.erf_inv(x:Array)->Array
jax.lax.erfc(x:Array)->Array
jax.lax.exp(x:Array)->Array
jax.lax.expand_dims(array:Array,dimensions:Tuple[int,...])->Array
jax.lax.expm1(x:Array)->Array
jax.lax.floor(x:Array)->Array
jax.lax.full(shape:Shape,fill_value:Array,dtype:Optional[DType]=None)->Array
jax.lax.full_like(x:Array,fill_value:Array,dtype:Optional[DType]=None,shape:Optional[Shape]=None)->Array
jax.lax.gather(operand:Array,start_indices:Array,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape)->Array
jax.lax.ge(x:Array,y:Array)->Array
jax.lax.gt(x:Array,y:Array)->Array
jax.lax.igamma(a:Array,x:Array)->Array
jax.lax.igamma_grad_a(a:Array,x:Array)->Array
jax.lax.igammac(a:Array,x:Array)->Array
jax.lax.imag(x:Array)->Array
jax.lax.index_in_dim(operand:Array,index:int,axis:int=0,keepdims:bool=True)->Array
jax.lax.index_take(src:Array,idxs:Array,axes:Sequence[int])->Array
jax.lax.infeed(token,shape=None,partitions=None)
jax.lax.integer_pow(x:Array,y:int)->Array
jax.lax.iota(dtype:DType,size:int)->Array
jax.lax.is_finite(x:Array)->Array
jax.lax.lax.ConvDimensionNumbers(NamedTuple)
jax.lax.lax.GatherDimensionNumbers(NamedTuple)
jax.lax.lax.ScatterDimensionNumbers(NamedTuple)
jax.lax.lax._abs_jvp_rule(g,ans,x)
jax.lax.lax._abstractify(x)
jax.lax.lax._add_inverse(r,x,y)
jax.lax.lax._add_transpose(t,x,y)
jax.lax.lax._after_all_abstract_eval(*operands)
jax.lax.lax._after_all_translation_rule(c,*operands)
jax.lax.lax._argminmax_dtype_rule(operand,*,axes,index_dtype)
jax.lax.lax._argminmax_gpu_translation_rule(op,a,*,axes,index_dtype)
jax.lax.lax._argminmax_shape_rule(operand,*,axes,index_dtype)
jax.lax.lax._argminmax_translation_rule(value_comparator,identity,c,operand,*,axes,index_dtype)
jax.lax.lax._balanced_eq(x,z,y)
jax.lax.lax._batch_dynamic_slice_indices(indices,bdims)
jax.lax.lax._bessel_i1e_jvp(g,y,x)
jax.lax.lax._bitcast_convert_type_dtype_rule(operand,*,new_dtype)
jax.lax.lax._bitcast_convert_type_shape_rule(operand,*,new_dtype)
jax.lax.lax._bitcast_convert_type_translation_rule(c,operand,*,new_dtype)
jax.lax.lax._brcast(x,*others)
jax.lax.lax._brcast_to(x,shape)
jax.lax.lax._broadcast_batch_rule(batched_args,batch_dims,*,sizes)
jax.lax.lax._broadcast_in_dim_batch_rule(batched_args,batch_dims,*,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_impl(operand,*,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_shape_rule(operand,*,shape,broadcast_dimensions)
jax.lax.lax._broadcast_in_dim_transpose_rule(t,*,shape,broadcast_dimensions)
jax.lax.lax._broadcast_shape_rule(operand,sizes)
jax.lax.lax._broadcast_translate(translate:Callable)
jax.lax.lax._broadcasting_select(c,which,x,y)
jax.lax.lax._broadcasting_shape_rule(name,*avals)
jax.lax.lax._canonicalize_axis(axis,num_dims)
jax.lax.lax._canonicalize_precision(precision)
jax.lax.lax._ceil_divide(x1,x2)
jax.lax.lax._check_conv_shapes(name,lhs_shape,rhs_shape,window_strides)
jax.lax.lax._check_same_dtypes(name,ignore_fp_precision,*ttypes)
jax.lax.lax._check_shapelike(fun_name,arg_name,obj)
jax.lax.lax._check_user_dtype_supported(dtype,fun_name=None)
jax.lax.lax._clamp_shape_rule(min,operand,max)
jax.lax.lax._common_reduce_window_shape_rule(operand,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._compute_squeeze_shape(shape,dimensions)
jax.lax.lax._concatenate_batch_rule(batched_args,batch_dims,*,dimension)
jax.lax.lax._concatenate_dtype_rule(*operands,**kwargs)
jax.lax.lax._concatenate_shape_rule(*operands,**kwargs)
jax.lax.lax._concatenate_translation_rule(c,*operands,**kwargs)
jax.lax.lax._concatenate_transpose_rule(t,*operands,dimension)
jax.lax.lax._conj_transpose_rule(t,x,*,input_dtype)
jax.lax.lax._const(example,val)
jax.lax.lax._conv_general_dilated_batch_rule(batched_args,batch_dims,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,**unused_kwargs)
jax.lax.lax._conv_general_dilated_dtype_rule(lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,**unused_kwargs)
jax.lax.lax._conv_general_dilated_masking_rule(padded_vals,logical_shapes,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_dilated_shape_rule(lhs:ShapedArray,rhs:ShapedArray,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,**unused_kwargs)->Tuple[int, ...]
jax.lax.lax._conv_general_dilated_translation_rule(c,lhs,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,precision,expand_complex_convolutions,**unused_kwargs)
jax.lax.lax._conv_general_dilated_transpose_lhs(g,rhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,batch_group_count,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_dilated_transpose_rhs(g,lhs,*,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers:ConvDimensionNumbers,feature_group_count:int,batch_group_count:int,lhs_shape,rhs_shape,precision)
jax.lax.lax._conv_general_proto(dimension_numbers)
jax.lax.lax._conv_general_vjp_lhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)->List[Tuple[int, int]]
jax.lax.lax._conv_general_vjp_rhs_padding(in_shape,window_dimensions,window_strides,out_shape,padding,lhs_dilation,rhs_dilation)
jax.lax.lax._conv_transpose_padding(k,s,padding)
jax.lax.lax._convert_element_type_dtype_rule(operand,*,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_shape_rule(operand,*,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_translation_rule(c,operand,*,new_dtype,old_dtype)
jax.lax.lax._convert_element_type_transpose_rule(t,*,new_dtype,old_dtype)
jax.lax.lax._cumred_batch_rule(prim,batched_args,batch_dims,*,axis:int)
jax.lax.lax._cumred_shape_rule(x,*,axis:int)
jax.lax.lax._cumred_tpu_translation_rule(window_reduce:Callable,x,*,axis:int)
jax.lax.lax._cumsum_transpose_rule(t,*,axis:int)
jax.lax.lax._cumulative_jvp_rule(primals,tangents,*,axis:int,prefix_scan:Callable)
jax.lax.lax._cumulative_reduction_primitive(name,prefix_scan_fn,jvp_rule,reduce_window_fn)
jax.lax.lax._delta(dtype:DType,shape:Shape,axes:Sequence[int])->Array
jax.lax.lax._device_put_raw(x)
jax.lax.lax._dilate_shape(shape,dilation)
jax.lax.lax._div_transpose_rule(cotangent,x,y)
jax.lax.lax._dot_general_batch_rule(batched_args,batch_dims,*,dimension_numbers,precision)
jax.lax.lax._dot_general_dtype_rule(lhs,rhs,*,dimension_numbers,precision)
jax.lax.lax._dot_general_masking_rule(padded_vals,logical_shapes,*,dimension_numbers,precision)
jax.lax.lax._dot_general_shape_rule(lhs,rhs,*,dimension_numbers,precision)
jax.lax.lax._dot_general_translation_rule(c,lhs,rhs,*,dimension_numbers,precision)
jax.lax.lax._dot_general_transpose_lhs(g,y,*,dimension_numbers,precision,swap_ans=False)
jax.lax.lax._dot_general_transpose_rhs(g,x,*,dimension_numbers,precision)
jax.lax.lax._dot_using_sum_of_products(lhs,rhs,*,dimension_numbers)
jax.lax.lax._dynamic_slice_batching_rule(batched_args,batch_dims,*,slice_sizes)
jax.lax.lax._dynamic_slice_dtype_rule(operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_indices(operand,start_indices)
jax.lax.lax._dynamic_slice_jvp(primals,tangents,*,slice_sizes)
jax.lax.lax._dynamic_slice_shape_rule(operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_translation_rule(c,operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_slice_transpose_rule(t,operand,*start_indices,slice_sizes)
jax.lax.lax._dynamic_update_slice_batching_rule(batched_args,batch_dims)
jax.lax.lax._dynamic_update_slice_dtype_rule(operand,update,*start_indices)
jax.lax.lax._dynamic_update_slice_jvp(primals,tangents)
jax.lax.lax._dynamic_update_slice_shape_rule(operand,update,*start_indices)
jax.lax.lax._dynamic_update_slice_translation_rule(c,operand,update,*start_indices)
jax.lax.lax._dynamic_update_slice_transpose_rule(t,operand,update,*start_indices)
jax.lax.lax._eq_meet(a,b)
jax.lax.lax._eye(dtype:DType,shape:Shape,offset:int)->Array
jax.lax.lax._flip_axes(x,axes)
jax.lax.lax._float_to_int_for_sort(x)
jax.lax.lax._gather_batching_rule(batched_args,batch_dims,*,dimension_numbers,slice_sizes)
jax.lax.lax._gather_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._gather_dtype_rule(operand,start_indices,**kwargs)
jax.lax.lax._gather_jvp_rule(g,operand,start_indices,*,dimension_numbers,slice_sizes)
jax.lax.lax._gather_shape_rule(operand,start_indices,*,dimension_numbers,slice_sizes)
jax.lax.lax._gather_translation_rule(c,operand,start_indices,*,dimension_numbers,slice_sizes)
jax.lax.lax._gather_transpose_rule(t,operand,start_indices,*,dimension_numbers,slice_sizes)
jax.lax.lax._generic_reduce_window_batch_rule(batched_args,batch_dims,*,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._get_max_identity(dtype:DType)->Array
jax.lax.lax._get_min_identity(dtype:DType)->Array
jax.lax.lax._get_monoid_reducer(monoid_op:Callable,x:Array)->Optional[Callable]
jax.lax.lax._get_monoid_window_reducer(monoid_op:Callable,x:Array)->Optional[Callable]
jax.lax.lax._identity(x)
jax.lax.lax._infeed_abstract_eval(token,*,shapes,partitions)
jax.lax.lax._infeed_translation_rule(c,token,*,shapes,partitions)
jax.lax.lax._integer_pow_dtype_rule(x,*,y)
jax.lax.lax._integer_pow_jvp(g,x,*,y)
jax.lax.lax._integer_pow_translation_rule(c,x,*,y)
jax.lax.lax._is_singleton_reshape(old,new)
jax.lax.lax._iscomplex(x)->bool
jax.lax.lax._iter(tracer)
jax.lax.lax._masked(padded_value,logical_shape,dimensions,value=0)
jax.lax.lax._masking_defreducer(prim,identity)
jax.lax.lax._minmax_translation_rule(c,x,y,*,minmax=None,cmp=None)
jax.lax.lax._mul_inverse(r,x,y)
jax.lax.lax._outfeed_abstract_eval(token,*xs)
jax.lax.lax._outfeed_translation_rule(c,token,*xs)
jax.lax.lax._pad_batch_rule(batched_args,batch_dims,*,padding_config)
jax.lax.lax._pad_dtype_rule(operand,padding_value,*,padding_config)
jax.lax.lax._pad_masking_rule(padded_vals,logical_shapes,padding_config)
jax.lax.lax._pad_shape_rule(operand,padding_value,*,padding_config)
jax.lax.lax._pad_translation_rule(c,operand,padding_value,*,padding_config)
jax.lax.lax._pad_transpose(t,operand,padding_value,*,padding_config)
jax.lax.lax._parallel_prefix_scan(x,axis:int,op:Callable,unit:Any)
jax.lax.lax._pow_jvp_lhs(g,ans,x,y)
jax.lax.lax._pow_jvp_rhs(g,ans,x,y)
jax.lax.lax._precision_config(precision)
jax.lax.lax._prescan_power_of_two(x,axis:int,op:Callable,unit)
jax.lax.lax._reduce_and(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_batch_rule(batched_args,batch_dims,*,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_chooser_jvp_rule(g,ans,operand,*,axes)
jax.lax.lax._reduce_chooser_shape_rule(operand,*,axes)
jax.lax.lax._reduce_chooser_translation_rule(prim,identity,c,operand,*,axes)
jax.lax.lax._reduce_logical_shape_rule(operand,*,axes)
jax.lax.lax._reduce_logical_translation_rule(prim,identity,c,operand,*,axes)
jax.lax.lax._reduce_max(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_min(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_number_dtype_rule(name,operand,*args,**kw)
jax.lax.lax._reduce_op_shape_rule(operand,*,axes,input_shape=None)
jax.lax.lax._reduce_or(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_prod(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_prod_jvp_rule(primals,tangents,*,axes)
jax.lax.lax._reduce_prod_translation_rule(c,operand,*,axes)
jax.lax.lax._reduce_shape_rule(operand,init_value,*,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_sum(operand:Array,axes:Sequence[int])->Array
jax.lax.lax._reduce_sum_shape_rule(operand,*,axes)
jax.lax.lax._reduce_sum_translation_rule(c,operand,*,axes)
jax.lax.lax._reduce_sum_transpose_rule(cotangent,operand,*,axes)
jax.lax.lax._reduce_translation_rule(c,operand,init_value,*,computation,jaxpr,consts,dimensions)
jax.lax.lax._reduce_window_batch_rule(reduce_window,batched_args,bdims,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_chooser_jvp_rule(prim,g,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_chooser_translation_rule(prim,identity,c,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_max(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.lax._reduce_window_min(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.lax._reduce_window_prod(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.lax._reduce_window_shape_rule(operand,init_value,*,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_sum(operand:Array,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.lax._reduce_window_sum_shape_rule(operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_sum_translation_rule(c,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_sum_transpose_rule(cotangent,operand,*,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reduce_window_translation_rule(c,operand,init_value,*,jaxpr,consts,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._reducer_masking_rule(prim,identity,padded_vals,logical_shapes,axes,input_shape=None)
jax.lax.lax._reduction_computation(c,jaxpr,consts,init_value)
jax.lax.lax._reduction_jaxpr(computation,aval)
jax.lax.lax._reshape_axis_into(src,dst,x)
jax.lax.lax._reshape_axis_out_of(src,size1,x)
jax.lax.lax._reshape_batch_rule(batched_args,batch_dims,*,new_sizes,dimensions)
jax.lax.lax._reshape_dtype_rule(operand,*,new_sizes,dimensions)
jax.lax.lax._reshape_impl(operand,*,new_sizes,dimensions)
jax.lax.lax._reshape_shape_rule(operand,*,new_sizes,dimensions)
jax.lax.lax._reshape_translation_rule(c,operand,*,new_sizes,dimensions)
jax.lax.lax._reshape_transpose_rule(t,operand,*,new_sizes,dimensions)
jax.lax.lax._rev_batch_rule(batched_args,batch_dims,*,dimensions)
jax.lax.lax._rev_shape_rule(operand,*,dimensions)
jax.lax.lax._rng_uniform_abstract_eval(a,b,*,shape)
jax.lax.lax._rng_uniform_translation_rule(c,a,b,*,shape)
jax.lax.lax._scatter_add_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_add_transpose_rule(t,operand,scatter_indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_batching_rule(scatter_op,batched_args,batch_dims,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_dimensions_proto(indices_shape,dimension_numbers)
jax.lax.lax._scatter_dtype_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_extremal_jvp(scatter_op,primals,tangents,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_jvp(primals,tangents,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_mul_jvp_rhs(g,x,i,y,*,dimension_numbers,indices_are_sorted,unique_indices,**kw)
jax.lax.lax._scatter_mul_transpose_rule(t,operand,scatter_indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._scatter_shape_rule(operand,scatter_indices,updates,**kwargs)
jax.lax.lax._scatter_translation_rule(c,operand,scatter_indices,updates,*,update_jaxpr,update_consts,dimension_numbers,indices_are_sorted,unique_indices)
jax.lax.lax._select_and_gather_add(tangents:Array,operand:Array,select_prim:core.Primitive,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],base_dilation:Sequence[int],window_dilation:Sequence[int])->Array
jax.lax.lax._select_and_gather_add_batching_rule(batched_args,batch_dims,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._select_and_gather_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._select_and_gather_add_shape_rule(tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._select_and_gather_add_translation(c,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation,max_bits=64)
jax.lax.lax._select_and_gather_add_transpose(t,tangents,operand,*,select_prim,window_dimensions,window_strides,padding,base_dilation,window_dilation)
jax.lax.lax._select_and_scatter(operand:Array,select:Callable,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]],source:Array,init_value:Array,scatter:Callable,base_dilation:Sequence[int],window_dilation:Sequence[int])->Array
jax.lax.lax._select_and_scatter_add(source:Array,operand:Array,select_prim:core.Primitive,window_dimensions:Shape,window_strides:Sequence[int],padding:Sequence[Tuple[int,int]])->Array
jax.lax.lax._select_and_scatter_add_batch_rule(batched_args,batch_dims,**kwargs)
jax.lax.lax._select_and_scatter_add_jvp(primals,tangents,*,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_shape_rule(source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_translation(c,source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_add_transpose(t,source,operand,*,select_prim,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_shape_rule(operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.lax.lax._select_and_scatter_translation(c,operand,source,init_value,*,select_jaxpr,select_consts,scatter_jaxpr,scatter_consts,window_dimensions,window_strides,padding)
jax.lax.lax._select_batch_rule(batched_args,batch_dims,**unused_kwargs)
jax.lax.lax._select_dtype_rule(pred,on_true,on_false)
jax.lax.lax._select_masking_rule(padded_vals,logical_shapes)
jax.lax.lax._select_shape_rule(pred,on_true,on_false)
jax.lax.lax._select_transpose_rule(t,pred,on_true,on_false)
jax.lax.lax._sign_translation_rule(c,x)
jax.lax.lax._slice_batching_rule(batched_args,batch_dims,*,start_indices,limit_indices,strides)
jax.lax.lax._slice_masking_rule(padded_vals,logical_shapes,start_indices,limit_indices,strides)
jax.lax.lax._slice_shape_rule(operand,*,start_indices,limit_indices,strides)
jax.lax.lax._slice_translation_rule(c,operand,*,start_indices,limit_indices,strides)
jax.lax.lax._slice_transpose_rule(t,operand,*,start_indices,limit_indices,strides)
jax.lax.lax._sort_abstract_eval(*args,**kwargs)
jax.lax.lax._sort_batch_rule(batched_args,batch_dims,*,dimension,is_stable,num_keys)
jax.lax.lax._sort_jvp(primals,tangents,*,dimension,is_stable,num_keys)
jax.lax.lax._sort_lt_comparator(*operands,num_keys=1)
jax.lax.lax._sort_translation_rule(c,*operands,dimension,is_stable,num_keys)
jax.lax.lax._squeeze_batch_rule(batched_args,batch_dims,*,dimensions)
jax.lax.lax._squeeze_dtype_rule(operand,*,dimensions)
jax.lax.lax._squeeze_shape_rule(operand,*,dimensions)
jax.lax.lax._squeeze_translation_rule(c,arg,*,dimensions)
jax.lax.lax._squeeze_transpose_rule(t,operand,*,dimensions)
jax.lax.lax._stop_gradient_batch_rule(batched_args,batch_dims)
jax.lax.lax._stop_gradient_jvp_rule(primals,tangents)
jax.lax.lax._sub_transpose(t,x,y)
jax.lax.lax._tie_in_batch_rule(batched_args,batch_dims)
jax.lax.lax._tie_in_impl(x,y)
jax.lax.lax._tie_in_transpose_rule(t,x,y)
jax.lax.lax._top_k_abstract_eval(operand,*,k)
jax.lax.lax._top_k_batch_rule(batched_args,batch_dims,*,k)
jax.lax.lax._top_k_jvp(primals,tangents,*,k)
jax.lax.lax._transpose_batch_rule(batched_args,batch_dims,*,permutation)
jax.lax.lax._transpose_impl(operand,*,permutation)
jax.lax.lax._transpose_masking_rule(padded_vals,logical_shapes,permutation)
jax.lax.lax._transpose_shape_rule(operand,*,permutation)
jax.lax.lax._tri(dtype:DType,shape:Shape,offset:int)->Array
jax.lax.lax._try_broadcast_shapes(shapes)
jax.lax.lax._upcast_fp16_for_computation(f)
jax.lax.lax.abs(x:Array)->Array
jax.lax.lax.acos(x:Array)->Array
jax.lax.lax.acosh(x:Array)->Array
jax.lax.lax.add(x:Array,y:Array)->Array
jax.lax.lax.after_all(*operands)
jax.lax.lax.argmax(operand:Array,axis:int,index_dtype:DType)->Tuple[Array, Array]
jax.lax.lax.argmin(operand:Array,axis:int,index_dtype:DType)->Tuple[Array, Array]
jax.lax.lax.asin(x:Array)->Array
jax.lax.lax.asinh(x:Array)->Array
jax.lax.lax.atan(x:Array)->Array
jax.lax.lax.atan2(x:Array,y:Array)->Array
jax.lax.lax.atanh(x:Array)->Array
jax.lax.lax.batch_matmul(lhs:Array,rhs:Array,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.bessel_i0e(x:Array)->Array
jax.lax.lax.bessel_i1e(x:Array)->Array
jax.lax.lax.betainc(a:Array,b:Array,x:Array)->Array
jax.lax.lax.betainc_grad_not_implemented(g,a,b,x)
jax.lax.lax.betainc_gradx(g,a,b,x)
jax.lax.lax.bitcast_convert_type(operand:Array,new_dtype:DType)->Array
jax.lax.lax.bitwise_and(x:Array,y:Array)->Array
jax.lax.lax.bitwise_not(x:Array)->Array
jax.lax.lax.bitwise_or(x:Array,y:Array)->Array
jax.lax.lax.bitwise_xor(x:Array,y:Array)->Array
jax.lax.lax.broadcast(operand:Array,sizes:Sequence[int])->Array
jax.lax.lax.broadcast_in_dim(operand:Array,shape:Shape,broadcast_dimensions:Sequence[int])->Array
jax.lax.lax.broadcast_shapes(*shapes)
jax.lax.lax.broadcast_to_rank(x:Array,rank:int)->Array
jax.lax.lax.broadcasted_iota(dtype:DType,shape:Shape,dimension:int)->Array
jax.lax.lax.ceil(x:Array)->Array
jax.lax.lax.clamp(min:Array,x:Array,max:Array)->Array
jax.lax.lax.collapse(operand:Array,start_dimension:int,stop_dimension:int)->Array
jax.lax.lax.complex(x:Array,y:Array)->Array
jax.lax.lax.concatenate(operands:Sequence[Array],dimension:int)->Array
jax.lax.lax.conj(x:Array)->Array
jax.lax.lax.conv(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:str,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.conv_dimension_numbers(lhs_shape,rhs_shape,dimension_numbers)
jax.lax.lax.conv_general_dilated(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]]=None,rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,feature_group_count:int=1,batch_group_count:int=1,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.conv_general_permutations(dimension_numbers)
jax.lax.lax.conv_general_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_shape_tuple(lhs_shape,rhs_shape,strides,pads,batch_group_count=1)
jax.lax.lax.conv_transpose(lhs:Array,rhs:Array,strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],rhs_dilation:Optional[Sequence[int]]=None,dimension_numbers:ConvGeneralDilatedDimensionNumbers=None,transpose_kernel:bool=False,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.conv_transpose_shape_tuple(lhs_shape,rhs_shape,window_strides,padding,dimension_numbers)
jax.lax.lax.conv_with_general_padding(lhs:Array,rhs:Array,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],lhs_dilation:Optional[Sequence[int]],rhs_dilation:Optional[Sequence[int]],precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.convert_element_type(operand:Array,new_dtype:DType)->Array
jax.lax.lax.cos(x:Array)->Array
jax.lax.lax.cosh(x:Array)->Array
jax.lax.lax.create_token(x)
jax.lax.lax.cummax(operand:Array,axis:int)->Array
jax.lax.lax.cummin(operand:Array,axis:int)->Array
jax.lax.lax.cumprod(operand:Array,axis:int)->Array
jax.lax.lax.cumsum(operand:Array,axis:int)->Array
jax.lax.lax.digamma(x:Array)->Array
jax.lax.lax.div(x:Array,y:Array)->Array
jax.lax.lax.dot(lhs:Array,rhs:Array,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.dot_general(lhs:Array,rhs:Array,dimension_numbers:DotDimensionNumbers,precision:Optional[PrecisionType]=None)->Array
jax.lax.lax.dynamic_index_in_dim(operand:Array,index:Array,axis:int=0,keepdims:bool=True)->Array
jax.lax.lax.dynamic_slice(operand:Array,start_indices:Sequence[Array],slice_sizes:Shape)->Array
jax.lax.lax.dynamic_slice_in_dim(operand:Array,start_index:Array,slice_size:int,axis:int=0)->Array
jax.lax.lax.dynamic_update_index_in_dim(operand:Array,update:Array,index:Array,axis:int)->Array
jax.lax.lax.dynamic_update_slice(operand:Array,update:Array,start_indices:Array)->Array
jax.lax.lax.dynamic_update_slice_in_dim(operand:Array,update:Array,start_index:Array,axis:int)->Array
jax.lax.lax.eq(x:Array,y:Array)->Array
jax.lax.lax.erf(x:Array)->Array
jax.lax.lax.erf_inv(x:Array)->Array
jax.lax.lax.erfc(x:Array)->Array
jax.lax.lax.exp(x:Array)->Array
jax.lax.lax.expand_dims(array:Array,dimensions:Tuple[int,...])->Array
jax.lax.lax.expm1(x:Array)->Array
jax.lax.lax.floor(x:Array)->Array
jax.lax.lax.full(shape:Shape,fill_value:Array,dtype:Optional[DType]=None)->Array
jax.lax.lax.full_like(x:Array,fill_value:Array,dtype:Optional[DType]=None,shape:Optional[Shape]=None)->Array
jax.lax.lax.gather(operand:Array,start_indices:Array,dimension_numbers:GatherDimensionNumbers,slice_sizes:Shape)->Array
jax.lax.lax.ge(x:Array,y:Array)->Array
jax.lax.lax.gt(x:Array,y:Array)->Array
jax.lax.lax.igamma(a:Array,x:Array)->Array
jax.lax.lax.igamma_grad_a(a:Array,x:Array)->Array
jax.lax.lax.igamma_grada(g,a,x)
jax.lax.lax.igamma_gradx(g,a,x)
jax.lax.lax.igammac(a:Array,x:Array)->Array
jax.lax.lax.igammac_grada(g,a,x)
jax.lax.lax.igammac_gradx(g,a,x)
jax.lax.lax.imag(x:Array)->Array
jax.lax.lax.index_in_dim(operand:Array,index:int,axis:int=0,keepdims:bool=True)->Array
jax.lax.lax.index_take(src:Array,idxs:Array,axes:Sequence[int])->Array
jax.lax.lax.infeed(token,shape=None,partitions=None)
jax.lax.lax.integer_pow(x:Array,y:int)->Array
jax.lax.lax.iota(dtype:DType,size:int)->Array
jax.lax.lax.is_finite(x:Array)->Array
jax.lax.lax.le(x:Array,y:Array)->Array
jax.lax.lax.lgamma(x:Array)->Array
jax.lax.lax.log(x:Array)->Array
jax.lax.lax.log1p(x:Array)->Array
jax.lax.lax.lt(x:Array,y:Array)->Array
jax.lax.lax.max(x:Array,y:Array)->Array
jax.lax.lax.min(x:Array,y:Array)->Array
jax.lax.lax.mul(x:Array,y:Array)->Array
jax.lax.lax.naryop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.lax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.lax.lax.ne(x:Array,y:Array)->Array
jax.lax.lax.neg(x:Array)->Array
jax.lax.lax.nextafter(x1:Array,x2:Array)->Array
jax.lax.lax.omnistaging_enabler()->None
jax.lax.lax.outfeed(token,xs)
jax.lax.lax.pad(operand:Array,padding_value:Array,padding_config:Sequence[Tuple[int,int,int]])->Array
jax.lax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.lax.lax.population_count(x:Array)->Array
jax.lax.lax.pow(x:Array,y:Array)->Array
jax.lax.lax.random_gamma_grad(a:Array,x:Array)->Array
jax.lax.lax.ranges_like(*xs)
jax.lax.lax.real(x:Array)->Array
jax.lax.lax.reciprocal(x:Array)->Array
jax.lax.lax.reduce(operand:Array,init_value:Array,computation:Callable,dimensions:Sequence[int])->Array
jax.lax.lax.reduce_window(operand:Array,init_value:Array,computation:Callable,window_dimensions:Shape,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.lax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding,base_dilation=None,window_dilation=None)
jax.lax.lax.rem(x:Array,y:Array)->Array
jax.lax.lax.remaining(original,*removed_lists)
jax.lax.lax.reshape(operand:Array,new_sizes:Shape,dimensions:Optional[Sequence[int]]=None)->Array
jax.lax.lax.rev(operand:Array,dimensions:Sequence[int])->Array
jax.lax.lax.rng_uniform(a,b,shape)
jax.lax.lax.round(x:Array)->Array
jax.lax.lax.rsqrt(x:Array)->Array
jax.lax.lax.scatter(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.lax.scatter_add(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.lax.scatter_max(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.lax.scatter_min(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.lax.scatter_mul(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.lax.select(pred:Array,on_true:Array,on_false:Array)->Array
jax.lax.lax.shift_left(x:Array,y:Array)->Array
jax.lax.lax.shift_right_arithmetic(x:Array,y:Array)->Array
jax.lax.lax.shift_right_logical(x:Array,y:Array)->Array
jax.lax.lax.sign(x:Array)->Array
jax.lax.lax.sin(x:Array)->Array
jax.lax.lax.sinh(x:Array)->Array
jax.lax.lax.slice(operand:Array,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)->Array
jax.lax.lax.slice_in_dim(operand:Array,start_index:Optional[int],limit_index:Optional[int],stride:int=1,axis:int=0)->Array
jax.lax.lax.sort(operand:Union[Array,Sequence[Array]],dimension:int=-1,is_stable:bool=True,num_keys:int=1)->Union[Array, Tuple[Array, ...]]
jax.lax.lax.sort_key_val(keys:Array,values:Array,dimension:int=-1,is_stable:bool=True)->Tuple[Array, Array]
jax.lax.lax.sqrt(x:Array)->Array
jax.lax.lax.square(x:Array)->Array
jax.lax.lax.squeeze(array:Array,dimensions:Tuple[int,...])->Array
jax.lax.lax.standard_abstract_eval(prim,shape_rule,dtype_rule,*args,**kwargs)
jax.lax.lax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.lax.lax.standard_translate(name,c,*args,**kwargs)
jax.lax.lax.stop_gradient(x)
jax.lax.lax.sub(x:Array,y:Array)->Array
jax.lax.lax.tan(x:Array)->Array
jax.lax.lax.tanh(x:Array)->Array
jax.lax.lax.tie_in(x:Array,y:Array)->Array
jax.lax.lax.top_k(operand:Array,k:int)->Tuple[Array, Array]
jax.lax.lax.transpose(operand:Array,permutation:Sequence[int])->Array
jax.lax.lax.unop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.lax.lax.zeros_like_array(x)
jax.lax.le(x:Array,y:Array)->Array
jax.lax.lgamma(x:Array)->Array
jax.lax.log(x:Array)->Array
jax.lax.log1p(x:Array)->Array
jax.lax.lt(x:Array,y:Array)->Array
jax.lax.max(x:Array,y:Array)->Array
jax.lax.min(x:Array,y:Array)->Array
jax.lax.mul(x:Array,y:Array)->Array
jax.lax.naryop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.naryop_dtype_rule(result_dtype,accepted_dtypes,name,*avals,**kwargs)
jax.lax.ne(x:Array,y:Array)->Array
jax.lax.neg(x:Array)->Array
jax.lax.nextafter(x1:Array,x2:Array)->Array
jax.lax.outfeed(token,xs)
jax.lax.pad(operand:Array,padding_value:Array,padding_config:Sequence[Tuple[int,int,int]])->Array
jax.lax.padtype_to_pads(in_shape,window_shape,window_strides,padding)
jax.lax.population_count(x:Array)->Array
jax.lax.pow(x:Array,y:Array)->Array
jax.lax.random_gamma_grad(a:Array,x:Array)->Array
jax.lax.real(x:Array)->Array
jax.lax.reciprocal(x:Array)->Array
jax.lax.reduce(operand:Array,init_value:Array,computation:Callable,dimensions:Sequence[int])->Array
jax.lax.reduce_window(operand:Array,init_value:Array,computation:Callable,window_dimensions:Shape,window_strides:Sequence[int],padding:Union[str,Sequence[Tuple[int,int]]],base_dilation:Optional[Sequence[int]]=None,window_dilation:Optional[Sequence[int]]=None)->Array
jax.lax.reduce_window_shape_tuple(operand_shape,window_dimensions,window_strides,padding,base_dilation=None,window_dilation=None)
jax.lax.rem(x:Array,y:Array)->Array
jax.lax.reshape(operand:Array,new_sizes:Shape,dimensions:Optional[Sequence[int]]=None)->Array
jax.lax.rev(operand:Array,dimensions:Sequence[int])->Array
jax.lax.rng_uniform(a,b,shape)
jax.lax.round(x:Array)->Array
jax.lax.rsqrt(x:Array)->Array
jax.lax.scatter(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.scatter_add(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.scatter_max(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.scatter_min(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.scatter_mul(operand:Array,scatter_indices:Array,updates:Array,dimension_numbers:ScatterDimensionNumbers,*,indices_are_sorted:bool=False,unique_indices:bool=False)->Array
jax.lax.select(pred:Array,on_true:Array,on_false:Array)->Array
jax.lax.shift_left(x:Array,y:Array)->Array
jax.lax.shift_right_arithmetic(x:Array,y:Array)->Array
jax.lax.shift_right_logical(x:Array,y:Array)->Array
jax.lax.sign(x:Array)->Array
jax.lax.sin(x:Array)->Array
jax.lax.sinh(x:Array)->Array
jax.lax.slice(operand:Array,start_indices:Sequence[int],limit_indices:Sequence[int],strides:Optional[Sequence[int]]=None)->Array
jax.lax.slice_in_dim(operand:Array,start_index:Optional[int],limit_index:Optional[int],stride:int=1,axis:int=0)->Array
jax.lax.sort(operand:Union[Array,Sequence[Array]],dimension:int=-1,is_stable:bool=True,num_keys:int=1)->Union[Array, Tuple[Array, ...]]
jax.lax.sort_key_val(keys:Array,values:Array,dimension:int=-1,is_stable:bool=True)->Tuple[Array, Array]
jax.lax.sqrt(x:Array)->Array
jax.lax.square(x:Array)->Array
jax.lax.squeeze(array:Array,dimensions:Tuple[int,...])->Array
jax.lax.standard_abstract_eval(prim,shape_rule,dtype_rule,*args,**kwargs)
jax.lax.standard_primitive(shape_rule,dtype_rule,name,translation_rule=None)
jax.lax.standard_translate(name,c,*args,**kwargs)
jax.lax.stop_gradient(x)
jax.lax.sub(x:Array,y:Array)->Array
jax.lax.tan(x:Array)->Array
jax.lax.tanh(x:Array)->Array
jax.lax.tie_in(x:Array,y:Array)->Array
jax.lax.top_k(operand:Array,k:int)->Tuple[Array, Array]
jax.lax.transpose(operand:Array,permutation:Sequence[int])->Array
jax.lax.unop(result_dtype,accepted_dtypes,name,translation_rule=None)
jax.lax.unop_dtype_rule(result_dtype,accepted_dtypes,name,aval,**kwargs)
jax.lax.zeros_like_array(x)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax/lax_parallel.py----------------------------------------
A:jax.lax.lax_parallel.(leaves, treedef)->jax.tree_util.tree_flatten(x)
A:jax.lax.lax_parallel.out_flat->jax.core.Primitive('psum').bind(*leaves, axis_name=axis_name, axis_index_groups=axis_index_groups)
A:jax.lax.lax_parallel.x->all_to_all(x, name, x_tosplit, xdim)
A:jax.lax.lax_parallel.n->len(dtype_args)
A:jax.lax.lax_parallel.len_0->len(axis_index_groups[0])
A:jax.lax.lax_parallel.axis_space->range(len_0 * len(axis_index_groups))
A:jax.lax.lax_parallel.outs->jax.tree_util.tree_map(partial(_expand, dim, size, axis_name), x)
A:jax.lax.lax_parallel.replica_groups->_replica_groups(axis_env, axis_name, None)
A:jax.lax.lax_parallel.dtype->c.get_shape(val).numpy_dtype()
A:jax.lax.lax_parallel.scalar->ShapedArray((), c.get_shape(dtype_args[0]).numpy_dtype())
A:jax.lax.lax_parallel.computation->jax.interpreters.xla.primitive_subcomputation(lax.add_p, scalar, scalar)
A:jax.lax.lax_parallel.replica_groups_protos->jax.lib.xla_client.make_replica_groups(replica_groups)
A:jax.lax.lax_parallel.args_by_type->collections.defaultdict(lambda : ([], []))
A:jax.lax.lax_parallel.is_complex->jax.dtypes.issubdtype(dtype, np.complexfloating)
A:jax.lax.lax_parallel.all_reduce->xops.AllReduce(xops.Tuple(c, dtype_args), computation, replica_groups_protos, None, None)
A:jax.lax.lax_parallel.psum->partial(_allreduce_translation_rule, lax.add_p, c, axis_name=axis_name, axis_env=axis_env, axis_index_groups=axis_index_groups, platform=platform)
A:jax.lax.lax_parallel.(nonzero_out_cts, treedef)->jax.tree_util.tree_flatten(cts)
A:jax.lax.lax_parallel.nonzero_in_cts->jax.core.Primitive('psum').bind(*nonzero_out_cts, axis_name=axis_name, axis_index_groups=axis_index_groups)
A:jax.lax.lax_parallel.psum_p->jax.core.Primitive('psum')
A:jax.lax.lax_parallel.pxla.soft_pmap_rules[psum_p]->partial(_allreduce_soft_pmap_rule, psum_p, lax._reduce_sum)
A:jax.lax.lax_parallel.pmax_p->jax.core.Primitive('pmax')
A:jax.lax.lax_parallel.xla.parallel_translations[pmax_p]->partial(_allreduce_translation_rule, lax.max_p)
A:jax.lax.lax_parallel.pmin_p->jax.core.Primitive('pmin')
A:jax.lax.lax_parallel.xla.parallel_translations[pmin_p]->partial(_allreduce_translation_rule, lax.min_p)
A:jax.lax.lax_parallel.group_size->len(replica_groups[0])
A:jax.lax.lax_parallel.(srcs, dsts)->unzip2(perm)
A:jax.lax.lax_parallel.grp->list(sorted(grp))
A:jax.lax.lax_parallel.inverse_perm->list(zip(dsts, srcs))
A:jax.lax.lax_parallel.ppermute_p->jax.core.Primitive('ppermute')
A:jax.lax.lax_parallel.split_count->len(replica_groups[0])
A:jax.lax.lax_parallel.stacked->jax.core.Primitive('all_to_all').bind(x, split_axis=split_axis + 1, concat_axis=0, axis_name=axis_name)
A:jax.lax.lax_parallel.out->jax.lax.lax.gather(operand, start_indices, dimension_numbers=dnums, slice_sizes=slice_sizes)
A:jax.lax.lax_parallel.all_to_all_p->jax.core.Primitive('all_to_all')
A:jax.lax.lax_parallel.shape->list(x.shape)
A:jax.lax.lax_parallel.y->_allgather(y, ydim, size, name)
A:jax.lax.lax_parallel.parallel.papply_primitive_rules[prim]->partial(_identity_papply, prim, argnum)
A:jax.lax.lax_parallel.result->prim.bind(operand, axes=tuple(other_axes), **kwargs)
A:jax.lax.lax_parallel.xbatch->adjust_dims(xbatch, xdim)
A:jax.lax.lax_parallel.xcontract->adjust_dims(xcontract, xdim)
A:jax.lax.lax_parallel.ybatch->adjust_dims(ybatch, ydim)
A:jax.lax.lax_parallel.ycontract->adjust_dims(ycontract, ydim)
A:jax.lax.lax_parallel.z->jax.lax.lax.dot_general(x, y, sub_dims(xdim, None, xc, yc, xb, yb), precision)
A:jax.lax.lax_parallel.(ok, out)->cases(x, y, xdim, ydim, lhs_contract, rhs_contract, lhs_batch, rhs_batch)
A:jax.lax.lax_parallel.old_sizes->tuple(np.insert(operand.shape, axis, size))
A:jax.lax.lax_parallel.left->numpy.prod(old_sizes[:old_axis])
A:jax.lax.lax_parallel.new_axis->find_new_axis(axis, old_sizes, new_sizes)
A:jax.lax.lax_parallel.lhs->jax.lax.lax.reshape(lhs, tuple(np.insert(lhs.shape, lhs_dim, 1)))
A:jax.lax.lax_parallel.sub_bdims->tuple(np.delete(broadcast_dimensions, dim))
A:jax.lax.lax_parallel.sub_shape->tuple(np.delete(shape, out_dim))
A:jax.lax.lax_parallel.padding_config->list(padding_config)
A:jax.lax.lax_parallel.padded->jax.lax.lax.pad(operand, padding_value, padding_config[:operand_dim] + padding_config[operand_dim + 1:])
A:jax.lax.lax_parallel.start_indices->list(start_indices)
A:jax.lax.lax_parallel.limit_indices->list(limit_indices)
A:jax.lax.lax_parallel.offset_dims->tuple((i - 1 if i > start_indices_dim else i for i in dimension_numbers.offset_dims))
A:jax.lax.lax_parallel.dnums->jax.lax.lax.GatherDimensionNumbers(offset_dims=offset_dims, collapsed_slice_dims=dimension_numbers.collapsed_slice_dims, start_index_map=dimension_numbers.start_index_map)
A:jax.lax.lax_parallel.size->prod([core.axis_frame(name).size for name in axis_name])
jax.lax.all_gather(x,axis_name)
jax.lax.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel._add_jaxvals_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._all_to_all_split_axis_rule(vals,which_mapped,split_axis,concat_axis,axis_name)
jax.lax.lax_parallel._all_to_all_translation_rule(c,x,*,split_axis,concat_axis,axis_name,axis_env,platform)
jax.lax.lax_parallel._all_to_all_transpose_rule(cts,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel._allgather(x,dim,size,axis_name)
jax.lax.lax_parallel._allreduce_soft_pmap_rule(prim,reducer,vals,mapped,chunk_size,*,axis_name,axis_index_groups)
jax.lax.lax_parallel._allreduce_translation_rule(prim,c,val,*,axis_name,axis_index_groups,axis_env,platform)
jax.lax.lax_parallel._broadcast_in_dim_papply_rule(name,size,vals,dims,shape,broadcast_dimensions)
jax.lax.lax_parallel._broadcasting_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel._conv_general_dilated_papply_rule(name,size,vals,dims,window_strides,padding,lhs_dilation,rhs_dilation,dimension_numbers,feature_group_count,precision,**unused_kwargs)
jax.lax.lax_parallel._convert_element_type_papply_rule(name,size,vals,dims,new_dtype,**params)
jax.lax.lax_parallel._defbroadcasting(prim)
jax.lax.lax_parallel._defidentity(prim,argnum=0)
jax.lax.lax_parallel._defreducer(prim,collective_prim)
jax.lax.lax_parallel._defvectorized(prim)
jax.lax.lax_parallel._dot_general_papply_rule(name,size,vals,dims,dimension_numbers,precision)
jax.lax.lax_parallel._drop(x,dim,axis_name)
jax.lax.lax_parallel._expand(dim,size,axis_name,x)
jax.lax.lax_parallel._gather_papply_rule(name,size,vals,dims,dimension_numbers,slice_sizes,operand_shape)
jax.lax.lax_parallel._identity_papply(prim,argnum,name,size,vals,axes,**params)
jax.lax.lax_parallel._moveaxis(src,dst,x)
jax.lax.lax_parallel._notuple_psum_translation_rule(c,*args,axis_name,axis_env,axis_index_groups,platform)
jax.lax.lax_parallel._pad_papply_rule(name,size,vals,dims,padding_config)
jax.lax.lax_parallel._ppermute_translation_rule(c,x,*,axis_name,axis_env,perm,platform)
jax.lax.lax_parallel._ppermute_transpose_rule(t,perm,axis_name)
jax.lax.lax_parallel._psum_translation_rule(c,*args,axis_name,axis_index_groups,axis_env,platform)
jax.lax.lax_parallel._psum_transpose_rule(cts,axis_name,axis_index_groups)
jax.lax.lax_parallel._reducer_papply(prim,collective,name,size,vals,papply_axes,axes,**kwargs)
jax.lax.lax_parallel._replica_groups(axis_env,axis_name,axis_index_groups)
jax.lax.lax_parallel._reshape_papply_rule(name,size,vals,axes,new_sizes,dimensions)
jax.lax.lax_parallel._select_papply_rule(name,size,vals,dims)
jax.lax.lax_parallel._slice_papply_rule(name,size,vals,dims,start_indices,limit_indices,strides,**kwargs)
jax.lax.lax_parallel._transpose_papply_rule(name,size,vals,dims,permutation)
jax.lax.lax_parallel._validate_axis_index_groups(axis_index_groups)
jax.lax.lax_parallel._vectorized_papply(prim,name,size,vals,axes,**params)
jax.lax.lax_parallel.all_gather(x,axis_name)
jax.lax.lax_parallel.all_to_all(x,axis_name,split_axis,concat_axis)
jax.lax.lax_parallel.omnistaging_enabler()->None
jax.lax.lax_parallel.pmax(x,axis_name,*,axis_index_groups=None)
jax.lax.lax_parallel.pmean(x,axis_name,*,axis_index_groups=None)
jax.lax.lax_parallel.pmin(x,axis_name,*,axis_index_groups=None)
jax.lax.lax_parallel.ppermute(x,axis_name,perm)
jax.lax.lax_parallel.pshuffle(x,axis_name,perm)
jax.lax.lax_parallel.psum(x,axis_name,*,axis_index_groups=None)
jax.lax.lax_parallel.pswapaxes(x,axis_name,axis)
jax.lax.pmax(x,axis_name,*,axis_index_groups=None)
jax.lax.pmean(x,axis_name,*,axis_index_groups=None)
jax.lax.pmin(x,axis_name,*,axis_index_groups=None)
jax.lax.ppermute(x,axis_name,perm)
jax.lax.pshuffle(x,axis_name,perm)
jax.lax.psum(x,axis_name,*,axis_index_groups=None)
jax.lax.pswapaxes(x,axis_name,axis)


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax/__init__.py----------------------------------------


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax/lax_control_flow.py----------------------------------------
A:jax.lax.lax_control_flow.T->TypeVar('T')
A:jax.lax.lax_control_flow.(wrapped_fun, out_tree)->flatten_fun_nokwargs(lu.wrap_init(fun), in_tree)
A:jax.lax.lax_control_flow.(jaxpr, out_pvals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(wrapped_fun, in_pvals, instantiate=True, stage_out=False)
A:jax.lax.lax_control_flow.(jaxpr, out_pvals, consts, out_tree)->_initial_style_untyped_jaxpr(fun, in_tree, in_avals)
A:jax.lax.lax_control_flow.out_avals->_map(raise_to_shaped, branches_2[0].out_avals)
A:jax.lax.lax_control_flow.const_avals->jax.util.concatenate(all_const_avals)
A:jax.lax.lax_control_flow.typed_jaxpr->jax.core.TypedJaxpr(pe.convert_constvars_jaxpr(jaxpr), (), const_avals + in_avals, out_avals)
A:jax.lax.lax_control_flow.(jaxprs, all_out_pvals, all_consts, all_out_trees)->unzip4([_initial_style_untyped_jaxpr(fun, in_tree, in_avals) for fun in funs])
A:jax.lax.lax_control_flow.newvar->jax.core.gensym(jaxprs, suffix='_')
A:jax.lax.lax_control_flow.all_const_avals->tuple((tuple((raise_to_shaped(core.get_aval(c)) for c in consts)) for consts in all_consts))
A:jax.lax.lax_control_flow.unused_const_vars->tuple((tuple((newvar(aval) for aval in const_avals)) for const_avals in all_const_avals))
A:jax.lax.lax_control_flow.prefix->jax.util.concatenate(unused_const_vars[:i])
A:jax.lax.lax_control_flow.suffix->jax.util.concatenate(unused_const_vars[i + 1:])
A:jax.lax.lax_control_flow.typed_jaxprs->_map(type_and_const_convert_jaxpr, jaxprs, all_out_pvals)
A:jax.lax.lax_control_flow.lower_dtype->jax.dtypes.canonicalize_dtype(lax.dtype(lower))
A:jax.lax.lax_control_flow.upper_dtype->jax.dtypes.canonicalize_dtype(lax.dtype(upper))
A:jax.lax.lax_control_flow.lower_->int(lower)
A:jax.lax.lax_control_flow.upper_->int(upper)
A:jax.lax.lax_control_flow.((_, _, result), _)->scan(_fori_scan_body_fun(body_fun), (lower, upper, init_val), None, length=upper_ - lower_)
A:jax.lax.lax_control_flow.(_, _, result)->while_loop(_fori_cond_fun, _fori_body_fun(body_fun), (lower, upper, init_val))
A:jax.lax.lax_control_flow.val->body_fun(val)
A:jax.lax.lax_control_flow.(init_vals, in_tree)->tree_flatten((init_val,))
A:jax.lax.lax_control_flow.init_avals->tuple(_map(_abstractify, init_vals))
A:jax.lax.lax_control_flow.(cond_jaxpr, cond_consts, cond_tree)->_initial_style_jaxpr(cond_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.(body_jaxpr, body_consts, body_tree)->_initial_style_jaxpr(body_fun, in_tree, init_avals)
A:jax.lax.lax_control_flow.in_tree_children->in_tree.children()
A:jax.lax.lax_control_flow.outs->jax.core.Primitive('custom_linear_solve').bind(*new_params + new_b, const_lengths=const_lengths, jaxprs=batched_jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(cond_consts, body_consts, init_vals)->split_list(args, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.batched->bool(cond_jaxpr.out_avals[0].shape)
A:jax.lax.lax_control_flow.init_carry->xops.Tuple(c, cond_consts + body_consts + init_vals)
A:jax.lax.lax_control_flow.cond_c->jax.lib.xla_bridge.make_computation_builder('cond_computation')
A:jax.lax.lax_control_flow.cond_carry->jax.lib.xla_bridge.parameter(cond_c, 0, c.get_shape(init_carry))
A:jax.lax.lax_control_flow.(x, _, z)->split_list(cond_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(pred,)->jax.interpreters.xla.jaxpr_subcomp(cond_c, cond_jaxpr.jaxpr, backend, axis_env, _map(partial(xb.constant, cond_c), cond_jaxpr.literals), extend_name_stack(name_stack, 'cond'), *x + z)
A:jax.lax.lax_control_flow.scalar->ShapedArray((), np.bool_)
A:jax.lax.lax_control_flow.or_->jax.interpreters.xla.primitive_subcomputation(lax.or_p, scalar, scalar)
A:jax.lax.lax_control_flow.pred->xops.Reduce(cond_c, [pred], [xb.constant(cond_c, np.array(False))], or_, list(range(cond_jaxpr.out_avals[0].ndim)))
A:jax.lax.lax_control_flow.body_c->jax.lib.xla_bridge.make_computation_builder('body_computation')
A:jax.lax.lax_control_flow.body_carry->jax.lib.xla_bridge.parameter(body_c, 0, c.get_shape(init_carry))
A:jax.lax.lax_control_flow.(x, y, z)->split_list(body_carry_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.new_z->_map(partial(_pred_bcast_select, body_c, body_pred), new_z, z, body_jaxpr.out_avals)
A:jax.lax.lax_control_flow.(body_pred,)->jax.interpreters.xla.jaxpr_subcomp(body_c, cond_jaxpr.jaxpr, backend, axis_env, _map(partial(xb.constant, body_c), cond_jaxpr.literals), extend_name_stack(name_stack, 'body_pred'), *x + z)
A:jax.lax.lax_control_flow.new_carry->xops.Tuple(body_c, list(itertools.chain(x, y, new_z)))
A:jax.lax.lax_control_flow.ans->xops.While(cond_c.build(pred), body_c.build(new_carry), init_carry)
A:jax.lax.lax_control_flow.(_, _, z)->split_list(ans_elts, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.pred_shape->fn(a, b).get_shape(pred).dimensions()
A:jax.lax.lax_control_flow.x_shape->fn(a, b).get_shape(x).dimensions()
A:jax.lax.lax_control_flow.y_shape->fn(a, b).get_shape(y).dimensions()
A:jax.lax.lax_control_flow.bcast_pred->xops.BroadcastInDim(pred, x_shape, list(range(len(pred_shape))))
A:jax.lax.lax_control_flow.(cconst_bat, bconst_bat, init_bat)->split_list(orig_batched, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jaxpr_batched, carry_bat_out)->jax.interpreters.batching.batch_jaxpr(body_jaxpr, size, batched, instantiate=carry_bat)
A:jax.lax.lax_control_flow.(cond_jaxpr_batched, (pred_bat,))->jax.interpreters.batching.batch_jaxpr(cond_jaxpr, size, cconst_bat + carry_bat, instantiate=bool(cond_jaxpr.out_avals[0].shape))
A:jax.lax.lax_control_flow.carry_bat_out->_map(partial(operator.or_, pred_bat), carry_bat_out)
A:jax.lax.lax_control_flow.carry_bat->_map(operator.or_, carry_bat, carry_bat_out)
A:jax.lax.lax_control_flow.(consts, init)->split_list(args, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.(const_dims, init_dims)->split_list(dims, [cond_nconsts + body_nconsts])
A:jax.lax.lax_control_flow.(cconst_nz, bconst_nz, init_nz)->split_list(nonzeros, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(body_jaxpr, body_nonzeros, instantiate=carry_nz)
A:jax.lax.lax_control_flow.carry_nz->_map(operator.or_, carry_nz, carry_nz_out)
A:jax.lax.lax_control_flow.(cconst, bconst, init)->split_list(primals, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(_, bconst_dot, init_dot)->split_list(tangents, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.bconst_dot->_prune_zeros(bconst_dot)
A:jax.lax.lax_control_flow.init_dot->_prune_zeros(init_dot)
A:jax.lax.lax_control_flow.body_jvp_rearranged->jax.interpreters.ad.rearrange_binders(body_jvp, [body_nconsts, num_carry], [len(bconst_dot), len(init_dot)], [num_carry], [len(init_dot)])
A:jax.lax.lax_control_flow.cond_jaxpr_augmented->jax.core.TypedJaxpr(cond_jaxpr_augmented, cond_jaxpr.literals, in_avals_aug, cond_jaxpr.out_avals)
A:jax.lax.lax_control_flow.out->jax.core.jaxpr_as_fun(jaxpr)(*all_args)
A:jax.lax.lax_control_flow.(out_carry, out_carry_dot)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.out_tangents_iter->iter(out_tangents)
A:jax.lax.lax_control_flow.params->dict(reverse=reverse, length=length, num_consts=num_consts, num_carry=num_carry, jaxpr=jaxpr, linear=linear, unroll=unroll)
A:jax.lax.lax_control_flow.partial_eval_jaxpr->partial(pe.partial_eval_jaxpr, trace_type=trace.master.trace_type)
A:jax.lax.lax_control_flow.(cond_consts_uk, body_consts_uk, carry_init_uk)->split_list(unknowns, [cond_nconsts, body_nconsts])
A:jax.lax.lax_control_flow.(body_jaxpr_known, _, carry_out_uk)->partial_eval_jaxpr(body_jaxpr, body_consts_uk + carry_uk, instantiate=carry_uk)
A:jax.lax.lax_control_flow.carry_uk->_map(operator.or_, carry_uk, carry_uk_out)
A:jax.lax.lax_control_flow.(cond_jaxpr_known, _, cond_uk)->partial_eval_jaxpr(cond_jaxpr, cond_consts_uk + carry_uk, instantiate=False)
A:jax.lax.lax_control_flow.out_known->jax.lax.lax.Primitive('while').bind(*in_consts, cond_nconsts=cond_nconsts, cond_jaxpr=cond_jaxpr_known, body_nconsts=body_nconsts, body_jaxpr=body_jaxpr_known)
A:jax.lax.lax_control_flow.while_p->jax.lax.lax.Primitive('while')
A:jax.lax.lax_control_flow.index_dtype->jax.dtypes.result_type(index)
A:jax.lax.lax_control_flow.branches->tuple(branches)
A:jax.lax.lax_control_flow.index->jax.lax.lax.convert_element_type(pred, np.int32)
A:jax.lax.lax_control_flow.lo->numpy.array(0, np.int32)
A:jax.lax.lax_control_flow.hi->numpy.array(len(branches) - 1, np.int32)
A:jax.lax.lax_control_flow.(ops, ops_tree)->tree_flatten((operand,))
A:jax.lax.lax_control_flow.ops_avals->tuple(_map(_abstractify, ops))
A:jax.lax.lax_control_flow.(jaxprs, consts, out_trees)->_initial_style_jaxprs_with_common_consts((true_fun, false_fun), ops_tree, ops_avals)
A:jax.lax.lax_control_flow.ba->inspect.signature(_cond_with_per_branch_args).bind(*args, **kwargs)
A:jax.lax.lax_control_flow.pred_dtype->jax.dtypes.result_type(pred)
A:jax.lax.lax_control_flow.c->fn(a, b)
A:jax.lax.lax_control_flow.op->xops.Tuple(c, args)
A:jax.lax.lax_control_flow.op_shape->fn(a, b).get_shape(op)
A:jax.lax.lax_control_flow.mid->numpy.array(mid, dtypes.canonicalize_dtype(lax.dtype(indices)))
A:jax.lax.lax_control_flow.bcast_indices->jax.lax.lax.broadcast_in_dim(indices, np.shape(branch_vals[0]), list(range(np.ndim(indices))))
A:jax.lax.lax_control_flow.branches_batched->tuple((batching.batch_jaxpr(jaxpr, size, bat, out_bat)[0] for jaxpr in branches))
A:jax.lax.lax_control_flow.branches_jvp->tuple((ad.jvp_jaxpr(jaxpr, ops_nz, instantiate=out_nz)[0] for jaxpr in branches))
A:jax.lax.lax_control_flow.ops_dot->_prune_zeros(ops_dot)
A:jax.lax.lax_control_flow.ops_lin->tuple(linear)
A:jax.lax.lax_control_flow.(out_primals, out_tangents)->split_list(out, [len(out_nz)])
A:jax.lax.lax_control_flow.(_, _, out_uks)->partial_eval_jaxpr(branch_jaxpr, ops_uk, instantiate=False)
A:jax.lax.lax_control_flow.(branch_jaxpr_1, branch_jaxpr_2, _)->partial_eval_jaxpr(branch_jaxpr, ops_uk, instantiate=out_uks)
A:jax.lax.lax_control_flow.branch_jaxpr_2->jax.interpreters.partial_eval.move_binders_to_front(branch_jaxpr_2, move)
A:jax.lax.lax_control_flow.res_avals->_map(raise_to_shaped, branch_jaxpr_2.in_avals[:branch_num_res])
A:jax.lax.lax_control_flow.branches_1->_join_cond_outputs(branches_1, all_res_avals, res_avals_per_branch, num_outs)
A:jax.lax.lax_control_flow.branches_2->_join_cond_pe_staged_jaxpr_inputs(branches_2, all_res_avals, res_avals_per_branch)
A:jax.lax.lax_control_flow.num_outs->len(branches_2[0].out_avals)
A:jax.lax.lax_control_flow.(all_res_avals, res_avals_per_branch)->_merge_branch_residuals(branch_res_avals)
A:jax.lax.lax_control_flow.num_res->len(res_indices)
A:jax.lax.lax_control_flow.(_, in_consts)->unzip2([t.pval for t in tracers])
A:jax.lax.lax_control_flow.out_consts_res->jax.lax.lax.Primitive('cond').bind(*in_consts, branches=branches_1, linear=linear)
A:jax.lax.lax_control_flow.(out_consts, res)->split_list(out_consts_res, [len(out_consts_res) - num_res])
A:jax.lax.lax_control_flow.index_tracer->trace.instantiate_const(tracers[0])
A:jax.lax.lax_control_flow.res_tracers->_map(trace.new_instantiated_const, res)
A:jax.lax.lax_control_flow.eqn->jax.interpreters.partial_eval.new_eqn_recipe(int_res_tracers + new_tracers + ext_res_tracers, out_tracers, scan_p, dict(reverse=reverse, length=length, jaxpr=jaxpr_2_opt, num_consts=num_consts_2, num_carry=num_carry, linear=tuple(linear_2), unroll=unroll), source_info_util.current())
A:jax.lax.lax_control_flow.branch_res_tagged_avals->_map(enumerate_equal, branch_res_avals)
A:jax.lax.lax_control_flow.all_tagged_avals->_ordered_unique(util.concatenate(branch_res_tagged_avals))
A:jax.lax.lax_control_flow.outs_and_residuals->jax.core.jaxpr_as_fun(jaxpr)(*args)
A:jax.lax.lax_control_flow.(outs, residuals)->split_list(outs_and_residuals, [num_non_res_outputs])
A:jax.lax.lax_control_flow.aug_residuals->jax.util.subvals(aug_residuals, zip(res_indices, residuals))
A:jax.lax.lax_control_flow.all_res_vars->_map(newvar, all_res_avals)
A:jax.lax.lax_control_flow.aug_res_vars->list(util.subvals(all_res_vars, zip(res_indices, res_vars)))
A:jax.lax.lax_control_flow.jaxpr_aug->jax.core.TypedJaxpr(jaxpr_aug, jaxpr.literals, aug_avals, jaxpr.out_avals)
A:jax.lax.lax_control_flow.d->collections.OrderedDict(((x, None) for x in xs))
A:jax.lax.lax_control_flow.(res_avals, primal_avals)->split_list(jaxpr.in_avals, [num_res])
A:jax.lax.lax_control_flow.primal_avals->_map(raise_to_shaped, primal_avals)
A:jax.lax.lax_control_flow.(res, cts_out)->split_list(args, [num_res])
A:jax.lax.lax_control_flow.cts_in->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, jaxpr.literals, primals, cts_out)
A:jax.lax.lax_control_flow.(_, cts_in)->split_list(cts_in, [num_res])
A:jax.lax.lax_control_flow.in_avals->_map(raise_to_shaped, branches[0].in_avals)
A:jax.lax.lax_control_flow.branches_trans->tuple((_transpose_cond_jaxpr(jaxpr, num_res) for jaxpr in branches))
A:jax.lax.lax_control_flow.lin_in_avals->_map(raise_to_shaped, [a for (a, l) in zip(in_avals, linear) if l])
A:jax.lax.lax_control_flow.cts->_map(ad.instantiate_zeros_aval, branches[0].out_avals, cts)
A:jax.lax.lax_control_flow.out_iter->iter(out)
A:jax.lax.lax_control_flow.jaxpr0_in_avals_str->_avals_short(jaxpr0.in_avals)
A:jax.lax.lax_control_flow.jaxpr0_out_avals_str->_avals_short(jaxpr0.out_avals)
A:jax.lax.lax_control_flow.avals->_map(core.get_aval, args)
A:jax.lax.lax_control_flow.cond_p->jax.lax.lax.Primitive('cond')
A:jax.lax.lax_control_flow.(init_flat, init_tree)->tree_flatten(init)
A:jax.lax.lax_control_flow.(xs_flat, xs_tree)->tree_flatten(xs)
A:jax.lax.lax_control_flow.(in_flat, in_tree)->tree_flatten((init, xs))
A:jax.lax.lax_control_flow.length->int(length)
A:jax.lax.lax_control_flow.unique_lengths->set(lengths)
A:jax.lax.lax_control_flow.(carry, y)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.ys->_map(_concatenate, y_avals, ys, ys_rem)
A:jax.lax.lax_control_flow.carry_avals->tuple(_map(_abstractify, init_flat))
A:jax.lax.lax_control_flow.x_avals->tuple(_map(ShapedArray, x_shapes, x_dtypes))
A:jax.lax.lax_control_flow.(jaxpr, consts, out_tree)->_initial_style_jaxpr(g, in_args_tree, args_avals)
A:jax.lax.lax_control_flow.out_tree_children->out_tree.children()
A:jax.lax.lax_control_flow.(consts, init, xs)->split_list(padded_vals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.x->jax.core.Primitive('custom_linear_solve').bind(*primals, **kwargs)
A:jax.lax.lax_control_flow.([i], carry, ys)->split_list(vals, [1, num_carry])
A:jax.lax.lax_control_flow.out_flat->jax.core.Primitive('custom_linear_solve').bind(*_flatten(all_consts) + b_flat, const_lengths=const_lengths, jaxprs=jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(carry_out, y_updates)->split_list(out_flat, [num_carry])
A:jax.lax.lax_control_flow.ys_out->_map(partial(_update_array, i_), y_avals, ys, y_updates)
A:jax.lax.lax_control_flow.ys_init->_map(partial(_empty_array, length), y_avals)
A:jax.lax.lax_control_flow.(_, *outs)->while_loop(cond_fun, body_fun, init_val)
A:jax.lax.lax_control_flow.(num_blocks, rem)->divmod(length, unroll)
A:jax.lax.lax_control_flow.partition->partial(_partition_leading, num_blocks, block_length)
A:jax.lax.lax_control_flow.xs_block->_map(partition, x_avals, xs)
A:jax.lax.lax_control_flow.prepend_aval->partial(_prepend_dim_to_aval, block_length)
A:jax.lax.lax_control_flow.x_block_avals->_map(prepend_aval, x_avals)
A:jax.lax.lax_control_flow.y_block_avals->_map(prepend_aval, y_avals)
A:jax.lax.lax_control_flow.f_impl_block->partial(_scan_impl_unrolled, reverse=reverse, length=block_length, num_consts=num_consts, num_carry=num_carry, linear=linear, f_impl=f_impl, x_avals=x_avals, y_avals=y_avals)
A:jax.lax.lax_control_flow.(carry, ys_blocks)->split_list(outs, [num_carry])
A:jax.lax.lax_control_flow.combine->partial(_combine_leading, num_blocks, block_length)
A:jax.lax.lax_control_flow.(_, _, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(_, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.f_impl->jax.core.jaxpr_as_fun(jaxpr)
A:jax.lax.lax_control_flow.split->partial(_split_leading_dim, length_div)
A:jax.lax.lax_control_flow.(xs_rem, xs)->unzip2(_map(split, x_avals, xs))
A:jax.lax.lax_control_flow.(xs, xs_rem)->unzip2(_map(split, x_avals, xs))
A:jax.lax.lax_control_flow.(carry, ys)->split_list(outs, [num_carry])
A:jax.lax.lax_control_flow.(carry, ys_rem)->split_list(outs, [num_carry])
A:jax.lax.lax_control_flow.(carry_avals, y_avals)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.(const_nz, init_nz, xs_nz)->split_list(nonzeros, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_jvp, nonzeros_out)->jax.interpreters.ad.jvp_jaxpr(jaxpr, nonzeros, instantiate=carry_nz + [False] * num_ys)
A:jax.lax.lax_control_flow.all_tangents->split_list(tangents, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts_dot, init_dot, xs_dot)->_map(_prune_zeros, all_tangents)
A:jax.lax.lax_control_flow.jaxpr_jvp_rearranged->jax.interpreters.ad.rearrange_binders(jaxpr_jvp, [num_consts, num_carry, num_xs], [len(consts_dot), len(init_dot), len(xs_dot)], [num_carry, num_ys], [len(init_dot), sum(nonzeros_out) - len(init_dot)])
A:jax.lax.lax_control_flow.(consts_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.jaxpr_jvp_linear->tuple(consts_linear + [True] * len(consts_dot) + init_linear + [True] * len(init_dot) + xs_linear + [True] * len(xs_dot))
A:jax.lax.lax_control_flow.(carry, carry_dot, ys, ys_dot)->split_list(out_flat, [num_carry, len(init_dot), num_ys])
A:jax.lax.lax_control_flow.tangents_out_iter->iter(carry_dot + ys_dot)
A:jax.lax.lax_control_flow.(const_uk, init_uk, xs_uk)->split_list(unknowns, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_1, jaxpr_2, out_uk)->partial_eval_jaxpr(jaxpr, unknowns, instantiate=carry_uk + [False] * num_ys)
A:jax.lax.lax_control_flow.(untyped_jaxpr_1, out_pvals_1, consts_1)->jax.interpreters.partial_eval.trace_to_jaxpr(lu.wrap_init(core.jaxpr_as_fun(jaxpr_1)), in_pvals_1, instantiate=[True] * (num_carry + num_ys) + [False] * num_res)
A:jax.lax.lax_control_flow.jaxpr_1_opt->jax.interpreters.partial_eval.TypedJaxpr(pe.convert_constvars_jaxpr(untyped_jaxpr_1), (), const_avals_1 + in_avals_1, out_avals_1)
A:jax.lax.lax_control_flow.(_, _, res_pvals)->split_list(out_pvals_1, [num_carry, num_ys])
A:jax.lax.lax_control_flow.jaxpr_2_opt->jax.interpreters.partial_eval.move_binders_to_front(jaxpr_2, move)
A:jax.lax.lax_control_flow.(out_carry, ys, res_and_units)->split_list(out_flat, [num_carry, num_ys])
A:jax.lax.lax_control_flow.ys_avals->_map(partial(_promote_aval_rank, length), y_avals)
A:jax.lax.lax_control_flow.int_res_tracers->_map(trace.new_instantiated_const, intensive_residuals)
A:jax.lax.lax_control_flow.ext_res_tracers->_map(trace.new_instantiated_const, extensive_residuals)
A:jax.lax.lax_control_flow.(consts_lin, init_lin, xs_lin)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(consts, _, xs)->split_list(args, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(ires, _)->split_list(consts, [num_ires])
A:jax.lax.lax_control_flow.(_, eres)->split_list(xs, [sum(xs_lin)])
A:jax.lax.lax_control_flow.(ct_carry, ct_ys)->split_list(cts, [num_carry])
A:jax.lax.lax_control_flow.ct_carry->_map(ad.instantiate_zeros_aval, carry_avals, ct_carry)
A:jax.lax.lax_control_flow.ct_ys->_map(ad.instantiate_zeros_aval, ys_avals, ct_ys)
A:jax.lax.lax_control_flow.ct_consts->_map(ad_util.zeros_like_aval, jaxpr.in_avals[num_ires:num_consts])
A:jax.lax.lax_control_flow.jaxpr_trans->_transpose_scan_jaxpr(num_ires, num_consts - num_ires, num_eres, jaxpr)
A:jax.lax.lax_control_flow.(ct_consts, ct_init, ct_xs)->split_list(outs, [num_consts - num_ires, num_carry])
A:jax.lax.lax_control_flow.(res1_avals, c_avals, a_avals, res2_avals)->split_list(jaxpr.in_avals, [num_res1, num_c, num_a])
A:jax.lax.lax_control_flow.num_b->len(jaxpr.out_avals)
A:jax.lax.lax_control_flow.b_avals->tuple(_map(_abstractify, b_flat))
A:jax.lax.lax_control_flow.(res1, c_bar, b_bar, res2)->split_list(res1_cbar_bbar_res2, [num_res1, num_c, num_b])
A:jax.lax.lax_control_flow.cbar_abar->jax.interpreters.ad.backward_pass(jaxpr.jaxpr, jaxpr.literals, primals, b_bar)
A:jax.lax.lax_control_flow.(_, new_c_bar, a_bar, _)->split_list(cbar_abar, [num_res1, num_c, num_a])
A:jax.lax.lax_control_flow.a_bar->_map(ad.instantiate_zeros_aval, a_avals, a_bar)
A:jax.lax.lax_control_flow.c_bar->_map(ad.instantiate_zeros_aval, c_avals, _map(ad.add_tangents, c_bar, new_c_bar))
A:jax.lax.lax_control_flow.(jaxpr, out_avals, consts)->jax.interpreters.partial_eval.trace_to_jaxpr_dynamic(wrapped_fun, in_avals)
A:jax.lax.lax_control_flow.(jaxpr, pvals_out, consts)->jax.interpreters.partial_eval.trace_to_jaxpr(traceable, pvals, instantiate=True)
A:jax.lax.lax_control_flow.(out_avals, _)->unzip2(pvals_out)
A:jax.lax.lax_control_flow.(const_batched, init_batched, xs_batched)->split_list(orig_batched, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(jaxpr_batched, batched_out)->jax.interpreters.batching.batch_jaxpr(jaxpr, size, batched, instantiate=carry_batched + [False] * num_ys)
A:jax.lax.lax_control_flow.carry_batched->_map(operator.or_, carry_batched, carry_batched_out)
A:jax.lax.lax_control_flow.(consts_bdims, init_bdims, xs_bdims)->split_list(dims, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(dynamic_length,)->jax.interpreters.masking.shape_as_value((length,))
A:jax.lax.lax_control_flow.masked_jaxpr->_masked_scan_jaxpr(jaxpr, num_consts, num_carry)
A:jax.lax.lax_control_flow.(const_linear, init_linear, xs_linear)->split_list(linear, [num_consts, num_carry])
A:jax.lax.lax_control_flow.out_vals->jax.core.Primitive('scan').bind(*itertools.chain([dynamic_length] + consts, [0], init, xs), reverse=reverse, length=max_length, jaxpr=masked_jaxpr, num_consts=1 + num_consts, num_carry=1 + num_carry, linear=tuple([False] + const_linear + [False] + init_linear + xs_linear), unroll=unroll)
A:jax.lax.lax_control_flow.fun->jax.core.jaxpr_as_fun(jaxpr)
A:jax.lax.lax_control_flow.([dynamic_length], consts, [i], carry, xs)->split_list(args, [1, num_consts, 1, num_carry])
A:jax.lax.lax_control_flow.(new_carry, ys)->split_list(out, [num_carry])
A:jax.lax.lax_control_flow.aval->ShapedArray((), dtypes.int_)
A:jax.lax.lax_control_flow.(const_avals, carry_avals, x_avals)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(const_avals, init_avals, x_avals)->split_list(avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(const_avals_jaxpr, init_avals_jaxpr, x_avals_jaxpr)->split_list(jaxpr.in_avals, [num_consts, num_carry])
A:jax.lax.lax_control_flow.(carry_avals_jaxpr, _)->split_list(jaxpr.out_avals, [num_carry])
A:jax.lax.lax_control_flow.x_avals_mapped->_map(partial(core.mapped_aval, length), x_avals)
A:jax.lax.lax_control_flow.scan_p->jax.core.Primitive('scan')
A:jax.lax.lax_control_flow.xla.initial_style_translations[scan_p]->jax.interpreters.xla.lower_fun_initial_style(_scan_impl)
A:jax.lax.lax_control_flow.(_, ys)->scan(g, (), xs)
A:jax.lax.lax_control_flow.result->_memcpy(dimension, logical_shape[dimension], padded_val, result, offset)
A:jax.lax.lax_control_flow.update->jax.lax.lax.dynamic_index_in_dim(src, i, axis)
A:jax.lax.lax_control_flow.(args_flat, in_args_tree)->tree_flatten((args, kwargs))
A:jax.lax.lax_control_flow.args_avals->tuple(_map(_abstractify, args_flat))
A:jax.lax.lax_control_flow.all_args->_map(lax.stop_gradient, (*consts, *args_flat))
A:jax.lax.lax_control_flow._RootTuple->collections.namedtuple('_RootTuple', 'f, solve, l_and_s')
A:jax.lax.lax_control_flow.params_list->split_list(args, list(const_lengths))
A:jax.lax.lax_control_flow.(guess_flat, in_args_tree)->tree_flatten((initial_guess,))
A:jax.lax.lax_control_flow.guess_avals->tuple(_map(_abstractify, guess_flat))
A:jax.lax.lax_control_flow.(f_jaxpr, f_consts, out_tree)->_initial_style_jaxpr(f, in_args_tree, guess_avals)
A:jax.lax.lax_control_flow.(in_tree,)->treedef_children(in_args_tree)
A:jax.lax.lax_control_flow.(solve_jaxpr, solve_consts, solution_tree)->_initial_style_jaxpr(partial(solve, _stop_gradient_fun(f)), in_args_tree, guess_avals)
A:jax.lax.lax_control_flow.(unchecked_zeros, f_jvp)->jax.linearize(f, x)
A:jax.lax.lax_control_flow.(l_and_s_jaxpr, l_and_s_consts, out_tree)->_initial_style_jaxpr(linearize_and_solve, treedef_tuple((in_tree,) * 2), guess_avals * 2)
A:jax.lax.lax_control_flow.const_lengths->_LinearSolveTuple(*_map(len, all_consts))
A:jax.lax.lax_control_flow.jaxprs->_LinearSolveTuple(matvec_jaxpr, vecmat_jaxpr, solve_jaxpr, tr_solve_jaxpr)
A:jax.lax.lax_control_flow.(params, initial_guess)->_split_root_args(args, const_lengths)
A:jax.lax.lax_control_flow.solution->_custom_root(const_lengths, jaxprs, *primals)
A:jax.lax.lax_control_flow.(params, _)->_split_linear_solve_args(primals, const_lengths)
A:jax.lax.lax_control_flow.(params_dot, _)->_split_root_args(tangents, const_lengths)
A:jax.lax.lax_control_flow.f->jax.core.jaxpr_as_fun(jaxprs.f)
A:jax.lax.lax_control_flow.linearize_and_solve->partial(core.jaxpr_as_fun(jaxprs.l_and_s), *params.l_and_s)
A:jax.lax.lax_control_flow.(_, rhs)->jax.interpreters.ad.jvp(lu.wrap_init(f_at_solution)).call_wrapped(params.f, params_dot.f)
A:jax.lax.lax_control_flow.solution_dot->_map(operator.neg, linearize_and_solve(*itertools.chain(solution, rhs)))
A:jax.lax.lax_control_flow.(_, vjp_fun)->jax.vjp(linear_fun, primals)
A:jax.lax.lax_control_flow.(y,)->vjp_fun(x)
A:jax.lax.lax_control_flow.actual_shapes->_map(np.shape, actual)
A:jax.lax.lax_control_flow.expected_shapes->_map(np.shape, expected)
A:jax.lax.lax_control_flow.(b_flat, in_args_tree)->tree_flatten((b,))
A:jax.lax.lax_control_flow.(matvec_jaxpr, matvec_consts, out_tree)->_initial_style_jaxpr(matvec, in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(tree,)->treedef_children(in_args_tree)
A:jax.lax.lax_control_flow.(solve_jaxpr, solve_consts, out_tree)->_initial_style_jaxpr(partial(solve, matvec), in_args_tree, b_avals)
A:jax.lax.lax_control_flow.vecmat->_transpose_function(matvec, b)
A:jax.lax.lax_control_flow.(vecmat_jaxpr, vecmat_consts, out_tree)->_initial_style_jaxpr(vecmat, in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(tr_solve_jaxpr, tr_solve_consts, out_tree)->_initial_style_jaxpr(partial(transpose_solve, vecmat), in_args_tree, b_avals)
A:jax.lax.lax_control_flow.(const_lengths, jaxprs, tree)->split_dict(kwargs, ['const_lengths', 'jaxprs', 'tree'])
A:jax.lax.lax_control_flow.(params, b)->_split_linear_solve_args(args, const_lengths)
A:jax.lax.lax_control_flow.zeros->_map(ad_util.Zero.from_value, x)
A:jax.lax.lax_control_flow.(_, out_tangent)->jax.interpreters.ad.jvp(lu.wrap_init(func)).call_wrapped(params + list(x), params_dot + zeros)
A:jax.lax.lax_control_flow.kwargs->dict(const_lengths=const_lengths, jaxprs=jaxprs, tree=tree)
A:jax.lax.lax_control_flow.(params_dot, b_dot)->_split_linear_solve_args(tangents, const_lengths)
A:jax.lax.lax_control_flow.matvec_tangents->_tangent_linear_map(core.jaxpr_as_fun(jaxprs.matvec), params.matvec, params_dot.matvec, *x)
A:jax.lax.lax_control_flow.rhs->_map(ad.add_tangents, b_dot, _map(operator.neg, matvec_tangents))
A:jax.lax.lax_control_flow.x_dot->jax.core.Primitive('custom_linear_solve').bind(*_flatten(params) + rhs, **kwargs)
A:jax.lax.lax_control_flow.cotangent_b->jax.core.Primitive('custom_linear_solve').bind(*_flatten(params.transpose()) + cotangent, const_lengths=const_lengths.transpose(), jaxprs=jaxprs.transpose(), tree=tree)
A:jax.lax.lax_control_flow.(params_dims, b_dims)->_split_linear_solve_args(dims, const_lengths)
A:jax.lax.lax_control_flow.(params_bat, orig_b_bat)->_split_linear_solve_args(orig_bat, const_lengths)
A:jax.lax.lax_control_flow.(solve_jaxpr_batched, solve_x_bat)->jax.interpreters.batching.batch_jaxpr(solve, size, solve_bat + b_bat, instantiate=x_bat)
A:jax.lax.lax_control_flow.(vecmat_jaxpr_batched, vecmat_x_bat)->jax.interpreters.batching.batch_jaxpr(vecmat, size, vecmat_bat + b_bat, instantiate=x_bat)
A:jax.lax.lax_control_flow.x_bat_out->_map(operator.or_, vecmat_x_bat, solve_x_bat)
A:jax.lax.lax_control_flow.(matvec_jaxpr_batched, matvec_b_bat)->jax.interpreters.batching.batch_jaxpr(matvec, size, matvec_bat + x_bat_out, instantiate=b_bat)
A:jax.lax.lax_control_flow.b_bat_out->_map(lambda m, s, o: m or s or o, matvec_b_bat, solve_t_b_bat, orig_b_bat)
A:jax.lax.lax_control_flow.(solve_t_jaxpr_batched, solve_t_b_bat)->jax.interpreters.batching.batch_jaxpr(solve_t, size, solve_t_bat + x_bat_out, instantiate=b_bat)
A:jax.lax.lax_control_flow.batched_jaxprs->_LinearSolveTuple(matvec_jaxpr_batched, vecmat_jaxpr_batched, solve_jaxpr_batched, solve_t_jaxpr_batched)
A:jax.lax.lax_control_flow.linear_solve_p->jax.core.Primitive('custom_linear_solve')
A:jax.lax.lax_control_flow.xla.initial_style_translations[linear_solve_p]->jax.interpreters.xla.lower_fun_initial_style(_custom_linear_solve_impl)
A:jax.lax.lax_control_flow.(elems_flat, tree)->tree_flatten(elems)
A:jax.lax.lax_control_flow.a->tree_unflatten(tree, a_flat)
A:jax.lax.lax_control_flow.b->tree_unflatten(tree, b_flat)
A:jax.lax.lax_control_flow.(c_flat, _)->tree_flatten(c)
A:jax.lax.lax_control_flow.num_elems->int(elems_flat[0].shape[0])
A:jax.lax.lax_control_flow.reduced_elems->lowered_fn([elem[0:-1:2] for elem in elems], [elem[1::2] for elem in elems])
A:jax.lax.lax_control_flow.reduced_reduced_elems->lowered_fn(reduced_elems, [elem[2:3] for elem in elems])
A:jax.lax.lax_control_flow.odd_elems->_scan(reduced_elems)
A:jax.lax.lax_control_flow.results->lowered_fn([odd_elem for odd_elem in odd_elems], [elem[2::2] for elem in elems])
A:jax.lax.lax_control_flow.scans->_scan(elems_flat)
A:jax.lax.lax_control_flow.(jaxpr, out_avals, consts, out_tree)->_initial_style_untyped_jaxpr(fun, in_tree, in_avals)
A:jax.lax.lax_control_flow.(jaxprs, all_out_avals, all_consts, all_out_trees)->unzip4((_initial_style_untyped_jaxpr(fun, in_tree, in_avals) for fun in funs))
A:jax.lax.lax_control_flow.consts->jax.util.concatenate(all_consts)
jax.lax.associative_scan(fn,elems)
jax.lax.cond(*args,**kwargs)
jax.lax.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False)
jax.lax.custom_root(f,initial_guess,solve,tangent_solve)
jax.lax.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow._LinearSolveTuple(collections.namedtuple('_LinearSolveTuple','matvec,vecmat,solve,transpose_solve'))
jax.lax.lax_control_flow._LinearSolveTuple.transpose(self)
jax.lax.lax_control_flow._abstractify(x)
jax.lax.lax_control_flow._avals_short(avals)
jax.lax.lax_control_flow._check_shapes(func_name,expected_name,actual,expected,tree)
jax.lax.lax_control_flow._check_tree(func_name,expected_name,actual_tree,expected_tree)
jax.lax.lax_control_flow._check_tree_and_avals(what,tree1,avals1,tree2,avals2)
jax.lax.lax_control_flow._combine_leading(sz0,sz1,aval,x)
jax.lax.lax_control_flow._concat_masking_rule(padded_vals,logical_shapes,dimension)
jax.lax.lax_control_flow._concatenate(aval,x1,x2)
jax.lax.lax_control_flow._cond(pred,true_fun:Callable,false_fun:Callable,operand)
jax.lax.lax_control_flow._cond_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._cond_batching_rule(args,dims,branches,linear)
jax.lax.lax_control_flow._cond_index_bcast_and_select_tree(indices,branch_vals)
jax.lax.lax_control_flow._cond_jvp(primals,tangents,branches,linear)
jax.lax.lax_control_flow._cond_partial_eval(trace,*tracers,branches,linear)
jax.lax.lax_control_flow._cond_translation_rule(c,axis_env,name_stack,avals,backend,index,*args,branches,linear)
jax.lax.lax_control_flow._cond_transpose(cts,*args,branches,linear)
jax.lax.lax_control_flow._cond_typecheck(*avals,branches,linear)
jax.lax.lax_control_flow._cond_with_per_branch_args(pred,true_operand,true_fun:Callable,false_operand,false_fun:Callable)
jax.lax.lax_control_flow._custom_linear_solve_impl(*args,**kwargs)
jax.lax.lax_control_flow._custom_linear_solve_jvp(primals,tangents,const_lengths,jaxprs,tree)
jax.lax.lax_control_flow._custom_root(const_lengths,jaxprs,*args)
jax.lax.lax_control_flow._disable_jit_impl(prim,interp,*args,**kwargs)
jax.lax.lax_control_flow._dynamic_index_array(i,aval,x)
jax.lax.lax_control_flow._empty_array(sz,aval)
jax.lax.lax_control_flow._flatten(args)
jax.lax.lax_control_flow._fori_body_fun(body_fun)
jax.lax.lax_control_flow._fori_cond_fun(loop_carry)
jax.lax.lax_control_flow._fori_scan_body_fun(body_fun)
jax.lax.lax_control_flow._index_array(i,aval,x)
jax.lax.lax_control_flow._initial_style_jaxpr(fun:Callable,in_tree,in_avals)
jax.lax.lax_control_flow._initial_style_jaxprs_with_common_consts(funs:Sequence[Callable],in_tree,in_avals)
jax.lax.lax_control_flow._initial_style_untyped_jaxpr(fun:Callable,in_tree,in_avals)
jax.lax.lax_control_flow._interleave(a,b)
jax.lax.lax_control_flow._join_cond_outputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr,num_non_res_outputs)
jax.lax.lax_control_flow._join_cond_pe_staged_jaxpr_inputs(jaxprs,all_res_avals,res_aval_indices_per_jaxpr)
jax.lax.lax_control_flow._linear_solve_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._linear_solve_batching_rule(args,dims,**kwargs)
jax.lax.lax_control_flow._linear_solve_transpose_rule(cotangent,*primals,**kwargs)
jax.lax.lax_control_flow._make_typed_jaxpr(traceable:lu.WrappedFun,in_avals:Sequence[core.AbstractValue])
jax.lax.lax_control_flow._masked_scan_jaxpr(jaxpr,num_consts,num_carry)
jax.lax.lax_control_flow._memcpy(axis,num,src,dst,offset)
jax.lax.lax_control_flow._merge_branch_residuals(branch_res_avals)
jax.lax.lax_control_flow._ordered_unique(xs)
jax.lax.lax_control_flow._partition_leading(sz0,sz1,aval,x)
jax.lax.lax_control_flow._pred_bcast_select(c,pred,x,y,x_y_aval:core.AbstractValue)
jax.lax.lax_control_flow._prepend_dim_to_aval(sz,aval)
jax.lax.lax_control_flow._promote_aval_rank(sz,aval)
jax.lax.lax_control_flow._prune_zeros(ts)
jax.lax.lax_control_flow._root_jvp(const_lengths,jaxprs,primals,tangents)
jax.lax.lax_control_flow._scan_abstract_eval(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.lax.lax_control_flow._scan_batching_rule(args,dims,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax.lax.lax_control_flow._scan_impl(*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.lax.lax_control_flow._scan_impl_block_unrolled(*args,reverse,length,num_consts,num_carry,linear,block_length,f_impl,x_avals,y_avals)
jax.lax.lax_control_flow._scan_impl_loop(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax.lax.lax_control_flow._scan_impl_unrolled(*args,reverse,length,num_consts,num_carry,linear,f_impl,x_avals,y_avals)
jax.lax.lax_control_flow._scan_jvp(primals,tangents,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax.lax.lax_control_flow._scan_masking_rule(padded_vals,logical_shapes,reverse,length,jaxpr,num_consts,num_carry,linear,unroll)
jax.lax.lax_control_flow._scan_partial_eval(trace,*tracers,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.lax.lax_control_flow._scan_transpose(cts,*args,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.lax.lax_control_flow._scan_typecheck(*avals,reverse,length,num_consts,num_carry,jaxpr,linear,unroll)
jax.lax.lax_control_flow._select_tree(indices,branch_vals)
jax.lax.lax_control_flow._split_leading_dim(i,aval,x)
jax.lax.lax_control_flow._split_linear_solve_args(args,const_lengths)
jax.lax.lax_control_flow._split_root_args(args,const_lengths)
jax.lax.lax_control_flow._stack(aval,vals)
jax.lax.lax_control_flow._stop_gradient_fun(f)
jax.lax.lax_control_flow._tangent_linear_map(func,params,params_dot,*x)
jax.lax.lax_control_flow._transpose_cond_jaxpr(jaxpr,num_res)
jax.lax.lax_control_flow._transpose_function(linear_fun,primals)
jax.lax.lax_control_flow._transpose_scan_jaxpr(num_res1,num_c,num_res2,jaxpr)
jax.lax.lax_control_flow._update_array(i,aval,xs,x)
jax.lax.lax_control_flow._while_loop_abstract_eval(*args,**kwargs)
jax.lax.lax_control_flow._while_loop_batching_rule(args,dims,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax.lax_control_flow._while_loop_jvp(primals,tangents,cond_nconsts,cond_jaxpr,body_nconsts,body_jaxpr)
jax.lax.lax_control_flow._while_loop_translation_rule(c,axis_env,name_stack,avals,backend,*args,cond_jaxpr,body_jaxpr,cond_nconsts,body_nconsts)
jax.lax.lax_control_flow._while_partial_eval(trace:pe.JaxprTrace,*tracers:pe.Tracer,cond_nconsts:int,cond_jaxpr:pe.TypedJaxpr,body_nconsts:int,body_jaxpr:pe.TypedJaxpr)->Sequence[pe.Tracer]
jax.lax.lax_control_flow._while_transpose_error(*_,**kwargs)
jax.lax.lax_control_flow.associative_scan(fn,elems)
jax.lax.lax_control_flow.cond(*args,**kwargs)
jax.lax.lax_control_flow.cond_bind(*args,branches,linear)
jax.lax.lax_control_flow.custom_linear_solve(matvec,b,solve,transpose_solve=None,symmetric=False)
jax.lax.lax_control_flow.custom_root(f,initial_guess,solve,tangent_solve)
jax.lax.lax_control_flow.fori_loop(lower,upper,body_fun,init_val)
jax.lax.lax_control_flow.map(f,xs)
jax.lax.lax_control_flow.omnistaging_enabler()->None
jax.lax.lax_control_flow.scan(f,init,xs,length=None,reverse=False,unroll=1)
jax.lax.lax_control_flow.scan_bind(*args,**params)
jax.lax.lax_control_flow.switch(index,branches:Sequence[Callable],operand)
jax.lax.lax_control_flow.while_loop(cond_fun:Callable[[T],bool],body_fun:Callable[[T],T],init_val:T)->T
jax.lax.map(f,xs)
jax.lax.scan(f,init,xs,length=None,reverse=False,unroll=1)
jax.lax.scan_bind(*args,**params)
jax.lax.switch(index,branches:Sequence[Callable],operand)
jax.lax.while_loop(cond_fun:Callable[[T],bool],body_fun:Callable[[T],T],init_val:T)->T


----------------------------------------/home/zhang/Packages/jax/jax0.1.75/lax/lax_fft.py----------------------------------------
A:jax.lax.lax_fft.dtype->_real_dtype(x.dtype)
A:jax.lax.lax_fft.x->interpreters.batching.moveaxis(x, bd, 0)
A:jax.lax.lax_fft.fft_lengths->tuple(fft_lengths)
A:jax.lax.lax_fft.y->fft(x, xla_client.FftType.FFT, fft_lengths)
A:jax.lax.lax_fft.dummy_primals->lax.full_like(t, 0.0, _real_dtype(t.dtype), dummy_shape)
A:jax.lax.lax_fft.(_, jvpfun)->vjp(partial(_naive_rfft, fft_lengths=fft_lengths), dummy_primals)
A:jax.lax.lax_fft.(result,)->jvpfun(t)
A:jax.lax.lax_fft.full->partial(lax.full_like, t, dtype=t.dtype)
A:jax.lax.lax_fft.mask->lax.concatenate([full(1.0, shape=(1,)), full(2.0, shape=(n - 2 + is_odd,)), full(1.0, shape=(1 - is_odd,))], dimension=0)
A:jax.lax.lax_fft.result->fft(t, fft_type, fft_lengths)
A:jax.lax.lax_fft.fft_p->Primitive('fft')
jax.lax.fft(x,fft_type,fft_lengths)
jax.lax.lax_fft._irfft_transpose(t,fft_lengths)
jax.lax.lax_fft._naive_rfft(x,fft_lengths)
jax.lax.lax_fft._promote_to_complex(arg)
jax.lax.lax_fft._promote_to_real(arg)
jax.lax.lax_fft._rfft_transpose(t,fft_lengths)
jax.lax.lax_fft.fft(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_abstract_eval(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_batching_rule(batched_args,batch_dims,fft_type,fft_lengths)
jax.lax.lax_fft.fft_impl(x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_translation_rule(c,x,fft_type,fft_lengths)
jax.lax.lax_fft.fft_transpose_rule(t,fft_type,fft_lengths)

